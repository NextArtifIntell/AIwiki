<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TNNLS_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tnnls---484">TNNLS - 484</h2>
<ul>
<li><details>
<summary>
(2021). IEEE transactions on neural networks and learning systems
information for authors. <em>TNNLS</em>, <em>32</em>(12), C4. (<a
href="https://doi.org/10.1109/TNNLS.2021.3126954">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Provides a listing of current committee members and society officers.},
  archive      = {J_TNNLS},
  doi          = {10.1109/TNNLS.2021.3126954},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {C4},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {IEEE transactions on neural networks and learning systems information for authors},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). IEEE computational intelligence society information.
<em>TNNLS</em>, <em>32</em>(12), C3. (<a
href="https://doi.org/10.1109/TNNLS.2021.3126956">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Provides a listing of current committee members and society officers.},
  archive      = {J_TNNLS},
  doi          = {10.1109/TNNLS.2021.3126956},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {C3},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {IEEE computational intelligence society information},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An occlusion compensation learning framework for improving
the rendering quality of light field. <em>TNNLS</em>, <em>32</em>(12),
5738–5752. (<a
href="https://doi.org/10.1109/TNNLS.2020.3027468">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Occlusions are common phenomena in light field rendering (LFR) technology applications. The 3-D spatial structures of some features may be missing or incorrect when capturing some samples due to occlusion discontinuities. Most prior works on LFR, however, have neglected occlusions from other objects in 3-D scenes that do not participate in the capturing and rendering of the light field. To improve rendering quality, this report proposes an occlusion probability learning framework (OPLF) based on a deep Boltzmann machine (DBM) to compensate for the occluded information. In the OPLF, an occlusion probability density model is applied to calculate the visibility scores, which are modeled as hidden variables. Additionally, the probability of occlusion is related to the visibility, the camera configuration (i.e., position and direction), and the relationship between the occlusion object and occluded object. Furthermore, a deep probability model based on the OPLF is used for learning the occlusion relationship between the camera and object in multiple layers. The proposed OPLF can optimize the LFR quality. Finally, to verify the claimed performance, we also compare the OPLF with the most advanced occlusion theory and light field reconstruction algorithms. The experimental results show that the proposed OPLF outperforms other known occlusion quantization schemes.},
  archive      = {J_TNNLS},
  author       = {Changjian Zhu and Hong Zhang and Weiyan Chen and Min Tan and Qiuming Liu},
  doi          = {10.1109/TNNLS.2020.3027468},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {5738-5752},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {An occlusion compensation learning framework for improving the rendering quality of light field},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Distributed nesterov gradient and heavy-ball double
accelerated asynchronous optimization. <em>TNNLS</em>, <em>32</em>(12),
5723–5737. (<a
href="https://doi.org/10.1109/TNNLS.2020.3027381">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we come up with a novel Nesterov gradient and heavy-ball double accelerated distributed synchronous optimization algorithm, called NHDA, and adopt a general asynchronous model to further propose an effective asynchronous algorithm, called ASY-NHDA, for distributed optimization problem over directed graphs, where each agent has access to a local objective function and computes the optimal solution via communicating only with its immediate neighbors. Our goal is to minimize a sum of all local objective functions satisfying strong convexity and Lipschitz continuity. Consider a general asynchronous model, where agents communicate with their immediate neighbors and start a new computation independently, that is, agents can communicate with their neighbors at any time without any coordination and use delayed information from their in-neighbors to compute a new update. Delays are arbitrary, unpredictable, and time-varying but bounded. The theoretical analysis of NHDA is based on analyzing the interaction among the consensus, the gradient tracking, and the optimization processes. As for the analysis of ASY-NHDA, we equivalently transform the asynchronous system into an augmented synchronous system without delays and prove its convergence through using the generalized small gain theorem. The results show that NHDA and ASY-NHDA converge to the optimal solution at a linear convergence as long as the largest step size is positive and less than an explicitly estimated upper bound, and the largest momentum parameter is nonnegative and less than an upper bound. Finally, we demonstrate the advantages of ASY-NHDA through simulations.},
  archive      = {J_TNNLS},
  author       = {Huaqing Li and Huqiang Cheng and Zheng Wang and Guo-Cheng Wu},
  doi          = {10.1109/TNNLS.2020.3027381},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {5723-5737},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Distributed nesterov gradient and heavy-ball double accelerated asynchronous optimization},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Semi-supervised domain adaptation via asymmetric joint
distribution matching. <em>TNNLS</em>, <em>32</em>(12), 5708–5722. (<a
href="https://doi.org/10.1109/TNNLS.2020.3027364">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An intrinsic problem in domain adaptation is the joint distribution mismatch between the source and target domains. Therefore, it is crucial to match the two joint distributions such that the source domain knowledge can be properly transferred to the target domain. Unfortunately, in semi-supervised domain adaptation (SSDA) this problem still remains unsolved. In this article, we therefore present an asymmetric joint distribution matching (AJDM) approach, which seeks a couple of asymmetric matrices to linearly match the source and target joint distributions under the relative chi-square divergence. Specifically, we introduce a least square method to estimate the divergence, which is free from estimating the two joint distributions. Furthermore, we show that our AJDM approach can be generalized to a kernel version, enabling it to handle nonlinearity in the data. From the perspective of Riemannian geometry, learning the linear and nonlinear mappings are both formulated as optimization problems defined on the product of Riemannian manifolds. Numerical experiments on synthetic and real-world data sets demonstrate the effectiveness of the proposed approach and testify its superiority over existing SSDA techniques.},
  archive      = {J_TNNLS},
  author       = {Sentao Chen and Mehrtash Harandi and Xiaona Jin and Xiaowei Yang},
  doi          = {10.1109/TNNLS.2020.3027364},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {5708-5722},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Semi-supervised domain adaptation via asymmetric joint distribution matching},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Robust matrix factorization with spectral embedding.
<em>TNNLS</em>, <em>32</em>(12), 5698–5707. (<a
href="https://doi.org/10.1109/TNNLS.2020.3027351">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nonnegative matrix factorization (NMF) and spectral clustering are two of the most widely used clustering techniques. However, NMF cannot deal with the nonlinear data, and spectral clustering relies on the postprocessing. In this article, we propose a Robust Matrix factorization with Spectral embedding (RMS) approach for data clustering, which inherits the advantages of NMF and spectral clustering, while avoiding their shortcomings. In addition, to cluster the data represented by multiple views, we present the multiview version of RMS (M-RMS), and the weights of different views are self-tuned. The main contributions of this research are threefold: 1) by integrating spectral clustering and matrix factorization, the proposed methods are able to capture the nonlinear data structure and obtain the cluster indicator directly; 2) instead of using the squared Frobenius-norm, the objectives are developed with the $\ell _{2,1}$ -norm, such that the effects of the outliers are alleviated; and 3) the proposed methods are totally parameter-free, which increases the applicability for various real-world problems. Extensive experiments on several single-view/multiview data sets demonstrate the effectiveness of our methods and verify their superior clustering performance over the state of the arts.},
  archive      = {J_TNNLS},
  author       = {Mulin Chen and Xuelong Li},
  doi          = {10.1109/TNNLS.2020.3027351},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {5698-5707},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Robust matrix factorization with spectral embedding},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptive neural network finite-time dynamic surface control
for nonlinear systems. <em>TNNLS</em>, <em>32</em>(12), 5688–5697. (<a
href="https://doi.org/10.1109/TNNLS.2020.3027335">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article addresses the problem of finite-time neural network (NN) adaptive dynamic surface control (DSC) design for a class of single-input single-output (SISO) nonlinear systems. Such designs adopt NNs to approximate unknown continuous system functions. To avoid the “explosion of complexity” problem, a novel nonlinear filter is developed in control design. Under the framework of adaptive backstepping control, an NN adaptive finite-time DSC design algorithm is proposed by adopting a smooth projection operator and finite-time Lyapunov stable theory. The developed control algorithm means that the tracking error converges to a small neighborhood of origin within finite time, which further verifies that all the signals of the controlled system possess globally finite-time stability (GFTS). Finally, both numerical and practical simulation examples and comparing results are provided to elucidate the superiority and effectiveness of the proposed control algorithm.},
  archive      = {J_TNNLS},
  author       = {Kewen Li and Yongming Li},
  doi          = {10.1109/TNNLS.2020.3027335},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {5688-5697},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive neural network finite-time dynamic surface control for nonlinear systems},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Global robust exponential dissipativity of uncertain
second-order BAM neural networks with mixed time-varying delays.
<em>TNNLS</em>, <em>32</em>(12), 5675–5687. (<a
href="https://doi.org/10.1109/TNNLS.2020.3027326">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article focuses on the global robust exponential dissipativity (GRED) of uncertain second-order BAM neural networks with mixed time-varying delays. First, a new differential inequality for the concerned second-order system is established. Second, by constructing some new Lyapunov–Krasovskii functionals (LKFs) and applying this new inequality and some other inequalities, some new GRED criteria in the form of linear matrix inequalities are presented. The global exponential attractive sets are also provided simultaneously. Different from the existing reduced-order methods, this article considers some new LKFs to directly analyze the dynamics of the addressed system via a nonreduced-order strategy. Finally, the correctness of the theoretical results is verified by simulation experiments.},
  archive      = {J_TNNLS},
  author       = {Kai Wu and Jigui Jian},
  doi          = {10.1109/TNNLS.2020.3027326},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {5675-5687},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Global robust exponential dissipativity of uncertain second-order BAM neural networks with mixed time-varying delays},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Automatic searching and pruning of deep neural networks for
medical imaging diagnostic. <em>TNNLS</em>, <em>32</em>(12), 5664–5674.
(<a href="https://doi.org/10.1109/TNNLS.2020.3027308">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The field of medical imaging diagnostic makes use of a modality of imaging tests, e.g., X-rays, ultrasounds, computed tomographies, and magnetic resonance imaging, to assist physicians with the diagnostic of patients’ illnesses. Due to their state-of-the-art results in many challenging image classification tasks, deep neural networks (DNNs) are suitable tools for use by physicians to provide diagnostic support when dealing with medical images. To further advance the field, the present work proposes a two-phase algorithm capable of automatically generating compact DNN architectures given a database, called here DNNDeepeningPruning. In the first phase, also called the deepening phase, the algorithm grows a DNN by adding blocks of residual layers one after another until the model overfits the given data. In the second phase, called the pruning phase, the algorithm prunes the created DNN model from the first phase to produce a DNN with a small amount of floating-point operations guided by some preference given by the user. The proposed algorithm unifies the two separate fields of DNN architecture searching and pruning under a single framework, and it is tested in two medical imaging data sets with satisfactory results.},
  archive      = {J_TNNLS},
  author       = {Francisco Erivaldo Fernandes and Gary G. Yen},
  doi          = {10.1109/TNNLS.2020.3027308},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {5664-5674},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Automatic searching and pruning of deep neural networks for medical imaging diagnostic},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Decentralized adaptive optimal tracking control for massive
autonomous vehicle systems with heterogeneous dynamics: A stackelberg
game. <em>TNNLS</em>, <em>32</em>(12), 5654–5663. (<a
href="https://doi.org/10.1109/TNNLS.2021.3100417">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, a decentralized optimal tracking control problem has been studied for a large-scale autonomous vehicle system with heterogeneous system dynamics. Due to the ultralarge number of agents, the notorious “curse of dimension” problem as well as the unrealistic assumption of the existence of reliable very large-scale communication links in uncertain environments have challenged the traditional multiagent system (MAS) algorithms for decades. The emerging mean-field game (MFG) theory has recently been widely adopted to generate a decentralized control method that deals with those challenges by encoding the large scale MASs’ information into a novel time-varying probability density functions (PDF) which can be obtained locally. However, the traditional MFG methods assume all agents are homogeneous, which is unrealistic in practical industrial applications, e.g., Internet of Things (IoTs), and so on. Therefore, a novel mean-field Stackelberg game (MFSG) is formulated based on the Stackelberg game, where all the agents have been classified as two different categories where one major leader’s decision dominates the other minor agents. Moreover, a hierarchical structure that treats all minor agents as a mean-field group is developed to tackle the assumption of homogeneous agents. Then, the actor-actor–critic–critic-mass ( $A^{2}C^{2}M$ ) algorithm with five neural networks is designed to learn the optimal policies by solving the MFSG. The Lyapunov theory is utilized to prove the convergence of $A^{2}C^{2}M$ neural networks and the closed-loop system’s stability. Finally, a series of numerical simulations are conducted to demonstrate the effectiveness of the developed method.},
  archive      = {J_TNNLS},
  author       = {Zejian Zhou and Hao Xu},
  doi          = {10.1109/TNNLS.2021.3100417},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {5654-5663},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Decentralized adaptive optimal tracking control for massive autonomous vehicle systems with heterogeneous dynamics: A stackelberg game},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Path tracking control of autonomous vehicles subject to
deception attacks via a learning-based event-triggered mechanism.
<em>TNNLS</em>, <em>32</em>(12), 5644–5653. (<a
href="https://doi.org/10.1109/TNNLS.2021.3056764">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates the problem of event-triggered secure path tracking control of autonomous ground vehicles (AGVs) under deception attacks. To relieve the burden of the shareable vehicle communication network and to improve the tracking performance in the presence of deception attacks, a learning-based event-triggered mechanism (ETM) is proposed. Different from existing ETMs, the triggering threshold of the proposed mechanism can be dynamically adjusted with conditions of the latest vehicle state. Each vehicle in this study is deemed as an agent, under which a novel control strategy is developed for these autonomous agents with deception attacks. With the assistance of Lyapunov stability theory, sufficient conditions are obtained to guarantee the stability and stabilization of the overall system. Finally, a simulation example is provided to demonstrate the effectiveness of the proposed theoretical results.},
  archive      = {J_TNNLS},
  author       = {Zhou Gu and Tingting Yin and Zhengtao Ding},
  doi          = {10.1109/TNNLS.2021.3056764},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {5644-5653},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Path tracking control of autonomous vehicles subject to deception attacks via a learning-based event-triggered mechanism},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust lidar-based localization scheme for unmanned ground
vehicle via multisensor fusion. <em>TNNLS</em>, <em>32</em>(12),
5633–5643. (<a
href="https://doi.org/10.1109/TNNLS.2020.3027983">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a robust and precise localization scheme for unmanned ground vehicle (UGV) in global positioning system (GPS)-denied and GPS-challenged environments via multisensor fusion approach. The localization scheme is proposed to be under an available point-cloud map. First, initialization in localization module is designed to calculate the initial position of UGV in map using the Gaussian projection approach and obtain the frame transformation between the 3-D lidar and the inertial measurement unit (IMU). Second, the best alignment between each scan frame and the available submap is obtained, and the pose of vehicle relative to the origin of map is calculated. Third, the precise pose of UGV is well predicted by integrating the data from 3-D lidar and IMU. Fourth, in order to visually verify the proposed localization scheme, the motion of vehicle is visualized by designing the visualization module. Note that the preprocessing module aims to process the raw scan data. The availability of our proposed localization scheme is verified by conducting experiments in our campus.},
  archive      = {J_TNNLS},
  author       = {Yuanqing Wu and Yanzhou Li and Wenhao Li and Hongyi Li and Renquan Lu},
  doi          = {10.1109/TNNLS.2020.3027983},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {5633-5643},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Robust lidar-based localization scheme for unmanned ground vehicle via multisensor fusion},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Leader-following event-triggered adaptive practical
consensus of multiple rigid spacecraft systems over jointly connected
networks. <em>TNNLS</em>, <em>32</em>(12), 5623–5632. (<a
href="https://doi.org/10.1109/TNNLS.2021.3056141">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we study the leader-following practical attitude consensus problem of a group of multiple uncertain rigid spacecraft systems over jointly connected networks by a distributed event-triggered control law. We first establish a lemma that allows the problem to be converted to a distributed practical stabilization problem of a well-defined uncertain dynamical system. Then, we combine the adaptive distributed observer technique and the adaptive control technique to design an event-triggered adaptive control law and an event-triggered mechanism to solve our problem. The effectiveness of our design is illustrated by a numerical example.},
  archive      = {J_TNNLS},
  author       = {Tianqi Wang and Jie Huang},
  doi          = {10.1109/TNNLS.2021.3056141},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {5623-5632},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Leader-following event-triggered adaptive practical consensus of multiple rigid spacecraft systems over jointly connected networks},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fault-tolerant adaptive learning control for quadrotor UAVs
with the time-varying CoG and full-state constraints. <em>TNNLS</em>,
<em>32</em>(12), 5610–5622. (<a
href="https://doi.org/10.1109/TNNLS.2021.3071094">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing control methods for quadrotor unmanned aerial vehicles (UAVs) are based on the primary assumption that the center of gravity (CoG) is fixed and is in the same position as the centroid, which is not necessarily true with swing load as continuously making CoG vary with the swing angle and substantially complicating the dynamic model of UAV. This article presents an adaptive learning and fault-tolerant control scheme for quadrotor UAVs with varying CoG and unknown moment of inertia. First, we establish the dynamic model of quadrotor UAVs in the presence of time-varying CoG, input saturation, and actuator fault. Then, we design a fault-tolerant adaptive learning controller for the quadrotor UAVs and show that both linear and angular velocity tracking errors are ensured to converge to a residual set around zero in the presence of full-state constraints. Furthermore, all signals in the closed-loop system are uniformly ultimately bounded. Simulation studies also confirm the effectiveness of the proposed control method.},
  archive      = {J_TNNLS},
  author       = {Zhixi Shen and Lian Tan and Shuangshuang Yu and Yongduan Song},
  doi          = {10.1109/TNNLS.2021.3071094},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {5610-5622},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Fault-tolerant adaptive learning control for quadrotor UAVs with the time-varying CoG and full-state constraints},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptive integral sliding mode control using fully connected
recurrent neural network for position and attitude control of quadrotor.
<em>TNNLS</em>, <em>32</em>(12), 5595–5609. (<a
href="https://doi.org/10.1109/TNNLS.2021.3071020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes an adaptive integral sliding mode control (ISMC) strategy for quadrotor control that ensures faster and finite-time convergence along with chattering attenuation. Quadrotor dynamics are assumed to be unknown because of the high degree of parametric uncertainties, including external disturbances. The equivalent control law obtained by ISMC consists of quadrotor dynamics and, thus, cannot be applied to the quadrotor. A new fully connected recurrent neural network (FCRNN) controller has been proposed to mimic the equivalent control instead of estimating the Quadrotor dynamics separately. The proposed FCRNN architecture consists of output feedback to the input layer and the hidden layer, which enhances the approximation capability of FCRNN. All hidden layer neurons receive self-feedback and feedback from other hidden layer neurons, which further strengthens FCRNN’s potential to capture complex dynamic characteristics. As learning should happen in finite time, the finite-time stability of the overall system has been guaranteed using the Lyapunov stability theory, and the update laws for FCRNN weights in real time are derived using the same. To show the effectiveness of the proposed approach, a comprehensive analysis has been done against existing SMC strategy and against well-known function approximation techniques, e.g., the radial basis function network (RBFN) and RNN.},
  archive      = {J_TNNLS},
  author       = {Subhash Chand Yogi and Vibhu Kumar Tripathi and Laxmidhar Behera},
  doi          = {10.1109/TNNLS.2021.3071020},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {5595-5609},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive integral sliding mode control using fully connected recurrent neural network for position and attitude control of quadrotor},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Data-driven adaptive disturbance observers for model-free
trajectory tracking control of maritime autonomous surface ships.
<em>TNNLS</em>, <em>32</em>(12), 5584–5594. (<a
href="https://doi.org/10.1109/TNNLS.2021.3093330">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we address the disturbance/ uncertainty estimation of maritime autonomous surface ships (MASSs) with unknown internal dynamics, unknown external disturbances, and unknown input gains. In contrast to existing disturbance observers where some prior knowledge on kinetic model parameters such as the control input gains is available in advance, reduced- and full-order data-driven adaptive disturbance observers (DADOs) are proposed for estimating unknown input gains, as well as total disturbance composed of unknown internal dynamics and external disturbances. An advantage of the proposed DADOs is that the total disturbance and input gains can be simultaneously estimated with guaranteed convergence via data-driven adaption. We apply the proposed full-order DADO for the trajectory tracking control of an MASS without kinetic modeling and present a model-free trajectory tracking control law for the ship based on the DADO and a backstepping technique. We report the simulation results to substantiate the efficacy of the proposed DADO approach to model-free trajectory tracking control of an autonomous surface ship without knowing its dynamics.},
  archive      = {J_TNNLS},
  author       = {Zhouhua Peng and Dan Wang and Jun Wang},
  doi          = {10.1109/TNNLS.2021.3093330},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {5584-5594},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Data-driven adaptive disturbance observers for model-free trajectory tracking control of maritime autonomous surface ships},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Composite learning control of overactuated manned
submersible vehicle with disturbance/uncertainty and measurement noise.
<em>TNNLS</em>, <em>32</em>(12), 5575–5583. (<a
href="https://doi.org/10.1109/TNNLS.2021.3053292">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, a novel composite learning control scheme based on nonlinear disturbance observer (NDOB), neural network (NN), and model-based state observer (MSOB) is investigated for the manned submersible vehicle. First, an MSOB is employed to reconstruct the real output signals from noise-contained measurements. Second, a composite estimation is developed where an NDOB is designed to estimate external disturbance and an NN is employed for model uncertainty. Furthermore, a control allocation technique is used to address the overactuated problem of the manned submersible vehicle. The rigorous stability analysis of the closed-loop manned submersible system is given via the Lyapunov theorem. Finally, several representative simulation results illustrate the superior control performance of the composite learning control scheme for the manned submersible vehicle.},
  archive      = {J_TNNLS},
  author       = {Xing Fang and Fei Liu and Xiang Gao},
  doi          = {10.1109/TNNLS.2021.3053292},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {5575-5583},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Composite learning control of overactuated manned submersible vehicle with Disturbance/Uncertainty and measurement noise},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Virtual guidance-based coordinated tracking control of
multi-autonomous underwater vehicles using composite neural learning.
<em>TNNLS</em>, <em>32</em>(12), 5565–5574. (<a
href="https://doi.org/10.1109/TNNLS.2021.3057068">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a virtual leader-based coordinated controller for the nonlinear multiple autonomous underwater vehicles (multi-AUVs) with the system uncertainties. To achieve the coordinated formation, a virtual AUV is set as the leader, while the desired command is designed using the relative position between each AUV and the virtual leader. The controller is designed based on the back-stepping scheme, and the online data-based learning scheme is used for uncertainty approximation. The highlight is that compared with previous learning methods which mostly focus on stability, the learning performance index is constructed using the collected online data in this article. The index is further used in the composite update law of the neural weights. The closed-loop system stability is analyzed via the Lyapunov approach. The simulation test on the five AUVs under fixed formation shows that the proposed method can achieve higher tracking performance with improved approximation accuracy.},
  archive      = {J_TNNLS},
  author       = {Yingxin Shou and Bin Xu and Aidong Zhang and Tao Mei},
  doi          = {10.1109/TNNLS.2021.3057068},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {5565-5574},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Virtual guidance-based coordinated tracking control of multi-autonomous underwater vehicles using composite neural learning},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Trajectory tracking control of autonomous ground vehicles
using adaptive learning MPC. <em>TNNLS</em>, <em>32</em>(12), 5554–5564.
(<a href="https://doi.org/10.1109/TNNLS.2020.3048305">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, an adaptive learning model predictive control (ALMPC) scheme is proposed for the trajectory tracking of perturbed autonomous ground vehicles (AGVs) subject to input constraints. In order to estimate the unknown system parameter, we propose a set-membership-based parameter estimator based on the recursive least-squares (RLS) technique with the ensured nonincreasing estimation error. Then, the estimated system parameter is employed in MPC to improve the prediction accuracy. In the proposed ALMPC scheme, a robustness constraint is introduced into the MPC optimization to handle parametric and additive uncertainties. For the designed robustness constraint, its shape is decided off-line based on the invariant set, whereas its shrinkage rate is updated online according to the estimated upper bound of the estimation error, leading to further reduced conservatism and slightly increased computational complexity compared with the robust MPC methods. Furthermore, it is theoretically shown that the proposed ALMPC algorithm is recursively feasible under some derived conditions, and the closed-loop system is input-to-state stable (ISS). Finally, a numerical example and comparison study are conducted to illustrate the efficacy of the proposed method.},
  archive      = {J_TNNLS},
  author       = {Kunwu Zhang and Qi Sun and Yang Shi},
  doi          = {10.1109/TNNLS.2020.3048305},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {5554-5564},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Trajectory tracking control of autonomous ground vehicles using adaptive learning MPC},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fractional-order adaptive fault-tolerant synchronization
tracking control of networked fixed-wing UAVs against actuator-sensor
faults via intelligent learning mechanism. <em>TNNLS</em>,
<em>32</em>(12), 5539–5553. (<a
href="https://doi.org/10.1109/TNNLS.2021.3059933">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents an enhanced fault-tolerant synchronization tracking control scheme using fractional-order (FO) calculus and intelligent learning architecture for networked fixed-wing unmanned aerial vehicles (UAVs) against actuator and sensor faults. To increase the flight safety of networked UAVs, a recurrent wavelet fuzzy neural network (RWFNN) learning system with feedback loops is first designed to compensate for the unknown terms induced by the inherent nonlinearities, unexpected actuator, and sensor faults. Then, FO sliding-mode control (FOSMC), involving the adjustable FO operators and the robustness of SMC, are dexterously proposed to further enhance flight safety and reduce synchronization tracking errors. Moreover, the dynamic parameters of the RWFNN learning system embedded in the networked fixed-wing UAVs are updated based on adaptive laws. Furthermore, the Lyapunov analysis ensures that all fixed-wing UAVs can synchronously track their references with bounded tracking errors. Finally, comparative simulations and hardware-in-the-loop experiments are conducted to demonstrate the validity of the proposed control scheme.},
  archive      = {J_TNNLS},
  author       = {Ziquan Yu and Youmin Zhang and Bin Jiang and Chun-Yi Su and Jun Fu and Ying Jin and Tianyou Chai},
  doi          = {10.1109/TNNLS.2021.3059933},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {5539-5553},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Fractional-order adaptive fault-tolerant synchronization tracking control of networked fixed-wing UAVs against actuator-sensor faults via intelligent learning mechanism},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptive decision-making for automated vehicles under
roundabout scenarios using optimization embedded reinforcement learning.
<em>TNNLS</em>, <em>32</em>(12), 5526–5538. (<a
href="https://doi.org/10.1109/TNNLS.2020.3042981">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The roundabout is a typical changeable, interactive scenario in which automated vehicles should make adaptive and safe decisions. In this article, an optimization embedded reinforcement learning (OERL) is proposed to achieve adaptive decision-making under the roundabout. The promotion is the modified actor of the Actor–Critic framework, which embeds the model-based optimization method in reinforcement learning to explore continuous behaviors in action space directly. Therefore, the proposed method can determine the macroscale behavior (change lane or not) and medium-scale behaviors of desired acceleration and action time simultaneously with high sample efficiency. When scenarios change, medium-scale behaviors can be adjusted timely by the embedded direct search method, promoting the adaptability of decision-making. More notably, the modified actor matches human drivers’ behaviors, macroscale behavior captures the human mind’s jump, and medium-scale behaviors are preferentially adjusted through driving skills. To enable the agent adapts to different types of the roundabout, task representation is designed to restructure the policy network. In experiments, the algorithm efficiency and the learned driving strategy are compared with decision-making containing macroscale behavior and constant medium-scale behaviors of the desired acceleration and action time. To investigate the adaptability, the performance under an untrained type of roundabout and two more dangerous situations are simulated to verify that the proposed method changes the decisions with changeable scenarios accordingly. The results show that the proposed method has high algorithm efficiency and better system performance.},
  archive      = {J_TNNLS},
  author       = {Yuxiang Zhang and Bingzhao Gao and Lulu Guo and Hongyan Guo and Hong Chen},
  doi          = {10.1109/TNNLS.2020.3042981},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {5526-5538},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive decision-making for automated vehicles under roundabout scenarios using optimization embedded reinforcement learning},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptive iterative learning control of multiple autonomous
vehicles with a time-varying reference under actuator faults.
<em>TNNLS</em>, <em>32</em>(12), 5512–5525. (<a
href="https://doi.org/10.1109/TNNLS.2021.3069209">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, a distributed adaptive iterative learning control for a group of uncertain autonomous vehicles with a time-varying reference is presented, where the autonomous vehicles are underactuated with parametric uncertainties, the actuators are subject to faults, and the control gains are not fully known. A time-varying reference is adopted, the assumption that the trajectory of the leader is linearly parameterized with some known functions is relaxed, and the control inputs are smooth. To design distributed control scheme for each vehicle, a local compensatory variable is generated based on information collected from its neighbors. The composite energy function is used in stability analysis. It is shown that uniform convergence of consensus errors is guaranteed. An illustrative example is given to demonstrate the effectiveness of the proposed control scheme.},
  archive      = {J_TNNLS},
  author       = {Jiangshuai Huang and Wei Wang and Xiaojie Su},
  doi          = {10.1109/TNNLS.2021.3069209},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {5512-5525},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive iterative learning control of multiple autonomous vehicles with a time-varying reference under actuator faults},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptive resilient event-triggered control design of
autonomous vehicles with an iterative single critic learning framework.
<em>TNNLS</em>, <em>32</em>(12), 5502–5511. (<a
href="https://doi.org/10.1109/TNNLS.2021.3053269">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates the adaptive resilient event-triggered control for rear-wheel-drive autonomous (RWDA) vehicles based on an iterative single critic learning framework, which can effectively balance the frequency/changes in adjusting the vehicle’s control during the running process. According to the kinematic equation of RWDA vehicles and the desired trajectory, the tracking error system during the autonomous driving process is first built, where the denial-of-service (DoS) attacking signals are injected into the networked communication and transmission. Combining the event-triggered sampling mechanism and iterative single critic learning framework, a new event-triggered condition is developed for the adaptive resilient control algorithm, and the novel utility function design is considered for driving the autonomous vehicle, where the control input can be guaranteed into an applicable saturated bound. Finally, we apply the new adaptive resilient control scheme to a case of driving the RWDA vehicles, and the simulation results illustrate the effectiveness and practicality successfully.},
  archive      = {J_TNNLS},
  author       = {Kun Zhang and Rong Su and Huaguang Zhang and Yunlin Tian},
  doi          = {10.1109/TNNLS.2021.3053269},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {5502-5511},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive resilient event-triggered control design of autonomous vehicles with an iterative single critic learning framework},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Decentralized adaptive neuro-output feedback saturated
control for INS and its application to AUV. <em>TNNLS</em>,
<em>32</em>(12), 5492–5501. (<a
href="https://doi.org/10.1109/TNNLS.2021.3050992">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates the problem of the decentralized adaptive output feedback saturated control problem for interconnected nonlinear systems with strong interconnections. A decentralized linear observer is first established to estimate the unknown states. Then, an auxiliary system is constructed to offset the effect of input saturation. With the aid of graph theory and neural network technique, a decentralized adaptive neuro-output feedback saturated controller is designed in a nonrecursive manner. A sufficient criterion is established to achieve the uniform ultimate boundedness (UUB) of the closed-loop system. An application example of autonomous underwater vehicle (AUV) is provided to verify the effectiveness of the developed algorithm.},
  archive      = {J_TNNLS},
  author       = {Guangdeng Zong and Haibin Sun and Sing Kiong Nguang},
  doi          = {10.1109/TNNLS.2021.3050992},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {5492-5501},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Decentralized adaptive neuro-output feedback saturated control for INS and its application to AUV},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DynaNet: Neural kalman dynamical model for motion estimation
and prediction. <em>TNNLS</em>, <em>32</em>(12), 5479–5491. (<a
href="https://doi.org/10.1109/TNNLS.2021.3112460">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamical models estimate and predict the temporal evolution of physical systems. State-space models (SSMs) in particular represent the system dynamics with many desirable properties, such as being able to model uncertainty in both the model and measurements, and optimal (in the Bayesian sense) recursive formulations, e.g., the Kalman filter. However, they require significant domain knowledge to derive the parametric form and considerable hand tuning to correctly set all the parameters. Data-driven techniques, e.g., recurrent neural networks, have emerged as compelling alternatives to SSMs with wide success across a number of challenging tasks, in part due to their impressive capability to extract relevant features from rich inputs. They, however, lack interpretability and robustness to unseen conditions. Thus, data-driven models are hard to be applied in safety-critical applications, such as self-driving vehicles. In this work, we present DynaNet, a hybrid deep learning and time-varying SSM, which can be trained end-to-end. Our neural Kalman dynamical model allows us to exploit the relative merits of both SSM and deep neural networks. We demonstrate its effectiveness in the estimation and prediction on a number of physically challenging tasks, including visual odometry, sensor fusion for visual-inertial navigation, and motion prediction. In addition, we show how DynaNet can indicate failures through investigation of properties, such as the rate of innovation (Kalman gain).},
  archive      = {J_TNNLS},
  author       = {Changhao Chen and Chris Xiaoxuan Lu and Bing Wang and Niki Trigoni and Andrew Markham},
  doi          = {10.1109/TNNLS.2021.3112460},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {5479-5491},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {DynaNet: Neural kalman dynamical model for motion estimation and prediction},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). USV formation and path-following control via deep
reinforcement learning with random braking. <em>TNNLS</em>,
<em>32</em>(12), 5468–5478. (<a
href="https://doi.org/10.1109/TNNLS.2021.3068762">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article addresses the problem of path following for underactuated unmanned surface vessels (USVs) formation via a modified deep reinforcement learning with random braking (DRLRB). A formation control model based on deep reinforcement learning (DRL) is constructed to urge USVs to form a preset formation. Specifically, an efficient reward function is designed from the perspective of velocity and error distance of each USV related to the given formation, and then a novel random braking mechanism is formulated to prevent the training of the decision-making network from falling into the local optimum and failing to achieve the training objectives. Following that, a virtual leader-based path-following guidance system is developed for the USV formation problem. Wherein, with the aid of DRLRB, our proposed system can adjust formation automatically and flexibly even when some USVs deviate from the formation. Simulation verifies the effectiveness and superiority of our formation and path-following control strategy.},
  archive      = {J_TNNLS},
  author       = {Yujiao Zhao and Yong Ma and Songlin Hu},
  doi          = {10.1109/TNNLS.2021.3068762},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {5468-5478},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {USV formation and path-following control via deep reinforcement learning with random braking},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Data-driven performance-prescribed reinforcement learning
control of an unmanned surface vehicle. <em>TNNLS</em>, <em>32</em>(12),
5456–5467. (<a
href="https://doi.org/10.1109/TNNLS.2021.3056444">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An unmanned surface vehicle (USV) under complicated marine environments can hardly be modeled well such that model-based optimal control approaches become infeasible. In this article, a self-learning-based model-free solution only using input–output signals of the USV is innovatively provided. To this end, a data-driven performance-prescribed reinforcement learning control (DPRLC) scheme is created to pursue control optimality and prescribed tracking accuracy simultaneously. By devising state transformation with prescribed performance, constrained tracking errors are substantially converted into constraint-free stabilization of tracking errors with unknown dynamics. Reinforcement learning paradigm using neural network-based actor–critic learning framework is further deployed to directly optimize controller synthesis deduced from the Bellman error formulation such that transformed tracking errors evolve a data-driven optimal controller. Theoretical analysis eventually ensures that the entire DPRLC scheme can guarantee prescribed tracking accuracy, subject to optimal cost. Both simulations and virtual-reality experiments demonstrate the remarkable effectiveness and superiority of the proposed DPRLC scheme.},
  archive      = {J_TNNLS},
  author       = {Ning Wang and Ying Gao and Xuefeng Zhang},
  doi          = {10.1109/TNNLS.2021.3056444},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {5456-5467},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Data-driven performance-prescribed reinforcement learning control of an unmanned surface vehicle},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Visual navigation with multiple goals based on deep
reinforcement learning. <em>TNNLS</em>, <em>32</em>(12), 5445–5455. (<a
href="https://doi.org/10.1109/TNNLS.2021.3057424">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning to adapt to a series of different goals in visual navigation is challenging. In this work, we present a model-embedded actor–critic architecture for the multigoal visual navigation task. To enhance the task cooperation in multigoal learning, we introduce two new designs to the reinforcement learning scheme: inverse dynamics model (InvDM) and multigoal colearning (MgCl) . Specifically, InvDM is proposed to capture the navigation-relevant association between state and goal and provide additional training signals to relieve the sparse reward issue. MgCl aims at improving the sample efficiency and supports the agent to learn from unintentional positive experiences. Besides, to further improve the scene generalization capability of the agent, we present an enhanced navigation model that consists of two self-supervised auxiliary task modules. The first module, which is named path closed-loop detection, helps to understand whether the state has been experienced. The second one, namely the state-target matching module, tries to figure out the difference between state and goal. Extensive results on the interactive platform AI2-THOR demonstrate that the agent trained with the proposed method converges faster than state-of-the-art methods while owning good generalization capability. The video demonstration is available at https://vsislab.github.io/mgvn .},
  archive      = {J_TNNLS},
  author       = {Zhenhuan Rao and Yuechen Wu and Zifei Yang and Wei Zhang and Shijian Lu and Weizhi Lu and ZhengJun Zha},
  doi          = {10.1109/TNNLS.2021.3057424},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {5445-5455},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Visual navigation with multiple goals based on deep reinforcement learning},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Safe reinforcement learning with stability guarantee for
motion planning of autonomous vehicles. <em>TNNLS</em>, <em>32</em>(12),
5435–5444. (<a
href="https://doi.org/10.1109/TNNLS.2021.3084685">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reinforcement learning with safety constraints is promising for autonomous vehicles, of which various failures may result in disastrous losses. In general, a safe policy is trained by constrained optimization algorithms, in which the average constraint return as a function of states and actions should be lower than a predefined bound. However, most existing safe learning-based algorithms capture states via multiple high-precision sensors, which complicates the hardware systems and is power-consuming. This article is focused on safe motion planning with the stability guarantee for autonomous vehicles with limited size and power. To this end, the risk-identification method and the Lyapunov function are integrated with the well-known soft actor–critic (SAC) algorithm. By borrowing the concept of Lyapunov functions in the control theory, the learned policy can theoretically guarantee that the state trajectory always stays in a safe area. A novel risk-sensitive learning-based algorithm with the stability guarantee is proposed to train policies for the motion planning of autonomous vehicles. The learned policy is implemented on a differential drive vehicle in a simulation environment. The experimental results show that the proposed algorithm achieves a higher success rate than the SAC.},
  archive      = {J_TNNLS},
  author       = {Lixian Zhang and Ruixian Zhang and Tong Wu and Rui Weng and Minghao Han and Ye Zhao},
  doi          = {10.1109/TNNLS.2021.3084685},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {5435-5444},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Safe reinforcement learning with stability guarantee for motion planning of autonomous vehicles},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Tracking control of unknown and constrained nonlinear
systems via neural networks with implicit weight and activation
learning. <em>TNNLS</em>, <em>32</em>(12), 5427–5434. (<a
href="https://doi.org/10.1109/TNNLS.2021.3085371">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For systems with irregular (asymmetric and positively-negatively alternating) constraints being imposed/removed during system operation, there is no uniformly applicable control method. In this work, a control design framework is established for uncertain pure-feedback systems subject to the aforementioned constraints. By introducing a novel transformation function and with the help of auxiliary constraining boundaries, the original output-constrained system is augmented to unconstrained one. Unknown nonlinearity is approximated by neural networks (NNs) with not only neural weight updating but also activation online adjustment. The resultant control scheme is able to deal with constraints imposed or removed at some time moments during system operation without the need for altering control structure. When applied to high-speed trains, the developed control scheme ensures position tracking under speed constraints, simulation demands, and confirms the effectiveness of the proposed method.},
  archive      = {J_TNNLS},
  author       = {Qian Cui and Yongduan Song},
  doi          = {10.1109/TNNLS.2021.3085371},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {5427-5434},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Tracking control of unknown and constrained nonlinear systems via neural networks with implicit weight and activation learning},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptive finite-time neural network control of nonlinear
systems with multiple objective constraints and application to
electromechanical system. <em>TNNLS</em>, <em>32</em>(12), 5416–5426.
(<a href="https://doi.org/10.1109/TNNLS.2020.3027689">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates an adaptive finite-time neural control for a class of strict feedback nonlinear systems with multiple objective constraints. In order to solve the main challenges brought by the state constraints and the emergence of finite-time stability, a new barrier Lyapunov function is proposed for the first time, not only can it solve multiobjective constraints effectively but also ensure that all states are always within the constraint intervals. Second, by combining the command filter method and backstepping control, the adaptive controller is designed. What is more, the proposed controller has the ability to avoid the “singularity” problem. The compensation mechanism is introduced to neutralize the error appearing in the filtering process. Furthermore, the neural network is used to approximate the unknown function in the design process. It is shown that the proposed finite-time neural adaptive control scheme achieves a good tracking effect. And each objective function does not violate the constraint bound. Finally, a simulation example of electromechanical dynamic system is given to prove the effectiveness of the proposed finite-time control strategy.},
  archive      = {J_TNNLS},
  author       = {Lei Liu and Wei Zhao and Yan-Jun Liu and Shaocheng Tong and Yue-Ying Wang},
  doi          = {10.1109/TNNLS.2020.3027689},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {5416-5426},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive finite-time neural network control of nonlinear systems with multiple objective constraints and application to electromechanical system},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multitask GANs for semantic segmentation and depth
completion with cycle consistency. <em>TNNLS</em>, <em>32</em>(12),
5404–5415. (<a
href="https://doi.org/10.1109/TNNLS.2021.3072883">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic segmentation and depth completion are two challenging tasks in scene understanding, and they are widely used in robotics and autonomous driving. Although several studies have been proposed to jointly train these two tasks using some small modifications, such as changing the last layer, the result of one task is not utilized to improve the performance of the other one despite that there are some similarities between these two tasks. In this article, we propose multitask generative adversarial networks (Multitask GANs), which are not only competent in semantic segmentation and depth completion but also improve the accuracy of depth completion through generated semantic images. In addition, we improve the details of generated semantic images based on CycleGAN by introducing multiscale spatial pooling blocks and the structural similarity reconstruction loss. Furthermore, considering the inner consistency between semantic and geometric structures, we develop a semantic-guided smoothness loss to improve depth completion results. Extensive experiments on the Cityscapes data set and the KITTI depth completion benchmark show that the Multitask GANs are capable of achieving competitive performance for both semantic segmentation and depth completion tasks.},
  archive      = {J_TNNLS},
  author       = {Chongzhen Zhang and Yang Tang and Chaoqiang Zhao and Qiyu Sun and Zhencheng Ye and Jürgen Kurths},
  doi          = {10.1109/TNNLS.2021.3072883},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {5404-5415},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multitask GANs for semantic segmentation and depth completion with cycle consistency},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Masked GAN for unsupervised depth and pose prediction with
scale consistency. <em>TNNLS</em>, <em>32</em>(12), 5392–5403. (<a
href="https://doi.org/10.1109/TNNLS.2020.3044181">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Previous work has shown that adversarial learning can be used for unsupervised monocular depth and visual odometry (VO) estimation, in which the adversarial loss and the geometric image reconstruction loss are utilized as the mainly supervisory signals to train the whole unsupervised framework. However, the performance of the adversarial framework and image reconstruction is usually limited by occlusions and the visual field changes between the frames. This article proposes a masked generative adversarial network (GAN) for unsupervised monocular depth and ego-motion estimations. The MaskNet and Boolean mask scheme are designed in this framework to eliminate the effects of occlusions and impacts of visual field changes on the reconstruction loss and adversarial loss, respectively. Furthermore, we also consider the scale consistency of our pose network by utilizing a new scale-consistency loss, and therefore, our pose network is capable of providing the full camera trajectory over a long monocular sequence. Extensive experiments on the KITTI data set show that each component proposed in this article contributes to the performance, and both our depth and trajectory predictions achieve competitive performance on the KITTI and Make3D data sets.},
  archive      = {J_TNNLS},
  author       = {Chaoqiang Zhao and Gary G. Yen and Qiyu Sun and Chongzhen Zhang and Yang Tang},
  doi          = {10.1109/TNNLS.2020.3044181},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {5392-5403},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Masked GAN for unsupervised depth and pose prediction with scale consistency},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deductive reinforcement learning for visual autonomous urban
driving navigation. <em>TNNLS</em>, <em>32</em>(12), 5379–5391. (<a
href="https://doi.org/10.1109/TNNLS.2021.3109284">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing deep reinforcement learning (RL) are devoted to research applications on video games, e.g., The Open Racing Car Simulator (TORCS) and Atari games. However, it remains under-explored for vision-based autonomous urban driving navigation (VB-AUDN). VB-AUDN requires a sophisticated agent working safely in structured, changing, and unpredictable environments; otherwise, inappropriate operations may lead to irreversible or catastrophic damages. In this work, we propose a deductive RL (DeRL) to address this challenge. A deduction reasoner (DR) is introduced to endow the agent with ability to foresee the future and to promote policy learning. Specifically, DR first predicts future transitions through a parameterized environment model. Then, DR conducts self-assessment at the predicted trajectory to perceive the consequences of current policy resulting in a more reliable decision-making process. Additionally, a semantic encoder module (SEM) is designed to extract compact driving representation from the raw images, which is robust to the changes of the environment. Extensive experimental results demonstrate that DeRL outperforms the state-of-the-art model-free RL approaches on the public CAR Learning to Act (CARLA) benchmark and presents a superior performance on success rate and driving safety for goal-directed navigation.},
  archive      = {J_TNNLS},
  author       = {Changxin Huang and Ronghui Zhang and Meizi Ouyang and Pengxu Wei and Junfan Lin and Jiang Su and Liang Lin},
  doi          = {10.1109/TNNLS.2021.3109284},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {5379-5391},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Deductive reinforcement learning for visual autonomous urban driving navigation},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Weak human preference supervision for deep reinforcement
learning. <em>TNNLS</em>, <em>32</em>(12), 5369–5378. (<a
href="https://doi.org/10.1109/TNNLS.2021.3084198">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The current reward learning from human preferences could be used to resolve complex reinforcement learning (RL) tasks without access to a reward function by defining a single fixed preference between pairs of trajectory segments. However, the judgment of preferences between trajectories is not dynamic and still requires human input over thousands of iterations. In this study, we proposed a weak human preference supervision framework, for which we developed a human preference scaling model that naturally reflects the human perception of the degree of weak choices between trajectories and established a human-demonstration estimator through supervised learning to generate the predicted preferences for reducing the number of human inputs. The proposed weak human preference supervision framework can effectively solve complex RL tasks and achieve higher cumulative rewards in simulated robot locomotion—MuJoCo games—relative to the single fixed human preferences. Furthermore, our established human-demonstration estimator requires human feedback only for less than 0.01\% of the agent’s interactions with the environment and significantly reduces the cost of human inputs by up to 30\% compared with the existing approaches. To present the flexibility of our approach, we released a video ( https://youtu.be/jQPe1OILT0M ) showing comparisons of the behaviors of agents trained on different types of human input. We believe that our naturally inspired human preferences with weakly supervised learning are beneficial for precise reward learning and can be applied to state-of-the-art RL systems, such as human-autonomy teaming systems.},
  archive      = {J_TNNLS},
  author       = {Zehong Cao and KaiChiu Wong and Chin-Teng Lin},
  doi          = {10.1109/TNNLS.2021.3084198},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {5369-5378},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Weak human preference supervision for deep reinforcement learning},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Consensus control for heterogeneous multivehicle systems: An
iterative learning approach. <em>TNNLS</em>, <em>32</em>(12), 5356–5368.
(<a href="https://doi.org/10.1109/TNNLS.2021.3071413">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates the consensus tracking problem of the heterogeneous multivehicle systems (MVSs) under a repeatable control environment. First, a unified iterative learning control (ILC) algorithm is presented for all autonomous vehicles, each of which is governed by both discrete- and continuous-time nonlinear dynamics. Then, several consensus criteria for MVSs with switching topology and external disturbances are established based on our proposed distributed ILC protocols. For discrete-time systems, all vehicles can perfectly track to the common reference trajectory over a specified finite time interval, and the corresponding digraphs may not have spanning trees. Existing approaches dealing with the continuous-time systems generally require that all vehicles have strictly identical initial conditions, being too ideal in practice. We relax this unpractical assumption and propose an extra distributed initial state learning protocol such that vehicles can take different initial states, leading to the fact that the finite time tracking is achieved ultimately regardless of the initial errors. Finally, a numerical example demonstrates the effectiveness of our theoretical results.},
  archive      = {J_TNNLS},
  author       = {Shuyuan Zhang and Lei Wang and Haihui Wang and Bai Xue},
  doi          = {10.1109/TNNLS.2021.3071413},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {5356-5368},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Consensus control for heterogeneous multivehicle systems: An iterative learning approach},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Visual navigation and landing control of an unmanned aerial
vehicle on a moving autonomous surface vehicle via adaptive learning.
<em>TNNLS</em>, <em>32</em>(12), 5345–5355. (<a
href="https://doi.org/10.1109/TNNLS.2021.3080980">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a visual navigation and landing control paradigm for an unmanned aerial vehicle (UAV) to land on a moving autonomous surface vehicle (ASV). Therein, an adaptive learning navigation rule with a multilayer nested guidance is designed to pinpoint the position of the ASV and to guide and control the UAV to fulfill horizontal tracking and vertical descending in a narrow landing region of the ASV by means of merely relative position feedback. To ensure the feasibility of the proposed control law, asymptotical stability conditions are derived based on Lyapunov stability theory. Landing experimental results are reported for a UAV-ASV system consisting of an M-100 UAV and a self-developed three-meters-long HUSTER-30 ASV on a lake to substantiate the efficacy of the proposed landing control method.},
  archive      = {J_TNNLS},
  author       = {Hai-Tao Zhang and Bin-Bin Hu and Zhecheng Xu and Zhi Cai and Bin Liu and Xudong Wang and Tao Geng and Sheng Zhong and Jin Zhao},
  doi          = {10.1109/TNNLS.2021.3080980},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {5345-5355},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Visual navigation and landing control of an unmanned aerial vehicle on a moving autonomous surface vehicle via adaptive learning},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Distributed path following of multiple under-actuated
autonomous surface vehicles based on data-driven neural predictors via
integral concurrent learning. <em>TNNLS</em>, <em>32</em>(12),
5334–5344. (<a
href="https://doi.org/10.1109/TNNLS.2021.3100147">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article addresses the problem of distributed path following of multiple under-actuated autonomous surface vehicles (ASVs) with completely unknown kinetic models. An integrated distributed guidance and learning control architecture is proposed for achieving a time-varying formation. Specifically, a robust distributed guidance law at the kinematic level is developed based on a consensus approach, a path-following mechanism, and an extended state observer. At the kinetic level, a model-free kinetic control law based on data-driven neural predictors via integral concurrent learning is designed such that the kinetic model can be learned by using recorded data. The advantage of the proposed method is two-folds. First, the proposed formation controllers are able to achieve various time-varying formations without using the velocities of neighboring vehicles. Second, the proposed control law is model-free without any parameter information on kinetic models. Simulation results substantiate the effectiveness of the proposed robust distributed guidance and model-free control laws for multiple under-actuated ASVs with fully unknown kinetic models.},
  archive      = {J_TNNLS},
  author       = {Lu Liu and Dan Wang and Zhouhua Peng and Qing-Long Han},
  doi          = {10.1109/TNNLS.2021.3100147},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {5334-5344},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Distributed path following of multiple under-actuated autonomous surface vehicles based on data-driven neural predictors via integral concurrent learning},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). YolTrack: Multitask learning based real-time multiobject
tracking and segmentation for autonomous vehicles. <em>TNNLS</em>,
<em>32</em>(12), 5323–5333. (<a
href="https://doi.org/10.1109/TNNLS.2021.3056383">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern autonomous vehicles are required to perform various visual perception tasks for scene construction and motion decision. The multiobject tracking and instance segmentation (MOTS) are the main tasks since they directly influence the steering and braking of the car. Implementing both tasks using a multitask learning neural network presents significant challenges in performance and complexity. Current work on MOTS devotes to improve the precision of the network with a two-stage tracking by detection model, which is difficult to satisfy the real-time requirement of autonomous vehicles. In this article, a real-time multitask network named YolTrack based on one-stage instance segmentation model is proposed to perform the MOTS task, achieving an inference speed of 29.5 frames per second (fps) with slight accuracy and precision drop. The YolTrack uses ShuffleNet V2 with feature pyramid network (FPN) as a backbone, from which two decoders are extended to generate instance segments and embedding vectors. Segmentation masks are used to improve the tracking performance by performing logic AND operation with feature maps, proving that foreground segmentation plays an important role in object tracking. The different scales of multiple tasks are balanced by the optimized geometric mean loss during the training phase. Experimental results on the KITTI MOTS data set show that YolTrack outperforms other state-of-the-art MOTS architectures in real-time aspect and is appropriate for deployment in autonomous vehicles.},
  archive      = {J_TNNLS},
  author       = {Xuepeng Chang and Huihui Pan and Weichao Sun and Huijun Gao},
  doi          = {10.1109/TNNLS.2021.3056383},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {5323-5333},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {YolTrack: Multitask learning based real-time multiobject tracking and segmentation for autonomous vehicles},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A reinforcement learning-based vehicle platoon control
strategy for reducing energy consumption in traffic oscillations.
<em>TNNLS</em>, <em>32</em>(12), 5309–5322. (<a
href="https://doi.org/10.1109/TNNLS.2021.3071959">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The vehicle platoon will be the most dominant driving mode on future roads. To the best of our knowledge, few reinforcement learning (RL) algorithms have been applied in vehicle platoon control, which has large-scale action and state spaces. Some RL-based methods were applied to solve single-agent problems. If we need to tackle multiagent problems, we will use multiagent RL algorithms since the parameters space grows exponentially with the increasing number of agents involved. Previous multiagent RL algorithms generally may provide redundant information to agents, indicating a large amount of useless or unrelated information, which may cause to be difficult for convergence training and pattern extractions from shared information. Also, random actions usually contribute to crashes, especially at the beginning of training. In this study, a communication proximal policy optimization (CommPPO) algorithm was proposed to tackle the above issues. In specific, the CommPPO model adopts a parameter-sharing structure to allow the dynamic variation of agent numbers, which can well handle various platoon dynamics, including splitting and merging. The communication protocol of the CommPPO consists of two parts. In the state part, the widely used predecessor–leader follower typology in the platoon is adopted to transmit global and local state information to agents. In the reward part, a new reward communication channel is proposed to solve the spurious reward and “lazy agent” problems in some existing multiagent RLs. Moreover, a curriculum learning approach is adopted to reduce crashes and speed up training. To validate the proposed strategy for platoon control, two existing multiagent RLs and a traditional platoon control strategy were applied in the same scenarios for comparison. Results showed that the CommPPO algorithm gained more rewards and achieved the largest fuel consumption reduction (11.6\%).},
  archive      = {J_TNNLS},
  author       = {Meng Li and Zehong Cao and Zhibin Li},
  doi          = {10.1109/TNNLS.2021.3071959},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {5309-5322},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A reinforcement learning-based vehicle platoon control strategy for reducing energy consumption in traffic oscillations},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Knowledge implementation and transfer with an adaptive
learning network for real-time power management of the plug-in hybrid
vehicle. <em>TNNLS</em>, <em>32</em>(12), 5298–5308. (<a
href="https://doi.org/10.1109/TNNLS.2021.3093429">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Essential decision-making tasks such as power management in future vehicles will benefit from the development of artificial intelligence technology for safe and energy-efficient operations. To develop the technique of using neural network and deep learning in energy management of the plug-in hybrid vehicle and evaluate its advantage, this article proposes a new adaptive learning network that incorporates a deep deterministic policy gradient (DDPG) network with an adaptive neuro-fuzzy inference system (ANFIS) network. First, the ANFIS network is built using a new global K-fold fuzzy learning (GKFL) method for real-time implementation of the offline dynamic programming result. Then, the DDPG network is developed to regulate the input of the ANFIS network with the real-world reinforcement signal. The ANFIS and DDPG networks are integrated to maximize the control utility (CU), which is a function of the vehicle’s energy efficiency and the battery state-of-charge. Experimental studies are conducted to testify the performance and robustness of the DDPG-ANFIS network. It has shown that the studied vehicle with the DDPG-ANFIS network achieves 8\% higher CU than using the MATLAB ANFIS toolbox on the studied vehicle. In five simulated real-world driving conditions, the DDPG-ANFIS network increased the maximum mean CU value by 138\% over the ANFIS-only network and 5\% over the DDPG-only network.},
  archive      = {J_TNNLS},
  author       = {Quan Zhou and Dezong Zhao and Bin Shuai and Yanfei Li and Huw Williams and Hongming Xu},
  doi          = {10.1109/TNNLS.2021.3093429},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {5298-5308},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Knowledge implementation and transfer with an adaptive learning network for real-time power management of the plug-in hybrid vehicle},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An ensemble broad learning scheme for semisupervised vehicle
type classification. <em>TNNLS</em>, <em>32</em>(12), 5287–5297. (<a
href="https://doi.org/10.1109/TNNLS.2021.3083508">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays vehicle type classification is a fundamental part of intelligent transportation systems (ITSs) and is widely used in various applications like traffic flow monitoring, security enforcement, and autonomous driving, etc. However, vehicle classification is usually used in supervised learning, which greatly limits the applicability for real ITS. This article proposes a semisupervised vehicle type classification scheme via ensemble broad learning for ITS. This presented method contains two main parts. In the first part, a collection of base broad learning system (BLS) classifiers is trained by semisupervised learning to avoid time-consuming training process and alleviate the increasingly unlabeled samples burden. In the second part, a dynamic ensemble structure constructed by trained classifier groups with different characteristics obtains the highest type probability and determine which the vehicle belongs, so as to achieve superior generalization performance than a single base classifier. Several experiments conducted on the pubic BIT-Vehicle dataset and MIO-TCD dataset demonstrate that the proposed method outperforms single BLS classifier and some mainstream methods on effectiveness and efficiency.},
  archive      = {J_TNNLS},
  author       = {Li Guo and Runze Li and Bin Jiang},
  doi          = {10.1109/TNNLS.2021.3083508},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {5287-5297},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {An ensemble broad learning scheme for semisupervised vehicle type classification},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Multivehicle task assignment based on collaborative
neurodynamic optimization with discrete hopfield networks.
<em>TNNLS</em>, <em>32</em>(12), 5274–5286. (<a
href="https://doi.org/10.1109/TNNLS.2021.3082528">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a collaborative neurodynamic optimization (CNO) approach to multivehicle task assignments (TAs). The original combinatorial quadratic optimization problem for TA is reformulated as a quadratic unconstrained binary optimization (QUBO) problem with a quadratic utility function and a penalty function for handling load capacity and cooperation constraints. In the framework of CNO with a population of discrete Hopfield networks (DHNs), a TA algorithm is proposed for solving the formulated QUBO problem. Superior experimental results in four typical multivehicle operation scenarios are reported to substantiate the efficacy of the proposed neurodynamics-based TA approach.},
  archive      = {J_TNNLS},
  author       = {Jiasen Wang and Jun Wang and Qing-Long Han},
  doi          = {10.1109/TNNLS.2021.3082528},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {5274-5286},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multivehicle task assignment based on collaborative neurodynamic optimization with discrete hopfield networks},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Gradient descent-based adaptive learning control for
autonomous underwater vehicles with unknown uncertainties.
<em>TNNLS</em>, <em>32</em>(12), 5266–5273. (<a
href="https://doi.org/10.1109/TNNLS.2021.3056585">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates the adaptive learning control problem for a class of nonlinear autonomous underwater vehicles (AUVs) with unknown uncertainties. The unknown nonlinear functions in the AUVs are approximated by radial basis function neural networks (RBFNNs), in which the weight updating laws are designed via gradient descent algorithm. The proposed gradient descent-based control scheme guarantees the semiglobal uniform ultimate boundedness (SUUB) of the system and the fast convergence of the weight updating laws. In order to reduce the computational burden during the backstepping control design process, the command-filter-based design technique is incorporated into the adaptive learning control strategy. Finally, simulation studies are given to demonstrate the effectiveness of the proposed method.},
  archive      = {J_TNNLS},
  author       = {Jianbin Qiu and Min Ma and Tong Wang and Huijun Gao},
  doi          = {10.1109/TNNLS.2021.3056585},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {5266-5273},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Gradient descent-based adaptive learning control for autonomous underwater vehicles with unknown uncertainties},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Guest editorial special issue on adaptive learning and
control for autonomous vehicles. <em>TNNLS</em>, <em>32</em>(12),
5264–5265. (<a
href="https://doi.org/10.1109/TNNLS.2021.3123833">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent developments in the field of neural networks, adaptive learning, and control will enable autonomous vehicles to operate in complex environments including urban, rural, and dangerous environments.},
  archive      = {J_TNNLS},
  author       = {Ahmet Enis Cetin and Qing-Guo Wang},
  doi          = {10.1109/TNNLS.2021.3123833},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {5264-5265},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Guest editorial special issue on adaptive learning and control for autonomous vehicles},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Farewell editorial a heartfelt thank you and looking into
the new era of TNNLS. <em>TNNLS</em>, <em>32</em>(12), 5262–5263. (<a
href="https://doi.org/10.1109/TNNLS.2021.3121784">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {“6 – 72 – 27,846”: Six volumes, seventy-two issues, twenty-seven thousand and eight hundred forty-six pages: How time flies! As I mark down these numbers, this December 2021 issue of IEEE Transactions on Neural Networks and Learning Systems (IEEE TNNLS) also marks the last issue for me as the Editor-in-Chief.},
  archive      = {J_TNNLS},
  author       = {Haibo He},
  doi          = {10.1109/TNNLS.2021.3121784},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {5262-5263},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Farewell editorial a heartfelt thank you and looking into the new era of TNNLS},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). IEEE computational intelligence society information.
<em>TNNLS</em>, <em>32</em>(11), C3. (<a
href="https://doi.org/10.1109/TNNLS.2021.3120363">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Provides a listing of current committee members and society officers.},
  archive      = {J_TNNLS},
  doi          = {10.1109/TNNLS.2021.3120363},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {C3},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {IEEE computational intelligence society information},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Distributed algorithms involving fixed step size for mixed
equilibrium problems with multiple set constraints. <em>TNNLS</em>,
<em>32</em>(11), 5254–5260. (<a
href="https://doi.org/10.1109/TNNLS.2020.3027288">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this brief, the problem of distributively solving a mixed equilibrium problem (EP) with multiple sets is investigated. A network of agents is employed to cooperatively find a point in the intersection of multiple convex sets ensuring that the sum of multiple bifunctions with a free variable is nonnegative. Each agent can only access information associated with its own bifunction and a local convex set. To solve this problem, a distributed algorithm involving a fixed step size is proposed by combining the mirror descent algorithm, the primal-dual algorithm, and the consensus algorithm. Under mild conditions on bifunctions and the graph, we prove that all agents’ states asymptotically converge to a solution of the mixed EP. A numerical simulation example is provided for demonstrating the effectiveness of theoretical results.},
  archive      = {J_TNNLS},
  author       = {Kaihong Lu and Qixin Zhu},
  doi          = {10.1109/TNNLS.2020.3027288},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {5254-5260},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Distributed algorithms involving fixed step size for mixed equilibrium problems with multiple set constraints},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Concept factorization with local centroids. <em>TNNLS</em>,
<em>32</em>(11), 5247–5253. (<a
href="https://doi.org/10.1109/TNNLS.2020.3027068">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data clustering is a fundamental problem in the field of machine learning. Among the numerous clustering techniques, matrix factorization-based methods have achieved impressive performances because they are able to provide a compact and interpretable representation of the input data. However, most of the existing works assume that each class has a global centroid, which does not hold for data with complicated structures. Besides, they cannot guarantee that the sample is associated with the nearest centroid. In this work, we present a concept factorization with the local centroids (CFLCs) approach for data clustering. The proposed model has the following advantages: 1) the samples from the same class are allowed to connect with multiple local centroids such that the manifold structure is captured; 2) the pairwise relationship between the samples and centroids is modeled to produce a reasonable label assignment; and 3) the clustering problem is formulated as a bipartite graph partitioning task, and an efficient algorithm is designed for optimization. Experiments on several data sets validate the effectiveness of the CFLC model and demonstrate its superior performance over the state of the arts.},
  archive      = {J_TNNLS},
  author       = {Mulin Chen and Xuelong Li},
  doi          = {10.1109/TNNLS.2020.3027068},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {5247-5253},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Concept factorization with local centroids},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Interval-based least squares for uncertainty-aware learning
in human-centric multimedia systems. <em>TNNLS</em>, <em>32</em>(11),
5241–5246. (<a
href="https://doi.org/10.1109/TNNLS.2020.3025834">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning (ML) methods are popular in several application areas of multimedia signal processing. However, most existing solutions in the said area, including the popular least squares, rely on penalizing predictions that deviate from the target ground-truth values. In other words, uncertainty in the ground-truth data is simply ignored. As a result, optimization and validation overemphasize a single-target value when, in fact, human subjects themselves did not unanimously agree to it. This leads to an unreasonable scenario where the trained model is not allowed the benefit of the doubt in terms of prediction accuracy. The problem becomes even more significant in the context of more recent human-centric and immersive multimedia systems where user feedback and interaction are influenced by higher degrees of freedom (leading to higher levels of uncertainty in the ground truth). To ameliorate this drawback, we propose an uncertainty aware loss function (referred to as $\text {MSE}^{*}$ ) that explicitly accounts for data uncertainty and is useful for both optimization (training) and validation. As examples, we demonstrate the utility of the proposed method for blind estimation of perceptual quality of audiovisual signals, panoramic images, and images affected by camera-induced distortions. The experimental results support the theoretical ideas in terms of reducing prediction errors. The proposed method is also relevant in the context of more recent paradigms, such as crowdsourcing, where larger uncertainty in ground truth is expected.},
  archive      = {J_TNNLS},
  author       = {Manish Narwaria and Aditya Tatu},
  doi          = {10.1109/TNNLS.2020.3025834},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {5241-5246},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Interval-based least squares for uncertainty-aware learning in human-centric multimedia systems},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Iterative learning model-free control for networked systems
with dual-direction data dropouts and actuator faults. <em>TNNLS</em>,
<em>32</em>(11), 5232–5240. (<a
href="https://doi.org/10.1109/TNNLS.2020.3027651">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we study the tracking problem for networked nonlinear discrete systems with actuator faults and dual-direction data dropouts. A novel adaptive fault-tolerant iterative learning model-free control strategy is designed. First, by utilizing the method called compact form dynamic linearization, the original nonlinear system model is transformed into an equivalent data-driven model, and the data model contains only one unknown parameter. Both the actuator fault and the system dynamics information are included in this parameter. Then, to model the physical processes of data dropout, a new mathematical relationship is constructed. Furthermore, an adaptive fault-tolerant iterative learning tracking control scheme is developed with only randomly received input/output data. Noting that the high learning rate or convergence rate is required in actual applications, a new varying parameter approach is designed to improve such rate. Finally, it is rigorously proved that the closed loop is stable in the sense of uniform ultimate boundedness, and numerical simulation results are conducted to validate the effectiveness of the designed control strategy.},
  archive      = {J_TNNLS},
  author       = {Jiannan Chen and Changchun Hua and Xinping Guan},
  doi          = {10.1109/TNNLS.2020.3027651},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {5232-5240},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Iterative learning model-free control for networked systems with dual-direction data dropouts and actuator faults},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Finite- and fixed-time cluster synchronization of
nonlinearly coupled delayed neural networks via pinning control.
<em>TNNLS</em>, <em>32</em>(11), 5222–5231. (<a
href="https://doi.org/10.1109/TNNLS.2020.3027312">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, the cluster synchronization problem for a class of the nonlinearly coupled delayed neural networks (NNs) in both finite- and fixed-time cases are investigated. Based on the Lyapunov stability theory and pinning control strategy, some criteria are provided to ensure the cluster synchronization of the nonlinearly coupled delayed NNs in both finite-and fixed-time aspects. Then, the settling time for stabilization that is dependent on the initial value and independent of the initial value is estimated, respectively. Finally, we illustrate the feasibility and practicality of the results via a numerical example.},
  archive      = {J_TNNLS},
  author       = {Xin Zhang and Wuneng Zhou and Hamid Reza Karimi and Yuqing Sun},
  doi          = {10.1109/TNNLS.2020.3027312},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {5222-5231},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Finite- and fixed-time cluster synchronization of nonlinearly coupled delayed neural networks via pinning control},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Event-triggered adaptive optimal control with output
feedback: An adaptive dynamic programming approach. <em>TNNLS</em>,
<em>32</em>(11), 5208–5221. (<a
href="https://doi.org/10.1109/TNNLS.2020.3027301">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents an event-triggered output-feedback adaptive optimal control method for continuous-time linear systems. First, it is shown that the unmeasurable states can be reconstructed by using the measured input and output data. An event-based feedback strategy is then proposed to reduce the number of controller updates and save communication resources. The discrete-time algebraic Riccati equation is iteratively solved through event-triggered adaptive dynamic programming based on both policy iteration (PI) and value iteration (VI) methods. The convergence of the proposed algorithm and the closed-loop stability is carried out by using the Lyapunov techniques. Two numerical examples are employed to verify the effectiveness of the design methodology.},
  archive      = {J_TNNLS},
  author       = {Fuyu Zhao and Weinan Gao and Zhong-Ping Jiang and Tengfei Liu},
  doi          = {10.1109/TNNLS.2020.3027301},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {5208-5221},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Event-triggered adaptive optimal control with output feedback: An adaptive dynamic programming approach},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Accuracy versus simplification in an approximate logic
neural model. <em>TNNLS</em>, <em>32</em>(11), 5194–5207. (<a
href="https://doi.org/10.1109/TNNLS.2020.3027298">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An approximate logic neural model (ALNM) is a novel single-neuron model with plastic dendritic morphology. During the training process, the model can eliminate unnecessary synapses and useless branches of dendrites. It will produce a specific dendritic structure for a particular task. The simplified structure of ALNM can be substituted by a logic circuit classifier (LCC) without losing any essential information. The LCC merely consists of the comparator and logic NOT, AND, and OR gates. Thus, it can be easily implemented in hardware. However, the architecture of ALNM affects the learning capacity, generalization capability, computing time and approximation of LCC. Thus, a Pareto-based multiobjective differential evolution (MODE) algorithm is proposed to simultaneously optimize ALNM’s topology and weights. MODE can generate a concise and accurate LCC for every specific task from ALNM. To verify the effectiveness of MODE, extensive experiments are performed on eight benchmark classification problems. The statistical results demonstrate that MODE is superior to conventional learning methods, such as the backpropagation algorithm and single-objective evolutionary algorithms. In addition, compared against several commonly used classifiers, both ALNM and LCC are capable of obtaining promising and competitive classification performances on the benchmark problems. Besides, the experimental results also verify that the LCC obtains the faster classification speed than the other classifiers.},
  archive      = {J_TNNLS},
  author       = {Junkai Ji and Yajiao Tang and Lijia Ma and Jianqiang Li and Qiuzhen Lin and Zheng Tang and Yuki Todo},
  doi          = {10.1109/TNNLS.2020.3027298},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {5194-5207},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Accuracy versus simplification in an approximate logic neural model},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Particle swarm optimization algorithm with self-organizing
mapping for nash equilibrium strategy in application of multiobjective
optimization. <em>TNNLS</em>, <em>32</em>(11), 5179–5193. (<a
href="https://doi.org/10.1109/TNNLS.2020.3027293">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, the Nash equilibrium strategy is used to solve the multiobjective optimization problems (MOPs) with the aid of an integrated algorithm combining the particle swarm optimization (PSO) algorithm and the self-organizing mapping (SOM) neural network. The Nash equilibrium strategy addresses the MOPs by comparing decision variables one by one under different objectives. The randomness of the PSO algorithm gives full play to the advantages of parallel computing and improves the rate of comparison calculation. In order to avoid falling into local optimal solutions and increase the diversity of particles, a nonlinear recursive function is introduced to adjust the inertia weight, which is called the adaptive particle swarm optimization (APSO). In addition, the neighborhood relations of current particles are constructed by SOM, and the leading particles are selected from the neighborhood to guide the local and global search, so as to achieve convergence. Compared with several advanced algorithms based on the eight multiobjective standard test functions with different Pareto solution sets and Pareto front characteristics in examples, the proposed algorithm has a better performance.},
  archive      = {J_TNNLS},
  author       = {Chenhui Zhao and Donghui Guo},
  doi          = {10.1109/TNNLS.2020.3027293},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {5179-5193},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Particle swarm optimization algorithm with self-organizing mapping for nash equilibrium strategy in application of multiobjective optimization},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A partial-node-based approach to state estimation for
complex networks with sensor saturations under random access protocol.
<em>TNNLS</em>, <em>32</em>(11), 5167–5178. (<a
href="https://doi.org/10.1109/TNNLS.2020.3027252">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, the robust finite-horizon state estimation problem is investigated for a class of time-varying complex networks (CNs) under the random access protocol (RAP) through available measurements from only a part of network nodes. The underlying CNs are subject to randomly occurring uncertainties, randomly occurring multiple delays, as well as sensor saturations. Several sequences of random variables are employed to characterize the random occurrences of parameter uncertainties and multiple delays. The RAP is adopted to orchestrate the data transmission at each time step based on a Markov chain. The aim of the addressed problem is to design a series of robust state estimators that make use of the available measurements from partial network nodes to estimate the network states, under the RAP and over a finite horizon, such that the estimation error dynamics achieves the prescribed $H_{\infty }$ performance requirement. Sufficient conditions are provided for the existence of such time-varying partial-node-based $H_{\infty }$ state estimators via stochastic analysis and matrix operations. The desired estimators are parameterized by solving certain recursive linear matrix inequalities. The effectiveness of the proposed state estimation algorithm is demonstrated via a simulation example.},
  archive      = {J_TNNLS},
  author       = {Nan Hou and Hongli Dong and Zidong Wang and Hongjian Liu},
  doi          = {10.1109/TNNLS.2020.3027252},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {5167-5178},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A partial-node-based approach to state estimation for complex networks with sensor saturations under random access protocol},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Decentralized filtering adaptive neural network control for
uncertain switched interconnected nonlinear systems. <em>TNNLS</em>,
<em>32</em>(11), 5156–5166. (<a
href="https://doi.org/10.1109/TNNLS.2020.3027232">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a novel decentralized filtering adaptive neural network control framework for uncertain switched interconnected nonlinear systems. Each subsystem has its own decentralized controller based on the established decentralized state predictor. For each subsystem, the nonlinear uncertainties are approximated by a Gaussian radial basis function (GRBF) neural network incorporated with a piecewise constant adaptive law, where the adaptive law will update adaptive parameters from the error dynamics between the host system and the decentralized state predictor by discarding the unknowns, whereas a decentralized filtering control law is derived to cancel both local and mismatched uncertainties from other subsystems, as well as achieve the local objective tracking of the host system. The achievement of global objective depends on the achievement of local objective for each subsystem. The matched uncertainties are canceled directly by adopting their opposite in the control signal, whereas a dynamic inversion of the system is required to eliminate the effect of the mismatched uncertainties on the output. By exploiting the average dwell time principle, the error bounds between the real system and the virtual reference system, which defines the best performance that can be achieved by the closed-loop system, are derived. A numerical example is given to illustrate the effectiveness of the decentralized filtering adaptive neural network control architecture by comparing against the model reference adaptive control (MRAC).},
  archive      = {J_TNNLS},
  author       = {Tong Ma},
  doi          = {10.1109/TNNLS.2020.3027232},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {5156-5166},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Decentralized filtering adaptive neural network control for uncertain switched interconnected nonlinear systems},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hybrid rubbing fault identification using a deep
learning-based observation technique. <em>TNNLS</em>, <em>32</em>(11),
5144–5155. (<a
href="https://doi.org/10.1109/TNNLS.2020.3027160">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A rub-impact fault is a complex, nonstationary, and nonlinear fault that occurs in turbines. Extracting features for diagnosing rubbing faults at their early stages requires complex and computationally expensive signal processing approaches that are not always suitable for industrial applications. In this article, a hybrid approach that uses a combination of deep learning and control theory algorithms is introduced for diagnosing rubbing faults of various intensities. Specifically, the system is first modeled based on the autoregressive with eXogenous input Laguerre (ARX-Laguerre) technique. In addition, the ARX-Laguerre proportional-integral observer (PIO) is used to increase the estimation accuracy for the vibration signals containing rubbing faults. Finally, a scalable deep neural network is applied to the output signal of the PIO to perform fault diagnosis and overcome potential problems that may appear when applying a linear observation technique to nonlinear signals. The experimental results demonstrate that the proposed hybrid approach improves the fault differentiation capabilities of a relatively simple linear observation technique when it is applied to a complex nonlinear rubbing fault signal and attains high fault classification accuracy. This result means that the proposed framework is highly suitable for applications in actual industrial environments.},
  archive      = {J_TNNLS},
  author       = {Alexander E. Prosvirin and Farzin Piltan and Jong-Myon Kim},
  doi          = {10.1109/TNNLS.2020.3027160},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {5144-5155},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Hybrid rubbing fault identification using a deep learning-based observation technique},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An improved nonparallel support vector machine.
<em>TNNLS</em>, <em>32</em>(11), 5129–5143. (<a
href="https://doi.org/10.1109/TNNLS.2020.3027062">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, an improved nonparallel support vector machine (INPSVM) is proposed for pattern classification. INPSVM inherits almost all advantages of nonparallel support vector machine (NPSVM), i.e., the kernel trick can be directly applied for the nonlinear case and the matrix inversion is avoided. These are completely different from the twin support vector machine (TSVM). Moreover, the INPSVM classifier has some incomparable advantages over TSVM and NPSVM. First, it can effectively eliminate the negative effect of noise, especially feature noise around the decision boundary. Second, the novel classifier has higher classification accuracy for both linear and nonlinear data sets compared with the other algorithms. Finally, a large number of experiments show that INPSVM is superior to other algorithms in efficiency, accuracy, and robustness.},
  archive      = {J_TNNLS},
  author       = {Liming Liu and Maoxiang Chu and Rongfen Gong and Li Zhang},
  doi          = {10.1109/TNNLS.2020.3027062},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {5129-5143},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {An improved nonparallel support vector machine},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Event-triggered nonlinear iterative learning control.
<em>TNNLS</em>, <em>32</em>(11), 5118–5128. (<a
href="https://doi.org/10.1109/TNNLS.2020.3027000">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An event-triggered nonlinear iterative learning control (ET-NILC) method is presented for repetitive nonaffine and nonlinear systems that have 2-D dynamic behavior along both time and iteration directions. Based on the virtual linear data model, the ET-NILC method is proposed by designing an event triggering condition based on the Lyapunov-like stability analysis conducted along the iteration direction. The learning gain function of ET-NILC is nonlinear and updated by designing an iterative learning parameter estimation law to enhance the robustness. From the perspective of the time dynamics, the proposed ET-NILC is a feedforward control and the event-triggering condition can be verified offline using tracking errors, event triggering errors, and the estimated parameters together. Moreover, the proposed ET-NILC is a data-driven scheme since it merely uses I/O data for the design. The results are also extended to repetitive multiple-input–multiple-output (MIMO) nonaffine nonlinear systems using the property of input-to-state stability as the basic mathematical tool. The convergence of the proposed ET-NILC methods is proved. Several simulations illustrate the effectiveness of the proposed methods.},
  archive      = {J_TNNLS},
  author       = {Na Lin and Ronghu Chi and Biao Huang and Zhongsheng Hou},
  doi          = {10.1109/TNNLS.2020.3027000},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {5118-5128},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Event-triggered nonlinear iterative learning control},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Stability analysis of continuous-time switched neural
networks with time-varying delay based on admissible edge-dependent
average dwell time. <em>TNNLS</em>, <em>32</em>(11), 5108–5117. (<a
href="https://doi.org/10.1109/TNNLS.2020.3026912">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates the stability of the switched neural networks (SNNs) with a time-varying delay. To effectively guarantee the stability of the considered system with unstable subsystems and reduce conservatism of the stability criteria, admissible edge-dependent average dwell time (AED-ADT) is first utilized to restrict switching signals for the continuous-time SNNs, and multiple Lyapunov–Kravosikii functionals (LKFs) combining relaxed integral inequalities are employed to develop two novel less-conservative stability conditions. Finally, the numeral examples clearly indicate that the proposed criteria can reduce conservatism and ensure the stability of continuous-time SNNs.},
  archive      = {J_TNNLS},
  author       = {Hui-Ting Wang and Yong He and Chuan-Ke Zhang},
  doi          = {10.1109/TNNLS.2020.3026912},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {5108-5117},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Stability analysis of continuous-time switched neural networks with time-varying delay based on admissible edge-dependent average dwell time},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Harmonization shared autoencoder gaussian process latent
variable model with relaxed hamming distance. <em>TNNLS</em>,
<em>32</em>(11), 5093–5107. (<a
href="https://doi.org/10.1109/TNNLS.2020.3026876">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiview learning has shown its superiority in visual classification compared with the single-view-based methods. Especially, due to the powerful representation capacity, the Gaussian process latent variable model (GPLVM)-based multiview approaches have achieved outstanding performances. However, most of them only follow the assumption that the shared latent variables can be generated from or projected to the multiple observations but fail to exploit the harmonization in the back constraint and adaptively learn a classifier according to these learned variables, which would result in performance degradation. To tackle these two issues, in this article, we propose a novel harmonization shared autoencoder GPLVM with a relaxed Hamming distance (HSAGP-RHD). Particularly, an autoencoder structure with the Gaussian process (GP) prior is first constructed to learn the shared latent variable for multiple views. To enforce the agreement among various views in the encoder, a harmonization constraint is embedded into the model by making consistency for the view-specific similarity. Furthermore, we also propose a novel discriminative prior, which is directly imposed on the latent variable to simultaneously learn the fused features and adaptive classifier in a unit model. In detail, the centroid matrix corresponding to the centroids of different categories is first obtained. A relaxed Hamming distance (RHD)-based measurement is subsequently presented to measure the similarity and dissimilarity between the latent variable and centroids, not only allowing us to get the closed-form solutions but also encouraging the points belonging to the same class to be close, while those belonging to different classes to be far. Due to this novel prior, the category of the out-of-sample is also allowed to be simply assigned in the testing phase. Experimental results conducted on three real-world data sets demonstrate the effectiveness of the proposed method compared with state-of-the-art approaches.},
  archive      = {J_TNNLS},
  author       = {Jinxing Li and Bob Zhang and Guangming Lu and Yong Xu and Feng Wu and David Zhang},
  doi          = {10.1109/TNNLS.2020.3026876},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {5093-5107},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Harmonization shared autoencoder gaussian process latent variable model with relaxed hamming distance},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Revisiting internal covariate shift for batch normalization.
<em>TNNLS</em>, <em>32</em>(11), 5082–5092. (<a
href="https://doi.org/10.1109/TNNLS.2020.3026784">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the success of batch normalization (BatchNorm) and a plethora of its variants, the exact reasons for its success are still shady. The original BatchNorm article explained it as a mechanism that reduces the internal covariate shift (ICS), i.e., the distribution shifts in the input of the layers during training. Recently, some articles manifested skepticism on this hypothesis and provided alternative explanations for the success of BatchNorm, such as the applicability of very high learning rates and the ability to smooth the landscape in optimization. In this work, we counter these alternative arguments by demonstrating the importance of reduction in ICS following an empirical approach. We demonstrated various ways to achieve the abovementioned alternative properties without any performance boost. In this light, we explored the importance of different BatchNorm parameters (i.e., batch statistics and affine transformation parameters) by visualizing their effectiveness in the performance and analyzed their connections with ICS. Afterward, we showed a different normalization scheme that fulfills all the alternative explanations except reduction in ICS. Despite having all the alternative properties, we observed its poor performance, which nullifies the alternative claims, rather signifies the importance of the ICS reduction. We performed comprehensive experiments on many variants of BatchNorm, finding that all of them similarly reduce ICS.},
  archive      = {J_TNNLS},
  author       = {Muhammad Awais and Md. Tauhid Bin Iqbal and Sung-Ho Bae},
  doi          = {10.1109/TNNLS.2020.3026784},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {5082-5092},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Revisiting internal covariate shift for batch normalization},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Translation-invariant kernels for multivariable
approximation. <em>TNNLS</em>, <em>32</em>(11), 5072–5081. (<a
href="https://doi.org/10.1109/TNNLS.2020.3026720">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Suitability of shallow (one-hidden-layer) networks with translation-invariant kernel units for function approximation and classification tasks is investigated. It is shown that a critical property influencing the capabilities of kernel networks is how the Fourier transforms of kernels converge to zero. The Fourier transforms of kernels suitable for multivariable approximation can have negative values but must be almost everywhere nonzero. In contrast, the Fourier transforms of kernels suitable for maximal margin classification must be everywhere nonnegative but can have large sets where they are equal to zero (e.g., they can be compactly supported). The behavior of the Fourier transforms of multivariable kernels is analyzed using the Hankel transform. The general results are illustrated by examples of both univariable and multivariable kernels (such as Gaussian, Laplace, rectangle, sinc, and cut power kernels).},
  archive      = {J_TNNLS},
  author       = {Věra Kůrková and David Coufal},
  doi          = {10.1109/TNNLS.2020.3026720},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {5072-5081},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Translation-invariant kernels for multivariable approximation},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Exponential h∞ state estimation for memristive neural
networks: Vector optimization approach. <em>TNNLS</em>, <em>32</em>(11),
5061–5071. (<a
href="https://doi.org/10.1109/TNNLS.2020.3026707">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents the theoretical results on the $H_\infty $ state estimation problem for a class of discrete-time memristive neural networks. By utilizing a Lyapunov–Krasovskii functional, sufficient conditions are derived to guarantee that the error system is exponentially mean-square stable; subsequently, the prespecified $H_\infty $ disturbance rejection attenuation level is also guaranteed. It should be noted that the vector optimization method is employed to find the maximum bound of function and the minimum disturbance turning simultaneously. Finally, the corresponding simulation results are included to show the effectiveness of the proposed methodology.},
  archive      = {J_TNNLS},
  author       = {Ruoxia Li and Jinde Cao},
  doi          = {10.1109/TNNLS.2020.3026707},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {5061-5071},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Exponential h∞ state estimation for memristive neural networks: Vector optimization approach},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Smoothness regularized multiview subspace clustering with
kernel learning. <em>TNNLS</em>, <em>32</em>(11), 5047–5060. (<a
href="https://doi.org/10.1109/TNNLS.2020.3026686">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiview subspace clustering has attracted an increasing amount of attention in recent years. However, most of the existing multiview subspace clustering methods assume linear relations between multiview data points when learning the affinity representation by means of the self-expression or fail to preserve the locality property of the original feature space in the learned affinity representation. To address the above issues, in this article, we propose a new multiview subspace clustering method termed smoothness regularized multiview subspace clustering with kernel learning (SMSCK). To capture the nonlinear relations between multiview data points, the proposed model maps the concatenated multiview observations into a high-dimensional kernel space, in which the linear relations reflect the nonlinear relations between multiview data points in the original space. In addition, to explicitly preserve the locality property of the original feature space in the learned affinity representation, the smoothness regularization is deployed in the subspace learning in the kernel space. Theoretical analysis has been provided to ensure that the optimal solution of the proposed model meets the grouping effect. The unique optimal solution of the proposed model can be obtained by an optimization strategy and the theoretical convergence analysis is also conducted. Extensive experiments are conducted on both image and document data sets, and the comparison results with state-of-the-art methods demonstrate the effectiveness of our method.},
  archive      = {J_TNNLS},
  author       = {Chang-Dong Wang and Man-Sheng Chen and Ling Huang and Jian-Huang Lai and Philip S. Yu},
  doi          = {10.1109/TNNLS.2020.3026686},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {5047-5060},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Smoothness regularized multiview subspace clustering with kernel learning},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Laplacian pyramid neural network for dense continuous-value
regression for complex scenes. <em>TNNLS</em>, <em>32</em>(11),
5034–5046. (<a
href="https://doi.org/10.1109/TNNLS.2020.3026669">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many computer vision tasks, such as monocular depth estimation and height estimation from a satellite orthophoto, have a common underlying goal, which is regression of dense continuous values for the pixels given a single image. We define them as dense continuous-value regression (DCR) tasks. Recent approaches based on deep convolutional neural networks significantly improve the performance of DCR tasks, particularly on pixelwise regression accuracy. However, it still remains challenging to simultaneously preserve the global structure and fine object details in complex scenes. In this article, we take advantage of the efficiency of Laplacian pyramid on representing multiscale contents to reconstruct high-quality signals for complex scenes. We design a Laplacian pyramid neural network (LAPNet), which consists of a Laplacian pyramid decoder (LPD) for signal reconstruction and an adaptive dense feature fusion (ADFF) module to fuse features from the input image. More specifically, we build an LPD to effectively express both global and local scene structures. In our LPD, the upper and lower levels, respectively, represent scene layouts and shape details. We introduce a residual refinement module to progressively complement high-frequency details for signal prediction at each level. To recover the signals at each individual level in the pyramid, an ADFF module is proposed to adaptively fuse multiscale image features for accurate prediction. We conduct comprehensive experiments to evaluate a number of variants of our model on three important DCR tasks, i.e., monocular depth estimation, single-image height estimation, and density map estimation for crowd counting. Experiments demonstrate that our method achieves new state-of-the-art performance in both qualitative and quantitative evaluation on the NYU-D V2 and KITTI for monocular depth estimation, the challenging Urban Semantic 3D (US3D) for satellite height estimation, and four challenging benchmarks for crowd counting. These results demonstrate that the proposed LAPNet is a universal and effective architecture for DCR problems.},
  archive      = {J_TNNLS},
  author       = {Xuejin Chen and Xiaotian Chen and Yiteng Zhang and Xueyang Fu and Zheng-Jun Zha},
  doi          = {10.1109/TNNLS.2020.3026669},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {5034-5046},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Laplacian pyramid neural network for dense continuous-value regression for complex scenes},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A shape-constrained neural data fusion network for health
index construction and residual life prediction. <em>TNNLS</em>,
<em>32</em>(11), 5022–5033. (<a
href="https://doi.org/10.1109/TNNLS.2020.3026644">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of sensor technologies, multisensor signals are now readily available for health condition monitoring and remaining useful life (RUL) prediction. To fully utilize these signals for a better health condition assessment and RUL prediction, health indices are often constructed through various data fusion techniques. Nevertheless, most of the existing methods fuse signals linearly, which may not be sufficient to characterize the health status for RUL prediction. To address this issue and improve the predictability, this article proposes a novel nonlinear data fusion approach, namely, a shape-constrained neural data fusion network for health index construction. Especially, a neural network-based structure is employed, and a novel loss function is formulated by simultaneously considering the monotonicity and curvature of the constructed health index and its variability at the failure time. A tailored adaptive moment estimation algorithm (Adam) is proposed for model parameter estimation. The effectiveness of the proposed method is demonstrated and compared through a case study using the Commercial Modular Aero-Propulsion System Simulation (C-MAPSS) data set.},
  archive      = {J_TNNLS},
  author       = {Zhen Li and Jianguo Wu and Xiaowei Yue},
  doi          = {10.1109/TNNLS.2020.3026644},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {5022-5033},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A shape-constrained neural data fusion network for health index construction and residual life prediction},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multimodel feature reinforcement framework using
moore–penrose inverse for big data analysis. <em>TNNLS</em>,
<em>32</em>(11), 5008–5021. (<a
href="https://doi.org/10.1109/TNNLS.2020.3026621">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fully connected representation learning (FCRL) is one of the widely used network structures in multimodel image classification frameworks. However, most FCRL-based structures, for instance, stacked autoencoder encode features and find the final cognition with separate building blocks, resulting in loosely connected feature representation. This article achieves a robust representation by considering a low-dimensional feature and the classifier model simultaneously. Thus, a new hierarchical subnetwork-based neural network (HSNN) is proposed in this article. The novelties of this framework are as follows: 1) it is an iterative learning process, instead of stacking separate blocks to obtain the discriminative encoding and the final classification results. In this sense, the optimal global features are generated; 2) it applies Moore–Penrose (MP) inverse-based batch-by-batch learning strategy to handle large-scale data sets, so that large data set, such as Place365 containing 1.8 million images, can be processed effectively. The experimental results on multiple domains with a varying number of training samples from ${\sim } 1 \,\,K$ to ${\sim }2~M$ show that the proposed feature reinforcement framework achieves better generalization performance compared with most state-of-the-art FCRL methods.},
  archive      = {J_TNNLS},
  author       = {Wandong Zhang and Q. M. Jonathan Wu and Yimin Yang and Thangarajah Akilan},
  doi          = {10.1109/TNNLS.2020.3026621},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {5008-5021},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multimodel feature reinforcement framework using Moore–Penrose inverse for big data analysis},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Masked autoencoder for distribution estimation on small
structured data sets. <em>TNNLS</em>, <em>32</em>(11), 4997–5007. (<a
href="https://doi.org/10.1109/TNNLS.2020.3026572">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autoregressive models are among the most successful neural network methods for estimating a distribution from a set of samples. However, these models, such as other neural methods, need large data sets to provide good estimations. We believe that knowing structural information about the data can improve their performance on small data sets. Masked autoencoder for distribution estimation (MADE) is a well-structured density estimator, which alters a simple autoencoder by setting a set of masks on its connections to satisfy the autoregressive condition. Nevertheless, this model does not benefit from extra information that we might know about the structure of the data. This information can especially be advantageous in case of training on small data sets. In this article, we propose two autoencoders for estimating the density of a small set of observations, where the data have a known Markov random field (MRF) structure. These methods modify the masking process of MADE, according to conditional dependencies inferred from the MRF structure, to reduce either the model complexity or the problem complexity. We compare the proposed methods with some related binary, discrete, and continuous density estimators on MNIST, binarized MNIST, OCR-letters, and two synthetic data sets.},
  archive      = {J_TNNLS},
  author       = {Ahmad Khajenezhad and Hatef Madani and Hamid Beigy},
  doi          = {10.1109/TNNLS.2020.3026572},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {4997-5007},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Masked autoencoder for distribution estimation on small structured data sets},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multiple kernel k-means clustering by selecting
representative kernels. <em>TNNLS</em>, <em>32</em>(11), 4983–4996. (<a
href="https://doi.org/10.1109/TNNLS.2020.3026532">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To cluster data that are not linearly separable in the original feature space, $k$ -means clustering was extended to the kernel version. However, the performance of kernel $k$ -means clustering largely depends on the choice of the kernel function. To mitigate this problem, multiple kernel learning has been introduced into the $k$ -means clustering to obtain an optimal kernel combination for clustering. Despite the success of multiple kernel $k$ -means clustering in various scenarios, few of the existing work update the combination coefficients based on the diversity of kernels, which leads to the result that the selected kernels contain high redundancy and would degrade the clustering performance and efficiency. We resolve this problem from the perspective of subset selection in this article. In particular, we first propose an effective strategy to select a diverse subset from the prespecified kernels as the representative kernels, and then incorporate the subset selection process into the framework of multiple $k$ -means clustering. The representative kernels can be indicated as a significant combination weights. Due to the nonconvexity of the obtained objective function, we develop an alternating minimization method to optimize the combination coefficients of the selected kernels and the cluster membership alternatively. In particular, an efficient optimization method is developed to reduce the time complexity of optimizing the kernel combination weights. Finally, extensive experiments on benchmark and real-world data sets demonstrate the effectiveness and superiority of our approach in comparison with existing methods.},
  archive      = {J_TNNLS},
  author       = {Yaqiang Yao and Yang Li and Bingbing Jiang and Huanhuan Chen},
  doi          = {10.1109/TNNLS.2020.3026532},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {4983-4996},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multiple kernel k-means clustering by selecting representative kernels},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Recursive variable projection algorithm for a class of
separable nonlinear models. <em>TNNLS</em>, <em>32</em>(11), 4971–4982.
(<a href="https://doi.org/10.1109/TNNLS.2020.3026482">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we study the recursive algorithms for a class of separable nonlinear models (SNLMs) in which the parameters can be partitioned into a linear part and a nonlinear part. Such models are very common in machine learning, system identification, and signal processing. Utilizing the special structure of the SNLMs, we propose a recursive variable projection (RVP) algorithm, in which at each recursion, the linear parameters of the model are eliminated, and the nonlinear parameters are updated by the recursive Levenberg–Marquart algorithm. Then, based on the updated nonlinear parameters, the linear parameters are updated by the recursive least-squares algorithm. According to a convergence analysis of the RVP algorithm, the parameter estimation error is mean-square bounded. Numerical examples confirm the satisfactory performance of the proposed algorithm.},
  archive      = {J_TNNLS},
  author       = {Min Gan and Yu Guan and Guang-Yong Chen and C. L. Philip Chen},
  doi          = {10.1109/TNNLS.2020.3026482},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {4971-4982},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Recursive variable projection algorithm for a class of separable nonlinear models},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Manifold learning based on straight-like geodesics and local
coordinates. <em>TNNLS</em>, <em>32</em>(11), 4956–4970. (<a
href="https://doi.org/10.1109/TNNLS.2020.3026426">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, a manifold learning algorithm based on straight-like geodesics and local coordinates is proposed, called SGLC-ML for short. The contribution and innovation of SGLC-ML lie in that; first, SGLC-ML divides the manifold data into a number of straight-like geodesics, instead of a number of local areas like many manifold learning algorithms do. Figuratively speaking, SGLC-ML covers manifold data set with a sparse net woven with threads (straight-like geodesics), while other manifold learning algorithms with a tight roof made of titles (local areas). Second, SGLC-ML maps all straight-like geodesics into straight lines of a low-dimensional Euclidean space. All these straight lines start from the same point and extend along the same coordinate axis. These straight lines are exactly the local coordinates of straight-like geodesics as described in the mathematical definition of the manifold. With the help of local coordinates, dimensionality reduction can be divided into two relatively simple processes: calculation and alignment of local coordinates. However, many manifold learning algorithms seem to ignore the advantages of local coordinates. The experimental results between SGLC-ML and other state-of-the-art algorithms are presented to verify the good performance of SGLC-ML.},
  archive      = {J_TNNLS},
  author       = {Zhengming Ma and Zengrong Zhan and Zijian Feng and Jiajing Guo},
  doi          = {10.1109/TNNLS.2020.3026426},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {4956-4970},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Manifold learning based on straight-like geodesics and local coordinates},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Cumulative permuted fractional entropy and its applications.
<em>TNNLS</em>, <em>32</em>(11), 4946–4955. (<a
href="https://doi.org/10.1109/TNNLS.2020.3026424">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fractional calculus and entropy are two essential mathematical tools, and their conceptions support a productive interplay in the study of system dynamics and machine learning. In this article, we modify the fractional entropy and propose the cumulative permuted fractional entropy (CPFE). A theoretical analysis is provided to prove that CPFE not only meets the basic properties of the Shannon entropy but also has unique characteristics of its own. We apply it to typical discrete distributions, simulated data, and real-world data to prove its efficiency in the application. This article demonstrates that CPFE can measure the complexity and uncertainty of complex systems so that it can perform reliable and accurate classification. Finally, we introduce CPFE to support vector machines (SVMs) and get CPFE-SVM. The CPFE can be used to process data to make the irregular data linearly separable. Compared with the other five state-of-the-art algorithms, CPFE-SVM has significantly higher accuracy and less computational burden. Therefore, the CPFE-SVM is especially suitable for the classification of irregular large-scale data sets. Also, it is insensitive to noise. Implications of the results and future research directions are also presented.},
  archive      = {J_TNNLS},
  author       = {Boyi Zhang and Pengjian Shang},
  doi          = {10.1109/TNNLS.2020.3026424},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {4946-4955},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Cumulative permuted fractional entropy and its applications},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Prescribed performance adaptive neural compensation control
for intermittent actuator faults by state and output feedback.
<em>TNNLS</em>, <em>32</em>(11), 4931–4945. (<a
href="https://doi.org/10.1109/TNNLS.2020.3026208">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the existing effects of intermittent jumps of unknown parameters during operation, effectively establishing transient and steady-state tracking performances in control systems with unknown intermittent actuator faults is very important. In this article, two prescribed performance adaptive neural control schemes based on command-filtered backstepping are developed for a class of uncertain strict-feedback nonlinear systems. Under the condition of system states being available for feedback, the state feedback control scheme is investigated. When the system states are not directly measured, a cascade high-gain observer is designed to reconstruct the system states, and in turn, the output feedback control scheme is presented. Since the projection operator and modified Lyapunov function are, respectively, used in the adaptive law design and stability analysis, it is proven that both schemes can not only ensure the boundedness of all closed-loop signals but also confine the tracking errors within prescribed arbitrarily small residual sets for all the time even if there exist the effects of intermittent jumps of unknown parameters. Thus, the prescribed system transient and steady-state performances in the sense of the tracking errors are established. Furthermore, we also prove that the tracking performance under output feedback is able to recover the tracking performance under state feedback as the observer gain decreases. Simulation studies are done to verify the effectiveness of the theoretical discussions.},
  archive      = {J_TNNLS},
  author       = {Yongqiang Nai and Qingyu Yang and Zongze Wu},
  doi          = {10.1109/TNNLS.2020.3026208},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {4931-4945},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Prescribed performance adaptive neural compensation control for intermittent actuator faults by state and output feedback},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Synchronization of inertial neural networks with
time-varying delays via quantized sampled-data control. <em>TNNLS</em>,
<em>32</em>(11), 4916–4930. (<a
href="https://doi.org/10.1109/TNNLS.2020.3026163">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article addresses the quantized sampled-data (QSD) synchronization for inertial neural networks (INNs) with heterogeneous time-varying delays, in which the sampled-data control and state quantization effect have been considered. By utilizing a proper variable substitution to transform the original system into a first-order differential system, choosing a new Lyapunov–Krasovskii functional (LKF) containing both the continuous terms and the discontinuous terms, and applying Jensen inequality and an improved reciprocally convex inequality to estimate the derivative of the LKF, the sufficient conditions for QSD synchronization for INNs are newly obtained in terms of linear matrix inequalities (LMIs), and the desired QSD controllers are designed by solving a set of LMIs. Finally, three numerical examples are provided to validate the effectiveness and benefit of the proposed results.},
  archive      = {J_TNNLS},
  author       = {Xinyu Zhong and Yanbo Gao},
  doi          = {10.1109/TNNLS.2020.3026163},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {4916-4930},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Synchronization of inertial neural networks with time-varying delays via quantized sampled-data control},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A bilevel learning model and algorithm for self-organizing
feed-forward neural networks for pattern classification. <em>TNNLS</em>,
<em>32</em>(11), 4901–4915. (<a
href="https://doi.org/10.1109/TNNLS.2020.3026114">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conventional artificial neural network (ANN) learning algorithms for classification tasks, either derivative-based optimization algorithms or derivative-free optimization algorithms work by training ANN first (or training and validating ANN) and then testing ANN, which are a two-stage and one-pass learning mechanism. Thus, this learning mechanism may not guarantee the generalization ability of a trained ANN. In this article, a novel bilevel learning model is constructed for self-organizing feed-forward neural network (FFNN), in which the training and testing processes are integrated into a unified framework. In this bilevel model, the upper level optimization problem is built for testing error on testing data set and network architecture based on network complexity, whereas the lower level optimization problem is constructed for network weights based on training error on training data set. For the bilevel framework, an interactive learning algorithm is proposed to optimize the architecture and weights of an FFNN with consideration of both training error and testing error. In this interactive learning algorithm, a hybrid binary particle swarm optimization (BPSO) taken as an upper level optimizer is used to self-organize network architecture, whereas the Levenberg–Marquardt (LM) algorithm as a lower level optimizer is utilized to optimize the connection weights of an FFNN. The bilevel learning model and algorithm have been tested on 20 benchmark classification problems. Experimental results demonstrate that the bilevel learning algorithm can significantly produce more compact FFNNs with more excellent generalization ability when compared with conventional learning algorithms.},
  archive      = {J_TNNLS},
  author       = {Hong Li and Li Zhang},
  doi          = {10.1109/TNNLS.2020.3026114},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {4901-4915},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A bilevel learning model and algorithm for self-organizing feed-forward neural networks for pattern classification},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Low-cost approximation-based adaptive tracking control of
output-constrained nonlinear systems. <em>TNNLS</em>, <em>32</em>(11),
4890–4900. (<a
href="https://doi.org/10.1109/TNNLS.2020.3026078">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For pure-feedback nonlinear systems under asymmetric output constraint, we present a low-cost neuroadaptive tracking control solution with salient features benefited from two design steps. In the first step, a novel output-dependent universal barrier function (ODUBF) is constructed such that not only the restrictive condition on constraining boundaries/functions is removed but also both constrained and unconstrained cases can be handled uniformly without the need for changing the control structure. In the second step, to reduce the computational burden caused by the neural network (NN)-based approximators, a single parameter estimator is developed so that the number of adaptive law is independent of the system order and the dimension of system parameters, making the control design inexpensive in computation. Furthermore, it is shown that all signals in the closed-loop system are semiglobally uniformly ultimately bounded, the tracking error converges to an adjustable neighborhood of the origin, and the violation of output constraint is prevented. The effectiveness of the proposed method can be validated via numerical simulation.},
  archive      = {J_TNNLS},
  author       = {Kai Zhao and Yongduan Song and Wenchao Meng and C. L. Philip Chen and Long Chen},
  doi          = {10.1109/TNNLS.2020.3026078},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {4890-4900},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Low-cost approximation-based adaptive tracking control of output-constrained nonlinear systems},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Discrete-time h2 neural control using reinforcement
learning. <em>TNNLS</em>, <em>32</em>(11), 4879–4889. (<a
href="https://doi.org/10.1109/TNNLS.2020.3026010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we discuss $\mathcal {H}_{2}$ control for unknown nonlinear systems in discrete time. A discrete-time recurrent neural network is used to model the nonlinear system, and then, the $\mathcal {H}_{2}$ tracking control is applied based on the neural model. Since this neural $\mathcal {H}_{2}$ control is very sensitive to the neural modeling error, we use reinforcement learning and another neural approximator to improve tracking accuracy and robustness of the controller. The stabilities of the neural identifier and the $\mathcal {H}_{2}$ tracking control are proven. The convergence of the approach is also given. The proposed method is validated with the control of the pan and tilt robot and the surge tank.},
  archive      = {J_TNNLS},
  author       = {Adolfo Perrusquía and Wen Yu},
  doi          = {10.1109/TNNLS.2020.3026010},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {4879-4889},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Discrete-time h2 neural control using reinforcement learning},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Incremental unsupervised domain-adversarial training of
neural networks. <em>TNNLS</em>, <em>32</em>(11), 4864–4878. (<a
href="https://doi.org/10.1109/TNNLS.2020.3025954">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the context of supervised statistical learning, it is typically assumed that the training set comes from the same distribution that draws the test samples. When this is not the case, the behavior of the learned model is unpredictable and becomes dependent upon the degree of similarity between the distribution of the training set and the distribution of the test set. One of the research topics that investigates this scenario is referred to as domain adaptation (DA). Deep neural networks brought dramatic advances in pattern recognition and that is why there have been many attempts to provide good DA algorithms for these models. Herein we take a different avenue and approach the problem from an incremental point of view, where the model is adapted to the new domain iteratively. We make use of an existing unsupervised domain-adaptation algorithm to identify the target samples on which there is greater confidence about their true label. The output of the model is analyzed in different ways to determine the candidate samples. The selected samples are then added to the source training set by self-labeling, and the process is repeated until all target samples are labeled. This approach implements a form of adversarial training in which, by moving the self-labeled samples from the target to the source set, the DA algorithm is forced to look for new features after each iteration. Our results report a clear improvement with respect to the non-incremental case in several data sets, also outperforming other state-of-the-art DA algorithms.},
  archive      = {J_TNNLS},
  author       = {Antonio-Javier Gallego and Jorge Calvo-Zaragoza and Robert B. Fisher},
  doi          = {10.1109/TNNLS.2020.3025954},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {4864-4878},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Incremental unsupervised domain-adversarial training of neural networks},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An efficient parameter-free learning automaton scheme.
<em>TNNLS</em>, <em>32</em>(11), 4849–4863. (<a
href="https://doi.org/10.1109/TNNLS.2020.3025937">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The learning automaton (LA) that simulates the interaction between an intelligent agent and a stochastic environment to learn the optimal action is an important tool in reinforcement learning. Being confronted with an unknown environment, most learning automata have more than one parameters to be tuned during a pretraining process in which the LA interacts with the environment. Only after the parameters are tuned properly, an LA can act most properly during the training procedure to obtain the optimal behavior. The cost of parameter tuning can be enormous, e.g., possibly millions of interactions are required to seek the best parameter configuration. Therefore, the parameter-free LA that uses identical parameters for every environment and saves further tuning has become the hot spot of this research. This article proposes an efficient parameter-free learning automaton (EPFLA) that depends on a separating function (SF). Taking advantage of both frequentist inference and Bayesian inference, the SF plays a dual role in the proposed scheme: 1) evaluating the difference in performance between actions in the environment and 2) exploring actions by coining an action selection strategy. A proof is provided to ensure the $\epsilon $ -optimality of EPFLA. Comprehensive comparisons verify the privileges of EPFLA over both parameter-based schemes and existing parameter-free schemes.},
  archive      = {J_TNNLS},
  author       = {Chong Di and Qilian Liang and Fangqi Li and Shenghong Li and Fucai Luo},
  doi          = {10.1109/TNNLS.2020.3025937},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {4849-4863},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {An efficient parameter-free learning automaton scheme},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Graph-based function perturbation analysis for observability
of multivalued logical networkss. <em>TNNLS</em>, <em>32</em>(11),
4839–4848. (<a
href="https://doi.org/10.1109/TNNLS.2020.3025912">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Observability is a fundamental concept for the synthesis of both linear systems and nonlinear systems. This article devotes to discussing the robustness of observability for multivalued logical networks (MVLNs) subject to function perturbation and establishing a graph-based framework. First, based on the transition graph of undistinguishable pairs of states, a new graph-based criterion is presented for the observability of MVLNs. Second, a candidate set consisting of all suspicious undistinguishable pairs of states is defined, based on the cardinality of which and the graph-based condition, a series of effective criteria are proposed for the robustness of observability subject to function perturbations. Finally, the obtained results are applied to the robust observability analysis of the p53-MDM2 negative feedback regulatory loop.},
  archive      = {J_TNNLS},
  author       = {Shuling Wang and Haitao Li},
  doi          = {10.1109/TNNLS.2020.3025912},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {4839-4848},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Graph-based function perturbation analysis for observability of multivalued logical networkss},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Extending the morphological hit-or-miss transform to deep
neural networks. <em>TNNLS</em>, <em>32</em>(11), 4826–4838. (<a
href="https://doi.org/10.1109/TNNLS.2020.3025723">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While most deep learning architectures are built on convolution, alternative foundations such as morphology are being explored for purposes such as interpretability and its connection to the analysis and processing of geometric structures. The morphological hit-or-miss operation has the advantage that it considers both foreground information and background information when evaluating the target shape in an image. In this article, we identify limitations in the existing hit-or-miss neural definitions and formulate an optimization problem to learn the transform relative to deeper architectures. To this end, we model the semantically important condition that the intersection of the hit and miss structuring elements (SEs) should be empty and present a way to express Don’t Care (DNC), which is important for denoting regions of an SE that are not relevant to detecting a target pattern. Our analysis shows that convolution, in fact, acts like a hit-to-miss transform through semantic interpretation of its filter differences. On these premises, we introduce an extension that outperforms conventional convolution on benchmark data. Quantitative experiments are provided on synthetic and benchmark data, showing that the direct encoding hit-or-miss transform provides better interpretability on learned shapes consistent with objects, whereas our morphologically inspired generalized convolution yields higher classification accuracy. Finally, qualitative hit and miss filter visualizations are provided relative to single morphological layer.},
  archive      = {J_TNNLS},
  author       = {Muhammad Aminul Islam and Bryce Murray and Andrew Buck and Derek T. Anderson and Grant J. Scott and Mihail Popescu and James Keller},
  doi          = {10.1109/TNNLS.2020.3025723},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {4826-4838},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Extending the morphological hit-or-miss transform to deep neural networks},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Internal feature selection method of CSP based on l1-norm
and dempster–shafer theory. <em>TNNLS</em>, <em>32</em>(11), 4814–4825.
(<a href="https://doi.org/10.1109/TNNLS.2020.3015505">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The common spatial pattern (CSP) algorithm is a well-recognized spatial filtering method for feature extraction in motor imagery (MI)-based brain–computer interfaces (BCIs). However, due to the influence of nonstationary in electroencephalography (EEG) and inherent defects of the CSP objective function, the spatial filters, and their corresponding features are not necessarily optimal in the feature space used within CSP. In this work, we design a new feature selection method to address this issue by selecting features based on an improved objective function. Especially, improvements are made in suppressing outliers and discovering features with larger interclass distances. Moreover, a fusion algorithm based on the Dempster–Shafer theory is proposed, which takes into consideration the distribution of features. With two competition data sets, we first evaluate the performance of the improved objective functions in terms of classification accuracy, feature distribution, and embeddability. Then, a comparison with other feature selection methods is carried out in both accuracy and computational time. Experimental results show that the proposed methods consume less additional computational cost and result in a significant increase in the performance of MI-based BCI systems.},
  archive      = {J_TNNLS},
  author       = {Jing Jin and Ruocheng Xiao and Ian Daly and Yangyang Miao and Xingyu Wang and Andrzej Cichocki},
  doi          = {10.1109/TNNLS.2020.3015505},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {4814-4825},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Internal feature selection method of CSP based on l1-norm and Dempster–Shafer theory},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A survey on explainable artificial intelligence (XAI):
Toward medical XAI. <em>TNNLS</em>, <em>32</em>(11), 4793–4813. (<a
href="https://doi.org/10.1109/TNNLS.2020.3027314">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, artificial intelligence and machine learning in general have demonstrated remarkable performances in many tasks, from image processing to natural language processing, especially with the advent of deep learning (DL). Along with research progress, they have encroached upon many different fields and disciplines. Some of them require high level of accountability and thus transparency, for example, the medical sector. Explanations for machine decisions and predictions are thus needed to justify their reliability. This requires greater interpretability, which often means we need to understand the mechanism underlying the algorithms. Unfortunately, the blackbox nature of the DL is still unresolved, and many machine decisions are still poorly understood. We provide a review on interpretabilities suggested by different research works and categorize them. The different categories show different dimensions in interpretability research, from approaches that provide “obviously” interpretable information to the studies of complex patterns. By applying the same categorization to interpretability in medical research, it is hoped that: 1) clinicians and practitioners can subsequently approach these methods with caution; 2) insight into interpretability will be born with more considerations for medical practices; and 3) initiatives to push forward data-based, mathematically grounded, and technically grounded medical education are encouraged.},
  archive      = {J_TNNLS},
  author       = {Erico Tjoa and Cuntai Guan},
  doi          = {10.1109/TNNLS.2020.3027314},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {4793-4813},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A survey on explainable artificial intelligence (XAI): Toward medical XAI},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). AANet: Adaptive attention network for COVID-19 detection
from chest x-ray images. <em>TNNLS</em>, <em>32</em>(11), 4781–4792. (<a
href="https://doi.org/10.1109/TNNLS.2021.3114747">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate and rapid diagnosis of COVID-19 using chest X-ray (CXR) plays an important role in large-scale screening and epidemic prevention. Unfortunately, identifying COVID-19 from the CXR images is challenging as its radiographic features have a variety of complex appearances, such as widespread ground-glass opacities and diffuse reticular-nodular opacities. To solve this problem, we propose an adaptive attention network (AANet), which can adaptively extract the characteristic radiographic findings of COVID-19 from the infected regions with various scales and appearances. It contains two main components: an adaptive deformable ResNet and an attention-based encoder. First, the adaptive deformable ResNet, which adaptively adjusts the receptive fields to learn feature representations according to the shape and scale of infected regions, is designed to handle the diversity of COVID-19 radiographic features. Then, the attention-based encoder is developed to model nonlocal interactions by self-attention mechanism, which learns rich context information to detect the lesion regions with complex shapes. Extensive experiments on several public datasets show that the proposed AANet outperforms state-of-the-art methods.},
  archive      = {J_TNNLS},
  author       = {Zhijie Lin and Zhaoshui He and Shengli Xie and Xu Wang and Ji Tan and Jun Lu and Beihai Tan},
  doi          = {10.1109/TNNLS.2021.3114747},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {4781-4792},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {AANet: Adaptive attention network for COVID-19 detection from chest X-ray images},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). New insights into drug repurposing for COVID-19 using deep
learning. <em>TNNLS</em>, <em>32</em>(11), 4770–4780. (<a
href="https://doi.org/10.1109/TNNLS.2021.3111745">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The coronavirus disease 2019 (COVID-19) has continued to spread worldwide since late 2019. To expedite the process of providing treatment to those who have contracted the disease and to ensure the accessibility of effective drugs, numerous strategies have been implemented to find potential anti-COVID-19 drugs in a short span of time. Motivated by this critical global challenge, in this review, we detail approaches that have been used for drug repurposing for COVID-19 and suggest improvements to the existing deep learning (DL) approach to identify and repurpose drugs to treat this complex disease. By optimizing hyperparameter settings, deploying suitable activation functions, and designing optimization algorithms, the improved DL approach will be able to perform feature extraction from quality big data, turning the traditional DL approach, referred to as a “black box,” which generalizes and learns the transmitted data, into a “glass box” that will have the interpretability of its rationale while maintaining a high level of prediction accuracy. When adopted for drug repurposing for COVID-19, this improved approach will create a new generation of DL approaches that can establish a cause and effect relationship as to why the repurposed drugs are suitable for treating COVID-19. Its ability can also be extended to repurpose drugs for other complex diseases, develop appropriate treatment strategies for new diseases, and provide precision medical treatment to patients, thus paving the way to discover new drugs that can potentially be effective for treating COVID-19.},
  archive      = {J_TNNLS},
  author       = {Chun Yen Lee and Yi-Ping Phoebe Chen},
  doi          = {10.1109/TNNLS.2021.3111745},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {4770-4780},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {New insights into drug repurposing for COVID-19 using deep learning},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021c). IEEE computational intelligence society information.
<em>TNNLS</em>, <em>32</em>(10), C3. (<a
href="https://doi.org/10.1109/TNNLS.2021.3112411">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Provides a listing of current committee members and society officers.},
  archive      = {J_TNNLS},
  doi          = {10.1109/TNNLS.2021.3112411},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {C3},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {IEEE computational intelligence society information},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Event-driven synchronization of switched complex networks:
A reachable-set-based design. <em>TNNLS</em>, <em>32</em>(10),
4761–4768. (<a
href="https://doi.org/10.1109/TNNLS.2020.3026646">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study is concerned with the event-driven synchronization for discrete-time switched complex networks. To mitigate the transmission frequency, the dynamic event-triggered mechanism is introduced to orchestrate information transmission. In addition, the investigated complex networks are subject to the unknown nonrandom perturbation with bounded peak, and thus, conventional approaches do not apply, and new approaches are required. For handling this situation, a novel reachable-set-based synchronization technique is then established. With the dwell time switching strategy, sufficient conditions with less conservativeness are formulated, under which the synchronization error is attracted exponentially to a bounded closed region for any initial conditions. Alternatively, for some specified initial sets, the synchronization error is constrained permanently in a bounded closed set. Finally, numerical simulations substantiate the effectiveness and applicability of the theoretical results.},
  archive      = {J_TNNLS},
  author       = {Hong Sang and Jun Zhao},
  doi          = {10.1109/TNNLS.2020.3026646},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {4761-4768},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Event-driven synchronization of switched complex networks: A reachable-set-based design},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptive propagation graph convolutional network.
<em>TNNLS</em>, <em>32</em>(10), 4755–4760. (<a
href="https://doi.org/10.1109/TNNLS.2020.3025110">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph convolutional networks (GCNs) are a family of neural network models that perform inference on graph data by interleaving vertexwise operations and message-passing exchanges across nodes. Concerning the latter, two key questions arise: 1) how to design a differentiable exchange protocol (e.g., a one-hop Laplacian smoothing in the original GCN) and 2) how to characterize the tradeoff in complexity with respect to the local updates. In this brief, we show that the state-of-the-art results can be achieved by adapting the number of communication steps independently at every node. In particular, we endow each node with a halting unit (inspired by Graves’ adaptive computation time [1]) that after every exchange decides whether to continue communicating or not. We show that the proposed adaptive propagation GCN (AP-GCN) achieves superior or similar results to the best proposed models so far on a number of benchmarks while requiring a small overhead in terms of additional parameters. We also investigate a regularization term to enforce an explicit tradeoff between communication and accuracy. The code for the AP-GCN experiments is released as an open-source library.},
  archive      = {J_TNNLS},
  author       = {Indro Spinelli and Simone Scardapane and Aurelio Uncini},
  doi          = {10.1109/TNNLS.2020.3025110},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {4755-4760},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive propagation graph convolutional network},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Gated value network for multilabel classification.
<em>TNNLS</em>, <em>32</em>(10), 4748–4754. (<a
href="https://doi.org/10.1109/TNNLS.2020.3019804">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a gated value network (GVN) for general multilabel classification (MLC) tasks. GVN was motivated by deep value network (DVN) that directly exploits the “compatibility” metric as the learning pursuit for MLC. Meanwhile, it further improves traditional DVN on twofold. First, GVN relaxes the complex variable optimization steps in DVN inference by incorporating a feedforward predictor for straightforward multilabel prediction. Second, GVN also introduces the gating mechanism to block confounding factors from the input data that allows more precise compatibility evaluations for data and their potential multilabels. The whole GVN framework is trained in an end-to-end manner with policy gradient approaches. We show the effectiveness and generalization of GVN on diverse learning tasks, including document classification, audio tagging, and image attribute prediction.},
  archive      = {J_TNNLS},
  author       = {Yimin Hou and Sen Wan and Feng Bao and Zhiquan Ren and Yunfeng Dong and Qionghai Dai and Yue Deng},
  doi          = {10.1109/TNNLS.2020.3019804},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {4748-4754},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Gated value network for multilabel classification},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Class-variant margin normalized softmax loss for deep face
recognition. <em>TNNLS</em>, <em>32</em>(10), 4742–4747. (<a
href="https://doi.org/10.1109/TNNLS.2020.3017528">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In deep face recognition, the commonly used softmax loss and its newly proposed variations are not yet sufficiently effective to handle the class imbalance and softmax saturation issues during the training process while extracting discriminative features. In this brief, to address both issues, we propose a class-variant margin (CVM) normalized softmax loss, by introducing a true-class margin and a false-class margin into the cosine space of the angle between the feature vector and the class-weight vector. The true-class margin alleviates the class imbalance problem, and the false-class margin postpones the early individual saturation of softmax. With negligible computational complexity increment during training, the new loss function is easy to implement in the common deep learning frameworks. Comprehensive experiments on the LFW, YTF, and MegaFace protocols demonstrate the effectiveness of the proposed CVM loss function.},
  archive      = {J_TNNLS},
  author       = {Wanping Zhang and Yongru Chen and Wenming Yang and Guijin Wang and Jing-Hao Xue and Qingmin Liao},
  doi          = {10.1109/TNNLS.2020.3017528},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {4742-4747},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Class-variant margin normalized softmax loss for deep face recognition},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Synaptic weight evolution and charge trapping mechanisms in
a synaptic pass-transistor operation with a direct potential output.
<em>TNNLS</em>, <em>32</em>(10), 4728–4741. (<a
href="https://doi.org/10.1109/TNNLS.2020.3047963">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present an intensive study on the weight modulation and charge trapping mechanisms of the synaptic transistor based on a pass-transistor concept for the direct voltage output. In this article, the pass-transistor concept for a metal–oxide–semiconductor field-effect transistor is employed to a synaptic transistor with a charge trapping layer, which is named a synaptic pass transistor (SPT). Based on this SPT concept, the voltage signal would be provided at the output terminal directly without requiring a complicated circuitry, whereas the conventional synaptic transistor with the current output needs a conversion circuit. For the SPT, the definition of the synaptic weight as a transfer efficiency and operation principles of the SPT with charge-trapping mechanisms is analyzed theoretically. The respective semiconductor device simulation results, such as synaptic output and weight modulations as a function of time for a synaptic depression and facilitation, are presented with detailed analysis. Also, it is shown that an SPT array configuration can perform a synaptic scaling by itself, i.e., a self-normalization of the weight, which is confirmed with the simulation results of learning a simple classification example. Moreover, to verify the potential usage of the SPT array as an analog artificial intelligence accelerator, a classification task for a standard data set, e.g., Modified National Institute of Standards and Technology database (MNIST), is also tested by monitoring the accuracy. Finally, it is found that SPTs proposed here can exhibit low power consumption at a device level as well as sufficient accuracy at the array level while more closely mimicking the biological synapse.},
  archive      = {J_TNNLS},
  author       = {Yeonsu Kang and Jiung Jang and Danyoung Cha and Sungsik Lee},
  doi          = {10.1109/TNNLS.2020.3047963},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {4728-4741},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Synaptic weight evolution and charge trapping mechanisms in a synaptic pass-transistor operation with a direct potential output},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Spatiotemporal behind-the-meter load and PV power
forecasting via deep graph dictionary learning. <em>TNNLS</em>,
<em>32</em>(10), 4713–4727. (<a
href="https://doi.org/10.1109/TNNLS.2020.3042434">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, with the rapid growth of rooftop photovoltaic (PV) generation in distribution networks, power system operators call for accurate forecasts of the behind-the-meter (BTM) load and PV generation. However, the existing forecasting methodologies are incapable of quantifying such BTM measurements as the smart meters can merely measure the net load time series. Motivated by this challenge, this article presents the spatiotemporal BTM load and PV forecasting (ST-BTMLPVF) problem. The objective is to disaggregate the historical net loads of neighboring residential units into their BTM load and PV generation and forecast the future values of these unobservable time series. To solve ST-BTMLPVF, we model the units as a spatiotemporal graph (ST-graph) where the nodes represent the net load measurements of units and edges reflect the mutual correlation between the units. An ST-graph autoencoder (ST-GAE) is devised to capture the spatiotemporal manifold of the ST-graph, and a novel spatiotemporal graph dictionary learning (STGDL) optimization is proposed to utilize the latent features of the ST-GAE to find the most significant spatiotemporal features of the net load. STGDL utilizes the captured features to estimate the historical BTM load and PV measurements, which are further used by a deep recurrent structure to forecast the future values of BTM load and PV generation at each unit. Numerical experiments on a real-world load and PV data set show the state-of-the-art performance of the proposed model, both for the BTM disaggregation and forecasting tasks.},
  archive      = {J_TNNLS},
  author       = {Mahdi Khodayar and Guangyi Liu and Jianhui Wang and Okyay Kaynak and Mohammad E. Khodayar},
  doi          = {10.1109/TNNLS.2020.3042434},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {4713-4727},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Spatiotemporal behind-the-meter load and PV power forecasting via deep graph dictionary learning},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Neural-network-based adaptive DSC design for switched
fractional-order nonlinear systems. <em>TNNLS</em>, <em>32</em>(10),
4703–4712. (<a
href="https://doi.org/10.1109/TNNLS.2020.3027339">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the particularity of the fractional-order derivative definition, the fractional-order control design is more complicated and difficult than the integer-order control design, and it has more practical significance. Therefore, in this article, a novel adaptive switching dynamic surface control (DSC) strategy is first presented for fractional-order nonlinear systems in the nonstrict feedback form with unknown dead zones and arbitrary switchings. In order to avoid the problem of computational complexity and to continuously obtain fractional derivatives for virtual control, the fractional-order DSC technique is applied. The virtual control law, dead-zone input, and the fractional-order adaptive laws are designed based on the fractional-order Lyapunov stability criterion. By combining the universal approximation of neural networks (NNs) and the compensation technique of unknown dead-zones, and stability theory of common Lyapunov function, an adaptive switching DSC controller is developed to ensure the stability of switched fractional-order systems in the presence of unknown dead-zone and arbitrary switchings. Finally, the validity and superiority of the proposed control method are tested by applying chaos suppression of fractional power systems and a numerical example.},
  archive      = {J_TNNLS},
  author       = {Shuai Sui and C. L. Philip Chen and Shaocheng Tong},
  doi          = {10.1109/TNNLS.2020.3027339},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {4703-4712},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Neural-network-based adaptive DSC design for switched fractional-order nonlinear systems},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Feature interaction for streaming feature selection.
<em>TNNLS</em>, <em>32</em>(10), 4691–4702. (<a
href="https://doi.org/10.1109/TNNLS.2020.3025922">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional feature selection methods assume that all data instances and features are known before learning. However, it is not the case in many real-world applications that we are more likely faced with data streams or feature streams or both. Feature streams are defined as features that flow in one by one over time, whereas the number of training examples remains fixed. Existing streaming feature selection methods focus on removing irrelevant and redundant features and selecting the most relevant features, but they ignore the interaction between features. A feature might have little correlation with the target concept by itself, but, when it is combined with some other features, they can be strongly correlated with the target concept. In other words, the interactive features contribute to the target concept as an integer greater than the sum of individuals. Nevertheless, most of the existing streaming feature selection methods treat features individually, but it is necessary to consider the interaction between features. In this article, we focus on the problem of feature interaction in feature streams and propose a new streaming feature selection method that can select features to interact with each other, named Streaming Feature Selection considering Feature Interaction (SFS-FI). With the formal definition of feature interaction, we design a new metric named interaction gain that can measure the interaction degree between the new arriving feature and the selected feature subset. Besides, we analyzed and demonstrated the relationship between feature relevance and feature interaction. Extensive experiments conducted on 14 real-world microarray data sets indicate the efficiency of our new method.},
  archive      = {J_TNNLS},
  author       = {Peng Zhou and Peipei Li and Shu Zhao and Xindong Wu},
  doi          = {10.1109/TNNLS.2020.3025922},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {4691-4702},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Feature interaction for streaming feature selection},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Model-driven architecture of extreme learning machine to
extract power flow features. <em>TNNLS</em>, <em>32</em>(10), 4680–4690.
(<a href="https://doi.org/10.1109/TNNLS.2020.3025905">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Probabilistic power flow (PPF) calculation is an important power system analysis tool considering the increasing uncertainties. However, existing calculation methods cannot simultaneously achieve high precision and fast calculation, which limits the practical application of the PPF. This article designs a specific architecture of the extreme learning machine (ELM) in a model-driven pattern to extract the power flow features and therefore accelerate the calculation of PPF. ELM is selected because of the unique characteristics of fast training and less intervention. The key challenge is that the learning capability of the ELM for extracting complex features is limited compared with deep neural networks. In this article, we use the physical properties of the power flow model to assist the learning process. To reduce the learning complexity of the power flow features, the feature decomposition and nonlinearity reduction method is proposed to extract the features of the power flow model. An enhanced ELM network architecture is designed. An optimization model for the hidden node parameters is established to improve the learning performance. Based on the proposed model-driven ELM architecture, a fast and accurate PPF calculation method is proposed. The simulations on the IEEE 57-bus and Polish 2383-bus systems demonstrate the effectiveness of the proposed method.},
  archive      = {J_TNNLS},
  author       = {Qian Gao and Zhifang Yang and Juan Yu and Wei Dai and Xingyu Lei and Bo Tang and Kaigui Xie and Wenyuan Li},
  doi          = {10.1109/TNNLS.2020.3025905},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {4680-4690},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Model-driven architecture of extreme learning machine to extract power flow features},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Explainable uncertainty-aware convolutional recurrent neural
network for irregular medical time series. <em>TNNLS</em>,
<em>32</em>(10), 4665–4679. (<a
href="https://doi.org/10.1109/TNNLS.2020.3025813">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Influenced by the dynamic changes in the severity of illness, patients usually take examinations in hospitals irregularly, producing a large volume of irregular medical time-series data. Performing diagnosis prediction from the irregular medical time series is challenging because the intervals between consecutive records significantly vary along time. Existing methods often handle this problem by generating regular time series from the irregular medical records without considering the uncertainty in the generated data, induced by the varying intervals. Thus, a novel Uncertainty-Aware Convolutional Recurrent Neural Network (UA-CRNN) is proposed in this article, which introduces the uncertainty information in the generated data to boost the risk prediction. To tackle the complex medical time series with subseries of different frequencies, the uncertainty information is further incorporated into the subseries level rather than the whole sequence to seamlessly adjust different time intervals. Specifically, a hierarchical uncertainty-aware decomposition layer (UADL) is designed to adaptively decompose time series into different subseries and assign them proper weights in accordance with their reliabilities. Meanwhile, an Explainable UA-CRNN (eUA-CRNN) is proposed to exploit filters with different passbands to ensure the unity of components in each subseries and the diversity of components in different subseries. Furthermore, eUA-CRNN incorporates with an uncertainty-aware attention module to learn attention weights from the uncertainty information, providing the explainable prediction results. The extensive experimental results on three real-world medical data sets illustrate the superiority of the proposed method compared with the state-of-the-art methods.},
  archive      = {J_TNNLS},
  author       = {Qingxiong Tan and Mang Ye and Andy Jinhua Ma and Baoyao Yang and Terry Cheuk-Fung Yip and Grace Lai-Hung Wong and Pong C. Yuen},
  doi          = {10.1109/TNNLS.2020.3025813},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {4665-4679},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Explainable uncertainty-aware convolutional recurrent neural network for irregular medical time series},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bearing-based adaptive neural formation scaling control for
autonomous surface vehicles with uncertainties and input saturation.
<em>TNNLS</em>, <em>32</em>(10), 4653–4664. (<a
href="https://doi.org/10.1109/TNNLS.2020.3025807">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When a group of autonomous surface vehicles (ASVs) sail from a wide waterway to a narrow waterway, one difficulty is to keep relative formation with collision avoidance. Scaling the formation sizes with formation shapes invariant is a promising way. This article investigates such a formation scaling control problem of ASVs with uncertainties and input saturation. A novel bearing-based adaptive neural formation scaling control scheme for ASVs is developed. The main idea of this formation scheme is as follows. Choose a small number of leader ASVs based on bearing rigidity theory and program their trajectories according to the kinematics of formation scaling maneuver. Steer remaining ASVs to follow leader ASVs via adaptive neural techniques and the formation sizes can be scaled only by leaders without redesigning control inputs of followers. To deal with the uncertainties of ASVs, weights updating of neural networks is simplified into one-parameter estimation in each control channel. Auxiliary systems are introduced for each ASV to reduce the effect of limited actuator capability. It is shown that desired formation scaling maneuver of ASVs can be achieved with the proposed formation scheme if the augmented formation is infinitesimally bearing rigid. Formation errors are guaranteed to be uniformly ultimately bounded. The main advantage of our scheme over existing results is that directional, computational, and actuator constraints are satisfied simultaneously in the formation scaling control of ASVs. Simulations and comparisons are provided to illustrate the effectiveness of theoretical results.},
  archive      = {J_TNNLS},
  author       = {Yu Lu and Changyun Wen and Tielong Shen and Weidong Zhang},
  doi          = {10.1109/TNNLS.2020.3025807},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {4653-4664},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Bearing-based adaptive neural formation scaling control for autonomous surface vehicles with uncertainties and input saturation},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning automata-based multiagent reinforcement learning
for optimization of cooperative tasks. <em>TNNLS</em>, <em>32</em>(10),
4639–4652. (<a
href="https://doi.org/10.1109/TNNLS.2020.3025711">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiagent reinforcement learning (MARL) has been extensively used in many applications for its tractable implementation and task distribution. Learning automata, which can be classified under MARL in the category of independent learner, are used to obtain the optimal joint action or some type of equilibrium. Learning automata have the following advantages. First, learning automata do not require any agent to observe the action of any other agent. Second, learning automata are simple in structure and easy to be implemented. Learning automata have been applied to function optimization, image processing, data clustering, recommender systems, and wireless sensor networks. However, a few learning automata-based algorithms have been proposed for optimization of cooperative repeated games and stochastic games. We propose an algorithm known as learning automata for optimization of cooperative agents (LA-OCA). To make learning automata applicable to cooperative tasks, we transform the environment to a P-model by introducing an indicator variable whose value is one when the maximal reward is obtained and is zero otherwise. Theoretical analysis shows that all the strict optimal joint actions are stable critical points of the model of LA-OCA in cooperative repeated games with an arbitrary finite number of players and actions. Simulation results show that LA-OCA obtains the pure optimal joint strategy with a success rate of 100\% in all of the three cooperative tasks and outperforms the other algorithms in terms of learning speed.},
  archive      = {J_TNNLS},
  author       = {Zhen Zhang and Dongqing Wang and Junwei Gao},
  doi          = {10.1109/TNNLS.2020.3025711},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {4639-4652},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Learning automata-based multiagent reinforcement learning for optimization of cooperative tasks},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A minibatch proximal stochastic recursive gradient algorithm
using a trust-region-like scheme and barzilai–borwein stepsizes.
<em>TNNLS</em>, <em>32</em>(10), 4627–4638. (<a
href="https://doi.org/10.1109/TNNLS.2020.3025383">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of minimizing the sum of an average of a large number of smooth convex component functions and a possibly nonsmooth convex function that admits a simple proximal mapping. This class of problems arises frequently in machine learning, known as regularized empirical risk minimization (ERM). In this article, we propose mSRGTR-BB, a minibatch proximal stochastic recursive gradient algorithm, which employs a trust-region-like scheme to select stepsizes that are automatically computed by the Barzilai–Borwein method. We prove that mSRGTR-BB converges linearly in expectation for strongly and nonstrongly convex objective functions. With proper parameters, mSRGTR-BB enjoys a faster convergence rate than the state-of-the-art minibatch proximal variant of the semistochastic gradient method (mS2GD). Numerical experiments on standard data sets show that the performance of mSRGTR-BB is comparable to and sometimes even better than mS2GD with best-tuned stepsizes and is superior to some modern proximal stochastic gradient methods.},
  archive      = {J_TNNLS},
  author       = {Tengteng Yu and Xin-Wei Liu and Yu-Hong Dai and Jie Sun},
  doi          = {10.1109/TNNLS.2020.3025383},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {4627-4638},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A minibatch proximal stochastic recursive gradient algorithm using a trust-region-like scheme and Barzilai–Borwein stepsizes},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Nonergodic complexity of proximal inertial gradient
descents. <em>TNNLS</em>, <em>32</em>(10), 4613–4626. (<a
href="https://doi.org/10.1109/TNNLS.2020.3025157">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The proximal inertial gradient descent (PIGD) is efficient for the composite minimization and applicable for broad of machine learning problems. In this article, we revisit the computational complexity of this algorithm and present other novel results, especially on the convergence rates of the objective function values. The nonergodic $O(1/k)$ rate is proved for PIGD with constant step size when the objective function is coercive. When the objective function fails to promise coercivity, we prove the sublinear rate with diminishing inertial parameters. In the case that the objective function satisfies the Polyak– Lojasiewicz (PŁ) property, the linear convergence is proved with much larger and general step size than the previous literature. We also extend our results to the multiblock version and present the computational complexity. Both cyclic and stochastic index selection strategies are considered.},
  archive      = {J_TNNLS},
  author       = {Tao Sun and Linbo Qiao and Dongsheng Li},
  doi          = {10.1109/TNNLS.2020.3025157},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {4613-4626},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Nonergodic complexity of proximal inertial gradient descents},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Modeling and control of hybrid 3-d gaits of snake-like
robots. <em>TNNLS</em>, <em>32</em>(10), 4603–4612. (<a
href="https://doi.org/10.1109/TNNLS.2020.3024585">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Snake-like robots move flexibly in complex environments due to their multiple degrees of freedom and various gaits. However, their existing 3-D models are not accurate enough, and most gaits are applicable to special environments only. This work investigates a 3-D model and designs hybrid 3-D gaits. In the proposed 3-D model, a robot is considered as a continuous beam system. Its normal reaction forces are computed based on the mechanics of materials. To improve the applicability of such robots to different terrains or tasks, this work designs hybrid 3-D gaits by mixing basic gaits in different parts of their bodies. Performances of hybrid gaits are analyzed based on extensive simulations. These gaits are compared with traditional gaits including lateral undulation, rectilinear, and sidewinding ones. Results of simulations and physical experiments are presented to demonstrate the performances of the proposed model and hybrid gaits of snake-like robots.},
  archive      = {J_TNNLS},
  author       = {Zhengcai Cao and Dong Zhang and MengChu Zhou},
  doi          = {10.1109/TNNLS.2020.3024585},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {4603-4612},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Modeling and control of hybrid 3-D gaits of snake-like robots},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Self-organizing nebulous growths for robust and incremental
data visualization. <em>TNNLS</em>, <em>32</em>(10), 4588–4602. (<a
href="https://doi.org/10.1109/TNNLS.2020.3023941">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nonparametric dimensionality reduction techniques, such as t-distributed Stochastic Neighbor Embedding (t-SNE) and uniform manifold approximation and projection (UMAP), are proficient in providing visualizations for data sets of fixed sizes. However, they cannot incrementally map and insert new data points into an already provided data visualization. We present self-organizing nebulous growths (SONG), a parametric nonlinear dimensionality reduction technique that supports incremental data visualization, i.e., incremental addition of new data while preserving the structure of the existing visualization. In addition, SONG is capable of handling new data increments, no matter whether they are similar or heterogeneous to the already observed data distribution. We test SONG on a variety of real and simulated data sets. The results show that SONG is superior to Parametric t-SNE, t-SNE, and UMAP in incremental data visualization. Especially, for heterogeneous increments, SONG improves over Parametric t-SNE by 14.98\% on the Fashion MNIST data set and 49.73\% on the MNIST data set regarding the cluster quality measured by the adjusted mutual information scores. On similar or homogeneous increments, the improvements are 8.36\% and 42.26\%, respectively. Furthermore, even when the abovementioned data sets are presented all at once, SONG performs better or comparable to UMAP and superior to t-SNE. We also demonstrate that the algorithmic foundations of SONG render it more tolerant to noise compared with UMAP and t-SNE, thus providing greater utility for data with high variance, high mixing of clusters, or noise.},
  archive      = {J_TNNLS},
  author       = {Damith A. Senanayake and Wei Wang and Shalin H. Naik and Saman Halgamuge},
  doi          = {10.1109/TNNLS.2020.3023941},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {4588-4602},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Self-organizing nebulous growths for robust and incremental data visualization},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust formation control for cooperative underactuated
quadrotors via reinforcement learning. <em>TNNLS</em>, <em>32</em>(10),
4577–4587. (<a
href="https://doi.org/10.1109/TNNLS.2020.3023711">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, the model-free robust formation control problem is addressed for cooperative underactuated quadrotors involving unknown nonlinear dynamics and disturbances. Based on the hierarchical control scheme and the reinforcement learning theory, a robust controller is proposed without knowledge of each quadrotor dynamics, consisting of a distributed observer to estimate the position state of the leader, a position controller to achieve the desired formation, and an attitude controller to control the rotational motion. Simulation results on the multiquadrotor system confirm the effectiveness of the proposed model-free robust formation control method.},
  archive      = {J_TNNLS},
  author       = {Wanbing Zhao and Hao Liu and Frank L. Lewis},
  doi          = {10.1109/TNNLS.2020.3023711},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {4577-4587},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Robust formation control for cooperative underactuated quadrotors via reinforcement learning},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Disturbance-aware neuro-optimal system control using
generative adversarial control networks. <em>TNNLS</em>,
<em>32</em>(10), 4565–4576. (<a
href="https://doi.org/10.1109/TNNLS.2020.3022950">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Disturbance, which is generally unknown to the controller, is unavoidable in real-world systems and it may affect the expected system state and output. Existing control methods, like robust model predictive control, can produce robust solutions to maintain the system stability. However, these robust methods trade the solution optimality for stability. In this article, a method called generative adversarial control networks (GACNs) is proposed to train a controller via demonstrations of the optimal controller. By formulating the optimal control problem in the presence of disturbance, the controller trained by GACNs obtains neuro-optimal solutions without knowing the future disturbance and determines the objective function explicitly. A joint loss, composed of the adversarial loss and the least square loss, is designed to be used in the training of the generator. Experimental results on simulated systems with disturbance show that GACNs outperform other compared control methods.},
  archive      = {J_TNNLS},
  author       = {Kai-Fung Chu and Albert Y. S. Lam and Chenchen Fan and Victor O. K. Li},
  doi          = {10.1109/TNNLS.2020.3022950},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {4565-4576},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Disturbance-aware neuro-optimal system control using generative adversarial control networks},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robot motor skill transfer with alternate learning in two
spaces. <em>TNNLS</em>, <em>32</em>(10), 4553–4564. (<a
href="https://doi.org/10.1109/TNNLS.2020.3021530">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent research achievements in learning from demonstration (LfD) illustrate that the reinforcement learning is effective for the robots to improve their movement skills. The current challenge mainly remains in how to generate new robot motions automatically to perform new tasks, which have a similar preassigned performance indicator but are different from the demonstration tasks. To deal with the abovementioned issue, this article proposes a framework to represent the policy and conduct imitation learning and optimization for robot intelligent trajectory planning, based on the improved locally weighted regression (iLWR) and policy improvement with path integral by dual perturbation (PI 2 -DP). Besides, the reward-guided weight searching and basis function’s adaptive evolving are performed alternately in two spaces, i.e., the basis function space and the weight space, to deal with the abovementioned problem. The alternate learning process constructs a sequence of two-tuples that join the demonstration task and new one together for motor skill transfer, so that the robot gradually acquires motor skill, from the task similar to demonstration to dissimilar tasks with different performance metrics. Classical via-points trajectory planning experiments are performed with the SCARA manipulator, a 10-degree of freedom (DOF) planar, and the UR robot. These results show that the proposed method is not only feasible but also effective.},
  archive      = {J_TNNLS},
  author       = {Jian Fu and Xiang Teng and Ce Cao and Zhaojie Ju and Ping Lou},
  doi          = {10.1109/TNNLS.2020.3021530},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {4553-4564},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Robot motor skill transfer with alternate learning in two spaces},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Optimal impulsive control using adaptive dynamic programming
and its application in spacecraft rendezvous. <em>TNNLS</em>,
<em>32</em>(10), 4544–4552. (<a
href="https://doi.org/10.1109/TNNLS.2020.3021037">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimal control of nonlinear impulsive systems with free impulse instants and the number of impulses is investigated in this study. A scheme based on adaptive dynamic programming is developed, which leads to a feedback (approximate) solution to the defined optimal impulsive control problem. This is done by proposing a learning algorithm for tuning parameters of a function approximator, which, once tuned offline, provides feedback solution on-the-fly. The scheme is shown to handle single and multiple impulsive actuators with a small online computational burden. Afterward, the controller is applied to a challenging problem, namely, the orbital maneuver of spacecraft with the fixed final time using impulsive actuators. The objective is triggering the actuators in a fuel-optimal manner such that the spacecraft transfers to the desired orbit at a prescribed time. It was shown that the proposed scheme leads to simultaneous and feedback path planning and control for the maneuver. The potentials of the scheme are analyzed in different scenarios, including enforcing a shorter final time, selecting different initial states, and incorporating actuator faults.},
  archive      = {J_TNNLS},
  author       = {Ali Heydari},
  doi          = {10.1109/TNNLS.2020.3021037},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {4544-4552},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Optimal impulsive control using adaptive dynamic programming and its application in spacecraft rendezvous},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An uplink communication-efficient approach to featurewise
distributed sparse optimization with differential privacy.
<em>TNNLS</em>, <em>32</em>(10), 4529–4543. (<a
href="https://doi.org/10.1109/TNNLS.2020.3020955">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In sparse empirical risk minimization (ERM) models, when sensitive personal data are used, e.g., genetic, healthcare, and financial data, it is crucial to preserve the differential privacy (DP) in training. In many applications, the information (i.e., features) of an individual is held by different organizations, which give rise to the prevalent yet challenging setting of the featurewise distributed multiparty model training. Such a setting is also beneficial to the scalability when the number of features exceeds the computation and storage capacity of a single node. However, existing private sparse optimizations are limited to centralized and samplewise distributed datasets only. In this article, we develop a differentially private algorithm for the sparse ERM model training under the featurewise distributed datasets setting. Our algorithm comes with guaranteed DP, nearly optimal utility, and reduced uplink communication complexity. Accordingly, we present a more generalized convergence analysis for block-coordinate Frank–Wolfe (BCFW) under arbitrary sampling (denoted as BCFW-AS in short), which significantly extends the known convergence results that apply to two specific sampling distributions only. To further reduce the uplink communication cost, we design an active private feature sharing scheme, which is new in both design and analysis of BCFW, to guarantee the convergence of communicating Johnson–Lindenstrauss transformed features. Empirical studies justify the new convergence as well as the nearly optimal utility theoretical results.},
  archive      = {J_TNNLS},
  author       = {Jian Lou and Yiu-ming Cheung},
  doi          = {10.1109/TNNLS.2020.3020955},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {4529-4543},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {An uplink communication-efficient approach to featurewise distributed sparse optimization with differential privacy},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Inductive structure consistent hashing via flexible
semantic calibration. <em>TNNLS</em>, <em>32</em>(10), 4514–4528. (<a
href="https://doi.org/10.1109/TNNLS.2020.3018790">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic-preserving hashing establishes efficient multimedia retrieval by transferring knowledge from original data to hash codes so that the latter can preserve the underlying visual and semantic similarities. However, it becomes a crucial bottleneck: how to effectively bridge the trilateral domain gaps (i.e., the visual, semantic, and hashing spaces) to further improve the retrieval accuracy. In this article, we propose an inductive structure consistent hashing (ISCH) method, which can interactively coordinate the semantic correlations between the visual feature space, the binary class space, and the discrete hashing space. Specifically, an inductive semantic space is formulated by a simple multilayer stacking class-encoder, which transforms the naive class information into flexible semantic embeddings. Meanwhile, we design a semantic dictionary learning model to facilitate the bilateral visual-semantic bridging and guide the class-encoder toward reliable semantics, which could well alleviate the visual-semantic bias problem. In particular, the visual descriptors and respective semantic class representations are regularized with a coinciding alignment module. In order to generate privileged hash codes, we further explore semantic and prototype binary code learning to jointly quantify the semantic and latent visual representations into unified discrete hash codes. Moreover, an efficient optimization algorithm is developed to address the resulting discrete programming problem. Comprehensive experiments conducted on four large-scale data sets, i.e., CIFAR-10, NUSWIDE, ImageNet, and MSCOCO, demonstrate the superiority of our method over the state-of-the-art alternatives against different evaluation protocols.},
  archive      = {J_TNNLS},
  author       = {Zheng Zhang and Luyao Liu and Yadan Luo and Zi Huang and Fumin Shen and Heng Tao Shen and Guangming Lu},
  doi          = {10.1109/TNNLS.2020.3018790},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {4514-4528},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Inductive structure consistent hashing via flexible semantic calibration},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). EDP: An efficient decomposition and pruning scheme for
convolutional neural network compression. <em>TNNLS</em>,
<em>32</em>(10), 4499–4513. (<a
href="https://doi.org/10.1109/TNNLS.2020.3018177">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Model compression methods have become popular in recent years, which aim to alleviate the heavy load of deep neural networks (DNNs) in real-world applications. However, most of the existing compression methods have two limitations: 1) they usually adopt a cumbersome process, including pretraining, training with a sparsity constraint, pruning/decomposition, and fine-tuning. Moreover, the last three stages are usually iterated multiple times. 2) The models are pretrained under explicit sparsity or low-rank assumptions, which are difficult to guarantee wide appropriateness. In this article, we propose an efficient decomposition and pruning (EDP) scheme via constructing a compressed-aware block that can automatically minimize the rank of the weight matrix and identify the redundant channels. Specifically, we embed the compressed-aware block by decomposing one network layer into two layers: a new weight matrix layer and a coefficient matrix layer. By imposing regularizers on the coefficient matrix, the new weight matrix learns to become a low-rank basis weight, and its corresponding channels become sparse. In this way, the proposed compressed-aware block simultaneously achieves low-rank decomposition and channel pruning by only one single data-driven training stage. Moreover, the network of architecture is further compressed and optimized by a novel Pruning &amp; Merging (PM) module which prunes redundant channels and merges redundant decomposed layers. Experimental results (17 competitors) on different data sets and networks demonstrate that the proposed EDP achieves a high compression ratio with acceptable accuracy degradation and outperforms state-of-the-arts on compression rate, accuracy, inference time, and run-time memory.},
  archive      = {J_TNNLS},
  author       = {Xiaofeng Ruan and Yufan Liu and Chunfeng Yuan and Bing Li and Weiming Hu and Yangxi Li and Stephen Maybank},
  doi          = {10.1109/TNNLS.2020.3018177},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {4499-4513},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {EDP: An efficient decomposition and pruning scheme for convolutional neural network compression},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Terminal sliding mode control of MEMS gyroscopes with
finite-time learning. <em>TNNLS</em>, <em>32</em>(10), 4490–4498. (<a
href="https://doi.org/10.1109/TNNLS.2020.3018107">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a neural terminal sliding mode controller (TSMC) with finite-time (FT) convergence for the uncertain MEMS gyroscope dynamics. To address the uncertainty, considering the periodic tracking property of MEMS gyroscopes, a composite learning mechanism driven by the learning performance evaluation signal is applied to learn the system dynamics. By selecting the terminal sliding mode surface, the TSMC is constructed with error feedback and feedforward compensation. Under the TSMC with the composite learning, the system tracking can be guaranteed to be with FT convergence, while the weights of the learning system will converge in FT. The stability of the closed-loop system is analyzed by the Lyapunov approach. The highlight is that the design can obtain the learning knowledge while the information can be directly reused in repeated tasks with no need of online update. The effectiveness of the proposed method is verified via reference signal tracking of MEMS gyroscopes, while the controller using the stored knowledge achieves better tracking performance of faster convergence and higher tracking accuracy with no need of weight update.},
  archive      = {J_TNNLS},
  author       = {Yuyan Guo and Bin Xu and Rui Zhang},
  doi          = {10.1109/TNNLS.2020.3018107},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {4490-4498},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Terminal sliding mode control of MEMS gyroscopes with finite-time learning},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Antidecay LSTM for siamese tracking with adversarial
learning. <em>TNNLS</em>, <em>32</em>(10), 4475–4489. (<a
href="https://doi.org/10.1109/TNNLS.2020.3018025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual tracking is one of the fundamental tasks in computer vision with many challenges, and it is mainly due to the changes in the target’s appearance in temporal and spatial domains. Recently, numerous trackers model the appearance of the targets in the spatial domain well by utilizing deep convolutional features. However, most of these CNN-based trackers only take the appearance variations between two consecutive frames in a video sequence into consideration. Besides, some trackers model the appearance of the targets in the long term by applying RNN, but the decay of the target’s features degrades the tracking performance. In this article, we propose the antidecay long short-term memory (AD-LSTM) for the Siamese tracking. Especially, we extend the architecture of the standard LSTM in two aspects for the visual tracking task. First, we replace all of the fully connected layers with convolutional layers to extract the features with spatial structure. Second, we improve the architecture of the cell unit. In this way, the information of the target appearance can flow through the AD-LSTM without decay as long as possible in the temporal domain. Meanwhile, since there is no ground truth for the feature maps generated by the AD-LSTM, we propose an adversarial learning algorithm to optimize the AD-LSTM. With the help of adversarial learning, the Siamese network can generate the response maps more accurately, and the AD-LSTM can generate the feature maps of the target more robustly. The experimental results show that our tracker performs favorably against the state-of-the-art trackers on six challenging benchmarks: OTB-100, TC-128, VOT2016, VOT2017, GOT-10k, and TrackingNet.},
  archive      = {J_TNNLS},
  author       = {Fei Zhao and Ting Zhang and Yi Wu and Ming Tang and Jinqiao Wang},
  doi          = {10.1109/TNNLS.2020.3018025},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {4475-4489},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Antidecay LSTM for siamese tracking with adversarial learning},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). IAUnet: Global context-aware feature learning for person
reidentification. <em>TNNLS</em>, <em>32</em>(10), 4460–4474. (<a
href="https://doi.org/10.1109/TNNLS.2020.3017939">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person reidentification (reID) by convolutional neural network (CNN)-based networks has achieved favorable performance in recent years. However, most of existing CNN-based methods do not take full advantage of spatial–temporal context modeling. In fact, the global spatial–temporal context can greatly clarify local distractions to enhance the target feature representation. To comprehensively leverage the spatial–temporal context information, in this work, we present a novel block, interaction–aggregation-update (IAU), for high-performance person reID. First, the spatial–temporal IAU (STIAU) module is introduced. STIAU jointly incorporates two types of contextual interactions into a CNN framework for target feature learning. Here, the spatial interactions learn to compute the contextual dependencies between different body parts of a single frame, while the temporal interactions are used to capture the contextual dependencies between the same body parts across all frames. Furthermore, a channel IAU (CIAU) module is designed to model the semantic contextual interactions between channel features to enhance the feature representation, especially for small-scale visual cues and body parts. Therefore, the IAU block enables the feature to incorporate the globally spatial, temporal, and channel context. It is lightweight, end-to-end trainable, and can be easily plugged into existing CNNs to form IAUnet. The experiments show that IAUnet performs favorably against state of the art on both image and video reID tasks and achieves compelling results on a general object categorization task. The source code is available at https://github.com/blue-blue272/ImgReID-IAnet .},
  archive      = {J_TNNLS},
  author       = {Ruibing Hou and Bingpeng Ma and Hong Chang and Xinqian Gu and Shiguang Shan and Xilin Chen},
  doi          = {10.1109/TNNLS.2020.3017939},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {4460-4474},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {IAUnet: Global context-aware feature learning for person reidentification},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Online learning with adaptive rebalancing in nonstationary
environments. <em>TNNLS</em>, <em>32</em>(10), 4445–4459. (<a
href="https://doi.org/10.1109/TNNLS.2020.3017863">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An enormous and ever-growing volume of data is nowadays becoming available in a sequential fashion in various real-world applications. Learning in nonstationary environments constitutes a major challenge, and this problem becomes orders of magnitude more complex in the presence of class imbalance. We provide new insights into learning from nonstationary and imbalanced data in online learning, a largely unexplored area. We propose the novel Adaptive REBAlancing (AREBA) algorithm that selectively includes in the training set a subset of the majority and minority examples that appeared so far, while at its heart lies an adaptive mechanism to continually maintain the class balance between the selected examples. We compare AREBA with strong baselines and other state-of-the-art algorithms and perform extensive experimental work in scenarios with various class imbalance rates and different concept drift types on both synthetic and real-world data. AREBA significantly outperforms the rest with respect to both learning speed and learning quality. Our code is made publicly available to the scientific community.},
  archive      = {J_TNNLS},
  author       = {Kleanthis Malialis and Christos G. Panayiotou and Marios M. Polycarpou},
  doi          = {10.1109/TNNLS.2020.3017863},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {4445-4459},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Online learning with adaptive rebalancing in nonstationary environments},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Synchronous fault-tolerant near-optimal control for
discrete-time nonlinear PE game. <em>TNNLS</em>, <em>32</em>(10),
4432–4444. (<a
href="https://doi.org/10.1109/TNNLS.2020.3017762">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, the synchronous fault-tolerant near-optimal control strategy design problem is studied for a class of discrete-time nonlinear pursuit-evasion (PE) games. In the studied PE game, the input saturation phenomenon and possible actuator fault are simultaneously taken into consideration. To accelerate the estimation speed, a novel nonlinear fault estimator is designed by introducing a nonlinear function. Then, for the purpose of obtaining the synchronous control strategy for the discrete-time PE games, an approximate Hamilton–Jacobi–Isaacs (HJI) equation is established, which is seldom utilized for the discrete-time approximate dynamic programming in most existing results. It should be noticed that the synchronous control strategy designed based on the approximate HJI equation can be convergent very fast because of its quasi-Newton’s iteration form. Furthermore, the sufficient condition is established to guarantee that the studied system is uniformly ultimately bounded. Finally, a numerical simulation of the hypersonic vehicle system is carried out to validate the proposed methodology.},
  archive      = {J_TNNLS},
  author       = {Yuan Yuan and Peng Zhang and Xuelong Li},
  doi          = {10.1109/TNNLS.2020.3017762},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {4432-4444},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Synchronous fault-tolerant near-optimal control for discrete-time nonlinear PE game},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An effective hybrid learning model for real-time event
summarization. <em>TNNLS</em>, <em>32</em>(10), 4419–4431. (<a
href="https://doi.org/10.1109/TNNLS.2020.3017747">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-time event summarization (RES) aims at extracting a handful of document updates from an overwhelming document stream as the real-time event summary that tracks and summarizes the evolving event of interest. It has been attracting much attention, especially with the growth of streaming applications. Despite the effectiveness of previous studies, obtaining relevant, nonredundant, and timely event summaries remains challenging in real-life applications. This study proposes an effective Hybrid learning model for RES (HRES), which attempts to resolve all three challenges (i.e., nonredundancy, relevance, and timeliness) of RES in a unified framework. The main idea is to: 1) exploit the factual background knowledge from the knowledge base (KB) to capture the informative knowledge and implicit information from the input document/query for better text matching; 2) design a memory network to memorize the input facts temporally from the historical document stream and avoid pushing redundant facts in subsequent timesteps; 3) leverage relevance prediction as an auxiliary task to strengthen the document modeling and help to extract relevant documents; and 4) consider both historical dependencies and future uncertainty of the real-time document stream by exploiting the reinforcement learning technique. Extensive experiments demonstrate that HRES has robust superiority over competitors and gains the state-of-the-art results.},
  archive      = {J_TNNLS},
  author       = {Min Yang and Qiang Qu and Ying Shen and Zhou Zhao and Xiaojun Chen and Chengming Li},
  doi          = {10.1109/TNNLS.2020.3017747},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {4419-4431},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {An effective hybrid learning model for real-time event summarization},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Analogy-detail networks for object recognition.
<em>TNNLS</em>, <em>32</em>(10), 4404–4418. (<a
href="https://doi.org/10.1109/TNNLS.2020.3017692">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The human visual system can recognize object categories accurately and efficiently and is robust to complex textures and noises. To mimic the analogy-detail dual-pathway human visual cognitive mechanism revealed in recent cognitive science studies, in this article, we propose a novel convolutional neural network (CNN) architecture named analogy-detail networks (ADNets) for accurate object recognition. ADNets disentangle the visual information and process them separately using two pathways: the analogy pathway extracts coarse and global features representing the gist (i.e., shape and topology) of the object, while the detail pathway extracts fine and local features representing the details (i.e., texture and edges) for determining object categories. We modularize the architecture and encapsulate the two pathways into the analogy-detail block as the CNN building block to construct ADNets. For implementation, we propose a general principle that transmutes typical CNN structures into the ADNet architecture and applies the transmutation on representative baseline CNNs. Extensive experiments on CIFAR10, CIFAR100, street view house numbers, and ImageNet data sets demonstrate that ADNets significantly reduce the test error rates of the baseline CNNs by up to 5.76\% and outperform other state-of-the-art architectures. Comprehensive analysis and visualizations further demonstrate that ADNets are interpretable and have a better shape-texture tradeoff for recognizing the objects with complex textures.},
  archive      = {J_TNNLS},
  author       = {Xiaoyu Tao and Xiaopeng Hong and Weiwei Shi and Xinyuan Chang and Yihong Gong},
  doi          = {10.1109/TNNLS.2020.3017692},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {4404-4418},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Analogy-detail networks for object recognition},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bayesian cycle-consistent generative adversarial networks
via marginalizing latent sampling. <em>TNNLS</em>, <em>32</em>(10),
4389–4403. (<a
href="https://doi.org/10.1109/TNNLS.2020.3017669">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent techniques built on generative adversarial networks (GANs), such as cycle-consistent GANs, are able to learn mappings among different domains built from unpaired data sets, through min–max optimization games between generators and discriminators. However, it remains challenging to stabilize the training process and thus cyclic models fall into mode collapse accompanied by the success of discriminator. To address this problem, we propose an novel Bayesian cyclic model and an integrated cyclic framework for interdomain mappings. The proposed method motivated by Bayesian GAN explores the full posteriors of cyclic model via sampling latent variables and optimizes the model with maximum a posteriori (MAP) estimation. Hence, we name it Bayesian CycleGAN. In addition, original CycleGAN cannot generate diversified results. But it is feasible for Bayesian framework to diversify generated images by replacing restricted latent variables in inference process. We evaluate the proposed Bayesian CycleGAN on multiple benchmark data sets, including Cityscapes, Maps, and Monet2photo. The proposed method improve the per-pixel accuracy by 15\% for the Cityscapes semantic segmentation task within origin framework and improve 20\% within the proposed integrated framework, showing better resilience to imbalance confrontation. The diversified results of Monet2Photo style transfer also demonstrate its superiority over original cyclic model. We provide codes for all of our experiments in https://github.com/ranery/Bayesian-CycleGAN .},
  archive      = {J_TNNLS},
  author       = {Haoran You and Yu Cheng and Tianheng Cheng and Chunliang Li and Pan Zhou},
  doi          = {10.1109/TNNLS.2020.3017669},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {4389-4403},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Bayesian cycle-consistent generative adversarial networks via marginalizing latent sampling},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Accelerated proximal subsampled newton method.
<em>TNNLS</em>, <em>32</em>(10), 4374–4388. (<a
href="https://doi.org/10.1109/TNNLS.2020.3017555">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Composite function optimization problem often arises in machine learning known as regularized empirical minimization. We introduce the acceleration technique to the Newton-type proximal method and propose a novel algorithm called accelerated proximal subsampled Newton method ( APSSN ). APSSN only subsamples a small subset of samples to construct an approximate Hessian that achieves computational efficiency. At the same time, APSSN still keeps a fast convergence rate. Furthermore, we obtain the scaled proximal mapping by solving its dual problem using the semismooth Newton method instead of resorting to the first-order methods. Due to our sampling strategy and the fast convergence rate of the semismooth Newton method, we can get the scaled proximal mapping efficiently. Both our theoretical analysis and empirical study show that APSSN is an effective and computationally efficient algorithm for composite function optimization problems.},
  archive      = {J_TNNLS},
  author       = {Haishan Ye and Luo Luo and Zhihua Zhang},
  doi          = {10.1109/TNNLS.2020.3017555},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {4374-4388},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Accelerated proximal subsampled newton method},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Rich visual knowledge-based augmentation network for visual
question answering. <em>TNNLS</em>, <em>32</em>(10), 4362–4373. (<a
href="https://doi.org/10.1109/TNNLS.2020.3017530">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual question answering (VQA) that involves understanding an image and paired questions develops very quickly with the boost of deep learning in relevant research fields, such as natural language processing and computer vision. Existing works highly rely on the knowledge of the data set. However, some questions require more professional cues other than the data set knowledge to answer questions correctly. To address such an issue, we propose a novel framework named a knowledge-based augmentation network (KAN) for VQA. We introduce object-related open-domain knowledge to assist the question answering. Concretely, we extract more visual information from images and introduce a knowledge graph to provide the necessary common sense or experience for the reasoning process. For these two augmented inputs, we design an attention module that can adjust itself according to the specific questions, such that the importance of external knowledge against detected objects can be balanced adaptively. Extensive experiments show that our KAN achieves state-of-the-art performance on three challenging VQA data sets, i.e., VQA v2, VQA-CP v2, and FVQA. In addition, our open-domain knowledge is also beneficial to VQA baselines. Code is available at https://github.com/yyyanglz/KAN .},
  archive      = {J_TNNLS},
  author       = {Liyang Zhang and Shuaicheng Liu and Donghao Liu and Pengpeng Zeng and Xiangpeng Li and Jingkuan Song and Lianli Gao},
  doi          = {10.1109/TNNLS.2020.3017530},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {4362-4373},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Rich visual knowledge-based augmentation network for visual question answering},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptive developmental resonance network. <em>TNNLS</em>,
<em>32</em>(10), 4347–4361. (<a
href="https://doi.org/10.1109/TNNLS.2020.3017490">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adaptive resonance theory (ART) networks, including developmental resonance network (DRN), basically use a vigilance parameter as a hyperparameter to determine whether a current input can belong to any existing categories or not. The problem here is that the clustering quality of those networks is sensitive to the vigilance parameter so that the users are required to fine-tune the parameter delicately beforehand. Another problem is that those networks only deal with a hyperrectangular decision boundary, which means they cannot learn categories of arbitrary shape. In addition, the order of data processing is a critical factor to categorize clusters correctly because each category can expand its boundary into the areas of other categories erroneously. To deal with these problems, we propose an advanced version of DRN, Adaptive DRN (A-DRN), which learns the vigilance parameters assigned for individual category nodes as well as category weights. The proposed A-DRN combines close categories to construct a cluster that contains the categories identifying a cluster boundary of arbitrary shape. Our A-DRN also employs a sliding window. The sliding window buffers sequential data points to presume the data distribution roughly, which helps our network to have a robust and consistent performance to a random order of input data. Through the experiments, we empirically demonstrate the effectiveness of A-DRN in both synthetic and real-world benchmark data sets.},
  archive      = {J_TNNLS},
  author       = {Gyeong-Moon Park and Jong-Hwan Kim},
  doi          = {10.1109/TNNLS.2020.3017490},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {4347-4361},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive developmental resonance network},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Off-policy reinforcement learning for tracking in
continuous-time systems on two time scales. <em>TNNLS</em>,
<em>32</em>(10), 4334–4346. (<a
href="https://doi.org/10.1109/TNNLS.2020.3017461">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article applies a singular perturbation theory to solve an optimal linear quadratic tracker problem for a continuous-time two-time-scale process. Previously, singular perturbation was applied for system regulation. It is shown that the two-time-scale tracking problem can be separated into a linear–quadratic tracker (LQT) problem for the slow system and a linear–quadratic regulator (LQR) problem for the fast system. We prove that the solutions to these two reduced-order control problems can approximate the LQT solution of the original control problem. The reduced-order slow LQT and fast LQR control problems are solved by off-policy integral reinforcement learning (IRL) using only measured data from the system. To test the effectiveness of the proposed method, we use an industrial thickening process as a simulation example and compare our method to a method with the known system model and a method without time-scale separation.},
  archive      = {J_TNNLS},
  author       = {Wenqian Xue and Jialu Fan and Victor G. Lopez and Yi Jiang and Tianyou Chai and Frank L. Lewis},
  doi          = {10.1109/TNNLS.2020.3017461},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {4334-4346},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Off-policy reinforcement learning for tracking in continuous-time systems on two time scales},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Distributed min–max learning scheme for neural networks with
applications to high-dimensional classification. <em>TNNLS</em>,
<em>32</em>(10), 4323–4333. (<a
href="https://doi.org/10.1109/TNNLS.2020.3017434">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, a novel learning methodology is introduced for the problem of classification in the context of high-dimensional data. In particular, the challenges introduced by high-dimensional data sets are addressed by formulating a $L_{1}$ regularized zero-sum game where optimal sparsity is estimated through a two-player game between the penalty coefficients/sparsity parameters and the deep neural network weights. In order to solve this game, a distributed learning methodology is proposed where additional variables are utilized to derive layerwise cost functions. Finally, an alternating minimization approach developed to solve the problem where the Nash solution provides optimal sparsity and compensation through the classifier. The proposed learning approach is implemented in a parallel and distributed environment through a novel computational algorithm. The efficiency of the approach is demonstrated both theoretically and empirically with nine data sets.},
  archive      = {J_TNNLS},
  author       = {Krishnan Raghavan and Shweta Garg and Sarangapani Jagannathan and V. A. Samaranayake},
  doi          = {10.1109/TNNLS.2020.3017434},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {4323-4333},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Distributed Min–Max learning scheme for neural networks with applications to high-dimensional classification},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Open set domain adaptation: Theoretical bound and algorithm.
<em>TNNLS</em>, <em>32</em>(10), 4309–4322. (<a
href="https://doi.org/10.1109/TNNLS.2020.3017213">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The aim of unsupervised domain adaptation is to leverage the knowledge in a labeled (source) domain to improve a model’s learning performance with an unlabeled (target) domain—the basic strategy being to mitigate the effects of discrepancies between the two distributions. Most existing algorithms can only handle unsupervised closed set domain adaptation (UCSDA), i.e., where the source and target domains are assumed to share the same label set. In this article, we target a more challenging but realistic setting: unsupervised open set domain adaptation (UOSDA), where the target domain has unknown classes that are not found in the source domain. This is the first study to provide learning bound for open set domain adaptation, which we do by theoretically investigating the risk of the target classifier on unknown classes. The proposed learning bound has a special term, namely, open set difference, which reflects the risk of the target classifier on unknown classes. Furthermore, we present a novel and theoretically guided unsupervised algorithm for open set domain adaptation, called distribution alignment with open difference (DAOD), which is based on regularizing this open set difference bound. The experiments on several benchmark data sets show the superior performance of the proposed UOSDA method compared with the state-of-the-art methods in the literature.},
  archive      = {J_TNNLS},
  author       = {Zhen Fang and Jie Lu and Feng Liu and Junyu Xuan and Guangquan Zhang},
  doi          = {10.1109/TNNLS.2020.3017213},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {4309-4322},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Open set domain adaptation: Theoretical bound and algorithm},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Attention in natural language processing. <em>TNNLS</em>,
<em>32</em>(10), 4291–4308. (<a
href="https://doi.org/10.1109/TNNLS.2020.3019893">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attention is an increasingly popular mechanism used in a wide range of neural architectures. The mechanism itself has been realized in a variety of formats. However, because of the fast-paced advances in this domain, a systematic overview of attention is still missing. In this article, we define a unified model for attention architectures in natural language processing, with a focus on those designed to work with vector representations of the textual data. We propose a taxonomy of attention models according to four dimensions: the representation of the input, the compatibility function, the distribution function, and the multiplicity of the input and/or output. We present the examples of how prior information can be exploited in attention models and discuss ongoing research efforts and open challenges in the area, providing the first extensive categorization of the vast body of literature in this exciting domain.},
  archive      = {J_TNNLS},
  author       = {Andrea Galassi and Marco Lippi and Paolo Torroni},
  doi          = {10.1109/TNNLS.2020.3019893},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {4291-4308},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Attention in natural language processing},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). PM₂.₅ monitoring: Use information abundance measurement and
wide and deep learning. <em>TNNLS</em>, <em>32</em>(10), 4278–4290. (<a
href="https://doi.org/10.1109/TNNLS.2021.3105394">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article devises a photograph-based monitoring model to estimate the real-time PM 2.5 concentrations, overcoming currently popular electrochemical sensor-based PM 2.5 monitoring methods’ shortcomings such as low-density spatial distribution and time delay. Combining the proposed monitoring model, the photographs taken by various camera devices (e.g., surveillance camera, automobile data recorder, and mobile phone) can widely monitor PM 2.5 concentration in megacities. This is beneficial to offering helpful decision-making information for atmospheric forecast and control, thus reducing the epidemic of COVID-19. To specify, the proposed model fuses Information Abundance measurement and Wide and Deep learning, dubbed as IAWD, for PM 2.5 monitoring. First, our model extracts two categories of features in a newly proposed DS transform space to measure the information abundance (IA) of a given photograph since the growth of PM 2.5 concentration decreases its IA. Second, to simultaneously possess the advantages of memorization and generalization, a new wide and deep neural network is devised to learn a nonlinear mapping between the above-mentioned extracted features and the groundtruth PM 2.5 concentration. Experiments on two recently established datasets totally including more than 100 000 photographs demonstrate the effectiveness of our extracted features and the superiority of our proposed IAWD model as compared to state-of-the-art relevant computing techniques.},
  archive      = {J_TNNLS},
  author       = {Ke Gu and Hongyan Liu and Zhifang Xia and Junfei Qiao and Weisi Lin and Daniel Thalmann},
  doi          = {10.1109/TNNLS.2021.3105394},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {4278-4290},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {PM₂.₅ monitoring: Use information abundance measurement and wide and deep learning},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021d). IEEE computational intelligence society information.
<em>TNNLS</em>, <em>32</em>(9), C3. (<a
href="https://doi.org/10.1109/TNNLS.2021.3106076">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Provides a listing of current committee members and society officers.},
  archive      = {J_TNNLS},
  doi          = {10.1109/TNNLS.2021.3106076},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {C3},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {IEEE computational intelligence society information},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptive dropout method based on biological principles.
<em>TNNLS</em>, <em>32</em>(9), 4267–4276. (<a
href="https://doi.org/10.1109/TNNLS.2021.3070895">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dropout is one of the most widely used methods to avoid overfitting neural networks. However, it rigidly and randomly activates neurons according to a fixed probability, which is not consistent with the activation mode of neurons in the human cerebral cortex. Inspired by gene theory and the activation mechanism of brain neurons, we propose a more intelligent adaptive dropout, in which a variational self-encoder (VAE) overlaps to an existing neural network to regularize its hidden neurons by adaptively setting activities to zero. Through alternating iterative training, the discarding probability of each hidden neuron can be learned according to the weights and thus effectively avoid the shortcomings of the standard dropout method. The experimental results in multiple data sets illustrate that this method can better suppress overfitting in various neural networks than can the standard dropout. Additionally, this adaptive dropout technique can reduce the number of neurons and improve training efficiency.},
  archive      = {J_TNNLS},
  author       = {Hailiang Li and Jian Weng and Yijun Mao and Yonghua Wang and Yiju Zhan and Qingling Cai and Wanrong Gu},
  doi          = {10.1109/TNNLS.2021.3070895},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {4267-4276},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive dropout method based on biological principles},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Online anomaly detection with bandwidth optimized
hierarchical kernel density estimators. <em>TNNLS</em>, <em>32</em>(9),
4253–4266. (<a
href="https://doi.org/10.1109/TNNLS.2020.3017675">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel unsupervised anomaly detection algorithm that can work for sequential data from any complex distribution in a truly online framework with mathematically proven strong performance guarantees. First, a partitioning tree is constructed to generate a doubly exponentially large hierarchical class of observation space partitions, and every partition region trains an online kernel density estimator (KDE) with its own unique dynamical bandwidth. At each time, the proposed algorithm optimally combines the class estimators to sequentially produce the final density estimation. We mathematically prove that the proposed algorithm learns the optimal partition with kernel bandwidths that are optimized in both region-specific and time-varying manner. The estimated density is then compared with a data-adaptive threshold to detect anomalies. Overall, the computational complexity is only linear in both the tree depth and data length. In our experiments, we observe significant improvements in anomaly detection accuracy compared with the state-of-the-art techniques.},
  archive      = {J_TNNLS},
  author       = {Mine Kerpicci and Huseyin Ozkan and Suleyman Serdar Kozat},
  doi          = {10.1109/TNNLS.2020.3017675},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {4253-4266},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Online anomaly detection with bandwidth optimized hierarchical kernel density estimators},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Continual learning using bayesian neural networks.
<em>TNNLS</em>, <em>32</em>(9), 4243–4252. (<a
href="https://doi.org/10.1109/TNNLS.2020.3017292">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Continual learning models allow them to learn and adapt to new changes and tasks over time. However, in continual and sequential learning scenarios, in which the models are trained using different data with various distributions, neural networks (NNs) tend to forget the previously learned knowledge. This phenomenon is often referred to as catastrophic forgetting. The catastrophic forgetting is an inevitable problem in continual learning models for dynamic environments. To address this issue, we propose a method, called continual Bayesian learning networks (CBLNs), which enables the networks to allocate additional resources to adapt to new tasks without forgetting the previously learned tasks. Using a Bayesian NN, CBLN maintains a mixture of Gaussian posterior distributions that are associated with different tasks. The proposed method tries to optimize the number of resources that are needed to learn each task and avoids an exponential increase in the number of resources that are involved in learning multiple tasks. The proposed method does not need to access the past training data and can choose suitable weights to classify the data points during the test time automatically based on an uncertainty criterion. We have evaluated the method on the MNIST and UCR time-series data sets. The evaluation results show that the method can address the catastrophic forgetting problem at a promising rate compared to the state-of-the-art models.},
  archive      = {J_TNNLS},
  author       = {Honglin Li and Payam Barnaghi and Shirin Enshaeifar and Frieder Ganz},
  doi          = {10.1109/TNNLS.2020.3017292},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {4243-4252},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Continual learning using bayesian neural networks},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Neural control of robot manipulators with trajectory
tracking constraints and input saturation. <em>TNNLS</em>,
<em>32</em>(9), 4231–4242. (<a
href="https://doi.org/10.1109/TNNLS.2020.3017202">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a control scheme for the robot manipulator&#39;s trajectory tracking task considering output error constraints and control input saturation. We provide an alternative way to remove the feasibility condition that most BLF-based controllers should meet and design a control scheme on the premise that constraint violation possibly happens due to the control input saturation. A bounded barrier Lyapunov function is proposed and adopted to handle the output error constraints. Besides, to suppress the input saturation effect, an auxiliary system is designed and emerged into the control scheme. Moreover, a simplified RBFNN structure is adopted to approximate the lumped uncertainties. Simulation and experimental results demonstrate the effectiveness of the proposed control scheme.},
  archive      = {J_TNNLS},
  author       = {Chenguang Yang and Dianye Huang and Wei He and Long Cheng},
  doi          = {10.1109/TNNLS.2020.3017202},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {4231-4242},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Neural control of robot manipulators with trajectory tracking constraints and input saturation},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Encoder–decoder full residual deep networks for robust
regression and spatiotemporal estimation. <em>TNNLS</em>,
<em>32</em>(9), 4217–4230. (<a
href="https://doi.org/10.1109/TNNLS.2020.3017200">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although increasing hidden layers can improve the ability of a neural network in modeling complex nonlinear relationships, deep layers may result in degradation of accuracy due to the problem of vanishing gradient. Accuracy degradation limits the applications of deep neural networks to predict continuous variables with a small sample size and/or weak or little invariance to translations. Inspired by residual convolutional neural network in computer vision, we developed an encoder–decoder full residual deep network to robustly regress and predict complex spatiotemporal variables. We embedded full shortcuts from each encoding layer to its corresponding decoding layer in a systematic encoder–decoder architecture for efficient residual mapping and error signal propagation. We demonstrated, theoretically and experimentally, that the proposed network structure with full residual connections can successfully boost the backpropagation of signals and improve learning outcomes. This novel method has been extensively evaluated and compared with four commonly used methods (i.e., plain neural network, cascaded residual autoencoder, generalized additive model, and XGBoost) across different testing cases for continuous variable predictions. For model evaluation, we focused on spatiotemporal imputation of satellite aerosol optical depth with massive nonrandomness missingness and spatiotemporal estimation of atmospheric fine particulate matter $\leq 2.5~\mu \text{m}$ (PM 2.5 ). Compared with the other approaches, our method achieved the state-of-the-art accuracy, had less bias in predicting extreme values, and generated more realistic spatial surfaces. This encoder–decoder full residual deep network can be an efficient and powerful tool in a variety of applications that involve complex nonlinear relationships of continuous variables, varying sample sizes, and spatiotemporal data with weak or little invariance to translation.},
  archive      = {J_TNNLS},
  author       = {Lianfa Li and Ying Fang and Jun Wu and Jinfeng Wang and Yong Ge},
  doi          = {10.1109/TNNLS.2020.3017200},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {4217-4230},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Encoder–Decoder full residual deep networks for robust regression and spatiotemporal estimation},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dual-rate adaptive optimal tracking control for dense medium
separation process using neural networks. <em>TNNLS</em>,
<em>32</em>(9), 4202–4216. (<a
href="https://doi.org/10.1109/TNNLS.2020.3017184">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dense medium separation (DMS) is of great significance for coal cleaning. The DMS control system always involves dense medium density adjustment and ash content control that are operating on fast and slow time scales, respectively. The inherent time-varying and strongly nonlinear characteristics of the DMS process give rise to challenges for the design of this multitime scale control system. To address this issue, this article proposes a dual-rate adaptive optimal tracking control approach for the DMS system. For the basic loop process, a nonlinear adaptive PI controller containing a neural network (NN)-based unmodeled dynamics compensator is proposed. Then, a lifting technique is used to unify the time scales of the two loops accompanied by formulating a generalized controlled object, whose dynamics is completely unknown. On this basis, a data-driven operation optimization control method that combines adaptive dynamic programming algorithm and reference control is developed, which is implemented using NNs. Finally, the stability of the proposed method is analyzed. The simulation results indicate its effectiveness.},
  archive      = {J_TNNLS},
  author       = {Wei Dai and Lingzhi Zhang and Jun Fu and Tianyou Chai and Xiaoping Ma},
  doi          = {10.1109/TNNLS.2020.3017184},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {4202-4216},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Dual-rate adaptive optimal tracking control for dense medium separation process using neural networks},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Synchronization of switched discrete-time neural networks
via quantized output control with actuator fault. <em>TNNLS</em>,
<em>32</em>(9), 4191–4201. (<a
href="https://doi.org/10.1109/TNNLS.2020.3017171">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article considers global exponential synchronization almost surely (GES a.s.) for a class of switched discrete-time neural networks (DTNNs). The considered system switches from one mode to another according to transition probability (TP) and evolves with mode-dependent average dwell time (MDADT), i.e., TP-based MDADT switching, which is more practical than classical average dwell time (ADT) switching. The logarithmic quantization technique is utilized to design mode-dependent quantized output controllers (QOCs). Noticing that external perturbations are unavoidable, actuator fault (AF) is also considered. New Lyapunov-Krasovskii functionals and analytical techniques are developed to obtain sufficient conditions to guarantee the GES a.s. It is discovered that the TP matrix plays an important role in achieving the GES a.s., the upper bound of the dwell time (DT) of unsynchronized subsystems can be very large, and the lower bound of the DT of synchronized subsystems can be very small. An algorithm is given to design the control gains, and an optimal algorithm is provided for reducing conservatism of the given results. Numerical examples demonstrate the effectiveness and the merits of the theoretical analysis.},
  archive      = {J_TNNLS},
  author       = {Xinsong Yang and Xiaoxiao Wan and Cheng Zunshui and Jinde Cao and Yang Liu and Leszek Rutkowski},
  doi          = {10.1109/TNNLS.2020.3017171},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {4191-4201},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Synchronization of switched discrete-time neural networks via quantized output control with actuator fault},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Neural-network-based iterative learning control for multiple
tasks. <em>TNNLS</em>, <em>32</em>(9), 4178–4190. (<a
href="https://doi.org/10.1109/TNNLS.2020.3017158">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Iterative learning control (ILC) can synthesize the feedforward control signal for the trajectory tracking control of a repetitive task, even when the system has strong nonlinear dynamics. This makes ILC be one of the most popular methods for trajectory tracking control. Restriction on a repetitive task, however, limits its application to multiple trajectories. This article proposes a neural-network-based ILC (NN-ILC) to deal with nonrepetitive tasks very effectively. A position-based ILC is designed to compensate the tracking error, based on which the multiple outputs of the ILC (ILC outputs) for multiple tasks are expressed as a function of the reference position, velocity, and acceleration. The proposed NN-ILC divides the ILC outputs of multiple tasks into two parts: the linear and nonlinear portions. The first part is expressed by a linear function, which is the linear portion of the function of the ILC outputs. The second part is expressed by a nonlinear function, which is estimated by complementary neural networks including a general neural network and a switching neural network. Finally, the two parts are combined and the ILC outputs of multiple tasks are expressed as a neural-network-based function. Two advantages of the proposed NN-ILC are emphasized. First, the ILC outputs of multiple tasks are compressed into a function by the proposed method, and thus, the memories can be saved. Second, in terms of generalizability, the neural-network-based function of the ILC outputs can easily predict position compensation for multiple tasks without extra iterative learning processes. Experimental results on a robot arm show that the proposed NN-ILC method can easily realize the ILC of multiple tasks. It can save memory comparing with the method of storing the data of multiple tasks and can predict the ILC output of any task, which can accelerate the iterative learning process.},
  archive      = {J_TNNLS},
  author       = {Dailin Zhang and Zining Wang and Tomizuka Masayoshi},
  doi          = {10.1109/TNNLS.2020.3017158},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {4178-4190},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Neural-network-based iterative learning control for multiple tasks},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Integration of neural network-based symbolic regression in
deep learning for scientific discovery. <em>TNNLS</em>, <em>32</em>(9),
4166–4177. (<a
href="https://doi.org/10.1109/TNNLS.2020.3017010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Symbolic regression is a powerful technique to discover analytic equations that describe data, which can lead to explainable models and the ability to predict unseen data. In contrast, neural networks have achieved amazing levels of accuracy on image recognition and natural language processing tasks, but they are often seen as black-box models that are difficult to interpret and typically extrapolate poorly. In this article, we use a neural network-based architecture for symbolic regression called the equation learner (EQL) network and integrate it with other deep learning architectures such that the whole system can be trained end-to-end through backpropagation. To demonstrate the power of such systems, we study their performance on several substantially different tasks. First, we show that the neural network can perform symbolic regression and learn the form of several functions. Next, we present an MNIST arithmetic task where a convolutional network extracts the digits. Finally, we demonstrate the prediction of dynamical systems where an unknown parameter is extracted through an encoder. We find that the EQL-based architecture can extrapolate quite well outside of the training data set compared with a standard neural network-based architecture, paving the way for deep learning to be applied in scientific exploration and discovery.},
  archive      = {J_TNNLS},
  author       = {Samuel Kim and Peter Y. Lu and Srijon Mukherjee and Michael Gilbert and Li Jing and Vladimir Čeperić and Marin Soljačić},
  doi          = {10.1109/TNNLS.2020.3017010},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {4166-4177},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Integration of neural network-based symbolic regression in deep learning for scientific discovery},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Meta-USR: A unified super-resolution network for multiple
degradation parameters. <em>TNNLS</em>, <em>32</em>(9), 4151–4165. (<a
href="https://doi.org/10.1109/TNNLS.2020.3016974">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent research on single image super-resolution (SISR) has achieved great success due to the development of deep convolutional neural networks. However, most existing SISR methods merely focus on super-resolution of a single fixed integer scale factor. This simplified assumption does not meet the complex conditions for real-world images which often suffer from various blur kernels or various levels of noise. More importantly, previous methods lack the ability to cope with arbitrary degradation parameters (scale factors, blur kernels, and noise levels) with a single model. A few methods can handle multiple degradation factors, e.g., noninteger scale factors, blurring, and noise, simultaneously within a single SISR model. In this work, we propose a simple yet powerful method termed meta-USR which is the first unified super-resolution network for arbitrary degradation parameters with meta-learning. In Meta-USR, a meta-restoration module (MRM) is proposed to enhance the traditional upscale module with the capability to adaptively predict the weights of the convolution filters for various combinations of degradation parameters. Thus, the MRM can not only upscale the feature maps with arbitrary scale factors but also restore the SR image with different blur kernels and noise levels. Moreover, the lightweight MRM can be placed at the end of the network, which makes it very efficient for iteratively/repeatedly searching the various degradation factors. We evaluate the proposed method through extensive experiments on several widely used benchmark data sets on SISR. The qualitative and quantitative experimental results show the superiority of our Meta-USR.},
  archive      = {J_TNNLS},
  author       = {Xuecai Hu and Zhang Zhang and Caifeng Shan and Zilei Wang and Liang Wang and Tieniu Tan},
  doi          = {10.1109/TNNLS.2020.3016974},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {4151-4165},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Meta-USR: A unified super-resolution network for multiple degradation parameters},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Performance-guaranteed fault-tolerant control for uncertain
nonlinear systems via learning-based switching scheme. <em>TNNLS</em>,
<em>32</em>(9), 4138–4150. (<a
href="https://doi.org/10.1109/TNNLS.2020.3016954">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article is concerned with the challenge of guaranteeing output constraints for fault-tolerant control (FTC) of a class of unknown multi-input single-output (MISO) nonlinear systems in the presence of actuator faults. Most industrial systems are equipped with redundant actuators and a fault detection-isolation mechanism for accommodating unexpected actuator faults. To simplify the system design and reduce the risk of false alarm or missed detection brought by the detection unit, a learning-based switching function scheme is proposed to automatically activate different sets of actuators in a rotational manner without human intervention. By this means, no explicit fault detection mechanism is needed. An additional step has been made to guarantee that the system output remains in user-defined time-varying asymmetric output constraints all the time during the occurrence of failures by utilizing error transformation techniques. The stability of the transformed system can equivalently deliver the result that the original system output stays in the required bounds. Hence, system crash or further catastrophic outcomes can be avoided. A neural network is integrated to embody the adaptive FTC design for dealing with unknown system dynamics. The dynamic surface control (DSC) technique is also invoked to decrease complexity. Furthermore, the stability analysis is carried out by the standard Lyapunov approach to guarantee that all the signals of the closed-loop system are semiglobally uniformly ultimately bounded. Finally, the simulation results are provided to verify the effectiveness of the proposed scheme.},
  archive      = {J_TNNLS},
  author       = {Zhengwei Ruan and Qinmin Yang and Shuzhi Sam Ge and Youxian Sun},
  doi          = {10.1109/TNNLS.2020.3016954},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {4138-4150},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Performance-guaranteed fault-tolerant control for uncertain nonlinear systems via learning-based switching scheme},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Distributed controller design and analysis of second-order
signed networks with communication delays. <em>TNNLS</em>,
<em>32</em>(9), 4123–4137. (<a
href="https://doi.org/10.1109/TNNLS.2020.3016946">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article concentrates on dealing with distributed control problems for second-order signed networks subject to not only cooperative but also antagonistic interactions. A distributed control protocol is proposed based on the nearest neighbor rules, with which necessary and sufficient conditions are developed for consensus of second-order signed networks whose communication topologies are described by strongly connected signed digraphs. Besides, another distributed control protocol in the presence of a communication delay is designed, for which a time margin of the delay can be determined simultaneously. It is shown that under the delay margin condition, necessary and sufficient consensus results can be derived even though second-order signed networks with a communication delay are considered. Simulation examples are included to illustrate the validity of our established consensus results of second-order signed networks.},
  archive      = {J_TNNLS},
  author       = {Mingjun Du and Deyuan Meng and Zheng-Guang Wu},
  doi          = {10.1109/TNNLS.2020.3016946},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {4123-4137},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Distributed controller design and analysis of second-order signed networks with communication delays},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Efficient active learning by querying discriminative and
representative samples and fully exploiting unlabeled data.
<em>TNNLS</em>, <em>32</em>(9), 4111–4122. (<a
href="https://doi.org/10.1109/TNNLS.2020.3016928">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Active learning is an important learning paradigm in machine learning and data mining, which aims to train effective classifiers with as few labeled samples as possible. Querying discriminative (informative) and representative samples are the state-of-the-art approach for active learning. Fully utilizing a large amount of unlabeled data provides a second chance to improve the performance of active learning. Although there have been several active learning methods proposed by combining with semisupervised learning, fast active learning with fully exploiting unlabeled data and querying discriminative and representative samples is still an open question. To overcome this challenging issue, in this article, we propose a new efficient batch mode active learning algorithm. Specifically, we first provide an active learning risk bound by fully considering the unlabeled samples in characterizing the informativeness and representativeness. Based on the risk bound, we derive a new objective function for batch mode active learning. After that, we propose a wrapper algorithm to solve the objective function, which essentially trains a semisupervised classifier and selects discriminative and representative samples alternately. Especially, to avoid retraining the semisupervised classifier from scratch after each query, we design two unique procedures based on the path-following technique, which can remove multiple queried samples from the unlabeled data set and add the queried samples into the labeled data set efficiently. Extensive experimental results on a variety of benchmark data sets not only show that our algorithm has a better generalization performance than the state-of-the-art active learning approaches but also show its significant efficiency.},
  archive      = {J_TNNLS},
  author       = {Bin Gu and Zhou Zhai and Cheng Deng and Heng Huang},
  doi          = {10.1109/TNNLS.2020.3016928},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {4111-4122},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Efficient active learning by querying discriminative and representative samples and fully exploiting unlabeled data},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A hybrid learning method for system identification and
optimal control. <em>TNNLS</em>, <em>32</em>(9), 4096–4110. (<a
href="https://doi.org/10.1109/TNNLS.2020.3016906">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a three-step method to perform system identification and optimal control of nonlinear systems. Our approach is mainly data-driven and does not require active excitation of the system to perform system identification. In particular, it is designed for systems for which only historical data under closed-loop control are available and where historical control commands exhibit low variability. In the first step, simple simulation models of the system are built and run under various conditions. In the second step, a neural network architecture is extensively trained on the simulation outputs to learn the system physics and retrained with historical data from the real system with stopping rules. These constraints avoid overfitting that arises by fitting closed-loop controlled systems. By doing so, we obtain one (or many) system model(s), represented by this architecture, whose behavior can be chosen to match more or less the real system. Finally, state-of-the-art reinforcement learning with a variant of domain randomization and distributed learning is used for optimal control of the system. We first illustrate the model identification strategy with a simple example, the pendulum with external torque. We then apply our method to model and optimize the control of a large building facility located in Switzerland. Simulation results demonstrate that this approach generates stable functional controllers that outperform on comfort and energy benchmark rule-based controllers.},
  archive      = {J_TNNLS},
  author       = {Baptiste Schubnel and Rafael E. Carrillo and Pierre-Jean Alet and Andreas Hutter},
  doi          = {10.1109/TNNLS.2020.3016906},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {4096-4110},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A hybrid learning method for system identification and optimal control},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). All-passive hardware implementation of multilayer perceptron
classifiers. <em>TNNLS</em>, <em>32</em>(9), 4086–4095. (<a
href="https://doi.org/10.1109/TNNLS.2020.3016901">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bottom-up-fabricated crossbars promise superior circuit density and 3-D integrability compared with the traditional CMOS-based implementations. However, their inherent stochasticity presents difficulties in building complex circuits from components that demand precise patterning and high registration accuracies. With fewer terminals than active devices, passive components offer higher device densities and registration tolerances, making them amenable to bottom-up synthesized nanocrossbars. Motivated by this preference for passivity, we explore, in this article, neuromorphic classifiers based on passive neurons and passive synapses. We demonstrate via SPICE simulations how a shallow network of the diode-resistor-based passive rectifier neurons and resistive voltage summers, despite its inherent inability to buffer, amplify, and negate signals, can recognize MNIST digits with 95.4\% accuracy. We introduce weight-to-conductance mappings that enable negative weights to be implemented in hardware without excessive memory overheads. The influences of soft and hard defects on the classification performance are evaluated, and the methods to boost fault-tolerance are proposed. The first-order evaluation of the area, speed, and power consumption of the passive multilayer perceptron classifiers is undertaken, and the results are compared with a benchmark study in neuromorphic hardware.},
  archive      = {J_TNNLS},
  author       = {Akshay Ananthakrishnan and Mark G. Allen},
  doi          = {10.1109/TNNLS.2020.3016901},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {4086-4095},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {All-passive hardware implementation of multilayer perceptron classifiers},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Parameter-efficient deep neural networks with bilinear
projections. <em>TNNLS</em>, <em>32</em>(9), 4075–4085. (<a
href="https://doi.org/10.1109/TNNLS.2020.3016688">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent research on deep neural networks (DNNs) has primarily focused on improving the model accuracy. Given a proper deep learning framework, it is generally possible to increase the depth or layer width to achieve a higher level of accuracy. However, the huge number of model parameters imposes more computational and memory usage overhead and leads to the parameter redundancy. In this article, we address the parameter redundancy problem in DNNs by replacing conventional full projections with bilinear projections (BPs). For a fully connected layer with $D$ input nodes and $D$ output nodes, applying BP can reduce the model space complexity from $\mathcal {O}(D^{2})$ to $\mathcal {O}(2D)$ , achieving a deep model with a sublinear layer size. However, the structured projection has a lower freedom of degree compared with the full projection, causing the underfitting problem. Therefore, we simply scale up the mapping size by increasing the number of output channels, which can keep and even boosts the model accuracy. This makes it very parameter-efficient and handy to deploy such deep models on mobile systems with memory limitations. Experiments on four benchmark data sets show that applying the proposed BP to DNNs can achieve even higher accuracies than conventional full DNNs while significantly reducing the model size.},
  archive      = {J_TNNLS},
  author       = {Litao Yu and Yongsheng Gao and Jun Zhou and Jian Zhang},
  doi          = {10.1109/TNNLS.2020.3016688},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {4075-4085},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Parameter-efficient deep neural networks with bilinear projections},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Exponential stability of fractional-order complex
multi-links networks with aperiodically intermittent control.
<em>TNNLS</em>, <em>32</em>(9), 4063–4074. (<a
href="https://doi.org/10.1109/TNNLS.2020.3016672">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, the exponential stability problem for fractional-order complex multi-links networks with aperiodically intermittent control is considered. Using the graph theory and Lyapunov method, two theorems, including a Lyapunov-type theorem and a coefficient-type theorem, are given to ensure the exponential stability of the underlying networks. The theoretical results show that the exponential convergence rate is dependent on the control gain and the order of fractional derivative. To be specific, the larger control gain, the higher the exponential convergence rate. Meanwhile, when aperiodically intermittent control degenerates into periodically intermittent control, a corollary is also provided to ensure the exponential stability of the underlying networks. Furthermore, to show the practicality of theoretical results, as an application, the exponential stability of fractional-order multi-links competitive neural networks with aperiodically intermittent control is investigated and a stability criterion is established. Finally, the effectiveness and feasibility of the theoretical results are demonstrated through a numerical example.},
  archive      = {J_TNNLS},
  author       = {Yao Xu and Shang Gao and Wenxue Li},
  doi          = {10.1109/TNNLS.2020.3016672},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {4063-4074},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Exponential stability of fractional-order complex multi-links networks with aperiodically intermittent control},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Uncertainty-gated stochastic sequential model for EHR
mortality prediction. <em>TNNLS</em>, <em>32</em>(9), 4052–4062. (<a
href="https://doi.org/10.1109/TNNLS.2020.3016670">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electronic health records (EHRs) are characterized as nonstationary, heterogeneous, noisy, and sparse data; therefore, it is challenging to learn the regularities or patterns inherent within them. In particular, sparseness caused mostly by many missing values has attracted the attention of researchers who have attempted to find a better use of all available samples for determining the solution of a primary target task through defining a secondary imputation problem. Methodologically, existing methods, either deterministic or stochastic, have applied different assumptions to impute missing values. However, once the missing values are imputed, most existing methods do not consider the fidelity or confidence of the imputed values in the modeling of downstream tasks. Undoubtedly, an erroneous or improper imputation of missing variables can cause difficulties in the modeling as well as a degraded performance. In this study, we present a novel variational recurrent network that: 1) estimates the distribution of missing variables (e.g., the mean and variance) allowing to represent uncertainty in the imputed values; 2) updates hidden states by explicitly applying fidelity based on a variance of the imputed values during a recurrence (i.e., uncertainty propagation over time); and 3) predicts the possibility of in-hospital mortality. It is noteworthy that our model can conduct these procedures in a single stream and learn all network parameters jointly in an end-to-end manner. We validated the effectiveness of our method using the public data sets of MIMIC-III and PhysioNet challenge 2012 by comparing with and outperforming other state-of-the-art methods for mortality prediction considered in our experiments. In addition, we identified the behavior of the model that well represented the uncertainties for the imputed estimates, which showed a high correlation between the uncertainties and mean absolute error (MAE) scores for imputation.},
  archive      = {J_TNNLS},
  author       = {Eunji Jun and Ahmad Wisnu Mulyadi and Jaehun Choi and Heung-Il Suk},
  doi          = {10.1109/TNNLS.2020.3016670},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {4052-4062},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Uncertainty-gated stochastic sequential model for EHR mortality prediction},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Generative adversarial networks-based data augmentation for
brain–computer interface. <em>TNNLS</em>, <em>32</em>(9), 4039–4051. (<a
href="https://doi.org/10.1109/TNNLS.2020.3016666">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The performance of a classifier in a brain-computer interface (BCI) system is highly dependent on the quality and quantity of training data. Typically, the training data are collected in a laboratory where the users perform tasks in a controlled environment. However, users&#39; attention may be diverted in real-life BCI applications and this may decrease the performance of the classifier. To improve the robustness of the classifier, additional data can be acquired in such conditions, but it is not practical to record electroencephalogram (EEG) data over several long calibration sessions. A potentially time- and cost-efficient solution is artificial data generation. Hence, in this study, we proposed a framework based on the deep convolutional generative adversarial networks (DCGANs) for generating artificial EEG to augment the training set in order to improve the performance of a BCI classifier. To make a comparative investigation, we designed a motor task experiment with diverted and focused attention conditions. We used an end-to-end deep convolutional neural network for classification between movement intention and rest using the data from 14 subjects. The results from the leave-one subject-out (LOO) classification yielded baseline accuracies of 73.04\% for diverted attention and 80.09\% for focused attention without data augmentation. Using the proposed DCGANs-based framework for augmentation, the results yielded a significant improvement of 7.32\% for diverted attention ( p &lt;; 0.01) and 5.45\% for focused attention ( p &lt;; 0.01). In addition, we implemented the method on the data set IVa from BCI competition III to distinguish different motor imagery tasks. The proposed method increased the accuracy by 3.57\% ( p &lt;; 0.02). This study shows that using GANs for EEG augmentation can significantly improve BCI performance, especially in real-life applications, whereby users&#39; attention may be diverted.},
  archive      = {J_TNNLS},
  author       = {Fatemeh Fahimi and Strahinja Dosen and Kai Keng Ang and Natalie Mrachacz-Kersting and Cuntai Guan},
  doi          = {10.1109/TNNLS.2020.3016666},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {4039-4051},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Generative adversarial networks-based data augmentation for Brain–Computer interface},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). CSLM: Convertible short-term and long-term memory in
differential neural computers. <em>TNNLS</em>, <em>32</em>(9),
4026–4038. (<a
href="https://doi.org/10.1109/TNNLS.2020.3016632">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {External memory-based neural networks, such as differentiable neural computers (DNCs), have recently gained importance and popularity to solve complex sequential learning tasks that pose challenges to conventional neural networks. However, a trained DNC usually has a low-memory utilization efficiency. This article introduces a variation of DNC architecture with a convertible short-term and long-term memory, named CSLM-DNC. Unlike the memory architecture of the original DNC, the new scheme of short-term and long-term memories offers different importance of memory locations for read and write, and they can be converted over time. This is mainly motivated by the human brain where short-term memory stores large amounts of noisy and unimportant information and decays rapidly, while long-term memory stores important information and lasts for a long time. The conversion of these two types of memory is allowed and is able to be learned according to their reading and writing frequency. We quantitatively and qualitatively evaluate the proposed CSLM-DNC architecture on the tasks of question answering, copy and repeat copy, showing that it can significantly improve memory efficiency and learning performance.},
  archive      = {J_TNNLS},
  author       = {Shiming Xiang and Bo Tang},
  doi          = {10.1109/TNNLS.2020.3016632},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {4026-4038},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {CSLM: Convertible short-term and long-term memory in differential neural computers},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Generic neural locomotion control framework for legged
robots. <em>TNNLS</em>, <em>32</em>(9), 4013–4025. (<a
href="https://doi.org/10.1109/TNNLS.2020.3016523">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we present a generic locomotion control framework for legged robots and a strategy for control policy optimization. The framework is based on neural control and black-box optimization. The neural control combines a central pattern generator (CPG) and a radial basis function (RBF) network to create a CPG-RBF network. The control network acts as a neural basis to produce arbitrary rhythmic trajectories for the joints of robots. The main features of the CPG-RBF network are: 1) it is generic since it can be applied to legged robots with different morphologies; 2) it has few control parameters, resulting in fast learning; 3) it is scalable, both in terms of policy/trajectory complexity and the number of legs that can be controlled using similar trajectories; 4) it does not rely heavily on sensory feedback to generate locomotion and is thus less prone to sensory faults; and 5) once trained, it is simple, minimal, and intuitive to use and analyze. These features will lead to an easy-to-use framework with fast convergence and the ability to encode complex locomotion control policies. In this work, we show that the framework can successfully be applied to three different simulated legged robots with varying morphologies and, even broken joints, to learn locomotion control policies. We also show that after learning, the control policies can also be successfully transferred to a real-world robot without any modifications. We, furthermore, show the scalability of the framework by implementing it as a central controller for all legs of a robot and as a decentralized controller for individual legs and leg pairs. By investigating the correlation between robot morphology and encoding type, we are able to present a strategy for control policy optimization. Finally, we show how sensory feedback can be integrated into the CPG-RBF network to enable online adaptation.},
  archive      = {J_TNNLS},
  author       = {Mathias Thor and Tomas Kulvicius and Poramate Manoonpong},
  doi          = {10.1109/TNNLS.2020.3016523},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {4013-4025},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Generic neural locomotion control framework for legged robots},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Producing stable periodic solutions of switched impulsive
delayed neural networks using a matrix-based cubic convex combination
approach. <em>TNNLS</em>, <em>32</em>(9), 3998–4012. (<a
href="https://doi.org/10.1109/TNNLS.2020.3016421">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article is dedicated to designing a novel periodic impulsive control strategy for producing globally exponentially stable periodic solutions for switched neural networks with discrete and finite distributed time-varying delays. First, tunable parameters and cubic convex combination approach are proposed to study the globally exponential convergence of switched neural networks. Second, a sufficient criterion for the existence, uniqueness, and globally exponential stability of a periodic solution is demonstrated by using contraction mapping theorem and the impulse-delay-dependent Lyapunov-Krasovskii functional method. It is worth emphasizing that the addressed Lyapunov-Krasovskii functional covers both triple integral terms and novel quadruple integral terms, which makes the conservatism of the above criteria decrease. Even if the original neural network models are unstable or the impulsive effects are strong, the addressed neural network model can produce a globally exponentially stable periodic solution. These results here, which include boundedness, globally uniformly exponential convergence, and globally exponentially stability of the periodic solution, generalize and improve the earlier publications. Finally, two numerical examples and their computer simulations are given to show the effectiveness of theoretical results.},
  archive      = {J_TNNLS},
  author       = {Peng Wan and Dihua Sun and Min Zhao},
  doi          = {10.1109/TNNLS.2020.3016421},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {3998-4012},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Producing stable periodic solutions of switched impulsive delayed neural networks using a matrix-based cubic convex combination approach},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Constrained clustering with dissimilarity propagation-guided
graph-laplacian PCA. <em>TNNLS</em>, <em>32</em>(9), 3985–3997. (<a
href="https://doi.org/10.1109/TNNLS.2020.3016397">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose a novel model for constrained clustering, namely, the dissimilarity propagation-guided graph-Laplacian principal component analysis (DP-GLPCA). By fully utilizing a limited number of weakly supervisory information in the form of pairwise constraints, the proposed DP-GLPCA is capable of capturing both the local and global structures of input samples to exploit their characteristics for excellent clustering. More specifically, we first formulate a convex semisupervised low-dimensional embedding model by incorporating a new dissimilarity regularizer into GLPCA (i.e., an unsupervised dimensionality reduction model), in which both the similarity and dissimilarity between low-dimensional representations are enforced with the constraints to improve their discriminability. An efficient iterative algorithm based on the inexact augmented Lagrange multiplier is designed to solve it with the global convergence guaranteed. Furthermore, we innovatively propose to propagate the cannot-link constraints (i.e., dissimilarity) to refine the dissimilarity regularizer to be more informative. The resulting DP model is iteratively solved, and we also prove that it can converge to a Karush-Kuhn-Tucker point. Extensive experimental results over nine commonly used benchmark data sets show that the proposed DP-GLPCA can produce much higher clustering accuracy than state-of-the-art constrained clustering methods. Besides, the effectiveness and advantage of the proposed DP model are experimentally verified. To the best of our knowledge, it is the first time to investigate DP, which is contrast to existing pairwise constraint propagation that propagates similarity. The code is publicly available at https://github.com/jyh-learning/DP-GLPCA.},
  archive      = {J_TNNLS},
  author       = {Yuheng Jia and Junhui Hou and Sam Kwong},
  doi          = {10.1109/TNNLS.2020.3016397},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {3985-3997},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Constrained clustering with dissimilarity propagation-guided graph-laplacian PCA},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Identification of autistic risk candidate genes and toxic
chemicals via multilabel learning. <em>TNNLS</em>, <em>32</em>(9),
3971–3984. (<a
href="https://doi.org/10.1109/TNNLS.2020.3016357">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a group of complex neurodevelopmental disorders, autism spectrum disorder (ASD) has been reported to have a high overall prevalence, showing an unprecedented spurt since 2000. Due to the unclear pathomechanism of ASD, it is challenging to diagnose individuals with ASD merely based on clinical observations. Without additional support of biochemical markers, the difficulty of diagnosis could impact therapeutic decisions and, therefore, lead to delayed treatments. Recently, accumulating evidence have shown that both genetic abnormalities and chemical toxicants play important roles in the onset of ASD. In this work, a new multilabel classification (MLC) model is proposed to identify the autistic risk genes and toxic chemicals on a large-scale data set. We first construct the feature matrices and partially labeled networks for autistic risk genes and toxic chemicals from multiple heterogeneous biological databases. Based on both global and local measure metrics, the simulation experiments demonstrate that the proposed model achieves superior classification performance in comparison with the other state-of-the-art MLC methods. Through manual validation with existing studies, 60\% and 50\% out of the top-20 predicted risk genes are confirmed to have associations with ASD and autistic disorder, respectively. To the best of our knowledge, this is the first computational tool to identify ASD-related risk genes and toxic chemicals, which could lead to better therapeutic decisions of ASD.},
  archive      = {J_TNNLS},
  author       = {Zhi-An Huang and Jia Zhang and Zexuan Zhu and Edmond Q. Wu and Kay Chen Tan},
  doi          = {10.1109/TNNLS.2020.3016357},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {3971-3984},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Identification of autistic risk candidate genes and toxic chemicals via multilabel learning},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multilevel edge features guided network for image denoising.
<em>TNNLS</em>, <em>32</em>(9), 3956–3970. (<a
href="https://doi.org/10.1109/TNNLS.2020.3016321">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image denoising is a challenging inverse problem due to complex scenes and information loss. Recently, various methods have been considered to solve this problem by building a well-designed convolutional neural network (CNN) or introducing some hand-designed image priors. Different from previous works, we investigate a new framework for image denoising, which integrates edge detection, edge guidance, and image denoising into an end-to-end CNN model. To achieve this goal, we propose a multilevel edge features guided network (MLEFGN). First, we build an edge reconstruction network (Edge-Net) to directly predict clear edges from the noisy image. Then, the Edge-Net is embedded as part of the model to provide edge priors, and a dual-path network is applied to extract the image and edge features, respectively. Finally, we introduce a multilevel edge features guidance mechanism for image denoising. To the best of our knowledge, the Edge-Net is the first CNN model specially designed to reconstruct image edges from the noisy image, which shows good accuracy and robustness on natural images. Extensive experiments clearly illustrate that our MLEFGN achieves favorable performance against other methods and plenty of ablation studies demonstrate the effectiveness of our proposed Edge-Net and MLEFGN. The code is available at https://github.com/MIVRC/MLEFGN-PyTorch.},
  archive      = {J_TNNLS},
  author       = {Faming Fang and Juncheng Li and Yiting Yuan and Tieyong Zeng and Guixu Zhang},
  doi          = {10.1109/TNNLS.2020.3016321},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {3956-3970},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multilevel edge features guided network for image denoising},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Self-supervised time series clustering with model-based
dynamics. <em>TNNLS</em>, <em>32</em>(9), 3942–3955. (<a
href="https://doi.org/10.1109/TNNLS.2020.3016291">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time series clustering is usually an essential unsupervised task in cases when category information is not available and has a wide range of applications. However, existing time series clustering methods usually either ignore temporal dynamics of time series or isolate the feature extraction from clustering tasks without considering the interaction between them. In this article, a time series clustering framework named self-supervised time series clustering network (STCN) is proposed to optimize the feature extraction and clustering simultaneously. In the feature extraction module, a recurrent neural network (RNN) conducts a one-step time series prediction that acts as the reconstruction of the input data, capturing the temporal dynamics and maintaining the local structures of the time series. The parameters of the output layer of the RNN are regarded as model-based dynamic features and then fed into a self-supervised clustering module to obtain the predicted labels. To bridge the gap between these two modules, we employ spectral analysis to constrain the similar features to have the same pseudoclass labels and align the predicted labels with pseudolabels as well. STCN is trained by iteratively updating the model parameters and the pseudoclass labels. Experiments conducted on extensive time series data sets show that STCN has state-of-the-art performance, and the visualization analysis also demonstrates the effectiveness of the proposed model.},
  archive      = {J_TNNLS},
  author       = {Qianli Ma and Sen Li and Wanqing Zhuang and Sen Li and Jiabing Wang and Delu Zeng},
  doi          = {10.1109/TNNLS.2020.3016291},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {3942-3955},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Self-supervised time series clustering with model-based dynamics},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SeGMA: Semi-supervised gaussian mixture autoencoder.
<em>TNNLS</em>, <em>32</em>(9), 3930–3941. (<a
href="https://doi.org/10.1109/TNNLS.2020.3016221">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a semi-supervised generative model, SeGMA, which learns a joint probability distribution of data and their classes and is implemented in a typical Wasserstein autoencoder framework. We choose a mixture of Gaussians as a target distribution in latent space, which provides a natural splitting of data into clusters. To connect Gaussian components with correct classes, we use a small amount of labeled data and a Gaussian classifier induced by the target distribution. SeGMA is optimized efficiently due to the use of the Cramer-Wold distance as a maximum mean discrepancy penalty, which yields a closed-form expression for a mixture of spherical Gaussian components and, thus, obviates the need of sampling. While SeGMA preserves all properties of its semi-supervised predecessors and achieves at least as good generative performance on standard benchmark data sets, it presents additional features: 1) interpolation between any pair of points in the latent space produces realistically looking samples; 2) combining the interpolation property with disentangling of class and style information, SeGMA is able to perform continuous style transfer from one class to another; and 3) it is possible to change the intensity of class characteristics in a data point by moving the latent representation of the data point away from specific Gaussian components.},
  archive      = {J_TNNLS},
  author       = {Marek Śmieja and Maciej Wołczyk and Jacek Tabor and Bernhard C. Geiger},
  doi          = {10.1109/TNNLS.2020.3016221},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {3930-3941},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {SeGMA: Semi-supervised gaussian mixture autoencoder},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Effective visual domain adaptation via generative
adversarial distribution matching. <em>TNNLS</em>, <em>32</em>(9),
3919–3929. (<a
href="https://doi.org/10.1109/TNNLS.2020.3016180">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of computer vision, without sufficient labeled images, it is challenging to train an accurate model. However, through visual adaptation from source to target domains, a relevant labeled dataset can help solve such problem. Many methods apply adversarial learning to diminish cross-domain distribution difference. They are able to greatly enhance the performance on target classification tasks. Generative adversarial network (GAN) loss is widely used in adversarial adaptation learning methods to reduce an across-domain distribution difference. However, it becomes difficult to decline such distribution difference if generator or discriminator in GAN fails to work as expected and degrades its performance. To solve such cross-domain classification problems, we put forward a novel adaptation framework called generative adversarial distribution matching (GADM). In GADM, we improve the objective function by taking cross-domain discrepancy distance into consideration and further minimize the difference through the competition between a generator and discriminator, thereby greatly decreasing cross-domain distribution difference. Experimental results and comparison with several state-of-the-art methods verify GADM&#39;s superiority in image classification across domains.},
  archive      = {J_TNNLS},
  author       = {Qi Kang and SiYa Yao and MengChu Zhou and Kai Zhang and Abdullah Abusorrah},
  doi          = {10.1109/TNNLS.2020.3016180},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {3919-3929},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Effective visual domain adaptation via generative adversarial distribution matching},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). H∞ state estimation for neural networks with general
activation function and mixed time-varying delays. <em>TNNLS</em>,
<em>32</em>(9), 3909–3918. (<a
href="https://doi.org/10.1109/TNNLS.2020.3016120">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article deals with H ∞ state estimation of neural networks with mixed delays. In order to make full use of delay information, novel delay-product Lyapunov-Krasovskii functional (LKF) by using parameterized delay interval is first constructed. Then, generalized free-weighting-matrix integral inequality is used to estimate the derivative of LKF to reduce the conservatism. Also, a more general activation function is further applied by combining with parameterized delay interval in order to obtain a more accurate estimator model. Finally, sufficient conditions are derived to confirm that the estimation error system is asymptotically stable with a prescribed H ∞ performance. Numerical examples are simulated to show the benefits of our proposed method.},
  archive      = {J_TNNLS},
  author       = {Wei Qian and Weiwei Xing and Shumin Fei},
  doi          = {10.1109/TNNLS.2020.3016120},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {3909-3918},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {H∞ state estimation for neural networks with general activation function and mixed time-varying delays},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adversarial learning with multi-modal attention for visual
question answering. <em>TNNLS</em>, <em>32</em>(9), 3894–3908. (<a
href="https://doi.org/10.1109/TNNLS.2020.3016083">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual question answering (VQA) has been proposed as a challenging task and attracted extensive research attention. It aims to learn a joint representation of the question–image pair for answer inference. Most of the existing methods focus on exploring the multi-modal correlation between the question and image to learn the joint representation. However, the answer-related information is not fully captured by these methods, which results that the learned representation is ineffective to reflect the answer of the question. To tackle this problem, we propose a novel model, i.e., adversarial learning with multi-modal attention (ALMA), for VQA. An adversarial learning-based framework is proposed to learn the joint representation to effectively reflect the answer-related information. Specifically, multi-modal attention with the Siamese similarity learning method is designed to build two embedding generators, i.e., question–image embedding and question–answer embedding. Then, adversarial learning is conducted as an interplay between the two embedding generators and an embedding discriminator. The generators have the purpose of generating two modality-invariant representations for the question–image and question–answer pairs, whereas the embedding discriminator aims to discriminate the two representations. Both the multi-modal attention module and the adversarial networks are integrated into an end-to-end unified framework to infer the answer. Experiments performed on three benchmark data sets confirm the favorable performance of ALMA compared with state-of-the-art approaches.},
  archive      = {J_TNNLS},
  author       = {Yun Liu and Xiaoming Zhang and Feiran Huang and Lei Cheng and Zhoujun Li},
  doi          = {10.1109/TNNLS.2020.3016083},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {3894-3908},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adversarial learning with multi-modal attention for visual question answering},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). General plane-based clustering with distribution loss.
<em>TNNLS</em>, <em>32</em>(9), 3880–3893. (<a
href="https://doi.org/10.1109/TNNLS.2020.3016078">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose a general model for plane-based clustering. The general model reveals the relationship between cluster assignment and cluster updating during clustering implementation, and it contains many existing plane-based clustering methods, e.g., k-plane clustering, proximal plane clustering, twin support vector clustering, and their extensions. Under this general model, one may obtain an appropriate clustering method for a specific purpose. The general model is a procedure corresponding to an optimization problem, which minimizes the total loss of the samples. Thereinto, the loss of a sample derives from both within-cluster and between-cluster information. We discuss the theoretical termination conditions and prove that the general model terminates in a finite number of steps at a local or weak local solution. Furthermore, we propose a distribution loss function that fluctuates with the input data and introduce it into the general model to obtain a plane-based clustering method (DPC). DPC can capture the data distribution precisely because of its statistical characteristics, and its termination that finitely terminates at a weak local solution is given immediately based on the general model. The experimental results show that our DPC outperforms the state-of-the-art plane-based clustering methods on many synthetic and benchmark data sets.},
  archive      = {J_TNNLS},
  author       = {Zhen Wang and Yuan-Hai Shao and Lan Bai and Chun-Na Li and Li-Ming Liu},
  doi          = {10.1109/TNNLS.2020.3016078},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {3880-3893},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {General plane-based clustering with distribution loss},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Convergence analysis of robust iterative learning control
against nonrepetitive uncertainties: System equivalence transformation.
<em>TNNLS</em>, <em>32</em>(9), 3867–3879. (<a
href="https://doi.org/10.1109/TNNLS.2020.3016057">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article is concerned with the robust convergence analysis of iterative learning control (ILC) against nonrepetitive uncertainties, where the contradiction between convergence conditions for the output tracking error and the input signal (or error) is addressed. A system equivalence transformation (SET) is proposed for robust ILC such that given any desired reference trajectories, the output tracking problems for general nonsquare multi-input, multi-output (MIMO) systems can be equivalently transformed into those for the specific class of square MIMO systems with the same input and output numbers. As a benefit of SET, a unified condition is only needed to guarantee both the uniform boundedness of all system signals and the robust convergence of the output tracking error, which avoids causing the condition contradiction problem in implementing the double-dynamics analysis approach to ILC. Simulation examples are included to demonstrate the validity of our established robust ILC results.},
  archive      = {J_TNNLS},
  author       = {Deyuan Meng and Jingyao Zhang},
  doi          = {10.1109/TNNLS.2020.3016057},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {3867-3879},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Convergence analysis of robust iterative learning control against nonrepetitive uncertainties: System equivalence transformation},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). New criteria on finite-time stability of fractional-order
hopfield neural networks with time delays. <em>TNNLS</em>,
<em>32</em>(9), 3858–3866. (<a
href="https://doi.org/10.1109/TNNLS.2020.3016038">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, the finite-time stability (FTS) of fractional-order Hopfield neural networks with time delays (FHNNTDs) is studied. A widely used inequality in investigating the stability of the fractional-order neural networks is fractional-order Gronwall inequality related to the Mittag-Leffler function, which cannot be directly used to study the stability of the factional-order neural networks with time delays. In the existing works related to fractional-order Gronwall inequality with time delays, the order λ &gt; 0 was divided into two cases: λ ∈ (0,0.5] and λ ∈ (0.5,+∞). In this article, a new fractional-order Gronwall integral inequality with time delay and the unified form for all the fractional order λ &gt; 0 is developed, which can be widely applied to investigate FTS of various fractional-order systems with time delays. Based on this new inequality, a new criterion for the FTS of FHNNTDs is derived. Compared with the existing criteria, in which fractional order λ ∈ (0,1) was divided into two cases, λ ∈ (0,0.5] and λ ∈ (0.5,1), the obtained results in this article are presented in the unified form of fractional order λ ∈ (0,1) and convenient to verify. More importantly, the criteria in this article are less conservative than some existing ones. Finally, two numerical examples are given to demonstrate the validity of the proposed results.},
  archive      = {J_TNNLS},
  author       = {Feifei Du and Jun-Guo Lu},
  doi          = {10.1109/TNNLS.2020.3016038},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {3858-3866},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {New criteria on finite-time stability of fractional-order hopfield neural networks with time delays},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Faceted text segmentation via multitask learning.
<em>TNNLS</em>, <em>32</em>(9), 3846–3857. (<a
href="https://doi.org/10.1109/TNNLS.2020.3015996">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text segmentation is a fundamental step in natural language processing (NLP) and information retrieval (IR) tasks. Most existing approaches do not explicitly take into account the facet information of documents for segmentation. Text segmentation and facet annotation are often addressed as separate problems, but they operate in a common input space. This article proposes FTS, which is a novel model for faceted text segmentation via multitask learning (MTL). FTS models faceted text segmentation as an MTL problem with text segmentation and facet annotation. This model employs the bidirectional long short-term memory (Bi-LSTM) network to learn the feature representation of sentences within a document. The feature representation is shared and adjusted with common parameters by MTL, which can help an optimization model to learn a better-shared and robust feature representation from text segmentation to facet annotation. Moreover, the text segmentation is modeled as a sequence tagging task using LSTM with a conditional random fields (CRFs) classification layer. Extensive experiments are conducted on five data sets from five domains: data structure, data mining, computer network, solid mechanics, and crystallography. The results indicate that the FTS model outperforms several highly cited and state-of-the-art approaches related to text segmentation and facet annotation.},
  archive      = {J_TNNLS},
  author       = {Bei Wu and Bifan Wei and Jun Liu and Kewei Wu and Meng Wang},
  doi          = {10.1109/TNNLS.2020.3015996},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {3846-3857},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Faceted text segmentation via multitask learning},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Location property of convolutional neural networks for image
classification. <em>TNNLS</em>, <em>32</em>(9), 3831–3845. (<a
href="https://doi.org/10.1109/TNNLS.2020.3015965">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When doing image classification, the core task of convolutional neural network (CNN)-based methods is to learn better feature representation. Our analysis has shown that a better feature representation in the layer before softmax operation (BSM-layer) means a better feature embedding location that has a larger distance to the separating hyperplane. By defining this property “Location Property” of CNN, the core task of CNN-based methods can be regarded as to find out the optimal feature embedding location in the BSM-layer. In order to achieve this, in this work, we first propose two feature embedding directions, principal embedding direction (PE-direction) and secondary embedding direction (SE-direction). And then, we further propose a loss-based optimization framework, location property loss (LP-loss), which can make feature representation move in the PE-direction and the SE-direction simultaneously during the training phase. LP-loss consists of two parts, LP PE and LP SE , where LP PE focuses on PE-direction, and LP SE focuses on SE-direction. Any loss function focusing on these two embedding directions can be chosen as LP PE and LP SE . Based on the analysis that softmax, L-softmax, and AM softmax can make the feature representation move in PE-direction to a different extent, any of them can be chosen as LP PE . Since there is no existing works can fulfill the purpose of LP SE , a novel loss, secondary optimal feature plane loss (S-OFP loss), is developed. S-OFP loss is designed to make feature representations belonging to the same category embed onto their corresponding S-OFP. It is proved that S-OFP loss is the optimal feature plane in the SE-direction. Experiments are done with shallow, moderate, and deep models on four benchmark data sets, including the MNIST, SVHN, CIFAR-10, and CIFAR-100, and results demonstrate that CNN models can obtain remarkable performance improvements with LP softmax , S-OFP and LP AM softmax , S-OFP, which verify the effectiveness of location property.},
  archive      = {J_TNNLS},
  author       = {Cong Liang and Haixia Zhang and Dongfeng Yuan and Minggao Zhang},
  doi          = {10.1109/TNNLS.2020.3015965},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {3831-3845},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Location property of convolutional neural networks for image classification},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Domain generalization for named entity boundary detection
via metalearning. <em>TNNLS</em>, <em>32</em>(9), 3819–3830. (<a
href="https://doi.org/10.1109/TNNLS.2020.3015912">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Named entity recognition (NER) aims to recognize mentions of rigid designators from text belonging to predefined semantic types, such as person, location, and organization. In this article, we focus on a fundamental subtask of NER, named entity boundary detection, which aims at detecting the start and end boundaries of an entity mention in the text, without predicting its semantic type. The entity boundary detection is essentially a sequence labeling problem. Existing sequence labeling methods either suffer from sparse boundary tags (i.e., entities are rare and nonentities are common) or they cannot well handle the issue of variable size output vocabulary (i.e., need to retrain models with respect to different vocabularies). To address these two issues, we propose a novel entity boundary labeling model that leverages pointer networks to effectively infer boundaries depending on the input sequence. On the other hand, training models on source domains that generalize to new target domains at the test time are a challenging problem because of the performance degradation. To alleviate this issue, we propose Metabdry, a novel domain generalization approach for entity boundary detection without requiring any access to target domain information. Especially, adversarial learning is adopted to encourage domain-invariant representations. Meanwhile, metalearning is used to explicitly simulate a domain shift during training so that metaknowledge from multiple resource domains can be effectively aggregated. As such, Metabdry explicitly optimizes the capability of “learning to generalize,” resulting in a more general and robust model to reduce the domain discrepancy. We first conduct experiments to demonstrate the effectiveness of our novel boundary labeling model. We then extensively evaluate Metabdry on eight data sets under domain generalization settings. The experimental results show that Metabdry achieves state-of-the-art results against the recent seven baselines.},
  archive      = {J_TNNLS},
  author       = {Jing Li and Shuo Shang and Lisi Chen},
  doi          = {10.1109/TNNLS.2020.3015912},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {3819-3830},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Domain generalization for named entity boundary detection via metalearning},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Can boosted randomness mimic learning algorithms of
geometric nature? Example of a simple algorithm that converges in
probability to hard-margin SVM. <em>TNNLS</em>, <em>32</em>(9),
3798–3818. (<a
href="https://doi.org/10.1109/TNNLS.2021.3059653">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the light of the general question posed in the title, we write down a very simple randomized learning algorithm, based on boosting, that can be seen as a nonstationary Markov random process. Surprisingly, the decision hyperplanes resulting from this algorithm converge in probability to the exact hard-margin solutions of support vector machines (SVMs). This fact is curious because the hard-margin hyperplane is not a statistical solution, but a purely geometric one-driven by margin maximization and strictly dependent on particular locations of some data points that are placed in the contact region of two classes, namely the support vectors. The proposed algorithm detects support vectors probabilistically, without being aware of their geometric definition. We give proofs of the main convergence theorem and several auxiliary lemmas. The analysis sheds new light on the relation between boosting and SVMs and also on the nature of SVM solutions since they can now be regarded equivalently as limits of certain random trajectories. In the experimental part, correctness of the proposed algorithm is verified against known SVM solvers: libsvm, liblinear, and also against optimization packages: cvxopt (Python) and Wolfram Mathematica.},
  archive      = {J_TNNLS},
  author       = {Przemysław Klęsk and Marcin Korzeń},
  doi          = {10.1109/TNNLS.2021.3059653},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {3798-3818},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Can boosted randomness mimic learning algorithms of geometric nature? example of a simple algorithm that converges in probability to hard-margin SVM},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Medical-VLBERT: Medical visual language BERT for COVID-19 CT
report generation with alternate learning. <em>TNNLS</em>,
<em>32</em>(9), 3786–3797. (<a
href="https://doi.org/10.1109/TNNLS.2021.3099165">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical imaging technologies, including computed tomography (CT) or chest X-Ray (CXR), are largely employed to facilitate the diagnosis of the COVID-19. Since manual report writing is usually too time-consuming, a more intelligent auxiliary medical system that could generate medical reports automatically and immediately is urgently needed. In this article, we propose to use the medical visual language BERT (Medical-VLBERT) model to identify the abnormality on the COVID-19 scans and generate the medical report automatically based on the detected lesion regions. To produce more accurate medical reports and minimize the visual-and-linguistic differences, this model adopts an alternate learning strategy with two procedures that are knowledge pretraining and transferring. To be more precise, the knowledge pretraining procedure is to memorize the knowledge from medical texts, while the transferring procedure is to utilize the acquired knowledge for professional medical sentences generations through observations of medical images. In practice, for automatic medical report generation on the COVID-19 cases, we constructed a dataset of 368 medical findings in Chinese and 1104 chest CT scans from The First Affiliated Hospital of Jinan University, Guangzhou, China, and The Fifth Affiliated Hospital of Sun Yat-sen University, Zhuhai, China. Besides, to alleviate the insufficiency of the COVID-19 training samples, our model was first trained on the large-scale Chinese CX-CHR dataset and then transferred to the COVID-19 CT dataset for further fine-tuning. The experimental results showed that Medical-VLBERT achieved state-of-the-art performances on terminology prediction and report generation with the Chinese COVID-19 CT dataset and the CX-CHR dataset. The Chinese COVID-19 CT dataset is available at https://covid19ct.github.io/ .},
  archive      = {J_TNNLS},
  author       = {Guangyi Liu and Yinghong Liao and Fuyu Wang and Bin Zhang and Lu Zhang and Xiaodan Liang and Xiang Wan and Shaolin Li and Zhen Li and Shuixing Zhang and Shuguang Cui},
  doi          = {10.1109/TNNLS.2021.3099165},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {3786-3797},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Medical-VLBERT: Medical visual language BERT for COVID-19 CT report generation with alternate learning},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021e). IEEE computational intelligence society information.
<em>TNNLS</em>, <em>32</em>(8), C3. (<a
href="https://doi.org/10.1109/TNNLS.2021.3096704">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Provides a listing of current committee members and society officers.},
  archive      = {J_TNNLS},
  doi          = {10.1109/TNNLS.2021.3096704},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {C3},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {IEEE computational intelligence society information},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Corrections to “learning to learn adaptive
classifier-predictor for few-shot learning” [aug 21 3458-3470].
<em>TNNLS</em>, <em>32</em>(8), 3784. (<a
href="https://doi.org/10.1109/TNNLS.2020.3017303">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the above article [1] , the results of “Fully-supervised (Upper bound)” in Tables III and IV were inadvertently set to intermediate records that were used as placeholders. This error has no effect on any of the interpretations and conclusions. Tables I and II of this amendment show the corrected results (highlighted in italics) of the original Tables III and IV.},
  archive      = {J_TNNLS},
  author       = {Nan Lai and Meina Kan and Chunrui Han and Xingguang Song and Shiguang Shan},
  doi          = {10.1109/TNNLS.2020.3017303},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3784},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Corrections to “Learning to learn adaptive classifier-predictor for few-shot learning” [Aug 21 3458-3470]},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Density encoding enables resource-efficient randomly
connected neural networks. <em>TNNLS</em>, <em>32</em>(8), 3777–3783.
(<a href="https://doi.org/10.1109/TNNLS.2020.3015971">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The deployment of machine learning algorithms on resource-constrained edge devices is an important challenge from both theoretical and applied points of view. In this brief, we focus on resource-efficient randomly connected neural networks known as random vector functional link (RVFL) networks since their simple design and extremely fast training time make them very attractive for solving many applied classification tasks. We propose to represent input features via the density-based encoding known in the area of stochastic computing and use the operations of binding and bundling from the area of hyperdimensional computing for obtaining the activations of the hidden neurons. Using a collection of 121 real-world data sets from the UCI machine learning repository, we empirically show that the proposed approach demonstrates higher average accuracy than the conventional RVFL. We also demonstrate that it is possible to represent the readout matrix using only integers in a limited range with minimal loss in the accuracy. In this case, the proposed approach operates only on small n-bits integers, which results in a computationally efficient architecture. Finally, through hardware field-programmable gate array (FPGA) implementations, we show that such an approach consumes approximately 11 times less energy than that of the conventional RVFL.},
  archive      = {J_TNNLS},
  author       = {Denis Kleyko and Mansour Kheffache and E. Paxon Frady and Urban Wiklund and Evgeny Osipov},
  doi          = {10.1109/TNNLS.2020.3015971},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3777-3783},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Density encoding enables resource-efficient randomly connected neural networks},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hierarchical one-class classifier with within-class
scatter-based autoencoders. <em>TNNLS</em>, <em>32</em>(8), 3770–3776.
(<a href="https://doi.org/10.1109/TNNLS.2020.3015860">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autoencoding is a vital branch of representation learning in deep neural networks (DNNs). The extreme learning machine-based autoencoder (ELM-AE) has been recently developed and has gained popularity for its fast learning speed and ease of implementation. However, the ELM-AE uses random hidden node parameters without tuning, which may generate meaningless encoded features. In this brief, we first propose a within-class scatter information constraint-based AE (WSI-AE) that minimizes both the reconstruction error and the within-class scatter of the encoded features. We then build stacked WSI-AEs into a one-class classification (OCC) algorithm based on the hierarchical regularized least-squared method. The effectiveness of our approach was experimentally demonstrated in comparisons with several state-of-the-art AEs and OCC algorithms. The evaluations were performed on several benchmark data sets.},
  archive      = {J_TNNLS},
  author       = {Tianlei Wang and Jiuwen Cao and Xiaoping Lai and Q. M. Jonathan Wu},
  doi          = {10.1109/TNNLS.2020.3015860},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3770-3776},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Hierarchical one-class classifier with within-class scatter-based autoencoders},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Transductive zero-shot action recognition via visually
connected graph convolutional networks. <em>TNNLS</em>, <em>32</em>(8),
3761–3769. (<a
href="https://doi.org/10.1109/TNNLS.2020.3015848">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the explosive growth of action categories, zero-shot action recognition aims to extend a well-trained model to novel/unseen classes. To bridge the large knowledge gap between seen and unseen classes, in this brief, we visually associate unseen actions with seen categories in a visually connected graph, and the knowledge is then transferred from the visual features space to semantic space via the grouped attention graph convolutional networks (GAGCNs). In particular, we extract visual features for all the actions, and a visually connected graph is built to attach seen actions to visually similar unseen categories. Moreover, the proposed grouped attention mechanism exploits the hierarchical knowledge in the graph so that the GAGCN enables propagating the visual-semantic connections from seen actions to unseen ones. We extensively evaluate the proposed method on three data sets: HMDB51, UCF101, and NTU RGB + D. Experimental results show that the GAGCN outperforms state-of-the-art methods.},
  archive      = {J_TNNLS},
  author       = {Yangyang Xu and Chu Han and Jing Qin and Xuemiao Xu and Guoqiang Han and Shengfeng He},
  doi          = {10.1109/TNNLS.2020.3015848},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3761-3769},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Transductive zero-shot action recognition via visually connected graph convolutional networks},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning rate for convex support tensor machines.
<em>TNNLS</em>, <em>32</em>(8), 3755–3760. (<a
href="https://doi.org/10.1109/TNNLS.2020.3015477">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tensors are increasingly encountered in prediction problems. We extend previous results for high-dimensional least-squares convex tensor regression to classification problems with a hinge loss and establish its asymptotic statistical properties. Based on a general convex decomposable penalty, the rate depends on both the intrinsic dimension and the Rademacher complexity of the class of linear functions of tensor predictors.},
  archive      = {J_TNNLS},
  author       = {Heng Lian},
  doi          = {10.1109/TNNLS.2020.3015477},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3755-3760},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Learning rate for convex support tensor machines},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Maximum correntropy criterion-based hierarchical one-class
classification. <em>TNNLS</em>, <em>32</em>(8), 3748–3754. (<a
href="https://doi.org/10.1109/TNNLS.2020.3015356">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the effectiveness of anomaly/outlier detection, one-class algorithms have been extensively studied in the past. The representatives include the shallow-structure methods and deep networks, such as the one-class support vector machine (OC-SVM), one-class extreme learning machine (OC-ELM), deep support vector data description (Deep SVDD), and multilayer OC-ELM (ML-OCELM/MK-OCELM). However, existing algorithms are generally built on the minimum mean-square-error (mse) criterion, which is robust to the Gaussian noises but less effective in dealing with large outliers. To alleviate this deficiency, a robust maximum correntropy criterion (MCC)-based OC-ELM (MC-OCELM) is first proposed and then further extended to a hierarchical network to enhance its capability in characterizing complex and large data (named HC-OCELM). The gradient derivation combining with a fixed-point iterative updation scheme is adopted for the output weight optimization. Experiments on many benchmark data sets are conducted for effectiveness validation. Comparisons to many state-of-the-art approaches are provided for the superiority demonstration.},
  archive      = {J_TNNLS},
  author       = {Jiuwen Cao and Haozhen Dai and Baiying Lei and Chun Yin and Huanqiang Zeng and Anton Kummert},
  doi          = {10.1109/TNNLS.2020.3015356},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3748-3754},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Maximum correntropy criterion-based hierarchical one-class classification},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Concurrent associative memories with synaptic delays.
<em>TNNLS</em>, <em>32</em>(8), 3736–3747. (<a
href="https://doi.org/10.1109/TNNLS.2020.3041048">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents concurrent associative memories with synaptic delays useful for processing sequences of real vectors. Associative memories with synaptic delays were introduced by the authors for symbolic sequential inputs and demonstrated several advantages over other sequential memories. They were easy to organize and train. It was demonstrated that they were more robust than long short-term memories in recognition of damaged sequences. The associative memories can be applied in combination with deep neural networks to solve such symbol grounding problems, such as speech recognition, and support sequential memories triggered by sensory inputs. Several practical considerations for developed memories were discussed and illustrated. A continuous speech database was used to compare the developed method with LSTM memories. Tests demonstrated that the developed approach is more robust in recognition of speech sequences, particularly when the test sequences are damaged.},
  archive      = {J_TNNLS},
  author       = {Janusz A. Starzyk and Marek Jaszuk and Łukasz Maciura and Adrian Horzyk},
  doi          = {10.1109/TNNLS.2020.3041048},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3736-3747},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Concurrent associative memories with synaptic delays},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptive event-triggered synchronization of
reaction–diffusion neural networks. <em>TNNLS</em>, <em>32</em>(8),
3723–3735. (<a
href="https://doi.org/10.1109/TNNLS.2020.3027284">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article focuses on the design of an adaptive event-triggered sampled-data control (ETSDC) mechanism for synchronization of reaction-diffusion neural networks (RDNNs) with random time-varying delays. Different from the existing ETSDC schemes with predetermined constant thresholds, an adaptive ETSDC mechanism is proposed for RDNNs. The adaptive ETSDC mechanism can be promptly adaptively adjusted since the threshold function is based on the current sampled and latest transmitted signals. Thus, the adaptive ETSDC mechanism can effectively save communication resources for RDNNs. By taking the influence of uncertain factors, the random time-varying delays are considered, which belongs to two intervals in a probabilistic way. Then, by constructing an appropriate Lyapunov-Krasovskii functional (LKF), new synchronization criteria are derived for RDNNs. By solving a set of linear matrix inequalities (LMIs), the desired adaptive ETSDC gain is obtained. Finally, the merits of the adaptive ETSDC mechanism and the effectiveness of the proposed results are verified by one numerical example.},
  archive      = {J_TNNLS},
  author       = {Ruimei Zhang and Deqiang Zeng and Ju H. Park and Yajuan Liu and Xiangpeng Xie},
  doi          = {10.1109/TNNLS.2020.3027284},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3723-3735},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive event-triggered synchronization of Reaction–Diffusion neural networks},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Clustered federated learning: Model-agnostic distributed
multitask optimization under privacy constraints. <em>TNNLS</em>,
<em>32</em>(8), 3710–3722. (<a
href="https://doi.org/10.1109/TNNLS.2020.3015958">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) is currently the most widely adopted framework for collaborative training of (deep) machine learning models under privacy constraints. Albeit its popularity, it has been observed that FL yields suboptimal results if the local clients&#39; data distributions diverge. To address this issue, we present clustered FL (CFL), a novel federated multitask learning (FMTL) framework, which exploits geometric properties of the FL loss surface to group the client population into clusters with jointly trainable data distributions. In contrast to existing FMTL approaches, CFL does not require any modifications to the FL communication protocol to be made, is applicable to general nonconvex objectives (in particular, deep neural networks), does not require the number of clusters to be known a priori, and comes with strong mathematical guarantees on the clustering quality. CFL is flexible enough to handle client populations that vary over time and can be implemented in a privacy-preserving way. As clustering is only performed after FL has converged to a stationary point, CFL can be viewed as a postprocessing method that will always achieve greater or equal performance than conventional FL by allowing clients to arrive at more specialized models. We verify our theoretical analysis in experiments with deep convolutional and recurrent neural networks on commonly used FL data sets.},
  archive      = {J_TNNLS},
  author       = {Felix Sattler and Klaus-Robert Müller and Wojciech Samek},
  doi          = {10.1109/TNNLS.2020.3015958},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3710-3722},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Clustered federated learning: Model-agnostic distributed multitask optimization under privacy constraints},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Novel inequalities to global mittag–leffler synchronization
and stability analysis of fractional-order quaternion-valued neural
networks. <em>TNNLS</em>, <em>32</em>(8), 3700–3709. (<a
href="https://doi.org/10.1109/TNNLS.2020.3015952">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article is concerned with the problem of the global Mittag-Leffler synchronization and stability for fractional-order quaternion-valued neural networks (FOQVNNs). The systems of FOQVNNs, which contain either general activation functions or linear threshold ones, are successfully established. Meanwhile, two distinct methods, such as separation and nonseparation, have been employed to solve the transformation of the studied systems of FOQVNNs, which dissatisfy the commutativity of quaternion multiplication. Moreover, two novel inequalities are deduced based on the general parameters. Compared with the existing inequalities, the new inequalities have their unique superiorities because they can make full use of the additional parameters. Due to the Lyapunov theory, two novel Lyapunov-Krasovskii functionals (LKFs) can be easily constructed. The novelty of LKFs comes from a wider range of parameters, which can be involved in the construction of LKFs. Furthermore, mainly based on the new inequalities and LKFs, more multiple and more flexible criteria are efficiently obtained for the discussed problem. Finally, four numerical examples are given to demonstrate the related effectiveness and availability of the derived criteria.},
  archive      = {J_TNNLS},
  author       = {Jianying Xiao and Jinde Cao and Jun Cheng and Shiping Wen and Ruimei Zhang and Shouming Zhong},
  doi          = {10.1109/TNNLS.2020.3015952},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3700-3709},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Novel inequalities to global Mittag–Leffler synchronization and stability analysis of fractional-order quaternion-valued neural networks},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Global exponential stability of memristive neural networks
with mixed time-varying delays. <em>TNNLS</em>, <em>32</em>(8),
3690–3699. (<a
href="https://doi.org/10.1109/TNNLS.2020.3015944">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates the Lagrange exponential stability and the Lyapunov exponential stability of memristive neural networks with discrete and distributed time-varying delays (DMNNs). By means of inequality techniques, theories of the M-matrix, and the comparison strategy, the Lagrange exponential stability of the underlying DMNNs is considered in the sense of Filippov, and the globally exponentially attractive set is estimated through employing the M-matrix and external input. Especially, when the external input is not concerned, the Lyapunov exponential stability of the corresponding DMNNs is developed immediately in the form of an M-matrix, which contains some published outcomes as special cases. Furthermore, by constructing an M-matrix-based differential system, the Lyapunov exponential stability of the DMNNs is studied, which is less conservative than some existing ones. Finally, three simulation examples are carried out to examine the validness of the theories.},
  archive      = {J_TNNLS},
  author       = {Yin Sheng and Tingwen Huang and Zhigang Zeng and Xiangshui Miao},
  doi          = {10.1109/TNNLS.2020.3015944},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3690-3699},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Global exponential stability of memristive neural networks with mixed time-varying delays},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Scalable kernel ordinal regression via doubly stochastic
gradients. <em>TNNLS</em>, <em>32</em>(8), 3677–3689. (<a
href="https://doi.org/10.1109/TNNLS.2020.3015937">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ordinal regression (OR) is one of the most important machine learning tasks. The kernel method is a major technique to achieve nonlinear OR. However, traditional kernel OR solvers are inefficient due to increased complexity introduced by multiple ordinal thresholds as well as the cost of kernel computation. Doubly stochastic gradient (DSG) is a very efficient and scalable kernel learning algorithm that combines random feature approximation with stochastic functional optimization. However, the theory and algorithm of DSG can only support optimization tasks within the unique reproducing kernel Hilbert space (RKHS), which is not suitable for OR problems where the multiple ordinal thresholds usually lead to multiple RKHSs. To address this problem, we construct a kernel whose RKHS can contain the decision function with multiple thresholds. Based on this new kernel, we further propose a novel DSG-like algorithm, DSGOR. In each iteration of DSGOR, we update the decision functional as well as the function bias with appropriately set learning rates for each. Our theoretic analysis shows that DSGOR can achieve O(1/t) convergence rate, which is as good as DSG, even though dealing with a much harder problem. Extensive experimental results demonstrate that our algorithm is much more efficient than traditional kernel OR solvers, especially on large-scale problems.},
  archive      = {J_TNNLS},
  author       = {Bin Gu and Xiang Geng and Xiang Li and Wanli Shi and Guansheng Zheng and Cheng Deng and Heng Huang},
  doi          = {10.1109/TNNLS.2020.3015937},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3677-3689},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Scalable kernel ordinal regression via doubly stochastic gradients},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Rain streaks removal for single image via kernel-guided
convolutional neural network. <em>TNNLS</em>, <em>32</em>(8), 3664–3676.
(<a href="https://doi.org/10.1109/TNNLS.2020.3015897">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently emerged deep learning methods have achieved great success in single image rain streaks removal. However, existing methods ignore an essential factor in the rain streaks generation mechanism, i.e., the motion blur leading to the line pattern appearances. Thus, they generally produce overderaining or underderaining results. In this article, inspired by the generation mechanism, we propose a novel rain streaks removal framework using a kernel-guided convolutional neural network (KGCNN), achieving state-of-the-art performance with a simple network architecture. More precisely, our framework consists of three steps. First, we learn the motion blur kernel by a plain neural network, termed parameter network, from the detail layer of a rainy patch. Then, we stretch the learned motion blur kernel into a degradation map with the same spatial size as the rainy patch. Finally, we use the stretched degradation map together with the detail patches to train a deraining network with a typical ResNet architecture, which produces the rain streaks with the guidance of the learned motion blur kernel. Experiments conducted on extensive synthetic and real data demonstrate the effectiveness of the proposed KGCNN, in terms of rain streaks removal and image detail preservation.},
  archive      = {J_TNNLS},
  author       = {Ye-Tao Wang and Xi-Le Zhao and Tai-Xiang Jiang and Liang-Jian Deng and Yi Chang and Ting-Zhu Huang},
  doi          = {10.1109/TNNLS.2020.3015897},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3664-3676},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Rain streaks removal for single image via kernel-guided convolutional neural network},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Neighborhood attention networks with adversarial learning
for link prediction. <em>TNNLS</em>, <em>32</em>(8), 3653–3663. (<a
href="https://doi.org/10.1109/TNNLS.2020.3015896">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we aim at developing neighborhood-based neural models for link prediction. We design a novel multispace neighbor attention mechanism to extract universal neighborhood features by capturing latent importance of neighbors and selectively aggregate their features in multiple latent spaces. Grounded on this mechanism, we propose two link prediction models, i.e., self neighborhood attention network (SNAN), which predicts the link of two nodes by encoding and matching their respective neighborhood information, and its extension cross neighborhood attention network (CNAN), where we additionally design a cross neighborhood attention to directly capture structural interactions between two nodes. Another key novelty of this work is that we propose an adversarial learning framework, where a negative sample generator is devised to improve the optimization of the proposed link prediction models by continuously providing highly informative negative samples in the adversarial game. We evaluate our models with extensive experiments on 12 benchmark data sets against 14 popular and state-of-the-art link prediction approaches. The results strongly demonstrate the significant and universal superiority of our models on various types of networks. The effectiveness and robustness of the proposed attention mechanism and adversarial learning framework are also verified by detailed ablation studies.},
  archive      = {J_TNNLS},
  author       = {Zhitao Wang and Yu Lei and Wenjie Li},
  doi          = {10.1109/TNNLS.2020.3015896},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3653-3663},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Neighborhood attention networks with adversarial learning for link prediction},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep learning-based model predictive control for continuous
stirred-tank reactor system. <em>TNNLS</em>, <em>32</em>(8), 3643–3652.
(<a href="https://doi.org/10.1109/TNNLS.2020.3015869">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A continuous stirred-tank reactor (CSTR) system is widely applied in wastewater treatment processes. Its control is a challenging industrial-process-control problem due to great difficulty to achieve accurate system identification. This work proposes a deep learning-based model predictive control (DeepMPC) to model and control the CSTR system. The proposed DeepMPC consists of a growing deep belief network (GDBN) and an optimal controller. First, GDBN can automatically determine its size with transfer learning to achieve high performance in system identification, and it serves just as a predictive model of a controlled system. The model can accurately approximate the dynamics of the controlled system with a uniformly ultimately bounded error. Second, quadratic optimization is conducted to obtain an optimal controller. This work analyzes the convergence and stability of DeepMPC. Finally, the DeepMPC is used to model and control a second-order CSTR system. In the experiments, DeepMPC shows a better performance in modeling, tracking, and antidisturbance than the other state-of-the-art methods.},
  archive      = {J_TNNLS},
  author       = {Gongming Wang and Qing-Shan Jia and Junfei Qiao and Jing Bi and MengChu Zhou},
  doi          = {10.1109/TNNLS.2020.3015869},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3643-3652},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Deep learning-based model predictive control for continuous stirred-tank reactor system},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Continuous model adaptation using online meta-learning for
smart grid application. <em>TNNLS</em>, <em>32</em>(8), 3633–3642. (<a
href="https://doi.org/10.1109/TNNLS.2020.3015858">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid development of deep learning algorithms provides us an opportunity to better understand the complexity in engineering systems, such as the smart grid. Most of the existing data-driven predictive models are trained using historical data and fixed during the execution stage, which cannot adapt well to real-time data. In this research, we propose a novel online meta-learning (OML) algorithm to continuously adapt pretrained base-learner through efficiently digesting real-time data to adaptively control the base-learner parameters using meta-optimizer. The simulation results show that: 1) both ML and OML can perform significantly better than online base learning. 2) OML can perform better than ML and online base learning when the training data are limited, or the training and real-time data have very different time-variant patterns.},
  archive      = {J_TNNLS},
  author       = {Jinghang Li and Mengqi Hu},
  doi          = {10.1109/TNNLS.2020.3015858},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3633-3642},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Continuous model adaptation using online meta-learning for smart grid application},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Decentralized adaptive command filtered neural tracking
control of large-scale nonlinear systems: An almost fast finite-time
framework. <em>TNNLS</em>, <em>32</em>(8), 3621–3632. (<a
href="https://doi.org/10.1109/TNNLS.2020.3015847">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, a decentralized adaptive finite-time tracking control scheme is proposed for a class of nonstrict feedback large-scale nonlinear interconnected systems with disturbances. First, a practical almost fast finite-time stability framework is established for a general nonlinear system, which is then applied to the design of the large-scale system under consideration. By fusing command filter technique and adaptive neural control and introducing two smooth functions, the “singular” and “explosion of complex” problems in the backstepping procedure are circumvented, while the obstacles caused by unknown interconnections are overcome. Moreover, according to the framework of practical almost fast finite-time stability, it is shown that all the closed-loop signals of the large-scale system are almost fast finite-time bounded, and the tracking errors can converge to arbitrarily small residual sets predefined in an almost fast finite time. Finally, a simulation example is presented to demonstrate the effectiveness of the proposed finite-time decentralized control scheme.},
  archive      = {J_TNNLS},
  author       = {Ji-Dong Liu and Ben Niu and Yong-Gui Kao and Ping Zhao and Dong Yang},
  doi          = {10.1109/TNNLS.2020.3015847},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3621-3632},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Decentralized adaptive command filtered neural tracking control of large-scale nonlinear systems: An almost fast finite-time framework},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A unified framework for multilingual speech recognition in
air traffic control systems. <em>TNNLS</em>, <em>32</em>(8), 3608–3620.
(<a href="https://doi.org/10.1109/TNNLS.2020.3015830">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work focuses on robust speech recognition in air traffic control (ATC) by designing a novel processing paradigm to integrate multilingual speech recognition into a single framework using three cascaded modules: an acoustic model (AM), a pronunciation model (PM), and a language model (LM). The AM converts ATC speech into phoneme-based text sequences that the PM then translates into a word-based sequence, which is the ultimate goal of this research. The LM corrects both phoneme- and word-based errors in the decoding results. The AM, including the convolutional neural network (CNN) and recurrent neural network (RNN), considers the spatial and temporal dependences of the speech features and is trained by the connectionist temporal classification loss. To cope with radio transmission noise and diversity among speakers, a multiscale CNN architecture is proposed to fit the diverse data distributions and improve the performance. Phoneme-to-word translation is addressed via a proposed machine translation PM with an encoder–decoder architecture. RNN-based LMs are trained to consider the code-switching specificity of the ATC speech by building dependences with common words. We validate the proposed approach using large amounts of real Chinese and English ATC recordings and achieve a 3.95\% label error rate on Chinese characters and English words, outperforming other popular approaches. The decoding efficiency is also comparable to that of the end-to-end model, and its generalizability is validated on several open corpora, making it suitable for real-time approaches to further support ATC applications, such as ATC prediction and safety checking.},
  archive      = {J_TNNLS},
  author       = {Yi Lin and Dongyue Guo and Jianwei Zhang and Zhengmao Chen and Bo Yang},
  doi          = {10.1109/TNNLS.2020.3015830},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3608-3620},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A unified framework for multilingual speech recognition in air traffic control systems},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fast and effective active clustering ensemble based on
density peak. <em>TNNLS</em>, <em>32</em>(8), 3593–3607. (<a
href="https://doi.org/10.1109/TNNLS.2020.3015795">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semisupervised clustering methods improve performance by randomly selecting pairwise constraints, which may lead to redundancy and instability. In this context, active clustering is proposed to maximize the efficacy of annotations by effectively using pairwise constraints. However, existing methods lack an overall consideration of the querying criteria and repeatedly run semisupervised clustering to update labels. In this work, we first propose an active density peak (ADP) clustering algorithm that considers both representativeness and informativeness. Representative instances are selected to capture data patterns, while informative instances are queried to reduce the uncertainty of clustering results. Meanwhile, we design a fast-update-strategy to update labels efficiently. In addition, we propose an active clustering ensemble framework that combines local and global uncertainties to query the most ambiguous instances for better separation between the clusters. A weighted voting consensus method is introduced for better integration of clustering results. We conducted experiments by comparing our methods with state-of-the-art methods on real-world data sets. Experimental results demonstrate the effectiveness of our methods.},
  archive      = {J_TNNLS},
  author       = {Yifan Shi and Zhiwen Yu and Wenming Cao and C. L. Philip Chen and Hau-San Wong and Guoqiang Han},
  doi          = {10.1109/TNNLS.2020.3015795},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3593-3607},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Fast and effective active clustering ensemble based on density peak},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Topology of learning in feedforward neural networks.
<em>TNNLS</em>, <em>32</em>(8), 3588–3592. (<a
href="https://doi.org/10.1109/TNNLS.2020.3015790">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding how neural networks learn remains one of the central challenges in machine learning research. From random at the start of training, the weights of a neural network evolve in such a way as to be able to perform a variety of tasks, such as classifying images. Here, we study the emergence of structure in the weights by applying methods from topological data analysis. We train simple feedforward neural networks on the MNIST data set and monitor the evolution of the weights. When initialized to zero, the weights follow trajectories that branch off recurrently, thus generating trees that describe the growth of the effective capacity of each layer. When initialized to tiny random values, the weights evolve smoothly along 2-D surfaces. We show that natural coordinates on these learning surfaces correspond to important factors of variation.},
  archive      = {J_TNNLS},
  author       = {Maxime Gabella},
  doi          = {10.1109/TNNLS.2020.3015790},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3588-3592},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Topology of learning in feedforward neural networks},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A parallel framework of adaptive dynamic programming
algorithm with off-policy learning. <em>TNNLS</em>, <em>32</em>(8),
3578–3587. (<a
href="https://doi.org/10.1109/TNNLS.2020.3015767">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, a model-free online adaptive dynamic programming (ADP) approach is developed for solving the optimal control problem of nonaffine nonlinear systems. Combining the off-policy learning mechanism with the parallel paradigm, multithread agents are employed to collect the transitions by interacting with the environment that significantly augments the number of sampled data. On the other hand, each thread agent explores the environment with different initial states under its own behavior policy that enhances the exploration capability and alleviates the correlation between the sampled data. After the policy evaluation process, only one step update is required for policy improvement based on the policy gradient method. The stability of the system under iterative control laws is guaranteed. Moreover, the convergence analysis is given to prove that the iterative Q-function is monotonically nonincreasing and finally converges to the solution of the Hamilton-Jacobi-Bellman (HJB) equation. For implementing the algorithm, the actor-critic (AC) structure is utilized with two neural networks (NNs) to approximate the Q-function and the control policy. Finally, the effectiveness of the proposed algorithm is verified by two numerical examples.},
  archive      = {J_TNNLS},
  author       = {Changyin Sun and Xiaofeng Li and Yuewen Sun},
  doi          = {10.1109/TNNLS.2020.3015767},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3578-3587},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A parallel framework of adaptive dynamic programming algorithm with off-policy learning},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A semiproximal support vector machine approach for binary
multiple instance learning. <em>TNNLS</em>, <em>32</em>(8), 3566–3577.
(<a href="https://doi.org/10.1109/TNNLS.2020.3015442">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We face a binary multiple instance learning (MIL) problem, whose objective is to discriminate between two kinds of point sets: positive and negative. In the MIL terminology, such sets are called bags, and the points inside each bag are called instances. Considering the case with two classes of instances (positive and negative) and inspired by a well-established instance-space support vector machine (SVM) model, we propose to extend to MIL classification the proximal SVM (PSVM) technique that has revealed very effective for supervised learning, especially in terms of computational time. In particular, our approach is based on a new instance-space model that exploits the benefits coming from both SVM (better accuracy) and PSVM (computational efficiency) paradigms. Starting from the standard MIL assumption, such a model is aimed at generating a hyperplane placed in the middle between two parallel hyperplanes: the first one is a proximal hyperplane that clusters the instances of the positive bags, while the second one constitutes a supporting hyperplane for the instances of the negative bags. Numerical results are presented on a set of MIL test data sets drawn from the literature.},
  archive      = {J_TNNLS},
  author       = {Matteo Avolio and Antonio Fuduli},
  doi          = {10.1109/TNNLS.2020.3015442},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3566-3577},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A semiproximal support vector machine approach for binary multiple instance learning},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Nonfragile h∞ state estimation for recurrent neural networks
with time-varying delays: On proportional–integral observer design.
<em>TNNLS</em>, <em>32</em>(8), 3553–3565. (<a
href="https://doi.org/10.1109/TNNLS.2020.3015376">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, a novel proportional–integral observer (PIO) design approach is proposed for the nonfragile $H_{\infty }$ state estimation problem for a class of discrete-time recurrent neural networks with time-varying delays. The developed PIO is equipped with more design freedom leading to better steady-state accuracy compared with the conventional Luenberger observer. The phenomena of randomly occurring gain variations, which are characterized by the Bernoulli distributed random variables with certain probabilities, are taken into consideration in the implementation of the addressed PIO. Attention is focused on the design of a nonfragile PIO such that the error dynamics of the state estimation is exponentially stable in a mean-square sense, and the prescribed $H_{\infty }$ performance index is also achieved. Sufficient conditions for the existence of the desired PIO are established by virtue of the Lyapunov–Krasovskii functional approach and the matrix inequality technique. Finally, a simulation example is provided to demonstrate the effectiveness of the proposed PIO design scheme.},
  archive      = {J_TNNLS},
  author       = {Di Zhao and Zidong Wang and Guoliang Wei and Xiaohui Liu},
  doi          = {10.1109/TNNLS.2020.3015376},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3553-3565},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Nonfragile h∞ state estimation for recurrent neural networks with time-varying delays: On Proportional–Integral observer design},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Demystifying deep learning in predictive spatiotemporal
analytics: An information-theoretic framework. <em>TNNLS</em>,
<em>32</em>(8), 3538–3552. (<a
href="https://doi.org/10.1109/TNNLS.2020.3015215">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning has achieved incredible success over the past years, especially in various challenging predictive spatiotemporal analytics (PSTA) tasks, such as disease prediction, climate forecast, and traffic prediction, where intrinsic dependence relationships among data exist and generally manifest at multiple spatiotemporal scales. However, given a specific PSTA task and the corresponding data set, how to appropriately determine the desired configuration of a deep learning model, theoretically analyze the model&#39;s learning behavior, and quantitatively characterize the model&#39;s learning capacity remains a mystery. In order to demystify the power of deep learning for PSTA in a theoretically sound and explainable way, in this article, we provide a comprehensive framework for deep learning model design and information-theoretic analysis. First, we develop and demonstrate a novel interactively and integratively connected deep recurrent neural network (I 2 DRNN) model. I 2 DRNN consists of three modules: an input module that integrates data from heterogeneous sources; a hidden module that captures the information at different scales while allowing the information to flow interactively between layers; and an output module that models the integrative effects of information from various hidden layers to generate the output predictions. Second, to theoretically prove that our designed model can learn multiscale spatiotemporal dependence in PSTA tasks, we provide an information-theoretic analysis to examine the information-based learning capacity (i-CAP) of the proposed model. In so doing, we can tackle an important open question in deep learning, that is, how to determine the necessary and sufficient configurations of a designed deep learning model with respect to the given learning data sets. Third, to validate the I 2 DRNN model and confirm its i-CAP, we systematically conduct a series of experiments involving both synthetic data sets and real-world PSTA tasks. The experimental results show that the I 2 DRNN model outperforms both classical and state-of-the-art models on all data sets and PSTA tasks. More importantly, as readily validated, the proposed model captures the multiscale spatiotemporal dependence, which is meaningful in the real-world context. Furthermore, the model configuration that corresponds to the best performance on a given data set always falls into the range between the necessary and sufficient configurations, as derived from the information-theoretic analysis.},
  archive      = {J_TNNLS},
  author       = {Qi Tan and Yang Liu and Jiming Liu},
  doi          = {10.1109/TNNLS.2020.3015215},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3538-3552},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Demystifying deep learning in predictive spatiotemporal analytics: An information-theoretic framework},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Long-tailed characteristic of spiking pattern alternation
induced by log-normal excitatory synaptic distribution. <em>TNNLS</em>,
<em>32</em>(8), 3525–3537. (<a
href="https://doi.org/10.1109/TNNLS.2020.3015208">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Studies of structural connectivity at the synaptic level show that in synaptic connections of the cerebral cortex, the excitatory postsynaptic potential (EPSP) in most synapses exhibits sub-mV values, while a small number of synapses exhibit large EPSPs ( $\gtrsim 1.0$ [mV]). This means that the distribution of EPSP fits a log-normal distribution. While not restricting structural connectivity, skewed and long-tailed distributions have been widely observed in neural activities, such as the occurrences of spiking rates and the size of a synchronously spiking population. Many studies have been modeled this long-tailed EPSP neural activity distribution; however, its causal factors remain controversial. This study focused on the long-tailed EPSP distributions and interlateral synaptic connections primarily observed in the cortical network structures, thereby having constructed a spiking neural network consistent with these features. Especially, we constructed two coupled modules of spiking neural networks with excitatory and inhibitory neural populations with a log-normal EPSP distribution. We evaluated the spiking activities for different input frequencies and with/without strong synaptic connections. These coupled modules exhibited intermittent intermodule-alternative behavior, given moderate input frequency and the existence of strong synaptic and intermodule connections. Moreover, the power analysis, multiscale entropy analysis, and surrogate data analysis revealed that the long-tailed EPSP distribution and intermodule connections enhanced the complexity of spiking activity at large temporal scales and induced nonlinear dynamics and neural activity that followed the long-tailed distribution.},
  archive      = {J_TNNLS},
  author       = {Sou Nobukawa and Haruhiko Nishimura and Nobuhiko Wagatsuma and Satoshi Ando and Teruya Yamanishi},
  doi          = {10.1109/TNNLS.2020.3015208},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3525-3537},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Long-tailed characteristic of spiking pattern alternation induced by log-normal excitatory synaptic distribution},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Stability analysis of the modified levenberg–marquardt
algorithm for the artificial neural network training. <em>TNNLS</em>,
<em>32</em>(8), 3510–3524. (<a
href="https://doi.org/10.1109/TNNLS.2020.3015200">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Levenberg-Marquardt and Newton are two algorithms that use the Hessian for the artificial neural network learning. In this article, we propose a modified Levenberg-Marquardt algorithm for the artificial neural network learning containing the training and testing stages. The modified Levenberg-Marquardt algorithm is based on the Levenberg-Marquardt and Newton algorithms but with the following two differences to assure the error stability and weights boundedness: 1) there is a singularity point in the learning rates of the Levenberg-Marquardt and Newton algorithms, while there is not a singularity point in the learning rate of the modified Levenberg-Marquardt algorithm and 2) the Levenberg-Marquardt and Newton algorithms have three different learning rates, while the modified Levenberg-Marquardt algorithm only has one learning rate. The error stability and weights boundedness of the modified Levenberg-Marquardt algorithm are assured based on the Lyapunov technique. We compare the artificial neural network learning with the modified Levenberg-Marquardt, Levenberg-Marquardt, Newton, and stable gradient algorithms for the learning of the electric and brain signals data set.},
  archive      = {J_TNNLS},
  author       = {José de Jesús Rubio},
  doi          = {10.1109/TNNLS.2020.3015200},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3510-3524},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Stability analysis of the modified Levenberg–Marquardt algorithm for the artificial neural network training},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Practical multisource transfer regression with source–target
similarity captures. <em>TNNLS</em>, <em>32</em>(8), 3498–3509. (<a
href="https://doi.org/10.1109/TNNLS.2020.3012457">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A key challenge in many applications of multisource transfer learning is to explicitly capture the diverse source-target similarities. In this article, we are concerned with stretching the set of practical approaches based on Gaussian process (GP) models to solve multisource transfer regression problems. Precisely, we first investigate the feasibility and performance of a family of transfer covariance functions that represent the pairwise similarity of each source and the target domain. We theoretically show that using such a transfer covariance function for general GP modeling can only capture the same similarity coefficient for all the sources, and thus may result in unsatisfactory transfer performance. This outcome, together with the scalability issues of a single GP based approach, leads us to propose TC MS Stack, an integrated framework incorporating a separate transfer covariance function for each source and stacking. Contrary to typical stacking approaches, TC MS Stack learns the source-target similarity in each base GP model by considering the dependencies of the other sources along the process. We introduce two instances of the proposed TC MS Stack. Extensive experiments on one synthetic and two real-world data sets, with learning settings up to 11 sources for the latter, demonstrate the effectiveness of our approach.},
  archive      = {J_TNNLS},
  author       = {Pengfei Wei and Ramon Sagarna and Yiping Ke and Yew-Soon Ong},
  doi          = {10.1109/TNNLS.2020.3012457},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3498-3509},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Practical multisource transfer regression with Source–Target similarity captures},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptive fusion of heterogeneous manifolds for subspace
clustering. <em>TNNLS</em>, <em>32</em>(8), 3484–3497. (<a
href="https://doi.org/10.1109/TNNLS.2020.3011717">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiview clustering (MVC) has recently received great interest due to its pleasing efficacy in combining the abundant and complementary information to improve clustering performance, which overcomes the drawbacks of view limitation existed in the standard single-view clustering. However, the existing MVC methods are mostly designed for vectorial data from linear spaces and, thus, are not suitable for multiple dimensional data with intrinsic nonlinear manifold structures, e.g., videos or image sets. Some works have introduced manifolds&#39; representation methods of data into MVC and obtained considerable improvements, but how to fuse multiple manifolds efficiently for clustering is still a challenging problem. Particularly, for heterogeneous manifolds, it is an entirely new problem. In this article, we propose to represent the complicated multiviews&#39; data as heterogeneous manifolds and a fusion framework of heterogeneous manifolds for clustering. Different from the empirical weighting methods, an adaptive fusion strategy is designed to weight the importance of different manifolds in a data-driven manner. In addition, the low-rank representation is generalized onto the fused heterogeneous manifolds to explore the low-dimensional subspace structures embedded in data for clustering. We assessed the proposed method on several public data sets, including human action video, facial image, and traffic scenario video. The experimental results show that our method obviously outperforms a number of state-of-the-art clustering methods.},
  archive      = {J_TNNLS},
  author       = {Boyue Wang and Yongli Hu and Junbin Gao and Yanfeng Sun and Fujiao Ju and Baocai Yin},
  doi          = {10.1109/TNNLS.2020.3011717},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3484-3497},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive fusion of heterogeneous manifolds for subspace clustering},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Context-aware learning for generative models.
<em>TNNLS</em>, <em>32</em>(8), 3471–3483. (<a
href="https://doi.org/10.1109/TNNLS.2020.3011671">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work studies the class of algorithms for learning with side-information that emerges by extending generative models with embedded context-related variables. Using finite mixture models (FMMs) as the prototypical Bayesian network, we show that maximum-likelihood estimation (MLE) of parameters through expectation-maximization (EM) improves over the regular unsupervised case and can approach the performances of supervised learning, despite the absence of any explicit ground-truth data labeling. By direct application of the missing information principle (MIP), the algorithms&#39; performances are proven to range between the conventional supervised and unsupervised MLE extremities proportionally to the information content of the contextual assistance provided. The acquired benefits regard higher estimation precision, smaller standard errors, faster convergence rates, and improved classification accuracy or regression fitness shown in various scenarios while also highlighting important properties and differences among the outlined situations. Applicability is showcased with three real-world unsupervised classification scenarios employing Gaussian mixture models. Importantly, we exemplify the natural extension of this methodology to any type of generative model by deriving an equivalent context-aware algorithm for variational autoencoders (VAs), thus broadening the spectrum of applicability to unsupervised deep learning with artificial neural networks. The latter is contrasted with a neural-symbolic algorithm exploiting side information.},
  archive      = {J_TNNLS},
  author       = {Serafeim Perdikis and Robert Leeb and Ricardo Chavarriaga and José del R. Millán},
  doi          = {10.1109/TNNLS.2020.3011671},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3471-3483},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Context-aware learning for generative models},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Learning to learn adaptive classifier–predictor for
few-shot learning. <em>TNNLS</em>, <em>32</em>(8), 3458–3470. (<a
href="https://doi.org/10.1109/TNNLS.2020.3011526">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot learning aims to learn a well-performing model from a few labeled examples. Recently, quite a few works propose to learn a predictor to directly generate model parameter weights with episodic training strategy of meta-learning and achieve fairly promising performance. However, the predictor in these works is task-agnostic, which means that the predictor cannot adjust to novel tasks in the testing phase. In this article, we propose a novel meta-learning method to learn how to learn task-adaptive classifier-predictor to generate classifier weights for few-shot classification. Specifically, a meta classifier-predictor module, (MPM) is introduced to learn how to adaptively update a task-agnostic classifier-predictor to a task-specialized one on a novel task with a newly proposed center-uniqueness loss function. Compared with previous works, our task-adaptive classifier-predictor can better capture characteristics of each category in a novel task and thus generate a more accurate and effective classifier. Our method is evaluated on two commonly used benchmarks for few-shot classification, i.e., miniImageNet and tieredImageNet. Ablation study verifies the necessity of learning task-adaptive classifier-predictor and the effectiveness of our newly proposed center-uniqueness loss. Moreover, our method achieves the state-of-the-art performance on both benchmarks, thus demonstrating its superiority.},
  archive      = {J_TNNLS},
  author       = {Nan Lai and Meina Kan and Chunrui Han and Xingguang Song and Shiguang Shan},
  doi          = {10.1109/TNNLS.2020.3011526},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3458-3470},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Learning to learn adaptive Classifier–Predictor for few-shot learning},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Achieving fair load balancing by invoking a learning
automata-based two-time-scale separation paradigm. <em>TNNLS</em>,
<em>32</em>(8), 3444–3457. (<a
href="https://doi.org/10.1109/TNNLS.2020.3010888">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we consider the problem of load balancing (LB), but, unlike the approaches that have been proposed earlier, we attempt to resolve the problem in a fair manner (or rather, it would probably be more appropriate to describe it as an ϵ-fair manner because, although the LB can, probably, never be totally fair, we achieve this by being “as close to fair as possible”). The solution that we propose invokes a novel stochastic learning automaton (LA) scheme, so as to attain a distribution of the load to a number of nodes, where the performance level at the different nodes is approximately equal and each user experiences approximately the same Quality of the Service (QoS) irrespective of which node that he/she is connected to. Since the load is dynamically varying, static resource allocation schemes are doomed to underperform. This is further relevant in cloud environments, where we need dynamic approaches because the available resources are unpredictable (or rather, uncertain) by virtue of the shared nature of the resource pool. Furthermore, we prove here that there is a coupling involving LA&#39;s probabilities and the dynamics of the rewards themselves, which renders the environments to be nonstationary. This leads to the emergence of the so-called property of “stochastic diminishing rewards.” Our newly proposed novel LA algorithm ϵ-optimally solves the problem, and this is done by resorting to a two-time-scale-based stochastic learning paradigm. As far as we know, the results presented here are of a pioneering sort, and we are unaware of any comparable results.},
  archive      = {J_TNNLS},
  author       = {Anis Yazidi and Ismail Hassan and Hugo L. Hammer and B. John Oommen},
  doi          = {10.1109/TNNLS.2020.3010888},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3444-3457},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Achieving fair load balancing by invoking a learning automata-based two-time-scale separation paradigm},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Real-time indoor localization for smartphones using
tensor-generative adversarial nets. <em>TNNLS</em>, <em>32</em>(8),
3433–3443. (<a
href="https://doi.org/10.1109/TNNLS.2020.3010724">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-accuracy location awareness in indoor environments is fundamentally important for mobile computing and mobile social networks. However, accurate radio frequency (RF) fingerprint-based localization is challenging due to real-time response requirements, limited RF fingerprint samples, and limited device storage. In this article, we propose a tensor generative adversarial net (Tensor-GAN) scheme for real-time indoor localization, which achieves improvements in terms of localization accuracy and storage consumption. First, with verification on real-world fingerprint data set, we model RF fingerprints as a 3-D low-tubal-rank tensor to effectively capture the multidimensional latent structures. Second, we propose a novel Tensor-GAN that is a three-player game among a regressor, a generator, and a discriminator. We design a tensor completion algorithm for the tubal-sampling pattern as the generator that produces new RF fingerprints as training samples, and the regressor estimates locations for RF fingerprints. Finally, on real-world fingerprint data set, we show that the proposed Tensor-GAN scheme improves localization accuracy from 0.42 m (state-of-the-art methods kNN, DeepFi, and AutoEncoder) to 0.19 m for 80\% of 1639 random testing points. Moreover, we implement a prototype Tensor-GAN that is downloaded as an Android smartphone App, which has a relatively small memory footprint, i.e., 57 KB.},
  archive      = {J_TNNLS},
  author       = {Xiao-Yang Liu and Xiaodong Wang},
  doi          = {10.1109/TNNLS.2020.3010724},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3433-3443},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Real-time indoor localization for smartphones using tensor-generative adversarial nets},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep learning for LiDAR point clouds in autonomous driving:
A review. <em>TNNLS</em>, <em>32</em>(8), 3412–3432. (<a
href="https://doi.org/10.1109/TNNLS.2020.3015992">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, the advancement of deep learning (DL) in discriminative feature learning from 3-D LiDAR data has led to rapid development in the field of autonomous driving. However, automated processing uneven, unstructured, noisy, and massive 3-D point clouds are a challenging and tedious task. In this article, we provide a systematic review of existing compelling DL architectures applied in LiDAR point clouds, detailing for specific tasks in autonomous driving, such as segmentation, detection, and classification. Although several published research articles focus on specific topics in computer vision for autonomous vehicles, to date, no general survey on DL applied in LiDAR point clouds for autonomous vehicles exists. Thus, the goal of this article is to narrow the gap in this topic. More than 140 key contributions in the recent five years are summarized in this survey, including the milestone 3-D deep architectures, the remarkable DL applications in 3-D semantic segmentation, object detection, and classification; specific data sets, evaluation metrics, and the state-of-the-art performance. Finally, we conclude the remaining challenges and future researches.},
  archive      = {J_TNNLS},
  author       = {Ying Li and Lingfei Ma and Zilong Zhong and Fei Liu and Michael A. Chapman and Dongpu Cao and Jonathan Li},
  doi          = {10.1109/TNNLS.2020.3015992},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3412-3432},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Deep learning for LiDAR point clouds in autonomous driving: A review},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). RCoNet: Deformable mutual information maximization and
high-order uncertainty-aware learning for robust COVID-19 detection.
<em>TNNLS</em>, <em>32</em>(8), 3401–3411. (<a
href="https://doi.org/10.1109/TNNLS.2021.3086570">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The novel 2019 Coronavirus (COVID-19) infection has spread worldwide and is currently a major healthcare challenge around the world. Chest computed tomography (CT) and X-ray images have been well recognized to be two effective techniques for clinical COVID-19 disease diagnoses. Due to faster imaging time and considerably lower cost than CT, detecting COVID-19 in chest X-ray (CXR) images is preferred for efficient diagnosis, assessment, and treatment. However, considering the similarity between COVID-19 and pneumonia, CXR samples with deep features distributed near category boundaries are easily misclassified by the hyperplanes learned from limited training data. Moreover, most existing approaches for COVID-19 detection focus on the accuracy of prediction and overlook uncertainty estimation, which is particularly important when dealing with noisy datasets. To alleviate these concerns, we propose a novel deep network named RCoNet k s for robust COVID-19 detection which employs Deformable Mutual Information Maximization (DeIM), Mixed High-order Moment Feature (MHMF), and Multiexpert Uncertainty-aware Learning (MUL). With DeIM, the mutual information (MI) between input data and the corresponding latent representations can be well estimated and maximized to capture compact and disentangled representational characteristics. Meanwhile, MHMF can fully explore the benefits of using high-order statistics and extract discriminative features of complex distributions in medical imaging. Finally, MUL creates multiple parallel dropout networks for each CXR image to evaluate uncertainty and thus prevent performance degradation caused by the noise in the data. The experimental results show that RCoNet k s achieves the state-of-the-art performance on an open-source COVIDx dataset of 15 134 original CXR images across several metrics. Crucially, our method is shown to be more effective than existing methods with the presence of noise in the data.},
  archive      = {J_TNNLS},
  author       = {Shunjie Dong and Qianqian Yang and Yu Fu and Mei Tian and Cheng Zhuo},
  doi          = {10.1109/TNNLS.2021.3086570},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3401-3411},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {RCoNet: Deformable mutual information maximization and high-order uncertainty-aware learning for robust COVID-19 detection},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Neuroadaptive fault-tolerant control under multiple
objective constraints with applications to tire production systems.
<em>TNNLS</em>, <em>32</em>(8), 3391–3400. (<a
href="https://doi.org/10.1109/TNNLS.2020.2967150">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many manufacturing systems not only involve nonlinearities and nonvanishing disturbances but also are subject to actuation failures and multiple yet possibly conflicting objectives, making the underlying control problem interesting and challenging. In this article, we present a neuroadaptive fault-tolerant control solution capable of addressing those factors concurrently. To cope with the multiple objective constraints, we propose a method to accommodate these multiple objectives in such a way that they are all confined in certain range, distinguishing itself from the traditional method that seeks for a common optimum (which might not even exist due to the complicated and conflicting objective requirement) for all the objective functions. By introducing a novel barrier function, we convert the system under multiple constraints into one without constraints, allowing for the nonconstrained control algorithms to be derived accordingly. The system uncertainties and the unknown actuation failures are dealt with by using the deep-rooted information-based method. Furthermore, by utilizing a transformed signal as the initial filter input, we integrate dynamic surface control (DSC) into backstepping design to eliminate the feasibility conditions completely and avoid off-line parameter optimization. It is shown that, with the proposed neuroadaptive control scheme, not only stable system operation is maintained but also each objective function is confined within the prespecified region, which could be asymmetric and time-varying. The effectiveness of the algorithm is validated via simulation on speed regulation of extruding machine in tire production lines.},
  archive      = {J_TNNLS},
  author       = {Qian Cui and Yujuan Wang and Yongduan Song},
  doi          = {10.1109/TNNLS.2020.2967150},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3391-3400},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Neuroadaptive fault-tolerant control under multiple objective constraints with applications to tire production systems},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Iterative learning model predictive control based on
iterative data-driven modeling. <em>TNNLS</em>, <em>32</em>(8),
3377–3390. (<a
href="https://doi.org/10.1109/TNNLS.2020.3016295">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Iterative learning model predictive control (ILMPC) has been recognized as an effective approach to realize high-precision tracking for batch processes with repetitive nature because of its excellent learning ability and closed-loop stability property. However, as a model-based strategy, ILMPC suffers from the unavailability of accurate first principal model in many complex nonlinear batch systems. On account of the abundant process data, nonlinear dynamics of batch systems can be identified precisely along the trials by neural network (NN), making it enforceable to design a data-driven ILMPC. In this article, by using a control-affine feedforward neural network (CAFNN), the features in the process data of the former batch are extracted to form a nonlinear affine model for the controller design in the current batch. Based on the CAFNN model, the ILMPC is formulated in a tube framework to attenuate the influence of modeling errors and track the reference trajectory with sustained accuracy. Due to the control-affine structure, the gradients of the objective function can be analytically computed offline, so as to improve the online computational efficiency and optimization feasibility of the tube ILMPC. The robust stability and the convergence of the data-driven ILMPC system are analyzed theoretically. The simulation on a typical batch reactor verifies the effectiveness of the proposed control method.},
  archive      = {J_TNNLS},
  author       = {Lele Ma and Xiangjie Liu and Xiaobing Kong and Kwang Y. Lee},
  doi          = {10.1109/TNNLS.2020.3016295},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3377-3390},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Iterative learning model predictive control based on iterative data-driven modeling},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Intelligent time-scale operator-splitting integration for
chemical reaction systems. <em>TNNLS</em>, <em>32</em>(8), 3366–3376.
(<a href="https://doi.org/10.1109/TNNLS.2020.3006348">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The wide range of time scales in chemical reaction systems has become an important problem in reactive flow simulations. This work proposes an intelligent time-scale operator-splitting (OS) chemistry integration method, which is effective in reduction of numerical stiffness and model complexity. Different from most existing publications, a pretrained backpropagation neural network is used to identify the slow and fast reactions and detect the sources of model stiffness on the fly, which replaces the expensive eigendecomposition of Jacobian matrix. With the fast–slow decomposition, the chemical source term can be represented as the sum of a stiff part and a nonstiff part. A stable time-scale OS integration is performed to solve the stiff chemical ordinary differential equations, which balances the computational cost with accuracy. In the simulation, a favorable comparison of the proposed integration method with the existing ODE solvers, such as implicit Euler, explicit Euler, and Runge–Kutta, is included to show its effectiveness and merits.},
  archive      = {J_TNNLS},
  author       = {Yu Zhang and Wenli Du},
  doi          = {10.1109/TNNLS.2020.3006348},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3366-3376},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Intelligent time-scale operator-splitting integration for chemical reaction systems},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Local–global modeling and distributed computing framework
for nonlinear plant-wide process monitoring with industrial big data.
<em>TNNLS</em>, <em>32</em>(8), 3355–3365. (<a
href="https://doi.org/10.1109/TNNLS.2020.2985223">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Industrial big data and complex process nonlinearity have introduced new challenges in plant-wide process monitoring. This article proposes a local-global modeling and distributed computing framework to achieve efficient fault detection and isolation for nonlinear plant-wide processes. First, a stacked autoencoder is used to extract dominant representations of each local process unit and establish the local inner monitor. Second, mutual information (MI) is used to determine the neighborhood variables of a local unit. Afterward, a joint representation learning is then performed between the local unit and the neighborhood variables to extract the outer-related representations and establish the outer-related monitor for the local unit. Finally, the outer-related representations from all process units are used to establish global monitoring systems. Given that the modeling of each unit can be performed individually, the computation process can be efficiently completed with different CPUs. The proposed modeling and monitoring method is applied to the Tennessee Eastman (TE) and laboratory-scale glycerol distillation processes to demonstrate the feasibility of the method.},
  archive      = {J_TNNLS},
  author       = {Qingchao Jiang and Shifu Yan and Hui Cheng and Xuefeng Yan},
  doi          = {10.1109/TNNLS.2020.2985223},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3355-3365},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Local–Global modeling and distributed computing framework for nonlinear plant-wide process monitoring with industrial big data},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Spatiotemporal graph convolution multifusion network for
urban vehicle emission prediction. <em>TNNLS</em>, <em>32</em>(8),
3342–3354. (<a
href="https://doi.org/10.1109/TNNLS.2020.3008702">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Urban vehicle emission prediction can help the regulation of vehicle pollution and traffic control. However, it is hard to predict the spatiotemporal variation of vehicle emission because of the spatial interactions and temporal correlations between different road segments as well as the high nonlinearity and complexity of vehicle emission variation. The existing methods solve the problem by splitting the region into standard segments or grids based on conventional deep learning methods, without considering that urban vehicle emission varies by graph-structured traffic road network and depends on many complex external environment factors. To address these issues, a spatiotemporal graph convolution multifusion network (ST-MFGCN) is proposed to leverage the graph structural properties as the inherent connectivity of road network for urban vehicle emission prediction, which can capture the vehicle emission spatiotemporal variation patterns and learn the effects of complex environmental factors. The proposed model consists of three parts: 1) a spatiotemporal graph convolution module to capture spatiotemporal dependencies by merging closeness, period, and trend sequences with temporal convolution as well as graph convolution is introduced to model the spatial dependencies; 2) an external factor component to divide multisource external factors into global and individual external features; and 3) a general fusion component to merge the spatiotemporal patterns and the external features as well as fit the mutation of emission measurement data by multifusion strategy. Finally, the proposed model is evaluated on the practical monitoring data of vehicle emission data in Hefei, and the results demonstrate that our proposed model can predict regional vehicle emissions effectively.},
  archive      = {J_TNNLS},
  author       = {Zhenyi Xu and Yu Kang and Yang Cao and Zhijun Li},
  doi          = {10.1109/TNNLS.2020.3008702},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3342-3354},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Spatiotemporal graph convolution multifusion network for urban vehicle emission prediction},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hierarchical quality monitoring for large-scale industrial
plants with big process data. <em>TNNLS</em>, <em>32</em>(8), 3330–3341.
(<a href="https://doi.org/10.1109/TNNLS.2019.2958184">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For large-scale industrial plants, quality-related process monitoring is challenging because of the complex features of multiunit, multimode, high-dimension data. Hence, a hierarchical quality monitoring (HQM) algorithm based on the distributed parallel semisupervised Gaussian mixture model (dp-S 2 GMM) is proposed in this article. In HQM, a large-scale process is first decomposed into a group of unit blocks according to the process structure. Subsequently, in each block, a quality regression model with multimode big process data is built using the dp-S 2 GMM, which is derived from a scalable stochastic variational inference semisupervised GMM (SVI-S 2 GMM). With the regression model, a hierarchical fault detection and diagnosis scheme in both quality-related and quality-unrelated subspaces is proposed from the variable level, block level to plant-wide level. Finally, an industrial case study on the Tennessee Eastman process demonstrates the feasibility and effectiveness of the proposed HQM algorithm.},
  archive      = {J_TNNLS},
  author       = {Le Yao and Weiming Shao and Zhiqiang Ge},
  doi          = {10.1109/TNNLS.2019.2958184},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3330-3341},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Hierarchical quality monitoring for large-scale industrial plants with big process data},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Data-driven intelligent warning method for membrane fouling.
<em>TNNLS</em>, <em>32</em>(8), 3318–3329. (<a
href="https://doi.org/10.1109/TNNLS.2020.3041293">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Membrane fouling has become a serious issue in membrane bioreactor (MBR) and may destroy the operation of the wastewater treatment process (WWTP). The goal of this article is to design a data-driven intelligent warning method for warning the future events of membrane fouling in MBR. The main novelties of the proposed method are threefold. First, a soft-computing model, based on the recurrent fuzzy neural network (RFNN), was proposed to identify the real-time values of membrane permeability. Second, a multistep prediction strategy was designed to predict the multiple outputs of membrane permeability accurately by decreasing the error accumulation over the predictive horizon. Third, a warning detection algorithm, using the state comprehensive evaluation (SCE) method, was developed to evaluate the pollution levels of MBR. Finally, the proposed method was inserted into a warning system to complete the predicting and warning missions and further tested in the real plants to evaluate its efficiency and effectiveness. Experimental results have verified the benefits of the proposed method.},
  archive      = {J_TNNLS},
  author       = {Xiaolong Wu and Honggui Han and Junfei Qiao},
  doi          = {10.1109/TNNLS.2020.3041293},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3318-3329},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Data-driven intelligent warning method for membrane fouling},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dual attention-based encoder–decoder: A customized
sequence-to-sequence learning for soft sensor development.
<em>TNNLS</em>, <em>32</em>(8), 3306–3317. (<a
href="https://doi.org/10.1109/TNNLS.2020.3015929">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Soft sensor techniques have been applied to predict the hard-to-measure quality variables based on the easy-to-measure process variables in industry scenarios. Since the products are usually produced with prearranged processing orders, the sequential dependence among different variables can be important for the process modeling. To use this property, a dual attention-based encoder-decoder is developed in this article, which presents a customized sequence-to-sequence learning for soft sensor. We reveal that different quality variables in the same process are sequentially dependent on each other and the process variables are natural time sequences. Hence, the encoder-decoder is constructed to explicitly exploit the sequential information of both the input, that is, the process variables, and the output, that is, the quality variables. The encoder and decoder modules are specified as the long short-term memory network. In addition, since different process variables and time points impose different effects on the quality variables, a dual attention mechanism is embedded into the encoder-decoder to concurrently search the quality-related process variables and time points for a fine-grained quality prediction. Comprehensive experiments are performed based on a real cigarette production process and a benchmark multiphase flow process, which illustrate the effectiveness of the proposed encoder-decoder and its sequence to sequence learning for soft sensor.},
  archive      = {J_TNNLS},
  author       = {Liangjun Feng and Chunhui Zhao and Youxian Sun},
  doi          = {10.1109/TNNLS.2020.3015929},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3306-3317},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Dual attention-based Encoder–Decoder: A customized sequence-to-sequence learning for soft sensor development},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A layer-wise data augmentation strategy for deep learning
networks and its soft sensor application in an industrial hydrocracking
process. <em>TNNLS</em>, <em>32</em>(8), 3296–3305. (<a
href="https://doi.org/10.1109/TNNLS.2019.2951708">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In industrial processes, inferential sensors have been extensively applied for prediction of quality variables that are difficult to measure online directly by hard sensors. Deep learning is a recently developed technique for feature representation of complex data, which has great potentials in soft sensor modeling. However, it often needs a large number of representative data to train and obtain a good deep network. Moreover, layer-wise pretraining often causes information loss and generalization degradation of high hidden layers. This greatly limits the implementation and application of deep learning networks in industrial processes. In this article, a layer-wise data augmentation (LWDA) strategy is proposed for the pretraining of deep learning networks and soft sensor modeling. In particular, the LWDA-based stacked autoencoder (LWDA-SAE) is developed in detail. Finally, the proposed LWDA-SAE model is applied to predict the 10\% and 50\% boiling points of the aviation kerosene in an industrial hydrocracking process. The results show that the LWDA-SAE-based soft sensor is superior to multilayer perceptron, traditional SAE, and the SAE with data augmentation only for its input layer (IDA-SAE). Moreover, LWDA-SAE can converge at a faster speed with a lower learning error than the other methods.},
  archive      = {J_TNNLS},
  author       = {Xiaofeng Yuan and Chen Ou and Yalin Wang and Chunhua Yang and Weihua Gui},
  doi          = {10.1109/TNNLS.2019.2951708},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3296-3305},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A layer-wise data augmentation strategy for deep learning networks and its soft sensor application in an industrial hydrocracking process},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Guest editorial special issue on deep integration of
artificial intelligence and data science for process manufacturing.
<em>TNNLS</em>, <em>32</em>(8), 3294–3295. (<a
href="https://doi.org/10.1109/TNNLS.2021.3092896">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Process manufacturing serves as the pillar of the continuous manufacturing industry such as oil, gas, chemicals, nonferrous metals, iron, and steel, and thus is closely related to almost every aspect of human life. On the one hand, in order to meet several urgent but challenging demands of increasing profits, reducing materials consumption, enhancing safety, and protecting the environment, it is necessary to facilitate the development of process manufacturing with the usage of some novel and advanced techniques such as artificial intelligence (AI) and computation intelligence (CI). On the other hand, with the increasing scale of process manufacturing, another challenge is how to effectively deal with a huge amount of industrial big data in the process industry for environmental perception, modelling, optimization, decision-making, autonomous intelligent control, fault detection, and risk analysis. Therefore, it is of fundamental importance to deeply integrate AI, CI, and data sciences to achieve accurate control and optimal decision-making for process industries.},
  archive      = {J_TNNLS},
  author       = {Feng Qian and Yaochu Jin and S. Joe Qin and Kai Sundmacher},
  doi          = {10.1109/TNNLS.2021.3092896},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3294-3295},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Guest editorial special issue on deep integration of artificial intelligence and data science for process manufacturing},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021f). IEEE computational intelligence society information.
<em>TNNLS</em>, <em>32</em>(7), C3. (<a
href="https://doi.org/10.1109/TNNLS.2021.3089403">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Provides a listing of current committee members and society officers.},
  archive      = {J_TNNLS},
  doi          = {10.1109/TNNLS.2021.3089403},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {C3},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {IEEE computational intelligence society information},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Human-in-the-loop low-shot learning. <em>TNNLS</em>,
<em>32</em>(7), 3287–3292. (<a
href="https://doi.org/10.1109/TNNLS.2020.3011559">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a human-in-the-loop scenario in the context of low-shot learning. Our approach was inspired by the fact that the viability of samples in novel categories cannot be sufficiently reflected by those limited observations. Some heterogeneous samples that are quite different from existing labeled novel data can inevitably emerge in the testing phase. To this end, we consider augmenting an uncertainty assessment module into low-shot learning system to account into the disturbance of those out-of-distribution (OOD) samples. Once detected, these OOD samples are passed to human beings for active labeling. Due to the discrete nature of this uncertainty assessment process, the whole Human-In-the-Loop Low-shot (HILL) learning framework is not end-to-end trainable. We hence revisited the learning system from the aspect of reinforcement learning and introduced the REINFORCE algorithm to optimize model parameters via policy gradient. The whole system gains noticeable improvements over existing low-shot learning approaches.},
  archive      = {J_TNNLS},
  author       = {Sen Wan and Yimin Hou and Feng Bao and Zhiquan Ren and Yunfeng Dong and Qionghai Dai and Yue Deng},
  doi          = {10.1109/TNNLS.2020.3011559},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {3287-3292},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Human-in-the-loop low-shot learning},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Intrinsic plasticity-based neuroadptive control with both
weights and excitability tuning. <em>TNNLS</em>, <em>32</em>(7),
3282–3286. (<a
href="https://doi.org/10.1109/TNNLS.2020.3011044">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This brief presents an intrinsic plasticity (IP)-driven neural-network-based tracking control approach for a class of nonlinear uncertain systems. Inspired by the neural plasticity mechanism of individual neuron in nervous systems, a learning rule referred to as IP is employed for adjusting the radial basis functions (RBFs), resulting in a neural network (NN) with both weights and excitability tuning, based on which neuroadaptive tracking control algorithms for multiple-input–multiple-output (MIMO) uncertain systems are derived. Both theoretical analysis and numerical simulation confirm the effectiveness of the proposed method.},
  archive      = {J_TNNLS},
  author       = {Qing Chen and Anguo Zhang and Yongduan Song},
  doi          = {10.1109/TNNLS.2020.3011044},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {3282-3286},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Intrinsic plasticity-based neuroadptive control with both weights and excitability tuning},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Output feedback q-learning for linear-quadratic
discrete-time finite-horizon control problems. <em>TNNLS</em>,
<em>32</em>(7), 3274–3281. (<a
href="https://doi.org/10.1109/TNNLS.2020.3010304">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An algorithm is proposed to determine output feedback policies that solve finite-horizon linear-quadratic (LQ) optimal control problems without requiring knowledge of the system dynamical matrices. To reach this goal, the Q-factors arising from finite-horizon LQ problems are first characterized in the state feedback case. It is then shown how they can be parameterized as functions of the input-output vectors. A procedure is then proposed for estimating these functions from input/output data and using these estimates for computing the optimal control via the measured inputs and outputs.},
  archive      = {J_TNNLS},
  author       = {Giuseppe C. Calafiore and Corrado Possieri},
  doi          = {10.1109/TNNLS.2020.3010304},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {3274-3281},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Output feedback Q-learning for linear-quadratic discrete-time finite-horizon control problems},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Neural network-based finite-time command filtering control
for switched nonlinear systems with backlash-like hysteresis.
<em>TNNLS</em>, <em>32</em>(7), 3268–3273. (<a
href="https://doi.org/10.1109/TNNLS.2020.3009871">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This brief is concerned with the finite-time tracking control problem for switched nonlinear systems with arbitrary switching and hysteresis input. The neural networks are utilized to cope with the unknown nonlinear functions. To present the finite-time adaptive neural control strategy, a new criterion of practical finite-time stability is first developed. Compared with the traditional command filter technique, the main advantage is that the improved error compensation signals are designed to remove the filtered error and the Levant differentiators are introduced to approximate the derivative of the virtual control signal. The finite-time adaptive neural controller is proposed via the new command filter backstepping technique, and the tracking error converges to a small neighborhood of the origin in finite time. Finally, the simulation results are provided to testify the validity of the proposed method.},
  archive      = {J_TNNLS},
  author       = {Cheng Fu and Qing-Guo Wang and Jinpeng Yu and Chong Lin},
  doi          = {10.1109/TNNLS.2020.3009871},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {3268-3273},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Neural network-based finite-time command filtering control for switched nonlinear systems with backlash-like hysteresis},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Competitive normalized least-squares regression.
<em>TNNLS</em>, <em>32</em>(7), 3262–3267. (<a
href="https://doi.org/10.1109/TNNLS.2020.3009777">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online learning has witnessed an increasing interest over the recent past due to its low computational requirements and its relevance to a broad range of streaming applications. In this brief, we focus on online regularized regression. We propose a novel efficient online regression algorithm, called online normalized least-squares (ONLS). We perform theoretical analysis by comparing the total loss of ONLS against the normalized gradient descent (NGD) algorithm and the best off-line LS predictor. We show, in particular, that ONLS allows for a better bias-variance tradeoff than those state-of-the-art gradient descent-based LS algorithms as well as a better control on the level of shrinkage of the features toward the null. Finally, we conduct an empirical study to illustrate the great performance of ONLS against some state-of-the-art algorithms using real-world data.},
  archive      = {J_TNNLS},
  author       = {Waqas Jamil and Abdelhamid Bouchachia},
  doi          = {10.1109/TNNLS.2020.3009777},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {3262-3267},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Competitive normalized least-squares regression},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptive neural network-based filter design for nonlinear
systems with multiple constraints. <em>TNNLS</em>, <em>32</em>(7),
3256–3261. (<a
href="https://doi.org/10.1109/TNNLS.2020.3009391">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Filter design for nonlinear systems, especially time delayed nonlinear systems, has always been an important and challenging problem. This brief investigates the filter design problem of nonlinear systems with multiple constraints: time delay, actuator, and sensor faults, and a new adaptive neural network-based filter design method is proposed. Comparing with the existing works where there is a shortcoming that the designed filters contain unknown time delay(s), the design method proposed in this brief overcomes the shortcoming and only the estimation of the unknown time delay exists in the filter. Furthermore, not only the system states can be estimated, but also the unknown time delay with actuator and sensor faults can be estimated in this brief. Finally, simulation results are given to show the effectiveness of the proposed new design method.},
  archive      = {J_TNNLS},
  author       = {Qikun Shen and Peng Shi and Ramesh K. Agarwal and Yan Shi},
  doi          = {10.1109/TNNLS.2020.3009391},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {3256-3261},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive neural network-based filter design for nonlinear systems with multiple constraints},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Frames learned by prime convolution layers in a deep
learning framework. <em>TNNLS</em>, <em>32</em>(7), 3247–3255. (<a
href="https://doi.org/10.1109/TNNLS.2020.3009059">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This brief addresses understandability of modern machine learning networks with respect to the statistical properties of their convolution layers. It proposes a set of tools for categorizing a convolution layer in terms of kernel property (meanlet, differencelet, or distrotlet) or kernel sequence property (frame spectra and intralayer correlation matrix). These tools are expected to be relevant for determining the generalization capabilities of a convolutional neural network. In particular, this brief highlights that the less frequency penalizing network among AlexNet, GoogleNet, RESNET101, and VGG19 is the more relevant one in terms of solutions for low-level ice-sheet feature enhancement.},
  archive      = {J_TNNLS},
  author       = {Abdourrahmane M. Atto and Rosie R. Bisset and Emmanuel Trouvé},
  doi          = {10.1109/TNNLS.2020.3009059},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {3247-3255},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Frames learned by prime convolution layers in a deep learning framework},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Extended dissipativity analysis for markovian jump neural
networks via double-integral-based delay-product-type lyapunov
functional. <em>TNNLS</em>, <em>32</em>(7), 3240–3246. (<a
href="https://doi.org/10.1109/TNNLS.2020.3008691">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This brief studies the problem of extended dissipativity analysis for the Markovian jump neural networks (MJNNs) with time-varying delay. A double-integral-based delay-product-type (DIDPT) Lyapunov functional is first constructed in this brief, which makes full use of the information of time delay. Moreover, some unnecessary constraints on the system structure are removed, which leads to more general results. A numerical example is employed to illustrate the advantages of the proposed method.},
  archive      = {J_TNNLS},
  author       = {Yufeng Tian and Zhanshan Wang},
  doi          = {10.1109/TNNLS.2020.3008691},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {3240-3246},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Extended dissipativity analysis for markovian jump neural networks via double-integral-based delay-product-type lyapunov functional},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptive synchronization of fractional-order output-coupling
neural networks via quantized output control. <em>TNNLS</em>,
<em>32</em>(7), 3230–3239. (<a
href="https://doi.org/10.1109/TNNLS.2020.3013619">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article focuses on the adaptive synchronization for a class of fractional-order coupled neural networks (FCNNs) with output coupling. The model is new for output coupling item in the FCNNs that treat FCNNs with state coupling as its particular case. Novel adaptive output controllers with logarithm quantization are designed to cope with the stability of the fractional-order error systems for the first attempt, which is also an effective way to synchronize fractional-order complex networks. Based on fractional-order Lyapunov functionals and linear matrix inequalities (LMIs) method, sufficient conditions rather than algebraic conditions are built to realize the synchronization of FCNNs with output coupling. A numerical simulation is put forward to substantiate the applicability of our results.},
  archive      = {J_TNNLS},
  author       = {Haibo Bao and Ju H. Park and Jinde Cao},
  doi          = {10.1109/TNNLS.2020.3013619},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {3230-3239},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive synchronization of fractional-order output-coupling neural networks via quantized output control},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Distributed cooperative learning control of uncertain
multiagent systems with prescribed performance and preserved
connectivity. <em>TNNLS</em>, <em>32</em>(7), 3217–3229. (<a
href="https://doi.org/10.1109/TNNLS.2020.3010690">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For an uncertain multiagent system, distributed cooperative learning control exerting the learning capability of the control system in a cooperative way is one of the most important and challenging issues. This article aims to address this issue for an uncertain high-order nonlinear multiagent system with guaranteed transient performance and preserved initial connectivity under an undirected and static communication topology. The considered multiagent system has an identical structure and the uncertain agent dynamics are estimated by localized radial basis function (RBF) neural networks (NNs) in a cooperative way. The NN weight estimates are rigorously proven to converge to small neighborhoods of their common optimal values along the union of all agents&#39; trajectories by a deterministic learning theory. Consequently, the associated uncertain dynamics can be locally accurately identified and can be stored and represented by constant RBF networks. Using the stored knowledge on identified system dynamics, an experience-based distributed controller is proposed to improve the control performance and reduce the computational burden. The theoretical results are demonstrated on an application to the formation control of a group of unmanned surface vehicles.},
  archive      = {J_TNNLS},
  author       = {Shi-Lu Dai and Shude He and Yufei Ma and Chengzhi Yuan},
  doi          = {10.1109/TNNLS.2020.3010690},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {3217-3229},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Distributed cooperative learning control of uncertain multiagent systems with prescribed performance and preserved connectivity},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Incremental concept learning via online generative memory
recall. <em>TNNLS</em>, <em>32</em>(7), 3206–3216. (<a
href="https://doi.org/10.1109/TNNLS.2020.3010581">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ability to learn more concepts from incrementally arriving data over time is essential for the development of a lifelong learning system. However, deep neural networks often suffer from forgetting previously learned concepts when continually learning new concepts, which is known as the catastrophic forgetting problem. The main reason for catastrophic forgetting is that past concept data are not available, and neural weights are changed during incrementally learning new concepts. In this article, we propose an incremental concept learning framework that includes two components, namely, ICLNet and RecallNet. ICLNet, which consists of a trainable feature extractor and a dynamic concept memory matrix, aims to learn new concepts incrementally. We propose a concept-contrastive loss to alleviate the magnitude of neural weight changes and mitigate the catastrophic forgetting problems. RecallNet aims to consolidate old concepts memory and recall pseudo samples, whereas ICLNet learns new concepts. We propose a balanced online memory recall strategy to reduce the information loss of old concept memory. We evaluate the proposed approach on the MNIST, Fashion-MNIST, and SVHN data sets and compare it with other pseudorehearsal-based approaches. Extensive experiments demonstrate the effectiveness of our approach.},
  archive      = {J_TNNLS},
  author       = {Huaiyu Li and Weiming Dong and Bao-Gang Hu},
  doi          = {10.1109/TNNLS.2020.3010581},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {3206-3216},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Incremental concept learning via online generative memory recall},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). A novel adaptive NN prescribed performance control for
stochastic nonlinear systems. <em>TNNLS</em>, <em>32</em>(7), 3196–3205.
(<a href="https://doi.org/10.1109/TNNLS.2020.3010333">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates the problem of neural network (NN)-based adaptive backstepping control design for stochastic nonlinear systems with unmodeled dynamics in finite-time prescribed performance. NNs are used to study the uncertain control plants, and the problem of unmodeled dynamics is tackled by the combination of the changing supply function and the dynamical signal function methods. The outstanding contribution of this article is that based on the finite-time performance function (FTPF), a modified finite-time adaptive NN control design strategy is proposed, which makes the controller design simpler. Eventually, by using the Itô&#39;s differential lemma, the backstepping recursive design technique, and the FTPFs, a novel adaptive prescribed performance tracking control scheme is presented, which can guarantee that all the variables in the control system are bounded in probability, and the tracking error can converge to a specified performance range in the finite time. Finally, both numerical simulation and applied simulation examples are provided to verify the effectiveness and applicability of the proposed method.},
  archive      = {J_TNNLS},
  author       = {Shuai Sui and C. L. Philip Chen and Shaocheng Tong},
  doi          = {10.1109/TNNLS.2020.3010333},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {3196-3205},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A novel adaptive NN prescribed performance control for stochastic nonlinear systems},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A universal framework for learning the elliptical mixture
model. <em>TNNLS</em>, <em>32</em>(7), 3181–3195. (<a
href="https://doi.org/10.1109/TNNLS.2020.3010198">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mixture modeling using elliptical distributions promises enhanced robustness, flexibility, and stability over the widely employed Gaussian mixture model (GMM). However, existing studies based on the elliptical mixture model (EMM) are restricted to several specific types of elliptical probability density functions, which are not supported by general solutions or systematic analysis frameworks; this significantly limits the rigor in the design and power of EMMs in applications. To this end, we propose a novel general framework for estimating and analyzing the EMMs, achieved through the Riemannian manifold optimization. First, we investigate the relationships between Riemannian manifolds and elliptical distributions, and the so established connection between the original manifold and a reformulated one indicates a mismatch between these manifolds, a major cause of failure of the existing optimization for solving general EMMs. We next propose a universal solver that is based on the optimization of a redesigned cost and prove the existence of the same optimum as in the original problem; this is achieved in a simple, fast and stable way. We further calculate the influence functions of the EMM as theoretical bounds to quantify robustness to outliers. Comprehensive numerical results demonstrate the ability of the proposed framework to accommodate EMMs with different properties of individual functions in a stable way and with fast convergence speed. Finally, the enhanced robustness and flexibility of the proposed framework over the standard GMM are demonstrated both analytically and through comprehensive simulations.},
  archive      = {J_TNNLS},
  author       = {Shengxi Li and Zeyang Yu and Danilo Mandic},
  doi          = {10.1109/TNNLS.2020.3010198},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {3181-3195},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A universal framework for learning the elliptical mixture model},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Joint optimization for pairwise constraint propagation.
<em>TNNLS</em>, <em>32</em>(7), 3168–3180. (<a
href="https://doi.org/10.1109/TNNLS.2020.3009953">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Constrained spectral clustering (SC) based on pairwise constraint propagation has attracted much attention due to the good performance. All the existing methods could be generally cast as the following two steps, i.e., a small number of pairwise constraints are first propagated to the whole data under the guidance of a predefined affinity matrix, and the affinity matrix is then refined in accordance with the resulting propagation and finally adopted for SC. Such a stepwise manner, however, overlooks the fact that the two steps indeed depend on each other, i.e., the two steps form a “chicken-egg” problem, leading to suboptimal performance. To this end, we propose a joint PCP model for constrained SC by simultaneously learning a propagation matrix and an affinity matrix. Especially, it is formulated as a bounded symmetric graph regularized low-rank matrix completion problem. We also show that the optimized affinity matrix by our model exhibits an ideal appearance under some conditions. Extensive experimental results in terms of constrained SC, semisupervised classification, and propagation behavior validate the superior performance of our model compared with state-of-the-art methods.},
  archive      = {J_TNNLS},
  author       = {Yuheng Jia and Wenhui Wu and Ran Wang and Junhui Hou and Sam Kwong},
  doi          = {10.1109/TNNLS.2020.3009953},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {3168-3180},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Joint optimization for pairwise constraint propagation},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). GBDT-MO: Gradient-boosted decision trees for multiple
outputs. <em>TNNLS</em>, <em>32</em>(7), 3156–3167. (<a
href="https://doi.org/10.1109/TNNLS.2020.3009776">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gradient-boosted decision trees (GBDTs) are widely used in machine learning, and the output of current GBDT implementations is a single variable. When there are multiple outputs, GBDT constructs multiple trees corresponding to the output variables. The correlations between variables are ignored by such a strategy causing redundancy of the learned tree structures. In this article, we propose a general method to learn GBDT for multiple outputs, called GBDT-MO. Each leaf of GBDT-MO constructs predictions of all variables or a subset of automatically selected variables. This is achieved by considering the summation of objective gains over all output variables. Moreover, we extend histogram approximation into the multiple-output case to speed up training. Various experiments on synthetic and real data sets verify that GBDT-MO achieves outstanding performance in terms of accuracy, training speed, and inference speed.},
  archive      = {J_TNNLS},
  author       = {Zhendong Zhang and Cheolkon Jung},
  doi          = {10.1109/TNNLS.2020.3009776},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {3156-3167},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {GBDT-MO: Gradient-boosted decision trees for multiple outputs},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Reconstructing quantum states with quantum reservoir
networks. <em>TNNLS</em>, <em>32</em>(7), 3148–3155. (<a
href="https://doi.org/10.1109/TNNLS.2020.3009716">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reconstructing quantum states is an important task for various emerging quantum technologies. The process of reconstructing the density matrix of a quantum state is known as quantum state tomography. Conventionally, tomography of arbitrary quantum states is challenging as the paradigm of efficient protocols has remained in applying specific techniques for different types of quantum states. Here, we introduce a quantum state tomography platform based on the framework of reservoir computing. It forms a quantum neural network and operates as a comprehensive device for reconstructing an arbitrary quantum state (finite-dimensional or continuous variable). This is achieved with only measuring the average occupation numbers in a single physical setup, without the need of any knowledge of optimum measurement basis or correlation measurements.},
  archive      = {J_TNNLS},
  author       = {Sanjib Ghosh and Andrzej Opala and Michał Matuszewski and Tomasz Paterek and Timothy C. H. Liew},
  doi          = {10.1109/TNNLS.2020.3009716},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {3148-3155},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Reconstructing quantum states with quantum reservoir networks},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SEAL: Semisupervised adversarial active learning on
attributed graphs. <em>TNNLS</em>, <em>32</em>(7), 3136–3147. (<a
href="https://doi.org/10.1109/TNNLS.2020.3009682">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Active learning (AL) on attributed graphs has received increasing attention with the prevalence of graph-structured data. Although AL has been widely studied for alleviating label sparsity issues with the conventional nonrelational data, how to make it effective over attributed graphs remains an open research question. Existing AL algorithms on node classification attempt to reuse the classic AL query strategies designed for nonrelational data. However, they suffer from two major limitations. First, different AL query strategies calculated in distinct scoring spaces are often naively combined to determine which nodes to be labeled. Second, the AL query engine and the learning of the classifier are treated as two separating processes, resulting in unsatisfactory performance. In this article, we propose a SEmisupervised Adversarial active Learning (SEAL) framework on attributed graphs, which fully leverages the representation power of deep neural networks and devises a novel AL query strategy for node classification in an adversarial way. Our framework learns two adversarial components; a graph embedding network that encodes both the unlabeled and labeled nodes into a common latent space, expecting to trick the discriminator to regard all nodes as already labeled, and a semisupervised discriminator network that distinguishes the unlabeled from the existing labeled nodes. The divergence score, generated by the discriminator in a unified latent space, serves as the informativeness measure to actively select the most informative node to be labeled by an oracle. The two adversarial components form a closed loop to mutually and simultaneously reinforce each other toward enhancing the AL performance. Extensive experiments on real-world networks validate the effectiveness of the SEAL framework with superior performance improvements to state-of-the-art baselines on node classification tasks.},
  archive      = {J_TNNLS},
  author       = {Yayong Li and Jie Yin and Ling Chen},
  doi          = {10.1109/TNNLS.2020.3009682},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {3136-3147},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {SEAL: Semisupervised adversarial active learning on attributed graphs},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Half-quadratic minimization for unsupervised feature
selection on incomplete data. <em>TNNLS</em>, <em>32</em>(7), 3122–3135.
(<a href="https://doi.org/10.1109/TNNLS.2020.3009632">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised feature selection (UFS) is a popular technique of reducing the dimensions of high-dimensional data. Previous UFS methods were often designed with the assumption that the whole information in the data set is observed. However, incomplete data sets that contain unobserved information can be often found in real applications, especially in industry. Thus, these existing UFS methods have a limitation on conducting feature selection on incomplete data. On the other hand, most existing UFS methods did not consider the sample importance for feature selection, i.e., different samples have various importance. As a result, the constructed UFS models easily suffer from the influence of outliers. This article investigates a new UFS method for conducting UFS on incomplete data sets to investigate the abovementioned issues. Specifically, the proposed method deals with unobserved information by using an indicator matrix to filter it out the process of feature selection and reduces the influence of outliers by employing the half-quadratic minimization technique to automatically assigning outliers with small or even zero weights and important samples with large weights. This article further designs an alternative optimization strategy to optimize the proposed objective function as well as theoretically and experimentally prove the convergence of the proposed optimization strategy. Experimental results on both real and synthetic incomplete data sets verified the effectiveness of the proposed method compared with previous methods, in terms of clustering performance on the low-dimensional space of the high-dimensional data.},
  archive      = {J_TNNLS},
  author       = {Heng Tao Shen and Yonghua Zhu and Wei Zheng and Xiaofeng Zhu},
  doi          = {10.1109/TNNLS.2020.3009632},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {3122-3135},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Half-quadratic minimization for unsupervised feature selection on incomplete data},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Distilling ordinal relation and dark knowledge for facial
age estimation. <em>TNNLS</em>, <em>32</em>(7), 3108–3121. (<a
href="https://doi.org/10.1109/TNNLS.2020.3009523">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose a knowledge distillation approach with two teachers for facial age estimation. Due to the nonstationary patterns of the facial-aging process, the relative order of age labels provides more reliable information than exact age values for facial age estimation. Thus, the first teacher is a novel ranking method capturing the ordinal relation among age labels. Especially, it formulates the ordinal relation learning as a task of recovering the original ordered sequences from shuffled ones. The second teacher adopts the same model as the student that treats facial age estimation as a multiclass classification task. The proposed method leverages the intermediate representations learned by the first teacher and the softened outputs of the second teacher as supervisory signals to improve the training procedure and final performance of the compact student for facial age estimation. Hence, the proposed knowledge distillation approach is capable of distilling the ordinal knowledge from the ranking model and the dark knowledge from the multiclass classification model into a compact student, which facilitates the implementation of facial age estimation on platforms with limited memory and computation resources, such as mobile and embedded devices. Extensive experiments involving several famous data sets for age estimation have demonstrated the superior performance of our proposed method over several existing state-of-the-art methods.},
  archive      = {J_TNNLS},
  author       = {Qilu Zhao and Junyu Dong and Hui Yu and Sheng Chen},
  doi          = {10.1109/TNNLS.2020.3009523},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {3108-3121},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Distilling ordinal relation and dark knowledge for facial age estimation},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Simplifying complex network stability analysis via
hierarchical node aggregation and optimal periodic control.
<em>TNNLS</em>, <em>32</em>(7), 3098–3107. (<a
href="https://doi.org/10.1109/TNNLS.2020.3009436">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, the stability of a hierarchical network with delayed output is discussed by applying a kind of optimal periodic control. To reduce the number of the nodes of the original hierarchical network, an aggregation algorithm is first presented to take some nodes with the same information as an aggregated node. Furthermore, the stability of the original hierarchical network can be guaranteed by the optimal periodic control of the aggregated hierarchical network. Then, an optimal control scheme is proposed to reduce the bandwidth waste in information transmission. In the control scheme, the time sequence is separated into two parts: the deterministic segment and the dynamic segment. With the optimal control scheme, two targets are achieved: 1) the outputs of the original and aggregated hierarchical system are both asymptotically stable and 2) the nodes with slow convergent rate can catch up with the convergence speeds of other nodes.},
  archive      = {J_TNNLS},
  author       = {Wenjun Xiong and Xinghuo Yu and Chen Liu and Guanghui Wen and Shiping Wen},
  doi          = {10.1109/TNNLS.2020.3009436},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {3098-3107},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Simplifying complex network stability analysis via hierarchical node aggregation and optimal periodic control},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Broad learning system based on maximum correntropy
criterion. <em>TNNLS</em>, <em>32</em>(7), 3083–3097. (<a
href="https://doi.org/10.1109/TNNLS.2020.3009417">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an effective and efficient discriminative learning method, broad learning system (BLS) has received increasing attention due to its outstanding performance in various regression and classification problems. However, the standard BLS is derived under the minimum mean square error (MMSE) criterion, which is, of course, not always a good choice due to its sensitivity to outliers. To enhance the robustness of BLS, we propose in this work to adopt the maximum correntropy criterion (MCC) to train the output weights, obtaining a correntropy-based BLS (C-BLS). Due to the inherent superiorities of MCC, the proposed C-BLS is expected to achieve excellent robustness to outliers while maintaining the original performance of the standard BLS in the Gaussian or noise-free environment. In addition, three alternative incremental learning algorithms, derived from a weighted regularized least-squares solution rather than pseudoinverse formula, for C-BLS are developed. With the incremental learning algorithms, the system can be updated quickly without the entire retraining process from the beginning when some new samples arrive or the network deems to be expanded. Experiments on various regression and classification data sets are reported to demonstrate the desirable performance of the new methods.},
  archive      = {J_TNNLS},
  author       = {Yunfei Zheng and Badong Chen and Shiyuan Wang and Weiqun Wang},
  doi          = {10.1109/TNNLS.2020.3009417},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {3083-3097},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Broad learning system based on maximum correntropy criterion},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Segmenting objects in day and night: Edge-conditioned CNN
for thermal image semantic segmentation. <em>TNNLS</em>, <em>32</em>(7),
3069–3082. (<a
href="https://doi.org/10.1109/TNNLS.2020.3009373">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite much research progress in image semantic segmentation, it remains challenging under adverse environmental conditions caused by imaging limitations of the visible spectrum, while thermal infrared cameras have several advantages over cameras for the visible spectrum, such as operating in total darkness, insensitive to illumination variations, robust to shadow effects, and strong ability to penetrate haze and smog. These advantages of thermal infrared cameras make the segmentation of semantic objects in day and night. In this article, we propose a novel network architecture, called edge-conditioned convolutional neural network (EC-CNN), for thermal image semantic segmentation. Particularly, we elaborately design a gated featurewise transform layer in EC-CNN to adaptively incorporate edge prior knowledge. The whole EC-CNN is end-to-end trained and can generate high-quality segmentation results with edge guidance. Meanwhile, we also introduce a new benchmark data set named “Segmenting Objects in Day And night” (SODA) for comprehensive evaluations in thermal image semantic segmentation. SODA contains over 7168 manually annotated and synthetically generated thermal images with 20 semantic region labels and from a broad range of viewpoints and scene complexities. Extensive experiments on SODA demonstrate the effectiveness of the proposed EC-CNN against state-of-the-art methods.},
  archive      = {J_TNNLS},
  author       = {Chenglong Li and Wei Xia and Yan Yan and Bin Luo and Jin Tang},
  doi          = {10.1109/TNNLS.2020.3009373},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {3069-3082},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Segmenting objects in day and night: Edge-conditioned CNN for thermal image semantic segmentation},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Frequency-dependent energy demand of dendritic responses to
deep brain stimulation in thalamic neurons: A model-based study.
<em>TNNLS</em>, <em>32</em>(7), 3056–3068. (<a
href="https://doi.org/10.1109/TNNLS.2020.3009293">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Thalamic deep brain stimulation (DBS) generates excitatory postsynaptic currents and action potentials (APs) by triggering large numbers of synaptic inputs to local cells, which also activates axonal spikes to antidromically invade the soma and dendrites. To maintain signaling, the evoked dendritic responses require metabolic energy to restore ion gradients in each dendrite. The objective of this study is to estimate the energy demand associated with dendritic responses to thalamic DBS. We use a morphologically realistic computational model to simulate dendritic activity in thalamocortical (TC) relay neurons with axonal intracellular stimulation or DBS-like extracellular stimulation. We determine the metabolic cost by calculating the number of adenosine triphosphate (ATP) expended to pump Na + and Ca 2+ ions out of each dendrite. The ATP demand of dendritic activity exhibits frequency dependence, which is determined by the number of spikes in the dendrites. Each backpropagating AP from the soma activates a spike in the dendrites, and the dendritic firing is dominated by antidromic activation of the soma. High stimulus frequencies decrease dendritic ATP cost by reducing the fidelity of antidromic activation. Synaptic inputs and stimulus-induced polarization govern the ATP cost of dendritic responses by facilitating/suppressing antidromic activation, which also influences the ATP cost by depolarizing/hyperpolarizing each dendrite. These findings are important for understanding the synaptic signaling energy in TC relay neurons and metabolism-dependent functional imaging data of thalamic DBS.},
  archive      = {J_TNNLS},
  author       = {Guosheng Yi and Jiang Wang},
  doi          = {10.1109/TNNLS.2020.3009293},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {3056-3068},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Frequency-dependent energy demand of dendritic responses to deep brain stimulation in thalamic neurons: A model-based study},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Observer-based quasi-synchronization of delayed dynamical
networks with parameter mismatch under impulsive effect. <em>TNNLS</em>,
<em>32</em>(7), 3046–3055. (<a
href="https://doi.org/10.1109/TNNLS.2020.3009271">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article focuses on the observer-based quasi-synchronization problem of delayed dynamical networks with parameter mismatch under impulsive effect. First, since the state of each node is unknown in the real situation, the state estimation strategy is proposed to estimate the state of each node, so as to design an appropriate synchronization controller. Then, the corresponding controller is constructed to synchronize the slave nodes with their leader node. In this article, we take the impulsive effect into consideration, which means that an impulsive signal will be applied to the system every so often. Due to the existence of parameter mismatch and time-varying delay, by constructing an appropriate Lyapunouv function, we will eventually obtain a differential equation with constant and time-varying delay terms. Then, we analyze its trajectory by introducing the Cauchy matrix and prove its boundedness by contradiction. Finally, a numerical simulation is presented to illustrate the validness of obtained results.},
  archive      = {J_TNNLS},
  author       = {Xiaoze Ni and Shiping Wen and Huamin Wang and Zhenyuan Guo and Song Zhu and Tingwen Huang},
  doi          = {10.1109/TNNLS.2020.3009271},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {3046-3055},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Observer-based quasi-synchronization of delayed dynamical networks with parameter mismatch under impulsive effect},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Reinforcement learning-based optimal tracking control of an
unknown unmanned surface vehicle. <em>TNNLS</em>, <em>32</em>(7),
3034–3045. (<a
href="https://doi.org/10.1109/TNNLS.2020.3009214">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, a novel reinforcement learning-based optimal tracking control (RLOTC) scheme is established for an unmanned surface vehicle (USV) in the presence of complex unknowns, including dead-zone input nonlinearities, system dynamics, and disturbances. To be specific, dead-zone nonlinearities are decoupled to be input-dependent sloped controls and unknown biases that are encapsulated into lumped unknowns within tracking error dynamics. Neural network (NN) approximators are further deployed to adaptively identify complex unknowns and facilitate a Hamilton–Jacobi–Bellman (HJB) equation that formulates optimal tracking. In order to derive a practically optimal solution, an actor–critic reinforcement learning framework is built by employing adaptive NN identifiers to recursively approximate the total optimal policy and cost function. Eventually, theoretical analysis shows that the entire RLOTC scheme can render tracking errors that converge to an arbitrarily small neighborhood of the origin, subject to optimal cost. Simulation results and comprehensive comparisons on a prototype USV demonstrate remarkable effectiveness and superiority.},
  archive      = {J_TNNLS},
  author       = {Ning Wang and Ying Gao and Hong Zhao and Choon Ki Ahn},
  doi          = {10.1109/TNNLS.2020.3009214},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {3034-3045},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Reinforcement learning-based optimal tracking control of an unknown unmanned surface vehicle},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Low tensor-ring rank completion by parallel matrix
factorization. <em>TNNLS</em>, <em>32</em>(7), 3020–3033. (<a
href="https://doi.org/10.1109/TNNLS.2020.3009210">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tensor-ring (TR) decomposition has recently attracted considerable attention in solving the low-rank tensor completion (LRTC) problem. However, due to an unbalanced unfolding scheme used during the update of core tensors, the conventional TR-based completion methods usually require a large TR rank to achieve the optimal performance, which leads to high computational cost in practical applications. To overcome this drawback, we propose a new method to exploit the low TR-rank structure in this article. Specifically, we first introduce a balanced unfolding operation called tensor circular unfolding, by which the relationship between TR rank and the ranks of tensor unfoldings is theoretically established. Using this new unfolding operation, we further propose an algorithm to exploit the low TR-rank structure by performing parallel low-rank matrix factorizations to all circularly unfolded matrices. To tackle the problem of nonuniform missing patterns, we apply a row weighting trick to each circularly unfolded matrix, which significantly improves the adaptive ability to various types of missing patterns. The extensive experiments have demonstrated that the proposed algorithm can achieve outstanding performance using a much smaller TR rank compared with the conventional TR-based completion algorithms; meanwhile, the computational cost is reduced substantially.},
  archive      = {J_TNNLS},
  author       = {Jinshi Yu and Guoxu Zhou and Chao Li and Qibin Zhao and Shengli Xie},
  doi          = {10.1109/TNNLS.2020.3009210},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {3020-3033},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Low tensor-ring rank completion by parallel matrix factorization},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Top-k feature selection framework using robust 0–1 integer
programming. <em>TNNLS</em>, <em>32</em>(7), 3005–3019. (<a
href="https://doi.org/10.1109/TNNLS.2020.3009209">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection (FS), which identifies the relevant features in a data set to facilitate subsequent data analysis, is a fundamental problem in machine learning and has been widely studied in recent years. Most FS methods rank the features in order of their scores based on a specific criterion and then select the k top-ranked features, where k is the number of desired features. However, these features are usually not the top- k features and may present a suboptimal choice. To address this issue, we propose a novel FS framework in this article to select the exact top- k features in the unsupervised, semisupervised, and supervised scenarios. The new framework utilizes the l 0,2 -norm as the matrix sparsity constraint rather than its relaxations, such as the l 1,2 -norm. Since the l 0,2 -norm constrained problem is difficult to solve, we transform the discrete l 0,2 -norm-based constraint into an equivalent 0-1 integer constraint and replace the 0-1 integer constraint with two continuous constraints. The obtained top- k FS framework with two continuous constraints is theoretically equivalent to the l 0,2 -norm constrained problem and can be optimized by the alternating direction method of multipliers (ADMM). Unsupervised and semisupervised FS methods are developed based on the proposed framework, and extensive experiments on real-world data sets are conducted to demonstrate the effectiveness of the proposed FS framework.},
  archive      = {J_TNNLS},
  author       = {Xiaoqin Zhang and Mingyu Fan and Di Wang and Peng Zhou and Dacheng Tao},
  doi          = {10.1109/TNNLS.2020.3009209},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {3005-3019},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Top-k feature selection framework using robust 0–1 integer programming},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A penalty strategy combined varying-parameter recurrent
neural network for solving time-varying multi-type constrained quadratic
programming problems. <em>TNNLS</em>, <em>32</em>(7), 2993–3004. (<a
href="https://doi.org/10.1109/TNNLS.2020.3009201">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To obtain the optimal solution to the time-varying quadratic programming (TVQP) problem with equality and multitype inequality constraints, a penalty strategy combined varying-parameter recurrent neural network (PS-VP-RNN) for solving TVQP problems is proposed and analyzed. By using a novel penalty function designed in this article, the inequality constraint of the TVQP can be transformed into a penalty term that is added into the objective function of TVQP problems. Then, based on the design method of VP-RNN, a PS-VP-RNN is designed and analyzed for solving the TVQP with penalty term. One of the greatest advantages of PS-VP-RNN is that it cannot only solve the TVQP with equality constraints but can also solve the TVQP with inequality and bounded constraints. The global convergence theorem of PS-VP-RNN is presented and proved. Finally, three numerical simulation experiments with different forms of inequality and bounded constraints verify the effectiveness and accuracy of PS-VP-RNN in solving the TVQP problems.},
  archive      = {J_TNNLS},
  author       = {Zhijun Zhang and Song Yang and Lunan Zheng},
  doi          = {10.1109/TNNLS.2020.3009201},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {2993-3004},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A penalty strategy combined varying-parameter recurrent neural network for solving time-varying multi-type constrained quadratic programming problems},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dynamics analysis and design for a bidirectional
super-ring-shaped neural network with n neurons and multiple delays.
<em>TNNLS</em>, <em>32</em>(7), 2978–2992. (<a
href="https://doi.org/10.1109/TNNLS.2020.3009166">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, the dynamics of delayed neural networks has always incurred the widespread concern of scholars. However, they are mostly confined to some simplified neural networks, which are only made up of a small amount of neurons. The main cause is that it is difficult to decompose and analyze generally high-dimensional characteristic matrices. In this article, for the first time, we can solve the computing issues of high-dimensional eigenmatrix by employing the formula of Coates flow graph, and the dynamics is considered for a bidirectional neural network with super-ring structure and multiple delays. Under certain circumstances, the characteristic equation of the linearized network can be transformed into the equation with integration element. By analyzing the equation, we find that the self-feedback coefficient and the delays have significant effects on the stability and Hopf bifurcation of the network. Then, we achieve some sufficient conditions of the stability and Hopf bifurcation on the network. Furthermore, the obtained conclusions are applied to design a standardized high-dimensional network with bidirectional ring structure, and the scale of the standardized high-dimensional network can be easily extended or reduced. Afterward, we propose some designing schemes to expand and reduce the dimension of the standardized high-dimensional network. Finally, the results of theories are coincident with that of experiments.},
  archive      = {J_TNNLS},
  author       = {Binbin Tao and Min Xiao and Wei Xing Zheng and Jinde Cao and Jingwen Tang},
  doi          = {10.1109/TNNLS.2020.3009166},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {2978-2992},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Dynamics analysis and design for a bidirectional super-ring-shaped neural network with n neurons and multiple delays},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Neural-network-based fully distributed adaptive consensus
for a class of uncertain multiagent systems. <em>TNNLS</em>,
<em>32</em>(7), 2965–2977. (<a
href="https://doi.org/10.1109/TNNLS.2020.3009098">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we revisit the problem of distributed neuroadaptive consensus for uncertain multiagent systems (MASs) in the presence of unmodeled nonlinearities as well as unknown disturbances. Robust consensus controllers comprising a linear feedback term, a discontinuous feedback term, and a neural network approximation term are constructed, where in each term, the weight part is endowed with some dynamical changing law. The asymptotic convergence of the consensus errors is theoretically proved based on the graph theory, nonsmooth analysis, and Barbalat’s lemma. Both leaderless consensus and leader–follower tracking problems are considered before the results are further extended to containment problem in the presence of multileaders. A dramatic feature of the proposed method, in comparison with related works, is the fully distributed fashion of the information, requiring neither the underlying Laplacian eigenvalues nor the input upper bounds of the leaders (if exist). Several numerical examples are presented to testify the theoretical results.},
  archive      = {J_TNNLS},
  author       = {Dongdong Yue and Jinde Cao and Qi Li and Qingshan Liu},
  doi          = {10.1109/TNNLS.2020.3009098},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {2965-2977},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Neural-network-based fully distributed adaptive consensus for a class of uncertain multiagent systems},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Gain-scheduled finite-time synchronization for
reaction–diffusion memristive neural networks subject to inconsistent
markov chains. <em>TNNLS</em>, <em>32</em>(7), 2952–2964. (<a
href="https://doi.org/10.1109/TNNLS.2020.3009081">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An innovative class of drive-response systems that are composed of Markovian reaction-diffusion memristive neural networks, where the drive and response systems follow inconsistent Markov chains, is proposed in this article. For this kind of nonlinear parameter-varying systems, a suitable gain-scheduled controller that involves a mode and memristor-dependent item is designed, so that the error system is bounded within a finite-time interval. Moreover, by constructing a novel Lyapunov-Krasovskii functional and employing the canonical Bessel-Legendre inequality and free-weighting matrix method, the conservatism of the finite-time synchronization criterion can be greatly reduced. Finally, two numerical examples are provided to illustrate the feasibility and practicability of the obtained results.},
  archive      = {J_TNNLS},
  author       = {Xiaona Song and Jingtao Man and Shuai Song and Choon Ki Ahn},
  doi          = {10.1109/TNNLS.2020.3009081},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {2952-2964},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Gain-scheduled finite-time synchronization for Reaction–Diffusion memristive neural networks subject to inconsistent markov chains},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Event-triggered adaptive dynamic programming for unmatched
uncertain nonlinear continuous-time systems. <em>TNNLS</em>,
<em>32</em>(7), 2939–2951. (<a
href="https://doi.org/10.1109/TNNLS.2020.3009015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, an event-triggered adaptive dynamic programming (ADP) method is proposed to solve the robust control problem of unmatched uncertain systems. First, the robust control problem with unmatched uncertainties is transformed into the optimal control design for an auxiliary system. Subsequently, to reduce controller executions and save computational and communication resources, an event-triggering mechanism is introduced. By using a critic neural network (NN) to approximate the value function, novel concurrent learning is developed to learn NN weights, which avoids the requirement of an initial admissible control and the persistence of excitation condition. Moreover, it is proven that the developed event-triggered ADP controller guarantees the robustness of the uncertain system and the uniform ultimate boundedness of the NN weight estimation error. Finally, by using the F-16 aircraft and the inverted pendulum with unmatched uncertainties as examples, the simulation results show the effectiveness of the developed event-triggered ADP method.},
  archive      = {J_TNNLS},
  author       = {Shan Xue and Biao Luo and Derong Liu},
  doi          = {10.1109/TNNLS.2020.3009015},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {2939-2951},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Event-triggered adaptive dynamic programming for unmatched uncertain nonlinear continuous-time systems},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Exploiting retraining-based mixed-precision quantization for
low-cost DNN accelerator design. <em>TNNLS</em>, <em>32</em>(7),
2925–2938. (<a
href="https://doi.org/10.1109/TNNLS.2020.3008996">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For successful deployment of deep neural networks (DNNs) on resource-constrained devices, retraining-based quantization has been widely adopted to reduce the number of DRAM accesses. By properly setting training parameters, such as batch size and learning rate, bit widths of both weights and activations can be uniformly quantized down to 4 bit while maintaining full precision accuracy. In this article, we present a retraining-based mixed-precision quantization approach and its customized DNN accelerator to achieve high energy efficiency. In the proposed quantization, in the middle of retraining, an additional bit (extra quantization level) is assigned to the weights that have shown frequent switching between two contiguous quantization levels since it means that both quantization levels cannot help to reduce quantization loss. We also mitigate the gradient noise that occurs in the retraining process by taking a lower learning rate near the quantization threshold. For the proposed novel mixed-precision quantized network (MPQ-network), we have implemented a customized accelerator using a 65-nm CMOS process. In the accelerator, the proposed processing elements (PEs) can be dynamically reconfigured to process variable bit widths from 2 to 4 bit for both weights and activations. The numerical results show that the proposed quantization can achieve $1.37 {\times }$ better compression ratio for VGG-9 using CIFAR-10 data set compared with a uniform 4-bit (both weights and activations) model without loss of classification accuracy. The proposed accelerator also shows $1.29\times $ of energy savings for VGG-9 using the CIFAR-10 data set over the state-of-the-art accelerator.},
  archive      = {J_TNNLS},
  author       = {Nahsung Kim and Dongyeob Shin and Wonseok Choi and Geonho Kim and Jongsun Park},
  doi          = {10.1109/TNNLS.2020.3008996},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {2925-2938},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Exploiting retraining-based mixed-precision quantization for low-cost DNN accelerator design},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Policy iteration approach to the infinite horizon average
optimal control of probabilistic boolean networks. <em>TNNLS</em>,
<em>32</em>(7), 2910–2924. (<a
href="https://doi.org/10.1109/TNNLS.2020.3008960">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article studies the optimal control of probabilistic Boolean control networks (PBCNs) with the infinite horizon average cost criterion. By resorting to the semitensor product (STP) of matrices, a nested optimality equation for the optimal control problem of PBCNs is proposed. The Laurent series expression technique and the Jordan decomposition method derive a novel policy iteration-type algorithm, where finite iteration steps can provide the optimal state feedback law, which is presented. Finally, the intervention problem of the probabilistic Ara operon in E. coil, as a biological application, is solved to demonstrate the effectiveness and feasibility of the proposed theoretical approach and algorithms.},
  archive      = {J_TNNLS},
  author       = {Yuhu Wu and Yuqian Guo and Mitsuru Toyoda},
  doi          = {10.1109/TNNLS.2020.3008960},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {2910-2924},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Policy iteration approach to the infinite horizon average optimal control of probabilistic boolean networks},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Time–frequency representation and convolutional neural
network-based emotion recognition. <em>TNNLS</em>, <em>32</em>(7),
2901–2909. (<a
href="https://doi.org/10.1109/TNNLS.2020.3008938">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emotions composed of cognizant logical reactions toward various situations. Such mental responses stem from physiological, cognitive, and behavioral changes. Electroencephalogram (EEG) signals provide a noninvasive and nonradioactive solution for emotion identification. Accurate and automatic classification of emotions can boost the development of human–computer interface. This article proposes automatic extraction and classification of features through the use of different convolutional neural networks (CNNs). At first, the proposed method converts the filtered EEG signals into an image using a time–frequency representation. Smoothed pseudo-Wigner–Ville distribution is used to transform time-domain EEG signals into images. These images are fed to pretrained AlexNet, ResNet50, and VGG16 along with configurable CNN. The performance of four CNNs is evaluated by measuring the accuracy, precision, Mathew’s correlation coefficient, F1-score, and false-positive rate. The results obtained by evaluating four CNNs show that configurable CNN requires very less learning parameters with better accuracy. Accuracy scores of 90.98\%, 91.91\%, 92.71\%, and 93.01\% obtained by AlexNet, ResNet50, VGG16, and configurable CNN show that the proposed method is best among other existing methods.},
  archive      = {J_TNNLS},
  author       = {Smith K. Khare and Varun Bajaj},
  doi          = {10.1109/TNNLS.2020.3008938},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {2901-2909},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Time–Frequency representation and convolutional neural network-based emotion recognition},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A neural network based on the metric projector for solving
SOCCVI problem. <em>TNNLS</em>, <em>32</em>(7), 2886–2900. (<a
href="https://doi.org/10.1109/TNNLS.2020.3008661">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose an efficient neural network for solving the second-order cone constrained variational inequality (SOCCVI). The network is constructed using the Karush–Kuhn–Tucker (KKT) conditions of the variational inequality (VI), which is used to recast the SOCCVI as a system of equations by using a smoothing function for the metric projection mapping to deal with the complementarity condition. Aside from standard stability results, we explore second-order sufficient conditions to obtain exponential stability. Especially, we prove the nonsingularity of the Jacobian of the KKT system based on the second-order sufficient condition and constraint nondegeneracy. Finally, we present some numerical experiments, illustrating the efficiency of the neural network in solving SOCCVI problems. Our numerical simulations reveal that, in general, the new neural network is more dominant than all other neural networks in the SOCCVI literature in terms of stability and convergence rates of trajectories to SOCCVI solution.},
  archive      = {J_TNNLS},
  author       = {Juhe Sun and Weichen Fu and Jan Harold Alcantara and Jein-Shan Chen},
  doi          = {10.1109/TNNLS.2020.3008661},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {2886-2900},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A neural network based on the metric projector for solving SOCCVI problem},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multiview variational sparse gaussian processes.
<em>TNNLS</em>, <em>32</em>(7), 2875–2885. (<a
href="https://doi.org/10.1109/TNNLS.2020.3008496">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gaussian process (GP) models are flexible nonparametric models widely used in a variety of tasks. Variational sparse GP (VSGP) scales GP models to large data sets by summarizing the posterior process with a set of inducing points. In this article, we extend VSGP to handle multiview data. We model each view with a VSGP and augment it with an additional set of inducing points. These VSGPs are coupled together by enforcing the means of their posteriors to agree at the locations of these inducing points. To learn these shared inducing points, we introduce an additional GP model that is defined in the concatenated feature space. Experiments on real-world data sets show that our multiview VSGP (MVSGP) model outperforms single-view VSGP consistently and is superior to state-of-the-art kernel-based multiview baselines for classification tasks.},
  archive      = {J_TNNLS},
  author       = {Liang Mao and Shiliang Sun},
  doi          = {10.1109/TNNLS.2020.3008496},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {2875-2885},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multiview variational sparse gaussian processes},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Semisupervised learning on graphs with an alternating
diffusion process. <em>TNNLS</em>, <em>32</em>(7), 2862–2874. (<a
href="https://doi.org/10.1109/TNNLS.2020.3008445">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph-based semisupervised learning is of great importance in many effective learning systems, particularly in agnostic settings where no parametric information or other prior knowledge about the data distribution is available. It leverages the graph structure to propagate labels from labeled nodes to unlabeled ones. Two separate stages are usually involved: constructing an affinity graph and propagating labels on the graph for transductive inference. It is suboptimal to manage them independently, as the correlation between the affinity graph and the labels would not be fully exploited. In this article, we integrate these two stages into one unified framework by formulating the graph construction as a regularized function estimation problem, similar to label propagation. We then propose an alternating diffusion process to solve them alternately, which allows us to learn the graph and unknown labels in an iterative fashion. With the proposed framework, we can construct a dynamic graph adapted to the given and predicted labels iteratively, resulting in more accurate and robust label propagation performance. Extensive experiments on synthetic data and various real-world data have demonstrated the superiority of the proposed method compared with other state-of-the-art methods.},
  archive      = {J_TNNLS},
  author       = {Qilin Li and Senjian An and Wanquan Liu and Ling Li},
  doi          = {10.1109/TNNLS.2020.3008445},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {2862-2874},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Semisupervised learning on graphs with an alternating diffusion process},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Identifying autism spectrum disorder from resting-state fMRI
using deep belief network. <em>TNNLS</em>, <em>32</em>(7), 2847–2861.
(<a href="https://doi.org/10.1109/TNNLS.2020.3007943">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing prevalence of autism spectrum disorder (ASD), it is important to identify ASD patients for effective treatment and intervention, especially in early childhood. Neuroimaging techniques have been used to characterize the complex biomarkers based on the functional connectivity anomalies in the ASD. However, the diagnosis of ASD still adopts the symptom-based criteria by clinical observation. The existing computational models tend to achieve unreliable diagnostic classification on the large-scale aggregated data sets. In this work, we propose a novel graph-based classification model using the deep belief network (DBN) and the Autism Brain Imaging Data Exchange (ABIDE) database, which is a worldwide multisite functional and structural brain imaging data aggregation. The remarkable connectivity features are selected through a graph extension of ${K}$ -nearest neighbors and then refined by a restricted path-based depth-first search algorithm. Thanks to the feature reduction, lower computational complexity could contribute to the shortening of the training time. The automatic hyperparameter-tuning technique is introduced to optimize the hyperparameters of the DBN by exploring the potential parameter space. The simulation experiments demonstrate the superior performance of our model, which is 6.4\% higher than the best result reported on the ABIDE database. We also propose to use the data augmentation and the oversampling technique to identify further the possible subtypes within the ASD. The interpretability of our model enables the identification of the most remarkable autistic neural correlation patterns from the data-driven outcomes.},
  archive      = {J_TNNLS},
  author       = {Zhi-An Huang and Zexuan Zhu and Chuen Heung Yau and Kay Chen Tan},
  doi          = {10.1109/TNNLS.2020.3007943},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {2847-2861},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Identifying autism spectrum disorder from resting-state fMRI using deep belief network},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Price trailing for financial trading using deep
reinforcement learning. <em>TNNLS</em>, <em>32</em>(7), 2837–2846. (<a
href="https://doi.org/10.1109/TNNLS.2020.2997523">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning methods have recently seen a growing number of applications in financial trading. Being able to automatically extract patterns from past price data and consistently apply them in the future has been the focus of many quantitative trading applications. However, developing machine learning-based methods for financial trading is not straightforward, requiring carefully designed targets/rewards, hyperparameter fine-tuning, and so on. Furthermore, most of the existing methods are unable to effectively exploit the information available across various financial instruments. In this article, we propose a deep reinforcement learning-based approach, which ensures that consistent rewards are provided to the trading agent, mitigating the noisy nature of profit-and-loss rewards that are usually used. To this end, we employ a novel price trailing-based reward shaping approach, significantly improving the performance of the agent in terms of profit, Sharpe ratio, and maximum drawdown. Furthermore, we carefully designed a data preprocessing method that allows for training the agent on different FOREX currency pairs, providing a way for developing market-wide RL agents and allowing, at the same time, to exploit more powerful recurrent deep learning models without the risk of overfitting. The ability of the proposed methods to improve various performance metrics is demonstrated using a challenging large-scale data set, containing 28 instruments, provided by Speedlab AG.},
  archive      = {J_TNNLS},
  author       = {Avraam Tsantekidis and Nikolaos Passalis and Anastasia-Sotiria Toufa and Konstantinos Saitas-Zarkias and Stergios Chairistanidis and Anastasios Tefas},
  doi          = {10.1109/TNNLS.2020.2997523},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {2837-2846},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Price trailing for financial trading using deep reinforcement learning},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Minimax and biobjective portfolio selection based on
collaborative neurodynamic optimization. <em>TNNLS</em>, <em>32</em>(7),
2825–2836. (<a
href="https://doi.org/10.1109/TNNLS.2019.2957105">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Portfolio selection is one of the important issues in financial investments. This article is concerned with portfolio selection based on collaborative neurodynamic optimization. The classic Markowitz mean–variance (MV) framework and its variant mean conditional value-at-risk (CVaR) are formulated as minimax and biobjective portfolio selection problems. Neurodynamic approaches are then applied for solving these optimization problems. For each of the problems, multiple neural networks work collaboratively to characterize the efficient frontier by means of particle swarm optimization (PSO)-based weight optimization. Experimental results with stock data from four major markets show the performance and characteristics of the collaborative neurodynamic approaches to the portfolio optimization problems.},
  archive      = {J_TNNLS},
  author       = {Man-Fai Leung and Jun Wang},
  doi          = {10.1109/TNNLS.2019.2957105},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {2825-2836},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Minimax and biobjective portfolio selection based on collaborative neurodynamic optimization},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A survey of stochastic computing neural networks for machine
learning applications. <em>TNNLS</em>, <em>32</em>(7), 2809–2824. (<a
href="https://doi.org/10.1109/TNNLS.2020.3009047">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural networks (NNs) are effective machine learning models that require significant hardware and energy consumption in their computing process. To implement NNs, stochastic computing (SC) has been proposed to achieve a tradeoff between hardware efficiency and computing performance. In an SC NN, hardware requirements and power consumption are significantly reduced by moderately sacrificing the inference accuracy and computation speed. With recent developments in SC techniques, however, the performance of SC NNs has substantially been improved, making it comparable with conventional binary designs yet by utilizing less hardware. In this article, we begin with the design of a basic SC neuron and then survey different types of SC NNs, including multilayer perceptrons, deep belief networks, convolutional NNs, and recurrent NNs. Recent progress in SC designs that further improve the hardware efficiency and performance of NNs is subsequently discussed. The generality and versatility of SC NNs are illustrated for both the training and inference processes. Finally, the advantages and challenges of SC NNs are discussed with respect to binary counterparts.},
  archive      = {J_TNNLS},
  author       = {Yidong Liu and Siting Liu and Yanzhi Wang and Fabrizio Lombardi and Jie Han},
  doi          = {10.1109/TNNLS.2020.3009047},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {2809-2824},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A survey of stochastic computing neural networks for machine learning applications},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). 4S-DT: Self-supervised super sample decomposition for
transfer learning with application to COVID-19 detection.
<em>TNNLS</em>, <em>32</em>(7), 2798–2808. (<a
href="https://doi.org/10.1109/TNNLS.2021.3082015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the high availability of large-scale annotated image datasets, knowledge transfer from pretrained models showed outstanding performance in medical image classification. However, building a robust image classification model for datasets with data irregularity or imbalanced classes can be a very challenging task, especially in the medical imaging domain. In this article, we propose a novel deep convolutional neural network, which we called self-supervised super sample decomposition for transfer learning (4S-DT) model. The 4S-DT encourages a coarse-to-fine transfer learning from large-scale image recognition tasks to a specific chest X-ray image classification task using a generic self-supervised sample decomposition approach. Our main contribution is a novel self-supervised learning mechanism guided by a super sample decomposition of unlabeled chest X-ray images. 4S-DT helps in improving the robustness of knowledge transformation via a downstream learning strategy with a class-decomposition (CD) layer to simplify the local structure of the data. The 4S-DT can deal with any irregularities in the image dataset by investigating its class boundaries using a downstream CD mechanism. We used 50000 unlabeled chest X-ray images to achieve our coarse-to-fine transfer learning with an application to COVID-19 detection, as an exemplar. The 4S-DT has achieved a high accuracy of 99.8\% on the larger of the two datasets used in the experimental study and an accuracy of 97.54\% on the smaller dataset, which was enriched by augmented images, out of which all real COVID-19 cases were detected.},
  archive      = {J_TNNLS},
  author       = {Asmaa Abbas and Mohammed M. Abdelsamea and Mohamed Medhat Gaber},
  doi          = {10.1109/TNNLS.2021.3082015},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {2798-2808},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {4S-DT: Self-supervised super sample decomposition for transfer learning with application to COVID-19 detection},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021g). IEEE computational intelligence society information.
<em>TNNLS</em>, <em>32</em>(6), C3. (<a
href="https://doi.org/10.1109/TNNLS.2021.3078594">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Provides a listing of current committee members and society officers.},
  archive      = {J_TNNLS},
  doi          = {10.1109/TNNLS.2021.3078594},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {C3},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {IEEE computational intelligence society information},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A hybrid CMOS-memristive approach to designing deep
generative models. <em>TNNLS</em>, <em>32</em>(6), 2790–2796. (<a
href="https://doi.org/10.1109/TNNLS.2020.3008154">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning and its applications have gained tremendous interest recently in both academia and industry. Restricted Boltzmann machines (RBMs) offer a key methodology to implement deep learning paradigms. This brief presents a novel approach for realizing hybrid CMOS-memristive-based deep generative models (DGMs). In our proposed DGM architecture, HfO x -based (filamentary-type switching) memristive devices are extensively used for realizing both computational as well as storage functions, such as: 1) synapses (weights); 2) internal neuron-state storage; 3) stochastic neuron activation; and 4) programmable signal normalization. To validate the proposed scheme, we have simulated two different architectures: 1) deep belief network (DBN) for classification and 2) stacked denoising autoencoder for the reconstruction of handwritten digits from the MNIST data set. The maximum test accuracy achieved by pretraining of the proposed DBN was 92.6\%, whereas the best case mean squared error (mse) achieved by pretraining of the proposed SDA network was 0.046. When the proposed model-based weights are used for weight initialization, they offer a significant advantage in terms of learning performance in comparison with randomized initialization.},
  archive      = {J_TNNLS},
  author       = {Vivek Parmar and Manan Suri},
  doi          = {10.1109/TNNLS.2020.3008154},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2790-2796},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A hybrid CMOS-memristive approach to designing deep generative models},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Q-neurons: Neuron activations based on stochastic jackson’s
derivative operators. <em>TNNLS</em>, <em>32</em>(6), 2782–2789. (<a
href="https://doi.org/10.1109/TNNLS.2020.3005167">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new generic type of artificial neurons called q-neurons. A q-neuron is a stochastic neuron with its activation function relying on Jackson&#39;s discrete q-derivative for a stochastic parameter q. We show how to generalize neural network architectures with q-neurons and demonstrate the scalability and ease of implementation of q-neurons into legacy deep learning frameworks. We report experimental results that consistently improve performance over state-of-the-art standard activation functions, both on training and test loss functions.},
  archive      = {J_TNNLS},
  author       = {Frank Nielsen and Ke Sun},
  doi          = {10.1109/TNNLS.2020.3005167},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2782-2789},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Q-neurons: Neuron activations based on stochastic jackson’s derivative operators},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). TAM: Targeted analysis model with reinforcement learning on
short texts. <em>TNNLS</em>, <em>32</em>(6), 2772–2781. (<a
href="https://doi.org/10.1109/TNNLS.2020.3009247">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mining topics on social media (e.g., Twitter and Facebook) is an important task for various applications, such as hot topic discovery, advertising, and promotion activities. Topic modeling techniques are helpful to find out topics that people are talking about. However, current full-analysis models cannot perform well on a focused analysis task—find out all topics related to one particular area in short documents. One reason is that the targeted topic is usually sparse in the corpus of short texts. Another one is, during clustering, even minor errors may compound and render the model useless. This article studies these problems and proposes a targeted analysis model (TAM) with reinforcement learning (RL) to extract any specific topic in a given corpus and perform fine-grained topic generation. In this work, we design a reward function of RL to prevent the false propagation problem induced by Gibbs sampling during the clustering. We amend the targeted topic modeling techniques to the case of RL and use policy search combined with the Gibbs EM algorithm for parameter estimation. Metrics of F1 score and the proposed normalized mutual information-F1 are exploited for the evaluation of clustering and topic generation, respectively. Our experiments have demonstrated that TAM can outperform state-of-the-art models—specifically achieving 25.7\% improvement on the F1 score for binary clustering on average.},
  archive      = {J_TNNLS},
  author       = {Junyang Chen and Zhiguo Gong and Wei Wang and Weiwen Liu and Ming Yang and Cong Wang},
  doi          = {10.1109/TNNLS.2020.3009247},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2772-2781},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {TAM: Targeted analysis model with reinforcement learning on short texts},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Intelligent trainer for dyna-style model-based deep
reinforcement learning. <em>TNNLS</em>, <em>32</em>(6), 2758–2771. (<a
href="https://doi.org/10.1109/TNNLS.2020.3008249">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Model-based reinforcement learning (MBRL) has been proposed as a promising alternative solution to tackle the high sampling cost challenge in the canonical RL, by leveraging a system dynamics model to generate synthetic data for policy training purpose. The MBRL framework, nevertheless, is inherently limited by the convoluted process of jointly optimizing control policy, learning system dynamics, and sampling data from two sources controlled by complicated hyperparameters. As such, the training process involves overwhelmingly manual tuning and is prohibitively costly. In this research, we propose a “reinforcement on reinforcement” (RoR) architecture to decompose the convoluted tasks into two decoupled layers of RL. The inner layer is the canonical MBRL training process which is formulated as a Markov decision process, called training process environment (TPE). The outer layer serves as an RL agent, called intelligent trainer, to learn an optimal hyperparameter configuration for the inner TPE. This decomposition approach provides much-needed flexibility to implement different trainer designs, referred to “train the trainer.” In our research, we propose and optimize two alternative trainer designs: 1) an unihead trainer and 2) a multihead trainer. Our proposed RoR framework is evaluated for five tasks in the OpenAI gym. Compared with three other baseline methods, our proposed intelligent trainer methods have a competitive performance in autotuning capability, with up to 56\% expected sampling cost saving without knowing the best parameter configurations in advance. The proposed trainer framework can be easily extended to tasks that require costly hyperparameter tuning.},
  archive      = {J_TNNLS},
  author       = {Linsen Dong and Yuanlong Li and Xin Zhou and Yonggang Wen and Kyle Guan},
  doi          = {10.1109/TNNLS.2020.3008249},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2758-2771},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Intelligent trainer for dyna-style model-based deep reinforcement learning},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hierarchical human-like deep neural networks for abstractive
text summarization. <em>TNNLS</em>, <em>32</em>(6), 2744–2757. (<a
href="https://doi.org/10.1109/TNNLS.2020.3008037">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Developing an abstractive text summarization (ATS) system that is capable of generating concise, appropriate, and plausible summaries for the source documents is a long-term goal of artificial intelligence (AI). Recent advances in ATS are overwhelmingly contributed by deep learning techniques, which have taken the state-of-the-art of ATS to a new level. Despite the significant success of previous methods, generating high-quality and human-like abstractive summaries remains a challenge in practice. The human reading cognition, which is essential for reading comprehension and logical thinking, is still relatively new territory and underexplored in deep neural networks. In this article, we propose a novel Hierarchical Human-like deep neural network for ATS (HH-ATS), inspired by the process of how humans comprehend an article and write the corresponding summary. Specifically, HH-ATS is composed of three primary components (i.e., a knowledge-aware hierarchical attention module, a multitask learning module, and a dual discriminator generative adversarial network), which mimic the three stages of human reading cognition (i.e., rough reading, active reading, and postediting). Experimental results on two benchmark data sets (CNN/Daily Mail and Gigaword) demonstrate that HH-ATS consistently and substantially outperforms the compared methods.},
  archive      = {J_TNNLS},
  author       = {Min Yang and Chengming Li and Ying Shen and Qingyao Wu and Zhou Zhao and Xiaojun Chen},
  doi          = {10.1109/TNNLS.2020.3008037},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2744-2757},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Hierarchical human-like deep neural networks for abstractive text summarization},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Generating face images with attributes for free.
<em>TNNLS</em>, <em>32</em>(6), 2733–2743. (<a
href="https://doi.org/10.1109/TNNLS.2020.3007790">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With superhuman-level performance of face recognition, we are more concerned about the recognition of fine-grained attributes, such as emotion, age, and gender. However, given that the label space is extremely large and follows a long-tail distribution, it is quite expensive to collect sufficient samples for fine-grained attributes. This results in imbalanced training samples and inferior attribute recognition models. To this end, we propose the use of arbitrary attribute combinations, without human effort, to synthesize face images. In particular, to bridge the semantic gap between high-level attribute label space and low-level face image, we propose a novel neural-network-based approach that maps the target attribute labels to an embedding vector, which can be fed into a pretrained image decoder to synthesize a new face image. Furthermore, to regularize the attribute for image synthesis, we propose to use a perceptual loss to make the new image explicitly faithful to target attributes. Experimental results show that our approach can generate photorealistic face images from attribute labels, and more importantly, by serving as augmented training samples, these images can significantly boost the performance of attribute recognition model. The code is open-sourced at this link.},
  archive      = {J_TNNLS},
  author       = {Yaoyao Liu and Qianru Sun and Xiangnan He and An-An Liu and Yuting Su and Tat-Seng Chua},
  doi          = {10.1109/TNNLS.2020.3007790},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2733-2743},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Generating face images with attributes for free},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning tracking control over unknown fading channels
without system information. <em>TNNLS</em>, <em>32</em>(6), 2721–2732.
(<a href="https://doi.org/10.1109/TNNLS.2020.3007765">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel data-driven learning control scheme is proposed for unknown systems with unknown fading sensor channels. The fading randomness is modeled by multiplicative and additive random variables subject to certain unknown distributions. In this scheme, we propose an error transmission mode and an iterative gradient estimation method. Unlike the conventional transmission mode where the output is directly transmitted back to the controller, in the error transmission mode, we send the desired reference to the plant such that tracking errors can be calculated locally and then transmitted back through the fading channel. Using the faded tracking error data only, the gradient for updating input is iteratively estimated by a random difference technique along the iteration axis. This gradient acts as the updating term of the control signal; therefore, information on the system and the fading channel is no longer required. The proposed scheme is proved effective in tracking the desired reference under random fading communication environments. Theoretical results are verified by simulations.},
  archive      = {J_TNNLS},
  author       = {Dong Shen and Xinghuo Yu},
  doi          = {10.1109/TNNLS.2020.3007765},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2721-2732},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Learning tracking control over unknown fading channels without system information},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Unsupervised network quantization via fixed-point
factorization. <em>TNNLS</em>, <em>32</em>(6), 2706–2720. (<a
href="https://doi.org/10.1109/TNNLS.2020.3007749">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The deep neural network (DNN) has achieved remarkable performance in a wide range of applications at the cost of huge memory and computational complexity. Fixed-point network quantization emerges as a popular acceleration and compression method but still suffers from huge performance degradation when extremely low-bit quantization is utilized. Moreover, current fixed-point quantization methods rely heavily on supervised retraining using large amounts of the labeled training data, while the labeled data are hard to obtain in the real-world applications. In this article, we propose an efficient framework, namely, fixed-point factorized network (FFN), to turn all weights into ternary values, i.e., {-1, 0, 1}. We highlight that the proposed FFN framework can achieve negligible degradation even without any supervised retraining on the labeled data. Note that the activations can be easily quantized into an 8-bit format; thus, the resulting networks only have low-bit fixed-point additions that are significantly more efficient than 32-bit floating-point multiply-accumulate operations (MACs). Extensive experiments on large-scale ImageNet classification and object detection on MS COCO show that the proposed FFN can achieve about more than 20× compression and remove most of the multiply operations with comparable accuracy. Codes are available on GitHub at https://github.com/wps712/FFN.},
  archive      = {J_TNNLS},
  author       = {Peisong Wang and Xiangyu He and Qiang Chen and Anda Cheng and Qingshan Liu and Jian Cheng},
  doi          = {10.1109/TNNLS.2020.3007749},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2706-2720},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Unsupervised network quantization via fixed-point factorization},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Convolutional neural network with developmental memory for
continual learning. <em>TNNLS</em>, <em>32</em>(6), 2691–2705. (<a
href="https://doi.org/10.1109/TNNLS.2020.3007548">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks (CNNs) are one of the most successful deep neural networks. Indeed, most of the recent applications related to computer vision are based on CNNs. However, when learning new tasks in a sequential manner, CNNs face catastrophic forgetting: they forget a considerable amount of previously learned tasks while adapting to novel tasks. To overcome this main barrier to continual learning with CNNs, we introduce developmental memory (DM) into a CNN, continually generating submemory networks to learn important features of individual tasks. A novel training method, referred to here as guided learning (GL), guides the newly generated submemory to become an expert on the new task, eventually improving the performance of the overall network. At the same time, the existing submemories attempt to preserve the knowledge of old tasks. Experiments on image classification tasks show that compared with the state-of-the-art algorithms, the proposed CNN with DM not only improves the classification performance on the new image task but also leads to less forgetting of previous image tasks to facilitate continual learning.},
  archive      = {J_TNNLS},
  author       = {Gyeong-Moon Park and Sahng-Min Yoo and Jong-Hwan Kim},
  doi          = {10.1109/TNNLS.2020.3007548},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2691-2705},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Convolutional neural network with developmental memory for continual learning},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). CASNet: A cross-attention siamese network for video salient
object detection. <em>TNNLS</em>, <em>32</em>(6), 2676–2690. (<a
href="https://doi.org/10.1109/TNNLS.2020.3007534">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent works on video salient object detection have demonstrated that directly transferring the generalization ability of image-based models to video data without modeling spatial-temporal information remains nontrivial and challenging. Considering both intraframe accuracy and interframe consistency of saliency detection, this article presents a novel cross-attention based encoder–decoder model under the Siamese framework (CASNet) for video salient object detection. A baseline encoder–decoder model trained with Lovász softmax loss function is adopted as a backbone network to guarantee the accuracy of intraframe salient object detection. Self- and cross-attention modules are incorporated into our model in order to preserve the saliency correlation and improve intraframe salient detection consistency. Extensive experimental results obtained by ablation analysis and cross-data set validation demonstrate the effectiveness of our proposed method. Quantitative results indicate that our CASNet model outperforms 19 state-of-the-art image- and video-based methods on six benchmark data sets.},
  archive      = {J_TNNLS},
  author       = {Yuzhu Ji and Haijun Zhang and Zequn Jie and Lin Ma and Q. M. Jonathan Wu},
  doi          = {10.1109/TNNLS.2020.3007534},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2676-2690},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {CASNet: A cross-attention siamese network for video salient object detection},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Inverse-free discrete ZNN models solving for future matrix
pseudoinverse via combination of extrapolation and ZeaD formulas.
<em>TNNLS</em>, <em>32</em>(6), 2663–2675. (<a
href="https://doi.org/10.1109/TNNLS.2020.3007509">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time-varying matrix pseudoinverse (TVMP) problem has been investigated by many researchers in recent years, but a new class of matrix termed Zhang matrix has been found and not been handled by some conventional models, e.g., Getz-Marsden dynamic model. On the other way, future matrix pseudoinverse (FMP), as a more challenging and intractable discrete-time problem, deserves more attention due to its significant role-playing on some engineering applications, such as redundant manipulator. Based on the zeroing neural network (ZNN), this article concentrates on designing new discrete ZNN models appropriately for computing the FMPs of all matrices of full rank, including the Zhang matrix. First, an inverse-free continuous ZNN model for computing TVMP is derived. Subsequently, Zhang et al. discretization (ZeaD) formulas and equidistant extrapolation formulas are used to discretize the continuous ZNN model to two discrete ZNN models for computing FMPs with different truncation errors. The numerical experiments are conducted for the five conventional discrete models and two new discrete ZNN models. Distinct numerical results substantiate the effectiveness and choiceness of newly proposed models. Finally, one of the newly proposed models is implemented on simulating and physical instances of robot manipulators, respectively, to show its practicability.},
  archive      = {J_TNNLS},
  author       = {Yunong Zhang and Yihong Ling and Min Yang and Song Yang and Zhijun Zhang},
  doi          = {10.1109/TNNLS.2020.3007509},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2663-2675},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Inverse-free discrete ZNN models solving for future matrix pseudoinverse via combination of extrapolation and ZeaD formulas},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Output-feedback robust control of uncertain systems via
online data-driven learning. <em>TNNLS</em>, <em>32</em>(6), 2650–2662.
(<a href="https://doi.org/10.1109/TNNLS.2020.3007414">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although robust control has been studied for decades, the output-feedback robust control design is still challenging in the control field. This article proposes a new approach to address the output-feedback robust control for continuous-time uncertain systems. First, we transform the robust control problem into an optimal control problem of the nominal linear system with a constructive cost function, which allows simplifying the control design. Then, a modified algebraic Riccati equation (MARE) is constructed by further investigating the corresponding relationship with the state-feedback optimal control. To solve the derived MARE online, the vectorization operation and Kronecker&#39;s product are applied to reformulate the output Lyapunov function, and then, a new online data-driven learning method is suggested to learn its solution. Consequently, only the measurable system input and output are used to derive the solution of the MARE. In this case, the output-feedback robust control gain can be obtained without using the unknown system states. The control system stability and convergence of the derived solution are rigorously proved. Two simulation examples are provided to demonstrate the efficacy of the suggested methods.},
  archive      = {J_TNNLS},
  author       = {Jing Na and Jun Zhao and Guanbin Gao and Zican Li},
  doi          = {10.1109/TNNLS.2020.3007414},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2650-2662},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Output-feedback robust control of uncertain systems via online data-driven learning},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). C-CNN: Contourlet convolutional neural networks.
<em>TNNLS</em>, <em>32</em>(6), 2636–2649. (<a
href="https://doi.org/10.1109/TNNLS.2020.3007412">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extracting effective features is always a challenging problem for texture classification because of the uncertainty of scales and the clutter of textural patterns. For texture classification, spectral analysis is traditionally employed in the frequency domain. Recent studies have shown the potential of convolutional neural networks (CNNs) when dealing with the texture classification task in the spatial domain. In this article, we try combining both approaches in different domains for more abundant information and proposed a novel network architecture named contourlet CNN (C-CNN). The network aims to learn sparse and effective feature representations for images. First, the contourlet transform is applied to get the spectral features from an image. Second, the spatial-spectral feature fusion strategy is designed to incorporate the spectral features into CNN architecture. Third, the statistical features are integrated into the network by the statistical feature fusion. Finally, the results are obtained by classifying the fusion features. We also investigated the behavior of the parameters in contourlet decomposition. Experiments on the widely used three texture data sets (kth-tips2-b, DTD, and CUReT) and five remote sensing data sets (UCM, WHU-RS, AID, RSSCN7, and NWPU-RESISC45) demonstrate that the proposed approach outperforms several well-known classification methods in terms of classification accuracy with fewer trainable parameters.},
  archive      = {J_TNNLS},
  author       = {Mengkun Liu and Licheng Jiao and Xu Liu and Lingling Li and Fang Liu and Shuyuan Yang},
  doi          = {10.1109/TNNLS.2020.3007412},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2636-2649},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {C-CNN: Contourlet convolutional neural networks},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Improved linear convergence of training CNNs with
generalizability guarantees: A one-hidden-layer case. <em>TNNLS</em>,
<em>32</em>(6), 2622–2635. (<a
href="https://doi.org/10.1109/TNNLS.2020.3007399">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We analyze the learning problem of one-hidden-layer nonoverlapping convolutional neural networks with the rectified linear unit (ReLU) activation function from the perspective of model estimation. The training outputs are assumed to be generated by the neural network with the unknown ground-truth parameters plus some additive noise, and the objective is to estimate the model parameters by minimizing a nonconvex squared loss function of the training data. Assuming that the training set contains a finite number of samples generated from the Gaussian distribution, we prove that the accelerated gradient descent (GD) algorithm with a proper initialization converges to the ground-truth parameters (up to the noise level) with a linear rate even though the learning problem is nonconvex. Moreover, the convergence rate is proved to be faster than the vanilla GD. The initialization can be achieved by the existing tensor initialization method. In contrast to the existing works that assume an infinite number of samples, we theoretically establish the sample complexity of the required number of training samples. Although the neural network considered here is not deep, this is the first work to show that accelerated GD algorithms can find the global optimizer of the nonconvex learning problem of neural networks. This is also the first work that characterizes the sample complexity of gradient-based methods in learning convolutional neural networks with the nonsmooth ReLU activation function. This work also provides the tightest bound so far of the estimation error with respect to the output noise.},
  archive      = {J_TNNLS},
  author       = {Shuai Zhang and Meng Wang and Jinjun Xiong and Sijia Liu and Pin-Yu Chen},
  doi          = {10.1109/TNNLS.2020.3007399},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2622-2635},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Improved linear convergence of training CNNs with generalizability guarantees: A one-hidden-layer case},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Enhancing explainability of neural networks through
architecture constraints. <em>TNNLS</em>, <em>32</em>(6), 2610–2621. (<a
href="https://doi.org/10.1109/TNNLS.2020.3007259">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prediction accuracy and model explainability are the two most important objectives when developing machine learning algorithms to solve real-world problems. Neural networks are known to possess good prediction performance but suffer from a lack of model interpretability. In this article, we propose to enhance the explainability of neural networks through the following architecture constraints: 1) sparse additive subnetworks; 2) projection pursuit with orthogonality constraint; and 3) smooth function approximation. It leads to an enhanced explainable neural network (ExNN) with a superior balance between prediction performance and model interpretability. We derive sufficient identifiability conditions for the proposed ExNN model. The multiple parameters are simultaneously estimated by a modified minibatch gradient descent method based on the backpropagation algorithm for calculating the derivatives and the Cayley transform for preserving the projection orthogonality. Through simulation study under six different scenarios, we compare the proposed method to several benchmarks, including least absolute shrinkage and selection operator, support vector machine, random forest, extreme learning machine, and multilayer perceptron. It is shown that the proposed ExNN model keeps the flexibility of pursuing high prediction accuracy while attaining improved interpretability. Finally, a real data example is employed as a showcase application.},
  archive      = {J_TNNLS},
  author       = {Zebin Yang and Aijun Zhang and Agus Sudjianto},
  doi          = {10.1109/TNNLS.2020.3007259},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2610-2621},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Enhancing explainability of neural networks through architecture constraints},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Discriminative ridge machine: A classifier for
high-dimensional data or imbalanced data. <em>TNNLS</em>,
<em>32</em>(6), 2595–2609. (<a
href="https://doi.org/10.1109/TNNLS.2020.3006877">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we introduce a discriminative ridge regression approach to supervised classification. It estimates a representation model while accounting for discriminativeness between classes, thereby enabling accurate derivation of categorical information. This new type of regression model extends the existing models, such as ridge, lasso, and group lasso, by explicitly incorporating discriminative information. As a special case, we focus on a quadratic model that admits a closed-form analytical solution. The corresponding classifier is called the discriminative ridge machine (DRM). Three iterative algorithms are further established for the DRM to enhance the efficiency and scalability for real applications. Our approach and the algorithms are applicable to general types of data including images, high-dimensional data, and imbalanced data. We compare the DRM with current state-of-the-art classifiers. Our extensive experimental results show the superior performance of the DRM and confirm the effectiveness of the proposed approach.},
  archive      = {J_TNNLS},
  author       = {Chong Peng and Qiang Cheng},
  doi          = {10.1109/TNNLS.2020.3006877},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2595-2609},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Discriminative ridge machine: A classifier for high-dimensional data or imbalanced data},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust neurooptimal control for a robot via adaptive dynamic
programming. <em>TNNLS</em>, <em>32</em>(6), 2584–2594. (<a
href="https://doi.org/10.1109/TNNLS.2020.3006850">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We aim at the optimization of the tracking control of a robot to improve the robustness, under the effect of unknown nonlinear perturbations. First, an auxiliary system is introduced, and optimal control of the auxiliary system can be seen as an approximate optimal control of the robot. Then, neural networks (NNs) are employed to approximate the solution of the Hamilton-Jacobi-Isaacs equation under the frame of adaptive dynamic programming. Next, based on the standard gradient attenuation algorithm and adaptive critic design, NNs are trained depending on the designed updating law with relaxing the requirement of initial stabilizing control. In light of the Lyapunov stability theory, all the error signals can be proved to be uniformly ultimately bounded. A series of simulation studies are carried out to show the effectiveness of the proposed control.},
  archive      = {J_TNNLS},
  author       = {Linghuan Kong and Wei He and Chenguang Yang and Changyin Sun},
  doi          = {10.1109/TNNLS.2020.3006850},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2584-2594},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Robust neurooptimal control for a robot via adaptive dynamic programming},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Adaptive NN distributed control for time-varying networks
of nonlinear agents with antagonistic interactions. <em>TNNLS</em>,
<em>32</em>(6), 2573–2583. (<a
href="https://doi.org/10.1109/TNNLS.2020.3006840">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes an adaptive neural network (NN) distributed control algorithm for a group of high-order nonlinear agents with nonidentical unknown control directions (UCDs) under signed time-varying topologies. An important lemma on the convergence property is first established for agents with antagonistic time-varying interactions, and then by using Nussbaum-type functions, a new class of NN distributed control algorithms is proposed. If the signed time-varying topologies are cut-balanced and uniformly in time structurally balanced, then convergence is achieved for a group of nonlinear agents. Moreover, the proposed algorithms are adopted to achieve the bipartite consensus of high-order nonlinear agents with nonidentical UCDs under signed graphs, which are uniformly quasi-strongly δ-connected. Finally, simulation examples are given to illustrate the effectiveness of the NN distributed control algorithms.},
  archive      = {J_TNNLS},
  author       = {Qingling Wang and Haris E. Psillakis and Changyin Sun and Frank L. Lewis},
  doi          = {10.1109/TNNLS.2020.3006840},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2573-2583},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive NN distributed control for time-varying networks of nonlinear agents with antagonistic interactions},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SentiVec: Learning sentiment-context vector via kernel
optimization function for sentiment analysis. <em>TNNLS</em>,
<em>32</em>(6), 2561–2572. (<a
href="https://doi.org/10.1109/TNNLS.2020.3006531">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning-based sentiment analysis (SA) methods have drawn more attention in recent years, which calls for more precise word embedding methods. This article proposes SentiVec, a kernel optimization function system for sentiment word embedding, which is based on two phases. The first phase is a supervised learning method, and the second phase consists of two unsupervised updating models, object-word-to-surrounding-words reward model (O2SR) and context-to-object-word reward model (C2OR). SentiVec is aimed at: 1) integrating the statistical information and sentiment orientation into sentiment word vectors and 2) propagating and updating the semantic information to all the word representations in a corpus. Extensive experimental results show that the optimal sentiment vectors successfully extract the features in terms of semantic and sentiment information, which makes it outperform the baseline methods on word similarity, word analogy, and SA tasks.},
  archive      = {J_TNNLS},
  author       = {Luyao Zhu and Wei Li and Yong Shi and Kun Guo},
  doi          = {10.1109/TNNLS.2020.3006531},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2561-2572},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {SentiVec: Learning sentiment-context vector via kernel optimization function for sentiment analysis},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Scene segmentation with dual relation-aware attention
network. <em>TNNLS</em>, <em>32</em>(6), 2547–2560. (<a
href="https://doi.org/10.1109/TNNLS.2020.3006524">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose a Dual Relation-aware Attention Network (DRANet) to handle the task of scene segmentation. How to efficiently exploit context is essential for pixel-level recognition. To address the issue, we adaptively capture contextual information based on the relation-aware attention mechanism. Especially, we append two types of attention modules on the top of the dilated fully convolutional network (FCN), which model the contextual dependencies in spatial and channel dimensions, respectively. In the attention modules, we adopt a self-attention mechanism to model semantic associations between any two pixels or channels. Each pixel or channel can adaptively aggregate context from all pixels or channels according to their correlations. To reduce the high cost of computation and memory caused by the abovementioned pairwise association computation, we further design two types of compact attention modules. In the compact attention modules, each pixel or channel is built into association only with a few numbers of gathering centers and obtains corresponding context aggregation over these gathering centers. Meanwhile, we add a cross-level gating decoder to selectively enhance spatial details that boost the performance of the network. We conduct extensive experiments to validate the effectiveness of our network and achieve new state-of-the-art segmentation performance on four challenging scene segmentation data sets, i.e., Cityscapes, ADE20K, PASCAL Context, and COCO Stuff data sets. In particular, a Mean IoU score of 82.9\% on the Cityscapes test set is achieved without using extra coarse annotated data.},
  archive      = {J_TNNLS},
  author       = {Jun Fu and Jing Liu and Jie Jiang and Yong Li and Yongjun Bao and Hanqing Lu},
  doi          = {10.1109/TNNLS.2020.3006524},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2547-2560},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Scene segmentation with dual relation-aware attention network},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Finite-/fixed-time synchronization of delayed coupled
discontinuous neural networks with unified control schemes.
<em>TNNLS</em>, <em>32</em>(6), 2535–2546. (<a
href="https://doi.org/10.1109/TNNLS.2020.3006516">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, it addresses the problem of finite-/fixed-time synchronization of delayed coupled discontinuous neural networks in the unified framework. To achieve the finite-/fixed-time synchronization and precise estimations of setting time, two novel different kinds of controllers are established, in which one is switching. Then, based on the finite-/fixed-time theorem and Lyapunov function theory, some useful criteria are obtained to select suitable controllers&#39; parameters, which can guarantee error systems converge in the finite time/fixed time with respect to coupled neural networks. Moreover, corresponding estimations of the setting time are also provided. Finally, two numerical examples are introduced to show the effectiveness of the proposed control protocols.},
  archive      = {J_TNNLS},
  author       = {Jian Xiao and Zhigang Zeng and Shiping Wen and Ailong Wu and Leimin Wang},
  doi          = {10.1109/TNNLS.2020.3006516},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2535-2546},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Finite-/Fixed-time synchronization of delayed coupled discontinuous neural networks with unified control schemes},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sparse representations for object- and ego-motion
estimations in dynamic scenes. <em>TNNLS</em>, <em>32</em>(6),
2521–2534. (<a
href="https://doi.org/10.1109/TNNLS.2020.3006467">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Disentangling the sources of visual motion in a dynamic scene during self-movement or ego motion is important for autonomous navigation and tracking. In the dynamic image segments of a video frame containing independently moving objects, optic flow relative to the next frame is the sum of the motion fields generated due to camera and object motion. The traditional ego-motion estimation methods assume the scene to be static, and the recent deep learning-based methods do not separate pixel velocities into object- and ego-motion components. We propose a learning-based approach to predict both ego-motion parameters and object-motion field (OMF) from image sequences using a convolutional autoencoder while being robust to variations due to the unconstrained scene depth. This is achieved by: 1) training with continuous ego-motion constraints that allow solving for ego-motion parameters independently of depth and 2) learning a sparsely activated overcomplete ego-motion field (EMF) basis set, which eliminates the irrelevant components in both static and dynamic segments for the task of ego-motion estimation. In order to learn the EMF basis set, we propose a new differentiable sparsity penalty function that approximates the number of nonzero activations in the bottleneck layer of the autoencoder and enforces sparsity more effectively than L1- and L2-norm-based penalties. Unlike the existing direct ego-motion estimation methods, the predicted global EMF can be used to extract OMF directly by comparing it against the optic flow. Compared with the state-of-the-art baselines, the proposed model performs favorably on pixelwise object- and ego-motion estimation tasks when evaluated on real and synthetic data sets of dynamic scenes.},
  archive      = {J_TNNLS},
  author       = {Hirak J. Kashyap and Charless C. Fowlkes and Jeffrey L. Krichmar},
  doi          = {10.1109/TNNLS.2020.3006467},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2521-2534},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Sparse representations for object- and ego-motion estimations in dynamic scenes},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Transfer increment for generalized zero-shot learning.
<em>TNNLS</em>, <em>32</em>(6), 2506–2520. (<a
href="https://doi.org/10.1109/TNNLS.2020.3006322">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zero-shot learning (ZSL) is a successful paradigm for categorizing objects from the previously unseen classes. However, it suffers from severe performance degradation in the generalized ZSL (GZSL) setting, i.e., to recognize the test images that are from both seen and unseen classes. In this article, we present a simple but effective mechanism for GZSL and more open scenarios based on a transfer-increment strategy. On the one hand, a dual-knowledge-source-based generative model is constructed to tackle the missing data problem. Specifically, the local relational knowledge extracted from the label-embedding space and the global relational knowledge, which is the estimated data center in the feature-embedding space, are concurrently considered to synthesize the virtual exemplars. On the other hand, we further explore the training issue for the generative models under the GZSL setting. Two incremental training modes are designed to learn directly the unseen classes from the synthesized exemplars instead of the training classifiers with the seen and synthesized unseen exemplars together. It not only presents an effective unseen class learning but also requires less computing and storage resources in practical application. Comprehensive experiments are conducted based on five benchmark data sets. In comparison with the state-of-the-art methods, both the generating and training processes are considered for virtual exemplars by the proposed transfer-increment strategy, which results in a significant improvement in the conventional and GZSL tasks.},
  archive      = {J_TNNLS},
  author       = {Liangjun Feng and Chunhui Zhao},
  doi          = {10.1109/TNNLS.2020.3006322},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2506-2520},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Transfer increment for generalized zero-shot learning},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Computing primitive of fully VCSEL-based all-optical spiking
neural network for supervised learning and pattern classification.
<em>TNNLS</em>, <em>32</em>(6), 2494–2505. (<a
href="https://doi.org/10.1109/TNNLS.2020.3006263">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose computing primitive for an all-optical spiking neural network (SNN) based on vertical-cavity surface-emitting lasers (VCSELs) for supervised learning by using biologically plausible mechanisms. The spike-timing-dependent plasticity (STDP) model was established based on the dynamics of the vertical-cavity semiconductor optical amplifier (VCSOA) subject to dual-optical pulse injection. The neuron-synapse self-consistent unified model of the all-optical SNN was developed, which enables reproducing the essential neuron-like dynamics and STDP function. Optical character numbers are trained and tested by the proposed fully VCSEL-based all-optical SNN. Simulation results show that the proposed all-optical SNN is capable of recognizing ten numbers by a supervised learning algorithm, in which the input and output patterns as well as the teacher signals of the all-optical SNN are represented by spatiotemporal fashions. Moreover, the lateral inhibition is not required in our proposed architecture, which is friendly to the hardware implementation. The system-level unified model enables architecture-algorithm codesigns and optimization of all-optical SNN. To the best of our knowledge, the computing primitive of an all-optical SNN based on VCSELs for supervised learning has not yet been reported, which paves the way toward fully VCSEL-based large-scale photonic neuromorphic systems with low power consumption.},
  archive      = {J_TNNLS},
  author       = {Shuiying Xiang and Zhenxing Ren and Ziwei Song and Yahui Zhang and Xingxing Guo and Genquan Han and Yue Hao},
  doi          = {10.1109/TNNLS.2020.3006263},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2494-2505},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Computing primitive of fully VCSEL-based all-optical spiking neural network for supervised learning and pattern classification},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Automatic generation control based on multiple neural
networks with actor-critic strategy. <em>TNNLS</em>, <em>32</em>(6),
2483–2493. (<a
href="https://doi.org/10.1109/TNNLS.2020.3006080">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the conventional automatic generation control (AGC) is inadequate to deal with the strong random disturbance issues induced by the ever-increasing penetration of renewable energy to the power grids, this article proposes a deep-reinforcement-learning-based three-network double-delay actor-critic (TDAC) control strategy for AGC to handle the above problem, which is mainly developed by multiple neural networks to fit the system action strategies and evaluate the value. The proposed strategy can increase the exploration efficiency and the quality of AGC and improve the system control performance using the modified actor-critic (AC) method with incentive heuristic mechanism, while a novel iterative way of the value function is also used to reduce the bias of optimization effectively for achieving optimal coordinated control of the power grid. The simulations are provided in the work to show the control performance of the strategy. Compared with other smart methods, the simulation study demonstrates that TDAC has excellent exploratory stability and learning ability. Meanwhile, it also can improve the dynamic performance of the power system and achieve the regional optimal coordinated control.},
  archive      = {J_TNNLS},
  author       = {Lei Xi and Junnan Wu and Yanchun Xu and Hongbin Sun},
  doi          = {10.1109/TNNLS.2020.3006080},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2483-2493},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Automatic generation control based on multiple neural networks with actor-critic strategy},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fixed-time synchronization of coupled neural networks with
discontinuous activation and mismatched parameters. <em>TNNLS</em>,
<em>32</em>(6), 2470–2482. (<a
href="https://doi.org/10.1109/TNNLS.2020.3005945">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article is concerned with fixed-time synchronization of the nonlinearly coupled neural networks with discontinuous activation and mismatched parameters. First, a novel lemma is proposed to study fixed-time stability, which is less conservative than those in most existing results. Then, based on the new lemma, a discontinuous neural network with mismatched parameters will synchronize to the target state within a settling time via two kinds of unified and simple controllers. The settling time is theoretically estimated, which is independent of the initial values of the considered network. In particular, the estimated settling time is closer to the real synchronization time than those given in the existing literature. Finally, two numerical simulations are presented to illustrate the effectiveness and correctness of our results.},
  archive      = {J_TNNLS},
  author       = {Na Li and Xiaoqun Wu and Jianwen Feng and Yuhua Xu and Jinhu Lü},
  doi          = {10.1109/TNNLS.2020.3005945},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2470-2482},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Fixed-time synchronization of coupled neural networks with discontinuous activation and mismatched parameters},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SP-GAN: Self-growing and pruning generative adversarial
networks. <em>TNNLS</em>, <em>32</em>(6), 2458–2469. (<a
href="https://doi.org/10.1109/TNNLS.2020.3005574">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a new Self-growing and Pruning Generative Adversarial Network (SP-GAN) for realistic image generation. In contrast to traditional GAN models, our SP-GAN is able to dynamically adjust the size and architecture of a network in the training stage by using the proposed self-growing and pruning mechanisms. To be more specific, we first train two seed networks as the generator and discriminator; each contains a small number of convolution kernels. Such small-scale networks are much easier and faster to train than large-capacity networks. Second, in the self-growing step, we replicate the convolution kernels of each seed network to augment the scale of the network, followed by fine-tuning the augmented/expanded network. More importantly, to prevent the excessive growth of each seed network in the self-growing stage, we propose a pruning strategy that reduces the redundancy of an augmented network, yielding the optimal scale of the network. Finally, we design a new adaptive loss function that is treated as a variable loss computational process for the training of the proposed SP-GAN model. By design, the hyperparameters of the loss function can dynamically adapt to different training stages. Experimental results obtained on a set of data sets demonstrate the merits of the proposed method, especially in terms of the stability and efficiency of network training. The source code of the proposed SP-GAN method is publicly available at https://github.com/Lambert-chen/SPGAN.git.},
  archive      = {J_TNNLS},
  author       = {Xiaoning Song and Yao Chen and Zhen-Hua Feng and Guosheng Hu and Dong-Jun Yu and Xiao-Jun Wu},
  doi          = {10.1109/TNNLS.2020.3005574},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2458-2469},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {SP-GAN: Self-growing and pruning generative adversarial networks},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Numerical spiking neural p systems. <em>TNNLS</em>,
<em>32</em>(6), 2443–2457. (<a
href="https://doi.org/10.1109/TNNLS.2020.3005538">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spiking neural P (SN P) systems are a class of discrete neuron-inspired computation models, where information is encoded by the numbers of spikes in neurons and the timing of spikes. However, due to the discontinuous nature of the integrate-and-fire behavior of neurons and the symbolic representation of information, SN P systems are incompatible with the gradient descent-based training algorithms, such as the backpropagation algorithm, and lack the capability of processing the numerical representation of information. In this work, motivated by the numerical nature of numerical P (NP) systems in the area of membrane computing, a novel class of SN P systems is proposed, called numerical SN P (NSN P) systems. More precisely, information is encoded by the values of variables, and the integrate-and-fire way of neurons and the distribution of produced values are described by continuous production functions. The computation power of NSN P systems is investigated. We prove that NSN P is Turing universal as number generating devices, where the production functions in each neuron are linear functions, each involving at most one variable; as number accepting devices, NSN P systems are proved to be universal as well, even if each neuron contains only one production function. These results show that even if a single neuron is simple in the sense that it contains one or two production functions and the production functions in each neuron are linear functions with one variable, a network of simple neurons are still computationally powerful. With the powerful computation power and the characteristic of continuous production functions, developing learning algorithms for NSN P systems is potentially exploitable.},
  archive      = {J_TNNLS},
  author       = {Tingfang Wu and Linqiang Pan and Qiang Yu and Kay Chen Tan},
  doi          = {10.1109/TNNLS.2020.3005538},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2443-2457},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Numerical spiking neural p systems},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Location-aware and regularization-adaptive correlation
filters for robust visual tracking. <em>TNNLS</em>, <em>32</em>(6),
2430–2442. (<a
href="https://doi.org/10.1109/TNNLS.2020.3005447">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Correlation filter (CF) has recently been widely used for visual tracking. The estimation of the search window and the filter-learning strategies is the key component of the CF trackers. Nevertheless, prevalent CF models separately address these issues in heuristic manners. The commonly used CF models directly set the estimated location in the previous frame as the search center for the current one. Moreover, these models usually rely on simple and fixed regularization for filter learning, and thus, their performance is compromised by the search window size and optimization heuristics. To break these limits, this article proposes a location-aware and regularization-adaptive CF (LRCF) for robust visual tracking. LRCF establishes a novel bilevel optimization model to address simultaneously the location-estimation and filter-training problems. We prove that our bilevel formulation can successfully obtain a globally converged CF and the corresponding object location in a collaborative manner. Moreover, based on the LRCF framework, we design two trackers named LRCF-S and LRCF-SA and a series of comparisons to prove the flexibility and effectiveness of the LRCF framework. Extensive experiments on different challenging benchmark data sets demonstrate that our LRCF trackers perform favorably against the state-of-the-art methods in practice.},
  archive      = {J_TNNLS},
  author       = {Risheng Liu and Qianru Chen and Yuansheng Yao and Xin Fan and Zhongxuan Luo},
  doi          = {10.1109/TNNLS.2020.3005447},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2430-2442},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Location-aware and regularization-adaptive correlation filters for robust visual tracking},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep residual autoencoders for expectation
maximization-inspired dictionary learning. <em>TNNLS</em>,
<em>32</em>(6), 2415–2429. (<a
href="https://doi.org/10.1109/TNNLS.2020.3005348">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a neural-network architecture, termed the constrained recurrent sparse autoencoder (CRsAE), that solves convolutional dictionary learning problems, thus establishing a link between dictionary learning and neural networks. Specifically, we leverage the interpretation of the alternating-minimization algorithm for dictionary learning as an approximate expectation-maximization algorithm to develop autoencoders that enable the simultaneous training of the dictionary and regularization parameter (ReLU bias). The forward pass of the encoder approximates the sufficient statistics of the E-step as the solution to a sparse coding problem, using an iterative proximal gradient algorithm called FISTA. The encoder can be interpreted either as a recurrent neural network or as a deep residual network, with two-sided ReLU nonlinearities in both cases. The M-step is implemented via a two-stage backpropagation. The first stage relies on a linear decoder applied to the encoder and a norm-squared loss. It parallels the dictionary update step in dictionary learning. The second stage updates the regularization parameter by applying a loss function to the encoder that includes a prior on the parameter motivated by Bayesian statistics. We demonstrate in an image-denoising task that CRsAE learns Gabor-like filters and that the EM-inspired approach for learning biases is superior to the conventional approach. In an application to recordings of electrical activity from the brain, we demonstrate that CRsAE learns realistic spike templates and speeds up the process of identifying spike times by 900× compared with algorithms based on convex optimization.},
  archive      = {J_TNNLS},
  author       = {Bahareh Tolooshams and Sourav Dey and Demba Ba},
  doi          = {10.1109/TNNLS.2020.3005348},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2415-2429},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Deep residual autoencoders for expectation maximization-inspired dictionary learning},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Toward discriminating and synthesizing motion traces using
deep probabilistic generative models. <em>TNNLS</em>, <em>32</em>(6),
2401–2414. (<a
href="https://doi.org/10.1109/TNNLS.2020.3005325">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mining knowledge from human mobility, such as discriminating motion traces left by different anonymous users, also known as the trajectory-user linking (TUL) problem, is an important task in many applications requiring location-based services (LBSs). However, it inevitably raises an issue that may be aggravated by TUL, i.e., how to defend against location attacks (e.g., deanonymization and location recovery). In this work, we present a Semisupervised Trajectory- User Linking model with Interpretable representation and Gaussian mixture prior (STULIG)-a novel deep probabilistic framework for jointly learning disentangled representation of user trajectories in a semisupervised manner and tackling the location recovery problem. STULIG characterizes multiple latent aspects of human trajectories and their labels into separate latent variables, which can be then used to interpret user check-in styles and improve the performance of trace classification. It can also generate synthetic yet plausible trajectories, thus protecting users&#39; actual locations while preserving the meaningful mobility information for various machine learning tasks. We analyze and evaluate STULIG&#39;s ability to learn disentangled representations, discriminating human traces and generating realistic motions on several real-world mobility data sets. As demonstrated by extensive experimental evaluations, in addition to outperforming the state-of-the-art methods, our method provides intuitive explanations of the classification and generation and sheds lights on the interpretable mobility mining.},
  archive      = {J_TNNLS},
  author       = {Fan Zhou and Xin Liu and Kunpeng Zhang and Goce Trajcevski},
  doi          = {10.1109/TNNLS.2020.3005325},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2401-2414},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Toward discriminating and synthesizing motion traces using deep probabilistic generative models},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep blind hyperspectral image super-resolution.
<em>TNNLS</em>, <em>32</em>(6), 2388–2400. (<a
href="https://doi.org/10.1109/TNNLS.2020.3005234">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The production of a high spatial resolution (HR) hyperspectral image (HSI) through the fusion of a low spatial resolution (LR) HSI with an HR multispectral image (MSI) has underpinned much of the recent progress in HSI super-resolution. The premise of these signs of progress is that both the degeneration from the HR HSI to LR HSI in the spatial domain and the degeneration from the HR HSI to HR MSI in the spectral domain are assumed to be known in advance. However, such a premise is difficult to achieve in practice. To address this problem, we propose to incorporate degeneration estimation into HSI super-resolution and present an unsupervised deep framework for “blind” HSIs super-resolution where the degenerations in both domains are unknown. In this framework, we model the latent HR HSI and the unknown degenerations with deep network structures to regularize them instead of using handcrafted (or shallow) priors. Specifically, we generate the latent HR HSI with an image-specific generator network and structure the degenerations in spatial and spectral domains through a convolution layer and a fully connected layer, respectively. By doing this, the proposed framework can be formulated as an end-to-end deep network learning problem, which is purely supervised by those two input images (i.e., LR HSI and HR MSI) and can be effectively solved by the backpropagation algorithm. Experiments on both natural scene and remote sensing HSI data sets show the superior performance of the proposed method in coping with unknown degeneration either in the spatial domain, spectral domain, or even both of them.},
  archive      = {J_TNNLS},
  author       = {Lei Zhang and Jiangtao Nie and Wei Wei and Yong Li and Yanning Zhang},
  doi          = {10.1109/TNNLS.2020.3005234},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2388-2400},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Deep blind hyperspectral image super-resolution},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sparse modal additive model. <em>TNNLS</em>, <em>32</em>(6),
2373–2387. (<a
href="https://doi.org/10.1109/TNNLS.2020.3005144">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sparse additive models have been successfully applied to high-dimensional data analysis due to the flexibility and interpretability of their representation. However, the existing methods are often formulated using the least-squares loss with learning the conditional mean, which is sensitive to data with the non-Gaussian noises, e.g., skewed noise, heavy-tailed noise, and outliers. To tackle this problem, we propose a new robust regression method, called as sparse modal additive model (SpMAM), by integrating the modal regression metric, the data-dependent hypothesis space, and the weighted l q,1 -norm regularizer (q ≥ 1) into the additive models. Specifically, the modal regression metric assures the model robustness to complex noises via learning the conditional mode, the data-dependent hypothesis space offers the model adaptivity via sample-based presentation, and the l q,1 -norm regularizer addresses the algorithmic interpretability via sparse variable selection. In theory, the proposed SpMAM enjoys statistical guarantees on asymptotic consistency for regression estimation and variable selection simultaneously. Experimental results on both synthetic and real-world benchmark data sets validate the effectiveness and robustness of the proposed model.},
  archive      = {J_TNNLS},
  author       = {Hong Chen and Yingjie Wang and Feng Zheng and Cheng Deng and Heng Huang},
  doi          = {10.1109/TNNLS.2020.3005144},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2373-2387},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Sparse modal additive model},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Formation control with collision avoidance through deep
reinforcement learning using model-guided demonstration. <em>TNNLS</em>,
<em>32</em>(6), 2358–2372. (<a
href="https://doi.org/10.1109/TNNLS.2020.3004893">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generating collision-free, time-efficient paths in an uncertain dynamic environment poses huge challenges for the formation control with collision avoidance (FCCA) problem in a leader-follower structure. In particular, the followers have to take both formation maintenance and collision avoidance into account simultaneously. Unfortunately, most of the existing works are simple combinations of methods dealing with the two problems separately. In this article, a new method based on deep reinforcement learning (RL) is proposed to solve the problem of FCCA. Especially, the learning-based policy is extended to the field of formation control, which involves a two-stage training framework: an imitation learning (IL) and later an RL. In the IL stage, a model-guided method consisting of a consensus theory-based formation controller and an optimal reciprocal collision avoidance strategy is designed to speed up training and increase efficiency. In the RL stage, a compound reward function is presented to guide the training. In addition, we design a formation-oriented network structure to perceive the environment. Long short-term memory is adopted to enable the network structure to perceive the information of obstacles of an uncertain number, and a transfer training approach is adopted to improve the generalization of the network in different scenarios. Numerous representative simulations are conducted, and our method is further deployed to an experimental platform based on a multiomnidirectional-wheeled car system. The effectiveness and practicability of our proposed method are validated through both the simulation and experiment results.},
  archive      = {J_TNNLS},
  author       = {Zezhi Sui and Zhiqiang Pu and Jianqiang Yi and Shiguang Wu},
  doi          = {10.1109/TNNLS.2020.3004893},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2358-2372},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Formation control with collision avoidance through deep reinforcement learning using model-guided demonstration},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Stochastic strongly convex optimization via distributed
epoch stochastic gradient algorithm. <em>TNNLS</em>, <em>32</em>(6),
2344–2357. (<a
href="https://doi.org/10.1109/TNNLS.2020.3004723">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article considers the problem of stochastic strongly convex optimization over a network of multiple interacting nodes. The optimization is under a global inequality constraint and the restriction that nodes have only access to the stochastic gradients of their objective functions. We propose an efficient distributed non-primal-dual algorithm, by incorporating the inequality constraint into the objective via a smoothing technique. We show that the proposed algorithm achieves an optimal O ((1)/(T)) ( T is the total number of iterations) convergence rate in the mean square distance from the optimal solution. In particular, we establish a high probability bound for the proposed algorithm, by showing that with a probability at least 1-δ, the proposed algorithm converges at a rate of O (ln(ln(T)/δ)/ T). Finally, we provide numerical experiments to demonstrate the efficacy of the proposed algorithm.},
  archive      = {J_TNNLS},
  author       = {Deming Yuan and Daniel W. C. Ho and Shengyuan Xu},
  doi          = {10.1109/TNNLS.2020.3004723},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2344-2357},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Stochastic strongly convex optimization via distributed epoch stochastic gradient algorithm},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Broad colorization. <em>TNNLS</em>, <em>32</em>(6),
2330–2343. (<a
href="https://doi.org/10.1109/TNNLS.2020.3004634">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The scribble- and example-based colorization methods have fastidious requirements for users, and the training process of deep neural networks for colorization is quite time-consuming. We instead proposed an automatic colorization approach with no dependence on user input and no need to endure long training time, which combines local features and global features of the input gray-scale images. Low-, mid-, and high-level features are united as local features representing cues existed in the gray-scale image. The global feature is regarded as data prior to guiding the colorization process. The local broad learning system is trained for getting the chrominance value of each pixel from the local features, which could be expressed as a chrominance map according to the position of pixels. Then, the global broad learning system is trained to refine the chrominance map. There are no requirements for users in our approach, and the training time of our framework is an order of magnitude faster than the traditional methods based on deep neural networks. To increase the user&#39;s subjective initiative, our system allows users to increase training data without retraining the system. Substantial experimental results have shown that our approach outperforms state-of-the-art methods.},
  archive      = {J_TNNLS},
  author       = {Yuxi Jin and Bin Sheng and Ping Li and C. L. Philip Chen},
  doi          = {10.1109/TNNLS.2020.3004634},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2330-2343},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Broad colorization},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Maximum information exploitation using broad learning system
for large-scale chaotic time-series prediction. <em>TNNLS</em>,
<em>32</em>(6), 2320–2329. (<a
href="https://doi.org/10.1109/TNNLS.2020.3004253">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How to make full use of the evolution information of chaotic systems for time-series prediction is a difficult issue in dynamical system modeling. In this article, we propose a maximum information exploitation broad learning system (MIE-BLS) for extreme information utilization of large-scale chaotic time-series modeling. An improved leaky integrator dynamical reservoir is introduced in order to capture the linear information of chaotic systems effectively. It can not only capture the information of the current state but also achieve the compromise with historical states in the dynamical system. Furthermore, the feature is mapped to the enhancement layer by nonlinear random mapping to exploit nonlinear information. The cascading mechanism promotes the information propagation and achieves feature reactivation in dynamical modeling. Discussions about maximum information exploration and the comparisons with ResNet, DenseNet, and HighwayNet are presented in this article. Simulation results on four large-scale data sets illustrate that MIE-BLS could achieve better performance of information exploration in large-scale dynamical system modeling.},
  archive      = {J_TNNLS},
  author       = {Min Han and Weijie Li and Shoubo Feng and Tie Qiu and C. L. Philip Chen},
  doi          = {10.1109/TNNLS.2020.3004253},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2320-2329},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Maximum information exploitation using broad learning system for large-scale chaotic time-series prediction},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). IncDet: In defense of elastic weight consolidation for
incremental object detection. <em>TNNLS</em>, <em>32</em>(6), 2306–2319.
(<a href="https://doi.org/10.1109/TNNLS.2020.3002583">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Elastic weight consolidation (EWC) has been successfully applied for general incremental learning to overcome the catastrophic forgetting issue. It adaptively constrains each parameter of the new model not to deviate much from its counterpart in the old model during fine-tuning on new class data sets, according to its importance weight for old tasks. However, the previous study demonstrates that it still suffers from catastrophic forgetting when directly used in object detection. In this article, we show EWC is effective for incremental object detection if with critical adaptations. First, we conduct controlled experiments to identify two core issues why EWC fails if trivially applied to incremental detection: 1) the absence of old class annotations in new class images makes EWC misclassify objects of old classes in these images as background and 2) the quadratic regularization loss in EWC easily leads to gradient explosion when balancing old and new classes. Then, based on the abovementioned findings, we propose the corresponding solutions to tackle these issues: 1) utilize pseudobounding box annotations of old classes on new data sets to compensate for the absence of old class annotations and 2) adopt a novel Huber regularization instead of the original quadratic loss to prevent from unstable training. Finally, we propose a general EWC-based incremental object detection framework and implement it under both Fast R-CNN and Faster R-CNN, showing its flexibility and versatility. In terms of either the final performance or the performance drop with respect to the upper bound of joint training on all seen classes, evaluations on the PASCAL VOC and COCO data sets show that our method achieves a new state of the art.},
  archive      = {J_TNNLS},
  author       = {Liyang Liu and Zhanghui Kuang and Yimin Chen and Jing-Hao Xue and Wenming Yang and Wayne Zhang},
  doi          = {10.1109/TNNLS.2020.3002583},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2306-2319},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {IncDet: In defense of elastic weight consolidation for incremental object detection},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021h). IEEE computational intelligence society information.
<em>TNNLS</em>, <em>32</em>(5), C3. (<a
href="https://doi.org/10.1109/TNNLS.2021.3073390">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Provides a listing of current committee members and society officers.},
  archive      = {J_TNNLS},
  doi          = {10.1109/TNNLS.2021.3073390},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {C3},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {IEEE computational intelligence society information},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Convolutional neural networks with dynamic regularization.
<em>TNNLS</em>, <em>32</em>(5), 2299–2304. (<a
href="https://doi.org/10.1109/TNNLS.2020.2997044">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Regularization is commonly used for alleviating overfitting in machine learning. For convolutional neural networks (CNNs), regularization methods, such as DropBlock and Shake-Shake, have illustrated the improvement in the generalization performance. However, these methods lack a self-adaptive ability throughout training. That is, the regularization strength is fixed to a predefined schedule, and manual adjustments are required to adapt to various network architectures. In this article, we propose a dynamic regularization method for CNNs. Specifically, we model the regularization strength as a function of the training loss. According to the change of the training loss, our method can dynamically adjust the regularization strength in the training procedure, thereby balancing the underfitting and overfitting of CNNs. With dynamic regularization, a large-scale model is automatically regularized by the strong perturbation, and vice versa. Experimental results show that the proposed method can improve the generalization capability on off-the-shelf network architectures and outperform state-of-the-art regularization methods.},
  archive      = {J_TNNLS},
  author       = {Yi Wang and Zhen-Peng Bian and Junhui Hou and Lap-Pui Chau},
  doi          = {10.1109/TNNLS.2020.2997044},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {2299-2304},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Convolutional neural networks with dynamic regularization},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Master–slave synchronization of delayed neural networks with
time-varying control. <em>TNNLS</em>, <em>32</em>(5), 2292–2298. (<a
href="https://doi.org/10.1109/TNNLS.2020.2996224">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This brief investigates the master-slave synchronization problem of delayed neural networks with general time-varying control. Assuming a linear feedback controller with time-varying control gain, the synchronization problem is recast into the stability problem of a delayed system with a time-varying coefficient. The main theorem is established in terms of the time average of the control gain by using the Lyapunov-Razumikhin theorem. Moreover, the proposed framework encompasses some general intermittent control schemes, such as the switched control gain with external disturbance and intermittent control with pulse-modulated gain function, while some useful corollaries are consequently deduced. Interestingly, our theorem also provides a solution for regaining stability under control failure. The validity of the theorem and corollaries is further demonstrated with numerical examples.},
  archive      = {J_TNNLS},
  author       = {Qiang Jia and Eric S. Mwanandiye and Wallace K. S. Tang},
  doi          = {10.1109/TNNLS.2020.2996224},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {2292-2298},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Master–Slave synchronization of delayed neural networks with time-varying control},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An optimal algorithm for the stochastic bandits while
knowing the near-optimal mean reward. <em>TNNLS</em>, <em>32</em>(5),
2285–2291. (<a
href="https://doi.org/10.1109/TNNLS.2020.2995920">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This brief studies a variation of the stochastic multiarmed bandit (MAB) problems, where the agent knows the a priori knowledge named the near-optimal mean reward (NoMR). In common MAB problems, an agent tries to find the optimal arm without knowing the optimal mean reward. However, in more practical applications, the agent can usually get an estimation of the optimal mean reward defined as NoMR. For instance, in an online Web advertising system based on MAB methods, a user&#39;s near-optimal average click rate (NoMR) can be roughly estimated from his/her demographic characteristics. As a result, application of the NoMR is efficient at improving the algorithm&#39;s performance. First, we formalize the stochastic MAB problem by knowing the NoMR that is in between the suboptimal mean reward and the optimal mean reward. Second, we use the cumulative regret as the performance metric for our problem, and we get that this problem&#39;s lower bound of the cumulative regret is Ω(1/Δ), where Δ is the difference between the suboptimal mean reward and the optimal mean reward. Compared with the conventional MAB problem with the increasing logarithmic lower bound of the regret, our regret lower bound is uniform with the learning step. Third, a novel algorithm, NoMR-BANDIT, is set forth to solve this problem. In NoMR-BANDIT, the NoMR is used to design an efficient exploration strategy. In addition, we analyzed the regret&#39;s upper bound in NoMR-BANDIT and concluded that it also has a uniform upper bound of O(1/Δ), which is in the same order as the lower bound. Consequently, NoMR-BANDIT is an optimal algorithm of this problem. To enhance our method&#39;s generalization, CASCADE-BANDIT based on NoMR-BANDIT is proposed to solve the problem, where NoMR is less than the suboptimal mean reward. CASCADE-BANDIT has an upper bound of O(Δ logn), where n represents the learning step, and the order of O(Δlogn) is the same with that of the conventional MAB methods. Finally, extensive experimental results demonstrated that the established NoMR-BANDIT is more efficient than the compared bandit solutions. After sufficient iterations, NOMR-BANDIT saved 10\%-80\% more cumulative regret than the state of the art.},
  archive      = {J_TNNLS},
  author       = {Shangdong Yang and Yang Gao},
  doi          = {10.1109/TNNLS.2020.2995920},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {2285-2291},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {An optimal algorithm for the stochastic bandits while knowing the near-optimal mean reward},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Mittag–leffler synchronization of delayed fractional
memristor neural networks via adaptive control. <em>TNNLS</em>,
<em>32</em>(5), 2279–2284. (<a
href="https://doi.org/10.1109/TNNLS.2020.2995718">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This brief is devoted to exploring the global Mittag-Leffler (ML) synchronization problem of fractional-order memristor neural networks (FOMNNs) with leakage delay via a hybrid adaptive controller. By applying Fillipov&#39;s theory and the Lyapunov functional method, the novel algebraic sufficient condition for the global ML synchronization of FOMNNs is derived. Finally, a simulation example is presented to show the practicability of our findings.},
  archive      = {J_TNNLS},
  author       = {Yonggui Kao and Ying Li and Ju H. Park and Xiangyong Chen},
  doi          = {10.1109/TNNLS.2020.2995718},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {2279-2284},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Mittag–Leffler synchronization of delayed fractional memristor neural networks via adaptive control},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021c). Two-level complex-valued hopfield neural networks.
<em>TNNLS</em>, <em>32</em>(5), 2274–2278. (<a
href="https://doi.org/10.1109/TNNLS.2020.2995413">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In multistate neural associative memories, some neurons have small noise and the others have large noise. If we know which neurons have small noise, the noise tolerance could be improved. In this brief, we provide a novel method to reinforce neurons with small noise and apply our new method to images with the Gaussian noise. A complex-valued multistate neuron is decomposed to two neurons, referred to as high and low neurons. For the Gaussian noise, the high neurons are expected to have small noise. The noise tolerance is improved by reinforcement of high neurons. The computer simulations support the efficiency of reinforced neurons.},
  archive      = {J_TNNLS},
  author       = {Masaki Kobayashi},
  doi          = {10.1109/TNNLS.2020.2995413},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {2274-2278},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Two-level complex-valued hopfield neural networks},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Outlier-resistant remote state estimation for recurrent
neural networks with mixed time-delays. <em>TNNLS</em>, <em>32</em>(5),
2266–2273. (<a
href="https://doi.org/10.1109/TNNLS.2020.2991151">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this brief, a new outlier-resistant state estimation (SE) problem is addressed for a class of recurrent neural networks (RNNs) with mixed time-delays. The mixed time delays comprise both discrete and distributed delays that occur frequently in signal transmissions among artificial neurons. Measurement outputs are sometimes subject to abnormal disturbances (resulting probably from sensor aging/outages/faults/failures and unpredictable environmental changes) leading to measurement outliers that would deteriorate the estimation performance if directly taken into the innovation in the estimator design. We propose to use a certain confidence-dependent saturation function to mitigate the side effects from the measurement outliers on the estimation error dynamics (EEDs). Through using a combination of Lyapunov-Krasovskii functional and inequality manipulations, a delay-dependent criterion is established for the existence of the outlier-resistant state estimator ensuring that the corresponding EED achieves the asymptotic stability with a prescribed H ∞ performance index. Then, the explicit characterization of the estimator gain is obtained by solving a convex optimization problem. Finally, numerical simulation is carried out to demonstrate the usefulness of the derived theoretical results.},
  archive      = {J_TNNLS},
  author       = {Jiahui Li and Zidong Wang and Hongli Dong and Gheorghita Ghinea},
  doi          = {10.1109/TNNLS.2020.2991151},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {2266-2273},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Outlier-resistant remote state estimation for recurrent neural networks with mixed time-delays},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). KGSNet: Key-point-guided super-resolution network for
pedestrian detection in the wild. <em>TNNLS</em>, <em>32</em>(5),
2251–2265. (<a
href="https://doi.org/10.1109/TNNLS.2020.3004819">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In real-world scenarios (i.e., in the wild), pedestrians are often far from the camera (i.e., small scale), and they often gather together and occlude with each other (i.e., heavily occluded). However, detecting these small-scale and heavily occluded pedestrians remains a challenging problem for the existing pedestrian detection methods. We argue that these problems arise because of two factors: 1) insufficient resolution of feature maps for handling small-scale pedestrians and 2) lack of an effective strategy for extracting body part information that can directly deal with occlusion. To solve the above-mentioned problems, in this article, we propose a key-point-guided super-resolution network (coined KGSNet) for detecting these small-scale and heavily occluded pedestrians in the wild. Specifically, to address factor 1), a super-resolution network is first trained to generate a clear super-resolution pedestrian image from a small-scale one. In the super-resolution network, we exploit key points of the human body to guide the super-resolution network to recover fine details of the human body region for easier pedestrian detection. To address factor 2), a part estimation module is proposed to encode the semantic information of different human body parts where four semantic body parts (i.e., head and upper/middle/bottom body) are extracted based on the key points. Finally, based on the generated clear super-resolved pedestrian patches padded with the extracted semantic body part images at the image level, a classification network is trained to further distinguish pedestrians/backgrounds from the inputted proposal regions. Both proposed networks (i.e., super-resolution network and classification network) are optimized in an alternating manner and trained in an end-to-end fashion. Extensive experiments on the challenging CityPersons data set demonstrate the effectiveness of the proposed method, which achieves superior performance over previous state-of-the-art methods, especially for those small-scale and heavily occluded instances. Beyond this, we also achieve state-of-the-art performance (i.e., 3.89\% MR -2 on the reasonable subset) on the Caltech data set.},
  archive      = {J_TNNLS},
  author       = {Yongqiang Zhang and Yancheng Bai and Mingli Ding and Shibiao Xu and Bernard Ghanem},
  doi          = {10.1109/TNNLS.2020.3004819},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {2251-2265},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {KGSNet: Key-point-guided super-resolution network for pedestrian detection in the wild},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Neural-network-based event-triggered adaptive control of
nonaffine nonlinear multiagent systems with dynamic uncertainties.
<em>TNNLS</em>, <em>32</em>(5), 2239–2250. (<a
href="https://doi.org/10.1109/TNNLS.2020.3003950">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article addresses the adaptive event-triggered neural control problem for nonaffine pure-feedback nonlinear multiagent systems with dynamic disturbance, unmodeled dynamics, and dead-zone input. Radial basis function neural networks are applied to approximate the unknown nonlinear function. A dynamic signal is constructed to deal with the design difficulties in the unmodeled dynamics. Moreover, to reduce the communication burden, we propose an event-triggered strategy with a varying threshold. Based on the Lyapunov function method and adaptive neural control approach, a novel event-triggered control protocol is constructed, which realizes that the outputs of all followers converge to a neighborhood of the leader&#39;s output and ensures that all signals are bounded in the closed-loop system. An illustrative simulation example is applied to verify the usefulness of the proposed algorithms.},
  archive      = {J_TNNLS},
  author       = {Hongjing Liang and Guangliang Liu and Huaguang Zhang and Tingwen Huang},
  doi          = {10.1109/TNNLS.2020.3003950},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {2239-2250},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Neural-network-based event-triggered adaptive control of nonaffine nonlinear multiagent systems with dynamic uncertainties},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Automated social text annotation with joint multilabel
attention networks. <em>TNNLS</em>, <em>32</em>(5), 2224–2238. (<a
href="https://doi.org/10.1109/TNNLS.2020.3002798">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automated social text annotation is the task of suggesting a set of tags for shared documents on social media platforms. The automated annotation process can reduce users&#39; cognitive overhead in tagging and improve tag management for better search, browsing, and recommendation of documents. It can be formulated as a multilabel classification problem. We propose a novel deep learning-based method for this problem and design an attention-based neural network with semantic-based regularization, which can mimic users&#39; reading and annotation behavior to formulate better document representation, leveraging the semantic relations among labels. The network separately models the title and the content of each document and injects an explicit, title-guided attention mechanism into each sentence. To exploit the correlation among labels, we propose two semantic-based loss regularizers, i.e., similarity and subsumption, which enforce the output of the network to conform to label semantics. The model with the semantic-based loss regularizers is referred to as the joint multilabel attention network (JMAN). We conducted a comprehensive evaluation study and compared JMAN to the state-of-the-art baseline models, using four large, real-world social media data sets. In terms of F 1 , JMAN significantly outperformed bidirectional gated recurrent unit (Bi-GRU) relatively by around 12.8\%-78.6\% and the hierarchical attention network (HAN) by around 3.9\%-23.8\%. The JMAN model demonstrates advantages in convergence and training speed. Further improvement of performance was observed against latent Dirichlet allocation (LDA) and support vector machine (SVM). When applying the semantic-based loss regularizers, the performance of HAN and Bi-GRU in terms of F 1 was also boosted. It is also found that dynamic update of the label semantic matrices (JMAN d ) has the potential to further improve the performance of JMAN but at the cost of substantial memory and warrants further study.},
  archive      = {J_TNNLS},
  author       = {Hang Dong and Wei Wang and Kaizhu Huang and Frans Coenen},
  doi          = {10.1109/TNNLS.2020.3002798},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {2224-2238},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Automated social text annotation with joint multilabel attention networks},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Nonnegative blind source separation for ill-conditioned
mixtures via john ellipsoid. <em>TNNLS</em>, <em>32</em>(5), 2209–2223.
(<a href="https://doi.org/10.1109/TNNLS.2020.3002618">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nonnegative blind source separation (nBSS) is often a challenging inverse problem, namely, when the mixing system is ill-conditioned. In this work, we focus on an important nBSS instance, known as hyperspectral unmixing (HU) in remote sensing. HU is a matrix factorization problem aimed at factoring the so-called endmember matrix, holding the material hyperspectral signatures, and the abundance matrix, holding the material fractions at each image pixel. The hyperspectral signatures are usually highly correlated, leading to a fast decay of the singular values (and, hence, high condition number) of the endmember matrix, so HU often introduces an ill-conditioned nBSS scenario. We introduce a new theoretical framework to attack such tough scenarios via the John ellipsoid (JE) in functional analysis. The idea is to identify the maximum volume ellipsoid inscribed in the data convex hull, followed by affinely mapping such ellipsoid into a Euclidean ball. By applying the same affine mapping to the data mixtures, we prove that the endmember matrix associated with the mapped data has condition number 1, the lowest possible, and that these (preconditioned) endmembers form a regular simplex. Exploiting this regular structure, we design a novel nBSS criterion with a provable identifiability guarantee and devise an algorithm to realize the criterion. Moreover, for the first time, the optimization problem for computing JE is exactly solved for a large-scale instance; our solver employs a split augmented Lagrangian shrinkage algorithm with all proximal operators solved by closed-form solutions. The competitiveness of the proposed method is illustrated by numerical simulations and real data experiments.},
  archive      = {J_TNNLS},
  author       = {Chia-Hsiang Lin and José M. Bioucas-Dias},
  doi          = {10.1109/TNNLS.2020.3002618},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {2209-2223},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Nonnegative blind source separation for ill-conditioned mixtures via john ellipsoid},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A cheap feature selection approach for the k-means
algorithm. <em>TNNLS</em>, <em>32</em>(5), 2195–2208. (<a
href="https://doi.org/10.1109/TNNLS.2020.3002576">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increase in the number of features that need to be analyzed in a wide variety of areas, such as genome sequencing, computer vision, or sensor networks, represents a challenge for the K-means algorithm. In this regard, different dimensionality reduction approaches for the K-means algorithm have been designed recently, leading to algorithms that have proved to generate competitive clusterings. Unfortunately, most of these techniques tend to have fairly high computational costs and/or might not be easy to parallelize. In this article, we propose a fully parallelizable feature selection technique intended for the K-means algorithm. The proposal is based on a novel feature relevance measure that is closely related to the K-means error of a given clustering. Given a disjoint partition of the features, the technique consists of obtaining a clustering for each subset of features and selecting the m features with the highest relevance measure. The computational cost of this approach is just O (m·max{n·K,logm}) per subset of features. We additionally provide a theoretical analysis on the quality of the obtained solution via our proposal and empirically analyze its performance with respect to well-known feature selection and feature extraction techniques. Such an analysis shows that our proposal consistently obtains the results with lower K-means error than all the considered feature selection techniques: Laplacian scores, maximum variance, multicluster feature selection, and random selection while also requiring similar or lower computational times than these approaches. Moreover, when compared with feature extraction techniques, such as random projections, the proposed approach also shows a noticeable improvement in both error and computational time.},
  archive      = {J_TNNLS},
  author       = {Marco Capó and Aritz Pérez and Jose A. Lozano},
  doi          = {10.1109/TNNLS.2020.3002576},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {2195-2208},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A cheap feature selection approach for the K-means algorithm},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Artificial evolution network: A computational perspective on
the expansibility of the nervous system. <em>TNNLS</em>, <em>32</em>(5),
2180–2194. (<a
href="https://doi.org/10.1109/TNNLS.2020.3002556">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neurobiologists recently found the brain can use sudden emerged channels to process information. Based on this finding, we put forward a question whether we can build a computation model that is able to integrate a sudden emerged new type of perceptual channel into itself in an online way. If such a computation model can be established, it will introduce a channel-free property to the computation model and meanwhile deepen our understanding about the extendibility of the brain. In this article, a biologically inspired neural network named artificial evolution (AE) network is proposed to handle the problem. When a new perceptual channel emerges, the neurons in the network can grow new connections to connect the emerged channel according to the Hebb rule. In this article, we design a sensory channel expansion experiment to test the AE network. The experimental results demonstrate that the AE network can handle the sudden emerged perceptual channels effectively.},
  archive      = {J_TNNLS},
  author       = {You-Lu Xing and Hui Sun and Gui-Huan Feng and Fu-Rao Shen and Jian Zhao},
  doi          = {10.1109/TNNLS.2020.3002556},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {2180-2194},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Artificial evolution network: A computational perspective on the expansibility of the nervous system},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Reservoir computing approaches for representation and
classification of multivariate time series. <em>TNNLS</em>,
<em>32</em>(5), 2169–2179. (<a
href="https://doi.org/10.1109/TNNLS.2020.3001377">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classification of multivariate time series (MTS) has been tackled with a large variety of methodologies and applied to a wide range of scenarios. Reservoir computing (RC) provides efficient tools to generate a vectorial, fixed-size representation of the MTS that can be further processed by standard classifiers. Despite their unrivaled training speed, MTS classifiers based on a standard RC architecture fail to achieve the same accuracy of fully trainable neural networks. In this article, we introduce the reservoir model space, an unsupervised approach based on RC to learn vectorial representations of MTS. Each MTS is encoded within the parameters of a linear model trained to predict a low-dimensional embedding of the reservoir dynamics. Compared with other RC methods, our model space yields better representations and attains comparable computational performance due to an intermediate dimensionality reduction procedure. As a second contribution, we propose a modular RC framework for MTS classification, with an associated open-source Python library. The framework provides different modules to seamlessly implement advanced RC architectures. The architectures are compared with other MTS classifiers, including deep learning models and time series kernels. Results obtained on the benchmark and real-world MTS data sets show that RC classifiers are dramatically faster and, when implemented using our proposed representation, also achieve superior classification accuracy.},
  archive      = {J_TNNLS},
  author       = {Filippo Maria Bianchi and Simone Scardapane and Sigurd Løkse and Robert Jenssen},
  doi          = {10.1109/TNNLS.2020.3001377},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {2169-2179},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Reservoir computing approaches for representation and classification of multivariate time series},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hybrid neural adaptive control for practical tracking of
markovian switching networks. <em>TNNLS</em>, <em>32</em>(5), 2157–2168.
(<a href="https://doi.org/10.1109/TNNLS.2020.3001009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While neural adaptive control is widely used for dealing with continuous- or discrete-time dynamical systems, less is known about its mechanism and performance in hybrid dynamical systems. This article develops analytical tools to investigate the neural adaptive tracking control of the hybrid Markovian switching networks with heterogeneous nonlinear dynamics and randomly switched topologies. A gradient-descent adaptation law built on neural networks (NNs) is presented for efficient distributed adaptive control. It is shown that the proposed control scheme can guarantee a stable closed-loop error system for any positive control gain and tuning gain. The tracking error is demonstrated to be practically uniformly exponentially stable with a threshold in the mean-square sense. This study further reveals how the topological structure affects the NN function, by measuring the influence of the switched topologies on the learning performance.},
  archive      = {J_TNNLS},
  author       = {Bin Hu and Xinghuo Yu and Zhi-Hong Guan and Jürgen Kurths and Guanrong Chen},
  doi          = {10.1109/TNNLS.2020.3001009},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {2157-2168},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Hybrid neural adaptive control for practical tracking of markovian switching networks},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Weakly supervised person re-ID: Differentiable graphical
learning and a new benchmark. <em>TNNLS</em>, <em>32</em>(5), 2142–2156.
(<a href="https://doi.org/10.1109/TNNLS.2020.2999517">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person reidentification (Re-ID) benefits greatly from the accurate annotations of existing data sets (e.g., CUHK03 and Market-1501), which are quite expensive because each image in these data sets has to be assigned with a proper label. In this work, we ease the annotation of Re-ID by replacing the accurate annotation with inaccurate annotation, i.e., we group the images into bags in terms of time and assign a bag-level label for each bag. This greatly reduces the annotation effort and leads to the creation of a large-scale Re-ID benchmark called SYSU- 30k. The new benchmark contains 30k individuals, which is about 20 times larger than CUHK03 (1.3k individuals) and Market-1501 (1.5k individuals), and 30 times larger than ImageNet (1k categories). It sums up to 29606918 images. Learning a Re-ID model with bag-level annotation is called the weakly supervised Re-ID problem. To solve this problem, we introduce a differentiable graphical model to capture the dependencies from all images in a bag and generate a reliable pseudolabel for each person&#39;s image. The pseudolabel is further used to supervise the learning of the Re-ID model. Compared with the fully supervised Re-ID models, our method achieves state-of-the-art performance on SYSU- 30k and other data sets. The code, data set, and pretrained model will be available at https://github.com/wanggrun/SYSU-30k.},
  archive      = {J_TNNLS},
  author       = {Guangrun Wang and Guangcong Wang and Xujie Zhang and Jianhuang Lai and Zhengtao Yu and Liang Lin},
  doi          = {10.1109/TNNLS.2020.2999517},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {2142-2156},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Weakly supervised person re-ID: Differentiable graphical learning and a new benchmark},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). When dictionary learning meets deep learning: Deep
dictionary learning and coding network for image recognition with
limited data. <em>TNNLS</em>, <em>32</em>(5), 2129–2141. (<a
href="https://doi.org/10.1109/TNNLS.2020.2997289">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a new deep dictionary learning and coding network (DDLCN) for image-recognition tasks with limited data. The proposed DDLCN has most of the standard deep learning layers (e.g., input/output, pooling, and fully connected), but the fundamental convolutional layers are replaced by our proposed compound dictionary learning and coding layers. The dictionary learning learns an overcomplete dictionary for input training data. At the deep coding layer, a locality constraint is added to guarantee that the activated dictionary bases are close to each other. Then, the activated dictionary atoms are assembled and passed to the compound dictionary learning and coding layers. In this way, the activated atoms in the first layer can be represented by the deeper atoms in the second dictionary. Intuitively, the second dictionary is designed to learn the fine-grained components shared among the input dictionary atoms; thus, a more informative and discriminative low-level representation of the dictionary atoms can be obtained. We empirically compare DDLCN with several leading dictionary learning methods and deep learning models. Experimental results on five popular data sets show that DDLCN achieves competitive results compared with state-of-the-art methods when the training data are limited. Code is available at https://github.com/Ha0Tang/DDLCN.},
  archive      = {J_TNNLS},
  author       = {Hao Tang and Hong Liu and Wei Xiao and Nicu Sebe},
  doi          = {10.1109/TNNLS.2020.2997289},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {2129-2141},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {When dictionary learning meets deep learning: Deep dictionary learning and coding network for image recognition with limited data},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Finite-time output synchronization of undirected and
directed coupled neural networks with output coupling. <em>TNNLS</em>,
<em>32</em>(5), 2117–2128. (<a
href="https://doi.org/10.1109/TNNLS.2020.2997195">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article focuses on the finite-time output synchronization problem for undirected and directed coupled neural networks with output coupling (CNNOC). Based on the designed state feedback controllers and some inequality techniques, we present several finite-time output synchronization criteria for these network models. In addition, two kinds of coupling-weight adjustment strategies are also developed to guarantee the finite-time output synchronization of undirected and directed CNNOC. Finally, two numerical examples are also provided to demonstrate the effectiveness of the theoretical results.},
  archive      = {J_TNNLS},
  author       = {Qing Wang and Jin-Liang Wang},
  doi          = {10.1109/TNNLS.2020.2997195},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {2117-2128},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Finite-time output synchronization of undirected and directed coupled neural networks with output coupling},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Privacy-preserving cost-sensitive learning. <em>TNNLS</em>,
<em>32</em>(5), 2105–2116. (<a
href="https://doi.org/10.1109/TNNLS.2020.2996972">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cost-sensitive learning methods guaranteeing privacy are becoming crucial nowadays in many applications where increasing use of sensitive personal information is observed. However, there has no optimal learning scheme developed in the literature to learn cost-sensitive classifiers under constraint of enforcing differential privacy. Our approach is to first develop a unified framework for existing cost-sensitive learning methods by incorporating the weight constant and weight functions into the classical regularized empirical risk minimization framework. Then, we propose two privacy-preserving algorithms with output perturbation and objective perturbation methods, respectively, to be integrated with the cost-sensitive learning framework. We showcase how this general framework can be used analytically by deriving the privacy-preserving cost-sensitive extensions of logistic regression and support vector machine. Experimental evidence on both synthetic and real data sets verifies that the proposed algorithms can reduce the misclassification cost effectively while satisfying the privacy requirement. A theoretical investigation is also conducted, revealing a very interesting analytic relation, i.e., that the choice of the weight constant and weight functions does not only influence the Fisher-consistent property (population minimizer of expected risk with a specific loss function leads to the Bayes optimal decision rule) but also interacts with privacy-preserving levels to affect the performance of classifiers significantly.},
  archive      = {J_TNNLS},
  author       = {Yi Yang and Shuai Huang and Wei Huang and Xiangyu Chang},
  doi          = {10.1109/TNNLS.2020.2996972},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {2105-2116},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Privacy-preserving cost-sensitive learning},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep multiscale detail networks for multiband spectral image
sharpening. <em>TNNLS</em>, <em>32</em>(5), 2090–2104. (<a
href="https://doi.org/10.1109/TNNLS.2020.2996498">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a new deep detail network architecture with grouped multiscale dilated convolutions to sharpen images contain multiband spectral information. Specifically, our end-to-end network directly fuses low-resolution multispectral and panchromatic inputs to produce high-resolution multispectral results, which is the same goal of the pansharpening in remote sensing. The proposed network architecture is designed by utilizing our domain knowledge and considering the two aims of the pansharpening: spectral and spatial preservations. For spectral preservation, the up-sampled multispectral images are directly added to the output for lossless spectral information propagation. For spatial preservation, we train the proposed network in the high-frequency domain instead of the commonly used image domain. Different from conventional network structures, we remove pooling and batch normalization layers to preserve spatial information and improve generalization to new satellites, respectively. To effectively and efficiently obtain multiscale contextual features at a fine-grained level, we propose a grouped multiscale dilated network structure to enlarge the receptive fields for each network layer. This structure allows the network to capture multiscale representations without increasing the parameter burden and network complexity. These representations are finally utilized to reconstruct the residual images which contain spatial details of PAN. Our trained network is able to generalize different satellite images without the need for parameter tuning. Moreover, our model is a general framework, which can be directly used for other kinds of multiband spectral image sharpening, e.g., hyperspectral image sharpening. Experiments show that our model performs favorably against compared methods in terms of both qualitative and quantitative qualities.},
  archive      = {J_TNNLS},
  author       = {Xueyang Fu and Wu Wang and Yue Huang and Xinghao Ding and John Paisley},
  doi          = {10.1109/TNNLS.2020.2996498},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {2090-2104},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Deep multiscale detail networks for multiband spectral image sharpening},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Rethinking RGB-d salient object detection: Models, data
sets, and large-scale benchmarks. <em>TNNLS</em>, <em>32</em>(5),
2075–2089. (<a
href="https://doi.org/10.1109/TNNLS.2020.2996406">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The use of RGB-D information for salient object detection (SOD) has been extensively explored in recent years. However, relatively few efforts have been put toward modeling SOD in real-world human activity scenes with RGB-D. In this article, we fill the gap by making the following contributions to RGB-D SOD: 1) we carefully collect a new S al i ent P erson (SIP) data set that consists of ~1 K high-resolution images that cover diverse real-world scenes from various viewpoints, poses, occlusions, illuminations, and background s; 2) we conduct a large-scale (and, so far, the most comprehensive) benchmark comparing contemporary methods, which has long been missing in the field and can serve as a baseline for future research, and we systematically summarize 32 popular models and evaluate 18 parts of 32 models on seven data sets containing a total of about 97k images; and 3) we propose a simple general architecture, called deep depth-depurator network (D 3 Net). It consists of a depth depurator unit (DDU) and a three-stream feature learning module (FLM), which performs low-quality depth map filtering and cross-modal feature learning, respectively. These components form a nested structure and are elaborately designed to be learned jointly. D 3 Net exceeds the performance of any prior contenders across all five metrics under consideration, thus serving as a strong model to advance research in this field. We also demonstrate that D 3 Net can be used to efficiently extract salient object masks from real scenes, enabling effective background-changing application with a speed of 65 frames/s on a single GPU. All the saliency maps, our new SIP data set, the D 3 Net model, and the evaluation tools are publicly available at https://github.com/DengPingFan/D3NetBenchmark .},
  archive      = {J_TNNLS},
  author       = {Deng-Ping Fan and Zheng Lin and Zhao Zhang and Menglong Zhu and Ming-Ming Cheng},
  doi          = {10.1109/TNNLS.2020.2996406},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {2075-2089},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Rethinking RGB-D salient object detection: Models, data sets, and large-scale benchmarks},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Using eye gaze to enhance generalization of imitation
networks to unseen environments. <em>TNNLS</em>, <em>32</em>(5),
2066–2074. (<a
href="https://doi.org/10.1109/TNNLS.2020.2996386">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vision-based autonomous driving through imitation learning mimics the behavior of human drivers by mapping driver view images to driving actions. This article shows that performance can be enhanced via the use of eye gaze. Previous research has shown that observing an expert&#39;s gaze patterns can be beneficial for novice human learners. We show here that neural networks can also benefit. We trained a conditional generative adversarial network to estimate human gaze maps accurately from driver-view images. We describe two approaches to integrating gaze information into imitation networks: eye gaze as an additional input and gaze modulated dropout. Both significantly enhance generalization to unseen environments in comparison with a baseline vanilla network without gaze, but gaze-modulated dropout performs better. We evaluated performance quantitatively on both single images and in closed-loop tests, showing that gaze modulated dropout yields the lowest prediction error, the highest success rate in overtaking cars, the longest distance between infractions, lowest epistemic uncertainty, and improved data efficiency. Using Grad-CAM, we show that gaze modulated dropout enables the network to concentrate on task-relevant areas of the image.},
  archive      = {J_TNNLS},
  author       = {Congcong Liu and Yuying Chen and Ming Liu and Bertram E. Shi},
  doi          = {10.1109/TNNLS.2020.2996386},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {2066-2074},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Using eye gaze to enhance generalization of imitation networks to unseen environments},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Reinforcement learning with task decomposition for
cooperative multiagent systems. <em>TNNLS</em>, <em>32</em>(5),
2054–2065. (<a
href="https://doi.org/10.1109/TNNLS.2020.2996209">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we study cooperative multiagent systems (MASs) with multiple tasks by using reinforcement learning (RL)-based algorithms. The target for a single-agent RL system is represented by its scalar reward signals. However, for an MAS with multiple cooperative tasks, the holistic reward signal consists of multiple parts to represent the tasks, which makes the problem complicated. Existing multiagent RL algorithms search distributed policies with holistic reward signals directly, making it difficult to obtain an optimal policy for each task. This article provides efficient learning-based algorithms such that each agent can learn a joint optimal policy to accomplish these multiple tasks cooperatively with other agents. The main idea of the algorithms is to decompose the holistic reward signal for each agent into multiple parts according to the subtasks, and then the proposed algorithms learn multiple value functions with the decomposed reward signals and update the policy with the sum of distributed value functions. In addition, this article presents a theoretical analysis of the proposed approach. Finally, the simulation results for both discrete decision-making and continuous control problems have demonstrated the effectiveness of the proposed algorithms.},
  archive      = {J_TNNLS},
  author       = {Changyin Sun and Wenzhang Liu and Lu Dong},
  doi          = {10.1109/TNNLS.2020.2996209},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {2054-2065},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Reinforcement learning with task decomposition for cooperative multiagent systems},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Synchronization control for discrete-time-delayed dynamical
networks with switching topology under actuator saturations.
<em>TNNLS</em>, <em>32</em>(5), 2040–2053. (<a
href="https://doi.org/10.1109/TNNLS.2020.2996094">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article is concerned with the synchronization control problem for a class of discrete-time dynamical networks with mixed delays and switching topology. The saturation phenomenon of physical actuators is specifically considered in designing feedback controllers. By exploring the mixed-delay-dependent sector conditions in combination with the piecewise Lyapunov-like functional and the average-dwell-time switching, a sufficient condition is first established under which all trajectories of the error dynamics are bounded for admissible initial conditions and nonzero external disturbances, while the l 2 - l ∞ performance constraint is satisfied. Furthermore, the exponential stability of the error dynamics is ensured for admissible initial conditions in the absence of disturbances. Second, by using some congruence transformations, the explicit condition guaranteeing the existence of desired controller gains is obtained in terms of the feasibility of a set of linear matrix inequalities. Then, three convex optimization problems are formulated regarding the disturbance tolerance, the l 2 - l ∞ performance, and the initial condition set, respectively. Finally, two simulation examples are given to show the effectiveness and merits of the proposed results.},
  archive      = {J_TNNLS},
  author       = {Yonggang Chen and Zidong Wang and Jun Hu and Qing-Long Han},
  doi          = {10.1109/TNNLS.2020.2996094},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {2040-2053},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Synchronization control for discrete-time-delayed dynamical networks with switching topology under actuator saturations},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Probabilistic knowledge transfer for lightweight deep
representation learning. <em>TNNLS</em>, <em>32</em>(5), 2030–2039. (<a
href="https://doi.org/10.1109/TNNLS.2020.2995884">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge-transfer (KT) methods allow for transferring the knowledge contained in a large deep learning model into a more lightweight and faster model. However, the vast majority of existing KT approaches are designed to handle mainly classification and detection tasks. This limits their performance on other tasks, such as representation/metric learning. To overcome this limitation, a novel probabilistic KT (PKT) method is proposed in this article. PKT is capable of transferring the knowledge into a smaller student model by keeping as much information as possible, as expressed through the teacher model. The ability of the proposed method to use different kernels for estimating the probability distribution of the teacher and student models, along with the different divergence metrics that can be used for transferring the knowledge, allows for easily adapting the proposed method to different applications. PKT outperforms several existing state-of-the-art KT techniques, while it is capable of providing new insights into KT by enabling several novel applications, as it is demonstrated through extensive experiments on several challenging data sets.},
  archive      = {J_TNNLS},
  author       = {Nikolaos Passalis and Maria Tzelepi and Anastasios Tefas},
  doi          = {10.1109/TNNLS.2020.2995884},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {2030-2039},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Probabilistic knowledge transfer for lightweight deep representation learning},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Combination of transferable classification with multisource
domain adaptation based on evidential reasoning. <em>TNNLS</em>,
<em>32</em>(5), 2015–2029. (<a
href="https://doi.org/10.1109/TNNLS.2020.2995862">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In applications of domain adaptation, there may exist multiple source domains, which can provide more or less complementary knowledge for pattern classification in the target domain. In order to improve the classification accuracy, a decision-level combination method is proposed for the multisource domain adaptation based on evidential reasoning. The classification results obtained from different source domains usually have different reliabilities/weights, which are calculated according to domain consistency. Therefore, the multiple classification results are discounted by the corresponding weights under belief functions framework, and then, Dempster&#39;s rule is employed to combine these discounted results. In order to reduce errors, a neighborhood-based cautious decision-making rule is developed to make the class decision depending on the combination result. The object is assigned to a singleton class if its neighborhoods can be (almost) correctly classified. Otherwise, it is cautiously committed to the disjunction of several possible classes. By doing this, we can well characterize the partial imprecision of classification and reduce the error risk as well. A unified utility value is defined here to reflect the benefit of such classification. This cautious decision-making rule can achieve the maximum unified utility value because partial imprecision is considered better than an error. Several real data sets are used to test the performance of the proposed method, and the experimental results show that our new method can efficiently improve the classification accuracy with respect to other related combination methods.},
  archive      = {J_TNNLS},
  author       = {Zhun-Ga Liu and Lin-Qing Huang and Kuang Zhou and Thierry Denœux},
  doi          = {10.1109/TNNLS.2020.2995862},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {2015-2029},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Combination of transferable classification with multisource domain adaptation based on evidential reasoning},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). L₂–l∞ state estimation for persistent dwell-time switched
coupled networks subject to round-robin protocol. <em>TNNLS</em>,
<em>32</em>(5), 2002–2014. (<a
href="https://doi.org/10.1109/TNNLS.2020.2995708">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article is concerned with the issue of l 2 - l ∞ state estimation for nonlinear coupled networks, where the variation of coupling mode is governed by a set of switching signals satisfying a persistent dwell-time property. To solve the problem of data collisions in a constrained communication network, the round-robin protocol, as an important scheduling strategy for orchestrating the transmission order of sensor nodes, is introduced. Redundant channels with signal quantization are used to improve the reliability of data transmission. The main purpose is to determine an estimator that can guarantee the exponential stability in mean square sense and an l 2 - l ∞ performance level of the estimation error system. Based on the Lyapunov method, sufficient conditions for the addressed problem are established. The desired estimator gains can be obtained by addressing a convex optimization case. The correctness and availability of the developed approach are finally explained via two illustrative examples.},
  archive      = {J_TNNLS},
  author       = {Hao Shen and Mengping Xing and Zhengguang Wu and Jinde Cao and Tingwen Huang},
  doi          = {10.1109/TNNLS.2020.2995708},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {2002-2014},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {L₂–l∞ state estimation for persistent dwell-time switched coupled networks subject to round-robin protocol},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning target-domain-specific classifier for partial
domain adaptation. <em>TNNLS</em>, <em>32</em>(5), 1989–2001. (<a
href="https://doi.org/10.1109/TNNLS.2020.2995648">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised domain adaptation (UDA) aims at reducing the distribution discrepancy when transferring knowledge from a labeled source domain to an unlabeled target domain. Previous UDA methods assume that the source and target domains share an identical label space, which is unrealistic in practice since the label information of the target domain is agnostic. This article focuses on a more realistic UDA scenario, i.e., partial domain adaptation (PDA), where the target label space is subsumed to the source label space. In the PDA scenario, the source outliers that are absent in the target domain may be wrongly matched to the target domain (technically named negative transfer), leading to performance degradation of UDA methods. This article proposes a novel target-domain-specific classifier learning-based domain adaptation (TSCDA) method. TSCDA presents a soft-weighed maximum mean discrepancy criterion to partially align feature distributions and alleviate negative transfer. Also, it learns a target-specific classifier for the target domain with pseudolabels and multiple auxiliary classifiers to further address the classifier shift. A module named peers-assisted learning is used to minimize the prediction difference between multiple target-specific classifiers, which makes the classifiers more discriminant for the target domain. Extensive experiments conducted on three PDA benchmark data sets show that TSCDA outperforms other state-of-the-art methods with a large margin, e.g., 4\% and 5.6\% averagely on Office-31 and Office-Home, respectively.},
  archive      = {J_TNNLS},
  author       = {Chuan-Xian Ren and Pengfei Ge and Peiyi Yang and Shuicheng Yan},
  doi          = {10.1109/TNNLS.2020.2995648},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {1989-2001},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Learning target-domain-specific classifier for partial domain adaptation},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Qualitative analysis and bifurcation in a neuron system with
memristor characteristics and time delay. <em>TNNLS</em>,
<em>32</em>(5), 1974–1988. (<a
href="https://doi.org/10.1109/TNNLS.2020.2995631">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article focuses on the hybrid effects of memristor characteristics, time delay, and biochemical parameters on neural networks. First, we propose a novel neuron system with memristor and time delays in which the memristor is characterized by a smooth continuous cubic function. Second, the existence of equilibria of this type of neuron system is examined in the parameter space. Sufficient conditions that ensure the stability of equilibria and occurrence of pitchfork bifurcation are given for the memristor-based neuron system without delay. Third, some novel criteria of the addressed neuron system are constructed for guaranteeing the delay-dependent and delay-independent stability. The specific conditions are provided for Hopf bifurcations, and the properties of Hopf bifurcation are ascertained using the center manifold reduction and the normal form theory. Moreover, there exists a phenomenon of bistability for the delayed memristor-based neuron system having three equilibria. Finally, the effectiveness of the theoretical results is demonstrated by numerical examples.},
  archive      = {J_TNNLS},
  author       = {Min Xiao and Wei Xing Zheng and Guoping Jiang and Jinde Cao},
  doi          = {10.1109/TNNLS.2020.2995631},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {1974-1988},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Qualitative analysis and bifurcation in a neuron system with memristor characteristics and time delay},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Data-driven terminal iterative learning consensus for
nonlinear multiagent systems with output saturation. <em>TNNLS</em>,
<em>32</em>(5), 1963–1973. (<a
href="https://doi.org/10.1109/TNNLS.2020.2995600">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article considers the problem of finite-time consensus for nonlinear multiagent systems (MASs), where the nonlinear dynamics are completely unknown and the output saturation exists. First, the mapping relationship between the output of each agent at the terminal time and the control input is established along the iteration domain. By using the terminal iterative learning control method, two novel distributed data-driven consensus protocols are proposed depending on the input and output saturated data of agents and its neighbors. Then, the convergence conditions independent of agents&#39; dynamics are developed for the MASs with fixed communication topology. It is shown that the proposed data-driven protocol can guarantee the system to achieve two different finite-time consensus objectives. Meanwhile, the design is also extended to the case of switching topologies. Finally, the effectiveness of the data-driven protocol is validated by a simulation example.},
  archive      = {J_TNNLS},
  author       = {Xuhui Bu and Jiaqi Liang and Zhongsheng Hou and Ronghu Chi},
  doi          = {10.1109/TNNLS.2020.2995600},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {1963-1973},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Data-driven terminal iterative learning consensus for nonlinear multiagent systems with output saturation},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Target controllability in multilayer networks via
minimum-cost maximum-flow method. <em>TNNLS</em>, <em>32</em>(5),
1949–1962. (<a
href="https://doi.org/10.1109/TNNLS.2020.2995596">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, to maximize the dimension of controllable subspace, we consider target controllability problem with maximum covered nodes set in multiplex networks. We call such an issue as maximum-cost target controllability problem. Likewise, minimum-cost target controllability problem is also introduced which is to find minimum covered node set and driver node set. To address these two issues, we first transform them into a minimum-cost maximum-flow problem based on graph theory. Then an algorithm named target minimum-cost maximum-flow (TMM) is proposed. It is shown that the proposed TMM ensures the target nodes in multiplex networks to be controlled with the minimum number of inputs as well as the maximum (minimum) number of covered nodes. Simulation results on Erdos-Rényi (ER-ER) networks, scale-free (SF-SF) networks, and real-life networks illustrate satisfactory performance of the TMM.},
  archive      = {J_TNNLS},
  author       = {Jie Ding and Changyun Wen and Guoqi Li and Pengfei Tu and Dongxu Ji and Ying Zou and Jiangshuai Huang},
  doi          = {10.1109/TNNLS.2020.2995596},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {1949-1962},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Target controllability in multilayer networks via minimum-cost maximum-flow method},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Network together: Node classification via cross-network deep
network embedding. <em>TNNLS</em>, <em>32</em>(5), 1935–1948. (<a
href="https://doi.org/10.1109/TNNLS.2020.2995483">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network embedding is a highly effective method to learn low-dimensional node vector representations with original network structures being well preserved. However, existing network embedding algorithms are mostly developed for a single network, which fails to learn generalized feature representations across different networks. In this article, we study a cross-network node classification problem, which aims at leveraging the abundant labeled information from a source network to help classify the unlabeled nodes in a target network. To succeed in such a task, transferable features should be learned for nodes across different networks. To this end, a novel cross-network deep network embedding (CDNE) model is proposed to incorporate domain adaptation into deep network embedding in order to learn label-discriminative and network-invariant node vector representations. On the one hand, CDNE leverages network structures to capture the proximities between nodes within a network, by mapping more strongly connected nodes to have more similar latent vector representations. On the other hand, node attributes and labels are leveraged to capture the proximities between nodes across different networks by making the same labeled nodes across networks have aligned latent vector representations. Extensive experiments have been conducted, demonstrating that the proposed CDNE model significantly outperforms the state-of-the-art network embedding algorithms in cross-network node classification.},
  archive      = {J_TNNLS},
  author       = {Xiao Shen and Quanyu Dai and Sitong Mao and Fu-Lai Chung and Kup-Sze Choi},
  doi          = {10.1109/TNNLS.2020.2995483},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {1935-1948},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Network together: Node classification via cross-network deep network embedding},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Online kernel learning with adaptive bandwidth by optimal
control approach. <em>TNNLS</em>, <em>32</em>(5), 1920–1934. (<a
href="https://doi.org/10.1109/TNNLS.2020.2995482">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online learning methods are designed to establish timely predictive models for machine learning problems. The methods for online learning of nonlinear systems are usually developed in the reproducing kernel Hilbert space (RKHS) associated with Gaussian kernel in which the kernel bandwidth is manually selected and remains steady during the entire modeling process in most cases. This setting may make the learning model rigid and inappropriate for complex data streams. Since the bandwidth appears in a nonlinear term of the kernel model, it raises substantial challenges in the development of learning methods with an adaptive bandwidth. In this article, we propose a novel approach to address this important open issue. By a carefully casted linearization scheme, the nonlinear learning problem is reasonably transformed into a state feedback control problem for a series of controllable systems. Then, by employing optimal control techniques, an effective algorithm is developed, and the parameters in the learning model including kernel bandwidth can be efficiently updated in a real-time manner. By taking advantage of the particular structure of the Gaussian kernel model, a theoretical analysis on the convergence and rationality of the proposed method is also provided. Compared with the kernel algorithms with a fixed bandwidth, our novel learning framework can not only achieve adaptive learning results with a better prediction accuracy but also show performance that is more robust with a faster convergence speed. Encouraging numerical results are provided to demonstrate the advantages of our new method.},
  archive      = {J_TNNLS},
  author       = {Jiaming Zhang and Hanwen Ning and Xingjian Jing and Tianhai Tian},
  doi          = {10.1109/TNNLS.2020.2995482},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {1920-1934},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Online kernel learning with adaptive bandwidth by optimal control approach},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Generalization bounds of multitask learning from perspective
of vector-valued function learning. <em>TNNLS</em>, <em>32</em>(5),
1906–1919. (<a
href="https://doi.org/10.1109/TNNLS.2020.2995428">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we study the generalization performance of multitask learning (MTL) by considering MTL as a learning process of vector-valued functions (VFs). We will answer two theoretical questions, given a small size training sample: 1) under what conditions does MTL perform better than single-task learning (STL)? And 2) under what conditions does MTL guarantee the consistency of all tasks during learning? In contrast to the conventional task-summation based MTL, the introduction of VF form enables us to detect the behavior of each task and the task-group relatedness in MTL. Specifically, the task-group relatedness examines how the success (or failure) of some tasks affects the performance of the other tasks. By deriving the specific deviation and symmetrization inequalities for VFs, we obtain a generalization bound for MTL to the upper bound of the joint probability that there is at least one task with a large generalization gap. To answer the first question, we discuss how the synergic relatedness between task groups affects the generalization performance of MTL and shows that MTL outperforms STL if almost any pair of complementary task groups is predominantly synergic. Moreover, to answer the second question, we present a sufficient condition to guarantee the consistency of each task in MTL, which requires that the function class of each task should not have high complexity. In addition, our findings provide a strategy to examine whether the task settings will enjoy the advantages of MTL.},
  archive      = {J_TNNLS},
  author       = {Chao Zhang and Dacheng Tao and Tao Hu and Bingchen Liu},
  doi          = {10.1109/TNNLS.2020.2995428},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {1906-1919},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Generalization bounds of multitask learning from perspective of vector-valued function learning},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Unified model solving nine types of time-varying problems in
the frame of zeroing neural network. <em>TNNLS</em>, <em>32</em>(5),
1896–1905. (<a
href="https://doi.org/10.1109/TNNLS.2020.2995396">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many time-varying problems have been solved using the zeroing neural network proposed by Zhang et al. In this article, nine types of time-varying problems, namely time-varying nonlinear equation system, time-varying linear equation system, time-varying convex nonlinear optimization under linear equalities, unconstrained time-varying convex nonlinear optimization, time-varying convex quadratic programming under linear equalities, unconstrained time-varying convex quadratic programming, time-varying nonlinear inequality system, time-varying linear inequality system, and time-varying division, are investigated to better understand the essence of zeroing neutral network. Discrete-form time-varying problems are studied by considering the nature of unknown future and the requirement of real-time computation for time-varying problems. A unified model is proposed in the frame of zeroing neural network to uniformly solve these time-varying problems on the basis of their connections and a newly developed discretization formula. Theoretical analyses and numerical experiments, including the tracking control of PUMA560 robot manipulator, verify the effectiveness and precision of the proposed unified model.},
  archive      = {J_TNNLS},
  author       = {Jian Li and Yang Shi and Hejun Xuan},
  doi          = {10.1109/TNNLS.2020.2995396},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {1896-1905},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Unified model solving nine types of time-varying problems in the frame of zeroing neural network},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SRN: Side-output residual network for object reflection
symmetry detection and beyond. <em>TNNLS</em>, <em>32</em>(5),
1881–1895. (<a
href="https://doi.org/10.1109/TNNLS.2020.2994325">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article establishes a baseline for object reflection symmetry detection in natural images by releasing a new benchmark named Sym-PASCAL and proposing an end-to-end deep learning approach for reflection symmetry. Sym-PASCAL spans challenges of multiobjects, object diversity, part invisibility, and clustered backgrounds, which is far beyond those in existing data sets. The end-to-end deep learning approach, referred to as a side-output residual network (SRN), leverages the output residual units (RUs) to fit the errors between the symmetry ground truth and the side outputs of multiple stages of a trunk network. By cascading RUs from deep to shallow, SRN exploits the “flow” of errors along multiple stages to effectively matching object symmetry at different scales and suppress the clustered backgrounds. SRN is interpreted as a boosting-like algorithm, which assembles features using RUs during network forward and backward propagations. SRN is further upgraded to a multitask SRN (MT-SRN) for joint symmetry and edge detection, demonstrating its generality to image-to-mask learning tasks. Experimental results verify that the Sym-PASCAL benchmark is challenging related to real-world images, SRN achieves state-of-the-art performance, and MT-SRN has the capability to simultaneously predict edge and symmetry mask without loss of performance.},
  archive      = {J_TNNLS},
  author       = {Wei Ke and Jie Chen and Jianbin Jiao and Guoying Zhao and Qixiang Ye},
  doi          = {10.1109/TNNLS.2020.2994325},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {1881-1895},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {SRN: Side-output residual network for object reflection symmetry detection and beyond},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Nontechnical losses detection through coordinated BiWGAN and
SVDD. <em>TNNLS</em>, <em>32</em>(5), 1866–1880. (<a
href="https://doi.org/10.1109/TNNLS.2020.2994116">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nontechnical losses (NTLs) are estimated to be considerable and increasing every year. Recently, high-resolution measurements from globally laid smart meters have brought deeper insights on users’ consumption patterns that can be exploited potentially by NTL detection. However, consumption-pattern-based NTL detection is now facing two major challenges: the inefficiency of harnessing high dimensionality and the severe lack of fraudulent samples. To overcome them, an NTL detection model based on deep learning and anomaly detection is proposed in this article, namely bidirectional Wasserstein GAN and support vector data description-based NTL detector (BSBND). Motivated by the powerful ability of generative adversarial networks (GANs) to learn deep representation from high-dimensional distributions of data, in the BSBND, we utilized a BiWGAN for feature extraction from high-dimensional raw consumption records, and a one-class classifier trained only on benign samples—SVDD—is adopted to map features into judgments. Moreover, a novel alternate coordinating algorithm is proposed to optimize the cooperation between the upstream BiWGAN and the downstream SVDD, and also, an interpreting algorithm is proposed to visualize the basis of each fraudulent judgment. Case studies have demonstrated the superiority of the BSBND over the state of the arts, the powerful feature extraction ability of BiWGAN, and also the effectiveness of the proposed coordinating and interpreting algorithms.},
  archive      = {J_TNNLS},
  author       = {Tianyu Hu and Qinglai Guo and Hongbin Sun and Tian-En Huang and Jian Lan},
  doi          = {10.1109/TNNLS.2020.2994116},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {1866-1880},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Nontechnical losses detection through coordinated BiWGAN and SVDD},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Watermarking deep neural networks in image processing.
<em>TNNLS</em>, <em>32</em>(5), 1852–1865. (<a
href="https://doi.org/10.1109/TNNLS.2020.2991378">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Publishing/sharing pretrained deep neural network (DNN) models is a common practice in the community of computer vision. The increasing popularity of pretrained models has made it a serious concern: how to protect the intellectual properties of model owners and avert illegal usages by malicious attackers. This article aims at developing a framework for watermarking DNNs, with a particular focus on low-level image processing tasks that map images to images. Using image denoising and superresolution as case studies, we develop a black-box watermarking method for pretrained models, which exploits the overparameterization of the DNNs in image processing. In addition, an auxiliary module for visualizing the watermark information is proposed for further verification. Extensive experiments show that the proposed watermarking framework has no noticeable impact on model performance and enjoys the robustness against the often-seen attacks.},
  archive      = {J_TNNLS},
  author       = {Yuhui Quan and Huan Teng and Yixin Chen and Hui Ji},
  doi          = {10.1109/TNNLS.2020.2991378},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {1852-1865},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Watermarking deep neural networks in image processing},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Simultaneous global and local graph structure preserving for
multiple kernel clustering. <em>TNNLS</em>, <em>32</em>(5), 1839–1851.
(<a href="https://doi.org/10.1109/TNNLS.2020.2991366">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple kernel learning (MKL) is generally recognized to perform better than single kernel learning (SKL) in handling nonlinear clustering problem, largely thanks to MKL avoids selecting and tuning predefined kernel. By integrating the self-expression learning framework, the graph-based MKL subspace clustering has recently attracted considerable attention. However, the graph structure of data in kernel space is largely ignored by previous MKL methods, which is a key concept of affinity graph construction for spectral clustering purposes. In order to address this problem, a novel MKL method is proposed in this article, namely, structure-preserving multiple kernel clustering (SPMKC). Specifically, SPMKC proposes a new kernel affine weight strategy to learn an optimal consensus kernel from a predefined kernel pool, which can assign a suitable weight for each base kernel automatically. Furthermore, SPMKC proposes a kernel group self-expressiveness term and a kernel adaptive local structure learning term to preserve the global and local structure of the input data in kernel space, respectively, rather than the original space. In addition, an efficient algorithm is proposed to solve the resulting unified objective function, which iteratively updates the consensus kernel and the affinity graph so that collaboratively promoting each of them to reach the optimum condition. Experiments on both image and text clustering demonstrate that SPMKC outperforms the state-of-the-art MKL clustering methods in terms of clustering performance and computational cost.},
  archive      = {J_TNNLS},
  author       = {Zhenwen Ren and Quansen Sun},
  doi          = {10.1109/TNNLS.2020.2991366},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {1839-1851},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Simultaneous global and local graph structure preserving for multiple kernel clustering},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Supervised feature selection with orthogonal regression and
feature weighting. <em>TNNLS</em>, <em>32</em>(5), 1831–1838. (<a
href="https://doi.org/10.1109/TNNLS.2020.2991336">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Effective features can improve the performance of a model and help us understand the characteristics and underlying structure of complex data. Previously proposed feature selection methods usually cannot retain more discriminative information. To address this shortcoming, we propose a novel supervised orthogonal least square regression model with feature weighting for feature selection. The optimization problem of the objective function can be solved by employing generalized power iteration and augmented Lagrangian multiplier methods. Experimental results show that the proposed method can more effectively reduce feature dimensionality and obtain better classification results than traditional feature selection methods. The convergence of our iterative method is also proved. Consequently, the effectiveness and superiority of the proposed method are verified both theoretically and experimentally.},
  archive      = {J_TNNLS},
  author       = {Xia Wu and Xueyuan Xu and Jianhong Liu and Hailing Wang and Bin Hu and Feiping Nie},
  doi          = {10.1109/TNNLS.2020.2991336},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {1831-1838},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Supervised feature selection with orthogonal regression and feature weighting},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Reachable set estimation for neural network control systems:
A simulation-guided approach. <em>TNNLS</em>, <em>32</em>(5), 1821–1830.
(<a href="https://doi.org/10.1109/TNNLS.2020.2991090">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The vulnerability of artificial intelligence (AI) and machine learning (ML) against adversarial disturbances and attacks significantly restricts their applicability in safety-critical systems including cyber-physical systems (CPS) equipped with neural network components at various stages of sensing and control. This article addresses the reachable set estimation and safety verification problems for dynamical systems embedded with neural network components serving as feedback controllers. The closed-loop system can be abstracted in the form of a continuous-time sampled-data system under the control of a neural network controller. First, a novel reachable set computation method in adaptation to simulations generated out of neural networks is developed. The reachability analysis of a class of feedforward neural networks called multilayer perceptrons (MLPs) with general activation functions is performed in the framework of interval arithmetic. Then, in combination with reachability methods developed for various dynamical system classes modeled by ordinary differential equations, a recursive algorithm is developed for over-approximating the reachable set of the closed-loop system. The safety verification for neural network control systems can be performed by examining the emptiness of the intersection between the over-approximation of reachable sets and unsafe sets. The effectiveness of the proposed approach has been validated with evaluations on a robotic arm model and an adaptive cruise control system.},
  archive      = {J_TNNLS},
  author       = {Weiming Xiang and Hoang-Dung Tran and Xiaodong Yang and Taylor T. Johnson},
  doi          = {10.1109/TNNLS.2020.2991090},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {1821-1830},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Reachable set estimation for neural network control systems: A simulation-guided approach},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Convolutional sparse support estimator-based COVID-19
recognition from x-ray images. <em>TNNLS</em>, <em>32</em>(5),
1810–1820. (<a
href="https://doi.org/10.1109/TNNLS.2021.3070467">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Coronavirus disease (COVID-19) has been the main agenda of the whole world ever since it came into sight. X-ray imaging is a common and easily accessible tool that has great potential for COVID-19 diagnosis and prognosis. Deep learning techniques can generally provide state-of-the-art performance in many classification tasks when trained properly over large data sets. However, data scarcity can be a crucial obstacle when using them for COVID-19 detection. Alternative approaches such as representation-based classification [collaborative or sparse representation (SR)] might provide satisfactory performance with limited size data sets, but they generally fall short in performance or speed compared to the neural network (NN)-based methods. To address this deficiency, convolution support estimation network (CSEN) has recently been proposed as a bridge between representation-based and NN approaches by providing a noniterative real-time mapping from query sample to ideally SR coefficient support, which is critical information for class decision in representation-based techniques. The main premises of this study can be summarized as follows: 1) A benchmark X-ray data set, namely QaTa-Cov19, containing over 6200 X-ray images is created. The data set covering 462 X-ray images from COVID-19 patients along with three other classes; bacterial pneumonia, viral pneumonia, and normal. 2) The proposed CSEN-based classification scheme equipped with feature extraction from state-of-the-art deep NN solution for X-ray images, CheXNet, achieves over 98\% sensitivity and over 95\% specificity for COVID-19 recognition directly from raw X-ray images when the average performance of 5-fold cross validation over QaTa-Cov19 data set is calculated. 3) Having such an elegant COVID-19 assistive diagnosis performance, this study further provides evidence that COVID-19 induces a unique pattern in X-rays that can be discriminated with high accuracy.},
  archive      = {J_TNNLS},
  author       = {Mehmet Yamaç and Mete Ahishali and Aysen Degerli and Serkan Kiranyaz and Muhammad E. H. Chowdhury and Moncef Gabbouj},
  doi          = {10.1109/TNNLS.2021.3070467},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {1810-1820},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Convolutional sparse support estimator-based COVID-19 recognition from X-ray images},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021i). IEEE computational intelligence society information.
<em>TNNLS</em>, <em>32</em>(4), C3. (<a
href="https://doi.org/10.1109/TNNLS.2021.3065516">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Provides a listing of current committee members and society officers.},
  archive      = {J_TNNLS},
  doi          = {10.1109/TNNLS.2021.3065516},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {C3},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {IEEE computational intelligence society information},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Mimic and fool: A task-agnostic adversarial attack.
<em>TNNLS</em>, <em>32</em>(4), 1801–1808. (<a
href="https://doi.org/10.1109/TNNLS.2020.2984972">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {At present, adversarial attacks are designed in a task-specific fashion. However, for downstream computer vision tasks such as image captioning and image segmentation, the current deep-learning systems use an image classifier such as VGG16, ResNet50, and Inception-v3 as a feature extractor. Keeping this in mind, we propose Mimic and Fool (MaF), a task-agnostic adversarial attack. Given a feature extractor, the proposed attack finds an adversarial image, which can mimic the image feature of the original image. This ensures that the two images give the same (or similar) output regardless of the task. We randomly select 1000 MSCOCO validation images for experimentation. We perform experiments on two image captioning models, Show and Tell, Show Attend and Tell, and one visual question answering (VQA) model, namely, end-to-end neural module network (N2NMN). The proposed attack achieves a success rate of 74.0\%, 81.0\%, and 87.1\% for Show and Tell, Show Attend and Tell, and N2NMN, respectively. We also propose a slight modification to our attack to generate natural-looking adversarial images. In addition, we also show the applicability of the proposed attack for invertible architecture. Since MaF only requires information about the feature extractor of the model, it can be considered as a gray-box attack.},
  archive      = {J_TNNLS},
  author       = {Akshay Chaturvedi and Utpal Garain},
  doi          = {10.1109/TNNLS.2020.2984972},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1801-1808},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Mimic and fool: A task-agnostic adversarial attack},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Attention-based road registration for GPS-denied UAS
navigation. <em>TNNLS</em>, <em>32</em>(4), 1788–1800. (<a
href="https://doi.org/10.1109/TNNLS.2020.3015660">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Matching and registration between aerial images and prestored road landmarks are critical techniques to enhance unmanned aerial system (UAS) navigation in the global positioning system (GPS)-denied urban environments. Current registration processes typically consist of two separate stages of road extraction and road registration. These two-stage registration approaches are time-consuming and less robust to noise. To that end, in this article, we, for the first time, investigate the problem of end-to-end Aerial-Road registration. Using deep learning, we develop a novel attention-based neural network architecture for Aerial-Road registration. In this model, we construct two-branch neural networks with shared weights to map two input images into a common embedding space. Besides, considering that road features are sparsely distributed in images, we incorporate a novel multibranch attention module to filter out false descriptor matches from the indiscriminative background in order to improve registration accuracy. Finally, the results from extensive experiments show that compared with state-of-the-art approaches, the mean absolute errors of our approach in rotation angle and the translations in the $x$ - and $y$ -directions are reduced down by a factor of 1.24, 1.38, and 1.44, respectively. Furthermore, as a byproduct, our experimental results prove the feasibility of a neural network multitask learning approach to simultaneously achieve accurate Aerial-Road matching and registration, thus providing an efficient and accurate UAS geolocalization.},
  archive      = {J_TNNLS},
  author       = {Teng Wang and Ye Zhao and Jiawei Wang and Arun K. Somani and Changyin Sun},
  doi          = {10.1109/TNNLS.2020.3015660},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1788-1800},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Attention-based road registration for GPS-denied UAS navigation},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A novel supertwisting zeroing neural network with
application to mobile robot manipulators. <em>TNNLS</em>,
<em>32</em>(4), 1776–1787. (<a
href="https://doi.org/10.1109/TNNLS.2020.2991088">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Various zeroing neural network (ZNN) models have been investigated to address the tracking control of robot manipulators for the capacity of parallel processing and nonlinearity handling. However, two limitations occur in the existing ZNN models. The first one is the convergence time that tends to be infinitely large. The second one is the research of robustness that remains in the analyses of stability and asymptotic convergence. To simultaneously enhance the convergence performance and robustness, this article proposes a new ZNN model by using a supertwisting (ST) algorithm, termed STZNN model, for the tracking control of mobile robot manipulators. The proposed STZNN model inherently possesses the advantages of finite-time convergence and robustness making the control process fast and robust. The bridge from the sliding mode control to the ZNN is built, and the essential connection between the ST algorithm and ZNN is explored by constructing a unified design process. Theorems and proofs about global stability, finite-time convergence, and robustness are provided. Finally, path-tracking applications, comparisons, and tests substantiate the effectiveness and superiority of the STZNN model for the tracking control handling of mobile robot manipulators.},
  archive      = {J_TNNLS},
  author       = {Dechao Chen and Shuai Li and Qing Wu},
  doi          = {10.1109/TNNLS.2020.2991088},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1776-1787},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A novel supertwisting zeroing neural network with application to mobile robot manipulators},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep attentive video summarization with distribution
consistency learning. <em>TNNLS</em>, <em>32</em>(4), 1765–1775. (<a
href="https://doi.org/10.1109/TNNLS.2020.2991083">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article studies supervised video summarization by formulating it into a sequence-to-sequence learning framework, in which the input and output are sequences of original video frames and their predicted importance scores, respectively. Two critical issues are addressed in this article: short-term contextual attention insufficiency and distribution inconsistency. The former lies in the insufficiency of capturing the short-term contextual attention information within the video sequence itself since the existing approaches focus a lot on the long-term encoder-decoder attention. The latter refers to the distributions of predicted importance score sequence and the ground-truth sequence is inconsistent, which may lead to a suboptimal solution. To better mitigate the first issue, we incorporate a self-attention mechanism in the encoder to highlight the important keyframes in a short-term context. The proposed approach alongside the encoder-decoder attention constitutes our deep attentive models for video summarization. For the second one, we propose a distribution consistency learning method by employing a simple yet effective regularization loss term, which seeks a consistent distribution for the two sequences. Our final approach is dubbed as Attentive and Distribution consistent video Summarization (ADSum). Extensive experiments on benchmark data sets demonstrate the superiority of the proposed ADSum approach against state-of-the-art approaches.},
  archive      = {J_TNNLS},
  author       = {Zhong Ji and Yuxiao Zhao and Yanwei Pang and Xi Li and Jungong Han},
  doi          = {10.1109/TNNLS.2020.2991083},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1765-1775},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Deep attentive video summarization with distribution consistency learning},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Neural-network-based adaptive event-triggered consensus
control of nonstrict-feedback nonlinear systems. <em>TNNLS</em>,
<em>32</em>(4), 1750–1764. (<a
href="https://doi.org/10.1109/TNNLS.2020.2991015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The event-triggered consensus control problem is studied for nonstrict-feedback nonlinear systems with a dynamic leader. Neural networks (NNs) are utilized to approximate the unknown dynamics of each follower and its neighbors. A novel adaptive event-trigger condition is constructed, which depends on the relative output measurement, the NN weights estimations, and the states of each follower. Based on the designed event-trigger condition, an adaptive NN controller is developed by using the backstepping control design technique. In the control design process, the algebraic loop problem is overcome by utilizing the property of NN basis functions and by designing novel adaptive parameter laws of the NN weights. The proposed adaptive NN event-triggered controller does not need continuous communication among neighboring agents, and it can substantially reduce the data communication and the frequency of the controller updates. It is proven that ultimately bounded leader-following consensus is achieved without exhibiting the Zeno behavior. The effectiveness of the theoretical results is verified through simulation studies.},
  archive      = {J_TNNLS},
  author       = {Wei Wang and Yongming Li and Shaocheng Tong},
  doi          = {10.1109/TNNLS.2020.2991015},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1750-1764},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Neural-network-based adaptive event-triggered consensus control of nonstrict-feedback nonlinear systems},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Convergence analysis of single latent factor-dependent,
nonnegative, and multiplicative update-based nonnegative latent factor
models. <em>TNNLS</em>, <em>32</em>(4), 1737–1749. (<a
href="https://doi.org/10.1109/TNNLS.2020.2990990">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A single latent factor (LF)-dependent, nonnegative, and multiplicative update (SLF-NMU) learning algorithm is highly efficient in building a nonnegative LF (NLF) model defined on a high-dimensional and sparse (HiDS) matrix. However, convergence characteristics of such NLF models are never justified in theory. To address this issue, this study conducts rigorous convergence analysis for an SLF-NMU-based NLF model. The main idea is twofold: 1) proving that its learning objective keeps nonincreasing with its SLF-NMU-based learning rules via constructing specific auxiliary functions; and 2) proving that it converges to a stable equilibrium point with its SLF-NMU-based learning rules via analyzing the Karush-Kuhn-Tucker (KKT) conditions of its learning objective. Experimental results on ten HiDS matrices from real applications provide numerical evidence that indicates the correctness of the achieved proof.},
  archive      = {J_TNNLS},
  author       = {Zhigang Liu and Xin Luo and Zidong Wang},
  doi          = {10.1109/TNNLS.2020.2990990},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1737-1749},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Convergence analysis of single latent factor-dependent, nonnegative, and multiplicative update-based nonnegative latent factor models},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Local–global memory neural network for medication
prediction. <em>TNNLS</em>, <em>32</em>(4), 1723–1736. (<a
href="https://doi.org/10.1109/TNNLS.2020.2989364">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electronic medical records (EMRs) play an important role in medical data mining and sequential data learning. In this article, we propose to use a sequential neural network with dynamic content-based memories to predict future medications, given EMRs. The local–global memory neural network contains two layers of memories: the local memory and the global memory. Particularly, our method learns the hidden knowledge within EMRs by locally remembering individual patterns of a patient (via local memory) and globally remembering group evidence of disease (via global memory). In addition, we show how our model can be modified to classify the hidden states of EMRs from different patients at each time step into different phases that indicate the progressions of medications in terms of a specific disease, in an unsupervised manner. Experimental results on real EMRs data sets show that, by learning EMRs with external local and global memories, with regard to a given disease, our model improves the prediction performance compared with several alternative methods.},
  archive      = {J_TNNLS},
  author       = {Jun Song and Yueyang Wang and Siliang Tang and Yin Zhang and Zhigang Chen and Zhongfei Zhang and Tong Zhang and Fei Wu},
  doi          = {10.1109/TNNLS.2020.2989364},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1723-1736},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Local–Global memory neural network for medication prediction},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep subdomain adaptation network for image classification.
<em>TNNLS</em>, <em>32</em>(4), 1713–1722. (<a
href="https://doi.org/10.1109/TNNLS.2020.2988928">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For a target task where the labeled data are unavailable, domain adaptation can transfer a learner from a different source domain. Previous deep domain adaptation methods mainly learn a global domain shift, i.e., align the global source and target distributions without considering the relationships between two subdomains within the same category of different domains, leading to unsatisfying transfer learning performance without capturing the fine-grained information. Recently, more and more researchers pay attention to subdomain adaptation that focuses on accurately aligning the distributions of the relevant subdomains. However, most of them are adversarial methods that contain several loss functions and converge slowly. Based on this, we present a deep subdomain adaptation network (DSAN) that learns a transfer network by aligning the relevant subdomain distributions of domain-specific layer activations across different domains based on a local maximum mean discrepancy (LMMD). Our DSAN is very simple but effective, which does not need adversarial training and converges fast. The adaptation can be achieved easily with most feedforward network models by extending them with LMMD loss, which can be trained efficiently via backpropagation. Experiments demonstrate that DSAN can achieve remarkable results on both object recognition tasks and digit classification tasks. Our code will be available at https://github.com/easezyc/deep-transfer-learning.},
  archive      = {J_TNNLS},
  author       = {Yongchun Zhu and Fuzhen Zhuang and Jindong Wang and Guolin Ke and Jingwu Chen and Jiang Bian and Hui Xiong and Qing He},
  doi          = {10.1109/TNNLS.2020.2988928},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1713-1722},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Deep subdomain adaptation network for image classification},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Efficient computation reduction in bayesian neural networks
through feature decomposition and memorization. <em>TNNLS</em>,
<em>32</em>(4), 1703–1712. (<a
href="https://doi.org/10.1109/TNNLS.2020.2987760">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Bayesian method is capable of capturing real-world uncertainties/incompleteness and properly addressing the overfitting issue faced by deep neural networks. In recent years, Bayesian neural networks (BNNs) have drawn tremendous attention to artificial intelligence (AI) researchers and proved to be successful in many applications. However, the required high computation complexity makes BNNs difficult to be deployed in computing systems with a limited power budget. In this article, an efficient BNN inference flow is proposed to reduce the computation cost and then is evaluated using both software and hardware implementations. A feature decomposition and memorization ( DM ) strategy is utilized to reform the BNN inference flow in a reduced manner. About half of the computations could be eliminated compared with the traditional approach that has been proved by theoretical analysis and software validations. Subsequently, in order to resolve the hardware resource limitations, a memory-friendly computing framework is further deployed to reduce the memory overhead introduced by the DM strategy. Finally, we implement our approach in Verilog and synthesize it with a 45-nm FreePDK technology. Hardware simulation results on multilayer BNNs demonstrate that, when compared with the traditional BNN inference method, it provides an energy consumption reduction of 73\% and a $4\times $ speedup at the expense of 14\% area overhead.},
  archive      = {J_TNNLS},
  author       = {Xiaotao Jia and Jianlei Yang and Runze Liu and Xueyan Wang and Sorin Dan Cotofana and Weisheng Zhao},
  doi          = {10.1109/TNNLS.2020.2987760},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1703-1712},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Efficient computation reduction in bayesian neural networks through feature decomposition and memorization},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). End-to-end video saliency detection via a deep contextual
spatiotemporal network. <em>TNNLS</em>, <em>32</em>(4), 1691–1702. (<a
href="https://doi.org/10.1109/TNNLS.2020.2986823">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an interesting and important problem in computer vision, learning-based video saliency detection aims to discover the visually interesting regions in a video sequence. Capturing the information within frame and between frame at different aspects (such as spatial contexts, motion information, temporal consistency across frames, and multiscale representation) is important for this task. A key issue is how to jointly model all these factors within a unified data-driven scheme in an end-to-end fashion. In this article, we propose an end-to-end spatiotemporal deep video saliency detection approach, which captures the information on spatial contexts and motion characteristics. Furthermore, it encodes the temporal consistency information across the consecutive frames by implementing a convolutional long short-term memory (Conv-LSTM) model. In addition, the multiscale saliency properties for each frame are adaptively integrated for final saliency prediction in a collaborative feature-pyramid way. Finally, the proposed deep learning approach unifies all the aforementioned parts into an end-to-end joint deep learning scheme. Experimental results demonstrate the effectiveness of our approach in comparison with the state-of-the-art approaches.},
  archive      = {J_TNNLS},
  author       = {Lina Wei and Shanshan Zhao and Omar Farouk Bourahla and Xi Li and Fei Wu and Yueting Zhuang and Junwei Han and Mingliang Xu},
  doi          = {10.1109/TNNLS.2020.2986823},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1691-1702},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {End-to-end video saliency detection via a deep contextual spatiotemporal network},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Reduced-order observer-based dynamic event-triggered
adaptive NN control for stochastic nonlinear systems subject to unknown
input saturation. <em>TNNLS</em>, <em>32</em>(4), 1678–1690. (<a
href="https://doi.org/10.1109/TNNLS.2020.2986281">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, a dynamic event-triggered control scheme for a class of stochastic nonlinear systems with unknown input saturation and partially unmeasured states is presented. First, a dynamic event-triggered mechanism (DEM) is designed to reduce some unnecessary transmissions from controller to actuator so as to achieve better resource efficiency. Unlike most existing event-triggered mechanisms, in which the threshold parameters are always fixed, the threshold parameter in the developed event-triggered condition is dynamically adjusted according to a dynamic rule. Second, an improved neural network that considers the reconstructed error is introduced to approximate the unknown nonlinear terms existed in the considered systems. Third, an auxiliary system with the same order as the considered system is constructed to deal with the influence of asymmetric input saturation, which is distinct from most existing methods for nonlinear systems with input saturation. Assuming that the partial state is unavailable in the system, a reduced-order observer is presented to estimate them. Furthermore, it is theoretically proven that the obtained control scheme can achieve the desired objects. Finally, a one-link manipulator system and a three-degree-of-freedom ship maneuvering system are presented to illustrate the effectiveness of the proposed control method.},
  archive      = {J_TNNLS},
  author       = {Lijie Wang and C. L. Philip Chen},
  doi          = {10.1109/TNNLS.2020.2986281},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1678-1690},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Reduced-order observer-based dynamic event-triggered adaptive NN control for stochastic nonlinear systems subject to unknown input saturation},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Design and analysis of two prescribed-time and robust ZNN
models with application to time-variant stein matrix equation.
<em>TNNLS</em>, <em>32</em>(4), 1668–1677. (<a
href="https://doi.org/10.1109/TNNLS.2020.2986275">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The zeroing neural network (ZNN) activated by nonlinear activation functions plays an important role in many fields. However, conventional ZNN can only realize finite-time convergence, which greatly limits the application of ZNN in a noisy environment. Generally, finite-time convergence depends on the original state of ZNN, but the original state is often unknown in advance. In addition, when meeting with different noises, the applied nonlinear activation functions cannot tolerate external disturbances. In this article, on the strength of this idea, two prescribed-time and robust ZNN (PTR-ZNN) models activated by two nonlinear activation functions are put forward to address the time-variant Stein matrix equation. The proposed two PTR-ZNN models own two remarkable advantages simultaneously: 1) prescribed-time convergence that does not rely on original states and 2) superior noise-tolerance performance that can tolerate time-variant bounded vanishing and nonvanishing noises. Furthermore, the detailed theoretical analysis is provided to guarantee the prescribed-time convergence and noise-tolerance performance, with the convergence upper bounds of steady-state residual errors calculated. Finally, simulative comparison results indicate the effectiveness and the superiority of the proposed two PTR-ZNN models for the time-variant Stein matrix equation solving.},
  archive      = {J_TNNLS},
  author       = {Jianhua Dai and Lei Jia and Lin Xiao},
  doi          = {10.1109/TNNLS.2020.2986275},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1668-1677},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Design and analysis of two prescribed-time and robust ZNN models with application to time-variant stein matrix equation},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Radial graph convolutional network for visual question
generation. <em>TNNLS</em>, <em>32</em>(4), 1654–1667. (<a
href="https://doi.org/10.1109/TNNLS.2020.2986029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we address the problem of visual question generation (VQG), a challenge in which a computer is required to generate meaningful questions about an image targeting a given answer. The existing approaches typically treat the VQG task as a reversed visual question answer (VQA) task, requiring the exhaustive match among all the image regions and the given answer. To reduce the complexity, we propose an innovative answer-centric approach termed radial graph convolutional network (Radial-GCN) to focus on the relevant image regions only. Our Radial-GCN method can quickly find the core answer area in an image by matching the latent answer with the semantic labels learned from all image regions. Then, a novel sparse graph of the radial structure is naturally built to capture the associations between the core node (i.e., answer area) and peripheral nodes (i.e., other areas); the graphic attention is subsequently adopted to steer the convolutional propagation toward potentially more relevant nodes for final question generation. Extensive experiments on three benchmark data sets show the superiority of our approach compared with the reference methods. Even in the unexplored challenging zero-shot VQA task, the synthesized questions by our method remarkably boost the performance of several state-of-the-art VQA methods from 0\% to over 40\%. The implementation code of our proposed method and the successfully generated questions are available at https://github.com/Wangt-CN/VQG-GCN.},
  archive      = {J_TNNLS},
  author       = {Xing Xu and Tan Wang and Yang Yang and Alan Hanjalic and Heng Tao Shen},
  doi          = {10.1109/TNNLS.2020.2986029},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1654-1667},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Radial graph convolutional network for visual question generation},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bipartite synchronization of multiple memristor-based neural
networks with antagonistic interactions. <em>TNNLS</em>, <em>32</em>(4),
1642–1653. (<a
href="https://doi.org/10.1109/TNNLS.2020.2985860">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, by introducing a signed graph to describe the coopetition interactions among network nodes, the mathematical model of multiple memristor-based neural networks (MMNNs) with antagonistic interactions is established. Since the cooperative and competitive interactions coexist, the states of MMNNs cannot reach complete synchronization. Instead, they will reach the bipartite synchronization: all nodes’ states will reach an identical absolute value but opposite sign. To reach bipartite synchronization, two kinds of the novel node- and edge-based adaptive strategies are proposed, respectively. First, based on the global information of the network nodes, a node-based adaptive control strategy is constructed to solve the bipartite synchronization problem of MMNNs. Secondly, a local edge-based adaptive algorithm is proposed, where the weight values of edges between two nodes will change according to the designed adaptive law. Finally, two simulation examples validate the effectiveness of the proposed adaptive controllers and bipartite synchronization criteria.},
  archive      = {J_TNNLS},
  author       = {Ning Li and Wei Xing Zheng},
  doi          = {10.1109/TNNLS.2020.2985860},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1642-1653},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Bipartite synchronization of multiple memristor-based neural networks with antagonistic interactions},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Generalized unitarily invariant gauge regularization for
fast low-rank matrix recovery. <em>TNNLS</em>, <em>32</em>(4),
1627–1641. (<a
href="https://doi.org/10.1109/TNNLS.2020.2985850">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spectral regularization is a widely used approach for low-rank matrix recovery (LRMR) by regularizing matrix singular values. Most of the existing LRMR solvers iteratively compute the singular values via applying singular value decomposition (SVD) on a dense matrix, which is computationally expensive and severely limits their applications to large-scale problems. To address this issue, we present a generalized unitarily invariant gauge (GUIG) function for LRMR. The proposed GUIG function does not act on the singular values; however, we show that it generalizes the well-known spectral functions, including the rank function, the Schatten- p quasi-norm, and logsum of singular values. The proposed GUIG regularization model can be formulated as a bilinear variational problem, which can be efficiently solved without computing SVD. Such a property makes it well suited for large-scale LRMR problems. We apply the proposed GUIG model to matrix completion and robust principal component analysis and prove the convergence of the algorithms. Experimental results demonstrate that the proposed GUIG method is not only more accurate but also much faster than the state-of-the-art algorithms, especially on large-scale problems.},
  archive      = {J_TNNLS},
  author       = {Xixi Jia and Xiangchu Feng and Weiwei Wang and Lei Zhang},
  doi          = {10.1109/TNNLS.2020.2985850},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1627-1641},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Generalized unitarily invariant gauge regularization for fast low-rank matrix recovery},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Nonconvex low-rank kernel sparse subspace learning for
keyframe extraction and motion segmentation. <em>TNNLS</em>,
<em>32</em>(4), 1612–1626. (<a
href="https://doi.org/10.1109/TNNLS.2020.2985817">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {By exploiting the kernel trick, the sparse subspace model is extended to the nonlinear version with one or a combination of predefined kernels, but the high-dimensional space induced by predefined kernels is not guaranteed to be able to capture the features of the nonlinear data in theory. In this article, we propose a nonconvex low-rank learning framework in an unsupervised way to learn a kernel to replace the predefined kernel in the sparse subspace model. The learned kernel by a nonconvex relaxation of rank can better exploiting the low-rank property of nonlinear data to induce a high-dimensional Hilbert space that more closely approaches the true feature space. Furthermore, we give a global closed-form optimal solution of the nonconvex rank minimization and prove it. Considering the low-rank and sparseness characteristics of motion capture data in its feature space, we use them to verify the better representation of nonlinear data with the learned kernel via two tasks: keyframe extraction and motion segmentation. The performances on both tasks demonstrate the advantage of our model over the sparse subspace model with predefined kernels and some other related state-of-art methods.},
  archive      = {J_TNNLS},
  author       = {Guiyu Xia and Beijia Chen and Huaijiang Sun and Qingshan Liu},
  doi          = {10.1109/TNNLS.2020.2985817},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1612-1626},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Nonconvex low-rank kernel sparse subspace learning for keyframe extraction and motion segmentation},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hierarchical optimal synchronization for linear systems via
reinforcement learning: A stackelberg–nash game perspective.
<em>TNNLS</em>, <em>32</em>(4), 1600–1611. (<a
href="https://doi.org/10.1109/TNNLS.2020.2985738">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Considering the fact that in the real world, a certain agent may have some sort of advantage to act before others, a novel hierarchical optimal synchronization problem for linear systems, composed of one major agent and multiple minor agents, is formulated and studied in this article from a Stackelberg-Nash game perspective. The major agent herein makes its decision prior to others, and then, all the minor agents determine their actions simultaneously. To seek the optimal controllers, the Hamilton-Jacobi-Bellman (HJB) equations in coupled forms are established, whose solutions are further proven to be stable and constitute the Stackelberg-Nash equilibrium. Due to the introduction of the asymmetric roles for agents, the established HJB equations are more strongly coupled and more difficult to solve than that given in most existing works. Therefore, we propose a new reinforcement learning (RL) algorithm, i.e., a two-level value iteration (VI) algorithm, which does not rely on complete system matrices. Furthermore, the proposed algorithm is shown to be convergent, and the converged values are exactly the optimal ones. To implement this VI algorithm, neural networks (NNs) are employed to approximate the value functions, and the gradient descent method is used to update the weights of NNs. Finally, an illustrative example is provided to verify the effectiveness of the proposed algorithm.},
  archive      = {J_TNNLS},
  author       = {Man Li and Jiahu Qin and Qichao Ma and Wei Xing Zheng and Yu Kang},
  doi          = {10.1109/TNNLS.2020.2985738},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1600-1611},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Hierarchical optimal synchronization for linear systems via reinforcement learning: A Stackelberg–Nash game perspective},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). LSTM-MSNet: Leveraging forecasts on sets of related time
series with multiple seasonal patterns. <em>TNNLS</em>, <em>32</em>(4),
1586–1599. (<a
href="https://doi.org/10.1109/TNNLS.2020.2985720">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generating forecasts for time series with multiple seasonal cycles is an important use case for many industries nowadays. Accounting for the multiseasonal patterns becomes necessary to generate more accurate and meaningful forecasts in these contexts. In this article, we propose long short-term memory multiseasonal net (LSTM-MSNet), a decomposition-based unified prediction framework to forecast time series with multiple seasonal patterns. The current state of the art in this space is typically univariate methods, in which the model parameters of each time series are estimated independently. Consequently, these models are unable to include key patterns and structures that may be shared by a collection of time series. In contrast, LSTM-MSNet is a globally trained LSTM network, where a single prediction model is built across all the available time series to exploit the cross-series knowledge in a group of related time series. Furthermore, our methodology combines a series of state-of-the-art multiseasonal decomposition techniques to supplement the LSTM learning procedure. In our experiments, we are able to show that on data sets from disparate data sources, e.g., the popular M4 forecasting competition, a decomposition step is beneficial, whereas, in the common real-world situation of homogeneous series from a single application, exogenous seasonal variables or no seasonal preprocessing at all are better choices. All options are readily included in the framework and allow us to achieve competitive results for both cases, outperforming many state-of-the-art multiseasonal forecasting methods.},
  archive      = {J_TNNLS},
  author       = {Kasun Bandara and Christoph Bergmeir and Hansika Hewamalage},
  doi          = {10.1109/TNNLS.2020.2985720},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1586-1599},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {LSTM-MSNet: Leveraging forecasts on sets of related time series with multiple seasonal patterns},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Observer-based adaptive neural networks control for
large-scale interconnected systems with nonconstant control gains.
<em>TNNLS</em>, <em>32</em>(4), 1575–1585. (<a
href="https://doi.org/10.1109/TNNLS.2020.2985417">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, an adaptive neural network (NN) decentralized output-feedback control design is studied for the uncertain strict-feedback large-scale interconnected nonlinear systems with nonconstant virtual and control gains. NNs are utilized to approximate the unknown nonlinear functions, and the immeasurable states are estimated via designing an NN decentralized state observer. By constructing the logarithm Lyapunov functions, an observer-based NN adaptive decentralized backstepping output-feedback control is developed in the framework of the decentralized backstepping control. The proposed adaptive decentralized backstepping output-feedback control can make that the closed-loop system is semiglobally uniformly ultimately bounded (SGUUB) and that the tracking and observer errors converge to a small neighborhood of the origin. The most important contribution of this article is that it removes the restrictive assumption in the existing results that both virtual and control gain functions in each subsystem must be constants. A numerical simulation example is provided to validate the effectiveness of the proposed control method and theory.},
  archive      = {J_TNNLS},
  author       = {Shaocheng Tong and Yongming Li and Yanjun Liu},
  doi          = {10.1109/TNNLS.2020.2985417},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1575-1585},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Observer-based adaptive neural networks control for large-scale interconnected systems with nonconstant control gains},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Iterative dynamic generic learning for face recognition from
a contaminated single-sample per person. <em>TNNLS</em>, <em>32</em>(4),
1560–1574. (<a
href="https://doi.org/10.1109/TNNLS.2020.2985099">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article focuses on a new and practical problem in single-sample per person face recognition (SSPP FR), i.e., SSPP FR with a contaminated biometric enrolment database (SSPP-ce FR), where the SSPP-based enrolment database is contaminated by nuisance facial variations in the wild, such as poor lightings, expression change, and disguises (e.g., wearing sunglasses, hat, and scarf). In SSPP-ce FR, the most popular generic learning methods will suffer serious performance degradation because the prototype plus variation (P+V) model used in these methods is no longer suitable in such scenarios. The reasons are twofold. First, the contaminated enrolment samples could yield bad prototypes to represent the persons. Second, the generated variation dictionary is simply based on the subtraction of the average face from generic samples of the same person and cannot well depict the intrapersonal variations. To address the SSPP-ce FR problem, we propose a novel iterative dynamic generic learning (IDGL) method, where the labeled enrolment database and the unlabeled query set are fed into a dynamic label feedback network for learning. Specifically, IDGL first recovers the prototypes for the contaminated enrolment samples via a semisupervised low-rank representation (SSLRR) framework and learns a representative variation dictionary by extracting the “sample-specific” corruptions from an auxiliary generic set. Then, it puts them into the P+V model to estimate labels for query samples. Subsequently, the estimated labels will be used as feedback to modify the SSLRR, thus updating new prototypes for the next round of P+V-based label estimation. With the dynamic learning network, the accuracy of the estimated labels is improved iteratively by virtue of the steadily enhanced prototypes. Experiments on various benchmark face data sets have demonstrated the superiority of IDGL over state-of-the-art counterparts.},
  archive      = {J_TNNLS},
  author       = {Meng Pang and Yiu-Ming Cheung and Qiquan Shi and Mengke Li},
  doi          = {10.1109/TNNLS.2020.2985099},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1560-1574},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Iterative dynamic generic learning for face recognition from a contaminated single-sample per person},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Unsupervised and semisupervised projection with graph
optimization. <em>TNNLS</em>, <em>32</em>(4), 1547–1559. (<a
href="https://doi.org/10.1109/TNNLS.2020.2984958">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph-based technique is widely used in projection, clustering, and classification tasks. In this article, we propose a novel and solid framework, named unsupervised projection with graph optimization (UPGO), for both dimensionality reduction and clustering. Different from the existing algorithms which treat graph construction and projection learning as two separate steps, UPGO unifies graph construction and projection learning into a general framework. It learns the graph similarity matrix adaptively based on the relationships among the low-dimensional representations. A constraint is introduced to the Laplacian matrix to learn a structured graph which contains the clustering structure, from which the clustering results can be obtained directly without requiring any postprocessing. The structured graph achieves the ideal neighbors assignment, based on which an optimal low-dimensional subspace can be learned. Moreover, we generalize UPGO to tackle the semisupervised case, namely semisupervised projection with graph optimization (SPGO), a framework for both dimensionality reduction and classification. An efficient algorithm is derived to optimize the proposed frameworks. We provide theoretical analysis about convergence analysis, computational complexity, and parameter determination. Experimental results on real-world data sets show the effectiveness of the proposed frameworks compared with the state-of-the-art algorithms. Results also confirm the generality of the proposed frameworks.},
  archive      = {J_TNNLS},
  author       = {Feiping Nie and Xia Dong and Xuelong Li},
  doi          = {10.1109/TNNLS.2020.2984958},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1547-1559},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Unsupervised and semisupervised projection with graph optimization},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Performance guaranteed consensus tracking control of
nonlinear multiagent systems: A finite-time function-based approach.
<em>TNNLS</em>, <em>32</em>(4), 1536–1546. (<a
href="https://doi.org/10.1109/TNNLS.2020.2984944">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we study the performance guaranteed consensus tracking problem for a class of high-order nonlinear multiagent systems subject to mismatched uncertainties and external disturbances. We first construct a finite-time function, with which a performance function is introduced that links the convergence time of the relative consensus errors with the neighbor agents. We then introduce two new lemmas that play a virtual role in addressing the consensus stability of closed-loop multiagent system, where a fully distributed adaptive control without using global information of the topology is developed. Different from most existing works for multiagent systems with prescribed performance that can only achieve uniformly ultimately bounded consensus, the proposed control scheme is able to ensure that the consensus errors converge to the pregiven compact sets within preassigned finite time rather than infinite time and the outputs of all the agents track the leader&#39;s trajectory asymptotically. Simulation verification also confirms the effectiveness of the proposed approach.},
  archive      = {J_TNNLS},
  author       = {Ye Cao and Yongduan Song},
  doi          = {10.1109/TNNLS.2020.2984944},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1536-1546},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Performance guaranteed consensus tracking control of nonlinear multiagent systems: A finite-time function-based approach},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). CED: A distance for complex mass functions. <em>TNNLS</em>,
<em>32</em>(4), 1525–1535. (<a
href="https://doi.org/10.1109/TNNLS.2020.2984918">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Evidence theory is an effective methodology for modeling and processing uncertainty that has been widely applied in various fields. In evidence theory, a number of distance measures have been presented, which play an important role in representing the degree of difference between pieces of evidence. However, the existing evidential distances focus on traditional basic belief assignments (BBAs) modeled in terms of real numbers and are not compatible with complex BBAs (CBBAs) extended to the complex plane. Therefore, in this article, a generalized evidential distance measure called the complex evidential distance (CED) is proposed, which can measure the difference or dissimilarity between CBBAs in complex evidence theory. This is the first work to consider distance measures for CBBAs, and it provides a promising way to measure the differences between pieces of evidence in a more general framework of complex plane space. Furthermore, the CED is a strict distance metric with the properties of nonnegativity, nondegeneracy, symmetry, and triangle inequality that satisfies the axioms of a distance. In particular, when the CBBAs degenerate into classical BBAs, the CED will degenerate into Jousselme et al.&#39;s distance. Therefore, the proposed CED is a generalization of the traditional evidential distance, but it has a greater ability to measure the difference or dissimilarity between pieces of evidence. Finally, a decision-making algorithm for pattern recognition is devised based on the CED and is applied to a medical diagnosis problem to illustrate its practicability.},
  archive      = {J_TNNLS},
  author       = {Fuyuan Xiao},
  doi          = {10.1109/TNNLS.2020.2984918},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1525-1535},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {CED: A distance for complex mass functions},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multilinear compressive learning. <em>TNNLS</em>,
<em>32</em>(4), 1512–1524. (<a
href="https://doi.org/10.1109/TNNLS.2020.2984831">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compressive learning (CL) is an emerging topic that combines signal acquisition via compressive sensing (CS) and machine learning to perform inference tasks directly on a small number of measurements. Many data modalities naturally have a multidimensional or tensorial format, with each dimension or tensor mode representing different features such as the spatial and temporal information in video sequences or the spatial and spectral information in hyperspectral images. However, in existing CL frameworks, the CS component utilizes either random or learned linear projection on the vectorized signal to perform signal acquisition, thus discarding the multidimensional structure of the signals. In this article, we propose multilinear CL (MCL), a framework that takes into account the tensorial nature of multidimensional signals in the acquisition step and builds the subsequent inference model on the structurally sensed measurements. Our theoretical complexity analysis shows that the proposed framework is more efficient compared to its vector-based counterpart in both memory and computation requirement. With extensive experiments, we also empirically show that our MCL framework outperforms the vector-based framework in object classification and face recognition tasks, and scales favorably when the dimensionalities of the original signals increase, making it highly efficient for high-dimensional multidimensional signals.},
  archive      = {J_TNNLS},
  author       = {Dat Thanh Tran and Mehmet Yamaç and Aysen Degerli and Moncef Gabbouj and Alexandros Iosifidis},
  doi          = {10.1109/TNNLS.2020.2984831},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1512-1524},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multilinear compressive learning},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Self-paced clustering ensemble. <em>TNNLS</em>,
<em>32</em>(4), 1497–1511. (<a
href="https://doi.org/10.1109/TNNLS.2020.2984814">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The clustering ensemble has emerged as an important extension of the classical clustering problem. It provides an elegant framework to integrate multiple weak base clusterings to generate a strong consensus result. Most existing clustering ensemble methods usually exploit all data to learn a consensus clustering result, which does not sufficiently consider the adverse effects caused by some difficult instances. To handle this problem, we propose a novel self-paced clustering ensemble (SPCE) method, which gradually involves instances from easy to difficult ones into the ensemble learning. In our method, we integrate the evaluation of the difficulty of instances and ensemble learning into a unified framework, which can automatically estimate the difficulty of instances and ensemble the base clusterings. To optimize the corresponding objective function, we propose a joint learning algorithm to obtain the final consensus clustering result. Experimental results on benchmark data sets demonstrate the effectiveness of our method.},
  archive      = {J_TNNLS},
  author       = {Peng Zhou and Liang Du and Xinwang Liu and Yi-Dong Shen and Mingyu Fan and Xuejun Li},
  doi          = {10.1109/TNNLS.2020.2984814},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1497-1511},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Self-paced clustering ensemble},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Co-learning non-negative correlated and uncorrelated
features for multi-view data. <em>TNNLS</em>, <em>32</em>(4), 1486–1496.
(<a href="https://doi.org/10.1109/TNNLS.2020.2984810">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view data can represent objects from different perspectives and thus provide complementary information for data analysis. A topic of great importance in multi-view learning is to locate a low-dimensional latent subspace, where common semantic features are shared by multiple data sets. However, most existing methods ignore uncorrelated items (i.e., view-specific features) and may cause semantic bias during the process of common feature learning. In this article, we propose a non-negative correlated and uncorrelated feature co-learning (CoUFC) method to address this concern. More specifically, view-specific (uncorrelated) features are identified for each view when learning the common (correlated) feature across views in the latent semantic subspace. By eliminating the effects of uncorrelated information, useful inter-view feature correlations can be captured. We design a new objective function in CoUFC and derive an optimization approach to solve the objective with the analysis on its convergence. Experiments on real-world sensor, image, and text data sets demonstrate that the proposed method outperforms the state-of-the-art multiview learning methods.},
  archive      = {J_TNNLS},
  author       = {Liang Zhao and Tao Yang and Jie Zhang and Zhikui Chen and Yi Yang and Z. Jane Wang},
  doi          = {10.1109/TNNLS.2020.2984810},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1486-1496},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Co-learning non-negative correlated and uncorrelated features for multi-view data},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Finite-time tracking control for nonlinear systems via
adaptive neural output feedback and command filtered backstepping.
<em>TNNLS</em>, <em>32</em>(4), 1474–1485. (<a
href="https://doi.org/10.1109/TNNLS.2020.2984773">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article is concerned with the tracking control problem for uncertain high-order nonlinear systems in the presence of input saturation. A finite-time control strategy combined with neural state observer and command filtered backstepping is proposed. The neural network models the unknown nonlinear dynamics, the finite-time command filter (FTCF) guarantees the approximation of its output to the derivative of virtual control signal in finite time at the backstepping procedure, and the fraction power-based error compensation system compensates for the filtering errors between FTCF and virtual signal. In addition, the input saturation problem is dealt with by introducing the auxiliary system. Overall, it is shown that the designed controller drives the output tracking error to the desired neighborhood of the origin at a finite time and all the signals in the closed-loop system are bounded at a finite time. Two simulation examples are given to demonstrate the control effectiveness.},
  archive      = {J_TNNLS},
  author       = {Lin Zhao and Jinpeng Yu and Qing-Guo Wang},
  doi          = {10.1109/TNNLS.2020.2984773},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1474-1485},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Finite-time tracking control for nonlinear systems via adaptive neural output feedback and command filtered backstepping},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Impulsive synchronization of unbounded delayed inertial
neural networks with actuator saturation and sampled-data control and
its application to image encryption. <em>TNNLS</em>, <em>32</em>(4),
1460–1473. (<a
href="https://doi.org/10.1109/TNNLS.2020.2984770">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The article considers the impulsive synchronization for inertial neural networks with unbounded delay and actuator saturation via sampled-data control. Based on an impulsive differential inequality, the difficulties caused by unbounded delay and impulsive effect may be effectively avoid. By applying polytopic representation technique, the actuator saturation term is first considered into the design of impulsive controller, and less conservative linear matrix inequality (LMI) criteria that guarantee asymptotical synchronization for the considered model via hybrid control are given. As special cases, the asymptotical synchronization of the considered model via sampled-data control and saturating impulsive control are also studied, respectively. Numerical simulations are presented to claim the effectiveness of theoretical analysis. A new image encryption algorithm is proposed to utilize the synchronization theory of hybrid control. The validity of image encryption algorithm can be obtained by experiments.},
  archive      = {J_TNNLS},
  author       = {Hongfei Li and Chuandong Li and Deqiang Ouyang and Sing Kiong Nguang},
  doi          = {10.1109/TNNLS.2020.2984770},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1460-1473},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Impulsive synchronization of unbounded delayed inertial neural networks with actuator saturation and sampled-data control and its application to image encryption},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Active multilabel crowd consensus. <em>TNNLS</em>,
<em>32</em>(4), 1448–1459. (<a
href="https://doi.org/10.1109/TNNLS.2020.2984729">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crowdsourcing is an economic and efficient strategy aimed at collecting annotations of data through an online platform. Crowd workers with different expertise are paid for their service, and the task requester usually has a limited budget. How to collect reliable annotations for multilabel data and how to compute the consensus within budget are an interesting and challenging, but rarely studied, problem. In this article, we propose a novel approach to accomplish active multilabel crowd consensus (AMCC). AMCC accounts for the commonality and individuality of workers and assumes that workers can be organized into different groups. Each group includes a set of workers who share a similar annotation behavior and label correlations. To achieve an effective multilabel consensus, AMCC models workers&#39; annotations via a linear combination of commonality and individuality and reduces the impact of unreliable workers by assigning smaller weights to their groups. To collect reliable annotations with reduced cost, AMCC introduces an active crowdsourcing learning strategy that selects sample-label-worker triplets. In a triplet, the selected sample and label are the most informative for the consensus model, and the selected worker can reliably annotate the sample at a low cost. Our experimental results on multilabel data sets demonstrate the advantages of AMCC over state-of-the-art solutions on computing crowd consensus and on reducing the budget by choosing cost-effective triplets.},
  archive      = {J_TNNLS},
  author       = {Guoxian Yu and Jinzheng Tu and Jun Wang and Carlotta Domeniconi and Xiangliang Zhang},
  doi          = {10.1109/TNNLS.2020.2984729},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1448-1459},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Active multilabel crowd consensus},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust few-shot learning for user-provided data.
<em>TNNLS</em>, <em>32</em>(4), 1433–1447. (<a
href="https://doi.org/10.1109/TNNLS.2020.2984710">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot learning (FSL) focuses on distilling transferrable knowledge from existing experience to cope with novel concepts for which the labeled data are scarce. A typical assumption in FSL is that the training examples of novel classes are all clean with no outlier interference. In many realistic applications where examples are provided by users, however, data are potentially noisy or unreadable. In this context, we introduce a novel research topic, robust FSL (RFSL), where we aim to address two types of outliers within user-provided data: the representation outlier (RO) and the label outlier (LO). Moreover, we introduce a metric for estimating robustness and use it to investigate the performance of several advanced methods to FSL when faced with user-provided outliers. In addition, we propose robust attentive profile networks (RapNets) to achieve outlier suppression. The results of a comprehensive evaluation of benchmark data sets demonstrate the shortcomings of current FSL methods and the superiority of the proposed RapNets when dealing with RFSL problems, establishing a benchmark for follow-up studies.},
  archive      = {J_TNNLS},
  author       = {Jiang Lu and Sheng Jin and Jian Liang and Changshui Zhang},
  doi          = {10.1109/TNNLS.2020.2984710},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1433-1447},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Robust few-shot learning for user-provided data},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A survey on learning-based approaches for modeling and
classification of human–machine dialog systems. <em>TNNLS</em>,
<em>32</em>(4), 1418–1432. (<a
href="https://doi.org/10.1109/TNNLS.2020.2985588">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development from traditional machine learning (ML) to deep learning (DL) and reinforcement learning (RL), dialog system equipped with learning mechanism has become the most effective solution to address human–machine interaction problems. The purpose of this article is to provide a comprehensive survey on learning-based human–machine dialog systems with a focus on the various dialog models. More specifically, we first introduce the fundamental process of establishing a dialog model. Second, we examine the features and classifications of the system dialog model, expound some representative models, and also compare the advantages and disadvantages of different dialog models. Third, we comb the commonly used database and evaluation metrics of the dialog model. Furthermore, the evaluation metrics of these dialog models are analyzed in detail. Finally, we briefly analyze the existing issues and point out the potential future direction on the human–machine dialog systems.},
  archive      = {J_TNNLS},
  author       = {Fuwei Cui and Qian Cui and Yongduan Song},
  doi          = {10.1109/TNNLS.2020.2985588},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1418-1432},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A survey on learning-based approaches for modeling and classification of Human–Machine dialog systems},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An uncertainty-aware transfer learning-based framework for
COVID-19 diagnosis. <em>TNNLS</em>, <em>32</em>(4), 1408–1417. (<a
href="https://doi.org/10.1109/TNNLS.2021.3054306">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The early and reliable detection of COVID-19 infected patients is essential to prevent and limit its outbreak. The PCR tests for COVID-19 detection are not available in many countries, and also, there are genuine concerns about their reliability and performance. Motivated by these shortcomings, this article proposes a deep uncertainty-aware transfer learning framework for COVID-19 detection using medical images. Four popular convolutional neural networks (CNNs), including VGG16, ResNet50, DenseNet121, and InceptionResNetV2, are first applied to extract deep features from chest X-ray and computed tomography (CT) images. Extracted features are then processed by different machine learning and statistical modeling techniques to identify COVID-19 cases. We also calculate and report the epistemic uncertainty of classification results to identify regions where the trained models are not confident about their decisions (out of distribution problem). Comprehensive simulation results for X-ray and CT image data sets indicate that linear support vector machine and neural network models achieve the best results as measured by accuracy, sensitivity, specificity, and area under the receiver operating characteristic (ROC) curve (AUC). Also, it is found that predictive uncertainty estimates are much higher for CT images compared to X-ray images.},
  archive      = {J_TNNLS},
  author       = {Afshar Shamsi and Hamzeh Asgharnezhad and Shirin Shamsi Jokandan and Abbas Khosravi and Parham M. Kebria and Darius Nahavandi and Saeid Nahavandi and Dipti Srinivasan},
  doi          = {10.1109/TNNLS.2021.3054306},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1408-1417},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {An uncertainty-aware transfer learning-based framework for COVID-19 diagnosis},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021j). IEEE computational intelligence society information.
<em>TNNLS</em>, <em>32</em>(3), C3. (<a
href="https://doi.org/10.1109/TNNLS.2021.3058495">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Provides a listing of current committee members and society officers.},
  archive      = {J_TNNLS},
  doi          = {10.1109/TNNLS.2021.3058495},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {C3},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {IEEE computational intelligence society information},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Large margin gaussian mixture classifier with a gabriel
graph geometric representation of data set structure. <em>TNNLS</em>,
<em>32</em>(3), 1400–1406. (<a
href="https://doi.org/10.1109/TNNLS.2020.2980559">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This brief presents a geometrical approach for obtaining large margin classifiers. The method aims at exploring the geometrical properties of the data set from the structure of a Gabriel graph, which represents pattern relations according to a given distance metric, such as the Euclidean distance. Once the graph is generated, geometrical support vectors (SVs) (analogous to support vector machines (SVMs) SVs) are obtained in order to yield the final large margin solution from a Gaussian mixture model. Experiments with 20 data sets have shown that the solutions obtained with the proposed method are statistically equivalent to those obtained with SVMs. However, the present method does not require optimization and can also be extended to large data sets using the cascade SVM concept.},
  archive      = {J_TNNLS},
  author       = {Luiz C. B. Torres and Cristiano L. Castro and Frederico Coelho and Antônio P. Braga},
  doi          = {10.1109/TNNLS.2020.2980559},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {1400-1406},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Large margin gaussian mixture classifier with a gabriel graph geometric representation of data set structure},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Social neighborhood graph and multigraph fusion ranking for
multifeature image retrieval. <em>TNNLS</em>, <em>32</em>(3), 1389–1399.
(<a href="https://doi.org/10.1109/TNNLS.2020.2984676">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A single feature is hard to describe the content of images from an overall perspective, which limits the retrieval performances of single-feature-based methods in image retrieval tasks. To fully describe the properties of images and improve the retrieval performances, multifeature fusion ranking-based methods are proposed. However, the effectiveness of multifeature fusion in image retrieval has not been theoretically explained. This article gives a theoretical proof to illustrate the role of independent features in improving the retrieval results. Based on the theoretical proof, the original ranking list generated with a single feature greatly influences the performances of multifeature fusion ranking. Inspired by the principle of three degrees of influence in social networks, this article proposes a reranking method named $k$ -nearest neighbors’ neighbors’ neighbors’ graph (N3G) to improve the original ranking list by a single feature. Furthermore, a multigraph fusion ranking (MFR) method motivated by the group relation theory in social networks for multifeature ranking is also proposed, which considers the correlations of all images in multiple neighborhood graphs. Evaluation experiments conducted on several representative data sets (e.g., UK-bench, Holiday, Corel-10K, and Cifar-10) validate that N3G and MFR outperform the other state-of-the-art methods.},
  archive      = {J_TNNLS},
  author       = {Shenglan Liu and Muxin Sun and Lin Feng and Hong Qiao and Shuyuan Chen and Yang Liu},
  doi          = {10.1109/TNNLS.2020.2984676},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {1389-1399},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Social neighborhood graph and multigraph fusion ranking for multifeature image retrieval},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). CAME: Content- and context-aware music embedding for
recommendation. <em>TNNLS</em>, <em>32</em>(3), 1375–1388. (<a
href="https://doi.org/10.1109/TNNLS.2020.2984665">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional recommendation methods suffer from limited performance, which can be addressed by incorporating abundant auxiliary/side information. This article focuses on a personalized music recommender system that incorporates rich content and context data in a unified and adaptive way to address the abovementioned problems. The content information includes music textual content, such as metadata, tags, and lyrics, and the context data incorporate users&#39; behaviors, including music listening records, music playing sequences, and sessions. Specifically, a heterogeneous information network (HIN) is first presented to incorporate different kinds of content and context data. Then, a novel method called content- and context-aware music embedding (CAME) is proposed to obtain the low-dimension dense real-valued feature representations (embeddings) of music pieces from HIN. Especially, one music piece generally highlights different aspects when interacting with various neighbors, and it should have different representations separately. CAME seamlessly combines deep learning techniques, including convolutional neural networks and attention mechanisms, with the embedding model to capture the intrinsic features of music pieces as well as their dynamic relevance and interactions adaptively. Finally, we further infer users&#39; general musical preferences as well as their contextual preferences for music and propose a content- and context-aware music recommendation method. Comprehensive experiments as well as quantitative and qualitative evaluations have been performed on real-world music data sets, and the results show that the proposed recommendation approach outperforms state-of-the-art baselines and is able to handle sparse data effectively.},
  archive      = {J_TNNLS},
  author       = {Dongjing Wang and Xin Zhang and Dongjin Yu and Guandong Xu and Shuiguang Deng},
  doi          = {10.1109/TNNLS.2020.2984665},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {1375-1388},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {CAME: Content- and context-aware music embedding for recommendation},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On μ-pseudo almost periodic solutions for clifford-valued
neutral type neural networks with delays in the leakage term.
<em>TNNLS</em>, <em>32</em>(3), 1365–1374. (<a
href="https://doi.org/10.1109/TNNLS.2020.2984655">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a class of Clifford-valued neutral type neural networks with delays in the leakage term. Using a direct method, that is, without decomposing the Clifford-valued system under consideration into a real-valued system, we obtain sufficient conditions for the existence and global exponential stability of μ-pseudo almost periodic solutions of the Cliffordvalued neural network under consideration. Finally, we give a numerical example to show the feasibility of our results.},
  archive      = {J_TNNLS},
  author       = {Yongkun Li and Nina Huo and Bing Li},
  doi          = {10.1109/TNNLS.2020.2984655},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {1365-1374},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {On μ-pseudo almost periodic solutions for clifford-valued neutral type neural networks with delays in the leakage term},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). RMoR-aion: Robust multioutput regression by simultaneously
alleviating input and output noises. <em>TNNLS</em>, <em>32</em>(3),
1351–1364. (<a
href="https://doi.org/10.1109/TNNLS.2020.2984635">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multioutput regression, referring to simultaneously predicting multiple continuous output variables with a single model, has drawn increasing attention in the machine learning community due to its strong ability to capture the correlations among multioutput variables. The methodology of output space embedding, built upon the low-rank assumption, is now the mainstream for multioutput regression since it can effectively reduce the parameter numbers while achieving effective performance. The existing low-rank methods, however, are sensitive to the noises of both inputs and outputs, referring to the noise problem. In this article, we develop a novel multioutput regression method by simultaneously alleviating input and output noises, namely, robust multioutput regression by alleviating input and output noises (RMoR-Aion), where both the noises of the input and output are exploited by leveraging auxiliary matrices. Furthermore, we propose a prediction output manifold constraint with the correlation information regarding the output variables to further reduce the adversarial effects of the noise. Our empirical studies demonstrate the effectiveness of RMoR-Aion compared with the state-of-the-art baseline methods, and RMoR-Aion is more stable in the settings with artificial noise.},
  archive      = {J_TNNLS},
  author       = {Ximing Li and Yang Wang and Zhao Zhang and Richang Hong and Zhuo Li and Meng Wang},
  doi          = {10.1109/TNNLS.2020.2984635},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {1351-1364},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {RMoR-aion: Robust multioutput regression by simultaneously alleviating input and output noises},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Energy-to-peak state estimation for switched neutral-type
neural networks with sector condition via sampled-data information.
<em>TNNLS</em>, <em>32</em>(3), 1339–1350. (<a
href="https://doi.org/10.1109/TNNLS.2020.2984629">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, the energy-to-peak state estimation problem is investigated for a class of switched neutral neural networks subject to the external perturbations with bounded energy. Both the values of the measurement outputs and switching signal of the subsystems are only available for the controllers at the discrete sampling instants. Unlike the results for nonswitched neural networks, the coexistence of the switching and sampling actions directly causes the asynchronous phenomena between the indexes of subsystems and their corresponding controllers. To address this situation, the piecewise time-dependent Lyapunov-Krasovskii functional and slow switching mechanism are introduced. Under the developed theorem conditions, we prove that the designed state estimator exponentially tracks the true value of the neural state with the accessible sampled-data information. Also, the influence of the exogenous perturbations on the peak value of the estimation error is constrained at a prescribed level. Finally, a neutral cellular neural network with switching parameters is employed to substantiate the effectiveness and applicability of the theoretical results.},
  archive      = {J_TNNLS},
  author       = {Hong Sang and Jun Zhao},
  doi          = {10.1109/TNNLS.2020.2984629},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {1339-1350},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Energy-to-peak state estimation for switched neutral-type neural networks with sector condition via sampled-data information},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Prior knowledge regularized multiview self-representation
and its applications. <em>TNNLS</em>, <em>32</em>(3), 1325–1338. (<a
href="https://doi.org/10.1109/TNNLS.2020.2984625">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To learn the self-representation matrices/tensor that encodes the intrinsic structure of the data, existing multiview self-representation models consider only the multiview features and, thus, impose equal membership preference across samples. However, this is inappropriate in real scenarios since the prior knowledge, e.g., explicit labels, semantic similarities, and weak-domain cues, can provide useful insights into the underlying relationship of samples. Based on this observation, this article proposes a prior knowledge regularized multiview self-representation (P-MVSR) model, in which the prior knowledge, multiview features, and high-order cross-view correlation are jointly considered to obtain an accurate self-representation tensor. The general concept of “prior knowledge” is defined as the complement of multiview features, and the core of P-MVSR is to take advantage of the membership preference, which is derived from the prior knowledge, to purify and refine the discovered membership of the data. Moreover, P-MVSR adopts the same optimization procedure to handle different prior knowledge and, thus, provides a unified framework for weakly supervised clustering and semisupervised classification. Extensive experiments on real-world databases demonstrate the effectiveness of the proposed P-MVSR model.},
  archive      = {J_TNNLS},
  author       = {Xiaolin Xiao and Yongyong Chen and Yue-Jiao Gong and Yicong Zhou},
  doi          = {10.1109/TNNLS.2020.2984625},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {1325-1338},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Prior knowledge regularized multiview self-representation and its applications},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Distributed optimization for two types of heterogeneous
multiagent systems. <em>TNNLS</em>, <em>32</em>(3), 1314–1324. (<a
href="https://doi.org/10.1109/TNNLS.2020.2984584">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article studies distributed optimization algorithms for heterogeneous multiagent systems under an undirected and connected communication graph. Two types of heterogeneities are discussed. First, we consider a class of multiagent systems composed of both continuous-time dynamic agents and discrete-time dynamic agents. The agents coordinate with each other to minimize a global objective function that is the sum of their local convex objective functions. A distributed subgradient method is proposed for each agent in the network. It is proved that driven by the proposed updating law, the agents&#39; position states converge to an optimal solution of the optimization problem, provided that the subgradients of the objective functions are bounded, the step size is not summable but square summable, and the sampling period is bounded by some constant. Second, we consider a class of multiagent systems composed of both first-order dynamic agents and second-order dynamic agents. It is proved that the agents&#39; position states converge to the unique optimal solution if the objective functions are strongly convex, continuously differentiable, and the gradients are globally Lipschitz. Numerical examples are given to verify the conclusions.},
  archive      = {J_TNNLS},
  author       = {Chao Sun and Maojiao Ye and Guoqiang Hu},
  doi          = {10.1109/TNNLS.2020.2984584},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {1314-1324},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Distributed optimization for two types of heterogeneous multiagent systems},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sparsely activated networks. <em>TNNLS</em>, <em>32</em>(3),
1304–1313. (<a
href="https://doi.org/10.1109/TNNLS.2020.2984514">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Previous literature on unsupervised learning focused on designing structural priors with the aim of learning meaningful features. However, this was done without considering the description length of the learned representations, which is a direct and unbiased measure of the model complexity. In this article, first, we introduce the φ metric that evaluates unsupervised models based on their reconstruction accuracy and the degree of compression of their internal representations. We then present and define two activation functions [Identity and rectified linear unit (ReLU)] as a base of reference and three sparse activation functions (top-k absolutes, Extrema-Pool indices, and Extrema) as candidate structures that minimize the previously defined φ. We last present sparsely activated networks (SANs) that consist of kernels with shared weights that, during encoding, are convolved with the input and then passed through a sparse activation function. During decoding, the same weights are convolved with the sparse activation map, and subsequently, the partial reconstructions from each weight are summed to reconstruct the input. We compare SANs using the five previously defined activation functions on a variety of data sets (Physionet, UCI-epilepsy, MNIST, and FMNIST) and show that models that are selected using φ have small description representation length and consist of interpretable kernels.},
  archive      = {J_TNNLS},
  author       = {Paschalis Bizopoulos and Dimitrios Koutsouris},
  doi          = {10.1109/TNNLS.2020.2984514},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {1304-1313},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Sparsely activated networks},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Resonant machine learning based on complex growth transform
dynamical systems. <em>TNNLS</em>, <em>32</em>(3), 1289–1303. (<a
href="https://doi.org/10.1109/TNNLS.2020.2984267">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional energy-based learning models associate a single energy metric to each configuration of variables involved in the underlying optimization process. Such models associate the lowest energy state with the optimal configuration of variables under consideration and are thus inherently dissipative. In this article, we propose an energy-efficient learning framework that exploits structural and functional similarities between a machine-learning network and a general electrical network satisfying Tellegen&#39;s theorem. In contrast to the standard energy-based models, the proposed formulation associates two energy components, namely, active and reactive energy with the network. The formulation ensures that the network&#39;s active power is dissipated only during the process of learning, whereas the reactive power is maintained to be zero at all times. As a result, in steady state, the learned parameters are stored and self-sustained by electrical resonance determined by the network&#39;s nodal inductances and capacitances. Based on this approach, this article introduces three novel concepts: 1) a learning framework where the network&#39;s active-power dissipation is used as a regularization for a learning objective function that is subjected to zero total reactive-power constraint; 2) a dynamical system based on complex-domain, continuous-time growth transforms that optimizes the learning objective function and drives the network toward electrical resonance under steady-state operation; and 3) an annealing procedure that controls the tradeoff between active-power dissipation and the speed of convergence. As a representative example, we show how the proposed framework can be used for designing resonant support vector machines (SVMs), where the support vectors correspond to an LC network with self-sustained oscillations. We also show that this resonant network dissipates less active power compared with its non-resonant counterpart.},
  archive      = {J_TNNLS},
  author       = {Oindrila Chatterjee and Shantanu Chakrabartty},
  doi          = {10.1109/TNNLS.2020.2984267},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {1289-1303},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Resonant machine learning based on complex growth transform dynamical systems},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Cascaded correlation refinement for robust deep tracking.
<em>TNNLS</em>, <em>32</em>(3), 1276–1288. (<a
href="https://doi.org/10.1109/TNNLS.2020.2984256">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent deep trackers have shown superior performance in visual tracking. In this article, we propose a cascaded correlation refinement approach to facilitate the robustness of deep tracking. The core idea is to address accurate target localization and reliable model update in a collaborative way. To this end, our approach cascades multiple stages of correlation refinement to progressively refine target localization. Thus, the localized object could be used to learn an accurate on-the-fly model for improving the reliability of model update. Meanwhile, we introduce an explicit measure to identify the tracking failure and then leverage a simple yet effective look-back scheme to adaptively incorporate the initial model and on-the-fly model to update the tracking model. As a result, the tracking model can be used to localize the target more accurately. Extensive experiments on OTB2013, OTB2015, VOT2016, VOT2018, UAV123, and GOT-10k demonstrate that the proposed tracker achieves the best robustness against the state of the arts.},
  archive      = {J_TNNLS},
  author       = {Shiming Ge and Chunhui Zhang and Shikun Li and Dan Zeng and Dacheng Tao},
  doi          = {10.1109/TNNLS.2020.2984256},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {1276-1288},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Cascaded correlation refinement for robust deep tracking},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Synchronization for quantized semi-markov switching neural
networks in a finite time. <em>TNNLS</em>, <em>32</em>(3), 1264–1275.
(<a href="https://doi.org/10.1109/TNNLS.2020.2984040">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Finite-time synchronization (FTS) is discussed for delayed semi-Markov switching neural networks (S-MSNNs) with quantized measurement, in which a logarithmic quantizer is employed. The stochastic phenomena of structural and parametrical changes are modeled by a semi-Markov process whose transition rates are time-varying to depend on the sojourn time. Practical systems subject to unpredictable structural changes, such as quadruple-tank process systems, are described by delayed S-MSNNs. A key issue under the consideration is how to design a feedback controller to guarantee the FTS between the master system and the slave system. For this purpose, by using the weak infinitesimal operator, sufficient conditions are constructed to realize FTS of the resulting error system over a finite-time interval. Then, the solvability conditions for the desired finite-time controller can be determined under a linear matrix inequality framework. Finally, the theoretical findings are illustrated by the quadruple-tank process model.},
  archive      = {J_TNNLS},
  author       = {Wenhai Qi and Ju H. Park and Guangdeng Zong and Jinde Cao and Jun Cheng},
  doi          = {10.1109/TNNLS.2020.2984040},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {1264-1275},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Synchronization for quantized semi-markov switching neural networks in a finite time},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sliding mode stabilization of memristive neural networks
with leakage delays and control disturbance. <em>TNNLS</em>,
<em>32</em>(3), 1254–1263. (<a
href="https://doi.org/10.1109/TNNLS.2020.2984000">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we investigate a class of memristive neural networks (MNNs) with time-varying delays and leakage delays via sliding mode control (SMC) with and without control disturbance. SMC is used to ensure MNNs&#39; stability. According to the characteristics of the MNNs, we consider the following three models: the first is the MNNs with time-varying delays, the second is the MNNs with time-varying delays and the control disturbance, and the third is the MNNs with time-varying delays, leakage delays, and the control disturbance. We quote some assumptions and lemmas to ensure that our main results are true. The sliding surface, the corresponding sliding mode controller, and the Lyapunov functions are constructed in different models to ensure MNNs&#39; stability. Finally, some examples and simulations verify the validity of our main results by solving linear matrix inequality (LMI), and the conclusions and analysis of the results are given.},
  archive      = {J_TNNLS},
  author       = {Bo Sun and Yuting Cao and Zhenyuan Guo and Zheng Yan and Shiping Wen and Tingwen Huang and Yiran Chen},
  doi          = {10.1109/TNNLS.2020.2984000},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {1254-1263},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Sliding mode stabilization of memristive neural networks with leakage delays and control disturbance},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Coverage-based designs improve sample mining and
hyperparameter optimization. <em>TNNLS</em>, <em>32</em>(3), 1241–1253.
(<a href="https://doi.org/10.1109/TNNLS.2020.2982936">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sampling one or more effective solutions from large search spaces is a recurring idea in machine learning (ML), and sequential optimization has become a popular solution. Typical examples include data summarization, sample mining for predictive modeling, and hyperparameter optimization. Existing solutions attempt to adaptively trade off between global exploration and local exploitation, in which the initial exploratory sample is critical to their success. While discrepancy-based samples have become the de facto approach for exploration, results from computer graphics suggest that coverage-based designs, e.g., Poisson disk sampling, can be a superior alternative. In order to successfully adopt coverage-based sample designs to ML applications, which were originally developed for 2-D image analysis, we propose fundamental advances by constructing a parameterized family of designs with provably improved coverage characteristics and developing algorithms for effective sample synthesis. Using experiments in sample mining and hyperparameter optimization for supervised learning, we show that our approach consistently outperforms the existing exploratory sampling methods in both blind exploration and sequential search with Bayesian optimization.},
  archive      = {J_TNNLS},
  author       = {Gowtham Muniraju and Bhavya Kailkhura and Jayaraman J. Thiagarajan and Peer-Timo Bremer and Cihan Tepedelenlioglu and Andreas Spanias},
  doi          = {10.1109/TNNLS.2020.2982936},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {1241-1253},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Coverage-based designs improve sample mining and hyperparameter optimization},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Toward mining capricious data streams: A generative
approach. <em>TNNLS</em>, <em>32</em>(3), 1228–1240. (<a
href="https://doi.org/10.1109/TNNLS.2020.2981386">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning with streaming data has received extensive attention during the past few years. Existing approaches assume that the feature space is fixed or changes by following explicit regularities, limiting their applicability in real-time applications. For example, in a smart healthcare platform, the feature space of the patient data varies when different medical service providers use nonidentical feature sets to describe the patients’ symptoms. To fill the gap, we in this article propose a novel learning paradigm, namely, Generative Learning With Streaming Capricious (GLSC) data, which does not make any assumption on the feature space dynamics. In other words, GLSC handles the data streams with a varying feature space, where each arriving data instance can arbitrarily carry new features and/or stop carrying partial old features. Specifically, GLSC trains a learner on a universal feature space that establishes relationships between old and new features, so that the patterns learned in the old feature space can be used in the new feature space. The universal feature space is constructed by leveraging the relatednesses among features. We propose a generative graphical model to model the construction process, and show that learning from the universal feature space can effectively improve the performance with theoretical guarantees. The experimental results demonstrate that GLSC achieves conspicuous performance on both synthetic and real data sets.},
  archive      = {J_TNNLS},
  author       = {Yi He and Baijun Wu and Di Wu and Ege Beyazit and Sheng Chen and Xindong Wu},
  doi          = {10.1109/TNNLS.2020.2981386},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {1228-1240},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Toward mining capricious data streams: A generative approach},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Actor–critic learning control with regularization and
feature selection in policy gradient estimation. <em>TNNLS</em>,
<em>32</em>(3), 1217–1227. (<a
href="https://doi.org/10.1109/TNNLS.2020.2981377">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Actor-critic (AC) learning control architecture has been regarded as an important framework for reinforcement learning (RL) with continuous states and actions. In order to improve learning efficiency and convergence property, previous works have been mainly devoted to solve regularization and feature learning problem in the policy evaluation. In this article, we propose a novel AC learning control method with regularization and feature selection for policy gradient estimation in the actor network. The main contribution is that ℓ 1 -regularization is used on the actor network to achieve the function of feature selection. In each iteration, policy parameters are updated by the regularized dual-averaging (RDA) technique, which solves a minimization problem that involves two terms: one is the running average of the past policy gradients and the other is the ℓ 1 -regularization term of policy parameters. Our algorithm can efficiently calculate the solution of the minimization problem, and we call the new adaptation of policy gradient RDApolicy gradient (RDA-PG). The proposed RDA-PG can learn stochastic and deterministic near-optimal policies. The convergence of the proposed algorithm is established based on the theory of two-timescale stochastic approximation. The simulation and experimental results show that RDA-PG performs feature selection successfully in the actor and learns sparse representations of the actor both in stochastic and deterministic cases. RDA-PG performs better than existing AC algorithms on standard RL benchmark problems with irrelevant features or redundant features.},
  archive      = {J_TNNLS},
  author       = {Luntong Li and Dazi Li and Tianheng Song and Xin Xu},
  doi          = {10.1109/TNNLS.2020.2981377},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {1217-1227},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Actor–Critic learning control with regularization and feature selection in policy gradient estimation},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Modal-regression-based structured low-rank matrix recovery
for multiview learning. <em>TNNLS</em>, <em>32</em>(3), 1204–1216. (<a
href="https://doi.org/10.1109/TNNLS.2020.2980960">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-rank Multiview Subspace Learning (LMvSL) has shown great potential in cross-view classification in recent years. Despite their empirical success, existing LMvSL-based methods are incapable of handling well view discrepancy and discriminancy simultaneously, which, thus, leads to performance degradation when there is a large discrepancy among multiview data. To circumvent this drawback, motivated by the block-diagonal representation learning, we propose structured low-rank matrix recovery (SLMR), a unique method of effectively removing view discrepancy and improving discriminancy through the recovery of the structured low-rank matrix. Furthermore, recent low-rank modeling provides a satisfactory solution to address the data contaminated by the predefined assumptions of noise distribution, such as Gaussian or Laplacian distribution. However, these models are not practical, since complicated noise in practice may violate those assumptions and the distribution is generally unknown in advance. To alleviate such a limitation, modal regression is elegantly incorporated into the framework of SLMR (termed MR-SLMR). Different from previous LMvSL-based methods, our MR-SLMR can handle any zero-mode noise variable that contains a wide range of noise, such as Gaussian noise, random noise, and outliers. The alternating direction method of multipliers (ADMM) framework and half-quadratic theory are used to optimize efficiently MR-SLMR. Experimental results on four public databases demonstrate the superiority of MR-SLMR and its robustness to complicated noise.},
  archive      = {J_TNNLS},
  author       = {Jiamiao Xu and Fangzhao Wang and Qinmu Peng and Xinge You and Shuo Wang and Xiao-Yuan Jing and C. L. Philip Chen},
  doi          = {10.1109/TNNLS.2020.2980960},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {1204-1216},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Modal-regression-based structured low-rank matrix recovery for multiview learning},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Lifelong visual-tactile cross-modal learning for robotic
material perception. <em>TNNLS</em>, <em>32</em>(3), 1192–1203. (<a
href="https://doi.org/10.1109/TNNLS.2020.2980892">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The material attribute of an object’s surface is critical to enable robots to perform dexterous manipulations or actively interact with their surrounding objects. Tactile sensing has shown great advantages in capturing material properties of an object’s surface. However, the conventional classification method based on tactile information may not be suitable to estimate or infer material properties, particularly during interacting with unfamiliar objects in unstructured environments. Moreover, it is difficult to intuitively obtain material properties from tactile data as the tactile signals about material properties are typically dynamic time sequences. In this article, a visual-tactile cross-modal learning framework is proposed for robotic material perception. In particular, we address visual-tactile cross-modal learning in the lifelong learning setting, which is beneficial to incrementally improve the ability of robotic cross-modal material perception. To this end, we proposed a novel lifelong cross-modal learning model. Experimental results on the three publicly available data sets demonstrate the effectiveness of the proposed method.},
  archive      = {J_TNNLS},
  author       = {Wendong Zheng and Huaping Liu and Fuchun Sun},
  doi          = {10.1109/TNNLS.2020.2980892},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {1192-1203},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Lifelong visual-tactile cross-modal learning for robotic material perception},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Anomaly detection of time series with smoothness-inducing
sequential variational auto-encoder. <em>TNNLS</em>, <em>32</em>(3),
1177–1191. (<a
href="https://doi.org/10.1109/TNNLS.2020.2980749">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep generative models have demonstrated their effectiveness in learning latent representation and modeling complex dependencies of time series. In this article, we present a smoothness-inducing sequential variational auto-encoder (VAE) (SISVAE) model for the robust estimation and anomaly detection of multidimensional time series. Our model is based on VAE, and its backbone is fulfilled by a recurrent neural network to capture latent temporal structures of time series for both the generative model and the inference model. Specifically, our model parameterizes mean and variance for each time-stamp with flexible neural networks, resulting in a nonstationary model that can work without the assumption of constant noise as commonly made by existing Markov models. However, such flexibility may cause the model fragile to anomalies. To achieve robust density estimation which can also benefit detection tasks, we propose a smoothness-inducing prior over possible estimations. The proposed prior works as a regularizer that places penalty at nonsmooth reconstructions. Our model is learned efficiently with a novel stochastic gradient variational Bayes estimator. In particular, we study two decision criteria for anomaly detection: reconstruction probability and reconstruction error. We show the effectiveness of our model on both synthetic data sets and public real-world benchmarks.},
  archive      = {J_TNNLS},
  author       = {Longyuan Li and Junchi Yan and Haiyang Wang and Yaohui Jin},
  doi          = {10.1109/TNNLS.2020.2980749},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {1177-1191},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Anomaly detection of time series with smoothness-inducing sequential variational auto-encoder},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A3C-GS: Adaptive moment gradient sharing with locks for
asynchronous actor–critic agents. <em>TNNLS</em>, <em>32</em>(3),
1162–1176. (<a
href="https://doi.org/10.1109/TNNLS.2020.2980743">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose an asynchronous gradient sharing mechanism for the parallel actor-critic algorithms with improved exploration characteristics. The proposed algorithm (A3C-GS) has the property of automatically diversifying worker policies in the short term for exploration, thereby reducing the need for entropy loss terms. Despite policy diversification, the algorithm converges to the optimal policy in the long term. We show in our analysis that the gradient sharing operation is a composition of two contractions. The first contraction performs gradient computation, while the second contraction is a gradient sharing operation coordinated by locks. From these two contractions, certain short- and long-term properties result. For the short term, gradient sharing induces temporary heterogeneity in policies for performing needed exploration. In the long term, under a suitably small learning rate and gradient clipping, convergence to the optimal policy is theoretically guaranteed. We verify our results with several high-dimensional experiments and compare A3C-GS against other on-policy policy-gradient algorithms. Our proposed algorithm achieved the highest weighted score. Despite lower entropy weights, it performed well in high-dimensional environments that require exploration due to sparse rewards and those that need navigation in 3-D environments for long survival tasks. It consistently performed better than the base asynchronous advantage actor-critic (A3C) algorithm.},
  archive      = {J_TNNLS},
  author       = {Alfonso B. Labao and Mygel Andrei M. Martija and Prospero C. Naval},
  doi          = {10.1109/TNNLS.2020.2980743},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {1162-1176},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A3C-GS: Adaptive moment gradient sharing with locks for asynchronous Actor–Critic agents},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Steady-state design of large-dimensional boolean networks.
<em>TNNLS</em>, <em>32</em>(3), 1149–1161. (<a
href="https://doi.org/10.1109/TNNLS.2020.2980632">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Analysis and design of steady states representing cell types, such as cell death or unregulated growth, are of significant interest in modeling genetic regulatory networks. In this article, the steady-state design of large-dimensional Boolean networks (BNs) is studied via model reduction and pinning control. Compared with existing literature, the pinning control design in this article is based on the original node&#39;s connection, but not on the state-transition matrix of BNs. Hence, the computational complexity is dramatically reduced in this article from O(2 n × 2 n ) to O(2 × 2&#39;), where n is the number of nodes in the large-dimensional BN and r &lt;; n is the largest number of in-neighbors of the reduced BN. Finally, the proposed method is well demonstrated by a T-LGL survival signaling network with 18 nodes and a model of survival signaling in large granular lymphocyte leukemia with 29 nodes. Just as shown in the simulations, the model reduction method reduces 99.98\% redundant states for the network with 18 nodes, and 99.99\% redundant states for the network with 29 nodes.},
  archive      = {J_TNNLS},
  author       = {Jie Zhong and Bowen Li and Yang Liu and Jianquan Lu and Weihua Gui},
  doi          = {10.1109/TNNLS.2020.2980632},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {1149-1161},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Steady-state design of large-dimensional boolean networks},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Data-driven iterative learning control for nonlinear
discrete-time MIMO systems. <em>TNNLS</em>, <em>32</em>(3), 1136–1148.
(<a href="https://doi.org/10.1109/TNNLS.2020.2980588">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article considers the tracking control of unknown nonlinear nonaffine repetitive discrete-time multi-input multi-output systems. Two data-driven iterative learning control (ILC) schemes are designed based on two equivalent dynamic linearization data models of an unknown ideal learning controller, which exists theoretically in the iteration domain. The two control schemes provide ways of selecting learning controllers based on the complexity of the controlled nonlinear systems. The learning control gain matrixes of the two learning controllers are optimized through the steepest descent method using only the measured input-output data of the nonlinear systems. The proposed ILC approaches are pure data-driven since no model information of the controlled systems is involved. The stability and convergence of the proposed ILC approaches are rigorously analyzed under reasonable conditions. Numerical simulation and an experiment based on a Gantry-type linear motor drive system are conducted to verify the effectiveness of the proposed data-driven ILC approaches.},
  archive      = {J_TNNLS},
  author       = {Xian Yu and Zhongsheng Hou and Marios M. Polycarpou and Li Duan},
  doi          = {10.1109/TNNLS.2020.2980588},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {1136-1148},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Data-driven iterative learning control for nonlinear discrete-time MIMO systems},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Regularizing hyperspectral and multispectral image fusion by
CNN denoiser. <em>TNNLS</em>, <em>32</em>(3), 1124–1135. (<a
href="https://doi.org/10.1109/TNNLS.2020.2980398">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperspectral image (HSI) and multispectral image (MSI) fusion, which fuses a low-spatial-resolution HSI (LR-HSI) with a higher resolution multispectral image (MSI), has become a common scheme to obtain high-resolution HSI (HR-HSI). This article presents a novel HSI and MSI fusion method (called as CNN-Fus), which is based on the subspace representation and convolutional neural network (CNN) denoiser, i.e., a well-trained CNN for gray image denoising. Our method only needs to train the CNN on the more accessible gray images and can be directly used for any HSI and MSI data sets without retraining. First, to exploit the high correlations among the spectral bands, we approximate the desired HR-HSI with the low-dimensional subspace multiplied by the coefficients, which can not only speed up the algorithm but also lead to more accurate recovery. Since the spectral information mainly exists in the LR-HSI, we learn the subspace from it via singular value decomposition. Due to the powerful learning performance and high speed of CNN, we use the well-trained CNN for gray image denoising to regularize the estimation of coefficients. Specifically, we plug the CNN denoiser into the alternating direction method of multipliers (ADMM) algorithm to estimate the coefficients. Experiments demonstrate that our method has superior performance over the state-of-the-art fusion methods.},
  archive      = {J_TNNLS},
  author       = {Renwei Dian and Shutao Li and Xudong Kang},
  doi          = {10.1109/TNNLS.2020.2980398},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {1124-1135},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Regularizing hyperspectral and multispectral image fusion by CNN denoiser},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Feature selection using a neural network with group lasso
regularization and controlled redundancy. <em>TNNLS</em>,
<em>32</em>(3), 1110–1123. (<a
href="https://doi.org/10.1109/TNNLS.2020.2980383">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a neural network-based feature selection (FS) scheme that can control the level of redundancy in the selected features by integrating two penalties into a single objective function. The Group Lasso penalty aims to produce sparsity in features in a grouped manner. The redundancy-control penalty, which is defined based on a measure of dependence among features, is utilized to control the level of redundancy among the selected features. Both the penalty terms involve the L 2,1 -norm of weight matrix between the input and hidden layers. These penalty terms are nonsmooth at the origin, and hence, one simple but efficient smoothing technique is employed to overcome this issue. The monotonicity and convergence of the proposed algorithm are specified and proved under suitable assumptions. Then, extensive experiments are conducted on both artificial and real data sets. Empirical results explicitly demonstrate the ability of the proposed FS scheme and its effectiveness in controlling redundancy. The empirical simulations are observed to be consistent with the theoretical results.},
  archive      = {J_TNNLS},
  author       = {Jian Wang and Huaqing Zhang and Junze Wang and Yifei Pu and Nikhil R. Pal},
  doi          = {10.1109/TNNLS.2020.2980383},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {1110-1123},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Feature selection using a neural network with group lasso regularization and controlled redundancy},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Shadow-cuts minimization/maximization and complex hopfield
neural networks. <em>TNNLS</em>, <em>32</em>(3), 1096–1109. (<a
href="https://doi.org/10.1109/TNNLS.2020.2980237">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we continue our very recent work by extending it to the complex case. Having been inspired by the real Hopfield neural network (HNN) results, our investigations here yield various novel results, some of which are as follows. First, extending the “biased pseudo-cut” concept to the complex HNN (CHNN) case, we introduce a “shadow-cut” that is defined as the sum of intercluster phased edges. Second, while the discrete-time real HNN strictly minimizes the “biased pseudo-cut” in each neuron state change, the CHNN “tends” to minimize the shadow-cut (as the CHNN energy function is minimized). Third, these definitions pose a novel L-phased graph clustering (partitioning) problem in which the sum of the shadow-cuts is minimized (or maximized) for the Hermitian complex and the directed graphs whose edges are (possibly arbitrary positive/negative) complex numbers. Finally, combining the CHNN and the pioneering algorithm GADIA of Babadi and Tarokh and their modified versions, we propose simple indirect algorithms to solve the defined shadow-cuts minimization/maximization problem. The proposed algorithms naturally include the CHNN as well as the GADIA as its special cases. The computer simulations confirm the findings.},
  archive      = {J_TNNLS},
  author       = {Zekeriya Uykan},
  doi          = {10.1109/TNNLS.2020.2980237},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {1096-1109},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Shadow-cuts Minimization/Maximization and complex hopfield neural networks},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Image set classification using a distance-based kernel over
affine grassmann manifold. <em>TNNLS</em>, <em>32</em>(3), 1082–1095.
(<a href="https://doi.org/10.1109/TNNLS.2020.2980059">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modeling image sets or videos as linear subspaces is quite popular for classification problems in machine learning. However, affine subspace modeling has not been explored much. In this article, we address the image sets classification problem by modeling them as affine subspaces. Affine subspaces are linear subspaces shifted from origin by an offset. The collection of the same dimensional affine subspaces of RD is known as affine Grassmann manifold (AGM) or affine Grassmannian that is a smooth and noncompact manifold. The non-Euclidean geometry of AGM and the nonunique representation of an affine subspace in AGM make the classification task in AGM difficult. In this article, we propose a novel affine subspace-based kernel that maps the points in AGM to a finite-dimensional Hilbert space. For this, we embed the AGM in a higher dimensional Grassmann manifold (GM) by embedding the offset vector in the Stiefel coordinates. The projection distance between two points in AGM is the measure of similarity obtained by the kernel function. The obtained kernel-gram matrix is further diagonalized to generate low-dimensional features in the Euclidean space corresponding to the points in AGM. Distance-preserving constraint along with sparsity constraint is used for minimum residual error classification by keeping the locally Euclidean structure of AGM in mind. Experimentation performed over four data sets for gait, object, hand, and body gesture recognition shows promising results compared with state-of-the-art techniques.},
  archive      = {J_TNNLS},
  author       = {Krishan Sharma and Renu Rameshan},
  doi          = {10.1109/TNNLS.2020.2980059},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {1082-1095},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Image set classification using a distance-based kernel over affine grassmann manifold},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A learning framework for n-bit quantized neural networks
toward FPGAs. <em>TNNLS</em>, <em>32</em>(3), 1067–1081. (<a
href="https://doi.org/10.1109/TNNLS.2020.2980041">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The quantized neural network (QNN) is an efficient approach for network compression and can be widely used in the implementation of field-programmable gate arrays (FPGAs). This article proposes a novel learning framework for n-bit QNNs, whose weights are constrained to the power of two. To solve the gradient vanishing problem, we propose a reconstructed gradient function for QNNs in the back-propagation algorithm that can directly get the real gradient rather than estimating an approximate gradient of the expected loss. We also propose a novel QNN structure named n-BQ-NN, which uses shift operation to replace the multiply operation and is more suitable for the inference on FPGAs. Furthermore, we also design a shift vector processing element (SVPE) array to replace all 16-bit multiplications with SHIFT operations in convolution operation on FPGAs. We also carry out comparable experiments to evaluate our framework. The experimental results show that the quantized models of ResNet, DenseNet, and AlexNet through our learning framework can achieve almost the same accuracies with the original fullprecision models. Moreover, when using our learning framework to train our n-BQ-NN from scratch, it can achieve state-of-the-art results compared with typical low-precision QNNs. Experiments on Xilinx ZCU102 platform show that our n-BQ-NN with our SVPE can execute 2.9 times faster than that with the vector processing element (VPE) in inference. As the SHIFT operation in our SVPE array will not consume digital signal processing (DSP) resources on FPGAs, the experiments have shown that the use of SVPE array also reduces average energy consumption to 68.7\% of the VPE array with 16 bit.},
  archive      = {J_TNNLS},
  author       = {Jun Chen and Liang Liu and Yong Liu and Xianfang Zeng},
  doi          = {10.1109/TNNLS.2020.2980041},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {1067-1081},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A learning framework for n-bit quantized neural networks toward FPGAs},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Mutual-collision-avoidance scheme synthesized by neural
networks for dual redundant robot manipulators executing cooperative
tasks. <em>TNNLS</em>, <em>32</em>(3), 1052–1066. (<a
href="https://doi.org/10.1109/TNNLS.2020.2980038">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Collision between dual robot manipulators during working process will lead to task failure and even robot damage. To avoid mutual collision of dual robot manipulators while doing collaboration tasks, a novel recurrent neural network (RNN)-based mutual-collision-avoidance (MCA) scheme for solving the motion planning problem of dual manipulators is proposed and exploited. Because of the high accuracy and low computation complexity, the linear variational inequality-based primal-dual neural network is used to solve the proposed scheme. The proposed scheme is applied to the collaboration trajectory tracking and cup-stacking tasks, and shows its effectiveness for avoiding collision between the dual robot manipulators. Through network iteration and online learning, the dual robot manipulators will learn the ability of MCA. Moreover, a line-segment-based distance measure algorithm is proposed to calculate the minimum distance between the dual manipulators. If the computed minimum distance is less than the first safe-related distance threshold, a speed brake operation is executed and guarantees that the robot cannot exceed the second safe-related distance threshold. Furthermore, the proposed MCA strategy is formulated as a standard quadratic programming problem, which is further solved by an RNN. Computer simulations and a real dual robot experiment further verify the effectiveness, accuracy, and physical realizability of the RNN-based MCA scheme when manipulators cooperatively execute the end-effector tasks.},
  archive      = {J_TNNLS},
  author       = {Zhijun Zhang and Lunan Zheng and Zhuoming Chen and Lingdong Kong and Hamid Reza Karimi},
  doi          = {10.1109/TNNLS.2020.2980038},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {1052-1066},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Mutual-collision-avoidance scheme synthesized by neural networks for dual redundant robot manipulators executing cooperative tasks},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Correlated parameters to accurately measure uncertainty in
deep neural networks. <em>TNNLS</em>, <em>32</em>(3), 1037–1051. (<a
href="https://doi.org/10.1109/TNNLS.2020.2980004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, a novel approach for training deep neural networks using Bayesian techniques is presented. The Bayesian methodology allows for an easy evaluation of model uncertainty and, additionally, is robust to overfitting. These are commonly the two main problems classical, i.e., non-Bayesian architectures have to struggle with. The proposed approach applies variational inference in order to approximate the intractable posterior distribution. In particular, the variational distribution is defined as the product of multiple multivariate normal distributions with tridiagonal covariance matrices. Every single normal distribution belongs either to the weights or to the biases corresponding to one network layer. The layerwise a posteriori variances are defined based on the corresponding expectation values, and furthermore, the correlations are assumed to be identical. Therefore, only a few additional parameters need to be optimized compared with non-Bayesian settings. The performance of the new approach is evaluated and compared with other recently developed Bayesian methods. Basis of the performance evaluations are the popular benchmark data sets MNIST and CIFAR-10. Among the considered approaches, the proposed one shows the best predictive accuracy. Moreover, extensive evaluations of the provided prediction uncertainty information indicate that the new approach often yields more useful uncertainty estimates than the comparison methods.},
  archive      = {J_TNNLS},
  author       = {Konstantin Posch and Juergen Pilz},
  doi          = {10.1109/TNNLS.2020.2980004},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {1037-1051},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Correlated parameters to accurately measure uncertainty in deep neural networks},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Mining maximal dynamic spatial colocation patterns.
<em>TNNLS</em>, <em>32</em>(3), 1026–1036. (<a
href="https://doi.org/10.1109/TNNLS.2020.2979875">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A spatial colocation pattern represents a subset of spatial features with instances that are prevalently located together in a geographic space. Although many algorithms for mining spatial colocation patterns have been proposed, the following problems still remain. these methods miss certain meaningful patterns (e.g., {Ganoderma_lucidumnew, maple_treedead} and {water_hyacinthnew(increase), algaedead(decrease)}) and obtain a wrong conclusion if the instances of two or more features increase/decrease (i.e., new/dead) in the same/approximate proportion, which has no effect on the prevalent patterns; and the efficiency of existing methods is low in mining prevalent spatial colocation patterns, because the number of prevalent spatial colocation patterns is quite large. Therefore, we first propose the concept of a dynamic spatial colocation pattern that can reflect the dynamic relationships among spatial features. Second, we mine a small number of prevalent maximal dynamic spatial colocation patterns that can derive all prevalent dynamic spatial colocation patterns, which can improve the efficiency of obtaining all prevalent dynamic spatial colocation patterns. Third, we propose an algorithm for mining prevalent maximal dynamic spatial colocation patterns and two pruning strategies. Finally, the effectiveness and efficiency of the proposed method and the pruning strategies are verified by extensive experiments over real/synthetic data sets.},
  archive      = {J_TNNLS},
  author       = {Xin Hu and Guoyin Wang and Jiangli Duan},
  doi          = {10.1109/TNNLS.2020.2979875},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {1026-1036},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Mining maximal dynamic spatial colocation patterns},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multicluster class-balanced ensemble. <em>TNNLS</em>,
<em>32</em>(3), 1014–1025. (<a
href="https://doi.org/10.1109/TNNLS.2020.2979839">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ensemble classifiers using clustering have significantly improved classification and prediction accuracies of many systems. These types of ensemble approaches create multiple clusters to train the base classifiers. However, the problem with this is that each class might have many clusters and each cluster might have different number of samples, so an ensemble decision based on large number of clusters and different number of samples per class within a cluster produces biased and inaccurate results. Therefore, in this article, we propose a novel methodology to create an appropriate number of strong data clusters for each class and then balance them. Furthermore, an ensemble framework is proposed with base classifiers trained on strong and balanced data clusters. The proposed approach is implemented and evaluated on 24 benchmark data sets from the University of California Irvine (UCI) machine learning repository. An analysis of results using the proposed approach and the existing state-of-the-art ensemble classifier approaches is conducted and presented. A significance test is conducted to further validate the efficacy of the results and a detailed analysis is presented.},
  archive      = {J_TNNLS},
  author       = {Zohaib Jan and Brijesh Verma},
  doi          = {10.1109/TNNLS.2020.2979839},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {1014-1025},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multicluster class-balanced ensemble},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust one-class kernel spectral regression. <em>TNNLS</em>,
<em>32</em>(3), 999–1013. (<a
href="https://doi.org/10.1109/TNNLS.2020.2979823">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The kernel null-space technique is known to be an effective one-class classification (OCC) technique. Nevertheless, the applicability of this method is limited due to its susceptibility to possible training data corruption and the inability to rank training observations according to their conformity with the model. This article addresses these shortcomings by regularizing the solution of the null-space kernel Fisher methodology in the context of its regression-based formulation. In this respect, first, the effect of the Tikhonov regularization in the Hilbert space is analyzed, where the one-class learning problem in the presence of contamination in the training set is posed as a sensitivity analysis problem. Next, the effect of the sparsity of the solution is studied. For both alternative regularization schemes, iterative algorithms are proposed which recursively update label confidences. Through extensive experiments, the proposed methodology is found to enhance robustness against contamination in the training set compared with the baseline kernel null-space method, as well as other existing approaches in the OCC paradigm, while providing the functionality to rank training samples effectively.},
  archive      = {J_TNNLS},
  author       = {Shervin Rahimzadeh Arashloo and Josef Kittler},
  doi          = {10.1109/TNNLS.2020.2979823},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {999-1013},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Robust one-class kernel spectral regression},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DMGAN: Adversarial learning-based decision making for
human-level plant-wide operation of process industries under
uncertainties. <em>TNNLS</em>, <em>32</em>(3), 985–998. (<a
href="https://doi.org/10.1109/TNNLS.2020.2979800">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To achieve plant-wide operational optimization and dynamic adjustment of operational index for an industrial process, knowledge-based methods have been widely employed over the past years. However, the extraction of knowledge base is a bottleneck for most existing approaches. To address this problem, we propose a novel framework based on the generative adversarial networks (GANs), termed as decision-making GAN (DMGAN), which directly learns from operational data and performs human-level decision making of the operational indices for plant-wide operation. In the proposed DMGAN, two adversarial criteria and three cycle consistency criteria are incorporated to encourage efficient posterior inference. To improve the generalization power of a generator with an increasing complexity of the industrial processes, a reinforced U-Net (RU-Net) is presented that improves the traditional U-Net by providing a more general combinator, a building block design, and drop-level regularization. In this article, we also propose three quantitative metrics for assessing the plant-wide operation performance. A case study based on the largest mineral processing factory in Western China is carried out, and the experimental results demonstrate the promising performance of the proposed DMGAN when compared with decision-making based on domain experts.},
  archive      = {J_TNNLS},
  author       = {Nianzu Zheng and Jinliang Ding and Tianyou Chai},
  doi          = {10.1109/TNNLS.2020.2979800},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {985-998},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {DMGAN: Adversarial learning-based decision making for human-level plant-wide operation of process industries under uncertainties},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dissipativity analysis for neural networks with time-varying
delays via a delay-product-type lyapunov functional approach.
<em>TNNLS</em>, <em>32</em>(3), 975–984. (<a
href="https://doi.org/10.1109/TNNLS.2020.2979778">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article is concerned with the problem of dissipativity and stability analysis for a class of neural networks (NNs) with time-varying delays. First, a new augmented Lyapunov- Krasovskii functional (LKF), including some delay-product-type terms, is proposed, in which the information on time-varying delay and system states is taken into full consideration. Second, by employing a generalized free-matrix-based inequality and its simplified version to estimate the derivative of the proposed LKF, some improved delay-dependent conditions are derived to ensure that the considered NNs are strictly (Q, S, R)-γ-dissipative. Furthermore, the obtained results are applied to passivity and stability analysis of delayed NNs. Finally, two numerical examples and a real-world problem in the quadruple tank process are carried out to illustrate the effectiveness of the proposed method.},
  archive      = {J_TNNLS},
  author       = {Hong-Hai Lian and Shen-Ping Xiao and Huaicheng Yan and Fuwen Yang and Hong-Bing Zeng},
  doi          = {10.1109/TNNLS.2020.2979778},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {975-984},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Dissipativity analysis for neural networks with time-varying delays via a delay-product-type lyapunov functional approach},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). LAGC: Lazily aggregated gradient coding for
straggler-tolerant and communication-efficient distributed learning.
<em>TNNLS</em>, <em>32</em>(3), 962–974. (<a
href="https://doi.org/10.1109/TNNLS.2020.2979762">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gradient-based distributed learning in parameter server (PS) computing architectures is subject to random delays due to straggling worker nodes and to possible communication bottlenecks between PS and workers. Solutions have been recently proposed to separately address these impairments based on the ideas of gradient coding (GC), worker grouping, and adaptive worker selection. This article provides a unified analysis of these techniques in terms of wall-clock time, communication, and computation complexity measures. Furthermore, in order to combine the benefits of GC and grouping in terms of robustness to stragglers with the communication and computation load gains of adaptive selection, novel strategies, named lazily aggregated GC (LAGC) and grouped-LAG (G-LAG), are introduced. Analysis and results show that G-LAG provides the best wall-clock time and communication performance while maintaining a low computational cost, for two representative distributions of the computing times of the worker nodes.},
  archive      = {J_TNNLS},
  author       = {Jingjing Zhang and Osvaldo Simeone},
  doi          = {10.1109/TNNLS.2020.2979762},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {962-974},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {LAGC: Lazily aggregated gradient coding for straggler-tolerant and communication-efficient distributed learning},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Twin-incoherent self-expressive locality-adaptive latent
dictionary pair learning for classification. <em>TNNLS</em>,
<em>32</em>(3), 947–961. (<a
href="https://doi.org/10.1109/TNNLS.2020.2979748">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The projective dictionary pair learning (DPL) model jointly seeks a synthesis dictionary and an analysis dictionary by extracting the block-diagonal coefficients with an incoherence-constrained analysis dictionary. However, DPL fails to discover the underlying subspaces and salient features at the same time, and it cannot encode the neighborhood information of the embedded coding coefficients, especially adaptively. In addition, although the data can be well reconstructed via the minimization of the reconstruction error, useful distinguishing salient feature information may be lost and incorporated into the noise term. In this article, we propose a novel self-expressive adaptive locality-preserving framework: twin-incoherent self-expressive latent DPL (SLatDPL). To capture the salient features from the samples, SLatDPL minimizes a latent reconstruction error by integrating the coefficient learning and salient feature extraction into a unified model, which can also be used to simultaneously discover the underlying subspaces and salient features. To make the coefficients block diagonal and ensure that the salient features are discriminative, our SLatDPL regularizes them by imposing a twin-incoherence constraint. Moreover, SLatDPL utilizes a self-expressive adaptive weighting strategy that uses normalized block-diagonal coefficients to preserve the locality of the codes and salient features. SLatDPL can use the class-specific reconstruction residual to handle new data directly. Extensive simulations on several public databases demonstrate the satisfactory performance of our SLatDPL compared with related methods.},
  archive      = {J_TNNLS},
  author       = {Zhao Zhang and Yulin Sun and Yang Wang and Zheng Zhang and Haijun Zhang and Guangcan Liu and Meng Wang},
  doi          = {10.1109/TNNLS.2020.2979748},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {947-961},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Twin-incoherent self-expressive locality-adaptive latent dictionary pair learning for classification},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Anam-net: Anamorphic depth embedding-based lightweight CNN
for segmentation of anomalies in COVID-19 chest CT images.
<em>TNNLS</em>, <em>32</em>(3), 932–946. (<a
href="https://doi.org/10.1109/TNNLS.2021.3054746">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Chest computed tomography (CT) imaging has become indispensable for staging and managing coronavirus disease 2019 (COVID-19), and current evaluation of anomalies/abnormalities associated with COVID-19 has been performed majorly by the visual score. The development of automated methods for quantifying COVID-19 abnormalities in these CT images is invaluable to clinicians. The hallmark of COVID-19 in chest CT images is the presence of ground-glass opacities in the lung region, which are tedious to segment manually. We propose anamorphic depth embedding-based lightweight CNN, called Anam-Net, to segment anomalies in COVID-19 chest CT images. The proposed Anam-Net has 7.8 times fewer parameters compared to the state-of-the-art UNet (or its variants), making it lightweight capable of providing inferences in mobile or resource constraint (point-of-care) platforms. The results from chest CT images (test cases) across different experiments showed that the proposed method could provide good Dice similarity scores for abnormal and normal regions in the lung. We have benchmarked Anam-Net with other state-of-the-art architectures, such as ENet, LEDNet, UNet++, SegNet, Attention UNet, and DeepLabV3+. The proposed Anam-Net was also deployed on embedded systems, such as Raspberry Pi 4, NVIDIA Jetson Xavier, and mobile-based Android application (CovSeg) embedded with Anam-Net to demonstrate its suitability for point-of-care platforms. The generated codes, models, and the mobile application are available for enthusiastic users at https://github.com/NaveenPaluru/Segmentation-COVID-19.},
  archive      = {J_TNNLS},
  author       = {Naveen Paluru and Aveen Dayal and Håvard Bjørke Jenssen and Tomas Sakinis and Linga Reddy Cenkeramaddi and Jaya Prakash and Phaneendra K. Yalavarthy},
  doi          = {10.1109/TNNLS.2021.3054746},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {932-946},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Anam-net: Anamorphic depth embedding-based lightweight CNN for segmentation of anomalies in COVID-19 chest CT images},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021k). IEEE computational intelligence society information.
<em>TNNLS</em>, <em>32</em>(2), C3. (<a
href="https://doi.org/10.1109/TNNLS.2021.3052670">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Provides a listing of current committee members and society officers.},
  archive      = {J_TNNLS},
  doi          = {10.1109/TNNLS.2021.3052670},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {C3},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {IEEE computational intelligence society information},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hypersphere-based weight imprinting for few-shot learning on
embedded devices. <em>TNNLS</em>, <em>32</em>(2), 925–930. (<a
href="https://doi.org/10.1109/TNNLS.2020.2979745">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weight imprinting (WI) was recently introduced as a way to perform gradient descent-free few-shot learning. Due to this, WI was almost immediately adapted for performing few-shot learning on embedded neural network accelerators that do not support back-propagation, e.g., edge tensor processing units. However, WI suffers from many limitations, e.g., it cannot handle novel categories with multimodal distributions and special care should be given to avoid overfitting the learned embeddings on the training classes since this can have a devastating effect on classification accuracy (for the novel categories). In this article, we propose a novel hypersphere-based WI approach that is capable of training neural networks in a regularized, imprinting-aware way effectively overcoming the aforementioned limitations. The effectiveness of the proposed method is demonstrated using extensive experiments on three image data sets.},
  archive      = {J_TNNLS},
  author       = {Nikolaos Passalis and Alexandros Iosifidis and Moncef Gabbouj and Anastasios Tefas},
  doi          = {10.1109/TNNLS.2020.2979745},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {925-930},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Hypersphere-based weight imprinting for few-shot learning on embedded devices},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Enhancing function approximation abilities of neural
networks by training derivatives. <em>TNNLS</em>, <em>32</em>(2),
916–924. (<a href="https://doi.org/10.1109/TNNLS.2020.2979706">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A method to increase the precision of feedforward networks is proposed. It requires prior knowledge of a target&#39;s function derivatives of several orders and uses this information in gradient-based training. Forward pass calculates not only the values of the output layer of a network but also their derivatives. The deviations of those derivatives from the target ones are used in an extended cost function, and then, the backward pass calculates the gradient of the extended cost with respect to weights, which is then used by a weights update algorithm. The most accurate approximation is obtained when the training starts with all available derivatives that are then step by step excluded from the extended cost function, starting with the highest orders up until only values are trained. Despite a substantial increase in arithmetic operations per pattern (compared with the conventional training), the method allows to obtain 140-1000 times more accurate approximation for simple cases if the total number of operations is equal. This precision also happens to be out of reach for the regular cost function. The method works well for solving differential equations with neural networks. The cost function is the deviation of the equation&#39;s residual from zero, and it can be extended by differentiating the equation itself so no prior information is required. This extension allows to solve 2-D nonlinear partial differential equation 13 times more accurately using seven times fewer grid points. The GPU-efficient algorithm for calculating the gradient of the extended cost function is proposed.},
  archive      = {J_TNNLS},
  author       = {V. I. Avrutskiy},
  doi          = {10.1109/TNNLS.2020.2979706},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {916-924},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Enhancing function approximation abilities of neural networks by training derivatives},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Scene graph generation with hierarchical context.
<em>TNNLS</em>, <em>32</em>(2), 909–915. (<a
href="https://doi.org/10.1109/TNNLS.2020.2979270">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scene graph generation has received increasing attention in recent years. Enhancing the predicate representations is an important entry point to this task. There are various methods to fully investigate the context of representation enhancement. In this brief, we analyze the decisive factors that can significantly affect the relation detection results. Our analysis shows that spatial correlations between objects, focused regions of objects, and global hints related to the relations have strong influences in relation prediction and contradiction elimination. Based on our analysis, we propose a hierarchical context network (HCNet) to generate a scene graph. HCNet consists of three contexts, including interaction context, depression context, and global context, which integrates information from pair, object, and graph levels. The experiments show that our method outperforms the state-of-the-art methods on the Visual Genome (VG) data set.},
  archive      = {J_TNNLS},
  author       = {Guanghui Ren and Lejian Ren and Yue Liao and Si Liu and Bo Li and Jizhong Han and Shuicheng Yan},
  doi          = {10.1109/TNNLS.2020.2979270},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {909-915},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Scene graph generation with hierarchical context},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Quaternion projection rule for rotor hopfield neural
networks. <em>TNNLS</em>, <em>32</em>(2), 900–908. (<a
href="https://doi.org/10.1109/TNNLS.2020.2979920">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A rotor Hopfield neural network (RHNN) is an extension of a complex-valued Hopfield neural network (CHNN) and has excellent noise tolerance. The RHNN decomposition theorem says that an RHNN decomposes into a CHNN and a symmetric CHNN. For a large number of training patterns, the projection rule for RHNNs generates large self-feedbacks, which deteriorates the noise tolerance. To remove self-feedbacks, we propose a projection rule using quaternions based on the decomposition theorem. Using computer simulations, we show that the quaternion projection rule improves noise tolerance.},
  archive      = {J_TNNLS},
  author       = {Masaki Kobayashi},
  doi          = {10.1109/TNNLS.2020.2979920},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {900-908},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Quaternion projection rule for rotor hopfield neural networks},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Quaternion-valued twin-multistate hopfield neural networks
with dual connections. <em>TNNLS</em>, <em>32</em>(2), 892–899. (<a
href="https://doi.org/10.1109/TNNLS.2020.2979904">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dual connections (DCs) utilize the noncommutativity of quaternions and improve the noise tolerance of quaternion Hopfield neural networks (QHNNs). In this article, we introduce DCs to twin-multistate QHNNs. We conduct computer simulations to investigate the noise tolerance. The QHNNs with DCs were weak against an increase in the number of training patterns, but they were robust against increased resolution factor. The simulation results can be explained from the standpoints of storage capacities and rotational invariance.},
  archive      = {J_TNNLS},
  author       = {Masaki Kobayashi},
  doi          = {10.1109/TNNLS.2020.2979904},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {892-899},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Quaternion-valued twin-multistate hopfield neural networks with dual connections},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Convergence analysis of adaptive exponential functional link
network. <em>TNNLS</em>, <em>32</em>(2), 882–891. (<a
href="https://doi.org/10.1109/TNNLS.2020.2979688">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The adaptive exponential functional link network (AEFLN) is a recently introduced novel linear-in-the-parameters nonlinear filter and is used in numerous nonlinear applications, including system identification, active noise control, and echo cancellation. The improved modeling accuracy offered by AEFLN for different nonlinear applications can be attributed to the exponentially varying sinusoidal basis functions used for nonlinear expansion. Even though AEFLN has been widely used for the identification of nonlinear systems, no theoretical analysis of AEFLN is available in the literature. Hence, in this article, a theoretical performance analysis of AEFLN trained using an adaptive exponential least mean square (AELMS) algorithm under the Gaussian input assumption is discussed. Expressions describing the mean as well as mean square behavior of the weight vector and adaptive exponential parameter are derived. Computer simulations are carried out, and the derived theoretical expressions show a close correspondence with simulation results.},
  archive      = {J_TNNLS},
  author       = {Vinal Patel and Sankha Subhra Bhattacharjee and Nithin V. George},
  doi          = {10.1109/TNNLS.2020.2979688},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {882-891},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Convergence analysis of adaptive exponential functional link network},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust kernelized multiview self-representation for subspace
clustering. <em>TNNLS</em>, <em>32</em>(2), 868–881. (<a
href="https://doi.org/10.1109/TNNLS.2020.2979685">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose a multiview self-representation model for nonlinear subspaces clustering. By assuming that the heterogeneous features lie within the union of multiple linear subspaces, the recent multiview subspace learning methods aim to capture the complementary and consensus from multiple views to boost the performance. However, in real-world applications, data feature usually resides in multiple nonlinear subspaces, leading to undesirable results. To this end, we propose a kernelized version of tensor-based multiview subspace clustering, which is referred to as Kt-SVD-MSC, to jointly learn self-representation coefficients in mapped high-dimensional spaces and multiple views correlation in unified tensor space. In view-specific feature space, a kernel-induced mapping is introduced for each view to ensure the separability of self-representation coefficients. In unified tensor space, a new kind of tensor low-rank regularizer is employed on the rotated self-representation coefficient tensor to preserve the global consistency across different views. We also derive an algorithm to efficiently solve the optimization problem with all the subproblems having closed-form solutions. Furthermore, by incorporating the nonnegative and sparsity constraints, the proposed method can be easily extended to a useful variant, meaning that several useful variants can be easily constructed in a similar way. Extensive experiments of the proposed method are tested on eight challenging data sets, in which a significant (even a breakthrough) advance over state-of-the-art multiview clustering is achieved.},
  archive      = {J_TNNLS},
  author       = {Yuan Xie and Jinyan Liu and Yanyun Qu and Dacheng Tao and Wensheng Zhang and Longquan Dai and Lizhuang Ma},
  doi          = {10.1109/TNNLS.2020.2979685},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {868-881},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Robust kernelized multiview self-representation for subspace clustering},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Probabilistic semi-supervised learning via sparse graph
structure learning. <em>TNNLS</em>, <em>32</em>(2), 853–867. (<a
href="https://doi.org/10.1109/TNNLS.2020.2979607">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a probabilistic semi-supervised learning (SSL) framework based on sparse graph structure learning. Different from existing SSL methods with either a predefined weighted graph heuristically constructed from the input data or a learned graph based on the locally linear embedding assumption, the proposed SSL model is capable of learning a sparse weighted graph from the unlabeled high-dimensional data and a small amount of labeled data, as well as dealing with the noise of the input data. Our representation of the weighted graph is indirectly derived from a unified model of density estimation and pairwise distance preservation in terms of various distance measurements, where latent embeddings are assumed to be random variables following an unknown density function to be learned, and pairwise distances are then calculated as the expectations over the density for the model robustness to the data noise. Moreover, the labeled data based on the same distance representations are leveraged to guide the estimated density for better class separation and sparse graph structure learning. A simple inference approach for the embeddings of unlabeled data based on point estimation and kernel representation is presented. Extensive experiments on various data sets show promising results in the setting of SSL compared with many existing methods and significant improvements on small amounts of labeled data.},
  archive      = {J_TNNLS},
  author       = {Li Wang and Raymond Chan and Tieyong Zeng},
  doi          = {10.1109/TNNLS.2020.2979607},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {853-867},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Probabilistic semi-supervised learning via sparse graph structure learning},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A vibration control method for hybrid-structured flexible
manipulator based on sliding mode control and reinforcement learning.
<em>TNNLS</em>, <em>32</em>(2), 841–852. (<a
href="https://doi.org/10.1109/TNNLS.2020.2979600">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The hybrid-structured flexible manipulator has a complex structure and strong coupling between state variables. Meanwhile, the natural frequency of the hybrid-structured flexible manipulator varies with the motion of the telescopic joint, so it is difficult to suppress the vibration quickly. In this article, the tip state signal of the hybrid-structured flexible manipulator is decomposed into elastic vibration signal and tip vibration equilibrium position signal, and a combined control method is proposed to improve tip positioning accuracy and trajectory tracking accuracy. In the proposed combined control method, an improved nominal model-based sliding mode controller (NMBSMC) is used as the main controller to output the driving torque, and an actor-critic-based reinforcement learning controller (ACBRLC) is used as an auxiliary controller to output small compensation torque. The improved NMBSMC can be divided into a nominal model-based sliding mode robust controller and a practical model-based integral sliding mode controller. Two sliding mode controllers with different structures make full use of the mathematical model and the measured data of the actual system to improve the vibration equilibrium position tracking accuracy. The ACBRLC uses the tip elastic vibration signal and the prioritized experience replay method to obtain the small reverse compensation torque, which is superimposed with the output of the NMBSMC to suppress tip vibration and improve the positioning accuracy of the hybrid-structured flexible manipulator. Finally, several groups of experiments are designed to verify the effectiveness and robustness of the proposed combined control method.},
  archive      = {J_TNNLS},
  author       = {Teng Long and En Li and Yunqing Hu and Lei Yang and Junfeng Fan and Zize Liang and Rui Guo},
  doi          = {10.1109/TNNLS.2020.2979600},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {841-852},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A vibration control method for hybrid-structured flexible manipulator based on sliding mode control and reinforcement learning},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning convolutional sparse coding on complex domain for
interferometric phase restoration. <em>TNNLS</em>, <em>32</em>(2),
826–840. (<a href="https://doi.org/10.1109/TNNLS.2020.2979546">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interferometric phase restoration has been investigated for decades and most of the state-of-the-art methods have achieved promising performances for InSAR phase restoration. These methods generally follow the nonlocal filtering processing chain, aiming at circumventing the staircase effect and preserving the details of phase variations. In this article, we propose an alternative approach for InSAR phase restoration, that is, Complex Convolutional Sparse Coding (ComCSC) and its gradient regularized version. To the best of the authors&#39; knowledge, this is the first time that we solve the InSAR phase restoration problem in a deconvolutional fashion. The proposed methods can not only suppress interferometric phase noise, but also avoid the staircase effect and preserve the details. Furthermore, they provide an insight into the elementary phase components for the interferometric phases. The experimental results on synthetic and realistic high- and medium-resolution data sets from TerraSAR-X StripMap and Sentinel-1 interferometric wide swath mode, respectively, show that our method outperforms those previous state-of-the-art methods based on nonlocal InSAR filters, particularly the state-of-the-art method: InSAR-BM3D. The source code of this article will be made publicly available for reproducible research inside the community.},
  archive      = {J_TNNLS},
  author       = {Jian Kang and Danfeng Hong and Jialin Liu and Gerald Baier and Naoto Yokoya and Begüm Demir},
  doi          = {10.1109/TNNLS.2020.2979546},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {826-840},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Learning convolutional sparse coding on complex domain for interferometric phase restoration},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multiview concept learning via deep matrix factorization.
<em>TNNLS</em>, <em>32</em>(2), 814–825. (<a
href="https://doi.org/10.1109/TNNLS.2020.2979532">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiview representation learning (MVRL) leverages information from multiple views to obtain a common representation summarizing the consistency and complementarity in multiview data. Most previous matrix factorization-based MVRL methods are shallow models that neglect the complex hierarchical information. The recently proposed deep multiview factorization models cannot explicitly capture consistency and complementarity in multiview data. We present the deep multiview concept learning (DMCL) method, which hierarchically factorizes the multiview data, and tries to explicitly model consistent and complementary information and capture semantic structures at the highest abstraction level. We explore two variants of the DMCL framework, DMCL-L and DMCL-N, with respectively linear/nonlinear transformations between adjacent layers. We propose two block coordinate descent-based optimization methods for DMCL-L and DMCL-N. We verify the effectiveness of DMCL on three real-world data sets for both clustering and classification tasks.},
  archive      = {J_TNNLS},
  author       = {Wei Zhao and Cai Xu and Ziyu Guan and Ying Liu},
  doi          = {10.1109/TNNLS.2020.2979532},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {814-825},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multiview concept learning via deep matrix factorization},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dynamical channel pruning by conditional accuracy change for
deep neural networks. <em>TNNLS</em>, <em>32</em>(2), 799–813. (<a
href="https://doi.org/10.1109/TNNLS.2020.2979517">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Channel pruning is an effective technique that has been widely applied to deep neural network compression. However, many existing methods prune from a pretrained model, thus resulting in repetitious pruning and fine-tuning processes. In this article, we propose a dynamical channel pruning method, which prunes unimportant channels at the early stage of training. Rather than utilizing some indirect criteria (e.g., weight norm, absolute weight sum, and reconstruction error) to guide connection or channel pruning, we design criteria directly related to the final accuracy of a network to evaluate the importance of each channel. Specifically, a channelwise gate is designed to randomly enable or disable each channel so that the conditional accuracy changes (CACs) can be estimated under the condition of each channel disabled. Practically, we construct two effective and efficient criteria to dynamically estimate CAC at each iteration of training; thus, unimportant channels can be gradually pruned during the training process. Finally, extensive experiments on multiple data sets (i.e., ImageNet, CIFAR, and MNIST) with various networks (i.e., ResNet, VGG, and MLP) demonstrate that the proposed method effectively reduces the parameters and computations of baseline network while yielding the higher or competitive accuracy. Interestingly, if we Double the initial Channels and then Prune Half (DCPH) of them to baseline’s counterpart, it can enjoy a remarkable performance improvement by shaping a more desirable structure.},
  archive      = {J_TNNLS},
  author       = {Zhiqiang Chen and Ting-Bing Xu and Changde Du and Cheng-Lin Liu and Huiguang He},
  doi          = {10.1109/TNNLS.2020.2979517},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {799-813},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Dynamical channel pruning by conditional accuracy change for deep neural networks},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Noniterative sparse LS-SVM based on globally representative
point selection. <em>TNNLS</em>, <em>32</em>(2), 788–798. (<a
href="https://doi.org/10.1109/TNNLS.2020.2979466">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A least squares support vector machine (LS-SVM) offers performance comparable to that of SVMs for classification and regression. The main limitation of LS-SVM is that it lacks sparsity compared with SVMs, making LS-SVM unsuitable for handling large-scale data due to computation and memory costs. To obtain sparse LS-SVM, several pruning methods based on an iterative strategy were recently proposed but did not consider the quantity constraint on the number of reserved support vectors, as widely used in real-life applications. In this article, a noniterative algorithm is proposed based on the selection of globally representative points (global-representation-based sparse least squares support vector machine, GRS-LSSVM) to improve the performance of sparse LS-SVM. For the first time, we present a model of sparse LS-SVM with a quantity constraint. In solving the optimal solution of the model, we find that using globally representative points to construct the reserved support vector set produces a better solution than other methods. We design an indicator based on point density and point dispersion to evaluate the global representation of points in feature space. Using the indicator, the top globally representative points are selected in one step from all points to construct the reserved support vector set of sparse LS-SVM. After obtaining the set, the decision hyperplane of sparse LS-SVM is directly computed using an algebraic formula. This algorithm only consumes O(N2) in computational complexity and O(N) in memory cost which makes it suitable for large-scale data sets. The experimental results show that the proposed algorithm has higher sparsity, greater stability, and lower computational complexity than the traditional iterative algorithms.},
  archive      = {J_TNNLS},
  author       = {Yuefeng Ma and Xun Liang and Gang Sheng and James T. Kwok and Maoli Wang and Guangshun Li},
  doi          = {10.1109/TNNLS.2020.2979466},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {788-798},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Noniterative sparse LS-SVM based on globally representative point selection},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust functional manifold clustering. <em>TNNLS</em>,
<em>32</em>(2), 777–787. (<a
href="https://doi.org/10.1109/TNNLS.2020.2979444">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In machine learning, it is common to interpret each data sample as a multivariate vector disregarding the correlations among covariates. However, the data may actually be functional, i.e., each data point is a function of some variable, such as time, and the function is discretely sampled. The naive treatment of functional data as traditional multivariate data can lead to poor performance due to the correlations. In this article, we focus on subspace clustering for functional data or curves and propose a new method robust to shift and rotation. The idea is to define a function or curve and all its versions generated by shift and rotation as an equivalent class and then to find the subspace structure among all equivalent classes as the surrogate for all curves. Experimental evaluation on synthetic and real data reveals that this method massively outperforms prior clustering methods in both speed and accuracy when clustering functional data.},
  archive      = {J_TNNLS},
  author       = {Yi Guo and Stephen Tierney and Junbin Gao},
  doi          = {10.1109/TNNLS.2020.2979444},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {777-787},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Robust functional manifold clustering},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptive neural control for a class of nonlinear multiagent
systems. <em>TNNLS</em>, <em>32</em>(2), 763–776. (<a
href="https://doi.org/10.1109/TNNLS.2020.2979266">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article studies the adaptive neural controller design for a class of uncertain multiagent systems described by ordinary differential equations (ODEs) and beams. Three kinds of agent models are considered in this study, i.e., beams, nonlinear ODEs, and coupled ODE and beams. Both beams and ODEs contain completely unknown nonlinearities. Moreover, the control signals are assumed to suffer from a class of generalized backlash nonlinearities. First, neural networks (NNs) are adopted to approximate the completely unknown nonlinearities. New barrier Lyapunov functions are constructed to guarantee the compact set conditions of the NNs. Second, new adaptive neural proportional integral (PI)-type controllers are proposed for the networked ODEs and beams. The parameters of the PI controllers are adaptively tuned by NNs, which can make the system output remain in a prescribed time-varying constraint. Two illustrative examples are presented to demonstrate the advantages of the obtained results.},
  archive      = {J_TNNLS},
  author       = {Shiqi Zheng and Peng Shi and Shuoyu Wang and Yan Shi},
  doi          = {10.1109/TNNLS.2020.2979266},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {763-776},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive neural control for a class of nonlinear multiagent systems},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Random sketching for neural networks with ReLU.
<em>TNNLS</em>, <em>32</em>(2), 748–762. (<a
href="https://doi.org/10.1109/TNNLS.2020.2979228">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Training neural networks is recently a hot topic in machine learning due to its great success in many applications. Since the neural networks’ training usually involves a highly nonconvex optimization problem, it is difficult to design optimization algorithms with perfect convergence guarantees to derive a neural network estimator of high quality. In this article, we borrow the well-known random sketching strategy from kernel methods to transform the training of shallow rectified linear unit (ReLU) nets into a linear least-squares problem. Using the localized approximation property of shallow ReLU nets and a recently developed dimensionality-leveraging scheme, we succeed in equipping shallow ReLU nets with a specific random sketching scheme. The efficiency of the suggested random sketching strategy is guaranteed by theoretical analysis and also verified via a series of numerical experiments. Theoretically, we show that the proposed random sketching is almost optimal in terms of both approximation capability and learning performance. This implies that random sketching does not degenerate the performance of shallow ReLU nets. Numerically, we show that random sketching can significantly reduce the computational burden of numerous backpropagation (BP) algorithms while maintaining their learning performance.},
  archive      = {J_TNNLS},
  author       = {Di Wang and Jinshan Zeng and Shao-Bo Lin},
  doi          = {10.1109/TNNLS.2020.2979228},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {748-762},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Random sketching for neural networks with ReLU},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Coarse alignment of topic and sentiment: A unified model for
cross-lingual sentiment classification. <em>TNNLS</em>, <em>32</em>(2),
736–747. (<a href="https://doi.org/10.1109/TNNLS.2020.2979225">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-lingual sentiment classification (CLSC) aims to leverage rich-labeled resources in the source language to improve prediction models of a resource-scarce domain in the target language. Existing feature representation learning-based approaches try to minimize the difference of latent features between different domains by exact alignment, which is achieved by either one-to-one topic alignment or matrix projection. Exact alignment, however, restricts the representation flexibility and further degrades the model performances on CLSC tasks if the distribution difference between two language domains is large. On the other hand, most previous studies proposed document-level models or ignored sentiment polarities of topics that might lead to insufficient learning of latent features. To solve the abovementioned problems, we propose a coarse alignment mechanism to enhance the model&#39;s representation by a group-to-group topic alignment into an aspect-level fine-grained model. First, we propose an unsupervised aspect, opinion, and sentiment unification model (AOS), which trimodels aspects, opinions, and sentiments of reviews from different domains and helps capture more accurate latent feature representation by a coarse alignment mechanism. To further boost AOS, we propose ps-AOS, a partial supervised AOS model, in which labeled source language data help minimize the difference of feature representations between two language domains with the help of logistics regression. Finally, an expectation-maximization framework with Gibbs sampling is then proposed to optimize our model. Extensive experiments on various multilingual product review data sets show that ps-AOS significantly outperforms various kinds of state-of-the-art baselines.},
  archive      = {J_TNNLS},
  author       = {Deqing Wang and Baoyu Jing and Chenwei Lu and Junjie Wu and Guannan Liu and Chenguang Du and Fuzhen Zhuang},
  doi          = {10.1109/TNNLS.2020.2979225},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {736-747},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Coarse alignment of topic and sentiment: A unified model for cross-lingual sentiment classification},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep coattention-based comparator for relative
representation learning in person re-identification. <em>TNNLS</em>,
<em>32</em>(2), 722–735. (<a
href="https://doi.org/10.1109/TNNLS.2020.2979190">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person re-identification (re-ID) favors discriminative representations over unseen shots to recognize identities in disjoint camera views. Effective methods are developed via pair-wise similarity learning to detect a fixed set of region features, which can be mapped to compute the similarity value. However, relevant parts of each image are detected independently without referring to the correlation on the other image. Also, region-based methods spatially position local features for their aligned similarities. In this article, we introduce the deep coattention-based comparator (DCC) to fuse codependent representations of paired images so as to correlate the best relevant parts and produce their relative representations accordingly. The proposed approach mimics the human foveation to detect the distinct regions concurrently across images and alternatively attends to fuse them into the similarity learning. Our comparator is capable of learning representations relative to a test shot and well-suited to reidentifying pedestrians in surveillance. We perform extensive experiments to provide the insights and demonstrate the state of the arts achieved by our method in benchmark data sets: 1.2 and 2.5 points gain in mean average precision (mAP) on DukeMTMC-reID and Market-1501, respectively.},
  archive      = {J_TNNLS},
  author       = {Lin Wu and Yang Wang and Junbin Gao and Meng Wang and Zheng-Jun Zha and Dacheng Tao},
  doi          = {10.1109/TNNLS.2020.2979190},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {722-735},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Deep coattention-based comparator for relative representation learning in person re-identification},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Large-scale heteroscedastic regression via gaussian process.
<em>TNNLS</em>, <em>32</em>(2), 708–721. (<a
href="https://doi.org/10.1109/TNNLS.2020.2979188">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heteroscedastic regression considering the varying noises among observations has many applications in the fields, such as machine learning and statistics. Here, we focus on the heteroscedastic Gaussian process (HGP) regression that integrates the latent function and the noise function in a unified nonparametric Bayesian framework. Though showing remarkable performance, HGP suffers from the cubic time complexity, which strictly limits its application to big data. To improve the scalability, we first develop a variational sparse inference algorithm, named VSHGP, to handle large-scale data sets. Furthermore, two variants are developed to improve the scalability and capability of VSHGP. The first is stochastic VSHGP (SVSHGP) that derives a factorized evidence lower bound, thus enhancing efficient stochastic variational inference. The second is distributed VSHGP (DVSHGP) that follows the Bayesian committee machine formalism to distribute computations over multiple local VSHGP experts with many inducing points and adopts hybrid parameters for experts to guard against overfitting and capture local variety. The superiority of DVSHGP and SVSHGP compared to the existing scalable HGP/homoscedastic GP is then extensively verified on various data sets.},
  archive      = {J_TNNLS},
  author       = {Haitao Liu and Yew-Soon Ong and Jianfei Cai},
  doi          = {10.1109/TNNLS.2020.2979188},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {708-721},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Large-scale heteroscedastic regression via gaussian process},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Neural network adaptive tracking control of uncertain MIMO
nonlinear systems with output constraints and event-triggered inputs.
<em>TNNLS</em>, <em>32</em>(2), 695–707. (<a
href="https://doi.org/10.1109/TNNLS.2020.2979174">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article is concerned with a neural adaptive tracking control scheme for a class of multiinput and multioutput (MIMO) nonaffine nonlinear systems with event-triggered mechanisms, which include the fixed thresholds, triggering control inputs, and decreasing functions of tracking errors. Unlike the existing results of nonaffine nonlinear controller decoupling, a novel nonlinear multiple control inputs separated design method is proposed based on the mean-value theorem and the Taylor expansion technique. By this way, a weaker condition of nonlinear decoupling is provided to instead of the previous ones. Then, introducing a prescribed performance barrier Lyapunov function (PPBLF) and using neural networks (NNs), the presented event-triggered controller can maintain better tracking performance and effectively alleviate the computation burden of the communication procedure. Furthermore, it is proved that all the closed-loop signals are bounded and the system output tracking errors are confined within the prescribed bounds. Finally, the simulation results are given to demonstrate the validity of the developed control scheme.},
  archive      = {J_TNNLS},
  author       = {Li-Bing Wu and Ju H. Park and Xiang-Peng Xie and Ya-Juan Liu},
  doi          = {10.1109/TNNLS.2020.2979174},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {695-707},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Neural network adaptive tracking control of uncertain MIMO nonlinear systems with output constraints and event-triggered inputs},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A novel learning algorithm to optimize deep neural networks:
Evolved gradient direction optimizer (EVGO). <em>TNNLS</em>,
<em>32</em>(2), 685–694. (<a
href="https://doi.org/10.1109/TNNLS.2020.2979121">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gradient-based algorithms have been widely used in optimizing parameters of deep neural networks’ (DNNs) architectures. However, the vanishing gradient remains as one of the common issues in the parameter optimization of such networks. To cope with the vanishing gradient problem, in this article, we propose a novel algorithm, evolved gradient direction optimizer (EVGO), updating the weights of DNNs based on the first-order gradient and a novel hyperplane we introduce. We compare the EVGO algorithm with other gradient-based algorithms, such as gradient descent, RMSProp, Adagrad, momentum, and Adam on the well-known Modified National Institute of Standards and Technology (MNIST) data set for handwritten digit recognition by implementing deep convolutional neural networks. Furthermore, we present empirical evaluations of EVGO on the CIFAR-10 and CIFAR-100 data sets by using the well-known AlexNet and ResNet architectures. Finally, we implement an empirical analysis for EVGO and other algorithms to investigate the behavior of the loss functions. The results show that EVGO outperforms all the algorithms in comparison for all experiments. We conclude that EVGO can be used effectively in the optimization of DNNs, and also, the proposed hyperplane may provide a basis for future optimization algorithms.},
  archive      = {J_TNNLS},
  author       = {Ibrahim Karabayir and Oguz Akbilgic and Nihat Tas},
  doi          = {10.1109/TNNLS.2020.2979121},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {685-694},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A novel learning algorithm to optimize deep neural networks: Evolved gradient direction optimizer (EVGO)},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep end-to-end one-class classifier. <em>TNNLS</em>,
<em>32</em>(2), 675–684. (<a
href="https://doi.org/10.1109/TNNLS.2020.2979049">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One-class classification (OCC) poses as an essential component in many machine learning and computer vision applications, including novelty, anomaly, and outlier detection systems. With a known definition for a target or normal set of data, one-class classifiers can determine if any given new sample spans within the distribution of the target class. Solving for this task in a general setting is particularly very challenging, due to the high diversity of samples from the target class and the absence of any supervising signal over the novelty (nontarget) concept, which makes designing end-to-end models unattainable. In this article, we propose an adversarial training approach to detect out-of-distribution samples in an end-to-end trainable deep model. To this end, we jointly train two deep neural networks, R and D. The latter plays as the discriminator while the former, during training, helps D characterize a probability distribution for the target class by creating adversarial examples and, during testing, collaborates with it to detect novelties. Using our OCC, we first test outlier detection on two image data sets, Modified National Institute of Standards and Technology (MNIST) and Caltech-256. Then, several experiments for video anomaly detection are performed on University of Minnesota (UMN) and University of California, San Diego (UCSD) data sets. Our proposed method can successfully learn the target class underlying distribution and outperforms other approaches.},
  archive      = {J_TNNLS},
  author       = {Mohammad Sabokrou and Mahmood Fathy and Guoying Zhao and Ehsan Adeli},
  doi          = {10.1109/TNNLS.2020.2979049},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {675-684},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Deep end-to-end one-class classifier},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Host–parasite: Graph LSTM-in-LSTM for group activity
recognition. <em>TNNLS</em>, <em>32</em>(2), 663–674. (<a
href="https://doi.org/10.1109/TNNLS.2020.2978942">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article aims to tackle the problem of group activity recognition in the multiple-person scene. To model the group activity with multiple persons, most long short-term memory (LSTM)-based methods first learn the person-level action representations by several LSTMs and then integrate all the person-level action representations into the following LSTM to learn the group-level activity representation. This type of solution is a two-stage strategy, which neglects the “host–parasite” relationship between the group-level activity (“host”) and person-level actions (“parasite”) in spatiotemporal space. To this end, we propose a novel graph LSTM-in-LSTM (GLIL) for group activity recognition by modeling the person-level actions and the group-level activity simultaneously. GLIL is a “host–parasite” architecture, which can be seen as several person LSTMs (P-LSTMs) in the local view or a graph LSTM (G-LSTM) in the global view. Specifically, P-LSTMs model the person-level actions based on the interactions among persons. Meanwhile, G-LSTM models the group-level activity, where the person-level motion information in multiple P-LSTMs is selectively integrated and stored into G-LSTM based on their contributions to the inference of the group activity class. Furthermore, to use the person-level temporal features instead of the person-level static features as the input of GLIL, we introduce a residual LSTM with the residual connection to learn the person-level residual features, consisting of temporal features and static features. Experimental results on two public data sets illustrate the effectiveness of the proposed GLIL compared with state-of-the-art methods.},
  archive      = {J_TNNLS},
  author       = {Xiangbo Shu and Liyan Zhang and Yunlian Sun and Jinhui Tang},
  doi          = {10.1109/TNNLS.2020.2978942},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {663-674},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Host–Parasite: Graph LSTM-in-LSTM for group activity recognition},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Finite-time consensus tracking neural network FTC of
multi-agent systems. <em>TNNLS</em>, <em>32</em>(2), 653–662. (<a
href="https://doi.org/10.1109/TNNLS.2020.2978898">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The finite-time consensus fault-tolerant control (FTC) tracking problem is studied for the nonlinear multi-agent systems (MASs) in the nonstrict feedback form. The MASs are subject to unknown symmetric output dead zones, actuator bias and gain faults, and unknown control coefficients. According to the properties of the neural network (NN), the unstructured uncertainties problem is solved. The Nussbaum function is used to address the output dead zones and unknown control directions problems. By introducing an arbitrarily small positive number, the “singularity” problem caused by combining the finite-time control and backstepping design is solved. According to the backstepping design and Lyapunov stability theory, a finite-time adaptive NN FTC controller is obtained, which guarantees that the tracking error converges to a small neighborhood of zero in a finite time, and all signals in the closed-loop system are bounded. Finally, the effectiveness of the proposed method is illustrated via a physical example.},
  archive      = {J_TNNLS},
  author       = {Guowei Dong and Hongyi Li and Hui Ma and Renquan Lu},
  doi          = {10.1109/TNNLS.2020.2978898},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {653-662},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Finite-time consensus tracking neural network FTC of multi-agent systems},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Neural learning-based fixed-time consensus tracking control
for nonlinear multiagent systems with directed communication networks.
<em>TNNLS</em>, <em>32</em>(2), 639–652. (<a
href="https://doi.org/10.1109/TNNLS.2020.2978854">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates the problem of fixed-time consensus tracking for nonlinear multiagent systems. Different from the existing studies where the follower systems are linear or pure integrator-type systems, in this article, the follower systems have completely unknown nonlinear functions and time-varying disturbances. Within this framework, a fixed-time observer-based distributed control strategy is proposed to realize the consensus tracking. First, a distributed fixed-time observer is designed for each follower to estimate the leader&#39;s state under directed networks. Then, based on the estimate, a fixed-time tracking control protocol is developed where novel approximation and estimation schemes are designed to tackle the nonlinear functions and disturbances. Furthermore, under the proposed control strategy, it is proved that the tracking errors converge into a small set near zero with a fixed-time convergence rate. Finally, the validity of the proposed method is verified by the simulation results.},
  archive      = {J_TNNLS},
  author       = {Yan Liu and Guang-Hong Yang},
  doi          = {10.1109/TNNLS.2020.2978854},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {639-652},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Neural learning-based fixed-time consensus tracking control for nonlinear multiagent systems with directed communication networks},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust environmental sound recognition with sparse key-point
encoding and efficient multispike learning. <em>TNNLS</em>,
<em>32</em>(2), 625–638. (<a
href="https://doi.org/10.1109/TNNLS.2020.2978764">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The capability for environmental sound recognition (ESR) can determine the fitness of individuals in a way to avoid dangers or pursue opportunities when critical sound events occur. It still remains mysterious about the fundamental principles of biological systems that result in such a remarkable ability. Additionally, the practical importance of ESR has attracted an increasing amount of research attention, but the chaotic and nonstationary difficulties continue to make it a challenging task. In this article, we propose a spike-based framework from a more brain-like perspective for the ESR task. Our framework is a unifying system with consistent integration of three major functional parts which are sparse encoding, efficient learning, and robust readout. We first introduce a simple sparse encoding, where key points are used for feature representation, and demonstrate its generalization to both spike- and nonspike-based systems. Then, we evaluate the learning properties of different learning rules in detail with our contributions being added for improvements. Our results highlight the advantages of multispike learning, providing a selection reference for various spike-based developments. Finally, we combine the multispike readout with the other parts to form a system for ESR. Experimental results show that our framework performs the best as compared to other baseline approaches. In addition, we show that our spike-based framework has several advantageous characteristics including early decision making, small dataset acquiring, and ongoing dynamic processing. Our framework is the first attempt to apply the multispike characteristic of nervous neurons to ESR. The outstanding performance of our approach would potentially contribute to draw more research efforts to push the boundaries of spike-based paradigm to a new horizon.},
  archive      = {J_TNNLS},
  author       = {Qiang Yu and Yanli Yao and Longbiao Wang and Huajin Tang and Jianwu Dang and Kay Chen Tan},
  doi          = {10.1109/TNNLS.2020.2978764},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {625-638},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Robust environmental sound recognition with sparse key-point encoding and efficient multispike learning},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A survey of the usages of deep learning for natural language
processing. <em>TNNLS</em>, <em>32</em>(2), 604–624. (<a
href="https://doi.org/10.1109/TNNLS.2020.2979670">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the last several years, the field of natural language processing has been propelled forward by an explosion in the use of deep learning models. This article provides a brief introduction to the field and a quick overview of deep learning architectures and methods. It then sifts through the plethora of recent studies and summarizes a large assortment of relevant contributions. Analyzed research areas include several core linguistic processing issues in addition to many applications of computational linguistics. A discussion of the current state of the art is then provided along with recommendations for future research in the field.},
  archive      = {J_TNNLS},
  author       = {Daniel W. Otter and Julian R. Medina and Jugal K. Kalita},
  doi          = {10.1109/TNNLS.2020.2979670},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {604-624},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A survey of the usages of deep learning for natural language processing},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Neural encoding and decoding with distributed sentence
representations. <em>TNNLS</em>, <em>32</em>(2), 589–603. (<a
href="https://doi.org/10.1109/TNNLS.2020.3027595">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Building computational models to account for the cortical representation of language plays an important role in understanding the human linguistic system. Recent progress in distributed semantic models (DSMs), especially transformer-based methods, has driven advances in many language understanding tasks, making DSM a promising methodology to probe brain language processing. DSMs have been shown to reliably explain cortical responses to word stimuli. However, characterizing the brain activities for sentence processing is much less exhaustively explored with DSMs, especially the deep neural network-based methods. What is the relationship between cortical sentence representations against DSMs? What linguistic features that a DSM catches better explain its correlation with the brain activities aroused by sentence stimuli? Could distributed sentence representations help to reveal the semantic selectivity of different brain areas? We address these questions through the lens of neural encoding and decoding, fueled by the latest developments in natural language representation learning. We begin by evaluating the ability of a wide range of 12 DSMs to predict and decipher the functional magnetic resonance imaging (fMRI) images from humans reading sentences. Most models deliver high accuracy in the left middle temporal gyrus (LMTG) and left occipital complex (LOC). Notably, encoders trained with transformer-based DSMs consistently outperform other unsupervised structured models and all the unstructured baselines. With probing and ablation tasks, we further find that differences in the performance of the DSMs in modeling brain activities can be at least partially explained by the granularity of their semantic representations. We also illustrate the DSM’s selectivity for concept categories and show that the topics are represented by spatially overlapping and distributed cortical patterns. Our results corroborate and extend previous findings in understanding the relation between DSMs and neural activation patterns and contribute to building solid brain–machine interfaces with deep neural network representations.},
  archive      = {J_TNNLS},
  author       = {Jingyuan Sun and Shaonan Wang and Jiajun Zhang and Chengqing Zong},
  doi          = {10.1109/TNNLS.2020.3027595},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {589-603},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Neural encoding and decoding with distributed sentence representations},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Handheld ultrasound video high-quality reconstruction using
a low-rank representation multipathway generative adversarial network.
<em>TNNLS</em>, <em>32</em>(2), 575–588. (<a
href="https://doi.org/10.1109/TNNLS.2020.3025380">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, the use of portable equipment has attracted much attention in the medical ultrasound field. Handheld ultrasound devices have great potential for improving the convenience of diagnosis, but noise-induced artifacts and low resolution limit their application. To enhance the video quality of handheld ultrasound devices, we propose a low-rank representation multipathway generative adversarial network (LRR MPGAN) with a cascade training strategy. This method can directly generate sequential, high-quality ultrasound video with clear tissue structures and details. In the cascade training process, the network is first trained with plane wave (PW) single-/multiangle video pairs to capture dynamic information and then fine-tuned with handheld/high-end image pairs to extract high-quality single-frame information. In the proposed GAN structure, a multipathway generator is applied to implement the cascade training strategy, which can simultaneously extract dynamic information and synthesize multiframe features. The LRR decomposition channel approach guarantees the fine reconstruction of both global features and local details. In addition, a novel ultrasound loss is added to the conventional mean square error (MSE) loss to acquire ultrasound-specific perceptual features. A comprehensive evaluation is conducted in the experiments, and the results confirm that the proposed method can effectively reconstruct high-quality ultrasound videos for handheld devices. With the aid of the proposed method, handheld ultrasound devices can be used to obtain convincing and convenient diagnoses.},
  archive      = {J_TNNLS},
  author       = {Zixia Zhou and Yi Guo and Yuanyuan Wang},
  doi          = {10.1109/TNNLS.2020.3025380},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {575-588},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Handheld ultrasound video high-quality reconstruction using a low-rank representation multipathway generative adversarial network},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Tridirectional transfer learning for predicting gastric
cancer morbidity. <em>TNNLS</em>, <em>32</em>(2), 561–574. (<a
href="https://doi.org/10.1109/TNNLS.2020.2979486">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Our previous study has constructed a deep learning model for predicting gastrointestinal infection morbidity based on environmental pollutant indicators in some regions in central China. This article aims to adapt the prediction model for three purposes: 1) predicting the morbidity of a different disease in the same region; 2) predicting the morbidity of the same disease in a different region; and 3) predicting the morbidity of a different disease in a different region. We propose a tridirectional transfer learning approach, which achieves the abovementioned three purposes by: 1) developing a combined univariate regression and multivariate Gaussian model for establishing the relationship between the morbidity of the target disease and that of the source disease together with the high-level pollutant features in the current source region; 2) using mapping-based deep transfer learning to extend the current model to predict the morbidity of the source disease in both source and target regions; and 3) applying the pattern of the combined model in the source region to the extended model to derive a new combined model for predicting the morbidity of the target disease in the target region. We select gastric cancer as the target disease and use the proposed transfer learning approach to predict its morbidity in the source region and three target regions. The results show that, given only a limited number of labeled samples, our approach achieves an average prediction accuracy of over 80\% in the source region and up to 78\% in the target regions, which can contribute considerably to improving medical preparedness and response.},
  archive      = {J_TNNLS},
  author       = {Qin Song and Yu-Jun Zheng and Wei-Guo Sheng and Jun Yang},
  doi          = {10.1109/TNNLS.2020.2979486},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {561-574},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Tridirectional transfer learning for predicting gastric cancer morbidity},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Cross-subject and cross-modal transfer for generalized
abnormal gait pattern recognition. <em>TNNLS</em>, <em>32</em>(2),
546–560. (<a href="https://doi.org/10.1109/TNNLS.2020.3009448">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For abnormal gait recognition, pattern-specific features indicating abnormalities are interleaved with the subject-specific differences representing biometric traits. Deep representations are, therefore, prone to overfitting, and the models derived cannot generalize well to new subjects. Furthermore, there is limited availability of abnormal gait data obtained from precise Motion Capture (Mocap) systems because of regulatory issues and slow adaptation of new technologies in health care. On the other hand, data captured from markerless vision sensors or wearable sensors can be obtained in home environments, but noises from such devices may prevent the effective extraction of relevant features. To address these challenges, we propose a cascade of deep architectures that can encode cross-modal and cross-subject transfer for abnormal gait recognition. Cross-modal transfer maps noisy data obtained from RGBD and wearable sensors to accurate 4-D representations of the lower limb and joints obtained from the Mocap system. Subsequently, cross-subject transfer allows disentangling subject-specific from abnormal pattern-specific gait features based on a multiencoder autoencoder architecture. To validate the proposed methodology, we obtained multimodal gait data based on a multicamera motion capture system along with synchronized recordings of electromyography (EMG) data and 4-D skeleton data extracted from a single RGBD camera. Classification accuracy was improved significantly in both Mocap and noisy modalities.},
  archive      = {J_TNNLS},
  author       = {Xiao Gu and Yao Guo and Fani Deligianni and Benny Lo and Guang-Zhong Yang},
  doi          = {10.1109/TNNLS.2020.3009448},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {546-560},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Cross-subject and cross-modal transfer for generalized abnormal gait pattern recognition},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep representation-based domain adaptation for
nonstationary EEG classification. <em>TNNLS</em>, <em>32</em>(2),
535–545. (<a href="https://doi.org/10.1109/TNNLS.2020.3010780">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the context of motor imagery, electroencephalography (EEG) data vary from subject to subject such that the performance of a classifier trained on data of multiple subjects from a specific domain typically degrades when applied to a different subject. While collecting enough samples from each subject would address this issue, it is often too time-consuming and impractical. To tackle this problem, we propose a novel end-to-end deep domain adaptation method to improve the classification performance on a single subject (target domain) by taking the useful information from multiple subjects (source domain) into consideration. Especially, the proposed method jointly optimizes three modules, including a feature extractor, a classifier, and a domain discriminator. The feature extractor learns the discriminative latent features by mapping the raw EEG signals into a deep representation space. A center loss is further employed to constrain an invariant feature space and reduce the intrasubject nonstationarity. Furthermore, the domain discriminator matches the feature distribution shift between source and target domains by an adversarial learning strategy. Finally, based on the consistent deep features from both domains, the classifier is able to leverage the information from the source domain and accurately predict the label in the target domain at the test time. To evaluate our method, we have conducted extensive experiments on two real public EEG data sets, data set IIa, and data set IIb of brain–computer interface (BCI) Competition IV. The experimental results validate the efficacy of our method. Therefore, our method is promising to reduce the calibration time for the use of BCI and promote the development of BCI.},
  archive      = {J_TNNLS},
  author       = {He Zhao and Qingqing Zheng and Kai Ma and Huiqi Li and Yefeng Zheng},
  doi          = {10.1109/TNNLS.2020.3010780},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {535-545},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Deep representation-based domain adaptation for nonstationary EEG classification},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Transformation-consistent self-ensembling model for
semisupervised medical image segmentation. <em>TNNLS</em>,
<em>32</em>(2), 523–534. (<a
href="https://doi.org/10.1109/TNNLS.2020.2995319">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A common shortfall of supervised deep learning for medical imaging is the lack of labeled data, which is often expensive and time consuming to collect. This article presents a new semisupervised method for medical image segmentation, where the network is optimized by a weighted combination of a common supervised loss only for the labeled inputs and a regularization loss for both the labeled and unlabeled data. To utilize the unlabeled data, our method encourages consistent predictions of the network-in-training for the same input under different perturbations. With the semisupervised segmentation tasks, we introduce a transformation-consistent strategy in the self-ensembling model to enhance the regularization effect for pixel-level predictions. To further improve the regularization effects, we extend the transformation in a more generalized form including scaling and optimize the consistency loss with a teacher model, which is an averaging of the student model weights. We extensively validated the proposed semisupervised method on three typical yet challenging medical image segmentation tasks: 1) skin lesion segmentation from dermoscopy images in the International Skin Imaging Collaboration (ISIC) 2017 data set; 2) optic disk (OD) segmentation from fundus images in the Retinal Fundus Glaucoma Challenge (REFUGE) data set; and 3) liver segmentation from volumetric CT scans in the Liver Tumor Segmentation Challenge (LiTS) data set. Compared with state-of-the-art, our method shows superior performance on the challenging 2-D/3-D medical images, demonstrating the effectiveness of our semisupervised method for medical image segmentation.},
  archive      = {J_TNNLS},
  author       = {Xiaomeng Li and Lequan Yu and Hao Chen and Chi-Wing Fu and Lei Xing and Pheng-Ann Heng},
  doi          = {10.1109/TNNLS.2020.2995319},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {523-534},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Transformation-consistent self-ensembling model for semisupervised medical image segmentation},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep learning for multigrade brain tumor classification in
smart healthcare systems: A prospective survey. <em>TNNLS</em>,
<em>32</em>(2), 507–522. (<a
href="https://doi.org/10.1109/TNNLS.2020.2995800">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Brain tumor is one of the most dangerous cancers in people of all ages, and its grade recognition is a challenging problem for radiologists in health monitoring and automated diagnosis. Recently, numerous methods based on deep learning have been presented in the literature for brain tumor classification (BTC) in order to assist radiologists for a better diagnostic analysis. In this overview, we present an in-depth review of the surveys published so far and recent deep learning-based methods for BTC. Our survey covers the main steps of deep learning-based BTC methods, including preprocessing, features extraction, and classification, along with their achievements and limitations. We also investigate the state-of-the-art convolutional neural network models for BTC by performing extensive experiments using transfer learning with and without data augmentation. Furthermore, this overview describes available benchmark data sets used for the evaluation of BTC. Finally, this survey does not only look into the past literature on the topic but also steps on it to delve into the future of this area and enumerates some research directions that should be followed in the future, especially for personalized and smart healthcare.},
  archive      = {J_TNNLS},
  author       = {Khan Muhammad and Salman Khan and Javier Del Ser and Victor Hugo C. de Albuquerque},
  doi          = {10.1109/TNNLS.2020.2995800},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {507-522},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Deep learning for multigrade brain tumor classification in smart healthcare systems: A prospective survey},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multitask learning for estimating multitype cardiac indices
in MRI and CT based on adversarial reverse mapping. <em>TNNLS</em>,
<em>32</em>(2), 493–506. (<a
href="https://doi.org/10.1109/TNNLS.2020.2984955">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The estimation of multitype cardiac indices from cardiac magnetic resonance imaging (MRI) and computed tomography (CT) images attracts great attention because of its clinical potential for comprehensive function assessment. However, the most exiting model can only work in one imaging modality (MRI or CT) without transferable capability. In this article, we propose the multitask learning method with the reverse inferring for estimating multitype cardiac indices in MRI and CT. Different from the existing forward inferring methods, our method builds a reverse mapping network that maps the multitype cardiac indices to cardiac images. The task dependencies are then learned and shared to multitask learning networks using an adversarial training approach. Finally, we transfer the parameters learned from MRI to CT. A series of experiments were conducted in which we first optimized the performance of our framework via ten-fold cross-validation of over 2900 cardiac MRI images. Then, the fine-tuned network was run on an independent data set with 2360 cardiac CT images. The results of all the experiments conducted on the proposed adversarial reverse mapping show excellent performance in estimating multitype cardiac indices.},
  archive      = {J_TNNLS},
  author       = {Chengjin Yu and Zhifan Gao and Weiwei Zhang and Guang Yang and Shu Zhao and Heye Zhang and Yanping Zhang and Shuo Li},
  doi          = {10.1109/TNNLS.2020.2984955},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {493-506},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multitask learning for estimating multitype cardiac indices in MRI and CT based on adversarial reverse mapping},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep graph-based multimodal feature embedding for
endomicroscopy image retrieval. <em>TNNLS</em>, <em>32</em>(2), 481–492.
(<a href="https://doi.org/10.1109/TNNLS.2020.2980129">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Representation learning is a critical task for medical image analysis in computer-aided diagnosis. However, it is challenging to learn discriminative features due to the limited size of the data set and the lack of labels. In this article, we propose a deep graph-based multimodal feature embedding (DGMFE) framework for medical image retrieval with application to breast tissue classification by learning discriminative features of probe-based confocal laser endomicroscopy (pCLE). We first build a multimodality graph model based on the visual similarity between pCLE data and reference histology images. The latent similar pCLE–histology pairs are extracted by walking with the cyclic path on the graph while the dissimilar pairs are extracted based on the geodesic distance. Given the similar and dissimilar pairs, the latent feature space is discovered by reconstructing the similarity between pCLE and histology images via deep Siamese neural networks. The proposed method is evaluated on a clinical database with 700 pCLE mosaics. The accuracy of image retrieval demonstrates that DGMFE can outperform previous works on feature learning. Especially, the top-1 accuracy in an eight-class retrieval task is 0.739, thus demonstrating a 10\% improvement compared to the state-of-the-art method.},
  archive      = {J_TNNLS},
  author       = {Yun Gu and Khushi Vyas and Mali Shen and Jie Yang and Guang-Zhong Yang},
  doi          = {10.1109/TNNLS.2020.2980129},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {481-492},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Deep graph-based multimodal feature embedding for endomicroscopy image retrieval},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Task similarity estimation through adversarial multitask
neural network. <em>TNNLS</em>, <em>32</em>(2), 466–480. (<a
href="https://doi.org/10.1109/TNNLS.2020.3028022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multitask learning (MTL) aims at solving the related tasks simultaneously by exploiting shared knowledge to improve performance on individual tasks. Though numerous empirical results supported the notion that such shared knowledge among tasks plays an essential role in MTL, the theoretical understanding of the relationships between tasks and their impact on learning shared knowledge is still an open problem. In this work, we are developing a theoretical perspective of the benefits involved in using information similarity for MTL. To this end, we first propose an upper bound on the generalization error by implementing the Wasserstein distance as the similarity metric. This indicates the practical principles of applying the similarity information to control the generalization errors. Based on those theoretical results, we revisited the adversarial multitask neural network and proposed a new training algorithm to learn the task relation coefficients and neural network parameters automatically. The computer vision benchmarks reveal the abilities of the proposed algorithms to improve the empirical performance. Finally, we test the proposed approach on real medical data sets, showing its advantage for extracting task relations.},
  archive      = {J_TNNLS},
  author       = {Fan Zhou and Changjian Shui and Mahdieh Abbasi and Louis-Émile Robitaille and Boyu Wang and Christian Gagné},
  doi          = {10.1109/TNNLS.2020.3028022},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {466-480},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Task similarity estimation through adversarial multitask neural network},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Guest editorial: Special issue on deep representation and
transfer learning for smart and connected health. <em>TNNLS</em>,
<em>32</em>(2), 464–465. (<a
href="https://doi.org/10.1109/TNNLS.2021.3049931">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks (NNs) have been proved to be efficient learning systems for supervised and unsupervised tasks. However, learning complex data representations using deep NNs can be difficult due to problems such as lack of data, exploding or vanishing gradients, high computational cost, or incorrect parameter initialization, among others. Deep representation and transfer learning (RTL) can facilitate the learning of data representations by taking advantage of transferable features learned by an NN model in a source domain, and adapting the model to a new domain.},
  archive      = {J_TNNLS},
  author       = {Vasile Palade and Stefan Wermter and Ariel Ruiz-Garcia and Antônio De Pádua Braga and Clive Cheong Took},
  doi          = {10.1109/TNNLS.2021.3049931},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {464-465},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Guest editorial: Special issue on deep representation and transfer learning for smart and connected health},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021l). IEEE computational intelligence society information.
<em>TNNLS</em>, <em>32</em>(1), C3. (<a
href="https://doi.org/10.1109/TNNLS.2020.3043905">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Provides a listing of current committee members and society officers.},
  archive      = {J_TNNLS},
  doi          = {10.1109/TNNLS.2020.3043905},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {C3},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {IEEE computational intelligence society information},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fast algorithms for quaternion-valued convolutional neural
networks. <em>TNNLS</em>, <em>32</em>(1), 457–462. (<a
href="https://doi.org/10.1109/TNNLS.2020.2979682">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we analyze algorithmic ways to reduce the arithmetic complexity of calculating quaternion-valued linear convolution and also synthesize a new algorithm for calculating this convolution. During the synthesis of the discussed algorithm, we use the fact that quaternion multiplication may be represented as a matrix-vector product. The matrix participating in the product has unique structural properties that allow performing its advantageous decomposition. Namely, this decomposition leads to reducing the multiplicative complexity of computations. In addition, we used the fact that when calculating the elements of the quaternion-valued convolution, the part of the calculations of all matrix-vector products is common. It gives an additional reduction in the number of additions of real numbers and, consequently, a decrease in the additive complexity of calculations. Thus, the use of the proposed algorithm will contribute to the acceleration of calculations in quaternion-valued convolution neural networks.},
  archive      = {J_TNNLS},
  author       = {Aleksandr Cariow and Galina Cariowa},
  doi          = {10.1109/TNNLS.2020.2979682},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {457-462},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Fast algorithms for quaternion-valued convolutional neural networks},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Data clustering via uncorrelated ridge regression.
<em>TNNLS</em>, <em>32</em>(1), 450–456. (<a
href="https://doi.org/10.1109/TNNLS.2020.2978755">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ridge regression is frequently utilized by both supervised and semisupervised learnings. However, the trivial solution might occur, when ridge regression is directly applied for clustering. To address this issue, an uncorrelated constraint is introduced to the ridge regression with embedding the manifold structure. In particular, we choose uncorrelated constraint over orthogonal constraint, since the closed-form solution can be obtained correspondingly. In addition to the proposed uncorrelated ridge regression, a soft pseudo label is utilized with ℓ 1 ball constraint for clustering. Moreover, a brand new strategy, i.e., a rescaled technique, is proposed such that optimal scaling within the uncorrelated constraint can be achieved automatically to avoid the inconvenience of tuning it manually. Equipped with the rescaled uncorrelated ridge regression with the soft label, a novel clustering method can be developed based on solving the related clustering model. Consequently, extensive experiments are provided to illustrate the effectiveness of the proposed method.},
  archive      = {J_TNNLS},
  author       = {Rui Zhang and Xuelong Li and Tong Wu and Yi Zhao},
  doi          = {10.1109/TNNLS.2020.2978755},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {450-456},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Data clustering via uncorrelated ridge regression},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Autoencoder constrained clustering with adaptive neighbors.
<em>TNNLS</em>, <em>32</em>(1), 443–449. (<a
href="https://doi.org/10.1109/TNNLS.2020.2978389">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The conventional subspace clustering method obtains explicit data representation that captures the global structure of data and clusters via the associated subspace. However, due to the limitation of intrinsic linearity and fixed structure, the advantages of prior structure are limited. To address this problem, in this brief, we embed the structured graph learning with adaptive neighbors into the deep autoencoder networks such that an adaptive deep clustering approach, namely, autoencoder constrained clustering with adaptive neighbors (ACC_AN), is developed. The proposed method not only can adaptively investigate the nonlinear structure of data via a parameter-free graph built upon deep features but also can iteratively strengthen the correlations among the deep representations in the learning process. In addition, the local structure of raw data is preserved by minimizing the reconstruction error. Compared to the state-of-the-art works, ACC_AN is the first deep clustering method embedded with the adaptive structured graph learning to update the latent representation of data and structured deep graph simultaneously.},
  archive      = {J_TNNLS},
  author       = {Xuelong Li and Rui Zhang and Qi Wang and Hongyuan Zhang},
  doi          = {10.1109/TNNLS.2020.2978389},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {443-449},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Autoencoder constrained clustering with adaptive neighbors},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Understanding convolutional neural networks with information
theory: An initial exploration. <em>TNNLS</em>, <em>32</em>(1), 435–442.
(<a href="https://doi.org/10.1109/TNNLS.2020.2968509">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel functional estimator for Rényi&#39;s α-entropy and its multivariate extension was recently proposed in terms of the normalized eigenspectrum of a Hermitian matrix of the projected data in a reproducing kernel Hilbert space (RKHS). However, the utility and possible applications of these new estimators are rather new and mostly unknown to practitioners. In this brief, we first show that this estimator enables straightforward measurement of information flow in realistic convolutional neural networks (CNNs) without any approximation. Then, we introduce the partial information decomposition (PID) framework and develop three quantities to analyze the synergy and redundancy in convolutional layer representations. Our results validate two fundamental data processing inequalities and reveal more inner properties concerning CNN training.},
  archive      = {J_TNNLS},
  author       = {Shujian Yu and Kristoffer Wickstrøm and Robert Jenssen and José C. Príncipe},
  doi          = {10.1109/TNNLS.2020.2968509},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {435-442},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Understanding convolutional neural networks with information theory: An initial exploration},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Evolving deep neural networks via cooperative coevolution
with backpropagation. <em>TNNLS</em>, <em>32</em>(1), 420–434. (<a
href="https://doi.org/10.1109/TNNLS.2020.2978857">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks (DNNs), characterized by sophisticated architectures capable of learning a hierarchy of feature representations, have achieved remarkable successes in various applications. Learning DNN’s parameters is a crucial but challenging task that is commonly resolved by using gradient-based backpropagation (BP) methods. However, BP-based methods suffer from severe initialization sensitivity and proneness to getting trapped into inferior local optima. To address these issues, we propose a DNN learning framework that hybridizes CC-based optimization with BP-based gradient descent, called BPCC, and implement it by devising a computationally efficient CC-based optimization technique dedicated to DNN parameter learning. In BPCC, BP will intermittently execute for multiple training epochs. Whenever the execution of BP in a training epoch cannot sufficiently decrease the training objective function value, CC will kick in to execute by using the parameter values derived by BP as the starting point. The best parameter values obtained by CC will act as the starting point of BP in its next training epoch. In CC-based optimization, the overall parameter learning task is decomposed into many subtasks of learning a small portion of parameters. These subtasks are individually addressed in a cooperative manner. In this article, we treat neurons as basic decomposition units. Furthermore, to reduce the computational cost, we devise a maturity-based subtask selection strategy to selectively solve some subtasks of higher priority. Experimental results demonstrate the superiority of the proposed method over common-practice DNN parameter learning techniques.},
  archive      = {J_TNNLS},
  author       = {Maoguo Gong and Jia Liu and A. K. Qin and Kun Zhao and Kay Chen Tan},
  doi          = {10.1109/TNNLS.2020.2978857},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {420-434},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Evolving deep neural networks via cooperative coevolution with backpropagation},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Safe approximate dynamic programming via kernelized
lipschitz estimation. <em>TNNLS</em>, <em>32</em>(1), 405–419. (<a
href="https://doi.org/10.1109/TNNLS.2020.2978805">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a method for obtaining safe initial policies for reinforcement learning via approximate dynamic programming (ADP) techniques for uncertain systems evolving with discrete-time dynamics. We employ the kernelized Lipschitz estimation to learn multiplier matrices that are used in semidefinite programming frameworks for computing admissible initial control policies with provably high probability. Such admissible controllers enable safe initialization and constraint enforcement while providing exponential stability of the equilibrium of the closed-loop system.},
  archive      = {J_TNNLS},
  author       = {Ankush Chakrabarty and Devesh K. Jha and Gregery T. Buzzard and Yebin Wang and Kyriakos G. Vamvoudakis},
  doi          = {10.1109/TNNLS.2020.2978805},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {405-419},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Safe approximate dynamic programming via kernelized lipschitz estimation},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Kernelized sparse bayesian matrix factorization.
<em>TNNLS</em>, <em>32</em>(1), 391–404. (<a
href="https://doi.org/10.1109/TNNLS.2020.2978761">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extracting low-rank and/or sparse structures using matrix factorization techniques has been extensively studied in the machine learning community. Kernelized matrix factorization (KMF) is a powerful tool to incorporate side information into the low-rank approximation model, which has been applied to solve the problems of data mining, recommender systems, image restoration, and machine vision. However, most existing KMF models rely on specifying the rows and columns of the data matrix through a Gaussian process prior and have to tune manually the rank. There are also computational issues of existing models based on regularization or the Markov chain Monte Carlo. In this article, we develop a hierarchical kernelized sparse Bayesian matrix factorization (KSBMF) model to integrate side information. The KSBMF automatically infers the parameters and latent variables including the reduced rank using the variational Bayesian inference. In addition, the model simultaneously achieves low-rankness through sparse Bayesian learning and columnwise sparsity through an enforced constraint on latent factor matrices. We further connect the KSBMF with the nonlocal image processing framework to develop two algorithms for image denoising and inpainting. Experimental results demonstrate that KSBMF outperforms the state-of-the-art approaches for these image-restoration tasks under various levels of corruption.},
  archive      = {J_TNNLS},
  author       = {Caoyuan Li and Hong-Bo Xie and Xuhui Fan and Richard Yi Da Xu and Sabine Van Huffel and Kerrie Mengersen},
  doi          = {10.1109/TNNLS.2020.2978761},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {391-404},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Kernelized sparse bayesian matrix factorization},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Naive gabor networks for hyperspectral image classification.
<em>TNNLS</em>, <em>32</em>(1), 376–390. (<a
href="https://doi.org/10.1109/TNNLS.2020.2978760">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, many convolutional neural network (CNN) methods have been designed for hyperspectral image (HSI) classification since CNNs are able to produce good representations of data, which greatly benefits from a huge number of parameters. However, solving such a high-dimensional optimization problem often requires a large number of training samples in order to avoid overfitting. In addition, it is a typical nonconvex problem affected by many local minima and flat regions. To address these problems, in this article, we introduce the naive Gabor networks or Gabor-Nets that, for the first time in the literature, design and learn CNN kernels strictly in the form of Gabor filters, aiming to reduce the number of involved parameters and constrain the solution space and, hence, improve the performances of CNNs. Specifically, we develop an innovative phase-induced Gabor kernel, which is trickily designed to perform the Gabor feature learning via a linear combination of local low-frequency and high-frequency components of data controlled by the kernel phase. With the phase-induced Gabor kernel, the proposed Gabor-Nets gains the ability to automatically adapt to the local harmonic characteristics of the HSI data and, thus, yields more representative harmonic features. Also, this kernel can fulfill the traditional complex-valued Gabor filtering in a real-valued manner, hence making Gabor-Nets easily perform in a usual CNN thread. We evaluated our newly developed Gabor-Nets on three well-known HSIs, suggesting that our proposed Gabor-Nets can significantly improve the performance of CNNs, particularly with a small training set.},
  archive      = {J_TNNLS},
  author       = {Chenying Liu and Jun Li and Lin He and Antonio Plaza and Shutao Li and Bo Li},
  doi          = {10.1109/TNNLS.2020.2978760},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {376-390},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Naive gabor networks for hyperspectral image classification},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). 3-d quasi-recurrent neural network for hyperspectral image
denoising. <em>TNNLS</em>, <em>32</em>(1), 363–375. (<a
href="https://doi.org/10.1109/TNNLS.2020.2978756">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose an alternating directional 3-D quasi-recurrent neural network for hyperspectral image (HSI) denoising, which can effectively embed the domain knowledge-structural spatiospectral correlation and global correlation along spectrum (GCS). Specifically, 3-D convolution is utilized to extract structural spatiospectral correlation in an HSI, while a quasi-recurrent pooling function is employed to capture the GCS. Moreover, the alternating directional structure is introduced to eliminate the causal dependence with no additional computation cost. The proposed model is capable of modeling spatiospectral dependence while preserving the flexibility toward HSIs with an arbitrary number of bands. Extensive experiments on HSI denoising demonstrate significant improvement over the state-of-the-art under various noise settings, in terms of both restoration accuracy and computation time. Our code is available at https://github.com/Vandermode/QRNN3D.},
  archive      = {J_TNNLS},
  author       = {Kaixuan Wei and Ying Fu and Hua Huang},
  doi          = {10.1109/TNNLS.2020.2978756},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {363-375},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {3-D quasi-recurrent neural network for hyperspectral image denoising},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Effective and efficient batch normalization using a few
uncorrelated data for statistics estimation. <em>TNNLS</em>,
<em>32</em>(1), 348–362. (<a
href="https://doi.org/10.1109/TNNLS.2020.2978753">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks (DNNs) thrive in recent years, wherein batch normalization (BN) plays an indispensable role. However, it has been observed that BN is costly due to the huge reduction and elementwise operations that are hard to be executed in parallel, which heavily reduces the training speed. To address this issue, in this article, we propose a methodology to alleviate the BN’s cost by using only a few sampled or generated data for mean and variance estimation at each iteration. The key challenge to reach this goal is how to achieve a satisfactory balance between normalization effectiveness and execution efficiency. We identify that the effectiveness expects less data correlation in sampling while the efficiency expects more regular execution patterns. To this end, we design two categories of approach: sampling or creating a few uncorrelated data for statistics’ estimation with certain strategy constraints. The former includes “batch sampling (BS)” that randomly selects a few samples from each batch and “feature sampling (FS)” that randomly selects a small patch from each feature map of all samples, and the latter is “virtual data set normalization (VDN)” that generates a few synthetic random samples to directly create uncorrelated data for statistics’ estimation. Accordingly, multiway strategies are designed to reduce the data correlation for accurate estimation and optimize the execution pattern for running acceleration in the meantime. The proposed methods are comprehensively evaluated on various DNN models, where the loss of model accuracy and the convergence rate are negligible. Without the support of any specialized libraries, $1.98\times $ BN layer acceleration and 23.2\% overall training speedup can be practically achieved on modern GPUs. Furthermore, our methods demonstrate powerful performance when solving the well-known “micro-BN” problem in the case of a tiny batch size. This article provides a promising solution for the efficient training of high-performance DNNs.},
  archive      = {J_TNNLS},
  author       = {Zhaodong Chen and Lei Deng and Guoqi Li and Jiawei Sun and Xing Hu and Ling Liang and Yufei Ding and Yuan Xie},
  doi          = {10.1109/TNNLS.2020.2978753},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {348-362},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Effective and efficient batch normalization using a few uncorrelated data for statistics estimation},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Global and local knowledge-aware attention network for
action recognition. <em>TNNLS</em>, <em>32</em>(1), 334–347. (<a
href="https://doi.org/10.1109/TNNLS.2020.2978613">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks (CNNs) have shown an effective way to learn spatiotemporal representation for action recognition in videos. However, most traditional action recognition algorithms do not employ the attention mechanism to focus on essential parts of video frames that are relevant to the action. In this article, we propose a novel global and local knowledge-aware attention network to address this challenge for action recognition. The proposed network incorporates two types of attention mechanism called statistic-based attention (SA) and learning-based attention (LA) to attach higher importance to the crucial elements in each video frame. As global pooling (GP) models capture global information, while attention models focus on the significant details to make full use of their implicit complementary advantages, our network adopts a three-stream architecture, including two attention streams and a GP stream. Each attention stream employs a fusion layer to combine global and local information and produces composite features. Furthermore, global-attention (GA) regularization is proposed to guide two attention streams to better model dynamics of composite features with the reference to the global information. Fusion at the softmax layer is adopted to make better use of the implicit complementary advantages between SA, LA, and GP streams and get the final comprehensive predictions. The proposed network is trained in an end-to-end fashion and learns efficient video-level features both spatially and temporally. Extensive experiments are conducted on three challenging benchmarks, Kinetics, HMDB51, and UCF101, and experimental results demonstrate that the proposed network outperforms most state-of-the-art methods.},
  archive      = {J_TNNLS},
  author       = {Zhenxing Zheng and Gaoyun An and Dapeng Wu and Qiuqi Ruan},
  doi          = {10.1109/TNNLS.2020.2978613},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {334-347},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Global and local knowledge-aware attention network for action recognition},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Statistical loss and analysis for deep learning in
hyperspectral image classification. <em>TNNLS</em>, <em>32</em>(1),
322–333. (<a href="https://doi.org/10.1109/TNNLS.2020.2978577">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, deep learning methods, especially the convolutional neural networks (CNNs), have shown impressive performance on extracting abstract and high-level features from the hyperspectral image. However, the general training process of CNNs mainly considers the pixelwise information or the samples&#39; correlation to formulate the penalization while ignores the statistical properties especially the spectral variability of each class in the hyperspectral image. These sample-based penalizations would lead to the uncertainty of the training process due to the imbalanced and limited number of training samples. To overcome this problem, this article characterizes each class from the hyperspectral image as a statistical distribution and further develops a novel statistical loss with the distributions, not directly with samples for deep learning. Based on the Fisher discrimination criterion, the loss penalizes the sample variance of each class distribution to decrease the intraclass variance of the training samples. Moreover, an additional diversity-promoting condition is added to enlarge the interclass variance between different class distributions, and this could better discriminate samples from different classes in the hyperspectral image. Finally, the statistical estimation form of the statistical loss is developed with the training samples through multivariant statistical analysis. Experiments over the real-world hyperspectral images show the effectiveness of the developed statistical loss for deep learning.},
  archive      = {J_TNNLS},
  author       = {Zhiqiang Gong and Ping Zhong and Weidong Hu},
  doi          = {10.1109/TNNLS.2020.2978577},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {322-333},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Statistical loss and analysis for deep learning in hyperspectral image classification},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Extended robust exponential stability of fuzzy switched
memristive inertial neural networks with time-varying delays on
mode-dependent destabilizing impulsive control protocol. <em>TNNLS</em>,
<em>32</em>(1), 308–321. (<a
href="https://doi.org/10.1109/TNNLS.2020.2978542">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates the problem of robust exponential stability of fuzzy switched memristive inertial neural networks (FSMINNs) with time-varying delays on mode-dependent destabilizing impulsive control protocol. The memristive model presented here is treated as a switched system rather than employing the theory of differential inclusion and set-value map. To optimize the robust exponentially stable process and reduce the cost of time, hybrid mode-dependent destabilizing impulsive and adaptive feedback controllers are simultaneously applied to stabilize FSMINNs. In the new model, the multiple impulsive effects exist between two switched modes, and the multiple switched effects may also occur between two impulsive instants. Based on switched analysis techniques, the Takagi-Sugeno (T-S) fuzzy method, and the average dwell time, extended robust exponential stability conditions are derived. Finally, simulation is provided to illustrate the effectiveness of the results.},
  archive      = {J_TNNLS},
  author       = {Yongbin Yu and Xiangxiang Wang and Shouming Zhong and Nijing Yang and Nyima Tashi},
  doi          = {10.1109/TNNLS.2020.2978542},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {308-321},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Extended robust exponential stability of fuzzy switched memristive inertial neural networks with time-varying delays on mode-dependent destabilizing impulsive control protocol},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Diverse instance-weighting ensemble based on region drift
disagreement for concept drift adaptation. <em>TNNLS</em>,
<em>32</em>(1), 293–307. (<a
href="https://doi.org/10.1109/TNNLS.2020.2978523">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Concept drift refers to changes in the distribution of underlying data and is an inherent property of evolving data streams. Ensemble learning, with dynamic classifiers, has proved to be an efficient method of handling concept drift. However, the best way to create and maintain ensemble diversity with evolving streams is still a challenging problem. In contrast to estimating diversity via inputs, outputs, or classifier parameters, we propose a diversity measurement based on whether the ensemble members agree on the probability of a regional distribution change. In our method, estimations over regional distribution changes are used as instance weights. Constructing different region sets through different schemes will lead to different drift estimation results, thereby creating diversity. The classifiers that disagree the most are selected to maximize diversity. Accordingly, an instance-based ensemble learning algorithm, called the diverse instance-weighting ensemble (DiwE), is developed to address concept drift for data stream classification problems. Evaluations of various synthetic and real-world data stream benchmarks show the effectiveness and advantages of the proposed algorithm.},
  archive      = {J_TNNLS},
  author       = {Anjin Liu and Jie Lu and Guangquan Zhang},
  doi          = {10.1109/TNNLS.2020.2978523},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {293-307},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Diverse instance-weighting ensemble based on region drift disagreement for concept drift adaptation},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Generalized learning riemannian space quantization: A case
study on riemannian manifold of SPD matrices. <em>TNNLS</em>,
<em>32</em>(1), 281–292. (<a
href="https://doi.org/10.1109/TNNLS.2020.2978514">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning vector quantization (LVQ) is a simple and efficient classification method, enjoying great popularity. However, in many classification scenarios, such as electroencephalogram (EEG) classification, the input features are represented by symmetric positive-definite (SPD) matrices that live in a curved manifold rather than vectors that live in the flat Euclidean space. In this article, we propose a new classification method for data points that live in the curved Riemannian manifolds in the framework of LVQ. The proposed method alters generalized LVQ (GLVQ) with the Euclidean distance to the one operating under the appropriate Riemannian metric. We instantiate the proposed method for the Riemannian manifold of SPD matrices equipped with the Riemannian natural metric. Empirical investigations on synthetic data and real-world motor imagery EEG data demonstrate that the performance of the proposed generalized learning Riemannian space quantization can significantly outperform the Euclidean GLVQ, generalized relevance LVQ (GRLVQ), and generalized matrix LVQ (GMLVQ). The proposed method also shows competitive performance to the state-of-the-art methods on the EEG classification of motor imagery tasks.},
  archive      = {J_TNNLS},
  author       = {Fengzhen Tang and Mengling Fan and Peter Tiňo},
  doi          = {10.1109/TNNLS.2020.2978514},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {281-292},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Generalized learning riemannian space quantization: A case study on riemannian manifold of SPD matrices},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Structure-exploiting discriminative ordinal multioutput
regression. <em>TNNLS</em>, <em>32</em>(1), 266–280. (<a
href="https://doi.org/10.1109/TNNLS.2020.2978508">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although the least-squares regression (LSR) has achieved great success in regression tasks, its discriminating ability is limited since the margins between classes are not specially preserved. To mitigate this issue, dragging techniques have been introduced to remodel the regression targets of LSR. Such variants have gained certain performance improvement, but their generalization ability is still unsatisfactory when handling real data. This is because structure-related information, which is typically contained in the data, is not exploited. To overcome this shortcoming, in this article, we construct a multioutput regression model by exploiting the intraclass correlations and input-output relationships via a structure matrix. We also discriminatively enlarge the regression margins by embedding a metric that is guided automatically by the training data. To better handle such structured data with ordinal labels, we encode the model output as cumulative attributes and, hence, obtain our proposed model, termed structure-exploiting discriminative ordinal multioutput regression (SEDOMOR). In addition, to further enhance its distinguishing ability, we extend the SEDOMOR to its nonlinear counterparts with kernel functions and deep architectures. We also derive the corresponding optimization algorithms for solving these models and prove their convergence. Finally, extensive experiments have testified the effectiveness and superiority of the proposed methods.},
  archive      = {J_TNNLS},
  author       = {Qing Tian and Meng Cao and Songcan Chen and Hujun Yin},
  doi          = {10.1109/TNNLS.2020.2978508},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {266-280},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Structure-exploiting discriminative ordinal multioutput regression},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). PEPSI++: Fast and lightweight network for image inpainting.
<em>TNNLS</em>, <em>32</em>(1), 252–265. (<a
href="https://doi.org/10.1109/TNNLS.2020.2978501">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Among the various generative adversarial network (GAN)-based image inpainting methods, a coarse-to-fine network with a contextual attention module (CAM) has shown remarkable performance. However, due to two stacked generative networks, the coarse-to-fine network needs numerous computational resources, such as convolution operations and network parameters, which result in low speed. To address this problem, we propose a novel network architecture called parallel extended-decoder path for semantic inpainting (PEPSI) network, which aims at reducing the hardware costs and improving the inpainting performance. PEPSI consists of a single shared encoding network and parallel decoding networks called coarse and inpainting paths. The coarse path produces a preliminary inpainting result to train the encoding network for the prediction of features for the CAM. Simultaneously, the inpainting path generates higher inpainting quality using the refined features reconstructed via the CAM. In addition, we propose Diet-PEPSI that significantly reduces the network parameters while maintaining the performance. In Diet-PEPSI, to capture the global contextual information with low hardware costs, we propose novel rate-adaptive dilated convolutional layers that employ the common weights but produce dynamic features depending on the given dilation rates. Extensive experiments comparing the performance with state-of-the-art image inpainting methods demonstrate that both PEPSI and Diet-PEPSI improve the qualitative scores, i.e., the peak signal-to-noise ratio (PSNR) and structural similarity (SSIM), as well as significantly reduce hardware costs, such as computational time and the number of network parameters.},
  archive      = {J_TNNLS},
  author       = {Yong-Goo Shin and Min-Cheol Sagong and Yoon-Jae Yeo and Seung-Wook Kim and Sung-Jea Ko},
  doi          = {10.1109/TNNLS.2020.2978501},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {252-265},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {PEPSI++: Fast and lightweight network for image inpainting},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Stability and stabilization in probability of probabilistic
boolean networks. <em>TNNLS</em>, <em>32</em>(1), 241–251. (<a
href="https://doi.org/10.1109/TNNLS.2020.2978345">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article studies the stability in probability of probabilistic Boolean networks and stabilization in the probability of probabilistic Boolean control networks. To simulate more realistic cellular systems, the probability of stability/stabilization is not required to be a strict one. In this situation, the target state is indefinite to have a probability of transferring to itself. Thus, it is a challenging extension of the traditional probability-one problem, in which the self-transfer probability of the target state must be one. Some necessary and sufficient conditions are proposed via the semitensor product of matrices. Illustrative examples are also given to show the effectiveness of the derived results.},
  archive      = {J_TNNLS},
  author       = {Chi Huang and Jianquan Lu and Guisheng Zhai and Jinde Cao and Guoping Lu and Matjaž Perc},
  doi          = {10.1109/TNNLS.2020.2978345},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {241-251},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Stability and stabilization in probability of probabilistic boolean networks},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multiple and complete stability of recurrent neural networks
with sinusoidal activation function. <em>TNNLS</em>, <em>32</em>(1),
229–240. (<a href="https://doi.org/10.1109/TNNLS.2020.2978267">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents new theoretical results on multistability and complete stability of recurrent neural networks with a sinusoidal activation function. Sufficient criteria are provided for ascertaining the stability of recurrent neural networks with various numbers of equilibria, such as a unique equilibrium, finite, and countably infinite numbers of equilibria. Multiple exponential stability criteria of equilibria are derived, and the attraction basins of equilibria are estimated. Furthermore, criteria for complete stability and instability of equilibria are derived for recurrent neural networks without time delay. In contrast to the existing stability results with a finite number of equilibria, the new criteria, herein, are applicable for both finite and countably infinite numbers of equilibria. Two illustrative examples with finite and countably infinite numbers of equilibria are elaborated to substantiate the results.},
  archive      = {J_TNNLS},
  author       = {Peng Liu and Jun Wang and Zhenyuan Guo},
  doi          = {10.1109/TNNLS.2020.2978267},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {229-240},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multiple and complete stability of recurrent neural networks with sinusoidal activation function},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Look more into occlusion: Realistic face frontalization and
recognition with BoostGAN. <em>TNNLS</em>, <em>32</em>(1), 214–228. (<a
href="https://doi.org/10.1109/TNNLS.2020.2978127">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many factors can affect face recognition, such as occlusion, pose, aging, and illumination. First and foremost are occlusion and large-pose problems, which may even lead to more than 10\% accuracy degradation. Recently, generative adversarial net (GAN) and its variants have been proved to be effective in processing pose and occlusion. For the former, pose-invariant feature representation and face frontalization based on GAN models have been studied to solve the pose variation problem. For the latter, frontal face completion on occlusions based on GAN models have also been presented, which is much concerned with facial structure and realistic pixel details rather than identity preservation. However, synthesizing and recognizing the occluded but profile faces is still an understudied problem. Therefore, in this article, to address this problem, we contribute an efficient but effective solution on how to synthesize and recognize faces with large-pose variations and simultaneously corrupted regions (e.g., nose and eyes). Specifically, we propose a boosting GAN (BoostGAN) for occluded but profile face frontalization, deocclusion, and recognition, which has two aspects: 1) with the assumption that face occlusion is incomplete and partial, multiple images with patch occlusion are fed into our model for knowledge boosting, i.e., identity and texture information and 2) a new aggregation structure integrated with a deep encoder–decoder network for coarse face synthesis and a boosting network for fine face generation is carefully designed. Exhaustive experiments on benchmark data sets with regular and irregular occlusions demonstrate that the proposed model not only shows clear photorealistic images but also presents powerful recognition performance over state-of-the-art GAN models for occlusive but profile face recognition in both the controlled and uncontrolled environments. To the best of our knowledge, this article proposes to solve face synthesis and recognition under poses and occlusions for the first time.},
  archive      = {J_TNNLS},
  author       = {Qingyan Duan and Lei Zhang},
  doi          = {10.1109/TNNLS.2020.2978127},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {214-228},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Look more into occlusion: Realistic face frontalization and recognition with BoostGAN},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). SCM: Spatially coherent matching with gaussian field
learning for nonrigid point set registration. <em>TNNLS</em>,
<em>32</em>(1), 203–213. (<a
href="https://doi.org/10.1109/TNNLS.2020.2978031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While point set registration has been studied in many areas of computer vision for decades, registering points encountering different degradations remains a challenging problem. In this article, we introduce a robust point pattern matching method, termed spatially coherent matching (SCM). The SCM algorithm consists of recovering correspondences and learning nonrigid transformations between the given model and scene point sets while preserving the local neighborhood structure. Precisely, the proposed SCM starts with the initial matches that are contaminated by degradations (e.g., deformation, noise, occlusion, rotation, multiview, and outliers), and the main task is to recover the underlying correspondences and learn the nonrigid transformation alternately. Based on unsupervised manifold learning, the challenging problem of point set registration can be formulated by the Gaussian fields criterion under a local preserving constraint, where the neighborhood structure could be preserved in each transforming. Moreover, the nonrigid transformation is modeled in a reproducing kernel Hilbert space, and we use a kernel approximation strategy to boost efficiency. Experimental results demonstrate that the proposed approach robustly rejecting mismatches and registers complex point set pairs containing large degradations.},
  archive      = {J_TNNLS},
  author       = {Gang Wang and Yufei Chen},
  doi          = {10.1109/TNNLS.2020.2978031},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {203-213},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {SCM: Spatially coherent matching with gaussian field learning for nonrigid point set registration},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Video SAR imaging based on low-rank tensor recovery.
<em>TNNLS</em>, <em>32</em>(1), 188–202. (<a
href="https://doi.org/10.1109/TNNLS.2020.2978017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to its ability of forming continuous images for a ground scene of interest, the video synthetic aperture radar (SAR) has been studied in recent years. However, as video SAR needs to reconstruct many frames, the data are of enormous amount and the imaging process is of large computational cost, which limits its applications. In this article, we exploit the redundancy property of multiframe video SAR data, which can be modeled as low-rank tensor, and formulate the video SAR imaging process as a low-rank tensor recovery problem, which is solved by an efficient alternating minimization method. We empirically compare the proposed method with several state-of-the-art video SAR imaging algorithms, including the fast back-projection (FBP) method and the compressed sensing (CS)-based method. Experiments on both simulated and real data show that the proposed low-rank tensor-based method requires significantly less amount of data samples while achieving similar or better imaging performance.},
  archive      = {J_TNNLS},
  author       = {Wei Pu and Xiaodong Wang and Junjie Wu and Yulin Huang and Jianyu Yang},
  doi          = {10.1109/TNNLS.2020.2978017},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {188-202},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Video SAR imaging based on low-rank tensor recovery},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multistability of fractional-order neural networks with
unbounded time-varying delays. <em>TNNLS</em>, <em>32</em>(1), 177–187.
(<a href="https://doi.org/10.1109/TNNLS.2020.2977994">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article addresses the multistability and attraction of fractional-order neural networks (FONNs) with unbounded time-varying delays. Several sufficient conditions are given to ensure the coexistence of equilibrium points (EPs) of FONNs with concave-convex activation functions. Moreover, by exploiting the analytical method and the property of the Mittag-Leffler function, it is shown that the multiple Mittag-Leffler stability of delayed FONNs is derived and the obtained criteria do not depend on differentiable time-varying delays. In particular, the criterion of the Mittag-Leffler stability can be simplified to M-matrix. In addition, the estimation of attraction basin of delayed FONNs is studied, which implies that the extension of attraction basin is independent of the magnitude of delays. Finally, three numerical examples are given to show the validity of the theoretical results.},
  archive      = {J_TNNLS},
  author       = {Fanghai Zhang and Zhigang Zeng},
  doi          = {10.1109/TNNLS.2020.2977994},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {177-187},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multistability of fractional-order neural networks with unbounded time-varying delays},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning with stochastic guidance for robot navigation.
<em>TNNLS</em>, <em>32</em>(1), 166–176. (<a
href="https://doi.org/10.1109/TNNLS.2020.2977924">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the sparse rewards and high degree of environmental variation, reinforcement learning approaches, such as deep deterministic policy gradient (DDPG), are plagued by issues of high variance when applied in complex real-world environments. We present a new framework for overcoming these issues by incorporating a stochastic switch, allowing an agent to choose between high- and low-variance policies. The stochastic switch can be jointly trained with the original DDPG in the same framework. In this article, we demonstrate the power of the framework in a navigation task, where the robot can dynamically choose to learn through exploration or to use the output of a heuristic controller as guidance. Instead of starting from completely random actions, the navigation capability of a robot can be quickly bootstrapped by several simple independent controllers. The experimental results show that with the aid of stochastic guidance, we are able to effectively and efficiently train DDPG navigation policies and achieve significantly better performance than state-of-the-art baseline models.},
  archive      = {J_TNNLS},
  author       = {Linhai Xie and Yishu Miao and Sen Wang and Phil Blunsom and Zhihua Wang and Changhao Chen and Andrew Markham and Niki Trigoni},
  doi          = {10.1109/TNNLS.2020.2977924},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {166-176},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Learning with stochastic guidance for robot navigation},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Exponential synchronization of delayed memristor-based
uncertain complex-valued neural networks for image protection.
<em>TNNLS</em>, <em>32</em>(1), 151–165. (<a
href="https://doi.org/10.1109/TNNLS.2020.2977614">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article solves the exponential synchronization issue of memristor-based complex-valued neural networks (MCVNNs) with time-varying uncertainties via feedback control. Compared with the traditional control methods, a more practical and general control scheme with the available uncertain information of the parameters is newly developed for MCVNNs. Our approach considers the proposed neural networks as two dynamic real-valued systems. Then, the less conservative exponential synchronization criteria are proposed by incorporating the framework of the Lyapunov method and inequality techniques. Under the proposed algorithm, not only can the stability of MCVNNs be guaranteed but also the behavior of such a system is appropriate for image protection. Meanwhile, the sensitive measure of the encryption and decryption can be converted into synchronization error. When monitoring the secure mechanism as a whole, the influence of error feasible domain on image decryption is analyzed. Simulation examples are provided to verify the efficacy of the proposed synchronization criterion and the results of practical application on image protection.},
  archive      = {J_TNNLS},
  author       = {Manman Yuan and Weiping Wang and Zhen Wang and Xiong Luo and Jürgen Kurths},
  doi          = {10.1109/TNNLS.2020.2977614},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {151-165},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Exponential synchronization of delayed memristor-based uncertain complex-valued neural networks for image protection},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Continual multiview task learning via deep matrix
factorization. <em>TNNLS</em>, <em>32</em>(1), 139–150. (<a
href="https://doi.org/10.1109/TNNLS.2020.2977497">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The state-of-the-art multitask multiview (MTMV) learning tackles a scenario where multiple tasks are related to each other via multiple shared feature views. However, in many real-world scenarios where a sequence of the multiview task comes, the higher storage requirement and computational cost of retraining previous tasks with MTMV models have presented a formidable challenge for this lifelong learning scenario. To address this challenge, in this article, we propose a new continual multiview task learning model that integrates deep matrix factorization and sparse subspace learning in a unified framework, which is termed deep continual multiview task learning (DCMvTL). More specifically, as a new multiview task arrives, DCMvTL first adopts a deep matrix factorization technique to capture hidden and hierarchical representations for this new coming multiview task while accumulating the fresh multiview knowledge in a layerwise manner. Then, a sparse subspace learning model is employed for the extracted factors at each layer and further reveals cross-view correlations via a self-expressive constraint. For model optimization, we derive a general multiview learning formulation when a new multiview task comes and apply an alternating minimization strategy to achieve lifelong learning. Extensive experiments on benchmark data sets demonstrate the effectiveness of our proposed DCMvTL model compared with the existing state-of-the-art MTMV and lifelong multiview task learning models.},
  archive      = {J_TNNLS},
  author       = {Gan Sun and Yang Cong and Yulun Zhang and Guoshuai Zhao and Yun Fu},
  doi          = {10.1109/TNNLS.2020.2977497},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {139-150},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Continual multiview task learning via deep matrix factorization},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Gradients cannot be tamed: Behind the impossible paradox of
blocking targeted adversarial attacks. <em>TNNLS</em>, <em>32</em>(1),
128–138. (<a href="https://doi.org/10.1109/TNNLS.2020.2977142">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite their accuracy, neural network-based classifiers are still prone to manipulation through adversarial perturbations. These perturbations are designed to be misclassified by the neural network while being perceptually identical to some valid inputs. The vast majority of such attack methods rely on white-box conditions, where the attacker has full knowledge of the attacked network&#39;s parameters. This allows the attacker to calculate the network&#39;s loss gradient with respect to some valid inputs and use this gradient in order to create an adversarial example. The task of blocking white-box attacks has proved difficult to address. While many defense methods have been suggested, they have had limited success. In this article, we examine this difficulty and try to understand it. We systematically explore the capabilities and limitations of defensive distillation, one of the most promising defense mechanisms against adversarial perturbations suggested so far, in order to understand this defense challenge. We show that contrary to commonly held belief, the ability to bypass defensive distillation is not dependent on an attack&#39;s level of sophistication. In fact, simple approaches, such as the targeted gradient sign method, are capable of effectively bypassing defensive distillation. We prove that defensive distillation is highly effective against nontargeted attacks but is unsuitable for targeted attacks. This discovery led to our realization that targeted attacks leverage the same input gradient that allows a network to be trained. This implies that blocking them comes at the cost of losing the network&#39;s ability to learn, presenting an impossible tradeoff to the research community.},
  archive      = {J_TNNLS},
  author       = {Ziv Katzir and Yuval Elovici},
  doi          = {10.1109/TNNLS.2020.2977142},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {128-138},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Gradients cannot be tamed: Behind the impossible paradox of blocking targeted adversarial attacks},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A neural network-based joint prognostic model for data
fusion and remaining useful life prediction. <em>TNNLS</em>,
<em>32</em>(1), 117–127. (<a
href="https://doi.org/10.1109/TNNLS.2020.2977132">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of sensor and information technology, now multisensor data relating to the system degradation process are readily available for condition monitoring and remaining useful life (RUL) prediction. The traditional data fusion and RUL prediction methods are either not flexible enough to capture the highly nonlinear relationship between the health condition and the multisensor data or have not fully utilized the past observations to capture the degradation trajectory. In this article, we propose a joint prognostic model (JPM), where Bayesian linear models are developed for multisensor data, and an artificial neural network is proposed to model the nonlinear relationship between the residual life, the model parameters of each sensor data, and the observation epoch. A Bayesian updating scheme is developed to calculate the posterior distributions of the model parameters of each sensor data, which are further used to estimate the posterior predictive distributions of the residual life. The effectiveness and advantages of the proposed JPM are demonstrated using the commercial modular aero-propulsion system simulation data set.},
  archive      = {J_TNNLS},
  author       = {Yuanyuan Gao and Yuxin Wen and Jianguo Wu},
  doi          = {10.1109/TNNLS.2020.2977132},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {117-127},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A neural network-based joint prognostic model for data fusion and remaining useful life prediction},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Global exponential synchronization of coupled delayed
memristive neural networks with reaction–diffusion terms via distributed
pinning controls. <em>TNNLS</em>, <em>32</em>(1), 105–116. (<a
href="https://doi.org/10.1109/TNNLS.2020.2977099">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents new theoretical results on global exponential synchronization of nonlinear coupled delayed memristive neural networks with reaction-diffusion terms and Dirichlet boundary conditions. First, a state-dependent memristive neural network model is introduced in terms of coupled partial differential equations. Next, two control schemes are introduced: distributed state feedback pinning control and distributed impulsive pinning control. A salient feature of these two pinning control schemes is that only partial information on the neighbors of pinned nodes is needed. By utilizing the Lyapunov stability theorem and Divergence theorem, sufficient criteria are derived to ascertain the global exponential synchronization of coupled neural networks via the two pining control schemes. Finally, two illustrative examples are elaborated to substantiate the theoretical results and demonstrate the advantages and disadvantages of the two control schemes.},
  archive      = {J_TNNLS},
  author       = {Zhenyuan Guo and Shiqin Wang and Jun Wang},
  doi          = {10.1109/TNNLS.2020.2977099},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {105-116},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Global exponential synchronization of coupled delayed memristive neural networks with Reaction–Diffusion terms via distributed pinning controls},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptive critic learning for constrained optimal
event-triggered control with discounted cost. <em>TNNLS</em>,
<em>32</em>(1), 91–104. (<a
href="https://doi.org/10.1109/TNNLS.2020.2976787">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article studies an optimal event-triggered control (ETC) problem of nonlinear continuous-time systems subject to asymmetric control constraints. The present nonlinear plant differs from many studied systems in that its equilibrium point is nonzero. First, we introduce a discounted cost for such a system in order to obtain the optimal ETC without making coordinate transformations. Then, we present an event-triggered Hamilton-Jacobi-Bellman equation (ET-HJBE) arising in the discounted-cost constrained optimal ETC problem. After that, we propose an event-triggering condition guaranteeing a positive lower bound for the minimal intersample time. To solve the ET-HJBE, we construct a critic network under the framework of adaptive critic learning. The critic network weight vector is tuned through a modified gradient descent method, which simultaneously uses historical and instantaneous state data. By employing the Lyapunov method, we prove that the uniform ultimate boundedness of all signals in the closed-loop system is guaranteed. Finally, we provide simulations of a pendulum system and an oscillator system to validate the obtained optimal ETC strategy.},
  archive      = {J_TNNLS},
  author       = {Xiong Yang and Qinglai Wei},
  doi          = {10.1109/TNNLS.2020.2976787},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {91-104},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive critic learning for constrained optimal event-triggered control with discounted cost},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). PoPPL: Pedestrian trajectory prediction by LSTM with
automatic route class clustering. <em>TNNLS</em>, <em>32</em>(1), 77–90.
(<a href="https://doi.org/10.1109/TNNLS.2020.2975837">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pedestrian path prediction is a very challenging problem because scenes are often crowded or contain obstacles. Existing state-of-the-art long short-term memory (LSTM)-based prediction methods have been mainly focused on analyzing the influence of other people in the neighborhood of each pedestrian while neglecting the role of potential destinations in determining a walking path. In this article, we propose classifying pedestrian trajectories into a number of route classes (RCs) and using them to describe the pedestrian movement patterns. Based on the RCs obtained from trajectory clustering, our algorithm, which we name the prediction of pedestrian paths by LSTM (PoPPL), predicts the destination regions through a bidirectional LSTM classification network in the first stage and then generates trajectories corresponding to the predicted destination regions through one of the three proposed LSTM-based architectures in the second stage. Our algorithm also outputs probabilities of multiple predicted trajectories that head toward the destination regions. We have evaluated PoPPL against other state-of-the-art methods on two public data sets. The results show that our algorithm outperforms other methods and incorporating potential destination prediction improves the trajectory prediction accuracy.},
  archive      = {J_TNNLS},
  author       = {Hao Xue and Du Q. Huynh and Mark Reynolds},
  doi          = {10.1109/TNNLS.2020.2975837},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {77-90},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {PoPPL: Pedestrian trajectory prediction by LSTM with automatic route class clustering},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Iterative weighted group thresholding method for group
sparse recovery. <em>TNNLS</em>, <em>32</em>(1), 63–76. (<a
href="https://doi.org/10.1109/TNNLS.2020.2975302">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a novel iterative weighted group thresholding method for group sparse recovery of signals from underdetermined linear systems. Based on an equivalent weighted group minimization problem with ℓ p p -norm (0 &lt;; p ≤ 1), we derive closed-form solutions for a subproblem with respect to some specific values of p when using the proximal gradient method. Then, we design the corresponding algorithmic framework, including stopping criterion and the method of nonmonotone line search, and prove that the solution sequence generated by the proposed algorithm converges under some mild conditions. Moreover, based on the proposed algorithm, we develop a homotopy algorithm with an adaptively updated group threshold. Extensive computational experiments on the simulated and real data show that our approach is competitive with state-of-the-art methods in terms of exact group selection, estimation accuracy, and computation time.},
  archive      = {J_TNNLS},
  author       = {Lanfan Jiang and Wenxing Zhu},
  doi          = {10.1109/TNNLS.2020.2975302},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {63-76},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Iterative weighted group thresholding method for group sparse recovery},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multitask learning and reinforcement learning for
personalized dialog generation: An empirical study. <em>TNNLS</em>,
<em>32</em>(1), 49–62. (<a
href="https://doi.org/10.1109/TNNLS.2020.2975035">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Open-domain dialog generation, which is a crucial component of artificial intelligence, is an essential and challenging problem. In this article, we present a personalized dialog system, which leverages the advantages of multitask learning and reinforcement learning for personalized dialogue generation (MRPDG). Specifically, MRPDG consists of two subtasks: 1) an author profiling module that recognizes user characteristics from the input sentence (auxiliary task) and 2) a personalized dialog generation system that generates informative, grammatical, and coherent responses with reinforcement learning algorithms (primary task). Three kinds of rewards are proposed to generate high-quality conversations. We investigate the effectiveness of three widely used reinforcement learning methods [i.e., Q-learning, policy gradient, and actor-critic (AC) algorithm] in a personalized dialog generation system and demonstrate that the AC algorithm achieves the best results on the underlying framework. Comprehensive experiments are conducted to evaluate the performance of the proposed model on two real-life data sets. Experimental results illustrate that MRPDG is able to produce high-quality personalized dialogs for users with different characteristics. Quantitatively, the proposed model can achieve better performance than the compared methods across different evaluation metrics, such as the human evaluation, BiLingual Evaluation Understudy (BLEU), and perplexity.},
  archive      = {J_TNNLS},
  author       = {Min Yang and Weiyi Huang and Wenting Tu and Qiang Qu and Ying Shen and Kai Lei},
  doi          = {10.1109/TNNLS.2020.2975035},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {49-62},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multitask learning and reinforcement learning for personalized dialog generation: An empirical study},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A two-timescale duplex neurodynamic approach to
mixed-integer optimization. <em>TNNLS</em>, <em>32</em>(1), 36–48. (<a
href="https://doi.org/10.1109/TNNLS.2020.2973760">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a two-timescale duplex neurodynamic approach to mixed-integer optimization, based on a biconvex optimization problem reformulation with additional bilinear equality or inequality constraints. The proposed approach employs two recurrent neural networks operating concurrently at two timescales. In addition, particle swarm optimization is used to update the initial neuronal states iteratively to escape from local minima toward better initial states. In spite of its minimal system complexity, the approach is proven to be almost surely convergent to optimal solutions. Its superior performance is substantiated via solving five benchmark problems.},
  archive      = {J_TNNLS},
  author       = {Hangjun Che and Jun Wang},
  doi          = {10.1109/TNNLS.2020.2973760},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {36-48},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A two-timescale duplex neurodynamic approach to mixed-integer optimization},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning student networks via feature embedding.
<em>TNNLS</em>, <em>32</em>(1), 25–35. (<a
href="https://doi.org/10.1109/TNNLS.2020.2970494">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep convolutional neural networks have been widely used in numerous applications, but their demanding storage and computational resource requirements prevent their applications on mobile devices. Knowledge distillation aims to optimize a portable student network by taking the knowledge from a well-trained heavy teacher network. Traditional teacher-student-based methods used to rely on additional fully connected layers to bridge intermediate layers of teacher and student networks, which brings in a large number of auxiliary parameters. In contrast, this article aims to propagate information from teacher to student without introducing new variables that need to be optimized. We regard the teacher-student paradigm from a new perspective of feature embedding. By introducing the locality preserving loss, the student network is encouraged to generate the low-dimensional features that could inherit intrinsic properties of their corresponding high-dimensional features from the teacher network. The resulting portable network, thus, can naturally maintain the performance as that of the teacher network. Theoretical analysis is provided to justify the lower computation complexity of the proposed method. Experiments on benchmark data sets and well-trained networks suggest that the proposed algorithm is superior to state-of-the-art teacher-student learning methods in terms of computational and storage complexity.},
  archive      = {J_TNNLS},
  author       = {Hanting Chen and Yunhe Wang and Chang Xu and Chao Xu and Dacheng Tao},
  doi          = {10.1109/TNNLS.2020.2970494},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {25-35},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Learning student networks via feature embedding},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A comprehensive survey on graph neural networks.
<em>TNNLS</em>, <em>32</em>(1), 4–24. (<a
href="https://doi.org/10.1109/TNNLS.2020.2978386">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning has revolutionized many machine learning tasks in recent years, ranging from image classification and video processing to speech recognition and natural language understanding. The data in these tasks are typically represented in the Euclidean space. However, there is an increasing number of applications, where data are generated from non-Euclidean domains and are represented as graphs with complex relationships and interdependency between objects. The complexity of graph data has imposed significant challenges on the existing machine learning algorithms. Recently, many studies on extending deep learning approaches for graph data have emerged. In this article, we provide a comprehensive overview of graph neural networks (GNNs) in data mining and machine learning fields. We propose a new taxonomy to divide the state-of-the-art GNNs into four categories, namely, recurrent GNNs, convolutional GNNs, graph autoencoders, and spatial-temporal GNNs. We further discuss the applications of GNNs across various domains and summarize the open-source codes, benchmark data sets, and model evaluation of GNNs. Finally, we propose potential research directions in this rapidly growing field.},
  archive      = {J_TNNLS},
  author       = {Zonghan Wu and Shirui Pan and Fengwen Chen and Guodong Long and Chengqi Zhang and Philip S. Yu},
  doi          = {10.1109/TNNLS.2020.2978386},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {4-24},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A comprehensive survey on graph neural networks},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Editorial: Staying healthy and strong together.
<em>TNNLS</em>, <em>32</em>(1), 2–3. (<a
href="https://doi.org/10.1109/TNNLS.2020.3040629">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {“Happy New Year!” As you open this January 2021 issue of IEEE T ransactions on N eural N etworks and L earning S ystems (IEEE TNNLS), I hope everyone enjoyed a healthy and happy holiday season!},
  archive      = {J_TNNLS},
  author       = {Haibo He},
  doi          = {10.1109/TNNLS.2020.3040629},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {2-3},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Editorial: Staying healthy and strong together},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
</ul>

</body>
</html>
