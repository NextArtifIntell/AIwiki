<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>THMS_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="thms---77">THMS - 77</h2>
<ul>
<li><details>
<summary>
(2021a). IEEE systems, man, and cybernetics society information.
<em>THMS</em>, <em>51</em>(6), C3. (<a
href="https://doi.org/10.1109/THMS.2021.3125211">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Provides a listing of current committee members and society officers.},
  archive      = {J_THMS},
  doi          = {10.1109/THMS.2021.3125211},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {12},
  number       = {6},
  pages        = {C3},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {IEEE systems, man, and cybernetics society information},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Delay time fractional-order model for the soft exoskeleton
glove control. <em>THMS</em>, <em>51</em>(6), 740–745. (<a
href="https://doi.org/10.1109/THMS.2021.3107491">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The biomechanical system of the soft exoskeleton glove is studied in this letter. In this context, several aspects have been treated. The delay time model with fractional-order exponent ${\boldsymbol{\beta }} = 1.5$ for a soft exoskeleton of a patient that suffered a cerebrovascular accident is proposed. The model contains two major elements of uncertainty: the first element is represented by the time delay that corresponds to the reaction time of the human factor and the second element is given by the disturbing muscular torque of the patient&#39;s hand. A robust control system is studied to ensure the desired performances of the tracking error relative to a reference position. The stability of the control system is demonstrated by the Lyapunov techniques. The control method and the resulting algorithms are implemented in the research stage of the soft EXOSKELETON-GLOVE (EX-GLOVE) model. The numerical simulations and experimental tests demonstrated the correctness of the proposed solutions.},
  archive      = {J_THMS},
  author       = {Mircea Ivanescu and Nirvana Popescu and Decebal Popescu},
  doi          = {10.1109/THMS.2021.3107491},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {12},
  number       = {6},
  pages        = {740-745},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Delay time fractional-order model for the soft exoskeleton glove control},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Load asymmetry angle estimation using multiple-view videos.
<em>THMS</em>, <em>51</em>(6), 734–739. (<a
href="https://doi.org/10.1109/THMS.2021.3112962">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A robust computer vision-based approach is developed to estimate the load asymmetry angle defined in the revised NIOSH lifting equation. The angle of asymmetry enables the computation of a recommended weight limit for repetitive lifting operations in a workplace to prevent lower back injuries. An open-source package OpenPose is applied to estimate the two-dimensional (2-D) locations of skeletal joints of the worker from two synchronous videos. Combining these joint location estimates, a computer vision correspondence and depth estimation method is developed to estimate the 3-D coordinates of skeletal joints during lifting. The angle of asymmetry is then deduced from a subset of these 3-D positions. Error analysis reveals unreliable angle estimates due to occlusions of upper limbs. A robust angle estimation method that mitigates this challenge is developed. We propose a method to flag unreliable angle estimates based on the average confidence level of 2-D joint estimates provided by OpenPose. An optimal threshold is derived that balances the percentage variance reduction of the estimation error and the percentage of angle estimates flagged. Tested with 360 lifting instances in a NIOSH-provided dataset, the standard deviation of angle estimation error is reduced from 10.13° to 4.99°. To realize this error variance reduction, 34% of estimated angles are flagged and require further validation.},
  archive      = {J_THMS},
  author       = {Xuan Wang and Yu Hen Hu and Ming-Lun Lu and Robert G. Radwin},
  doi          = {10.1109/THMS.2021.3112962},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {12},
  number       = {6},
  pages        = {734-739},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Load asymmetry angle estimation using multiple-view videos},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A systematic review on motor-imagery
brain-connectivity-based computer interfaces. <em>THMS</em>,
<em>51</em>(6), 725–733. (<a
href="https://doi.org/10.1109/THMS.2021.3115094">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This review article discusses the definition and implementation of brain–computer interface (BCI) system relying on brain connectivity (BC) and machine learning/deep learning (DL) for motor imagery (MI)-based applications. During the past few years, many approaches have been explored in terms of types of neurological sources of information, feature extraction, and intention prediction for BCI applications. Two novel aspects are becoming increasingly interesting for the BCI community: BC modeling and DL. The former aims at describing the interactions among different brain regions as connectivity patterns that reflect the dynamics of information flow either at rest or when performing a task. The latter is becoming pervasive for its capability of modeling and predicting complex data, where a huge amount of information is involved. In this scenario, we conducted a systematic literature review on BCI studies that led to the selection of 34 articles meeting all the required criteria. This provides evidence of the rapid growth of the topic over the past few years, though being still in its infancy. The last part of this article is dedicated to this new frontier of BCI that we call MI BC-based computer interfaces highlighting the potential of BC features. This, jointly with DL as enabling technology, has the potential of improving the performance of electroencephalography-based systems.},
  archive      = {J_THMS},
  author       = {Lorenza Brusini and Francesca Stival and Francesco Setti and Emanuele Menegatti and Gloria Menegaz and Silvia Francesca Storti},
  doi          = {10.1109/THMS.2021.3115094},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {12},
  number       = {6},
  pages        = {725-733},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {A systematic review on motor-imagery brain-connectivity-based computer interfaces},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Influencing human escape maneuvers with perceptual cues in
the presence of a visual task. <em>THMS</em>, <em>51</em>(6), 715–724.
(<a href="https://doi.org/10.1109/THMS.2021.3108962">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual engagement is common in many situations where human operators must perform tasks in challenging environments. This visual engagement has the potential to impact the safety of these operators when dealing with dynamic threats. Perceptual cues have been shown to elicit physical evasion maneuvers, thereby improving safety. In this article, we investigate the effects of cues and visual engagement on rapid whole-body responses. The visual task, inspired by the Trail Making Test (TMT), served as a proxy for visual engagement in the real world. Our continuous TMT minigame and threat simulation are implemented in a virtual reality environment. Participants attempt to maximize their performance score by quickly solving TMTs and dodging dynamic threats from various in-plane directions. They are provided with no cues (control), visual cues, and vibrotactile cues indicating impending threat directions. Participant&#39;s ability to dodge threats is quantified by failure rate and reaction time within field of view and for all approach directions. An index of difficulty highlighted perceptual cue response sensitivity to varying threat speeds and sizes. This article provides two core key contributions and other interesting findings: 1) the results illustrate that tactile cues enable statistically significantly better dodging rates than visual cues or with human vision alone (control condition); and 2) that visual engagement degrades human evasion performance in a statistically significant way. Finally, tactile cue responses appear to be less sensitive than visual cues to visually engaging tasks within the higher portion of difficulty index range that is investigated.},
  archive      = {J_THMS},
  author       = {Aakash Bajpai and Karen M. Feigh and Anirban Mazumdar and Aaron J. Young},
  doi          = {10.1109/THMS.2021.3108962},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {12},
  number       = {6},
  pages        = {715-724},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Influencing human escape maneuvers with perceptual cues in the presence of a visual task},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Individualized mutual adaptation in human-agent teams.
<em>THMS</em>, <em>51</em>(6), 706–714. (<a
href="https://doi.org/10.1109/THMS.2021.3107675">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ability to collaborate with previously unseen human teammates is crucial for artificial agents to be effective in human-agent teams (HATs). Due to individual differences and complex team dynamics, it is hard to develop a single agent policy to match all potential teammates. In this article, we study both human-human and HAT in a dyadic cooperative task, Team Space Fortress. Results show that the team performance is influenced by both players’ individual skill level and their ability to collaborate with different teammates by adopting complementary policies. Based on human-human team results, we propose an adaptive agent that identifies different human policies and assigns a complementary partner policy to optimize team performance. The adaptation method relies on a novel similarity metric to infer human policy and then selects the most complementary policy from a pretrained library of exemplar policies. We conducted human-agent experiments to evaluate the adaptive agent and examine mutual adaptation in HAT. Results show that both human adaptation and agent adaptation contribute to team performance.},
  archive      = {J_THMS},
  author       = {Huao Li and Tianwei Ni and Siddharth Agrawal and Fan Jia and Suhas Raja and Yikang Gui and Dana Hughes and Michael Lewis and Katia Sycara},
  doi          = {10.1109/THMS.2021.3107675},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {12},
  number       = {6},
  pages        = {706-714},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Individualized mutual adaptation in human-agent teams},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Exploration of teammate trust and interaction dynamics in
human-autonomy teaming. <em>THMS</em>, <em>51</em>(6), 696–705. (<a
href="https://doi.org/10.1109/THMS.2021.3115058">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article considers human-autonomy teams (HATs) in which two human team members interact and collaborate with an autonomous teammate to achieve a common task while dealing with unexpected technological failures that were imposed either in automation or autonomy. A Wizard of Oz methodology is used to simulate the autonomous teammate. One of the critical aspects of HAT performance is the trust that develops over time as team members interact with each other in a dynamic task environment. For this reason, it is important to examine the dynamic nature of teammate trust through real-time measures of team interactions. This article examines team interaction and trust to understand better how they change under automation and autonomy failures. Thus, we address two research questions: 1) How does trust in HATs evolve over time?; and 2) How is the relationship between team interaction and trust impacted by the failures? We hypothesize that trust in HATs will decrease as autonomy failures increase. We also hypothesize that team interaction would be related to the development of trust and recovery from the failures. The results implicate three general trends: 1) team interaction dynamics are linked to the development of trust in HATs; 2) trust in the autonomous teammate is only associated with recovery from autonomy failures; 3) team interaction dynamics are related to both automation and autonomy failure recovery.},
  archive      = {J_THMS},
  author       = {Mustafa Demir and Nathan J. McNeese and Jaime C. Gorman and Nancy J. Cooke and Christopher W. Myers and David A. Grimm},
  doi          = {10.1109/THMS.2021.3115058},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {12},
  number       = {6},
  pages        = {696-705},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Exploration of teammate trust and interaction dynamics in human-autonomy teaming},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Phase variable based recognition of human locomotor
activities across diverse gait patterns. <em>THMS</em>, <em>51</em>(6),
684–695. (<a href="https://doi.org/10.1109/THMS.2021.3107256">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human locomotor activity (LA) recognition is important in the control of exoskeletons and prostheses and in patient monitoring. This article presents a practical recognition approach that can classify level walking, stair ascent, and stair descent activities across different subjects and diverse gait patterns. The thigh angle is measured and utilized in this method to construct a phase curve in an activity-specific coordinate frame during a stride. The LA is recognized by matching the curvature of its phase curve to the expected one. The factors affecting the adaptability of the proposed method to gait variations are analyzed and compensated for. The proposed method is evaluated with eight subjects who are asked to perform the three types of activity at two different cadences: 70 steps/min and 110 steps/min. Experimental results show that the proposed classifier outperforms an existing phase variable based classifier in all validation experiments and a ${\boldsymbol{k}}$ -nearest neighbor classifier when using nonsubject-specific training data, indicating that the proposed method has superior adaptability to changes in human and in strides. Moreover, the feature used in the proposed method has demonstrated the potential in quantitatively indicating the extent of neuromotor impairments of patients.},
  archive      = {J_THMS},
  author       = {Xiaowei Tan and Bi Zhang and Guangjun Liu and Xingang Zhao and Yiwen Zhao},
  doi          = {10.1109/THMS.2021.3107256},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {12},
  number       = {6},
  pages        = {684-695},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Phase variable based recognition of human locomotor activities across diverse gait patterns},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Levels of automation and transparency: Interaction design
considerations in assistive robots for older adults. <em>THMS</em>,
<em>51</em>(6), 673–683. (<a
href="https://doi.org/10.1109/THMS.2021.3107516">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is important to encourage older adults to remain active when interacting with assistive robots. This study proposes a schematic model for integrating levels of automation (LOAs) and transparency (LoTs) in assistive robots to match the preferences and expectations of older adults. Metrics to evaluate LOA and LoT design combinations are defined. We develop two distinctive test cases to examine interaction design considerations for robots working for this population in everyday tasks: a person-following task with a mobile robot and a table-setting task with a robot manipulator. Evaluations in user studies with older adults reveal that LOA and LoT combinations influence interaction elements. Low LOA and high LoT encouraged activity engagement while receiving adequate information regarding the robot&#39;s behavior. The variety of objective and subjective metrics is essential to provide a holistic framework for evaluating the interaction.},
  archive      = {J_THMS},
  author       = {Samuel Adeolu Olatunji and Tal Oron-Gilad and Noa Markfeld and Dana Gutman and Vardit Sarne-Fleischmann and Yael Edan},
  doi          = {10.1109/THMS.2021.3107516},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {12},
  number       = {6},
  pages        = {673-683},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Levels of automation and transparency: Interaction design considerations in assistive robots for older adults},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Attentive sequence-to-sequence modeling of stroke gestures
articulation performance. <em>THMS</em>, <em>51</em>(6), 663–672. (<a
href="https://doi.org/10.1109/THMS.2021.3112961">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Production time of stroke gestures is a fundamental measure of user performance with graphical user interfaces. However, production time represents an overall quantification of the user&#39;s gesture articulation process and therefore provides an incomplete picture of such process. Moreover, previous approaches assumed stroke gestures as synchronous point sequences, when most gesture-driven applications have to deal with asynchronous point sequences. Furthermore, deep generative models of human handwriting ignore the temporal information, thereby missing a key component of the user&#39;s gesture articulation process. To solve these issues, we introduce Ditto , a sequence-to-sequence deep learning model that estimates the velocity profile of any stroke gesture using spatial information only, providing thus a fine-grained estimation of the moment-by-moment behavior of the user&#39;s articulation performance. We show that this unique capability makes Ditto remarkably accurate while handling gestures of any type: unistrokes, multistrokes, and multitouch gestures. Our model, code, and associated web application are available as open source software.},
  archive      = {J_THMS},
  author       = {Lokesh Kumar T. and Luis A. Leiva},
  doi          = {10.1109/THMS.2021.3112961},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {12},
  number       = {6},
  pages        = {663-672},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Attentive sequence-to-sequence modeling of stroke gestures articulation performance},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Performance analysis of learning from demonstration
approaches during a fine movement generation. <em>THMS</em>,
<em>51</em>(6), 653–662. (<a
href="https://doi.org/10.1109/THMS.2021.3107523">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning from demonstration (LfD) is a well-established method of movement demonstration; however, the performance of different LfD approaches during a fine movement generation is still unknown. In this study, we compare kinesthetic teaching, teleoperation, and cooperative robot tool approaches on two different tasks, where a submillimeter accuracy is required. Additionally, we analyze the influence of a visual enhancement feature on each of the approaches and the influence of a spatial scaling feature on the teleoperation approach. The participants are a well-balanced group (regarding age, gender, and expertise), with $65 \%$ having no previous experience using robots. In our study, we found that all approaches achieved a submillimeter median positioning error. However, when no additional features are used, the cooperative robot tool (CRT) approach outperforms other approaches since it consistently achieves the lowest positioning error. Besides the positioning error, the generated velocity and the participants’ feedback (via a questionnaire) also indicates that it is the most suitable approach for an accurate submillimeter movement generation. We also concludes that the visual enhancement feature and the spatial scaling feature has a significant influence on the performance of all approaches. When the two features are used, the generated positioning error drops considerably. When the visual enhancement feature is used, kinesthetic teaching performs in some cases as good as the CRT approach, while the teleoperation with the spatial scaling feature approach in some cases even outperforms the CRT approach. However, we still consider the CRT to be the best approach for fine movement generation since these features cannot be used in every possible scenario.},
  archive      = {J_THMS},
  author       = {Aljaž Baumkircher and Marko Munih and Matjaž Mihelj},
  doi          = {10.1109/THMS.2021.3107523},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {12},
  number       = {6},
  pages        = {653-662},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Performance analysis of learning from demonstration approaches during a fine movement generation},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A review of virtual reality studies on autonomous
vehicle–pedestrian interaction. <em>THMS</em>, <em>51</em>(6), 641–652.
(<a href="https://doi.org/10.1109/THMS.2021.3107517">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An increasing number of studies employ virtual reality (VR) to evaluate interactions between autonomous vehicles (AVs) and pedestrians. VR simulators are valued for their cost-effectiveness, flexibility in developing various traffic scenarios, safe conduct of user studies, and acceptable ecological validity. Reviewing the literature between 2010 and 2020, we found 31 empirical studies using VR as a testing apparatus for both implicit and explicit communication. By performing a systematic analysis, we identified current coverage of critical use cases, obtained a comprehensive account of factors influencing pedestrian behavior in simulated traffic scenarios, and assessed evaluation measures. Based on the findings, we present a set of recommendations for implementing VR pedestrian simulators and propose directions for future research.},
  archive      = {J_THMS},
  author       = {Tram Thi Minh Tran and Callum Parker and Martin Tomitsch},
  doi          = {10.1109/THMS.2021.3107517},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {12},
  number       = {6},
  pages        = {641-652},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {A review of virtual reality studies on autonomous Vehicle–Pedestrian interaction},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Evaluating the effect of poor contrast ratio in simulated
sensor-based vision systems on performance. <em>THMS</em>,
<em>51</em>(6), 632–640. (<a
href="https://doi.org/10.1109/THMS.2021.3114144">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sensor-based vision systems enable approaches to altitudes closer to the runway in low visibility conditions. These systems utilize imaging sensors capable of providing forward vision of the runway and the resulting imagery is displayed on a head-up display. Previous studies were primarily limited to nominal cases. Off-nominal cases assessments are limited only to display failures. The sensors integrated to these systems have known limitations and can produce degraded display output with respect to atmospheric conditions. An evaluation is conducted to assess the potential human factors implications of a simulated display of poor sensor output, operationalized as a difference in contrast ratio of the display output for simulated approach and landing operations. Pilots fly six different approach and landing scenarios in a simulator for two visibility levels and three levels of display information quality (none, poor, and good display output). Measures of performance include approach and landing performance, attention allocation, workload, and decision-making. The experiment results indicate that landing performance and decision making are negatively impacted by poor display output, while there is no evidence of an impact on workload or situation awareness.},
  archive      = {J_THMS},
  author       = {Ramanathan Annamalai and Michael C. Dorneich and Güliz Tokadlı},
  doi          = {10.1109/THMS.2021.3114144},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {12},
  number       = {6},
  pages        = {632-640},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Evaluating the effect of poor contrast ratio in simulated sensor-based vision systems on performance},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Assessing alternative approaches for conveying automated
vehicle “intentions.” <em>THMS</em>, <em>51</em>(6), 622–631. (<a
href="https://doi.org/10.1109/THMS.2021.3106892">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Research suggests the general public has inherent distrust in highly automated vehicles (HAV), typically stemming from a lack of vehicle system transparency while in motion (e.g., the user not being informed how the car will react in the upcoming scene) and not having an effective way to control the vehicle in the event of a system failure. To better understand user trust and perceptions of comfort and safety while riding in an HAV, this study evaluated in-situ human-machine interface (HMI) systems (visual, auditory, and mixed-modal) to relay vehicle “intentions” (e.g., the vehicle&#39;s response to roadway stimuli) to the user. A considerable link was found between HMI modality and users’ reported levels of comfort, safety, and trust during experimentation. In addition, several key behavioral traits are identified as factors that contributed to users’ baseline comfort levels with the HAV. Moving forward, it will be necessary for HAV systems to keep users in-the-loop in an effort to increase system transparency and overall understanding. Vehicle feedback should consistently and accurately represent the driving landscape and clearly communicate vehicle system state to users.},
  archive      = {J_THMS},
  author       = {Alexis Basantis and Marty Miller and Zachary Doerzaph and M. Lucas Neurauter},
  doi          = {10.1109/THMS.2021.3106892},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {12},
  number       = {6},
  pages        = {622-631},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Assessing alternative approaches for conveying automated vehicle “Intentions”},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The effects of haptic feedback and transition type on
transfer of control between drivers and vehicle automation.
<em>THMS</em>, <em>51</em>(6), 613–621. (<a
href="https://doi.org/10.1109/THMS.2021.3107255">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When driving conditions exceed the design limits of an automation system, transfers of control back to a human driver become necessary. Such transfers may compromise safety, since a driver typically requires time to get back into the loop. Various schemes for combining human and automatic control over steering have been proposed to address the challenges of transferring control smoothly. This article presents the findings from a simulator study that compared four such schemes. The driver initiated a transfer of control either by pressing a button and assuming full control of the vehicle (switching transition) or by overpowering an automation that continued to act on the vehicle. Two additional factors in the $\text{2} \times \text{2} \times \text{2}$ factorial design are the availability of haptic feedback and the inclusion of a secondary task. To examine the robustness of each scheme, all driving scenarios involved nominal and off-nominal conditions. The findings from this study demonstrate the benefits of haptic shared control (i.e., overpowering transitions with haptic feedback) for facilitating transfers of control between a human driver and vehicle automation. Haptic-feedback-enhanced drivers’ awareness of automation input and the status of the vehicle while the overpowering transitions improved the smoothness of interventions, compared to switching transitions.},
  archive      = {J_THMS},
  author       = {Steven Cutlip and Yuzhi Wan and Nadine Sarter and R. Brent Gillespie},
  doi          = {10.1109/THMS.2021.3107255},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {12},
  number       = {6},
  pages        = {613-621},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {The effects of haptic feedback and transition type on transfer of control between drivers and vehicle automation},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Extending the evaluation of social assistive robots with
accessibility indicators: The AUSUS evaluation framework. <em>THMS</em>,
<em>51</em>(6), 601–612. (<a
href="https://doi.org/10.1109/THMS.2021.3112976">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The introduction of robots in the real world requires previous evaluation of the envisaged performance. Factors like usability, user experience, social acceptance, or societal impact, among others, have been taken into account in evaluation frameworks defined during the last years. However, one of the most important factors that need to be evaluated in any kind of interaction is whether all the users are able to work with the system with the same opportunities and easiness, and none of the current human–robot interaction (HRI) evaluation frameworks include this factor yet. This article proposes an extension of a popular HRI evaluation framework, including accessibility as a new evaluation factor. The proposed approach, named AUSUS, considers Accessibility, Usability, Social acceptance, User experience, and Societal impact. This article presents a use case of the framework, which is evaluated through a socially assistive robotic platform created to perform comprehensive geriatric assessment: the CLARC system. The details of the evaluation process in a hospital and a retirement home are reported, and the main difficulties and recommendations of using AUSUS are discussed.},
  archive      = {J_THMS},
  author       = {Ana Iglesias and Javier García and Ángel García-Olaya and Raquel Fuentetaja and Fernando Fernández and Adrián Romero-Garcés and Rebeca Marfil and Antonio Bandera and Karine Lan Hing Ting and Dimitri Voilmy and Álvaro Dueñas and Cristina Suárez-Mejías},
  doi          = {10.1109/THMS.2021.3112976},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {12},
  number       = {6},
  pages        = {601-612},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Extending the evaluation of social assistive robots with accessibility indicators: The AUSUS evaluation framework},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Motion planning for human–robot collaboration using an
objective-switching strategy. <em>THMS</em>, <em>51</em>(6), 590–600.
(<a href="https://doi.org/10.1109/THMS.2021.3112953">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motion planning of collaborative robots is often required to simultaneously satisfy the contradictory objectives of reliably avoiding human workers and safely approaching them. In this article, we propose a new strategy that adaptively selects one of two objective functions based on the current operational region of the robot; the objective function for avoiding the worker with a distance larger than the safe distance, and the objective function for approaching the worker with a speed limitation for human safety. This strategy improves the worker safety while limiting the negative impact of the speed limit on time efficiency. The chattering related to the switching between the two objectives is solved using a continuous approximation of the switching based on uncertainties of the predicted worker&#39;s motion. We implement the proposed motion planning with the objective-switching strategy into a collaborative assembly system to deal with the worker moving with high irregularities. We experimentally evaluate the effectiveness of the proposed motion planning from the safety and efficiency points of view.},
  archive      = {J_THMS},
  author       = {Akira Kanazawa and Jun Kinugawa and Kazuhiro Kosuge},
  doi          = {10.1109/THMS.2021.3112953},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {12},
  number       = {6},
  pages        = {590-600},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Motion planning for Human–Robot collaboration using an objective-switching strategy},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Toward robots’ behavioral transparency of temporal
difference reinforcement learning with a human teacher. <em>THMS</em>,
<em>51</em>(6), 578–589. (<a
href="https://doi.org/10.1109/THMS.2021.3116119">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The high request for autonomous human–robot interaction (HRI), combined with the potential of machine learning (ML) techniques, allow us to deploy ML mechanisms in robot control. However, the use of ML can make robots’ behavior unclear to the observer during the learning phase. Recently, transparency in HRI has been investigated to make such interactions more comprehensible. In this work, we propose a model to improve the transparency during reinforcement learning (RL) tasks for HRI scenarios: the model supports transparency by having the robot show nonverbal emotional-behavioral cues. Our model considered human feedback as the reward of the RL algorithm and it presents emotional-behavioral responses based on the progress of the robot learning. The model is managed only by the temporal-difference error. We tested the architecture in a teaching scenario with the iCub humanoid robot. The results highlight that when the robot expresses its emotional-behavioral response, the human teacher is able to understand its learning process better. Furthermore, people prefer to interact with an expressive robot as compared to a mechanical one. Movement-based signals proved to be more effective in revealing the internal state of the robot than facial expressions. In particular, gaze movements were effective in showing the robot&#39;s next intentions. In contrast, communicating uncertainty through robot movements sometimes led to action misinterpretation, highlighting the importance of balancing transparency and the legibility of the robot goal. We also found a reliable temporal window in which to register teachers’ feedback that can be used by the robot as a reward.},
  archive      = {J_THMS},
  author       = {Marco Matarese and Alessandra Sciutti and Francesco Rea and Silvia Rossi},
  doi          = {10.1109/THMS.2021.3116119},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {12},
  number       = {6},
  pages        = {578-589},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Toward robots’ behavioral transparency of temporal difference reinforcement learning with a human teacher},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). ArmSym: A virtual human–robot interaction laboratory for
assistive robotics. <em>THMS</em>, <em>51</em>(6), 568–577. (<a
href="https://doi.org/10.1109/THMS.2021.3106865">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Research in human–robot interaction for assistive robotics usually presents many technical challenges for experimenters, forcing researchers to split their time between solving technical problems and conducting experiments. In addition, previous work in virtual reality setups tends to focus on a single assistive robotics application. In order to alleviate these problems, we present ArmSym, a virtual reality laboratory with a fully simulated and developer-friendly robot arm. The system is intended as a testbed to run many sorts of experiments on human control of a robotic arm in a realistic environment, ranging from an upper limb prosthesis to a wheelchair-mounted robotic manipulator. To highlight the possibilities of this system, we perform a study comparing different sorts of prosthetic control types. Looking at nonimpaired subjects, we study different psychological metrics that evaluate the interaction of the user with the robot under different control conditions. Subjects report a perception of embodiment in the absence of realistic cutaneous touch, supporting previous studies in the topic. We also find interesting correlations between control and perceived ease of use. Overall our results confirm that ArmSym can be used to gather data from immersive experiences prosthetics, opening the door to closer collaboration between device engineers and experience designers in the future.},
  archive      = {J_THMS},
  author       = {Samuel Bustamante and Jan Peters and Bernhard Schölkopf and Moritz Grosse-Wentrup and Vinay Jayaram},
  doi          = {10.1109/THMS.2021.3106865},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {12},
  number       = {6},
  pages        = {568-577},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {ArmSym: A virtual Human–Robot interaction laboratory for assistive robotics},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). IEEE systems, man, and cybernetics society information.
<em>THMS</em>, <em>51</em>(5), C3. (<a
href="https://doi.org/10.1109/THMS.2021.3109931">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Provides a listing of current committee members and society officers.},
  archive      = {J_THMS},
  doi          = {10.1109/THMS.2021.3109931},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {10},
  number       = {5},
  pages        = {C3},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {IEEE systems, man, and cybernetics society information},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Two fall-related and kinematic data-based approaches for an
instrumented conventional cane. <em>THMS</em>, <em>51</em>(5), 554–563.
(<a href="https://doi.org/10.1109/THMS.2021.3097984">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stimulating the usability and acceptability of canes within the geriatric community is crucial for fall prevention. Clinical evidences support the use of canes to increase the user&#39;s balance and reduce the fall risk. Its nonuse is correlated with a higher number of falls. This fateful event draws tragic consequences such as death or injuries that might affect the victim&#39;s psychology, reducing their confidence and mobility. However, users tend to put aside canes and canes&#39; acceptability is reduced. To increase their usability and acceptability, appellative and time-effective technology-based strategies, such as automatic fall detection systems, should be incorporated. First, steps toward a fall-related approach demand the ability to identify that a fall event is eminent relying only on information derived from the device. This study proposes and compares two fall event approaches. A machine learning framework determines the best models and features for three classification problems: fall event, fall phase, and fall category. The long short-term memory (LSTM) is the most suitable classifier for fall event and phase classification problems, requiring the first 32 and 17 features, respectively, ranked by the Relief-F and minimum-redundancy maximum-relevancy methods (accuracy &gt;99% and &gt;95%). K-nearest neighbors with the first 40 features ranked by the Relief-F method presented the best performance for fall category classification (accuracy &gt;74%). A finite-state machine (FSM) combined accelerometer and gyroscope&#39;s data to detect the cane&#39;s fall with a clear proximity to the LSTM performance (accuracy &gt;99%). Both approaches detect a fall before its occurrence with mean lead times higher than 373 (LSTM) and 266 ms (FSM).},
  archive      = {J_THMS},
  author       = {Nuno Ferrete Ribeiro and Cristina P. Santos},
  doi          = {10.1109/THMS.2021.3097984},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {10},
  number       = {5},
  pages        = {554-563},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Two fall-related and kinematic data-based approaches for an instrumented conventional cane},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Separability of motor imagery directions using
subject-specific discriminative EEG features. <em>THMS</em>,
<em>51</em>(5), 544–553. (<a
href="https://doi.org/10.1109/THMS.2021.3086009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electroencephalogram (EEG) based brain–computer interface (BCI) is an augmented communication modality between the brain and computer that exclusively depends on noninvasively recorded neuro-electrical activity. To establish multiple control commands in BCI systems, it is essential to efficiently identify unique EEG activation patterns underlying multiple motor tasks. This article investigates the brain activation modulations elicited by imagined movement directions and the EEG features that characterize them. An experiment was conducted in which ten subjects imagined two-dimensional right-hand movement in the left, right, up, and down directions in the vertical plane. The frequency and phase characteristics of EEG rhythms are explored to identify the features that discriminate movement imagination toward multiple directions. The proposed decoding step consists of Fisher&#39;s analysis of instantaneous phase values to identify EEG channels with optimal discriminative information followed by extraction of phase-locking values and temporal-spatial features, which are then used for classification. The proposed approach offers significantly higher ( p &lt; 0.05) performance than the state-of-the-art method while employing subject-specific time segments and channels. Hence, an average accuracy of 69.25% is obtained for binary classification of all pairs of movement directions with a performance increment of 8.33% compared to the conventional approaches using fixed channel-time paradigm. The results illustrate the potential use of noninvasive EEG signals in decoding imagined movement kinematics, and further research is needed to investigate and optimize the separability in multiple directions.},
  archive      = {J_THMS},
  author       = {Kavitha P. Thomas and Neethu Robinson and Vinod A. Prasad},
  doi          = {10.1109/THMS.2021.3086009},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {10},
  number       = {5},
  pages        = {544-553},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Separability of motor imagery directions using subject-specific discriminative EEG features},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multiuser stereoscopic projection techniques for CAVE-type
virtual reality systems. <em>THMS</em>, <em>51</em>(5), 535–543. (<a
href="https://doi.org/10.1109/THMS.2021.3102520">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the development of increasingly popular head mounted displays, CAVE-type systems may still be considered one of the most immersive virtual reality systems with many advantages. However, a serious limitation of most CAVE-type systems is the generation of a three-dimensional (3-D) image from the perspective of only one person. This problem is significant because in some applications, the participants must cooperate with each other in the virtual world. This paper presents the adaptation of a one-user Cave Automatic Virtual Environment (CAVE) installation in the Immersive 3-D Visualization Lab at the Gdańsk University of Technology to a two-user stereoscopy system. Simultaneous use of two alternative one-user stereoscopies available in the I3DVL (a technique with spectrum separation—Infitec, and active stereo) and a simple electronic circuit have allowed us to transform the one-user stereoscopy CAVE installation to a two-user stereoscopic system. The experiments performed concentrated on several objective measurable parameters. The calculated crosstalk value was low, approximately 1%, which can be considered negligible and shows the proper operation of the proposed technique. Additionally, initial experiments based on the tested two-user application and related to user comfort in the developed two-user stereoscopy are discussed in this paper. However, this topic still needs further research. The proposed solutions are a cheap alternative to adapt the existing one-user CAVE-type systems which support two projection techniques to a two-user system.},
  archive      = {J_THMS},
  author       = {Jacek Lebiedź and Adam Mazikowski},
  doi          = {10.1109/THMS.2021.3102520},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {10},
  number       = {5},
  pages        = {535-543},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Multiuser stereoscopic projection techniques for CAVE-type virtual reality systems},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Interaction with gaze, gesture, and speech in a flexibly
configurable augmented reality system. <em>THMS</em>, <em>51</em>(5),
524–534. (<a href="https://doi.org/10.1109/THMS.2021.3097973">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal interaction has become a recent research focus since it offers better user experience in augmented reality (AR) systems. However, most existing works only combine two modalities at a time, e.g., gesture and speech. Multimodal interactive system integrating gaze cue has rarely been investigated. In this article, we propose a multimodal interactive system that integrates gaze, gesture, and speech in a flexibly configurable AR system. Our lightweight head-mounted device supports accurate gaze tracking, hand gesture recognition, and speech recognition simultaneously. The system can be easily configured into various modality combinations, which enables us to investigate the effects of different interaction techniques. We evaluate the efficiency of these modalities using two tasks: the lamp brightness adjustment task and the cube manipulation task. We also collect subjective feedback when using such systems. The experimental results demonstrate that the Gaze+Gesture+Speech modality is superior in terms of efficiency, and the Gesture+Speech modality is more preferred by users. Our system opens the pathway toward a multimodal interactive AR system that enables flexible configuration.},
  archive      = {J_THMS},
  author       = {Zhimin Wang and Haofei Wang and Huangyue Yu and Feng Lu},
  doi          = {10.1109/THMS.2021.3097973},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {10},
  number       = {5},
  pages        = {524-534},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Interaction with gaze, gesture, and speech in a flexibly configurable augmented reality system},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A collaborative filtering approach toward plug-and-play
myoelectric robot control. <em>THMS</em>, <em>51</em>(5), 514–523. (<a
href="https://doi.org/10.1109/THMS.2021.3098115">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Previous works in the literature have claimed that the characteristics of electromyography (EMG) signals depend on each person, and thus, EMG interfaces need to be carefully calibrated for each user in myoelectric control. In this study, we show that the EMG interface used to estimate the joint torques of a user can be constructed simply by incorporating other users’ data without typical calibration process. To achieve this plug-and-play capability, we introduce the concept of collaborative filtering to estimate the joint torque of a novel user by exploiting the preidentified relationships between motion-body features, including EMG signals, and the joint torques of other users. To validate our proposed approach, we compare the performance of estimating joint torque by the proposed method with that by conventional linear regression models as a baseline. We considered the following two baseline methods. Linear-own: The parameters of the linear model are calibrated for each subject from his/her own training data. Linear-others: The parameters of the linear model are calibrated with the other users’ data in which the novel user&#39;s data are not included. As a result, the estimated joint torques from our proposed approach reveal a better estimation performance than those from the baseline approaches. Furthermore, we also successfully demonstrate online myoelectric control of an upper limb exoskeleton robot with an attached mannequin arm.},
  archive      = {J_THMS},
  author       = {Jun-ichiro Furukawa and Shinya Chiyohara and Tatsuya Teramae and Asuka Takai and Jun Morimoto},
  doi          = {10.1109/THMS.2021.3098115},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {10},
  number       = {5},
  pages        = {514-523},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {A collaborative filtering approach toward plug-and-play myoelectric robot control},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Variable damping control for pHRI: Considering stability,
agility, and human effort in controlling human interactive robots.
<em>THMS</em>, <em>51</em>(5), 504–513. (<a
href="https://doi.org/10.1109/THMS.2021.3090064">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a multi-degree-of-freedom variable damping controller to manage the trade-off between stability and agility and to reduce user effort in physical human-robot interaction. The controller accounts for the human body&#39;s inherent impedance properties and applies a range of robotic damping from negative (energy injection) to positive (energy dissipation) values based on the user&#39;s intent of motion. To evaluate the effectiveness of the proposed controller in balancing the trade-off between stability/agility and reducing user effort, two studies are performed on both the human upper-extremity and lower-extremity to represent both industrial and rehabilitation applications of the proposed controller. These studies required subjects to perform a series of multidimensional target reaching tasks while the human user interacted with either the end-effector of a robotic arm for the upper-extremity study or a wearable ankle robot for the lower-extremity study. Stability, agility, and user effort are quantified by a variety of performance metrics. Stability is quantified by both overshoot and stabilization time. Mean and maximum speed are used to quantify agility. To quantify the user effort, both overall and maximum muscle activation, and mean and maximum root-mean-squared interaction force are calculated. The results of both the upper- and lower-extremity studies demonstrate that the controller is able to reduce user effort while increasing agility at a negligible cost to stability.},
  archive      = {J_THMS},
  author       = {Fatemeh Zahedi and James Arnold and Connor Phillips and Hyunglae Lee},
  doi          = {10.1109/THMS.2021.3090064},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {10},
  number       = {5},
  pages        = {504-513},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Variable damping control for pHRI: Considering stability, agility, and human effort in controlling human interactive robots},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Lifting posture prediction with generative models for
improving occupational safety. <em>THMS</em>, <em>51</em>(5), 494–503.
(<a href="https://doi.org/10.1109/THMS.2021.3102511">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lifting tasks have been identified to be highly associated with work-related low back pain. Posture prediction can be used for simulating workers’ posture of lifting tasks and thus facilitate the prevention of low back pain (LBP). This study adopts two generative models, conditional variational encoder and conditional generative adversarial network, to predict lifting postures. A regular feed-forward neural network (FNN) developed upon previous studies is also investigated for comparison purposes. Ground-truth lifting posture data collected by a motion capture system is used for training and testing the models. The models are trained with datasets of different size and loss functions, and the results are compared. The conditional variational autoencoder and the regular FNN achieved comparable top performance in lifting posture prediction in terms of accuracy and posture validity. Both generative models are able to partially capture the variability of constrained postures. Overall, the results prove that using a generative model is able to predict postures with reasonable accuracy and validity (RMSE of coordinates = 0.049 m; RMSE of joint angles = 19.58 $^\circ$ ). The predicted postures can support biomechanical analysis and ergonomics assessment of a lifting task to reduce the risk of low back injuries.},
  archive      = {J_THMS},
  author       = {Li Li and Saiesh Prabhu and Ziyang Xie and Hanwen Wang and Lu Lu and Xu Xu},
  doi          = {10.1109/THMS.2021.3102511},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {10},
  number       = {5},
  pages        = {494-503},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Lifting posture prediction with generative models for improving occupational safety},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Computer interface for real-time gait biofeedback using a
wearable integrated sensor system for data acquisition. <em>THMS</em>,
<em>51</em>(5), 484–493. (<a
href="https://doi.org/10.1109/THMS.2021.3090738">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computer interfaces with visual biofeedback (BF) capabilities in robot-assisted gait training applications can help design subject-specific therapies, define tasks based on the patient&#39;s condition, and increase the training outcomes. This study presents the use of an open-source real-time computer interface as a subject-specific visual BF and monitoring tool during gait. The interface is used in combination with a low-cost Wearable integrated Sensor System for Data Acquisition (WiSSDA), which consists of a 3-D printed lower extremity exoskeleton equipped with rotary encoders and pressure-sensitive insoles that measure biometric data during the gait cycle. The system obtains the lower extremities’ joint kinematics in the sagittal plane and estimates the vertical component of the foot contact forces and the foot center of pressure. Experimental tests were performed to evaluate different gait parameters using the computer interface as visual BF to guide the subject with reference boundaries of kinematic and kinetic parameters as target trajectories. The results were used to evaluate the interface&#39;s performance for specific gait tasks. The results demonstrated the capabilities of the computer interface combined with WiSSDA for robot-assisted gait training applications using BF; providing an open-source, portable, and ambulatory device that can be used in outdoor environments. The proposed system can help bring gait rehabilitation outside the laboratory environment, eliminating the need for stationary equipment.},
  archive      = {J_THMS},
  author       = {Inigo Sanz-Pena and Julio Blanco and Joo H. Kim},
  doi          = {10.1109/THMS.2021.3090738},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {10},
  number       = {5},
  pages        = {484-493},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Computer interface for real-time gait biofeedback using a wearable integrated sensor system for data acquisition},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Discovering tactical memory from observed human performance
in machine learning. <em>THMS</em>, <em>51</em>(5), 474–483. (<a
href="https://doi.org/10.1109/THMS.2021.3097910">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article describes an investigation for composing a representation of significant past events that are retained in memory by an observed human actor. These memories influence that actor&#39;s performance of a task or making a decision. More specifically, we seek to infer which aspects of the environment and which events have significant effect on an observed actor&#39;s future decisions. We introduce a new memory modeling algorithm, memory composition learning, which processes traces of an observed actor&#39;s performance, and from these, composes a set of memory features that describe important events in his/her memory that affected these actions. These memory features are subsequently used to produce memory-enhanced traces that can be used by machine learning algorithms to learn memory-influenced behaviors. We implemented a prototype of our approach and evaluated it in two simulated domains, one with synthetic memory-influenced vacuum cleaner agents and one involving human subjects controlling a lawn mower that required memory-influenced behaviors. Results show that our approach is able to discover intuitive representations of tactical memory from observed behavior in both domains, and that these memory representations contributed to improved machine learning of human behavior.},
  archive      = {J_THMS},
  author       = {Josiah Wong and Avelino J. Gonzalez},
  doi          = {10.1109/THMS.2021.3097910},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {10},
  number       = {5},
  pages        = {474-483},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Discovering tactical memory from observed human performance in machine learning},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Automation error type and methods of communicating
automation reliability affect trust and performance: An empirical study
in the cyber domain. <em>THMS</em>, <em>51</em>(5), 463–473. (<a
href="https://doi.org/10.1109/THMS.2021.3051137">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Antiphishing aid systems, among other automated systems, are not perfectly reliable. Automated systems can make errors, thereby resulting in false alarms or misses. An automated system&#39;s capabilities need to be communicated to the users to maintain proper user trust. System capabilities can be learned through an explicit description or from experience. Using a phishing-detection system as a testbed in this article, we systematically varied automation error type and the method of communicating system reliability in a factorial design and measured their effects on human performance and trust in the automation. Participants were asked to classify emails as legitimate or phishing with assistance from the phishing-detection system. The results from 510 participants suggest that learning through experience with feedback improved trust calibration for both objective and subjective trust measures in most conditions. Moreover, false alarms lowered trust more than misses for both unreliable and reliable systems, and false alarms turned out to be beneficial for proper trust calibration when using unreliable systems. Design implications of the results include using feedback whenever possible and choosing false alarms over misses for unreliable systems.},
  archive      = {J_THMS},
  author       = {Jing Chen and Scott Mishler and Bin Hu},
  doi          = {10.1109/THMS.2021.3051137},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {10},
  number       = {5},
  pages        = {463-473},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Automation error type and methods of communicating automation reliability affect trust and performance: An empirical study in the cyber domain},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Identifying drone operator by deep learning and ensemble
learning of IMU and control data. <em>THMS</em>, <em>51</em>(5),
451–462. (<a href="https://doi.org/10.1109/THMS.2021.3102508">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Drone flight controls and ground stations are known to be vulnerable to attacks. Besides posing a threat to integrity and confidentiality of drone data, their vulnerabilities endanger safety. Onboard continuous authentication is a vital countermeasure to hijacking attempts. Motivated by the success of Machine Learning (ML) techniques in the field of behavioral biometrics, this paper investigates the use of sensor readings generated onboard drones and of control data reaching them from the ground to feed an onboard ML model continuously authenticating pilots. We analyze fifteen inertial measurement units (IMU) and four radio control signals obtained from the drone&#39;s onboard sensors or coming from its remote controller, to identify the controlling pilot. We investigate three sequence classification schemes. In the first scheme, raw sensor sequences are directly fed to a deep Long/Short-Term Memory (LSTM) learner. In the second scheme, frequency-domain features are extracted from the data sequences and interpreted by an ensemble of random trees. In the third scheme, instantaneous sensor readings are classified using the same ensemble learning technique as in the second scheme, yet a final decision fusion method is adopted to provide a sequence-based decision. We compare the three schemes in terms of accuracy, complexity, and delay. The winning scheme is validated and tested against an unseen intruder scenario. Our tests show that an LSTM model trained with data from 19 users is able to identify the operating user at a 97% accuracy, while it can identify an unknown intruder at an average accuracy of 73%.},
  archive      = {J_THMS},
  author       = {Ruba Alkadi and Sultan Al-Ameri and Abdulhadi Shoufan and Ernesto Damiani},
  doi          = {10.1109/THMS.2021.3102508},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {10},
  number       = {5},
  pages        = {451-462},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Identifying drone operator by deep learning and ensemble learning of IMU and control data},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A deep learning framework of autonomous pilot agent for air
traffic controller training. <em>THMS</em>, <em>51</em>(5), 442–450. (<a
href="https://doi.org/10.1109/THMS.2021.3102827">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, a deep learning-based framework is proposed to implement an autonomous pilot agent (APA), which serves as a human pseudo-pilot to assist air traffic controller (ATCO) training. A novel paradigm, including speech recognition, language understanding, pilot repetition generation (PRG), and text-to-speech (TTS), is designed to formulate the framework pipeline, which also incorporates a simulation system interface. We mainly focus on the PRG and TTS models to address the ATC specificities in this work. The neural architecture is proposed to generate the text repetition instruction by using a sequence-to-sequence text mapping. The Transformer block is improved to implement a high-efficient TTS model, in which the nonautoregressive mechanism is applied to achieve the parallel synthesis. A dedicated phoneme vocabulary is designed to cope with the multilingual issue in the ATC domain and address the out-of-vocabulary problem. With the APA framework, a virtual training mode is proposed to complete the training task without the limitation of time and location. Experimental results on a real-world dataset show that the proposed APA framework replaces the human pilot with considerable high confidence in a real-time manner during the simulation training. Most importantly, the APA framework and the virtual training system are able to cope with the dilemma of physical attendance (like COVID-19) and improve the equipment utilization capacity for the ATCO training.},
  archive      = {J_THMS},
  author       = {Yi Lin and YuanKai Wu and Dongyue Guo and Pan Zhang and Changyu Yin and Bo Yang and Jianwei Zhang},
  doi          = {10.1109/THMS.2021.3102827},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {10},
  number       = {5},
  pages        = {442-450},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {A deep learning framework of autonomous pilot agent for air traffic controller training},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Impact of pilot’s expertise on selection, use, trust, and
acceptance of automation. <em>THMS</em>, <em>51</em>(5), 432–441. (<a
href="https://doi.org/10.1109/THMS.2021.3090199">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automation regroups a variety of advanced tools meant to improve performance and decrease human workload. This article was designed to investigate how different automation solutions, engaging different human-machine cooperation modes, interact with expertise. Aircraft pilots (i.e., experts) and nonpilots (i.e., novices) were presented with a set of simplified flight piloting tasks monitored simultaneously using the Open MATB (Open Multiattribute Task Battery) in four different automation conditions (manual, assisted, cooperative, and supervisory control). Experts&#39; performances at the Open MATB were higher than those of novices. Experts also exhibited a lower level of workload. Apart from function delegation, where no human performance is required, automation solutions benefit experts and novices similarly. Participants were then asked to choose repeatedly between the four automation conditions. Experts and novices exhibited different strategies. Novices spread their choices over the different automation conditions, whereas experts clearly favor cooperative control, the automation solution that enhanced their expertise.},
  archive      = {J_THMS},
  author       = {Jordan Navarro and Sarah Allali and Nicolas Cabrignac and Julien Cegarra},
  doi          = {10.1109/THMS.2021.3090199},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {10},
  number       = {5},
  pages        = {432-441},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Impact of pilot&#39;s expertise on selection, use, trust, and acceptance of automation},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On task dependence of helicopter pilot biodynamic
feedthrough and neuromuscular admittance: An experimental and numerical
study. <em>THMS</em>, <em>51</em>(5), 421–431. (<a
href="https://doi.org/10.1109/THMS.2020.3044971">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, the results of a piloted flight simulator campaign aimed at measuring biomechanical performance indicators of a helicopter pilot performing complex, realistic tasks are presented. The upper limbs&#39; motion and the activation of the main muscle groups of the left arm were measured during ship-deck landings, performed flying several helicopter configurations with sea conditions of variable intensity. The analysis of the results shows an increase in the muscle activity relative to the increase in task difficulty, in agreement with subjective ratings (Bedford workload scale). The study provides useful indications to improve the corresponding biomechanical simulations, as well as to characterize pilot&#39;s performance during specific tasks.},
  archive      = {J_THMS},
  author       = {Andrea Zanoni and Matteo Zago and Rita Paolini and Giuseppe Quaranta and Manuela Galli and Pierangelo Masarati},
  doi          = {10.1109/THMS.2020.3044971},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {10},
  number       = {5},
  pages        = {421-431},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {On task dependence of helicopter pilot biodynamic feedthrough and neuromuscular admittance: An experimental and numerical study},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Impact of human-centered vestibular system model for motion
control in a driving simulator. <em>THMS</em>, <em>51</em>(5), 411–420.
(<a href="https://doi.org/10.1109/THMS.2021.3102506">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study presents a driving simulator experiment to evaluate three different motion cueing algorithms based on model predictive control. The difference among these motion strategies lies in the type of mathematical model used. The first one contains only the dynamic model of the platform, while the others integrate additionally two different vestibular system models. We compare these three strategies to discuss the tradeoffs when including a vestibular system model in the control loop from the user&#39;s viewpoint. The study is conducted in autonomous mode and in free driving mode, as both play an important role in motion cueing validation. A total of 38 individuals participated in the experiment; 19 drove the simulator in free driving mode and the remaining using the autonomous driving mode. For both driving modes, substantial differences is observed. The analysis shows that one of the vestibular system models is suitable for driving simulators, as it thoroughly restores high-frequency accelerations and is well noted by the participants, especially those in the free driving mode. Further tests are needed to analyze the advantages of integrating the chosen vestibular system model in the control design for motion cuieng algorithms. Regarding the autonomous mode, further research is needed to examine the influence of the vestibular system model on the motion performance, as the behavior of the autonomous model may implicitly interfere with subjective assessments.},
  archive      = {J_THMS},
  author       = {Carolina Rengifo and Jean-Rémy Chardonnet and Hakim Mohellebi and Andras Kemeny},
  doi          = {10.1109/THMS.2021.3102506},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {10},
  number       = {5},
  pages        = {411-420},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Impact of human-centered vestibular system model for motion control in a driving simulator},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021c). IEEE systems, man, and cybernetics society information.
<em>THMS</em>, <em>51</em>(4), C3. (<a
href="https://doi.org/10.1109/THMS.2021.3097225">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Provides a listing of current committee members and society officers.},
  archive      = {J_THMS},
  doi          = {10.1109/THMS.2021.3097225},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {C3},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {IEEE systems, man, and cybernetics society information},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The benefit of assisted and unassisted eco-driving for
electrified powertrains. <em>THMS</em>, <em>51</em>(4), 403–407. (<a
href="https://doi.org/10.1109/THMS.2021.3086057">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Eco-driving assistance systems that encourage drivers to engage in fuel-saving behavior are effective at improving energy-efficiency, with recent research directed towards incorporating predictive models of energy losses in these systems to optimize recommendations. In this article, we evaluate a predictive eco-driving assistance system on three powertrains: a combustion engine-driven vehicle, a parallel hybrid electric vehicle, and a battery electric vehicle. In each case, energy consumption is found by applying a quasi-static model to driving simulator data for a simulated route including urban, rural, and highway sections. We find that both assisted and unassisted eco-driving has a beneficial effect in all cases, with the assistance system leading to reductions in energy usage of 6.1%, 15.9%, and 16.6% for the combustion engine, hybrid electric, and battery electric powertrains, respectively.},
  archive      = {J_THMS},
  author       = {Xingda Yan and Craig Kevin Allison and James M. Fleming and Neville A. Stanton and Roberto Lot},
  doi          = {10.1109/THMS.2021.3086057},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {403-407},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {The benefit of assisted and unassisted eco-driving for electrified powertrains},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Classification of officers’ driving situations based on
eye-tracking and driver performance measures. <em>THMS</em>,
<em>51</em>(4), 394–402. (<a
href="https://doi.org/10.1109/THMS.2021.3090787">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motor vehicle crashes are a leading cause of police officers&#39; deaths in the line of duty. These crashes are mainly attributed to officers&#39; use of in-vehicle technologies while driving, distraction, fatigue, and high-speed driving conditions. The objective of this study is to classify officers&#39; driving situations using a combination of driver behavior and eye-tracking measures. The study compared three algorithms, including random forest (RF), support vector machine (SVM), and random Fourier features (RFF) to classify officers&#39; driving situations (i.e., normal vs. pursuit driving) and in-vehicle technology use. The results suggested that driver behavior measures, combined with RF or SVM methods, are most promising for classifying officers&#39; driving condition (accuracy of about 90%). However, it might be more efficient to apply RFF with driver behavior measures to classify officers&#39; use of in-vehicle technologies while driving due to the time cost reduction of RFF as compared to SVM and RF algorithms. The findings can be applied to improve future police vehicles, training protocols, and to provide adaptive technology solutions to reduce officers&#39; driving distraction and workload.},
  archive      = {J_THMS},
  author       = {Maryam Zahabi and Yinsong Wang and Shahin Shahrampour},
  doi          = {10.1109/THMS.2021.3090787},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {394-402},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Classification of officers’ driving situations based on eye-tracking and driver performance measures},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Radar command group time entropy signature as a visual
monitoring enhancement for air traffic controllers. <em>THMS</em>,
<em>51</em>(4), 384–393. (<a
href="https://doi.org/10.1109/THMS.2021.3076044">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The activity of generic flight path monitoring of an aircraft on the radar screen is one the major problem which results in air traffic controllers (ATCOs) losing awareness of it. A novel eye signature, known as the radar command group (RCG) time entropy signature, is thus presented in this paper. This signature seeks to model the monitoring behavior of ATCOs using eye-tracking technique during this activity. Acquisition and representation of this monitoring behavior is achieved by first identifying the respective fixation count and mouseover label movement on the aircraft to establish a RCG. The regularity of a RCG exhibited on an aircraft is then calculated using the information entropy formula. Real time simulations are conducted for 88 one-hour experimental sessions with licensed and non-licensed participants comprising of three expertise levels, using scenarios that mimic actual air traffic, consisting of various flight path configuration. Test results of the RCG time entropy signature showed that, an aircraft flying on a flight path with longer distance and no change in altitude likely results in a more regular generic flight path monitoring. Furthermore, it can also be used to differentiate the novice participants from intermediates and experts, as novices lack the training. An ATCO&#39;s expertise level can thus be determined and benchmarked accordingly, allowing this signature to be applied on future ATCO training.},
  archive      = {J_THMS},
  author       = {Hong Jie Wee and Sun Woh Lye and Jean-Philippe Pinheiro and Beatrice Pesquet Popescu},
  doi          = {10.1109/THMS.2021.3076044},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {384-393},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Radar command group time entropy signature as a visual monitoring enhancement for air traffic controllers},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Using operator gaze tracking to design wrist mechanism for
surgical robots. <em>THMS</em>, <em>51</em>(4), 376–383. (<a
href="https://doi.org/10.1109/THMS.2021.3076038">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article assessed how surgical robot parameters influenced operator viewpoint during a simulated surgical procedure. Surgical robots are useful tools in minimally invasive surgery. However, even with robots, suturing is difficult because the needle is sometimes obscured by tissue or manipulators and is thus not always visible during the procedure. This is especially true in pediatric surgery, where the surgical environment is smaller than in adult surgery. Hence, surgeons must carefully track the instruments and tissues to understand and predict their current and expected situations. In this article, we used gaze-tracking techniques to analyze the location and timing of the gaze of participants while they manipulated a virtual robotic surgical simulation system. To differentiate between the ideal and actual viewpoint trajectories, we conducted experiments with and without obstacles (i.e., simulated tissue and the manipulator arm). In the obstacle condition, we modulated the wrist length of the manipulator to bring it into view. In the no-obstacle condition, the participants mostly watched the suture needle tip. In the with-obstacle condition, the participants spent less time watching the instruments and more time watching the target point. The amount of time spent watching the target point increased as wrist length increased. Given this tradeoff relationship, we examined the proportion of time the participants spent looking at the instruments or target points by wrist length. We calculated the Pareto solutions and clarified the relationship between wrist length and the watching parts.},
  archive      = {J_THMS},
  author       = {Satoshi Miura and Ryutaro Ohta and Yang Cao and Kazuya Kawamura and Yo Kobayashi and Masakatsu G. Fujie},
  doi          = {10.1109/THMS.2021.3076038},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {376-383},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Using operator gaze tracking to design wrist mechanism for surgical robots},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Incentive mechanism for mobile devices in dynamic crowd
sensing system. <em>THMS</em>, <em>51</em>(4), 365–375. (<a
href="https://doi.org/10.1109/THMS.2020.3032102">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile crowdsensing (MCS) has gained much attention due to the proliferation of smart devices equipped with powerful sensors. Large-scale users are the foundation of MCS, so designing incentive mechanisms to motivate users to participate in MCS is necessary. Existing works on incentive mechanisms usually assume a scenario where a group of tasks arrive at the platform at the same time and are immediately assigned to users. We argue that a more realistic MCS scenario can delay a task, which is called the assignment duration time, to wait for appropriate users. In this scenario, we focus on proposing a truthful incentive mechanism to reduce the overall social cost. Due to the uncertainty of coming users, the problems of selecting the appropriate users and calculating the payment for each recruited user (winner) are more complicated. To overcome these challenges, we design a dynamic truthful incentive mechanism (DTIM) including winner selection and payment decision processes. The former uniformly recruits users before the assignment deadline of tasks and dynamically readjusts the recruiting frequency of other tasks to select winners iteratively, which achieves an approximation ratio. Furthermore, the latter determines truthful payment for each winner to encourage user participation as well as avoid being deceived, which achieves truthfulness, individual rationality, and computational efficiency. Finally, massive simulations based on a real dataset roma/taxi validate the DTIM, which can effectively reduce the overall social cost and make a truthful payment for each winner.},
  archive      = {J_THMS},
  author       = {Hengzhi Wang and Yongjian Yang and En Wang and Liang Wang and Qiang Li and Zhiyong Yu},
  doi          = {10.1109/THMS.2020.3032102},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {365-375},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Incentive mechanism for mobile devices in dynamic crowd sensing system},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sequential weakly labeled multiactivity localization and
recognition on wearable sensors using recurrent attention networks.
<em>THMS</em>, <em>51</em>(4), 355–364. (<a
href="https://doi.org/10.1109/THMS.2021.3086008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the popularity and development of the wearable devices such as smartphones, human activity recognition (HAR) based on sensors has become as a key research area in human computer interaction and ubiquitous computing. The emergence of deep learning leads to a recent shift in the research of HAR, which requires massive strictly labeled data in supervised learning scenario. In comparison with video data, activity data recorded from accelerometer or gyroscope are often more difficult to interpret and segment. Recently, several attention mechanisms are proposed to handle the weakly labeled human activity data, which do not require accurate data annotation. However, these attention-based models can only handle the weakly labeled dataset whose sample includes one target activity, as a result it limits efficiency and practicality. In the article, we propose a recurrent attention networks (RAN) to handle sequential weakly labeled multiactivity recognition and location tasks. The model can repeatedly perform steps of attention on multiple activities of one sample and each step is corresponding to the current focused activity. The effectiveness of the RAN model is validated on a collected sequential weakly labeled multiactivity dataset and the other two public datasets. The experiment results show that our RAN model can simultaneously infer multiactivity types from the coarse-grained sequential weak labels and determine specific locations of every target activity with only knowledge of which types of activities contained in the long sequence. It will greatly reduce the burden of manual labeling. 1},
  archive      = {J_THMS},
  author       = {Kun Wang and Jun He and Lei Zhang},
  doi          = {10.1109/THMS.2021.3086008},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {355-364},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Sequential weakly labeled multiactivity localization and recognition on wearable sensors using recurrent attention networks},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A novel application of flexible inertial sensors for
ambulatory measurement of gait kinematics. <em>THMS</em>,
<em>51</em>(4), 346–354. (<a
href="https://doi.org/10.1109/THMS.2021.3086017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ambulatory motion sensors can generate a mobile system for human kinematics measurement as an alternative to the more constrained optical motion capture systems. Flexible inertial sensors are emerging as a potential solution to overcome the drawback of the conventional inertial measurement sensors with a soft, flexible design that allows for direct body-conforming adhesion to the skin. Thus, this study aims to establish the viability of the flexible inertial sensors for the ambulatory measurement of human knee flexion-extension (F/E) during level walking. The accuracy of flexible inertial sensors with our proposed algorithm is investigated by comparing the derived kinematics with those measured using a conventional motion capture system. Three volunteers (two males and one female, aged 24-27) served as subjects. Three flexible inertial sensors were placed on the subjects&#39; right legs, two on the thigh (lateral thigh and distal anterolateral thigh), and one on the lateral shank, and subjects performed level walking with varied walking cadence (80, 100, and 120 steps/min). The overall root-mean-square-error across all participants is 7.8° for distal anterolateral thigh sensor (using the data of distal anterolateral thigh sensor and shank sensor to estimate knee F/E) and 4.5° for lateral thigh sensor (using the data of lateral thigh sensor and shank sensor to estimate knee F/E). The results illustrate that our proposed method can generate comparable results with motion capture systems and is insensitive to different walking cadence. A significant effect of sensor placement is observed, and a more proximal lateral thigh strategy is suggested. The advantages of the flexible inertial sensors in human motion analysis over the existing inertial measurement units are also highlighted.},
  archive      = {J_THMS},
  author       = {Wei Yin and Curran Reddy and Yu Zhou and Xudong Zhang},
  doi          = {10.1109/THMS.2021.3086017},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {346-354},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {A novel application of flexible inertial sensors for ambulatory measurement of gait kinematics},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Cobots in industry 4.0: A roadmap for future practice
studies on human–robot collaboration. <em>THMS</em>, <em>51</em>(4),
335–345. (<a href="https://doi.org/10.1109/THMS.2021.3092684">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the vision of Industry 4.0 and cobots, working conditions in industrial settings are starting to change. We review related literature from the fields of human–robot interaction, work and organizational psychology, and sociology of work, as well as an exemplary project case study, and identify research gaps regarding the implications of cobots for work environments. We argue that we are in a transition phase from automation to actual collaboration with robots in manufacturing, and that this will open up a new problem space for investigations, in which a practice lens will be crucial. Based on this, we propose a research agenda for social practice and workplace studies to explore the sociotechnical environment of Industry 4.0 involving cobots at the individual, team, and organizational levels.},
  archive      = {J_THMS},
  author       = {Astrid Weiss and Ann-Kathrin Wortmeier and Bettina Kubicek},
  doi          = {10.1109/THMS.2021.3092684},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {335-345},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Cobots in industry 4.0: A roadmap for future practice studies on Human–Robot collaboration},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Best viewpoints for external robots or sensors assisting
other robots. <em>THMS</em>, <em>51</em>(4), 324–334. (<a
href="https://doi.org/10.1109/THMS.2021.3090765">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work creates a model of the value of different external viewpoints of a robot performing tasks. The current state of the practice is to use a teleoperated assistant robot to provide a view of a task being performed by a primary robot; however, the choice of viewpoints is ad hoc and does not always lead to improved performance. This research applies a psychomotor approach to develop a model of the relative quality of external viewpoints using Gibsonian affordances. In this approach, viewpoints for the affordances are rated based on the psychomotor behavior of human operators and clustered into manifolds of viewpoints with the equivalent value. The value of 30 viewpoints is quantified in a study with 31 expert robot operators for four affordances (reachability, passability, manipulability, and traversability) using a computer-based simulator of two robots. The adjacent viewpoints with similar values are clustered into ranked manifolds using agglomerative hierarchical clustering. The results show the validity of the affordance-based approach by confirming that there are manifolds of statistically significantly different viewpoint values, viewpoint values are statistically significantly dependent on the affordances, and viewpoint values are independent of a robot. Furthermore, the best manifold for each affordance provides a statistically significant improvement with a large Cohen&#39;s $d$ effect size (1.1–2.3) in the performance (improving time by 14%–59% and reducing errors by 87%–100%) and improvement in the performance variation over the worst manifold. This model will enable autonomous selection of the best possible viewpoint and path planning for the assistant robot.},
  archive      = {J_THMS},
  author       = {Jan Dufek and Xuesu Xiao and Robin R. Murphy},
  doi          = {10.1109/THMS.2021.3090765},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {324-334},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Best viewpoints for external robots or sensors assisting other robots},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Toward long-term FMG model-based estimation of applied hand
force in dynamic motion during human–robot interactions. <em>THMS</em>,
<em>51</em>(4), 310–323. (<a
href="https://doi.org/10.1109/THMS.2021.3087902">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Physical human-robot interaction (pHRI) is reliant on human actions and can be addressed by studying human upper-limb motions during interactions. Use of force myography (FMG) signals, which detect muscle contractions, can be useful in developing machine learning algorithms as controls. In this paper, a novel long-term calibrated FMG-based trained model is presented to estimate applied force in dynamic motion during real-time interactions between a human and a linear robot. The proposed FMG-based pHRI framework was investigated in new, unseen, real-time scenarios for the first time. Initially, a long-term reference dataset (multiple source distributions) of upper-limb FMG data was generated as five participants interacted with the robot applying force in five different dynamic motions. Ten other participants interacted with the robot in two intended motions to evaluate the out-of-distribution (OOD) target data (new, unlearned), which was different than the population data. Two practical scenarios were considered for assessment: i) a participant applied force in a new, unlearned motion (scenario 1), and ii) a new, unlearned participant applied force in an intended motion (scenario 2). In each scenario, few long-term FMG-based models were trained using a baseline dataset [reference dataset (scenario 1, 2) and/or a learnt participant dataset (scenario 1)] and a calibration dataset (collected during evaluation). Real-time evaluation showed that the proposed long-term calibrated FMG-based models (LCFMG) could achieve estimation accuracies of 80%-94% in all scenarios. These results are useful towards integrating and generalizing human activity data in a robot control scheme by avoiding extensive HRI training phase in regular applications.},
  archive      = {J_THMS},
  author       = {Umme Zakia and Carlo Menon},
  doi          = {10.1109/THMS.2021.3087902},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {310-323},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Toward long-term FMG model-based estimation of applied hand force in dynamic motion during Human–Robot interactions},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Human-machine interaction sensing technology based on hand
gesture recognition: A review. <em>THMS</em>, <em>51</em>(4), 300–309.
(<a href="https://doi.org/10.1109/THMS.2021.3086003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human machine interaction (HMI) is an interactive way of information exchange between human and machine. By collecting the information that can be conveyed by the person to express the intention, and then transforming and processing the information, the machine can work according to the intention of the person. However, the traditional HMI including mouse, keyboard etc. usually requires a fixed operating space, which limits people&#39;s actions and cannot directly reflect people&#39;s intentions. It requires people to learn systematically how to operate skillfully, which indirectly affects work efficiency. Hand gesture, as one of the important ways for human to convey information and express intuitive intention, has the advantages of high degree of differentiation, strong flexibility and high efficiency of information transmission, which makes hand gesture recognition (HGR) as one of the research hotspots in the field of HMI. In order to enable readers to systematically and quickly understand the research status of HGR and grasp the basic problems and development direction of HGR, this article takes the sensing method used by HGR technology as the entry point, and makes a detailed elaboration and systematic summary by referring to a large number of research achievements in recent years.},
  archive      = {J_THMS},
  author       = {Lin Guo and Zongxing Lu and Ligang Yao},
  doi          = {10.1109/THMS.2021.3086003},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {300-309},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Human-machine interaction sensing technology based on hand gesture recognition: A review},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Who/what is my teammate? Team composition considerations in
human–AI teaming. <em>THMS</em>, <em>51</em>(4), 288–299. (<a
href="https://doi.org/10.1109/THMS.2021.3086018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There are many unknowns regarding the characteristics and dynamics of human-AI teams, including a lack of understanding of how certain human-human teaming concepts may or may not apply to human-AI teams and how this composition affects team performance. This article outlines an experimental research study that investigates essential aspects of human-AI teaming such as team performance, team situation awareness, and perceived team cognition in various mixed composition teams (human-only, human-human-AI, human-AI-AI, and AI-only) through a simulated emergency response management scenario. Results indicate dichotomous outcomes regarding perceived team cognition and performance metrics, as perceived team cognition was not predictive of performance. Performance metrics like team situational awareness and team score showed that teams composed of all human participants performed at a lower level than mixed human-AI teams, with the AI-only teams attaining the highest performance. Perceived team cognition was highest in human-only teams, with mixed composition teams reporting perceived team cognition 58% below the all-human teams. These results inform future mixed teams of the potential performance gains in utilizing mixed teams&#39; over human-only teams in certain applications, while also highlighting mixed teams&#39; adverse effects on perceived team cognition.},
  archive      = {J_THMS},
  author       = {Nathan J. McNeese and Beau G. Schelble and Lorenzo Barberis Canonico and Mustafa Demir},
  doi          = {10.1109/THMS.2021.3086018},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {288-299},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Who/What is my teammate? team composition considerations in Human–AI teaming},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Predicting emotion reactions for human–computer
conversation: A variational approach. <em>THMS</em>, <em>51</em>(4),
279–287. (<a href="https://doi.org/10.1109/THMS.2020.3044975">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although emotional conversation generation has attracted widespread attention in recent years, research works on the emotional interactional mechanism are still scarce, which makes it difficult for existing emotional dialogue system to automatically determine a suitable emotion type for conversation generation. Such a response emotion planning task is often difficult due to the “gap” problem: we need to predict the emotional probability distribution of the upcoming responses, which have not yet been generated. In this article, we propose a novel method, namely interactional emotion learning (IEL), which adopts an intuitive way to eliminate the “gap” problem: we design a variational learning network, called potential response learning, to learn the latent distribution of responses for given conversation in a semantic space. Then, we predict an appropriate emotion for response generation based on both the dialogue context and the learned latent distribution. Extensive experiments have been performed on three off-the-shelf conversation datasets, and the experimental results show that the proposed variational learning network significantly boosts the prediction ability of our approach, and our IEL method outperforms the state-of-the-art dialogue classification methods in the emotion planning task.},
  archive      = {J_THMS},
  author       = {Rui Zhang and Zhenyu Wang and Zhenhua Huang and Li Li and Mengdan Zheng},
  doi          = {10.1109/THMS.2020.3044975},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {279-287},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Predicting emotion reactions for Human–Computer conversation: A variational approach},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021d). IEEE systems, man, and cybernetics society information.
<em>THMS</em>, <em>51</em>(3), C3. (<a
href="https://doi.org/10.1109/THMS.2021.3079829">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Provides a listing of current committee members and society officers.},
  archive      = {J_THMS},
  doi          = {10.1109/THMS.2021.3079829},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {6},
  number       = {3},
  pages        = {C3},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {IEEE systems, man, and cybernetics society information},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Investigating the p300 response as a marker of working
memory in virtual training environments. <em>THMS</em>, <em>51</em>(3),
265–277. (<a href="https://doi.org/10.1109/THMS.2021.3064815">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conventional performance metrics fail to offer high-resolution evaluation of learning and memory during training tasks; the P300 component of the event-related potential (ERP) is a promising tool for enhancing the assessment of training quality in virtual environments, but this technique is yet to be investigated. A driver training simulator and scenario were developed to explore the capability of the P300 for this purpose. A user study was conducted with 32 participants divided into two groups objectively determined by driving performance scores, thus enabling observations of the P300 response to be equated to varying levels of learning and memory. Participant electroencephalogram data were recorded during the procedure, which was postprocessed to filter and extract ERPs to capture neural responses to specific events in the virtual training scenario. These were combined to produce a result for each participant, which was then grand averaged to create an overall ERP for each group. Across the eight electrode sites, statistically significant differences were found between the grand average waveforms of the two groups, with high memory retention producing significantly greater peak-to-peak amplitude (U = 9.00, p = 0.045), peak latency (U = 0.00, p $&lt;; $ 0.001), and positive area (U = 13.00, p = 0.05) of the waveform than low memory retention. The evidenced relationship between the P300 response and working memory in this context suggests that it has the potential for monitoring learning and memory in stimulus-driven virtual training systems.},
  archive      = {J_THMS},
  author       = {Thomas G. Simpson and Karen Rafferty},
  doi          = {10.1109/THMS.2021.3064815},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {6},
  number       = {3},
  pages        = {265-277},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Investigating the p300 response as a marker of working memory in virtual training environments},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Neurophysiological evaluation of haptic feedback for
myoelectric prostheses. <em>THMS</em>, <em>51</em>(3), 253–264. (<a
href="https://doi.org/10.1109/THMS.2021.3066856">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Evaluations of haptic feedback in myoelectric prostheses are generally limited to task performance outcomes, which while necessary, fail to capture the mental effort of the user operating the prosthesis. Cognitive load is usually investigated with reaction time metrics and secondary task accuracy, which are indirect, and may not capture the time-varying nature of mental effort. Here, we propose wearable, wireless functional near infrared spectroscopy (fNIRS) neuroimaging to provide a continuous direct assessment of operator mental effort during use of a prosthesis. Utilizing fNIRS in a two-alternative forced-choice stiffness discrimination task, we asked participants to differentiate objects using their natural hand, a (traditional) myoelectric prosthesis without sensory feedback, and a myoelectric prosthesis with haptic (vibrotactile) feedback of grip force. Results showed that discrimination accuracy and mental effort are optimal with the natural hand, followed by the prosthesis featuring haptic feedback, and then the traditional prosthesis, particularly for objects whose stiffness were difficult to differentiate. This experiment highlights the utility of haptic feedback in improving task performance and lowering cognitive load for prosthesis use, and demonstrates the potential for fNIRS to provide a robust measure of cognitive effort for other human-in-the-loop systems.},
  archive      = {J_THMS},
  author       = {Neha Thomas and Garrett Ung and Hasan Ayaz and Jeremy D. Brown},
  doi          = {10.1109/THMS.2021.3066856},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {6},
  number       = {3},
  pages        = {253-264},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Neurophysiological evaluation of haptic feedback for myoelectric prostheses},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Assessment of machine learning models for classification of
movement patterns during a weight-shifting exergame. <em>THMS</em>,
<em>51</em>(3), 242–252. (<a
href="https://doi.org/10.1109/THMS.2021.3059716">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In exercise gaming (exergaming), reward systems are typically based on rules/templates from joint movement patterns. These rules or templates need broad ranges in definitions of correct movement patterns to accommodate varying body shapes and sizes. This can lead to inaccurate rewards and, thus, inefficient exercise, which can be detrimental to progress. If exergames are to be used in serious settings like rehabilitation, accurate rewards for correctly performed movements are crucial. This article aims to investigate the level of accuracy machine learning/deep learning models can achieve in classification of correct repetitions naturally elicited from a weight-shifting exergame. Twelve healthy elderly (10F, age 70.4 SD 11.4) are recruited. Movements are captured using a marker-based 3-D motion-capture system. Random forest (RF), support vector machine, k-nearest neighbors, and multilayer perceptron (MLP) are the employed models, trained and tested on whole body movement patterns and on subsets of joints. MLP and RF reached the highest recall and F1-score, respectively, when using combined data from joint subsets. MLP recall range are 91% to 94%, and RF F1-score range 79% to 80%. MLP and RF also reached the highest recall and F1-score in each joint subset, respectively. Here, MLP ranged from 93% to 97% recall, while RF ranged from 73% to 80% F1-score. Recall results, show that &gt;9 out of 10 repetitions are classified correctly, indicating that MLP/RF can be used to identify correctly performed repetitions of a weight-shifting exercise when using full-body data and when using joint subset data.},
  archive      = {J_THMS},
  author       = {Elise Klæbo Vonstad and Beatrix Vereijken and Kerstin Bach and Xiaomeng Su and Jan Harald Nilsen},
  doi          = {10.1109/THMS.2021.3059716},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {6},
  number       = {3},
  pages        = {242-252},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Assessment of machine learning models for classification of movement patterns during a weight-shifting exergame},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Implementation of a character recognition system based on
finger-joint tracking using a depth camera. <em>THMS</em>,
<em>51</em>(3), 229–241. (<a
href="https://doi.org/10.1109/THMS.2021.3066854">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The joint tracking-based writing system refers to writing characters by changing the position of the finger joint. It is a new research field in interaction-based input systems. However, joint tracking is a very challenging task. In this article, we present a new method for a finger-joint tracking-based character recognition system using a 3-D camera. The proposed method tracks the finger-joint from 3-D information to identify a numerical digit, alphabet, character, special key, or symbol using the distance between the thumb tip and another finger-joint. The recognition is based on Euclidean distance thresholding and geometric slope techniques. Joint data are stored in a 3-D matrix to assign the 3-D coordinate values. The exact character is identified according to the specified definitions. First, a single hand-based digit recognition method is introduced, in which the left or right hand is used. Second, a double hand-based writing system is presented in which both hands are used simultaneously; this system features a full keyboard with 124 different characters. Our results show an overall accuracy of 91.95% and 91.85% for single-hand and double-hand recognition, respectively; with a recognition time of less than 60 ms for each character. An important contribution of this article is that the proposed system can work in both light and dark environments, requires only a small computation area and has a large number of character sets (124 characters). In addition, a region-based user study has been conducted to verify the approach.},
  archive      = {J_THMS},
  author       = {Md. Shahinur Alam and Ki-Chul Kwon and Nam Kim},
  doi          = {10.1109/THMS.2021.3066854},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {6},
  number       = {3},
  pages        = {229-241},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Implementation of a character recognition system based on finger-joint tracking using a depth camera},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Archery skill assessment using an acceleration sensor.
<em>THMS</em>, <em>51</em>(3), 221–228. (<a
href="https://doi.org/10.1109/THMS.2020.3046435">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A key skill in archery is the ability to suppress postural tremor while aiming at a target. Providing feedback during daily archery practice is a potentially effective way of suppressing tremor. However, postural tremor is subtle and difficult to measure using vision-based techniques. This article proposes a feedback method that uses a bow equipped with a small, lightweight acceleration sensor. First, we automatically detect an archer&#39;s shooting execution cycle, including the aiming, release, and follow-through phases, by using binary classification, and then, we quantify postural tremor during aiming. Then, from the quantified postural tremor, we regress the expected total score that the archer would obtain in a series of shots during a real game. We performed an experiment with 11 members of a university archery club and achieved 1) a precision of 0.72 and recall of 0.80 in shooting detection and 2) an absolute correlation coefficient of 0.74 in score prediction with leave-one-subject-out cross-validation.},
  archive      = {J_THMS},
  author       = {Takayuki Ogasawara and Hanako Fukamachi and Kenryu Aoyagi and Shiro Kumano and Hiroyoshi Togo and Koichiro Oka},
  doi          = {10.1109/THMS.2020.3046435},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {6},
  number       = {3},
  pages        = {221-228},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Archery skill assessment using an acceleration sensor},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An improved IOHMM-based stochastic driver lane-changing
model. <em>THMS</em>, <em>51</em>(3), 211–220. (<a
href="https://doi.org/10.1109/THMS.2021.3066851">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The prediction and estimation of the lane-changing state of the host car and surrounding cars are important parts of an advanced driving assistant system, which mainly depend on the understanding of the driver lane-changing behavior. To learn driver lane-changing maneuver well, this article provides a novel stochastic driver lane-changing model based on an improved input–output hidden Markov model (IOHMM) framework. First, an improved IOHMM is proposed to address the deficiency that the traditional IOHMM cannot remember previous data and describe continuous output. Then, based on the improved IOHMM framework, a driver lane-changing model is established considering the intention and behavior of the driver in the lane-changing process. The model parameters can be learned from the collected lane-changing data using the maximum likelihood estimation and generalized estimation-maximization methods. Finally, the model is applied to a real driver lane-changing process. It is verified that the proposed model has good performance in predicting the future motion maneuver of the host vehicle and estimating the current motion state of the surrounding cars.},
  archive      = {J_THMS},
  author       = {Qingyun Chen and Wanzhong Zhao and Can Xu and Chunyan Wang and Lin Li and Shijuan Dai},
  doi          = {10.1109/THMS.2021.3066851},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {6},
  number       = {3},
  pages        = {211-220},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {An improved IOHMM-based stochastic driver lane-changing model},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Driving skill modeling using neural networks for
performance-based haptic assistance. <em>THMS</em>, <em>51</em>(3),
198–210. (<a href="https://doi.org/10.1109/THMS.2021.3061409">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article addresses a data-driven framework, modeling expert driving skills for performance-based haptic assistance using neural networks (NNs). We have built a haptic driving training simulator to collect expert driving data and to provide proper haptic feedback. We establish an expert driving skill model by training NNs with the collected data. Then, the skill model is applied to the performance-based haptic assistance to provide optimized references of the steering/pedaling movements. We evaluate the skill model and its application to the performance-based haptic assistance in two user experiments. The results of the first experiment demonstrate that our skill model has appropriately captured experts&#39; steering/pedaling skills. The results of the second experiment show that our performance-based haptic assistance can help novice drivers perform steering as expert drivers, but cannot assist their pedaling performance.},
  archive      = {J_THMS},
  author       = {Hojin Lee and Hyoungkyun Kim and Seungmoon Choi},
  doi          = {10.1109/THMS.2021.3061409},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {6},
  number       = {3},
  pages        = {198-210},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Driving skill modeling using neural networks for performance-based haptic assistance},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). How does explanation-based knowledge influence driver
take-over in conditional driving automation? <em>THMS</em>,
<em>51</em>(3), 188–197. (<a
href="https://doi.org/10.1109/THMS.2021.3051342">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article focuses on explanation-based knowledge about system limitations (SLs) under conditional driving automation (society of automotive engineers level 3) and aims to reveal how this knowledge influences driver intervention. By illustrating the relationships between the driving environment, system, and mental model, knowledge in dynamic decision-making processing for responding to an issued request to intervene (RtI), occurrence of SL, concept of RtI, and scene(s) related to SL are determined by knowledge-based learning. Based on three concepts, the knowledge is examined at five levels: 1) no explanation, 2) occurrence of SL, 3) concept of RtI, 4) some typical scenes related to SL, and 5) all of the above. Data collection is conducted on a driving simulator, and 100 people with no experience of automated driving participated. The experimental results show that instructing drivers in typical situations contributes to a greater increase in the rate of successful intervention in car control from 55% to 95%. Furthermore, instructing them on the concept of RtI is conducive to a significant reduction in response time from 5.48 to 3.62 s in their first experience of RtI. It is also revealed that the knowledge-based learning effect dwindles but does not vanish even after drivers experience RtI a number of times. Compared to explaining all possible situations to a driver, introducing typical situations results in better take-over performances even in critical or unexplained scenarios. This article demonstrates the importance and necessity of this knowledge, especially the explanation of sample scenes related to SL, which contributes to drivers&#39; take-over behavior.},
  archive      = {J_THMS},
  author       = {Huiping Zhou and Makoto Itoh and Satoshi Kitazaki},
  doi          = {10.1109/THMS.2021.3051342},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {6},
  number       = {3},
  pages        = {188-197},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {How does explanation-based knowledge influence driver take-over in conditional driving automation?},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Does explicit categorization taxonomy facilitate performing
goal-directed task analysis? <em>THMS</em>, <em>51</em>(3), 177–187. (<a
href="https://doi.org/10.1109/THMS.2021.3066456">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Situation awareness (SA) is an important factor that affects the performance of operators who work in complex environments. SA is defined as the perception of elements in an environment (level 1), understanding their meaning (level 2), and the projection of their state into the future (level 3). The first step to assess SA is identifying its requirements, for which goal-directed task analysis (GDTA) is the recommended technique. GDTA is a type of cognitive task analysis that focuses on the goals that a human operator must achieve, and the information required to accomplish them. The result of GDTA is a list of information (SA elements) that are categorized into the three SA levels. However, GDTA-based studies typically categorize SA elements into the three SA levels without stating their categorization rules. Therefore, this article proposes a taxonomy to categorize SA elements obtained via GDTA into SA levels. First, we present the results of a systematic literature review (N = 87) to gain insight into how analysts apply their classification criteria. Then, we propose a categorization taxonomy based on the ISO 15939 standard. To validate the proposed taxonomy, we analyze a subset of GDTA results in two cases, and the results of the analysis show that the proposed categorization rules are applicable to the two analyzed cases.},
  archive      = {J_THMS},
  author       = {Ayad Nasser-dine and Alexandre Moïse and James Lapalme},
  doi          = {10.1109/THMS.2021.3066456},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {6},
  number       = {3},
  pages        = {177-187},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Does explicit categorization taxonomy facilitate performing goal-directed task analysis?},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021e). IEEE systems, man, and cybernetics society information.
<em>THMS</em>, <em>51</em>(2), C3. (<a
href="https://doi.org/10.1109/THMS.2021.3062976">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Provides a listing of current committee members and society officers.},
  archive      = {J_THMS},
  doi          = {10.1109/THMS.2021.3062976},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {4},
  number       = {2},
  pages        = {C3},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {IEEE systems, man, and cybernetics society information},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Holoscopic 3D microgesture recognition by deep neural
network model based on viewpoint images and decision fusion.
<em>THMS</em>, <em>51</em>(2), 162–171. (<a
href="https://doi.org/10.1109/THMS.2020.3047914">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Finger microgestures have been widely used in human computer interaction (HCI), particularly for interactive applications, such as virtual reality (VR) and augmented reality (AR) technologies, to provide immersive experience. However, traditional 2D image-based microgesture recognition suffers from low accuracy due to the limitations of 2D imaging sensors, which have no depth information. In this article, we proposed an innovative 3D microgesture recognition system based on a holoscopic 3D imaging sensor. Due to the lack of holoscopic 3D datasets, a comprehensive holoscopic 3D microgesture (HoMG) database is created and used to develop a robust 3D microgesture recognition method. Then, a fast algorithm is proposed to extract multiviewpoint images from one holoscopic image. Furthermore, we applied a CNN model with an attention-based residual block to each viewpoint image to improve the algorithm performance. Finally, bagging classification tree decision-level fusion is applied to combine the predictions. The experimental results demonstrate that the proposed method outperforms state-of-the-art methods and delivers a better accuracy than existing methods.},
  archive      = {J_THMS},
  author       = {Yi Liu and Min Peng and Mohammad Rafiq Swash and Tong Chen and Rui Qin and Hongying Meng},
  doi          = {10.1109/THMS.2020.3047914},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {4},
  number       = {2},
  pages        = {162-171},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Holoscopic 3D microgesture recognition by deep neural network model based on viewpoint images and decision fusion},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Assessing collaborative physical tasks via gestural
analysis. <em>THMS</em>, <em>51</em>(2), 152–161. (<a
href="https://doi.org/10.1109/THMS.2021.3051305">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent studies have shown that gestures are useful indicators of understanding, learning, and memory retention. However, and specially in collaborative settings, current metrics that estimate task understanding often neglect the information expressed through gestures. This work introduces the physical instruction assimilation (PIA) metric, a novel approach to estimate task understanding by analyzing the way in which collaborators use gestures to convey, assimilate, and execute physical instructions. PIA estimates task understanding by inspecting the number of necessary gestures required to complete a shared task. PIA is calculated based on the multiagent gestural instruction comparer (MAGIC) architecture, a previously proposed framework to represent, assess, and compare gestures. To evaluate our metric, we collected gestures from collaborators remotely completing the following three tasks: block assembly, origami, and ultrasound training. The PIA scores of these individuals are compared against two other metrics used to estimate task understanding: number of errors and amount of idle time during the task. Statistically significant correlations between PIA and these metrics are found. Additionally, a Taguchi design is used to evaluate PIA&#39;s sensitivity to changes in the MAGIC architecture. The factors evaluated the effect of changes in time, order, and motion trajectories of the collaborators&#39; gestures. PIA is shown to be robust to these changes, having an average mean change of 0.45. These results hint that gestures, in the form of the assimilation of physical instructions, can reveal insights of task understanding and complement other commonly used metrics.},
  archive      = {J_THMS},
  author       = {Edgar Rojas-Muñoz and Juan Wachs},
  doi          = {10.1109/THMS.2021.3051305},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {4},
  number       = {2},
  pages        = {152-161},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Assessing collaborative physical tasks via gestural analysis},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Characteristics that make linear time-invariant dynamic
systems difficult for humans to control. <em>THMS</em>, <em>51</em>(2),
141–151. (<a href="https://doi.org/10.1109/THMS.2020.3046164">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present results from an experiment in which 55 human subjects interact with a dynamic system 40 times over a one-week period. The subjects are divided into five groups. For each interaction, a subject performs a command-following task, where the reference command is the same for all subjects and all trials; however, each group interacts with a different linear time-invariant dynamic system. We use a subsystem identification algorithm to estimate the control strategy that each subject uses on each trial. The experimental and identification results are used to examine the impact of the system characteristics (e.g., poles, zeros, relative degree, system order, phase lag) on the subjects’ command-following performance and the control strategies that the subjects learn. Results demonstrate that phase lag (which arises from higher relative degree and nonminimum-phase zeros) tends to make dynamic systems more difficult for humans to control, whereas higher system order does not necessarily make a system more difficult to control. The identification results demonstrate that improvement in performance is attributed to: 1) using a comparatively accurate approximation of the inverse dynamics in feedforward; and 2) using a feedback controller with comparatively high gain. Results also demonstrate that system phase lag is an important impediment to a subject&#39;s ability to approximate the inverse dynamics in feedforward, and that a key aspect of approximating the inverse dynamics in feedforward is learning to use the correct amount of phase lead in feedforward.},
  archive      = {J_THMS},
  author       = {Seyyed Alireza Seyyed Mousavi and Xingye Zhang and T. Michael Seigler and Jesse B. Hoagg},
  doi          = {10.1109/THMS.2020.3046164},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {4},
  number       = {2},
  pages        = {141-151},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Characteristics that make linear time-invariant dynamic systems difficult for humans to control},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Electromyography for teleoperated tasks in weightlessness.
<em>THMS</em>, <em>51</em>(2), 130–140. (<a
href="https://doi.org/10.1109/THMS.2020.3047975">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The cooperation between robots and astronauts will become a core element of future space missions. This is accompanied by the demand for suitable input devices. An interface based on electromyography (EMG) represents a small, light, and wearable device to generate a continuous three-dimensional (3D) control signal from voluntarily muscle activation of the operator&#39;s arm. We analyzed the influence of microgravity on task performance during a two-dimensional (2D) task on a screen. Six subjects performed aiming and tracking tasks in parabolic flights. Three different levels of fixation-fixed feet using foot straps, semi-free by using a foot rail, and free-floating feet-are tested to investigate how much user fixation is required to operate via the interface. The user study showed that weightlessness affects the usage of the interface only to a small extent. Success rates between 89${\%}$ and 96${\%}$ are reached within all conditions during microgravity. A significant effect between 0 and 1G could not be identified for the test series of fixed and semi-free feet, while free-floating feet showed significantly worse results in fine and gross motion times in 0G compared to ground tests (with success rates of 92${\%}$ for 0G and 99${\%}$ for 1G). Further adaptation to the altered proprioception may be needed here. Hence, foot rails as already mounted in the International Space Station (ISS) would be sufficient to use the interface in weightlessness. Low impact of microgravity, high success rates, and an easy handling of the system, indicates a high potential of an EMG-based interface for teleoperation in space.},
  archive      = {J_THMS},
  author       = {Annette Hagengruber and Ulrike Leipscher and Bjoern M. Eskofier and Jörn Vogel},
  doi          = {10.1109/THMS.2020.3047975},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {4},
  number       = {2},
  pages        = {130-140},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Electromyography for teleoperated tasks in weightlessness},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Use of mobile EEG in decoding hand movement speed and
position. <em>THMS</em>, <em>51</em>(2), 120–129. (<a
href="https://doi.org/10.1109/THMS.2021.3056274">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the majority of brain–computer interface (BCI) research currently restricted to the controlled settings in labs, there is a growing interest to study the feasibility of BCI applications in the real world. This interest is largely driven by the availability of ergonomic electroencephalography (EEG) recording devices. In this article, we investigate the feasibility of applying a commercial EEG device in motor decoding and propose a signal processing strategy to classify and reconstruct hand movement speed and position. An experiment is designed to simultaneously record EEG and hand position, while the user executes movement toward the left or right direction at two different speeds. A visual interface is designed to guide the user to perform the task under each condition. Data are recorded from 21 subjects. The classification of direction-dependent and -independent speeds is implemented using spatial and spectral features. An optimized movement parameter estimation strategy is proposed to reconstruct the instantaneous position and speed of the hand from EEG. Average performances of $73.36\ ({ \pm \! 11.95})\% $ , $69.46\ ({ \pm \! 13.39})\% $ , and $68.98\ ({ \pm \! 14.79})\% $ are obtained for direction independent, right and left direction fast versus slow classification, respectively. The average correlation between recorded and reconstructed hand position and speed along the x -axis is in the range of [0.22, 0.57] and [0.26, 0.58], respectively. The reported results validate the use of commercial-grade EEG and low-frequency components of EEG relevant to motor decoding for real-world motor kinematics BCI applications.},
  archive      = {J_THMS},
  author       = {Neethu Robinson and Tan Wei Jie Chester and Smitha KG},
  doi          = {10.1109/THMS.2021.3056274},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {4},
  number       = {2},
  pages        = {120-129},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Use of mobile EEG in decoding hand movement speed and position},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A self-paced BCI with a collaborative controller for highly
reliable wheelchair driving: Experimental tests with physically disabled
individuals. <em>THMS</em>, <em>51</em>(2), 109–119. (<a
href="https://doi.org/10.1109/THMS.2020.3047597">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Brain-controlled wheelchairs (BCWs) are a promising solution for people with severe motor disabilities, who cannot use conventional interfaces. However, the low reliability of electroencephalographic signal decoding and the high user&#39;s workload imposed by continuous control of a wheelchair requires effective approaches. In this article, we propose a self-paced P300-based brain-computer interface (BCI) combined with dynamic time-window commands and a collaborative controller. The self-paced approach allows users to switch between control and noncontrol states without requiring any additional task or mental strategy, while the dynamic time-window commands allow balancing the reliability and speed of the BCI. The collaborative controller, combining user&#39;s intentions and navigation information, offers the possibility to navigate in complex environments and to improve the overall system reliability. The feasibility of the proposed approach and the impact of each system component (self-paced, dynamic time window, and collaborative controller) are systematically validated in a set of experiments conducted with seven able-bodied participants and six physically disabled participants steering a robotic wheelchair in real-office-like environments. These two groups controlled the BCW with a final driving accuracy greater than 99%. Quantitative and subjective results, assessed through questionnaires, attest to the effectiveness of the proposed approach. Altogether, these findings contribute to improving the usability of BCWs and, hence, the potential for their use by target users in home settings.},
  archive      = {J_THMS},
  author       = {Aniana Cruz and Gabriel Pires and Ana Lopes and Carlos Carona and Urbano J. Nunes},
  doi          = {10.1109/THMS.2020.3047597},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {4},
  number       = {2},
  pages        = {109-119},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {A self-paced BCI with a collaborative controller for highly reliable wheelchair driving: Experimental tests with physically disabled individuals},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). EEG correlates of difficulty levels in dynamical transitions
of simulated flying and mapping tasks. <em>THMS</em>, <em>51</em>(2),
99–108. (<a href="https://doi.org/10.1109/THMS.2020.3038339">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decoding the subjective perception of task difficulty may help improve operator performance, i.e., automatically optimize the task difficulty level. Here, we aim to decode a compound of cognitive states that covaries with the task difficulty level. We designed a protocol composed of two different subtasks, flying and visual recognition, to induce different difficulty levels. We first showed that electroencephalography (EEG) signals can be a reliable source for discriminating different compound states. To gain insight into the underlying components in the compound states, we examined the attentional index and engagement index as in our previous study. We showed that, first, attention and engagement are essential components but fail to provide the best accuracy, and, second, our model is consistent with our previous study, which means that lateralized modulations in the α bands are representative of the flying task. We also analyzed a practical issue in the design of adaptive human-machine interaction (HMI) systems, namely, the latency of changes in the user&#39;s compound state. We hypothesized that the EEG correlates of the task difficulty level do not instantaneously reflect the changes in the task difficulty. We validated the hypothesis by measuring the time required for our decoders to provide stable accuracy after the task changed. This amount of time, or latency, could be as high as ten seconds. The results suggest that the latency of changes in the user&#39;s compound state between different tasks is a factor that should be taken into account when building adaptive HMI systems.},
  archive      = {J_THMS},
  author       = {Ping-Keng Jao and Ricardo Chavarriaga and Fabio Dell’Agnola and Adriana Arza and David Atienza and José del R. Millán},
  doi          = {10.1109/THMS.2020.3038339},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {4},
  number       = {2},
  pages        = {99-108},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {EEG correlates of difficulty levels in dynamical transitions of simulated flying and mapping tasks},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A survey of eye tracking in automobile and aviation studies:
Implications for eye-tracking studies in marine operations.
<em>THMS</em>, <em>51</em>(2), 87–98. (<a
href="https://doi.org/10.1109/THMS.2021.3053196">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the last decade researchers have increasingly considered eye tracking of the operators of cars and airplanes as a means to address human error and evaluate operational effectiveness. This article presents a systematic survey of recently published papers about this approach in service to the question as to whether eye tracking can be used to address operational safety in marine operations. The surveyed papers are selected systematically and were categorized according to several defined characteristics. Eye tracking depends on defining operators&#39; areas of interest (AOIs) and measuring operators focus on them over time. We identified the method of defining AOIs as a key distinction between studies; the papers fell into four categories, depending on whether researchers relied on an expert, based it on the stimulus itself, or used an attention map or a clustering algorithm to define the AOIs they used. The article also summarizes and analyzes the design and procedure of the eye-tracking experiments in the papers. Based on the features of marine operation, instruction on AOI definition in different scenarios is extracted; guidelines on experimental design and procedure selection are provided. In the article&#39;s conclusion we apply the results to a case study of a heavy-lifting operation to demonstrate the effectiveness of eye-tracking in marine operations.},
  archive      = {J_THMS},
  author       = {Runze Mao and Guoyuan Li and Hans Petter Hildre and Houxiang Zhang},
  doi          = {10.1109/THMS.2021.3053196},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {4},
  number       = {2},
  pages        = {87-98},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {A survey of eye tracking in automobile and aviation studies: Implications for eye-tracking studies in marine operations},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). 3D model-based gaze tracking via iris features with a single
camera and a single light source. <em>THMS</em>, <em>51</em>(2), 75–86.
(<a href="https://doi.org/10.1109/THMS.2020.3035176">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional 3D gaze estimation methods are usually based on the models of pupil refraction and corneal reflection. These methods typically rely on multiple light sources. The 3D gaze can be estimated using single-camera-single-light-source systems only when certain user-dependent eye parameters are available a priori, which is rarely the case. This article proposes a 3D gaze estimation method which works based on iris features using a single camera and a single light source. User-dependent eye parameters involving the iris radius and the cornea radius are user-calibrated. The 3D line-of-sight is estimated from the optical axis and the positional relationship between the optical axis and the visual axis, and then optimized using a binocular stereo vision model. The feasibility and robustness of the proposed method are assessed by simulations and practical experiments. The system configuration required by the method is simpler than that required by the state-of-the-art methods, which shows significant potential value, especially with regard to mobile device applications.},
  archive      = {J_THMS},
  author       = {Jiahui Liu and Jiannan Chi and Wenxue Hu and Zhiliang Wang},
  doi          = {10.1109/THMS.2020.3035176},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {4},
  number       = {2},
  pages        = {75-86},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {3D model-based gaze tracking via iris features with a single camera and a single light source},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Delivering critical stimuli for decision making in VR
training: Evaluation study of a firefighter training scenario.
<em>THMS</em>, <em>51</em>(2), 65–74. (<a
href="https://doi.org/10.1109/THMS.2020.3030746">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal for a virtual reality (VR) training system is to enable trainees to acquire all the knowledge they need to perform effectively in a real environment. Such a system should provide an experience so authentic that no further real-world training is necessary, meaning that it is sufficient to train in VR. We evaluate the impact of a haptic thermal stimulus, which is of paramount importance to decision making, on trainees performance and knowledge acquisition. A thermal device was created to deliver the stimulus. As a proof of concept, a procedure from firefighter training is selected, in which sensing the temperature of a door with one&#39;s hand is essential. The sample consisted of 48 subjects divided among three experimental scenarios: one in which a virtual thermometer is used (visual stimulus), another in which the temperature is felt with the hand (thermal stimulus) and a third in which both methods are used (visual + thermal stimuli). For the performance evaluation, we measured the total time taken, the numbers of correctly executed procedures and identified neutral planes, the deviation from the target height, and the responses to a knowledge transfer questionnaire. Presence, cybersickness, and usability are measured to evaluate the impact of the haptic thermal stimulus. Considering the thermal stimulus condition as the baseline, we conclude that the significantly different results in the performance among the conditions indicate that the better performance in the visual-only condition is not representative of the real-life performance. Consequently, VR training applications need to deliver the correct stimuli for decision making.},
  archive      = {J_THMS},
  author       = {Pedro Monteiro and Miguel Melo and António Valente and José Vasconcelos-Raposo and Maximino Bessa},
  doi          = {10.1109/THMS.2020.3030746},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {4},
  number       = {2},
  pages        = {65-74},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Delivering critical stimuli for decision making in VR training: Evaluation study of a firefighter training scenario},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021f). IEEE systems, man, and cybernetics society information.
<em>THMS</em>, <em>51</em>(1), C3. (<a
href="https://doi.org/10.1109/THMS.2020.3047555">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Provides a listing of current committee members and society officers.},
  archive      = {J_THMS},
  doi          = {10.1109/THMS.2020.3047555},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {2},
  number       = {1},
  pages        = {C3},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {IEEE systems, man, and cybernetics society information},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hand gesture recognition using multiple acoustic
measurements at wrist. <em>THMS</em>, <em>51</em>(1), 56–62. (<a
href="https://doi.org/10.1109/THMS.2020.3041201">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates the use of acoustic signals recorded at the human wrist for hand gesture recognition. The prototype consists of 40 microphones to be worn at the wrist. The gesture recognition performance is evaluated through the identification of 36 gestures in American sign language (ASL), including 26 ASL alphabetical characters and 10 ASL numbers. The optimal area for sensor band placement (distal/proximal) is examined to reveal the location of the highest discrimination accuracy. Ten subjects are recruited to perform over ten trials for each set of hand gestures. Using mutual information-based feature selection methods, two time-domain features: difference absolute mean value and log-energy entropy, is selected from 35 commonly used time-domain features for the hand gesture classification. As a proof-of-concept, two common classification methods, linear discriminant analysis and support vector machine, is used to classify hand gestures from the acoustic measurements recorded by each subject. Results show the intrasubject average classification accuracy above 90% using the two features with all 40 microphones, while the average classification accuracy exceeding 84% is obtained using ten microphones. These results indicate that acoustic signatures from the human wrist can be utilized for hand gesture recognition, while the use of few, simple features, with low computational requirements is sufficient to characterize some hand gestures. The proposed technique demonstrates a promising means for developing a low-cost wearable hand gesture recognition device using microphones.},
  archive      = {J_THMS},
  author       = {Nabeel Siddiqui and Rosa H. M. Chan},
  doi          = {10.1109/THMS.2020.3041201},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {2},
  number       = {1},
  pages        = {56-62},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Hand gesture recognition using multiple acoustic measurements at wrist},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Gesture recognition using reflected visible and infrared
lightwave signals. <em>THMS</em>, <em>51</em>(1), 44–55. (<a
href="https://doi.org/10.1109/THMS.2020.3043302">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we demonstrate the ability to recognize hand gestures in a noncontact wireless fashion using only incoherent light signals reflected from a human subject. Fundamentally distinguished from radar, lidar, and camera-based sensing systems, this sensing modality uses only a low-cost light source (e.g., LED) and a sensor (e.g., photodetector). The lightwave-based gesture recognition system identifies different gestures from the variations in light intensity reflected from the subject&#39;s hand within a short (20-35 cm) range. As users perform different gestures, scattered light forms unique, statistically repeatable, time-domain signatures. These signatures can be learned by repeated sampling to obtain the training model against which unknown gesture signals are tested and categorized. These time-domain variations of the lightwave signals reflected from hand are denoised, standardized, and then classified by using machine learning classification tools such as $K$-nearest neighbors and support vector machine. Performance evaluations have been conducted with eight gestures, five subjects, different distances and lighting conditions, and visible and infrared light sources. The results demonstrate the best hand gesture recognition performance of infrared sensing at 20 cm with an average of 96% accuracy. The developed gesture recognition system is low-cost, effective, and noncontact technology for numerous human-computer interaction applications.},
  archive      = {J_THMS},
  author       = {Li Yu and Hisham Abuella and Md Zobaer Islam and John F. O’Hara and Christopher Crick and Sabit Ekin},
  doi          = {10.1109/THMS.2020.3043302},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {2},
  number       = {1},
  pages        = {44-55},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Gesture recognition using reflected visible and infrared lightwave signals},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Gesture-radar: A dual doppler radar based system for robust
recognition and quantitative profiling of human gestures. <em>THMS</em>,
<em>51</em>(1), 32–43. (<a
href="https://doi.org/10.1109/THMS.2020.3036637">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gesture recognition is key to enabling natural human-computer interactions. Existing approaches based on wireless sensing focus on accurate identification of arm gesture types. It remains a challenge to recognize and profile the details of arm gestures for precise interaction control. In addition, current approaches have strict positioning requirements between radars and users, making them difficult for real-world deployment. In this article, we adopt the multisensor approach and present gesture-radar-a dual Doppler radar-based gesture recognition and profiling system, which can capture subtle arm gestures with less positioning or environmental dependence. Gesture-radar uses two vertically placed Doppler radars to collect complementary sensing data of gestures, based on which cross-analysis can be performed for gesture recognition and profiling. Specifically, we first propose a two-stage classification model and enhance the signal proximity matching method by applying constraint functions to the DTW algorithm, aiming to improve the accuracy of gesture type recognition. Afterward, we establish and analyze unique features from the time-frequency spectrogram, which can be used to characterize in-depth gesture details, e.g., the angle or range of an arm movement. Experimental results show that gesture-radar achieves up to 93.5% average accuracy for gesture type recognition, and over 80% precision for profiling gesture details. This proves that the proposed approach is viable and can work in real-world environments.},
  archive      = {J_THMS},
  author       = {Zhu Wang and Zhiwen Yu and Xinye Lou and Bin Guo and Liming Chen},
  doi          = {10.1109/THMS.2020.3036637},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {2},
  number       = {1},
  pages        = {32-43},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Gesture-radar: A dual doppler radar based system for robust recognition and quantitative profiling of human gestures},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A comparison of expert ratings and marker-less hand tracking
along OSATS-derived motion scales. <em>THMS</em>, <em>51</em>(1), 22–31.
(<a href="https://doi.org/10.1109/THMS.2020.3035763">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Objective: This study creates linear and generalized additive models (GAMs) of video-recorded two-dimensional hand motion (synonymously referred to as hand movements or hand kinematics) to predict expert-rated performance along a series of surgical motion scales. Background: Surgical performance assessments are costly and time consuming. Automatically quantifying hand motion may offload some burden of surgical coaching and intervention by automatically collecting features of psychomotor performance. Methods: Five experts rated anonymized video clips of benchtop suturing and tying tasks (n = 219) along four visual-analog (0-10) performance scales: fluidity of motion, motion economy, tissue handling, and hand coordination. Custom software tracked both participant hands across successive video frames and populated a robust feature set to train a series of predictive models to reproduce the expert ratings. Results: A GAM (which accounts for nonlinear effects) predicted fluidity of motion ratings with slope = 0.71, intercept = 1.98, and R 2 = 0.77 for clinicians of different experience levels. Fluidity of motion and motion economy models outperformed those created to predict hand coordination and tissue handling ratings. Conclusions: Hand motion tracking may not address all contextual features of surgical tasks. Future work will explore how well simulation-based models extrapolate to more dynamic settings of the operating room.},
  archive      = {J_THMS},
  author       = {David P. Azari and Brady L. Miller and Brian V. Le and Jacob A. Greenberg and Reginald C. Bruskewitz and Kristin L. Long and Guanhua Chen and Robert G. Radwin},
  doi          = {10.1109/THMS.2020.3035763},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {2},
  number       = {1},
  pages        = {22-31},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {A comparison of expert ratings and marker-less hand tracking along OSATS-derived motion scales},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Local domain adaptation for cross-domain activity
recognition. <em>THMS</em>, <em>51</em>(1), 12–21. (<a
href="https://doi.org/10.1109/THMS.2020.3039196">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sensor-based human activity recognition (HAR) aims to recognize a human&#39;s physical actions by using sensors attached to different body parts. As a user-specific application, HAR often suffers poor generalization from training on an individual to testing on another individual, or from one body part to another body part. To tackle this cross-domain HAR problem, this article proposes a domain adaptation (DA) method called local domain adaptation (LDA), whose core is to align cluster-to-cluster distributions between the source domain and the target domain. On the one hand, LDA differs from existing set-to-set alignment by reducing the distribution discrepancy at a finer granularity. On the other hand, LDA is superior to the class-to-class alignment because it can provide more accurate soft labels for the target domain. Specifically, LDA contains three main steps: 1) groups the activity class into several high-level abstract clusters; 2) maps the original data of each cluster in both domains into the same low-dimension subspace to align the intracluster data distribution; 3) predicts the class labels for target domain in the low-dimension subspace. Experimental results on two public HAR benchmark datasets show that LDA outperforms state-of-the-art DA methods for the cross-domain HAR.},
  archive      = {J_THMS},
  author       = {Jiachen Zhao and Fang Deng and Haibo He and Jie Chen},
  doi          = {10.1109/THMS.2020.3039196},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {2},
  number       = {1},
  pages        = {12-21},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Local domain adaptation for cross-domain activity recognition},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An adaptive general type-2 fuzzy logic approach for
psychophysiological state modeling in real-time human–machine
interfaces. <em>THMS</em>, <em>51</em>(1), 1–11. (<a
href="https://doi.org/10.1109/THMS.2020.3027531">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, a new type-2 fuzzy-based modeling approach is proposed to assess human operators’ psychophysiological states for both safety and reliability of human–machine interface systems. Such a new modeling technique combines type-2 fuzzy sets with state tracking to update the rule base through a Bayesian process. These new configurations successfully lead to an adaptive, robust, and transparent computational framework that can be utilized to identify dynamic (i.e., real time) features without prior training. The proposed framework is validated on mental arithmetic cognitive real-time experiments with ten participants. It is found that the proposed framework outperforms other paradigms (i.e., an adaptive neuro-fuzzy inference system and an adaptive general type-2 fuzzy c-means modeling approach) in terms of disturbance rejection and learning capabilities. The proposed framework achieved the best performance compared to other models that have been presented in the related literature. Therefore, the new framework can be a promising development in human–machine interface systems. It can be further utilized to develop advanced control mechanisms, investigate the origins of human compromised task performance, and identify and remedy psychophysiological breakdown in the early stages.},
  archive      = {J_THMS},
  author       = {Changjiang He and Mahdi Mahfouf and Luis A. Torres-Salomao},
  doi          = {10.1109/THMS.2020.3027531},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {2},
  number       = {1},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {An adaptive general type-2 fuzzy logic approach for psychophysiological state modeling in real-time Human–Machine interfaces},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
</ul>

</body>
</html>
