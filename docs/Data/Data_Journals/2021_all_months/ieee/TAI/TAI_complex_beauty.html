<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TAI_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tai---50">TAI - 50</h2>
<ul>
<li><details>
<summary>
(2021). CapsCovNet: A modified capsule network to diagnose COVID-19
from multimodal medical imaging. <em>TAI</em>, <em>2</em>(6), 608–617.
(<a href="https://doi.org/10.1109/TAI.2021.3104791">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since the end of 2019, novel coronavirus disease (COVID-19) has brought about a plethora of unforeseen changes to the world as we know it. Despite our ceaseless fight against it, COVID-19 has claimed millions of lives, and the death toll exacerbated due to its extremely contagious and fast-spreading nature. To control the spread of this highly contagious disease, a rapid and accurate diagnosis can play a very crucial part. Motivated by this context, a parallelly concatenated convolutional block-based capsule network is proposed in this article as an efficient tool to diagnose the COVID-19 patients from multimodal medical images. Concatenation of deep convolutional blocks of different filter sizes allows us to integrate discriminative spatial features by simultaneously changing the receptive field and enhances the scalability of the model. Moreover, concatenation of capsule layers strengthens the model to learn more complex representation by presenting the information in a fine to coarser manner. The proposed model is evaluated on three benchmark datasets, in which two of them are chest radiograph datasets and the rest is an ultrasound imaging dataset. The architecture that we have proposed through extensive analysis and reasoning achieved outstanding performance in COVID-19 detection task, which signifies the potentiality of the proposed model.},
  archive      = {J_TAI},
  author       = {A. F. M. Saif and Tamjid Imtiaz and Shahriar Rifat and Celia Shahnaz and Wei-Ping Zhu and M. Omair Ahmad},
  doi          = {10.1109/TAI.2021.3104791},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {6},
  pages        = {608-617},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {CapsCovNet: A modified capsule network to diagnose COVID-19 from multimodal medical imaging},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Two-dimensional broad learning system for data analytic.
<em>TAI</em>, <em>2</em>(6), 594–607. (<a
href="https://doi.org/10.1109/TAI.2021.3110500">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, a novel learning algorithm called the broad learning system (BLS) has been established by using the theory of pseudoinverse and compressed sensing technology. As an alternative to deep learning, BLS has gained extensive attention in data analytics. Unlike the existing deep structures based learning tools (such as deep neural networks), it is able to rapidly achieve incremental learning, and does not require tedious retraining to remodel the system. However, given image data modeling tasks, 1-D BLS requires a conventional vectorization operation, the dimension of input will be quite large for high resolution image. This operation may result in huge computational complexity, thereby increasing the time of model construction. To resolve this issue, a 2-D version of BLS, namely 2-D-BLS, is first proposed to fast construct randomized learner models by using matrix inputs. Specifically, the proposed method employs left and right projecting vectors to replace the usual high dimensional input weight. Moreover, the theoretical analyses on the superiority of 2-D-BLS against BLS are presented in terms of the algorithm complexity and the difference of random parameter space. Empirical results on MNIST dataset, NORB 3-D object image recognition dataset, ORL dataset, YaleB dataset, and handwritten digit dataset demonstrate that the proposed 2-D-BLS outperforms the existing extensions of BLS and state-of-the-art 2-D machine learning methods in terms of the learning efficiency, showing good potential of 2-D-BLS for image data analytics. Impact Statement—as a novel learning tool, BLS has achieved great success in many fields. Image recognition is one of the most widely used fields of artificial intelligence. But, BLS is a 1-D model, the vectorization operation is necessary to convert matrix input data into a row or a column vector. This operation will lead to “explosion of dimensionality” and increase the time of model construction when it is coming to image data analytic with 2-D input. This article first proposes a 2-D extension of BLS for high resolution image data analytic. The proposed method can not only avoid the “explosion of dimensionality,” but also construct the whole structure with matrix-inputs rapidly and dynamically. The improvement of this method is expected to provide a novel and great choice for the field of image recognition, such as face recognition, industrial inspection, behavior recognition, etc., in the future.},
  archive      = {J_TAI},
  author       = {Wei Dai and Rui Zhang and Qianjin Wang and Yuhu Cheng and Xuesong Wang},
  doi          = {10.1109/TAI.2021.3110500},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {6},
  pages        = {594-607},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Two-dimensional broad learning system for data analytic},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Preserving similarity and staring decisis for feature
selection. <em>TAI</em>, <em>2</em>(6), 584–593. (<a
href="https://doi.org/10.1109/TAI.2021.3105084">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection plays an important role in many areas that are relevant to machine learning. Information theory is widely applied to feature selection methods because it can measure linear and nonlinear correlations among variables. However, previous information theory based feature selection methods ignore two critical problems. First, features are collected or annotated manually; it is inevitable to make some incorrect features or outliers. Second, previous information theory based methods pay much attention to the correlations among features while ignoring the correlations between samples. To address these problems, this article proposes an information theory based feature selection method: preserving similarity and staring decisis. The proposed method first employs the similarity between samples to obtain a new transformed data that avoids outliers or incorrect features in original data. Meanwhile, the new transformed data enhances the separability of features, which is beneficial to feature selection. Additionally, different from previous methods that consider all the already-selected features in the feature selection process, the proposed method stares the last already-selected feature only, which reduces the computational cost. Extensive experiments demonstrate that the proposed method outperforms nine state-of-the-art methods in terms of multiple criteria significantly.},
  archive      = {J_TAI},
  author       = {Wanfu Gao and Liang Hu and Yonghao Li and Ping Zhang},
  doi          = {10.1109/TAI.2021.3105084},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {6},
  pages        = {584-593},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Preserving similarity and staring decisis for feature selection},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MACE: Model agnostic concept extractor for explaining image
classification networks. <em>TAI</em>, <em>2</em>(6), 574–583. (<a
href="https://doi.org/10.1109/TAI.2021.3111138">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep convolutional networks have been quite successful at various image classification tasks. The current methods to explain the predictions of a pretrained model rely on gradient information, often resulting in saliency maps that focus on the foreground object as a whole. However, humans typically reason by dissecting an image and pointing out the presence of smaller concepts. The final output is often an aggregation of the presence or absence of these smaller concepts. In this work, we propose MACE: a model agnostic concept extractor, which can explain the working of a convolutional network through smaller concepts. The MACE framework dissects the feature maps generated by a convolution network for an image to extract concept-based prototypical explanations. Furthermore, it estimates the relevance of the extracted concepts to the pretrained model’s predictions, a critical aspect for explaining the individual class predictions, missing in existing approaches. We validate our framework using VGG16 and ResNet50 CNN architectures and datasets like Animals With Attributes 2 (AWA2) and Places365. Our experiments demonstrate that the concepts extracted by the MACE framework increase the human interpretability of the explanations and are faithful to the underlying pretrained black-box model.},
  archive      = {J_TAI},
  author       = {Ashish Kumar and Karan Sehgal and Prerna Garg and Vidhya Kamakshi and Narayanan Chatapuram Krishnan},
  doi          = {10.1109/TAI.2021.3111138},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {6},
  pages        = {574-583},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {MACE: Model agnostic concept extractor for explaining image classification networks},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An out-of-distribution attack resistance approach to emotion
categorization. <em>TAI</em>, <em>2</em>(6), 564–573. (<a
href="https://doi.org/10.1109/TAI.2021.3105371">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks are a powerful model for feature extraction. They produce features that enable state-of-the-art performance on many tasks, including emotion categorization. However, their homogeneous representation of knowledge has made them prone to attacks, i.e., small modification in train or test data to mislead the models. Emotion categorization can usually be performed to be either in-distribution (train and test with the same dataset) or out-of-distribution (train on one or more dataset(s) and test on a different dataset). Our already developed landmark-based technique, which is robust for in-distribution improvement against attacks in emotion categorization, could translate to out-of-distribution classification problems. This is important as different databases might have different variations such as in color or level of expressiveness of emotion. We compared the landmark-based method with four state-of-the-art deep models (EfficientNetB0, InceptionV3, ResNet50, and VGG19), as well as emotion categorization tools (i.e., Python Facial Expression Analysis Toolbox and the Microsoft Azure face application programming interface) by performing a cross-database experiment across six commonly used databases, i.e., extended Cohn–Kanade, Japanese female facial expression, Karolinska directed emotional faces, National Institute of Mental Health Child Emotional Faces Picture Set, real-world affective faces, and psychological image collection at Stirling databases. The landmark-based method has achieved a significantly higher accuracy, achieving an average of 47.44% compared with most of the deep networks ( &lt;inline-formula xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:xlink=&quot;http://www.w3.org/1999/xlink&quot;&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$&amp;lt; $&lt;/tex-math&gt;&lt;/inline-formula&gt; 36%) and the emotion categorization tools ( &lt;inline-formula xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:xlink=&quot;http://www.w3.org/1999/xlink&quot;&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$&amp;lt; $&lt;/tex-math&gt;&lt;/inline-formula&gt; 37%) with considerably less execution time. This highlights that out-of-distribution emotion categorization is a much harder task due to detecting underlying emotional cues than emotion categorization in-distribution where superficial patterns are detected to &lt;inline-formula xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:xlink=&quot;http://www.w3.org/1999/xlink&quot;&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$&amp;gt;$&lt;/tex-math&gt;&lt;/inline-formula&gt; 97% accuracy. Impact Statement—Recognising emotions from people&#39;s faces has real-world applications for computer-based perception as it is often vital for interpersonal communication. Emotion recognition tasks nowadays are addressed using deep learning models that model colour distribution so classify images rather than emotion. This homogeneous knowledge representation is in contrast to emotion categorization, which is hypothesised as more heterogeneous landmark-based. This is investigated through out-of-distribution emotion categorization problems, where the test samples are drawn from a different dataset to training images. Our landmark-based method achieves a significantly higher classification performance (on average) compared with four state-of-the-art deep networks (EfficientNetB0, InceptionV3, ResNet50 and VGG19), as well as other emotion categorization tools such as Py-Feat and the Azure Face API. We conclude that this improved generalization is relevant for future developments of emotion categorization tools.},
  archive      = {J_TAI},
  author       = {Harisu Abdullahi Shehu and Will N. Browne and Hedwig Eisenbarth},
  doi          = {10.1109/TAI.2021.3105371},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {6},
  pages        = {564-573},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {An out-of-distribution attack resistance approach to emotion categorization},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sparsity-induced graph convolutional network for
semisupervised learning. <em>TAI</em>, <em>2</em>(6), 549–563. (<a
href="https://doi.org/10.1109/TAI.2021.3096489">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The graph representation (GR) in a data space reveals the intrinsic information as well as the natural relationships of data, which is regarded as a powerful means of representation for solving the semisupervised learning problem. To effectively learn on a predefined graph with both labeled data and unlabeled data, the graph convolutional network (GCN) was proposed and has attracted a lot of attention due to its high-performance graph-based feature extraction along with its low computational complexity. Nevertheless, the performance of GCNs is highly sensitive to the quality of the graph, meaning with high probability, the GCNs will achieve poor performances on a badly defined graphs. In numerous real-world semisupervised learning problems, the graph connecting each entity in the data space implicitly exists so that there is no naturally predefined graph in these problems. To overcome the issues, in this article, we apply unified GR techniques and GCNs in a framework that can be implemented in semisupervised learning problems. To achieve this framework, we propose sparsity-induced graph convolutional network (SIGCN) for semisupervised learning. SIGCN introduces the sparsity to formulate significant relationships between instances by constructing a newly proposed &lt;inline-formula xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:xlink=&quot;http://www.w3.org/1999/xlink&quot;&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$L_0$&lt;/tex-math&gt;&lt;/inline-formula&gt; -based graph (termed as the sparsity-induced graph) before applying graph convolution to capture the high-quality features based on this graph for label propagation. We prove and demonstrate the feasibility of the unified framework as well as effectiveness in capturing features. Extensive experiments and comparisons were performed to show that the proposed SIGCN obtains a state-of-the-art performance in the semisupervised learning problem.},
  archive      = {J_TAI},
  author       = {Jianhang Zhou and Shaoning Zeng and Bob Zhang},
  doi          = {10.1109/TAI.2021.3096489},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {6},
  pages        = {549-563},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Sparsity-induced graph convolutional network for semisupervised learning},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Dynamic instance-wise classification in correlated feature
spaces. <em>TAI</em>, <em>2</em>(6), 537–548. (<a
href="https://doi.org/10.1109/TAI.2021.3109858">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a typical supervised machine learning setting, the predictions on all test instances are based on a common subset of features discovered during model training. However, using a different subset of features that is most informative for each test instance individually may improve not only the prediction accuracy but also the overall interpretability of the model. At the same time, feature selection methods for classification have been known to be the most effective when many features are irrelevant and/or uncorrelated. In fact, feature selection ignoring correlations between features can lead to poor classification performance. In this work, a Bayesian network is utilized to model feature dependencies. Using the dependence network, a new method is proposed that sequentially selects the best feature to evaluate for each test instance individually and stops the selection process to make a prediction once it determines that no further improvement can be achieved with respect to classification accuracy. The optimum number of features to acquire and the optimum classification strategy are derived for each test instance. The theoretical properties of the optimum solution are analyzed, and a new algorithm is proposed that takes advantage of these properties to implement a robust and scalable solution for high-dimensional settings. The effectiveness, generalizability, and scalability of the proposed method are illustrated on a variety of real-world datasets from diverse application domains. Impact statement— The ability to rationalize which features to use to classify each data instance is of paramount importance in a wide range of application domains, including but not limited to medicine, criminal justice, and cybersecurity. Correlations between features, and the need for variable selection at the same stage as classification, in such application domains present additional challenges to machine learning related to classification accuracy and computationally intractability. The proposed framework presents, to the best of our knowledge, the first practical solution that balances between classification accuracy and sparsity at the instance level, by dynamically choosing the most informative features, relative to each instance, from a set of potentially correlated features, for classifying each individual instance. The proposed framework achieves reductions up to 82% in the average number of features used by state-of-the-art methods without sacrificing accuracy and is robust for up to 10% of missing features. Broad positive societal implications include: fast, accurate, and cost-efficient inference in complex dynamic settings; and ease of interpretation of and trust in machine learning outcomes by domain experts (e.g., doctors and lawyers).},
  archive      = {J_TAI},
  author       = {Yasitha Warahena Liyanage and Daphney-Stavroula Zois and Charalampos Chelmis},
  doi          = {10.1109/TAI.2021.3109858},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {6},
  pages        = {537-548},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Dynamic instance-wise classification in correlated feature spaces},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Redefining wireless communication for 6G: Signal processing
meets deep learning with deep unfolding. <em>TAI</em>, <em>2</em>(6),
528–536. (<a href="https://doi.org/10.1109/TAI.2021.3108129">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The year 2019 witnessed the rollout of the 5G standard, which promises to offer significant data rate improvement over 4G. While 5G is still in its infancy, there has been an increased shift in the research community for communication technologies beyond 5G. The recent emergence of machine learning approaches for enhancing wireless communications and empowering them with much-desired intelligence holds immense potential for redefining wireless communication for 6G. The evolving communication systems will be bottlenecked in terms of latency, throughput, and reliability by the underlying signal processing at the physical layer. In this position letter, we motivate the need to redesign iterative signal processing algorithms by leveraging deep unfolding techniques to fulfill the physical layer requirements for 6G networks. To this end, we begin by presenting the service requirements and the key challenges posed by the envisioned 6G communication architecture. We outline the deficiencies of the traditional algorithmic principles and data-hungry deep learning (DL) approaches in the context of 6G networks. Specifically, deep unfolded signal processing is presented by sketching the interplay between domain knowledge and DL. The deep unfolded approaches reviewed in this letter are positioned explicitly in the context of the requirements imposed by the next generation of cellular networks. Finally, this letter motivates open research challenges to truly realize hardware-efficient edge intelligence for future 6G networks. Impact Statement—In this letter, we discuss why the infusion of domain knowledge into machine learning frameworks holds the key to future embedded intelligent communication systems. Applying traditional signal processing and deep learning approaches independently entails significant computational and memory constraints. This becomes challenging in the context of future communication networks, such as 6G with significant communication demands where dense deployments of embedded Internet of Things (IoT) devices are envisioned. Hence, we put forth deep unfolded approaches as the potential enabling technology for 6G artificial intelligence (AI) radio to mitigate the computational and memory demands as well as to fulfill the future 6G latency, reliability, and throughput requirements. To this end, we present a general deep unfolding methodology that can be applied to iterative signal processing algorithms. Thereafter, we survey some initial steps taken in this direction and more importantly discuss the potential it has in overcoming challenges in the context of 6G requirements. This letter concludes by providing future research directions in this promising realm.},
  archive      = {J_TAI},
  author       = {Anu Jagannath and Jithin Jagannath and Tommaso Melodia},
  doi          = {10.1109/TAI.2021.3108129},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {6},
  pages        = {528-536},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Redefining wireless communication for 6G: Signal processing meets deep learning with deep unfolding},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fair server selection in edge computing with
&lt;inline-formula&gt;&lt;tex-math notation=“LaTeX”&gt;<span
class="math inline"><em>Q</em></span>&lt;/tex-math&gt;&lt;/inline-formula&gt;-value-normalized
action-suppressed quadruple q-learning. <em>TAI</em>, <em>2</em>(6),
519–527. (<a href="https://doi.org/10.1109/TAI.2021.3105087">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge computing is a promising paradigm that brings servers closer to users, leading to lower latencies and enabling latency-sensitive applications such as cloud gaming, virtual/augmented reality, telepresence, and telecollaboration. Due to the high number of possible edge servers and incoming user requests, the optimum choice of user-server matching has become a difficult challenge, especially in the 5G era where the network can offer very low latencies. In this article, we introduce the problem of fair server selection as not only complying with an application’s latency threshold but also reducing the variance of the latency among users in the same session. Due to the dynamic and rapidly evolving nature of such an environment and the capacity limitation of the servers, we propose as solution a reinforcement learning (RL) method in the form of a quadruple Q-Learning model with action suppression, &lt;inline-formula xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:xlink=&quot;http://www.w3.org/1999/xlink&quot;&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$Q$&lt;/tex-math&gt;&lt;/inline-formula&gt; -value normalization, and a reward function that minimizes the variance of the latency. Our evaluations in the context of a cloud gaming application show that, compared to an existing methods, our proposed method not only better meets the application’s latency threshold but is also more fair with a reduction of up to 35% in the standard deviation of the latencies experienced by users.},
  archive      = {J_TAI},
  author       = {Alaa Eddin Alchalabi and Shervin Shirmohammadi and Shady Mohammed and Sorin Stoian and Karthigesu Vijayasuganthan},
  doi          = {10.1109/TAI.2021.3105087},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {6},
  pages        = {519-527},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Fair server selection in edge computing with &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$Q$&lt;/tex-math&gt;&lt;/inline-formula&gt;-value-normalized action-suppressed quadruple Q-learning},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Domain adaptation without source data. <em>TAI</em>,
<em>2</em>(6), 508–518. (<a
href="https://doi.org/10.1109/TAI.2021.3110179">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain adaptation assumes that samples from source and target domains are freely accessible during a training phase. However, such an assumption is rarely plausible in the real world and possibly causes data privacy issues, especially when the label of the source domain can be a sensitive attribute as an identifier. To avoid accessing source data that could contain sensitive information, we introduce source data free domain adaptation (SFDA). Our key idea is to leverage a pretrained model from the source domain and progressively update the target model in a self-learning manner. We observe that target samples with lower self-entropy measured by the pretrained source model are more likely to be classified correctly. From this, we select the reliable samples with the self-entropy criterion and define these as class prototypes. We then assign pseudolabels for every target sample based on the similarity score with class prototypes. We further propose point-to-set distance-based filtering, which does not require any tunable hyperparameters to reduce uncertainty from the pseudolabeling process. Finally, we train the target model with the filtered pseudolabels with regularization from the pretrained source model. Surprisingly, without direct usage of labeled source samples, our SFDA outperforms conventional domain adaptation methods on benchmark datasets. Impact Statement—This study addresses the data privacy issue, especially in unsupervised domain adaptation. Based on our privacy-preserving domain adaptation, various stakeholders, including enterprises and government organizations, can be free of concern about privacy issues with their labeled source dataset. Furthermore, the proposed data-free approach can contribute to creating a positive social impact, especially in large-scale datasets. Recently, since the size of data across various fields has been scaling up, it is almost incapable for individual researchers to directly utilize a large scale of data during training. For this reason, a new social trend of sharing pretrained models, e.g., EfficientNet and BERT, led by global enterprises with their huge amount of resources has been rising up. From this viewpoint, our approach thus enables more people to participate in the domain adaptation field specifically when the source data are large scale and contain sensitive attributes.},
  archive      = {J_TAI},
  author       = {Youngeun Kim and Donghyeon Cho and Kyeongtak Han and Priyadarshini Panda and Sungeun Hong},
  doi          = {10.1109/TAI.2021.3110179},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {6},
  pages        = {508-518},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Domain adaptation without source data},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Exploiting causal structure for robust model selection in
unsupervised domain adaptation. <em>TAI</em>, <em>2</em>(6), 494–507.
(<a href="https://doi.org/10.1109/TAI.2021.3101185">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many real-world settings, such as healthcare, machine learning models are trained and validated on one labeled domain and tested or deployed on another, where feature distributions differ, i.e., there is covariate shift . When annotations are costly or prohibitive, an unsupervised domain adaptation (UDA) regime can be leveraged requiring only unlabeled samples in the target domain. Existing UDA methods are unable to factor in a model’s predictive loss based on predictions in the target domain and, therefore, suboptimally leverage density ratios of only the input covariates in each domain. In this article, we propose a model selection method for leveraging model predictions on a target domain without labels by exploiting the domain invariance of causal structure. We assume or learn a causal graph from the source domain and select models that produce predicted distributions in the target domain that have the highest likelihood of fitting our causal graph. We thoroughly analyze our method under oracle knowledge using synthetic data. We then show on several real-world datasets, including several COVID-19 examples, that our method is able to improve on the state-of-the-art UDA algorithms for model selection.},
  archive      = {J_TAI},
  author       = {Trent Kyono and Mihaela van der Schaar},
  doi          = {10.1109/TAI.2021.3101185},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {6},
  pages        = {494-507},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Exploiting causal structure for robust model selection in unsupervised domain adaptation},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Neuroevolution in deep neural networks: Current trends and
future challenges. <em>TAI</em>, <em>2</em>(6), 476–493. (<a
href="https://doi.org/10.1109/TAI.2021.3067574">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A variety of methods have been applied to the architectural configuration and learning or training of artificial deep neural networks (DNN). These methods play a crucial role in the success or failure of the DNN for most problems and applications. Evolutionary algorithms (EAs) are gaining momentum as a computationally feasible method for the automated optimization of DNNs. Neuroevolution is a term, which describes these processes of automated configuration and training of DNNs using EAs. While many works exist in the literature, no comprehensive surveys currently exist focusing exclusively on the strengths and limitations of using neuroevolution approaches in DNNs. Absence of such surveys can lead to a disjointed and fragmented field preventing DNNs researchers potentially adopting neuroevolutionary methods in their own research, resulting in lost opportunities for wider application within real-world deep learning problems. This article presents a comprehensive survey, discussion, and evaluation of the state-of-the-art in using EAs for architectural configuration and training of DNNs. This article highlights the most pertinent current issues and challenges in neuroevolution and identifies multiple promising future research directions. Impact Statement—The concept of deep learning originated from the study of artificial neural networks (ANNs). ANNs have achieved extraordinary results in a variety of diverse application areas. Numerous methods have been applied to the architectural configuration and learning or training of artificial DNN and these methods play a crucial role in the success or failure of the DNN for most problems and applications. Recently, EAs have been gaining momentum as a computationally feasible method (called neuroevolution) for the automated configuration and learning or training of DNNs. This article reviews over 170 recent scientific papers describing how major EAs paradigms are being applied by researchers to the configuration and optimization of multiple DNNs. By articulating a clear understanding of the context, state-of-the-art, and feasibility of Neuroevolution, researchers in AI, EAs, and DNN will benefit from this article. The impact of this article comes from contributing toward enhancing research capacity, knowledge, and skills for researchers currently working in neuroevolution and actively engaging those considering becoming involved in this area.},
  archive      = {J_TAI},
  author       = {Edgar Galván and Peter Mooney},
  doi          = {10.1109/TAI.2021.3067574},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {6},
  pages        = {476-493},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Neuroevolution in deep neural networks: Current trends and future challenges},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Towards machine learning as an enabler of computational
creativity. <em>TAI</em>, <em>2</em>(6), 460–475. (<a
href="https://doi.org/10.1109/TAI.2021.3100456">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computational creativity composes a collection of activities that are capable of achieving or simulating behaviors, which can be deemed creative. A frequently articulated criticism for related systems is that the creative capability yet remains with the software designer rather than the computational creative system itself. The rise of machine learning (ML) enables new ways of combining, exploring, and transforming conceptual spaces to achieve creative results. This article demonstrates that the learning occurring within the computational machine through ML enables creative capabilities therein, allowing the computational creative system to be more creative on its own than ever before. Thus, we perceive ML as a key enabler of computational creativity. In this article, we consolidate research from the computer science, computational creativity, and information systems communities, which has been treated separately so far. We build on a framework of human creativity to examine the relationship between creative capabilities and ML mechanisms in ML-based computational creative systems. Specifically, we explicate, which creative capabilities are already established through ML mechanisms in computational creative systems as strengths. Furthermore, we explicate challenges pointing towards further potential of ML-based computational creative systems to enhance the inherent creative capabilities. Our results reveal that ML-based computational creative systems advance the previously static and explicit principles of non-ML-based computational creative systems, yielding creative capabilities on the machine&#39;s own, which yet have been in the realm of human actors. Impact Statement—Creativity is a core organizational skill as it enables innovation. With markets becoming increasingly volatile, constant innovation is essential for organizations to create and sustain their economic competitive advantage. Due to the undisputable relevance of creativity for individuals, organizations, and societies, artificial intelligence research has set out to enable creative behavior in computational systems. Especially, ML methods lately became a highly frequented means within computational creative systems. However, while oftentimes applied, ML is seldomly explicitly assessed for its potential to facilitate computational creativity. In this article, we analyze extant computational creative systems relying on ML. The findings serve as a guidance for the design of these systems in various contexts and as pointers for future research to advance the creative capabilities of such systems.},
  archive      = {J_TAI},
  author       = {Deborah Mateja and Armin Heinzl},
  doi          = {10.1109/TAI.2021.3100456},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {12},
  number       = {6},
  pages        = {460-475},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Towards machine learning as an enabler of computational creativity},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A transfer learning model for gesture recognition based on
the deep features extracted by CNN. <em>TAI</em>, <em>2</em>(5),
447–458. (<a href="https://doi.org/10.1109/TAI.2021.3098253">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The surface electromyogram (sEMG) based hand gesture recognition is prevalent in human–computer interface systems. However, the generalization of the recognition model does not perform well on cross-subject and cross-day. Transfer learning, which applies the pretrained model to another task, has demonstrated its effectiveness in solving this kind of problem. In this regard, this article first proposes a multiscale kernel convolutional neural network (MKCNN) model to extract and fuse multiscale features of the multichannel sEMG signals. Based on the proposed MKCNN model, a transfer learning model named TL-MKCNN combines the MKCNN and its Siamese network by a custom distribution normalization module (DNM) and a distribution alignment module (DAM) to realize domain adaptation. The DNM can cluster the deep features extracted from different domains to their category center points embedded in the feature space, and the DAM further aligns the overall distribution of the deep features from different domains. The MKCNN model and the TL-MKCNN model are tested on various benchmark databases to verify the effectiveness of the transfer learning framework. The experimental results show that, on the benchmark database NinaPro DB6, the average accuracies of TL-MKCNN can achieve 97.22% on within-session, 74.48% on cross-subject, and 90.30% on cross-day, which are 4.31%, 11.58%, and 5.51% higher than those of the MKCNN model on within-session, cross-subject, and cross-day, respectively. Compared with the state-of-the-art works, the TL-MKCNN obtains 13.38% and 37.88% accuracy improvement on cross-subject and cross-day, respectively.},
  archive      = {J_TAI},
  author       = {Yongxiang Zou and Long Cheng},
  doi          = {10.1109/TAI.2021.3098253},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  number       = {5},
  pages        = {447-458},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {A transfer learning model for gesture recognition based on the deep features extracted by CNN},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep feature representation based imitation learning for
autonomous helicopter aerobatics. <em>TAI</em>, <em>2</em>(5), 437–446.
(<a href="https://doi.org/10.1109/TAI.2021.3053511">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The drastic changes in flight parameters during aerobatics and the high instability of the system make the control of autonomous aerobatics unusually difficult. In this paper, we propose a deep feature representation based imitation learning method for autonomous aerobatics, which leverages expert demonstrations to efficiently learn the mapping of high-dimensional flight observations onto continuous actions (pitch, tail, and thrust). Different from the existing methods, our proposed method requires neither trajectory specification and alignment nor any assumptions and processing of system uncertainties, so it can greatly simplify the controller solution steps and reduce the computational burden. Particularly, our method uses the proposed deep feature representation network (DFR-network) to directly map the experts’ demonstration trajectory to a deep representation space spanned by a set of learned subspaces which represent the motion patterns with the same statistical property among demonstration trajectories. Various aerobatic maneuvers can be encoded in the deep representation space through a simple combination of embedding features. Accordingly, the proposed method can perform arbitrary aerobatic maneuvers by observing a limit set of expert demonstrations. The effectiveness of the deep feature representation based imitation learning method is verified on the real-world flight data. Experiments show that compared with the existing methods, our proposed method has higher control accuracy, stronger robustness and anti-interference ability.},
  archive      = {J_TAI},
  author       = {Shaofeng Chen and Yang Cao and Yu Kang and Pengfei Li and Bingyu Sun},
  doi          = {10.1109/TAI.2021.3053511},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  number       = {5},
  pages        = {437-446},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Deep feature representation based imitation learning for autonomous helicopter aerobatics},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Muscle-synergy-based planning and neural-adaptive control
for a prosthetic arm. <em>TAI</em>, <em>2</em>(5), 424–436. (<a
href="https://doi.org/10.1109/TAI.2021.3091038">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Upper limb loss has significant effects on the individual’s quality of life. Artificial prosthetic limbs as an alternative to the lost limb are designed to allow amputees to regain motor function. Motion classification via extracted surface electromyogram (sEMG) signals is widely utilized to realize a friendly human–robot interface in the control of the prosthesis. However, limited classification of discrete motion patterns from sEMG prevents intuitive motor control. Thus, instead of using discrete patterns, decoding the human intention continuously from sEMG would significantly benefit the prosthesis control. In this article, we propose a muscle-synergy-based intention decoding and motion planning that can model a broad set of complex upper limb movements as a combination of motor primitives. A novel muscle activation-to-force mapping model is developed to detect muscular effort of the healthy side to drive the affected side. A neural-network-approximation-based controller is developed for the bionic neuroprosthetic arm to execute the movement. Operational experiments with prosthetic movement control were performed on four healthy participants and an upper limb amputee participant. Results of controlling prosthetic arm ( &lt;inline-formula xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:xlink=&quot;http://www.w3.org/1999/xlink&quot;&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$0.94\pm 0.02$&lt;/tex-math&gt;&lt;/inline-formula&gt; of &lt;inline-formula xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:xlink=&quot;http://www.w3.org/1999/xlink&quot;&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$\Re ^{2}$&lt;/tex-math&gt;&lt;/inline-formula&gt; in the horizontal reaching tasks and &lt;inline-formula xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:xlink=&quot;http://www.w3.org/1999/xlink&quot;&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$0.95\pm 0.01$&lt;/tex-math&gt;&lt;/inline-formula&gt; in the vertical reaching tasks for the healthy subjects, 0.95 and 0.97 for the amputee) demonstrate that our control method could successfully capture human movement intention and effectively control the movement of prosthesis.},
  archive      = {J_TAI},
  author       = {Guoxin Li and Zhijun Li and Junjun Li and Yueyue Liu and Hong Qiao},
  doi          = {10.1109/TAI.2021.3091038},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  number       = {5},
  pages        = {424-436},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Muscle-synergy-based planning and neural-adaptive control for a prosthetic arm},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Enhanced broad siamese network for facial emotion
recognition in human–robot interaction. <em>TAI</em>, <em>2</em>(5),
413–423. (<a href="https://doi.org/10.1109/TAI.2021.3105621">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robotics is one important domain of artificial intelligence applications. Proliferation of robotics makes our lives more convenient and colorful. Emotion recognition is one important task for a robot to communicate with human beings. Although different works have been proposed to complete the task of emotion recognition, conventional methods often require a lot of time and memory resources. Since a robot is often required to react timely and equipped with limited resources, we propose an enhanced Siamese network algorithm to resolve such a problem. We combine Siamese network with broad learning system while further improving mechanism of similarity metric. Experiment results show that our method can achieve a comparable performance to conventional deep learning method while reducing consumption of computing time and memory resources.},
  archive      = {J_TAI},
  author       = {Yikai Li and Tong Zhang and C. L. Philip Chen},
  doi          = {10.1109/TAI.2021.3105621},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  number       = {5},
  pages        = {413-423},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Enhanced broad siamese network for facial emotion recognition in Human–Robot interaction},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep neural network approach in EMG-based force estimation
for human–robot interaction. <em>TAI</em>, <em>2</em>(5), 404–412. (<a
href="https://doi.org/10.1109/TAI.2021.3066565">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the human–robot interaction, especially when hand contact appears directly on the robot arm, the dynamics of the human arm presents an essential component in human–robot interaction and object manipulation. Modeling and estimation of the human arm dynamics show great potential for achieving more natural and safer interaction. To enrich the dexterity and guarantee the accuracy of the manipulation, mapping the motor functionality of muscle using biosignals becomes a popular topic. In this article, a novel algorithm was constructed using deep learning to explore the potential model between surface electromyography (sEMG) signals of the human arm and interaction force for human–robot interaction. Its features were extracted by adopting the convolutional neural network from the sEMG signals automatically without using prior knowledge of the biomechanical model. The experiments prove the lower error ( &lt;inline-formula xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:xlink=&quot;http://www.w3.org/1999/xlink&quot;&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$&amp;lt; \text{0.4}\,N$&lt;/tex-math&gt;&lt;/inline-formula&gt; ) of the designed regression by comparing it with other approaches, such as artificial neural network and long short-term memory. It should be also mentioned that the antinoise ability is an important index to apply this technique in practical applications. Hence, we also add different Gaussian noises into the dataset to demonstrate the robustness against measurement noises by using the proposed model. Finally, it demonstrates the performance of the proposed algorithm using the Myo controller and KUKA LWR4+ robot.},
  archive      = {J_TAI},
  author       = {Hang Su and Wen Qi and Zhijun Li and Ziyang Chen and Giancarlo Ferrigno and Elena De Momi},
  doi          = {10.1109/TAI.2021.3066565},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  number       = {5},
  pages        = {404-412},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Deep neural network approach in EMG-based force estimation for Human–Robot interaction},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Vector detection network: An application study on robots
reading analog meters in the wild. <em>TAI</em>, <em>2</em>(5), 394–403.
(<a href="https://doi.org/10.1109/TAI.2021.3105936">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Analog meters equipped with one or multiple pointers are wildly utilized to monitor vital devices’ status in industrial sites for safety concerns. Reading these legacy meters autonomously remains an open problem since estimating pointer origin and direction under imaging damping factors imposed in the wild could be challenging. Nevertheless, high accuracy, flexibility, and real-time performance are demanded. In this work, we propose the vector detection network (VDN) to detect analog meters’ pointers given their images, eliminating the barriers for autonomously reading such meters using intelligent agents like robots. We tackled the pointer as a 2-D vector, whose initial point coincides with the tip, and the direction is along tail-to-tip. The network estimates a confidence map, wherein the peak pixels are treated as vectors’ initial points, along with a two-layer scalar map, whose pixel values at each peak form the scalar components in the directions of the coordinate axes. We established the Pointer-10 K dataset composing of real-world analog meter images to evaluate our approach due to no similar dataset is available for now. Experiments on the dataset demonstrated that our methods generalize well to various meters, robust to harsh imaging factors, and run in real-time.},
  archive      = {J_TAI},
  author       = {Zhipeng Dong and Yi Gao and Yunhui Yan and Fei Chen},
  doi          = {10.1109/TAI.2021.3105936},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  number       = {5},
  pages        = {394-403},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Vector detection network: An application study on robots reading analog meters in the wild},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Graph-based knowledge acquisition with convolutional
networks for distribution network patrol robots. <em>TAI</em>,
<em>2</em>(5), 384–393. (<a
href="https://doi.org/10.1109/TAI.2021.3087116">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the popularization of smart grids, patrol robots have become critical devices in the distribution networks to check the state of equipment. In order to enrich the knowledge of patrol robots in this complex scenario, this article presents the graph-based knowledge acquisition method with convolutional networks for distribution network patrol robots. The proposed method uses a graph convolutional network-based path-related embedding algorithm to complete the knowledge of the distribution network knowledge graph. The proposed algorithm generates the embeddings of entities and relations through aggregating the associated entities in the associated paths, instead of only the connected entities. The graph convolutional network consists of multiple graph convolution layers, and the message-passing process treats different entities discriminatorily according to the association strengths. For determining the plausibility of the knowledge, a scoring function is provided with the convolution operator. The experimental datasets are from a real grid company and contain four kinds of equipment. The experiments apply the proposed method to the equipment defects analysis, including the defect gradation, the coarse defect classification, and the fine defect classification. The proposed method is compared with some embedding methods. The experimental results verify that the proposed method outperforms the other methods for the real distribution network datasets, and the proposed method can analyze the defects effectively.},
  archive      = {J_TAI},
  author       = {Dapeng Yan and Hui Cao and Tao Wang and Ruilin Chen and Shuangsi Xue},
  doi          = {10.1109/TAI.2021.3087116},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  number       = {5},
  pages        = {384-393},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Graph-based knowledge acquisition with convolutional networks for distribution network patrol robots},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Guest editorial: Special issue on artificial intelligence
for robotics. <em>TAI</em>, <em>2</em>(5), 382–383. (<a
href="https://doi.org/10.1109/TAI.2021.3119155">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The papers in this special section focus on artificial intelligence (AI) applications for robotics. AI technologies, covering cognition, analysis, inference, and decision-making, enable robots to act smartly and greatly enhance robots’ capabilities to assist and support humans. As a primary carrier of AI technologies, robotics is one of the applications for AI to demonstrate its strong ability. Augmenting robots with AI technologies for the engineering systems, robots are expected to have more pervasive applications in industry, agriculture, logistics, medicine, and to name but a few. This special issue will be dedicated to AI for robotics, including AI to support human–robot interaction, AI for multirobot systems, AI learning algorithms in robotics, ethics of AI in the context of robotics, and applications of AI-enabled robots. These papers offer innovative ideas and concepts, discoveries and improvements, and novel applications to the field of AI for robotics.},
  archive      = {J_TAI},
  author       = {Wei He and Zhijun Li and Erdal Kayacan and Catherine Huang and Michael V. Basin},
  doi          = {10.1109/TAI.2021.3119155},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  number       = {5},
  pages        = {382-383},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Guest editorial: Special issue on artificial intelligence for robotics},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). GarbageNet: A unified learning framework for robust garbage
classification. <em>TAI</em>, <em>2</em>(4), 372–380. (<a
href="https://doi.org/10.1109/TAI.2021.3081055">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recyclability of domestic waste plays a crucial role in the modern society, which helps reduce multiple types of pollution and brings economic effect. To achieve this goal, garbage classification is one of the most important steps during the recycling process. Prevailing deep learning techniques empower high-performance visual recognition models and can benefit the automation of garbage classification task. Nevertheless, there exist three challenges when directly leveraging deep recognition models for this task, referring to lack of sufficient data, high cost of category increment, and noisy data quality. In this article, we present a novel incremental learning framework, GarbageNet, to address the aforementioned challenges. First, weakly-supervised transfer learning guarantees the capacity of feature extractor. Second, for new categories of garbages, GarbageNet embeds them as anchors for reference and classifies the test samples by finding their nearest neighbors in the latent space. Third, an attentive mixup of training data is utilized for suppressing the negative effect of mislabeled data. We evaluate our method on real-world datasets, and the empirical results demonstrate that GarbageNet achieves the state-of-the-art performance with regard to accuracy, robustness, and extendability. The proposed method won the first place in the HUAWEI Cloud Garbage Classification Challenge in 2019. Impact Statement—This article contributes to a vision-based garbage recognition system for the recyclability of domestic waste. To the best of our knowledge, it is the first article that explores a realistic vision-based solution for the garbage recognition with a real-world dataset and a comprehensive benchmark on current state-of-the-art visual recognition models. The proposed method addresses several challenges and achieves state-of-the-art performance. We believe that this interdisciplinary research contributes to the AI for the environment, which helps promote environmental ethics, bring rotation economy, and relieve the pressure of consumption doctrine in smart city.},
  archive      = {J_TAI},
  author       = {Jianfei Yang and Zhaoyang Zeng and Kai Wang and Han Zou and Lihua Xie},
  doi          = {10.1109/TAI.2021.3081055},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {4},
  pages        = {372-380},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {GarbageNet: A unified learning framework for robust garbage classification},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). YogaHelp: Leveraging motion sensors for learning correct
execution of yoga with feedback. <em>TAI</em>, <em>2</em>(4), 362–371.
(<a href="https://doi.org/10.1109/TAI.2021.3096175">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {By leveraging motion sensors, the study of physical activities has become effortless and convenient. Yoga is an excellent form of physical activity or exercise that involves all parts of the body. Regular practice of yoga improves both the physical and mental health of the people, if the practitioner performs it correctly. Though the existing exercise recognition methods have shown quite a good performance using deep neural networks, they fail to assess the correctness of execution. Feedback describing correctness level is very useful to an amateur practitioner. In this article, we aim to develop a YogaHelp system to help amateurs for learning the correct execution of yoga without any supervision of a trainer. The importance of such a system is much higher during a pandemic situation, where the trainers can not be availed due to the risk of catching an infection. YogaHelp leverages the motion sensors including accelerometer and gyroscope, to recognize 12 linked-steps of sun salutation (Surya Namaskar) yoga along with their correctness level. YogaHelp employs a deep learning model built using convolutional layers for yoga step recognition without explicit feature extraction. The novelty of the system lies in the feedback that describes the speed of execution and angular deviation of the posture from the standard ones. We develop a prototype for collecting a training dataset from eight professional yoga trainers and for validating the effectiveness of the system.},
  archive      = {J_TAI},
  author       = {Ashish Gupta and Hari Prabhat Gupta},
  doi          = {10.1109/TAI.2021.3096175},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {4},
  pages        = {362-371},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {YogaHelp: Leveraging motion sensors for learning correct execution of yoga with feedback},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptive intelligent controller design-based ISS modular
approach for uncertain nonlinear systems with time-varying full-state
constraints. <em>TAI</em>, <em>2</em>(4), 352–361. (<a
href="https://doi.org/10.1109/TAI.2021.3093499">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The present article focuses on an input-to-state stability (ISS) problem of nonlinear strict-feedback system with time-varying full-state constraints. The adaptive neural controllers are designed by using backstepping, barrier Lyapunov function (BLF), and ISS small-gain approach. BLF is proved to satisfy the small-gain condition; thus, the conception of input-to-state stable time-varying BLF (ISSTVBLF) is produced. Also, the ISSTVBLF is novel embedded into the back-stepping design for a subsystem, which can prevent the full-state constraints from being violated all the time and avoid the designed difficulties resulting from the requirement of the entire BLF for the time-varying constrained system. The neural network approximate technique is utilized to estimate the unknown nonlinear functions. Then, a systematic procedure is developed to derive new adaptive tracking controllers based on the small-gain theorem. It is proved that all the closed-loop signals are semiglobal, uniform, and ultimate boundedness and the tracking error is driven to a small domain around zero. Finally, simulation study results illustrate the effectiveness of the proposed control schemes. Impact Statement—There are constraints in various systems. In this paper a strict feedback system with time-varying full-state constraints is studied. Handling the time-varying unknown nonlinearities and high-order coupling terms is a difficult question. It tends to be conservative to choose transformation functions to transform the constrained system to an unconstrained one. The controllers introduced in this paper, which are constructed by applying back-stepping, BLF and ISS small-gain approach, can overcome the difficulty of designing an overall Lyapunov function. Checking ISS small-gain condition is another difficulty, we construct a novel class K function for BLF in the design process, then the subsystem of the imbedded BLF is proved to be ISS. Simulation study results illustrate the effectiveness of proposed control schemes.},
  archive      = {J_TAI},
  author       = {Yang Chen and Yan-Jun Liu and Shaocheng Tong and C. L. Philip Chen and Lei Liu},
  doi          = {10.1109/TAI.2021.3093499},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {4},
  pages        = {352-361},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Adaptive intelligent controller design-based ISS modular approach for uncertain nonlinear systems with time-varying full-state constraints},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Suspension regulation of medium-low-speed maglev trains via
deep reinforcement learning. <em>TAI</em>, <em>2</em>(4), 341–351. (<a
href="https://doi.org/10.1109/TAI.2021.3097313">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article studies the suspension regulation problem of medium-low-speed maglev trains (mlsMTs), which is not well solved by most model-based controllers. We propose a sample-based controller by reformulating it as a continuous Markov decision process (MDP) with unknown transition probabilities. Then, we propose a reinforcement learning algorithm with actor-critic neural networks to solve the MDP. To reduce estimation errors in the Q-function, we adopt a double Q-learning scheme and propose a novel initialization method to accelerate the convergence by exploiting the PID controller. Finally, we illustrate that the proposed controllers outperform the existing PID controller with a real dataset from the mlsMT in Changsha, China, and are even comparable to model-based controllers, which, however, assume that the complete information of the model is known, via simulations. Impact Statement—The control problem of levitation systems is essential for maglev trains. The advanced control methods require an exact dynamical model, which is difficult to establish in practice due to uncertainties and complex dynamics. Reinforcement learning (RL), as a model-free method, learns a controller directly from data. We are the first to propose deep RL algorithms to address the levitation control problem. By learning the real dataset provided by CRRC, the proposed algorithms outperform the well-known PID controller significantly. In particular, our controller responses 50 times faster than PID even without additional efforts on modeling. Moreover, the proposed initialization method can be applied in a variety of RL-based control problems to improve performance.},
  archive      = {J_TAI},
  author       = {Feiran Zhao and Keyou You and Shiji Song and Wenyue Zhang and Laisheng Tong},
  doi          = {10.1109/TAI.2021.3097313},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {4},
  pages        = {341-351},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Suspension regulation of medium-low-speed maglev trains via deep reinforcement learning},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A deep adversarial learning prognostics model for remaining
useful life prediction of rolling bearing. <em>TAI</em>, <em>2</em>(4),
329–340. (<a href="https://doi.org/10.1109/TAI.2021.3097311">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Remaining useful life (RUL) prediction for condition-based maintenance decision making plays a key role in prognostics and health management (PHM). Accurately predicting RUL of the rotating components of complex machines becomes a challenging task for PHM. For many existing methods, the current prediction error of RUL prediction may be accumulated into the future predictions, and thus can lead to a prediction error superposition problem. In this article, the formation mechanism of prediction error superposition is analyzed, and for the first time a deep adversarial long short-term memory (LSTM) prognostic framework is proposed to overcome the major issue related to prediction error superposition. In the proposed framework, a generative adversarial network (GAN) architecture combining the LSTM network and autoencoder (AE) is investigated for bearing RUL monitoring. In the proposed deep adversarial learning prediction framework, due to the potential involvement of long-term and complex tasks, the LSTM network (generator) is used to predict the degradation process of rolling bearings based on available historical data, and a simple but useful AE (discriminator) is used to determine and refine the accuracy of the prediction. Therefore, the AE plays the adversarial role of the LSTM network, and the prediction accuracy of the LSTM network can be significantly improved. For illustration purpose, two practical case studies, which use a series of bearing degradation data and the IEEE PHM 2012 PRONOSTIA datasets, respectively, are presented to show the prediction performance of the proposed method. Experimental results show that the proposed method works very well for vibration monitoring and performs better in comparison with the reference machine learning and deep learning approaches. Impact Statement—The damage of rolling bearing usually leads to a significant consequence to industrial production process. However, the existing remaining useful life (RUL) prediction methods for rolling bearing have a prediction error superposition problem that can affect the multistep prediction performance. The new adversarial learning prognostics model proposed in this article can overcome the problem. The proposed method uses LSTM network as a generator to predict RUL life for rolling bearing, and uses AE as a discriminator to estimate the prediction accuracy. The method can significantly improve the multistep prediction accuracy of RUL for rolling bearing, and provides reliable and scientific strategy in PHM of mechatronics equipment.},
  archive      = {J_TAI},
  author       = {Bi-Liang Lu and Zhao-Hua Liu and Hua-Liang Wei and Lei Chen and Hongqiang Zhang and Xiao-Hua Li},
  doi          = {10.1109/TAI.2021.3097311},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {4},
  pages        = {329-340},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {A deep adversarial learning prognostics model for remaining useful life prediction of rolling bearing},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Toward the development of versatile brain–computer
interfaces. <em>TAI</em>, <em>2</em>(4), 314–328. (<a
href="https://doi.org/10.1109/TAI.2021.3097307">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in artificial intelligence demand an automated framework for the development of versatile brain–computer interface (BCI) systems. In this article, we proposed a novel automated framework that reveals the importance of multidomain features with feature selection to increase the performance of a learning algorithm for motor imagery electroencephalogram task classification on the utility of signal decomposition methods. A framework is explored by investigating several combinations of signal decomposition methods with feature selection techniques. Thus, this article also provides a comprehensive comparison among the aforementioned modalities and validates them with several performance measures, robust ranking, and statistical analysis (Wilcoxon and Friedman) on public benchmark databases. Among all the combinations, the variational mode decomposition, multidomain features obtained with linear regression, and the cascade-forward neural network provide better classification accuracy results for both subject-dependent and independent BCI systems in comparison with other state-of-the-art methods. Impact Statement—The brain–computer interface (BCI) is a revolutionary device that utilizes cognitive function explicitly for the interaction of external devices without any motor intervention. BCI systems based on motor imagery have shown efficacy for stroke patient treatment, but poor performance, nonflexible characteristics, and lengthy training sessions have limited their use in clinical practice. The proposed automated framework overcomes these limitations. With the significant improvement of up to 26.1% and 26.4% in comparison with the available literature, the proposed automated framework could offer help to BCI device developers to develop flexible BCI devices and provide interaction for motor-disabled users.},
  archive      = {J_TAI},
  author       = {Muhammad Tariq Sadiq and Xiaojun Yu and Zhaohui Yuan and Muhammad Zulkifal Aziz and Siuly Siuly and Weiping Ding},
  doi          = {10.1109/TAI.2021.3097307},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {4},
  pages        = {314-328},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Toward the development of versatile Brain–Computer interfaces},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learn2Evade: Learning-based generative model for evading PDF
malware classifiers. <em>TAI</em>, <em>2</em>(4), 299–313. (<a
href="https://doi.org/10.1109/TAI.2021.3103139">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent research has shown that a small perturbation to an input may forcibly change the prediction of a machine learning (ML) model. Such variants are commonly referred to as adversarial examples . Early studies have focused mostly on ML models for image processing and expanded to other applications, including those for malware classification. In this article, we focus on the problem of finding adversarial examples against ML-based portable document format (PDF) malware classifiers. We deem that our problem is more challenging than those against ML models for image processing because of the highly complex data structure of PDF and of an additional constraint that the generated PDF should exhibit malicious behavior. To resolve our problem, we propose a variant of generative adversarial networks that generate evasive variant PDF malware (without any crash), which can be classified as benign by various existing classifiers yet maintaining the original malicious behavior. Our model exploits the target classifier as the second discriminator to rapidly generate an evasive variant PDF with our new feature selection process that includes unique features extracted from malicious PDF files. We evaluate our technique against three representative PDF malware classifiers (Hidost’13, Hidost’16, and PDFrate-v2) and further examine its effectiveness with AntiVirus engines from VirusTotal. To the best of our knowledge, our work is the first to analyze the performance against the commercial AntiVirus engines. Our model finds, with great speed, evasive variants for all selected seeds against state-of-the-art PDF malware classifiers and raises a serious security concern in the presence of adversaries. Impact statement—PDF has been one of the most popular media to conceal adversarial contents for many years. The reason being that adversaries can exploit the complex structure of PDF in their favor by hiding malicious content. In 2019, more than 73k PDF-based attacks were reported in one month, which accounts for 17% of newly detected threats. With increasing popularity, many ML-based techniques have been proposed for PDF malware classifiers. Such defense mechanisms include support vector machine and random forest classification models trained with a structural map of PDF (Hidost’13 and Hidost’16). Furthermore, ensemble training has been applied with metadata collected from PDF as the training data (PDFrate-v2). In recent studies, many researchers have attempted and succeeded in generating evasive PDF malware (adversarial examples) that bypass such defense techniques. However, the current method heavily relies on a random mutation algorithm resulting in repeated computation for a significant period of time. To resolve this, we propose a novel solution by employing a variant of generative adversarial networks, which is trained to identify intrinsic properties of PDF and to generate evasive PDF malware with the minimum perturbation to the original PDF. Our solution successfully generated evasive PDF malware with a maximum number of 12 manipulation operations and found effective against ML-based classifiers and AntiVirus engines provided by VirusTotal.},
  archive      = {J_TAI},
  author       = {Ho Bae and Younghan Lee and Yohan Kim and Uiwon Hwang and Sungroh Yoon and Yunheung Paek},
  doi          = {10.1109/TAI.2021.3103139},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {4},
  pages        = {299-313},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Learn2Evade: Learning-based generative model for evading PDF malware classifiers},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). CovSegNet: A multi encoder–decoder architecture for improved
lesion segmentation of COVID-19 chest CT scans. <em>TAI</em>,
<em>2</em>(3), 283–297. (<a
href="https://doi.org/10.1109/TAI.2021.3064913">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic lung lesion segmentation of chest computer tomography (CT) scans is considered a pivotal stage toward accurate diagnosis and severity measurement of COVID-19. Traditional U-shaped encoder–decoder architecture and its variants suffer from diminutions of contextual information in pooling/upsampling operations with increased semantic gaps among encoded and decoded feature maps as well as instigate vanishing gradient problems for its sequential gradient propagation that result in suboptimal performance. Moreover, operating with 3-D CT volume poses further limitations due to the exponential increase of computational complexity making the optimization difficult. In this article, an automated COVID-19 lesion segmentation scheme is proposed utilizing a highly efficient neural network architecture, namely CovSegNet, to overcome these limitations. Additionally, a two-phase training scheme is introduced where a deeper 2-D network is employed for generating region-of-interest (ROI)-enhanced CT volume followed by a shallower 3-D network for further enhancement with more contextual information without increasing computational burden. Along with the traditional vertical expansion of Unet, we have introduced horizontal expansion with multistage encoder–decoder modules for achieving optimum performance. Additionally, multiscale feature maps are integrated into the scale transition process to overcome the loss of contextual information. Moreover, a multiscale fusion module is introduced with a pyramid fusion scheme to reduce the semantic gaps between subsequent encoder/decoder modules while facilitating the parallel optimization for efficient gradient propagation. Outstanding performances have been achieved in three publicly available datasets that largely outperform other state-of-the-art approaches. The proposed scheme can be easily extended for achieving optimum segmentation performances in a wide variety of applications. Impact Statement—With lower sensitivity (60–70%), elongated testing time, and a dire shortage of testing kits, traditional RTPCR based COVID-19 diagnostic scheme heavily relies on postCT based manual inspection for further investigation. Hence, automating the process of infected lesions extraction from chestCT volumes will be major progress for faster accurate diagnosis of COVID-19. However, in challenging conditions with diffused, blurred, and varying shaped edges of COVID-19 lesions, conventional approaches fail to provide precise segmentation of lesions that can be deleterious for false estimation and loss of information. The proposed scheme incorporating an efficient neural network architecture (CovSegNet) overcomes the limitations of traditional approaches that provide significant improvement of performance (8.4% in averaged dice measurement scale) over two datasets. Therefore, this scheme can be an effective, economical tool for the physicians for faster infection analysis to greatly reduce the spread and massive death toll of this deadly virus through mass-screening.},
  archive      = {J_TAI},
  author       = {Tanvir Mahmud and Md Awsafur Rahman and Shaikh Anowarul Fattah and Sun-Yuan Kung},
  doi          = {10.1109/TAI.2021.3064913},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {3},
  pages        = {283-297},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {CovSegNet: A multi Encoder–Decoder architecture for improved lesion segmentation of COVID-19 chest CT scans},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Constraint preserving score for automatic hyperparameter
tuning of dimensionality reduction methods for visualization.
<em>TAI</em>, <em>2</em>(3), 269–282. (<a
href="https://doi.org/10.1109/TAI.2021.3094774">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In data analysis, visualization through dimensionality reduction (DR) is one of the most effective ways to understand a dataset. However, the quality of a visualization is hard to evaluate quantitatively and the hyperparameters of visualization algorithms are sometimes difficult to tune for end-users. This article proposes a score for visualization assessment that can be used to ease the choice of hyperparameter values for widely used DR methods like &lt;inline-formula xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:xlink=&quot;http://www.w3.org/1999/xlink&quot;&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$t$&lt;/tex-math&gt;&lt;/inline-formula&gt; -distributed stochastic neighbor embedding, LargeVis, and uniform manifold approximation and projection. We present the constraint preserving score , a computationally efficient score to measure visualization quality. The idea is to measure how well a visualization preserves the information encoded in pairwise constraints like group information or similarity/dissimilarity relationships between instances. Based on this quantitative measure, we use Bayesian optimization to effectively explore the solution space of all visualizations and find the most suitable one. The proposed score is flexible as it can measure quality in different ways depending on the provided constraints. Experiments show its interest for end-users, its complementarity with existing visualization quality measures, and its flexibility to easily express different quality aspects. Impact Statement—When working with high-dimensional data, visualization techniques are useful tools to help us to understand patterns in data. Widely used visualization methods such as &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$t$&lt;/tex-math&gt;&lt;/inline-formula&gt;-distributed stochastic neighbor embedding, LargeVis, and uniform manifold approximation and projection require tuning several hyperparameters, which is a tedious task for end-users. The visualizations are usually assessed qualitatively and subjectively by users since we lack quantitative measures that fit their needs. Our work tackles this problem by proposing a novel score based on user&#39;s constraints to measure visualization quality. This score can, thus, be used to automatically tune the hyperparameters of visualization methods. For real-world datasets, there are typically multiple aspects hidden in the data under the form of local or global structures, or relationships between data groups. One visualization gives us one vantage point to look at the data and, thus, reveals one specific aspect of the data. Assessing the visualization quality is still an open question and each state-of-the-art visualization quality metric is designed to capture only one specific aspect like local neighborhood structure. However, our proposed constraints preserving score can capture other different aspects of the visualization like the global structure or semantic relationships between groups according to the information encoded in the input constraints. Our score measures how well the information encoded in input constraints is preserved in a visualization, and suggests the best visualization corresponding to the users’ needs. This score can have a large impact since it is very easy to use and works with any visualization method. Domain experts can express their knowledge in a simple form of similar or dissimilar groups of points. If needed, end-users can use a small amount of labeled data to express their constraints.},
  archive      = {J_TAI},
  author       = {Viet Minh Vu and Adrien Bibal and Benoît Frénay},
  doi          = {10.1109/TAI.2021.3094774},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {3},
  pages        = {269-282},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Constraint preserving score for automatic hyperparameter tuning of dimensionality reduction methods for visualization},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Arrhythmic heartbeat classification using ensemble of random
forest and support vector machine algorithm. <em>TAI</em>,
<em>2</em>(3), 260–268. (<a
href="https://doi.org/10.1109/TAI.2021.3083689">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, an automated heartbeat classification has been proposed to prevent the growing threats of cardiovascular diseases around the world. The MIT-BIH arrhythmia database has been used for the training and testing of the proposed approach. The database contains 90 095 normal (N), 2781 supraventricular (SVEB), 7008 ventricular (VEB), 802 fusions (F), and 15 unclassified (Q) beats each of 30 min duration. Total 61 features have been extracted using the time-series feature extraction library (TSFEL). Feature selection or reduction methods applied are feature scaling, removal of highly correlated and low variance features, and random forest (RF) recursive feature elimination. The methodological novelty of this study is mainly the incorporation of TSFEL during feature extractions, synthetic minority oversampling technique to create a balanced dataset, and an ensemble of RF and support vector machine (SVM) using weighted majority algorithm for heartbeat classification to improve the results. Grid search has been performed to optimize the hyperparameters of RF and SVM classifiers. A final evaluation has been carried out considering a “subject-specific” scheme. The sensitivity that our approach has achieved for the arrhythmic heartbeat classes is as follows: N: 99.50%, SVEB (S): 74.20%, VEB (V): 94.22%, F: 73.21%, and Q: 0%. The corresponding positive predictive values are N: 98.67%, SVEB (S): 90.09%, VEB (V): 95.95%, F: 88.35%, and Q: 0%. In comparison with machine learning and deep learning based state-of-the-art approaches, significant improvement in efficiency has been found. Impact Statement—Automated classifications of heartbeats from ECG signals are very much required to speed up the diagnosis of cardiovascular diseases (CVDs). Our study demonstrates 98.21% accuracy compared to existing state-of-the-art approaches. This study impacts the intensive health care system by offering an easy, quick, and cost-effective diagnosis of CVDs. Besides, remote patients can also receive the treatments timely. Early intervention and prevention of CVDs can prevent almost 30% of the total death every year. The proposed approach has a societal and economic impact too, as the health, welfare, and productivity of the citizens as well as of the country will be improved significantly. Approximately 2% of gross domestic product (GDP) will be improved across low- and middle-income countries as the proposed approaches will reduce the direct (treatment cost) and indirect costs (loss in productivities at workplaces) associated with CVDs.},
  archive      = {J_TAI},
  author       = {Shreya Bhattacharyya and Souvik Majumder and Papiya Debnath and Manash Chanda},
  doi          = {10.1109/TAI.2021.3083689},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {3},
  pages        = {260-268},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Arrhythmic heartbeat classification using ensemble of random forest and support vector machine algorithm},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Reconstruction for indoor scenes based on an interpretable
inference. <em>TAI</em>, <em>2</em>(3), 251–259. (<a
href="https://doi.org/10.1109/TAI.2021.3093505">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autonomous robots for medical and emergency supplies are a potential way to avoid contact with people in quarantine and control the spread of contagious diseases in an indoor scene. However, scene understanding and reconstruction through a single low-cost camera remains a challenge. It is known that absolute precise depth cannot be calculated accurately from a single image, but the relative pose of different planes, which can be inferred from geometric features in a 2-D image, are more likely to be used in understanding scenes and its reconstruction. In this article, we present an interpretable model to bridge the gap between 2-D scene understanding and three-dimensional (3-D) reconstruction without prior training or any precise depth data. Based on 2-D semantic information in our previous works, the 3-D relative pose of estimated planes can be estimated. At that point, indoor scenes are approximated in the reconstruction. The approach behaves as an interpretable characteristic and requires no prior training or knowledge of the camera&#39;s internal parameters. We compare the quantitative performance on the percentage of incorrectly reconstructed planes by relative pose estimation. The results demonstrated that the method can successfully understand and reconstruct indoor scenes including both Manhattan and curved non-Manhattan structures.},
  archive      = {J_TAI},
  author       = {Luping Wang and Hui Wei},
  doi          = {10.1109/TAI.2021.3093505},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {3},
  pages        = {251-259},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Reconstruction for indoor scenes based on an interpretable inference},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust vehicle detection in high-resolution aerial images
with imbalanced data. <em>TAI</em>, <em>2</em>(3), 238–250. (<a
href="https://doi.org/10.1109/TAI.2021.3081057">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vehicle detection in images from unmanned aerial vehicles (UAVs) plays an important role in traffic surveillance and urban planning due to the popularity of UAVs. However, the class imbalance problem is an important factor that restricts the performance of vehicle detectors. There are two types of class imbalance in UAV images, i.e., foreground-background imbalance and foreground–foreground imbalance. For anchor-based single stage detector, as many ground truths cannot be assigned to corresponding anchors because of low intersection over union, it makes the foreground-background imbalance problem more severe. Therefore, we propose a novel bag-based single-stage detector, which treats each position on the feature map as a bag. A simple and adaptive definition of bags is proposed along with the positive sample definition method, which is utilized to ensure more ground truths can be assigned to proper bags. In addition, we utilize online hard example mining method to control the proportion of positive and negative samples during the training process. To address the foreground–foreground imbalance, we propose a novel data augmentation algorithm, which allows us to create appropriate visual context for under-represented class. Extensive experiments demonstrate the superiority of the proposed algorithm, compared with other state-of-the-art solutions. Impact Statement—Recently, unmanned aerial vehicles (UAVs) are widely used in intelligent transportation due to their low price and high flexibility, which makes vehicle detection in UAV images important for automatically gathering of traffic information. However, the class imbalance problem, which is common in object detection where some classes have far fewer frequencies in the dataset, has an adverse effect on the performance of vehicle detectors. The data augmentation method and deep learning based vehicle detector proposed in this article are able to reduce the negative impact and improve detection performance by at least 1.27% in mean average precision index. In addition, compared with algorithms with similar detection performance, our method is at least 15 ms faster. The proposed method can benefit users in a wide variety of applications including UAV transportation, traffic surveillance, and urban planning.},
  archive      = {J_TAI},
  author       = {Xianghui Li and Xinde Li and Zhijun Li and Xinran Xiong and Mohammad Omar Khyam and Changyin Sun},
  doi          = {10.1109/TAI.2021.3081057},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {3},
  pages        = {238-250},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Robust vehicle detection in high-resolution aerial images with imbalanced data},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Aspect-based capsule network with mutual attention for
recommendations. <em>TAI</em>, <em>2</em>(3), 228–237. (<a
href="https://doi.org/10.1109/TAI.2021.3077831">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Review text is a valuable source of information for recommendation systems and often contains rich semantics with user preferences and item attributes. Recently, mainstream recommendation approaches have been using deep learning techniques to utilize review text. These efforts employ an affinity matrix to model the correlations between users and items and in turn aggregate the contextual features of users and items to form latent representations with some interpretability. However, constructing this simple interaction does not capture the complex correlations between users and items. Furthermore, a single representation of latent features is not sufficient to express user preferences and item attributes in reviews with multiple granularities. To this end, we propose an aspect-based capsule network and mutual attention for recommendation systems, named Acapnet. We design a novel contextual mutual attention mechanism for modeling the fine-grained correlations in the contextual features of users and items. We employ reverse dynamic routing to aggregate user and item context features into aspect features (capsules) for rating predictions; the vector length of a capsule can be used as a basis for estimating aspect importance. Extensive experiments are conducted on five real-world datasets with different features. The results show that the proposed Acapnet is superior to recent state-of-the-art models in terms of performance. Further analysis reveals that our model effectively mitigates the homogeneity among various aspect capsules with strong interpretability. Impact Statement—On e-commerce platforms such as Amazon and Yelp, recommendation systems play a key role as a filtering technology that helps consumers find the best product or service among many choices. However, recent studies have shown that although it can provide relatively accurate services, it often fails to explain why that product is recommended to the user. The model we propose in this article, Acapnet, effectively overcomes the limitation. Compared to state-of-the-art recommendation methods, Acapnet not only outperforms in terms of performance, but also offers some superiority on interpretability.},
  archive      = {J_TAI},
  author       = {Zhenyu Yang and Xuesong Wang and Yuhu Cheng and GuoJing Liu},
  doi          = {10.1109/TAI.2021.3077831},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {3},
  pages        = {228-237},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Aspect-based capsule network with mutual attention for recommendations},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Maximum probability theorem: A framework for probabilistic
machine learning. <em>TAI</em>, <em>2</em>(3), 214–227. (<a
href="https://doi.org/10.1109/TAI.2021.3086046">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a theoretical framework of probabilistic learning derived from the maximum probability (MP) theorem shown in this article. In this probabilistic framework, a model is defined as an event in the probability space, and a model or the associated event —either the true underlying model or the parameterized model—has a quantified probability measure. This quantification of a model&#39;s probability measure is derived by the MP theorem , in which we have shown that an event&#39;s probability measure has an upper bound given its conditional distribution on an arbitrary random variable. Through this alternative framework, the notion of model parameters is encompassed in the definition of the model or the associated event . Therefore, this framework deviates from the conventional approach of assuming a prior on the model parameters. Instead, the regularizing effects of assuming prior over parameters are imposed through maximizing probabilities of models or, according to information theory, minimizing the information content of a model. The probability of a model in our framework is invariant to reparameterization and is solely dependent on the model&#39;s likelihood function. In addition, rather than maximizing the posterior in a conventional Bayesian setting, the objective function in our alternative framework is defined as the probability of set operations (e.g., intersection) on the event of the true underlying model and the event of the model at hand. Our theoretical framework adds clarity to probabilistic learning through solidifying the definition of probabilistic models, quantifying their probabilities, and providing a visual understanding of objective functions. Impact Statement—The choice of prior distribution over the parameters of probabilistic machine learning models determines the regularization of learning algorithms in the Bayesian perspective. The complexity in choice of prior over the parameters and the form of regularization is relative to the complexity of the models being used. Thereby, finding priors for parameters of complex models is often not tractable. We address this problem by uncovering the maximum probability (MP) theorem as a direct consequence of Kolmogorov&#39;s probability theory. Through the lens of the MP theorem, the process of regularizing models is understood and automated. The regularization process is defined as the maximization of the probability of the model. The probability of the model is understood by the MP theorem and is determined by the behavior of the model. The effects of maximizing the probability of the model can be backpropagated in a gradient-based optimization process. Consequently, the MP framework provides a form of black-box regularization and eliminates the need for case-by-case analysis of models to determine priors.},
  archive      = {J_TAI},
  author       = {Amir Emad Marvasti and Ehsan Emad Marvasti and Ulas Bagci and Hassan Foroosh},
  doi          = {10.1109/TAI.2021.3086046},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {3},
  pages        = {214-227},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Maximum probability theorem: A framework for probabilistic machine learning},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A global–local attentive relation detection model for
knowledge-based question answering. <em>TAI</em>, <em>2</em>(2),
200–212. (<a href="https://doi.org/10.1109/TAI.2021.3068697">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge-based question answering (KBQA) is an essential but challenging task for artificial intelligence and natural language processing. A key challenge pertains to the design of effective algorithms for relation detection. Conventional methods model questions and candidate relations separately through the knowledge bases (KBs) without considering the rich word-level interactions between them. This approach may result in local optimal results. This article presents a global–local attentive relation detection model (GLAR) that utilizes the local module to learn the features of word-level interactions and employs the global module to acquire nonlinear relationships between questions and their candidate relations located in KBs. This article also reports on the application of an end-to-end retrieval-based KBQA system incorporating the proposed relation detection model. Experimental results obtained on two datasets demonstrated GLAR&#39;s remarkable performance in the relation detection task. Furthermore, the functioning of end-to-end KBQA systems was significantly improved through the relation detection model, whose results on both datasets outperformed even state-of-the-art methods. Impact Statement—Knowledge-based question answering (KBQA) aims at answering user questions posed over the knowledge bases (KBs). KBQA helps users access knowledge in the KBs more easily, and it works on two subtasks: entity mention detection and relation detection. While existing relation detection algorithms perform well on the global representation of questions and relations sequences, they ignore some local semantic information on interaction cases between them. The technology proposed in this article takes both global and local interactions into account. With superior improvement on two relation detection tasks and two KBQA end tasks, the technology provides more precise answers. It could be used in more applications, including intelligent customer service, intelligent finance, and others.},
  archive      = {J_TAI},
  author       = {Chen Qiu and Guangyou Zhou and Zhihua Cai and Anders Søgaard},
  doi          = {10.1109/TAI.2021.3068697},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {4},
  number       = {2},
  pages        = {200-212},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {A Global–Local attentive relation detection model for knowledge-based question answering},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Explainable AI for a no-teardown vehicle component cost
estimation: A top-down approach. <em>TAI</em>, <em>2</em>(2), 185–199.
(<a href="https://doi.org/10.1109/TAI.2021.3065011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The broader ambition of this article is to popularize an approach for the fair distribution of the quantity of a system&#39;s output to its subsystems while allowing for underlying complex subsystem level interactions. Particularly, we present the use of this framework on a very specific (but generalizable) application, interesting for a more general AI audience. We detail a data-driven approach to vehicle price modeling and its component price estimation by leveraging a combination of concepts from machine learning and game theory. We show an alternative to common teardown methodologies and surveying approaches for component and vehicle price estimation at the manufacturer&#39;s suggested retail price (MSRP) level that has the advantage of bypassing uncertainties involved in gathering teardown data, the need to perform expensive and biased surveying, and the need to perform retail price equivalent or indirect cost multiplier adjustments to mark up direct manufacturing costs to MSRP. This novel exercise not only provides accurate pricing of the technologies at the customer level, but also shows the, a priori known, large gaps in pricing strategies between manufacturers, vehicle classes, market segments, etc. There is also clear interaction between the price of technologies and other specifications present in vehicles. Those results are indication that old methods of manufacturer-level component costing, aggregation, and application of flat and rigid adjustment factors should be carefully examined. The findings are based on a database developed by Argonne, which includes over 64 000 vehicles covering MY1990 to MY2020 with hundreds of vehicle specs.},
  archive      = {J_TAI},
  author       = {Ayman Moawad and Ehsan Islam and Namdoo Kim and Ram Vijayagopal and Aymeric Rousseau and Wei Biao Wu},
  doi          = {10.1109/TAI.2021.3065011},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {4},
  number       = {2},
  pages        = {185-199},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Explainable AI for a no-teardown vehicle component cost estimation: A top-down approach},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Dynamic instance-wise joint feature selection and
classification. <em>TAI</em>, <em>2</em>(2), 169–184. (<a
href="https://doi.org/10.1109/TAI.2021.3077212">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, a dynamic instance-wise joint feature selection and classification framework during testing is presented. Specifically, the proposed framework sequentially selects features one at a time for each data instance, given previously selected features, and stops this process to classify the instance once it determines that including additional features will not improve the final classification decision. In contrast to most of the existing work that utilizes a set of features, common for all data instances, the proposed framework utilizes different features to classify each data instance. An optimization problem is defined for each data instance in terms of the number of selected features and the associated classification accuracy. The optimum solution is derived, and its structure is analyzed. Based on the optimum solution and its properties, two new algorithms are designed. The expected number of features needed to achieve a given classification accuracy is also analytically derived. Finally, the performance of the proposed algorithms is illustrated on 11 public datasets, thus demonstrating their effectiveness and scalability across a broad range of application domains. Impact Statement—In many domains, including but not limited to medicine and criminal justice, experts need to reach an accurate decision in a timely manner using limited resources (e.g., costly tests and time-consuming evidence collection). At the same time, it is desirable to tailor decisions to each individual case (e.g., patient and defendant). Most of the existing machine learning algorithms, however, ignore resource constraints and/or acquire a general solution for all cases, while not scaling in big data settings. The algorithms proposed in this article address such challenges enabling tailored and timely decision making. With a reduction of up to 66% in the average number of features, while maintaining similar accuracy levels, the proposed algorithms can be used for dynamic instance-wise joint feature selection and classification in scenarios involving over one million variables.},
  archive      = {J_TAI},
  author       = {Yasitha Warahena Liyanage and Daphney-Stavroula Zois and Charalampos Chelmis},
  doi          = {10.1109/TAI.2021.3077212},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {4},
  number       = {2},
  pages        = {169-184},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Dynamic instance-wise joint feature selection and classification},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A survey on multiview clustering. <em>TAI</em>,
<em>2</em>(2), 146–168. (<a
href="https://doi.org/10.1109/TAI.2021.3065894">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering is a machine learning paradigm of dividing sample subjects into a number of groups such that subjects in the same groups are more similar to those in other groups. With advances in information acquisition technologies, samples can frequently be viewed from different angles or in different modalities, generating multiview data. Multiview clustering (MVC), that clusters subjects into subgroups using multiview data, has attracted more and more attentions. Although MVC methods have been developed rapidly, there has not been enough survey to summarize and analyze the current progress. Therefore, we propose a novel taxonomy of the MVC approaches. Similar to other machine learning methods, we categorize them into generative and discriminative classes. In the discriminative class, based on the way of view integration, we split it further into five groups—common eigenvector matrix, common coefficient matrix, common indicator matrix, direct combination, and combination after projection. Furthermore, we relate MVC to other topics: multiview representation, ensemble clustering, multitask clustering, multiview supervised, and semisupervised learning. Several representative real-world applications are elaborated for practitioners. Some benchmark multiview datasets are introduced and representative MVC algorithms from each group are empirically evaluated to analyze how they perform on benchmark datasets. To promote future development of MVC approaches, we point out several open problems that may require further investigation and thorough examination. Impact Statement—Multiview clustering has gained the success in a variety of applications in the past decade. In order to obtain a comprehensive picture of the MVC development, we provide a new categorization of existing MVC methods and introduce the representative algorithms in each category. At last, we point out open problems that are worth investigating to advance the MVC study. More promising MVC methods to solve these open problems may appear following this review paper from which a large number of applications can benefit.},
  archive      = {J_TAI},
  author       = {Guoqing Chao and Shiliang Sun and Jinbo Bi},
  doi          = {10.1109/TAI.2021.3065894},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {4},
  number       = {2},
  pages        = {146-168},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {A survey on multiview clustering},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Graph convolutional neural network for human action
recognition: A comprehensive survey. <em>TAI</em>, <em>2</em>(2),
128–145. (<a href="https://doi.org/10.1109/TAI.2021.3076974">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video-based human action recognition is one of the most important and challenging areas of research in the field of computer vision. Human action recognition has found many pragmatic applications in video surveillance, human–computer interaction, entertainment, autonomous driving, etc. Owing to the recent development of deep learning methods for human action recognition, the performance of action recognition has significantly enhanced for challenging datasets. Deep learning techniques are mainly used for recognizing actions in images and videos comprising of Euclidean data. A recent development in deep learning methods is the extension of these techniques to non-Euclidean data or graph data with many nodes and edges. Human body skeleton resembles a graph, therefore, the graph convolutional network (GCN) is applicable to the non-Euclidean body skeleton. In the past few years, GCN has emerged as an important tool for skeleton-based action recognition. Therefore, we conduct a survey using GCN methods for action recognition. Herein, we present a comprehensive overview of recent GCN techniques for action recognition, propose a taxonomy for the categorization of GCN techniques for action recognition, carry out a detailed study of the benchmark datasets, enlist relevant resources and open-source codes, and finally provide an outline for future research directions and trends. To the best of authors’ knowledge, this is the first survey for action recognition using GCN techniques. Impact Statement— Graph convolutional neural networks have marked a great progress in recent years. There is a similarity between body skeleton and a graph; therefore, GCNs have been widely used for skeleton-based action recognition. In this article, we summarize recent graph-based action recognition techniques, provide a deeper insight of these methods, list source-codes and available resources. This article will help the researchers to develop a basic understanding of graph convolutional methods for action recognition, benefit from useful resources, and think about future directions.},
  archive      = {J_TAI},
  author       = {Tasweer Ahmad and Lianwen Jin and Xin Zhang and Songxuan Lai and Guozhi Tang and Luojun Lin},
  doi          = {10.1109/TAI.2021.3076974},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {4},
  number       = {2},
  pages        = {128-145},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Graph convolutional neural network for human action recognition: A comprehensive survey},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Graph learning: A survey. <em>TAI</em>, <em>2</em>(2),
109–127. (<a href="https://doi.org/10.1109/TAI.2021.3076021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graphs are widely used as a popular representation of the network structure of connected data. Graph data can be found in a broad spectrum of application domains such as social systems, ecosystems, biological networks, knowledge graphs, and information systems. With the continuous penetration of artificial intelligence technologies, graph learning (i.e., machine learning on graphs) is gaining attention from both researchers and practitioners. Graph learning proves effective for many tasks, such as classification, link prediction, and matching. Generally, graph learning methods extract relevant features of graphs by taking advantage of machine learning algorithms. In this survey, we present a comprehensive overview on the state-of-the-art of graph learning. Special attention is paid to four categories of existing graph learning methods, including graph signal processing, matrix factorization, random walk, and deep learning. Major models and algorithms under these categories are reviewed, respectively. We examine graph learning applications in areas such as text, images, science, knowledge graphs, and combinatorial optimization. In addition, we discuss several promising research directions in this field.},
  archive      = {J_TAI},
  author       = {Feng Xia and Ke Sun and Shuo Yu and Abdul Aziz and Liangtian Wan and Shirui Pan and Huan Liu},
  doi          = {10.1109/TAI.2021.3076021},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {4},
  number       = {2},
  pages        = {109-127},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Graph learning: A survey},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Privacy and artificial intelligence. <em>TAI</em>,
<em>2</em>(2), 96–108. (<a
href="https://doi.org/10.1109/TAI.2021.3088084">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial intelligence is a rapidly developing field of research with many practical applications. Congruent to advances in technologies that enable big data, deep learning, and neural networks to train, learn, and predict, artificial intelligence creates new risks that are difficult to predict and manage. Such risks include economic turmoil, existential crises, and the dissolution of individual privacy. If unchecked, the capabilities of artificially intelligent systems could pose a fundamental threat to privacy in their operation or these systems may leak information under adversarial conditions. In this article, we survey the literature and provide various scenarios for the use of artificial intelligence, highlighting potential risks to privacy and offering various mitigating strategies. For the purpose of this research, a North American perspective of privacy is adopted. Impact statement—While an appreciation of the privacy risks associated with artificial intelligence is important, a thorough understanding of the assortment of different technologies that comprise artificial intelligence better prepares those implementing such systems in assessing privacy impacts. This can be achieved through the independent consideration of each constituent of an artificially intelligent system and its interactions. Under individual consideration, privacy-enhancing tools can be applied in a targeted manner to reduce the risk associated with specific components of an artificially intelligent system. A generalized North American approach to assess privacy risks in such systems is proposed that will retain applicability as the field of research evolves and can be adapted to account for various sociopolitical influences. With such an approach, privacy risks in artificial intelligent systems can be well understood, measured, and reduced.},
  archive      = {J_TAI},
  author       = {James Curzon and Tracy Ann Kosa and Rajen Akalu and Khalil El-Khatib},
  doi          = {10.1109/TAI.2021.3088084},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {4},
  number       = {2},
  pages        = {96-108},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Privacy and artificial intelligence},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Editorial: What is artificial intelligence? <em>TAI</em>,
<em>2</em>(2), 94–95. (<a
href="https://doi.org/10.1109/TAI.2021.3096243">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Let me share the first definition I will use in my role as editor-in-chief; not as a hard boundary, but as a guiding definition for a first pass to filter papers that do not fit within the scope of this journal. The definition is short and concise, but will nonetheless likely be met with severe scepticism from readers. Definition 1: Artificial Intelligence is the automation of cognition. Presenting AI from a &quot;cognition&quot; perspective, and recalling that the origin of the word &quot;cognition&quot; is the Latin word cogito, which means &quot;to know,&quot; sends a clear message that mere development of software and classic industrial automation are outside the scope of this journal. Digitization of documents and systems is a necessary step for organizations to develop AI capabilities, but such a basic form of digitization is not within the scope of Transactions on Artificial Intelligence. That said, designing the knowledge base or the machine learning algorithms for digitization may be within scope. It is important to note that the above definition did not limit &quot;cognition&quot; to humans. , I see it as potentially limiting when the journal receives a paper on human-AI interaction or the ethics of AI. These papers comprise important subject matter for Transactions on Artificial Intelligence; however, they are not automation of cognition papers. This calls for a second definition to complement that already offered. Definition 2: Artificial Intelligence is social and cognitive phenomena that enable a machine to socially integrate with a society to perform competitive tasks requiring cognitive processes and communicate with other entities in society by exchanging messages with high information content and shorter representations. The above definition does not see AI as a technology or a product alone, but as social and cognitive phenomena. No definition for AI will be error free, sufficiently universal, or concisely unambiguous. Each definition may be more appropriate for particular contexts or even just for a particular timeframe. As discussed above, the purpose here is not to agree on a universal definition, but to have a definition in this context, so we know what we disagree about. The two definitions above do not set hard rules on which papers are within or outside the scope of this journal. They are first layer soft filters that need to be augmented with the experience of the editor-in-chief and associate editors to make professional judgments to shape the scope of IEEE Transactions on Artificial Intelligence. I welcome community responses and opinions via email, and look forward to learning what members of the community see as the pros and cons of these definitions.},
  archive      = {J_TAI},
  author       = {Hussein Abbass},
  doi          = {10.1109/TAI.2021.3096243},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {4},
  number       = {2},
  pages        = {94-95},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Editorial: What is artificial intelligence?},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptive interval type-2 fuzzy filter: An AI agent for
handling uncertainties to preserve image naturalness. <em>TAI</em>,
<em>2</em>(1), 83–92. (<a
href="https://doi.org/10.1109/TAI.2021.3077522">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial intelligence (AI) offers fuzzy set theory (FST) as one of the popular AI agents and decision making tools for digital image processing to increase the robustness of vision-based applications. FST is capable to handle uncertainties while enhancing image quality and preserving its naturalness. This article proposes a novel Adaptive Interval Type-2 Fuzzy Filter (AIT2FF) as an AI agent for preserving the naturalness of nonuniformly illuminated images. The proposed AIT2FF estimates coarse illumination of the input image, which is further processed to obtain the reflectance and refined coarse illumination for the composition of enhanced image. The estimated coarse illumination preserves the naturalness by eliminating uncertainties due to grayness ambiguities in homogeneous regions and spatial ambiguities at edges. The effectiveness of the proposed filter has been presented quantitatively in terms of lightness order error (LOE) and naturalness image quality evaluator (NIQE) score on images from the high dynamic range dataset and images captured with commercial digital cameras. The qualitative comparison visualizes that the enhanced images obtained using the proposed filter maintains visual realism and naturalness. The applications of the proposed filter have also been presented for image dehazing, vehicle tracking, and gradients estimation. Moreover, the detection results for dehazed images have been compared with state-of-the-art methods on real-world task driven testing set from REalistic Single Image DEhazing- &lt;inline-formula xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:xlink=&quot;http://www.w3.org/1999/xlink&quot;&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$\beta$&lt;/tex-math&gt;&lt;/inline-formula&gt; dataset. The comparisons demonstrate that the enhanced images obtained using the proposed AIT2FF approach are better and outperform others in terms of LOE and NIQE measures. Impact Statement—The objective of vision-based applications is to automate the human visual perception. This requires a preprocessed input image with fine details. The image details to achieve human vision suffer from uncertainties in homogeneous regions and at edges. This result in an unnatural preprocessed image and degrades the performance. For example: Noisy images are not good to train deep learning models. AI offers one of the best tools, i.e., fuzzy set theory, which is capable to handle uncertainties. This article proposes an adaptive interval Type-2 fuzzy filter for elimination of uncertainties. The proposed filter enhances the image details and maintains its natural appearance. The elimination of uncertainties using proposed filter drops the error value from 2.573 to 1.929 for nonuniformly illuminated images. The proposed approach is able to boost the performance of various vision-based applications like object detection, vehicle tracking, security, medical diagnostics, etc.},
  archive      = {J_TAI},
  author       = {Teena Sharma and Nishchal K. Verma},
  doi          = {10.1109/TAI.2021.3077522},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {2},
  number       = {1},
  pages        = {83-92},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Adaptive interval type-2 fuzzy filter: An AI agent for handling uncertainties to preserve image naturalness},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptive feedforward neural network control with an
optimized hidden node distribution. <em>TAI</em>, <em>2</em>(1), 71–82.
(<a href="https://doi.org/10.1109/TAI.2021.3074106">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Composite adaptive radial basis function neural network (RBFNN) control with a lattice distribution of hidden nodes has three inherent demerits: 1) the approximation domain of adaptive RBFNNs is difficult to be determined a priori ; 2) only a partial persistence of excitation (PE) condition can be guaranteed; 3) in general, the required number of hidden nodes of RBFNNs is enormous. This article proposes an adaptive feedforward RBFNN controller with an optimized distribution of hidden nodes to suitably address the above demerits. The distribution of the hidden nodes calculated by a K-means algorithm is optimally distributed along the desired state trajectory. The adaptive RBFNN satisfies the PE condition for the periodic reference trajectory. The weights of all hidden nodes will converge to the optimal values. This proposed method considerably reduces the number of hidden nodes, while achieving a better approximation ability. The proposed control scheme shares a similar rationality to that of the classical PID control in two special cases, which can thus be seen as an enhanced PID scheme with a better approximation ability. For the controller implemented by digital devices, the proposed method, for a manipulator with unknown dynamics, potentially achieves better control performance than model-based schemes with accurate dynamics. Simulation results demonstrate the effectiveness of the proposed scheme. This result provides a deeper insight into the coordination of the adaptive neural network control and the deterministic learning theory. Impact Statement —Adaptive RBFNN control learns to control a robot manipulator when both the structures and parameters of the target robot are unknown in advance. Unfortunately, current adaptive RBFNN controllers need a large-scale neural network to approximate the dynamics of the robot manipulator, and the learning performance cannot be guaranteed to converge. The proposed method in this article not only reduces the scale of neural networks to substantially alleviate the computational burden but also evidently achieves better learning performance. Simulation examples show that this method increases the control accuracy by more than nine times and reduces the scale of neural networks by 35 times as compared to the traditional lattice scheme. Intuitively, people usually believe that a model-based controller with an accurate dynamic model may achieve the best control performance. However, compared with the model-based controller with an accurate dynamic model, the proposed control scheme with an unknown dynamic model even further increases the control accuracy by 1.5 times. This technology provides a more straightforward path for engineers, who may be not experts in complicated control system analysis methods, to design an adaptive robotic controller to achieve enhanced performance.},
  archive      = {J_TAI},
  author       = {Qiong Liu and Dongyu Li and Shuzhi Sam Ge and Zhong Ouyang},
  doi          = {10.1109/TAI.2021.3074106},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {2},
  number       = {1},
  pages        = {71-82},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Adaptive feedforward neural network control with an optimized hidden node distribution},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Interpreting the latent space of GANs via measuring
decoupling. <em>TAI</em>, <em>2</em>(1), 58–70. (<a
href="https://doi.org/10.1109/TAI.2021.3071642">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the success of generative adversarial networks (GANs) on various real-world applications, the controllability and security of GANs have raised more and more concerns from the community. Specifically, understanding the latent space of GANs, i.e., obtaining the completely decoupled latent space, is essential for applications in some secure scenarios. At present, there is no quantitative method to measure the decoupling of latent space, which is not conducive to the development of the community. In this article, we propose two methods to measure the sensitivity of latent dimensions: one is a sequential intervention method, and the other is an optimization-based method that measures the sensitivity in both the value and the direction. With these two methods, the decoupling of latent space can be measured by the sparsity of the sensitivity vector obtained. The effectiveness of the proposed methods has been verified by experiments on the representative GANs. Code will be available at https://github.com/iceli1007/latent-analysis-of .},
  archive      = {J_TAI},
  author       = {Ziqiang Li and Rentuo Tao and Jie Wang and Fu Li and Hongjing Niu and Mingdao Yue and Bin Li},
  doi          = {10.1109/TAI.2021.3071642},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {2},
  number       = {1},
  pages        = {58-70},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Interpreting the latent space of GANs via measuring decoupling},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SemanticHash: Hash coding via semantics-guided label
prototype learning. <em>TAI</em>, <em>2</em>(1), 42–57. (<a
href="https://doi.org/10.1109/TAI.2021.3068322">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose SemanticHash, a simple and effective deep neural network model, to leverage semantic word embeddings (e.g., BERT) in hash codes learning. Both images and class labels are compressed into &lt;inline-formula xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:xlink=&quot;http://www.w3.org/1999/xlink&quot;&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$K$&lt;/tex-math&gt;&lt;/inline-formula&gt; -bit binary vectors by using the visual (or the semantic) hash functions, which are jointly learned and aligned to optimize the semantic consistency. The &lt;inline-formula xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:xlink=&quot;http://www.w3.org/1999/xlink&quot;&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$K$&lt;/tex-math&gt;&lt;/inline-formula&gt; -dimensional class label prototypes—projected from semantic word embeddings—guide the hash mapping on the image side and vice versa, creating the &lt;inline-formula xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:xlink=&quot;http://www.w3.org/1999/xlink&quot;&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$K$&lt;/tex-math&gt;&lt;/inline-formula&gt; -bit image hash codes being aligned with their semantic prototypes and therefore more discriminative. Extensive experimental results on four benchmarks, CIFAR10, NUS-WIDE, ImageNet, and MS-COCO datasets, demonstrate the effectiveness of our approach. We also perform studies to analyze the effects of quantization and word semantic spaces and to explain the relations among the learned class prototypes. Finally, the generalization capability of the proposed approach is further demonstrated. It achieves competitive performance in comparison with state-of-the-art unsupervised and zero-shot hashing methods. Impact Statement—Hash code learning is an important technology that enables efficient image retrieval on large-scale data. While existing hashing algorithms can effectively generate compact binary codes in a supervised learning setting trained with a moderate-size dataset, they are demanding to be scalable to large datasets and do not generalize to unseen datasets. The proposed approach overcomes these limitations. Compared with state-of-the-art ones, our solution achieves 2.1% of average performance improvement on four moderate-size benchmarks and 4.7% of improvement on ImageNet, a large-scale dataset with over 1.2 M training images. With superior performance on popular benchmarks for binary hash code learning, the technology introduced performs well on cross-dataset and zero-shot (i.e., the testing concepts are unseen during training) scenarios too. Our approach attains over 17.7% of zero-shot retrieval performance improvement when compared to the state-of-the-art in the area. This article thus provides a powerful solution to utilize massive data for fast and accurate image retrieval in the big data era.},
  archive      = {J_TAI},
  author       = {Cheng-Hao Tu and Huei-Fang Yang and Shih-Min Yang and Mei-Chen Yeh and Chu-Song Chen},
  doi          = {10.1109/TAI.2021.3068322},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {2},
  number       = {1},
  pages        = {42-57},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {SemanticHash: Hash coding via semantics-guided label prototype learning},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Effectiveness of deep learning on serial fusion based
biometric systems. <em>TAI</em>, <em>2</em>(1), 28–41. (<a
href="https://doi.org/10.1109/TAI.2021.3064003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a framework for multibiometric systems, which combines a deep learning technique with the serial fusion method. Deep learning techniques have been used in unimodal and parallel fusion-based multimodal biometric systems in the past few years. While deep learning techniques have been successful in improving the authentication accuracy, a biometric system is still challenged by two issues: 1) a unimodal system suffers from environmental interference, spoofing attacks, and nonuniversality, and 2) a parallel fusion-based multimodal system suffers from user inconvenience as it requires the user to provide multiple biometrics, which in turn takes longer verification times. A serial fusion method can improve user convenience in a multibiometric system by requiring a user to submit only a subset of the available biometrics. To our knowledge, the effectiveness of using a deep learning technique with a serial fusion method in multibiometric systems is still underexplored. In this article, we close this research gap. We develop a three-stage multibiometric system using a user&#39;s fingerprint, palm, and face and test three serial fusion methods with a Siamese neural network. Our experiments achieve an AUC of 0.9996, where the genuine users require only 1.56 biometrics (instead of all 3) on an average. Impact statement—We work on enhancing the user convenience and reducing the verification error in a multibiometric system. An improved multibiometric system can help law enforcement, homeland security, defense, and our daily lives by providing better access control. With the advent of deep learning technologies, the accuracy of multibiometric systems have been improved significantly; however, its applicability is still in question because of long verification times required by parallel fusion in a multibiometric system. Our proposed multibiometric framework alleviates this user inconvenience issue by utilizing a serial fusion strategy in decision making and improves accuracy by leveraging deep learning technology in feature extraction and score generation.},
  archive      = {J_TAI},
  author       = {Tiffanie Edwards and Md Shafaeat Hossain},
  doi          = {10.1109/TAI.2021.3064003},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {2},
  number       = {1},
  pages        = {28-41},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Effectiveness of deep learning on serial fusion based biometric systems},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Simpson’s paradox in COVID-19 case fatality rates: A
mediation analysis of age-related causal effects. <em>TAI</em>,
<em>2</em>(1), 18–27. (<a
href="https://doi.org/10.1109/TAI.2021.3073088">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We point out an instantiation of Simpson&#39;s paradox in COVID-19 case fatality rates ( cfr s): comparing a large-scale study from China (February 17) with early reports from Italy (March 9), we find that cfr s are lower in Italy for every age group, but higher overall. This phenomenon is explained by a stark difference in case demographic between the two countries. Using this as a motivating example, we introduce basic concepts from mediation analysis and show how these can be used to quantify different direct and indirect effects when assuming a coarse-grained causal graph involving country, age, and case fatality. We curate an age-stratified cfr dataset with &lt;inline-formula xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:xlink=&quot;http://www.w3.org/1999/xlink&quot;&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$&amp;gt;$&lt;/tex-math&gt;&lt;/inline-formula&gt; 750 k cases and conduct a case study, investigating total, direct, and indirect (age-mediated) causal effects between different countries and at different points in time. This allows us to separate age-related effects from others unrelated to age and facilitates a more transparent comparison of cfr s across countries at different stages of the COVID-19 pandemic. Using longitudinal data from Italy, we discover a sign reversal of the direct causal effect in mid-March, which temporally aligns with the reported collapse of the healthcare system in parts of the country. Moreover, we find that direct and indirect effects across 132 pairs of countries are only weakly correlated, suggesting that a country&#39;s policy and case demographic may be largely unrelated. We point out limitations and extensions for future work, and finally, discuss the role of causal reasoning in the broader context of using AI to combat the COVID-19 pandemic. Impact Statement—During a global pandemic, understanding the causal effects of risk factors such as age on COVID-19 fatality is an important scientific question. Since randomised controlled trials are typically infeasible or unethical in this context, causal investigations based on observational data—such as the one carried out in this article—will, therefore, be crucial in guiding our understanding of the available data. Causal inference, in particular mediation analysis, can be used to resolve apparent statistical paradoxes; help educate the public and decision-makers alike; avoid unsound comparisons; and answer a range of causal questions pertaining to the pandemic, subject to transparently stated assumptions. Our exposition helps clarify how mediation analysis can be used to investigate direct and indirect effects along different causal paths and thus serves as a stepping stone for future studies of other important risk factors for COVID-19 besides age.},
  archive      = {J_TAI},
  author       = {Julius von Kügelgen and Luigi Gresele and Bernhard Schölkopf},
  doi          = {10.1109/TAI.2021.3073088},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {2},
  number       = {1},
  pages        = {18-27},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Simpson&#39;s paradox in COVID-19 case fatality rates: A mediation analysis of age-related causal effects},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Optimal policy for bernoulli bandits: Computation and
algorithm gauge. <em>TAI</em>, <em>2</em>(1), 2–17. (<a
href="https://doi.org/10.1109/TAI.2021.3074122">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bernoulli multi-armed bandits are a reinforcement learning model used to study a variety of choice optimization problems. Often such optimizations concern a finite-time horizon. In principle, statistically optimal policies can be computed via dynamic programming, but doing so is considered infeasible due to prohibitive computational requirements and implementation complexity. Hence, suboptimal algorithms are applied in practice, despite their unknown level of suboptimality. In this article, we demonstrate that optimal policies can be efficiently computed for large time horizons or number of arms thanks to a novel memory organization and indexing scheme. We use optimal policies to gauge the suboptimality of several well-known finite- and infinite-time horizon algorithms including Whittle and Gittins indices, epsilon-greedy, Thompson sampling, and upper-confidence bound (UCB) algorithms. Our simulation study shows that all but one evaluated algorithm perform significantly worse than the optimal policy. The Whittle index offers a nearly optimal strategy for multi-armed Bernoulli bandits despite its suboptimal decisions—up to 10%—compared to an optimal policy table. Lastly, we discuss optimizations of known algorithms. We derive a novel solution from UCB1-tuned. It outperforms other infinite-time horizon algorithms when dealing with many arms. Impact statement—Bernoulli bandits are a reinforcement learning model used to improve decisions with binary outcomes. They have various applications ranging from headline news selection to clinical trials. Existing bandit algorithms are suboptimal. This article provides the first practical computation method, which determines the optimal decisions in Bernoulli bandits. It provides the lowest achievable decision regret (maximum expected benefit). In clinical trials, where an algorithm selects treatments for subsequent patients, our method can substantially reduce the number of unsuccessfully treated patients—by up to 5×. The optimal strategy is also used for new comprehensive evaluations of well-known suboptimal algorithms. This can significantly improve decision effectiveness in various applications.},
  archive      = {J_TAI},
  author       = {Sebastian Pilarski and Slawomir Pilarski and Dániel Varró},
  doi          = {10.1109/TAI.2021.3074122},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {2},
  number       = {1},
  pages        = {2-17},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Optimal policy for bernoulli bandits: Computation and algorithm gauge},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
</ul>

</body>
</html>
