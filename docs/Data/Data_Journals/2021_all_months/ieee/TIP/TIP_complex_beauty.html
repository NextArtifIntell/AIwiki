<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TIP_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tip---706">TIP - 706</h2>
<ul>
<li><details>
<summary>
(2021). IEEE signal processing society information. <em>TIP</em>,
<em>30</em>, C3. (<a
href="https://doi.org/10.1109/TIP.2021.3129405">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presents a listing of the editorial board, board of governors, current staff, committee members, and/or society editors for this issue of the publication.},
  archive      = {J_TIP},
  doi          = {10.1109/TIP.2021.3129405},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {C3},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {IEEE signal processing society information},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). IEEE transactions on image processing publication
information. <em>TIP</em>, <em>30</em>, C1. (<a
href="https://doi.org/10.1109/TIP.2021.3129399">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Provides a listing of current committee members and society officers.},
  archive      = {J_TIP},
  doi          = {10.1109/TIP.2021.3129399},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {C1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {IEEE transactions on image processing publication information},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Part-guided relational transformers for fine-grained visual
recognition. <em>TIP</em>, <em>30</em>, 9470–9481. (<a
href="https://doi.org/10.1109/TIP.2021.3126490">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fine-grained visual recognition is to classify objects with visually similar appearances into subcategories, which has made great progress with the development of deep CNNs. However, handling subtle differences between different subcategories still remains a challenge. In this paper, we propose to solve this issue in one unified framework from two aspects, i.e. , constructing feature-level interrelationships, and capturing part-level discriminative features. This framework, namely PArt-guided Relational Transformers (PART), is proposed to learn the discriminative part features with an automatic part discovery module, and to explore the intrinsic correlations with a feature transformation module by adapting the Transformer models from the field of natural language processing. The part discovery module efficiently discovers the discriminative regions which are highly-corresponded to the gradient descent procedure. Then the second feature transformation module builds correlations within the global embedding and multiple part embedding, enhancing spatial interactions among semantic pixels. Moreover, our proposed approach does not rely on additional part branches in the inference time and reaches state-of-the-art performance on 3 widely-used fine-grained object recognition benchmarks. Experimental results and explainable visualizations demonstrate the effectiveness of our proposed approach.},
  archive      = {J_TIP},
  author       = {Yifan Zhao and Jia Li and Xiaowu Chen and Yonghong Tian},
  doi          = {10.1109/TIP.2021.3126490},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {9470-9481},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Part-guided relational transformers for fine-grained visual recognition},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). HTD: Heterogeneous task decoupling for two-stage object
detection. <em>TIP</em>, <em>30</em>, 9456–9469. (<a
href="https://doi.org/10.1109/TIP.2021.3126423">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decoupling the sibling head has recently shown great potential in relieving the inherent task-misalignment problem in two-stage object detectors. However, existing works design similar structures for the classification and regression, ignoring task-specific characteristics and feature demands. Besides, the shared knowledge that may benefit the two branches is neglected, leading to potential excessive decoupling and semantic inconsistency. To address these two issues, we propose Heterogeneous task decoupling (HTD) framework for object detection, which utilizes a Progressive Graph (PGraph) module and a Border-aware Adaptation (BA) module for task-decoupling. Specifically, we first devise a Semantic Feature Aggregation (SFA) module to aggregate global semantics with image-level supervision, serving as the shared knowledge for the task-decoupled framework. Then, the PGraph module performs progressive graph reasoning, including local spatial aggregation and global semantic interaction, to enhance semantic representations of region proposals for classification. The proposed BA module integrates multi-level features adaptively, focusing on the low-level border activation to obtain representations with spatial and border perception for regression. Finally, we utilize the aggregated knowledge from SFA to keep the instance-level semantic consistency (ISC) of decoupled frameworks. Extensive experiments demonstrate that HTD outperforms existing detection works by a large margin, and achieves single-model 50.4\%AP and 33.2\% AP s on COCO test-dev set using ResNet-101-DCN backbone, which is the best entry among state-of-the-arts under the same configuration. Our code is available at https://github.com/CityU-AIM-Group/HTD .},
  archive      = {J_TIP},
  author       = {Wuyang Li and Zhen Chen and Baopu Li and Dingwen Zhang and Yixuan Yuan},
  doi          = {10.1109/TIP.2021.3126423},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {9456-9469},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {HTD: Heterogeneous task decoupling for two-stage object detection},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). RLStereo: Real-time stereo matching based on reinforcement
learning. <em>TIP</em>, <em>30</em>, 9442–9455. (<a
href="https://doi.org/10.1109/TIP.2021.3126418">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many state-of-the-art stereo matching algorithms based on deep learning have been proposed in recent years, which usually construct a cost volume and adopt cost filtering by a series of 3D convolutions. In essence, the possibility of all the disparities is exhaustively represented in the cost volume, and the estimated disparity holds the maximal possibility. The cost filtering could learn contextual information and reduce mismatches in ill-posed regions. However, this kind of methods has two main disadvantages: 1) cost filtering is very time-consuming, and it is thus difficult to simultaneously satisfy the requirements for both speed and accuracy; 2) thickness of the cost volume determines the disparity range which can be estimated, and the pre-defined disparity range may not meet the demand of practical application. This paper proposes a novel real-time stereo matching method called RLStereo, which is based on reinforcement learning and abandons the cost volume or the routine of exhaustive search. The trained RLStereo makes only a few actions iteratively to search the value of the disparity for each pair of stereo images. Experimental results show the effectiveness of the proposed method, which achieves comparable performances to state-of-the-art algorithms with real-time speed on the public large-scale testset, i.e., Scene Flow.},
  archive      = {J_TIP},
  author       = {Menglong Yang and Fangrui Wu and Wei Li},
  doi          = {10.1109/TIP.2021.3126418},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {9442-9455},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {RLStereo: Real-time stereo matching based on reinforcement learning},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Examinee-examiner network: Weakly supervised accurate
coronary lumen segmentation using centerline constraint. <em>TIP</em>,
<em>30</em>, 9429–9441. (<a
href="https://doi.org/10.1109/TIP.2021.3125490">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate coronary lumen segmentation on coronary-computed tomography angiography (CCTA) images is crucial for quantification of coronary stenosis and the subsequent computation of fractional flow reserve. Many factors including difficulty in labeling coronary lumens, various morphologies in stenotic lesions, thin structures and small volume ratio with respect to the imaging field complicate the task. In this work, we fused the continuity topological information of centerlines which are easily accessible, and proposed a novel weakly supervised model, Examinee-Examiner Network (EE-Net), to overcome the challenges in automatic coronary lumen segmentation. First, the EE-Net was proposed to address the fracture in segmentation caused by stenoses by combining the semantic features of lumens and the geometric constraints of continuous topology obtained from the centerlines. Then, a Centerline Gaussian Mask Module was proposed to deal with the insensitiveness of the network to the centerlines. Subsequently, a weakly supervised learning strategy, Examinee-Examiner Learning, was proposed to handle the weakly supervised situation with few lumen labels by using our EE-Net to guide and constrain the segmentation with customized prior conditions. Finally, a general network layer, Drop Output Layer, was proposed to adapt to the class imbalance by dropping well-segmented regions and weights the classes dynamically. Extensive experiments on two different data sets demonstrated that our EE-Net has good continuity and generalization ability on coronary lumen segmentation task compared with several widely used CNNs such as 3D-UNet. The results revealed our EE-Net with great potential for achieving accurate coronary lumen segmentation in patients with coronary artery disease. Code at http://github.com/qiyaolei/Examinee-Examiner-Network .},
  archive      = {J_TIP},
  author       = {Yaolei Qi and Han Xu and Yuting He and Guanyu Li and Zehang Li and Youyong Kong and Jean-Louis Coatrieux and Huazhong Shu and Guanyu Yang and Shengxian Tu},
  doi          = {10.1109/TIP.2021.3125490},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {9429-9441},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Examinee-examiner network: Weakly supervised accurate coronary lumen segmentation using centerline constraint},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Real-time computation of 3D wireframes in computer-generated
holography. <em>TIP</em>, <em>30</em>, 9418–9428. (<a
href="https://doi.org/10.1109/TIP.2021.3125495">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computer-Generated Holography (CGH) algorithms simulate numerical diffraction, being applied in particular for holographic display technology. Due to the wave-based nature of diffraction, CGH is highly computationally intensive, making it especially challenging for driving high-resolution displays in real-time. To this end, we propose a technique for efficiently calculating holograms of 3D line segments. We express the solutions analytically and devise an efficiently computable approximation suitable for massively parallel computing architectures. The algorithms are implemented on a GPU (with CUDA), and we obtain a 70-fold speedup over the reference point-wise algorithm with almost imperceptible quality loss. We report real-time frame rates for CGH of complex 3D line-drawn objects, and validate the algorithm in both a simulation environment as well as on a holographic display setup.},
  archive      = {J_TIP},
  author       = {David Blinder and Takashi Nishitsuji and Peter Schelkens},
  doi          = {10.1109/TIP.2021.3125495},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {9418-9428},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Real-time computation of 3D wireframes in computer-generated holography},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). 3D interactive segmentation with semi-implicit
representation and active learning. <em>TIP</em>, <em>30</em>,
9402–9417. (<a href="https://doi.org/10.1109/TIP.2021.3125491">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Segmenting complex 3D geometry is a challenging task due to rich structural details and complex appearance variations of target object. Shape representation and foreground-background delineation are two of the core components of segmentation. Explicit shape models, such as mesh based representations, suffer from poor handling of topological changes. On the other hand, implicit shape models, such as level-set based representations, have limited capacity for interactive manipulation. Fully automatic segmentation for separating foreground objects from background generally utilizes non-interoperable machine learning methods, which heavily rely on the off-line training dataset and are limited to the discrimination power of the chosen model. To address these issues, we propose a novel semi-implicit representation method, namely Non-Uniform Implicit B-spline Surface (NU-IBS), which adaptively distributes parametrically blended patches according to geometrical complexity. Then, a two-stage cascade classifier is introduced to carry out efficient foreground and background delineation, where a simplistic Naïve-Bayesian model is trained for fast background elimination, followed by a stronger pseudo-3D Convolutional Neural Network (CNN) multi-scale classifier to precisely identify the foreground objects. A localized interactive and adaptive segmentation scheme is incorporated to boost the delineation accuracy by utilizing the information iteratively gained from user intervention. The segmentation result is obtained via deforming an NU-IBS according to the probabilistic interpretation of delineated regions, which also imposes a homogeneity constrain for individual segments. The proposed method is evaluated on a 3D cardiovascular Computed Tomography Angiography (CTA) image dataset and Brain Tumor Image Segmentation Benchmark 2015 (BraTS2015) 3D Magnetic Resonance Imaging (MRI) dataset.},
  archive      = {J_TIP},
  author       = {Jingjing Deng and Xianghua Xie},
  doi          = {10.1109/TIP.2021.3125491},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {9402-9417},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {3D interactive segmentation with semi-implicit representation and active learning},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Image noise reduction based on a fixed wavelet frame and
CNNs applied to CT. <em>TIP</em>, <em>30</em>, 9386–9401. (<a
href="https://doi.org/10.1109/TIP.2021.3125489">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Radiation exposure in CT imaging leads to increased patient risk. This motivates the pursuit of reduced-dose scanning protocols, in which noise reduction processing is indispensable to warrant clinically acceptable image quality. Convolutional Neural Networks (CNNs) have received significant attention as an alternative for conventional noise reduction and are able to achieve state-of-the art results. However, the internal signal processing in such networks is often unknown, leading to sub-optimal network architectures. The need for better signal preservation and more transparency motivates the use of Wavelet Shrinkage Networks (WSNs), in which the Encoding-Decoding (ED) path is the fixed wavelet frame known as Overcomplete Haar Wavelet Transform (OHWT) and the noise reduction stage is data-driven. In this work, we considerably extend the WSN framework by focusing on three main improvements. First, we simplify the computation of the OHWT that can be easily reproduced. Second, we update the architecture of the shrinkage stage by further incorporating knowledge of conventional wavelet shrinkage methods. Finally, we extensively test its performance and generalization, by comparing it with the RED and FBPConvNet CNNs. Our results show that the proposed architecture achieves similar performance to the reference in terms of MSSIM (0.667, 0.662 and 0.657 for DHSN2, FBPConvNet and RED, respectively) and achieves excellent quality when visualizing patches of clinically important structures. Furthermore, we demonstrate the enhanced generalization and further advantages of the signal flow, by showing two additional potential applications, in which the new DHSN2 is used as regularizer: (1) iterative reconstruction and (2) ground-truth free training of the proposed noise reduction architecture. The presented results prove that the tight integration of signal processing and deep learning leads to simpler models with improved generalization.},
  archive      = {J_TIP},
  author       = {Luis Albert Zavala-Mondragón and Peter H. N. de With and Fons van der Sommen},
  doi          = {10.1109/TIP.2021.3125489},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {9386-9401},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Image noise reduction based on a fixed wavelet frame and CNNs applied to CT},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Burst photography for learning to enhance extremely dark
images. <em>TIP</em>, <em>30</em>, 9372–9385. (<a
href="https://doi.org/10.1109/TIP.2021.3125394">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Capturing images under extremely low-light conditions poses significant challenges for the standard camera pipeline. Images become too dark and too noisy, which makes traditional enhancement techniques almost impossible to apply. Recently, learning-based approaches have shown very promising results for this task since they have substantially more expressive capabilities to allow for improved quality. Motivated by these studies, in this paper, we aim to leverage burst photography to boost the performance and obtain much sharper and more accurate RGB images from extremely dark raw images. The backbone of our proposed framework is a novel coarse-to-fine network architecture that generates high-quality outputs progressively. The coarse network predicts a low-resolution, denoised raw image, which is then fed to the fine network to recover fine-scale details and realistic textures. To further reduce the noise level and improve the color accuracy, we extend this network to a permutation invariant structure so that it takes a burst of low-light images as input and merges information from multiple images at the feature-level. Our experiments demonstrate that our approach leads to perceptually more pleasing results than the state-of-the-art methods by producing more detailed and considerably higher quality images.},
  archive      = {J_TIP},
  author       = {Ahmet Serdar Karadeniz and Erkut Erdem and Aykut Erdem},
  doi          = {10.1109/TIP.2021.3125394},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {9372-9385},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Burst photography for learning to enhance extremely dark images},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adversarial domain adaptation with prototype-based
normalized output conditioner. <em>TIP</em>, <em>30</em>, 9359–9371. (<a
href="https://doi.org/10.1109/TIP.2021.3124674">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain adversarial training has become a prevailing and effective paradigm for unsupervised domain adaptation (UDA). To successfully align the multi-modal data structures across domains, the following works exploit discriminative information in the adversarial training process, e.g., using multiple class-wise discriminators and involving conditional information in the input or output of the domain discriminator. However, these methods either require non-trivial model designs or are inefficient for UDA tasks. In this work, we attempt to address this dilemma by devising simple and compact conditional domain adversarial training methods. We first revisit the simple concatenation conditioning strategy where features are concatenated with output predictions as the input of the discriminator. We find the concatenation strategy suffers from the weak conditioning strength. We further demonstrate that enlarging the norm of concatenated predictions can effectively energize the conditional domain alignment. Thus we improve concatenation conditioning by normalizing the output predictions to have the same norm of features, and term the derived method as Normalized OutpUt coNditioner (NOUN). However, conditioning on raw output predictions for domain alignment, NOUN suffers from inaccurate predictions of the target domain. To this end, we propose to condition the cross-domain feature alignment in the prototype space rather than in the output space. Combining the novel prototype-based conditioning with NOUN, we term the enhanced method as PROtotype-based Normalized OutpUt coNditioner (PRONOUN). Experiments on both object recognition and semantic segmentation show that NOUN can effectively align the multi-modal structures across domains and even outperform state-of-the-art domain adversarial training methods. Together with prototype-based conditioning, PRONOUN further improves the adaptation performance over NOUN on multiple object recognition benchmarks for UDA. Code is available at https://github.com/tim-learn/NOUN .},
  archive      = {J_TIP},
  author       = {Dapeng Hu and Jian Liang and Qibin Hou and Hanshu Yan and Yunpeng Chen},
  doi          = {10.1109/TIP.2021.3124674},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {9359-9371},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Adversarial domain adaptation with prototype-based normalized output conditioner},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Spatially adaptive feature refinement for efficient
inference. <em>TIP</em>, <em>30</em>, 9345–9358. (<a
href="https://doi.org/10.1109/TIP.2021.3125263">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatial redundancy commonly exists in the learned representations of convolutional neural networks (CNNs), leading to unnecessary computation on high-resolution features. In this paper, we propose a novel Spatially Adaptive feature Refinement (SAR) approach to reduce such superfluous computation. It performs efficient inference by adaptively fusing information from two branches: one conducts standard convolution on input features at a lower spatial resolution , and the other one selectively refines a set of regions at the original resolution. The two branches complement each other in feature learning, and both of them evoke much less computation than standard convolution. SAR is a flexible method that can be conveniently plugged into existing CNNs to establish models with reduced spatial redundancy. Experiments on CIFAR and ImageNet classification, COCO object detection and PASCAL VOC semantic segmentation tasks validate that the proposed SAR can consistently improve the network performance and efficiency. Notably, our results show that SAR only refines less than 40\% of the regions in the feature representations of a ResNet for 97\% of the samples in the validation set of ImageNet to achieve comparable accuracy with the original model, revealing the high computational redundancy in the spatial dimension of CNNs.},
  archive      = {J_TIP},
  author       = {Yizeng Han and Gao Huang and Shiji Song and Le Yang and Yitian Zhang and Haojun Jiang},
  doi          = {10.1109/TIP.2021.3125263},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {9345-9358},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Spatially adaptive feature refinement for efficient inference},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Multi-hierarchical category supervision for
weakly-supervised temporal action localization. <em>TIP</em>,
<em>30</em>, 9332–9344. (<a
href="https://doi.org/10.1109/TIP.2021.3124671">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weakly Supervised Temporal Action Localization (WTAL) aims to localize action segments in untrimmed videos with only video-level category labels in the training phase. In WTAL, an action generally consists of a series of sub-actions, and different categories of actions may share the common sub-actions. However, to distinguish different categories of actions with only video-level class labels, current WTAL models tend to focus on discriminative sub-actions of the action, while ignoring those common sub-actions shared with different categories of actions. This negligence of common sub-actions would lead to the located action segments incomplete, i.e. , only containing discriminative sub-actions. Different from current approaches of designing complex network architectures to explore more complete actions, in this paper, we introduce a novel supervision method named multi-hierarchical category supervision (MHCS) to find more sub-actions rather than only the discriminative ones. Specifically, action categories sharing similar sub-actions will be constructed as super-classes through hierarchical clustering. Hence, training with the new generated super-classes would encourage the model to pay more attention to the common sub-actions, which are ignored training with the original classes. Furthermore, our proposed MHCS is model-agnostic and non-intrusive, which can be directly applied to existing methods without changing their structures. Through extensive experiments, we verify that our supervision method can improve the performance of four state-of-the-art WTAL methods on three public datasets: THUMOS14, ActivityNet1.2, and ActivityNet1.3.},
  archive      = {J_TIP},
  author       = {Guozhang Li and Jie Li and Nannan Wang and Xinpeng Ding and Zhifeng Li and Xinbo Gao},
  doi          = {10.1109/TIP.2021.3124671},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {9332-9344},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Multi-hierarchical category supervision for weakly-supervised temporal action localization},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Video text tracking with a spatio-temporal complementary
model. <em>TIP</em>, <em>30</em>, 9321–9331. (<a
href="https://doi.org/10.1109/TIP.2021.3124313">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text tracking is to track multiple texts in a video, and construct a trajectory for each text. Existing methods tackle this task by utilizing the tracking-by-detection framework, i.e., detecting the text instances in each frame and associating the corresponding text instances in consecutive frames. We argue that the tracking accuracy of this paradigm is severely limited in more complex scenarios, e.g., owing to motion blur, etc., the missed detection of text instances causes the break of the text trajectory. In addition, different text instances with similar appearance are easily confused, leading to the incorrect association of the text instances. To this end, a novel spatio-temporal complementary text tracking model is proposed in this paper. We leverage a Siamese Complementary Module to fully exploit the continuity characteristic of the text instances in the temporal dimension, which effectively alleviates the missed detection of the text instances, and hence ensures the completeness of each text trajectory. We further integrate the semantic cues and the visual cues of the text instance into a unified representation via a text similarity learning network, which supplies a high discriminative power in the presence of text instances with similar appearance, and thus avoids the mis-association between them. Our method achieves state-of-the-art performance on several public benchmarks. The source code is available at https://github.com/lsabrinax/VideoTextSCM .},
  archive      = {J_TIP},
  author       = {Yuzhe Gao and Xing Li and Jiajian Zhang and Yu Zhou and Dian Jin and Jing Wang and Shenggao Zhu and Xiang Bai},
  doi          = {10.1109/TIP.2021.3124313},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {9321-9331},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Video text tracking with a spatio-temporal complementary model},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Stroke-based scene text erasing using synthetic data for
training. <em>TIP</em>, <em>30</em>, 9306–9320. (<a
href="https://doi.org/10.1109/TIP.2021.3125260">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scene text erasing, which replaces text regions with reasonable content in natural images, has drawn significant attention in the computer vision community in recent years. There are two potential subtasks in scene text erasing: text detection and image inpainting. Both subtasks require considerable data to achieve better performance; however, the lack of a large-scale real-world scene-text removal dataset does not allow existing methods to realize their potential. To compensate for the lack of pairwise real-world data, we made considerable use of synthetic text after additional enhancement and subsequently trained our model only on the dataset generated by the improved synthetic text engine. Our proposed network contains a stroke mask prediction module and background inpainting module that can extract the text stroke as a relatively small hole from the cropped text image to maintain more background content for better inpainting results. This model can partially erase text instances in a scene image with a bounding box or work with an existing scene-text detector for automatic scene text erasing. The experimental results from the qualitative and quantitative evaluation on the SCUT-Syn, ICDAR2013, and SCUT-EnsText datasets demonstrate that our method significantly outperforms existing state-of-the-art methods even when they are trained on real-world data.},
  archive      = {J_TIP},
  author       = {Zhengmi Tang and Tomo Miyazaki and Yoshihiro Sugaya and Shinichiro Omachi},
  doi          = {10.1109/TIP.2021.3125260},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {9306-9320},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Stroke-based scene text erasing using synthetic data for training},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning human-object interaction via interactive semantic
reasoning. <em>TIP</em>, <em>30</em>, 9294–9305. (<a
href="https://doi.org/10.1109/TIP.2021.3125258">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human-Object Interaction (HOI) detection devotes to learn how humans interact with surrounding objects via inferring triplets of $\langle $ human, verb, object $\rangle $ . Recent HOI detection methods infer HOIs by directly extracting appearance features and spatial configuration from related visual targets of human and object, but neglect powerful interactive semantic reasoning between these targets. Meanwhile, existing spatial encodings of visual targets have been simply concatenated to appearance features, which is unable to dynamically promote the visual feature learning. To solve these problems, we first present a novel semantic-based Interactive Reasoning Block, in which interactive semantics implied among visual targets are efficiently exploited. Beyond inferring HOIs using discrete instance features, we then design a HOI Inferring Structure to parse pairwise interactive semantics among visual targets in scene-wide level and instance-wide level. Furthermore, we propose a Spatial Guidance Model based on the location of human body-parts and object, which serves as a geometric guidance to dynamically enhance the visual feature learning. Based on the above modules, we construct a framework named Interactive-Net for HOI detection, which is fully differentiable and end-to-end trainable. Extensive experiments show that our proposed framework outperforms existing HOI detection methods on both V-COCO and HICO-DET benchmarks and improves the baseline about 5.9\% and 17.7\% relatively, validating its efficacy in detecting HOIs.},
  archive      = {J_TIP},
  author       = {Dongming Yang and Yuexian Zou and Zhu Li and Ge Li},
  doi          = {10.1109/TIP.2021.3125258},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {9294-9305},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Learning human-object interaction via interactive semantic reasoning},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Deep unsupervised active learning via matrix sketching.
<em>TIP</em>, <em>30</em>, 9280–9293. (<a
href="https://doi.org/10.1109/TIP.2021.3124317">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing unsupervised active learning methods aim at minimizing the data reconstruction loss by using the linear models to choose representative samples for manually labeling in an unsupervised setting. Thus these methods often fail in modelling data with complex non-linear structure. To address this issue, we propose a new deep unsupervised Active Learning method for classification tasks, inspired by the idea of Matrix Sketching, called ALMS. Specifically, ALMS leverages a deep auto-encoder to embed data into a latent space, and then describes all the embedded data with a small size sketch to summarize the major characteristics of the data. In contrast to previous approaches that reconstruct the whole data matrix for selecting the representative samples, ALMS aims to select a representative subset of samples to well approximate the sketch, which can preserve the major information of data meanwhile significantly reducing the number of network parameters. This makes our algorithm alleviate the issue of model overfitting and readily cope with large datasets. Actually, the sketch provides a type of self-supervised signal to guide the learning of the model. Moreover, we propose to construct an auxiliary self-supervised task by classifying real/fake samples, in order to further improve the representation ability of the encoder. We thoroughly evaluate the performance of ALMS on both single-label and multi-label classification tasks, and the results demonstrate its superior performance against the state-of-the-art methods. The code can be found at https://github.com/lrq99/ALMS .},
  archive      = {J_TIP},
  author       = {Changsheng Li and Rongqing Li and Ye Yuan and Guoren Wang and Dong Xu},
  doi          = {10.1109/TIP.2021.3124317},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {9280-9293},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Deep unsupervised active learning via matrix sketching},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-scale single image dehazing using laplacian and
gaussian pyramids. <em>TIP</em>, <em>30</em>, 9270–9279. (<a
href="https://doi.org/10.1109/TIP.2021.3123551">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Model-based single image dehazing was widely studied due to its extensive applications. Ambiguity between object radiance and haze and noise amplification in sky regions are two inherent problems of model-based single image dehazing. In this paper, a dark direct attenuation prior (DDAP) is proposed to address the former problem. A novel haze line averaging is proposed to reduce the morphological artifacts caused by the DDAP which enables a weighted guided image filter with a smaller radius to further reduce the morphological artifacts while preserve the fine structure in the image. A multi-scale dehazing algorithm is then proposed to address the latter problem by adopting Laplacian and Gaussian pyramids to decompose the hazy image into different levels and applying different haze removal and noise reduction approaches to restore the scene radiance at the different levels. The resultant pyramid is collapsed to restore a haze-free image. Experiment results demonstrate that the proposed algorithm outperforms state-of-the-art dehazing algorithms.},
  archive      = {J_TIP},
  author       = {Zhengguo Li and Haiyan Shu and Chaobing Zheng},
  doi          = {10.1109/TIP.2021.3123551},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {9270-9279},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Multi-scale single image dehazing using laplacian and gaussian pyramids},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Image comes dancing with collaborative parsing-flow video
synthesis. <em>TIP</em>, <em>30</em>, 9259–9269. (<a
href="https://doi.org/10.1109/TIP.2021.3123549">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transferring human motion from a source to a target person poses great potential in computer vision and graphics applications. A crucial step is to manipulate sequential future motion while retaining the appearance characteristic. Previous work has either relied on crafted 3D human models or trained a separate model specifically for each target person, which is not scalable in practice. This work studies a more general setting, in which we aim to learn a single model to parsimoniously transfer motion from a source video to any target person given only one image of the person, named as Collaborative Parsing-Flow Network (CPF-Net). The paucity of information regarding the target person makes the task particularly challenging to faithfully preserve the appearance in varying designated poses. To address this issue, CPF-Net integrates the structured human parsing and appearance flow to guide the realistic foreground synthesis which is merged into the background by a spatio-temporal fusion module. In particular, CPF-Net decouples the problem into stages of human parsing sequence generation, foreground sequence generation and final video generation. The human parsing generation stage captures both the pose and the body structure of the target. The appearance flow is beneficial to keep details in synthesized frames. The integration of human parsing and appearance flow effectively guides the generation of video frames with realistic appearance. Finally, the dedicated designed fusion network ensure the temporal coherence. We further collect a large set of human dancing videos to push forward this research field. Both quantitative and qualitative results show our method substantially improves over previous approaches and is able to generate appealing and photo-realistic target videos given any input person image. All source code and dataset will be released at https://github.com/xiezhy6/CPF-Net .},
  archive      = {J_TIP},
  author       = {Bowen Wu and Zhenyu Xie and Xiaodan Liang and Yubei Xiao and Haoye Dong and Liang Lin},
  doi          = {10.1109/TIP.2021.3123549},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {9259-9269},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Image comes dancing with collaborative parsing-flow video synthesis},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Joint feature disentanglement and hallucination for few-shot
image classification. <em>TIP</em>, <em>30</em>, 9245–9258. (<a
href="https://doi.org/10.1109/TIP.2021.3124322">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot learning (FSL) refers to the learning task that generalizes from base to novel concepts with only few examples observed during training. One intuitive FSL approach is to hallucinate additional training samples for novel categories. While this is typically done by learning from a disjoint set of base categories with sufficient amount of training data, most existing works did not fully exploit the intra-class information from base categories, and thus there is no guarantee that the hallucinated data would represent the class of interest accordingly. In this paper, we propose Feature Disentanglement and Hallucination Network (FDH-Net), which jointly performs feature disentanglement and hallucination for FSL purposes. More specifically, our FDH-Net is able to disentangle input visual data into class-specific and appearance-specific features. With both data recovery and classification constraints, hallucination of image features for novel categories using appearance information extracted from base categories can be achieved. We perform extensive experiments on two fine-grained datasets (CUB and FLO) and two coarse-grained ones ( mini -ImageNet and CIFAR-100). The results confirm that our framework performs favorably against state-of-the-art metric-learning and hallucination-based FSL models.},
  archive      = {J_TIP},
  author       = {Chia-Ching Lin and Hsin-Li Chu and Yu-Chiang Frank Wang and Chin-Laung Lei},
  doi          = {10.1109/TIP.2021.3124322},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {9245-9258},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Joint feature disentanglement and hallucination for few-shot image classification},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Rethinking motion representation: Residual frames with 3D
ConvNets. <em>TIP</em>, <em>30</em>, 9231–9244. (<a
href="https://doi.org/10.1109/TIP.2021.3124156">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, 3D convolutional networks yield good performance in action recognition. However, an optical flow stream is still needed for motion representation to ensure better performance, whose cost is very high. In this paper, we propose a cheap but effective way to extract motion features from videos utilizing residual frames as the input data in 3D ConvNets. By replacing traditional stacked RGB frames with residual ones, 35.6\% and 26.6\% points improvements over top-1 accuracy can be achieved on the UCF101 and HMDB51 datasets when trained from scratch using ResNet-18-3D. We deeply analyze the effectiveness of this modality compared to normal RGB video clips, and find that better motion features can be extracted using residual frames with 3D ConvNets. Considering that residual frames contain little information of object appearance, we further use a 2D convolutional network to extract appearance features and combine them together to form a two-path solution. In this way, we can achieve better performance than some methods which even used an additional optical flow stream. Moreover, the proposed residual-input path can outperform RGB counterpart on unseen datasets when we apply trained models to video retrieval tasks. Huge improvements can also be obtained when the residual inputs are applied to video-based self-supervised learning methods, revealing better motion representation and generalization ability of our proposal.},
  archive      = {J_TIP},
  author       = {Li Tao and Xueting Wang and Toshihiko Yamasaki},
  doi          = {10.1109/TIP.2021.3124156},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {9231-9244},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Rethinking motion representation: Residual frames with 3D ConvNets},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). It GAN do better: GAN-based detection of objects on images
with varying quality. <em>TIP</em>, <em>30</em>, 9220–9230. (<a
href="https://doi.org/10.1109/TIP.2021.3124155">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel generative framework which uses Generative Adversarial Networks (GANs) to generate features that provide robustness for object detection on reduced-quality images. The proposed GAN-based Detection of Objects (GAN-DO) framework is not restricted to any particular architecture and can be generalized to several deep neural network (DNN) based architectures. The resulting deep neural network maintains the exact architecture as the selected baseline model without adding to the model parameter complexity or inference speed. We first evaluate the effect of image quality on both object classification and object bounding box regression. We then test the models resulting from our proposed GAN-DO framework, using two state-of-the-art object detection architectures as the baseline models. We also evaluate the effect of the number of re-trained parameters in the generator of GAN-DO on the accuracy of the final trained model. Performance results provided using GAN-DO on object detection datasets establish an improved robustness to varying image quality and a higher mAP compared to the existing approaches.},
  archive      = {J_TIP},
  author       = {Charan D. Prakash and Lina J. Karam},
  doi          = {10.1109/TIP.2021.3124155},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {9220-9230},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {It GAN do better: GAN-based detection of objects on images with varying quality},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DS-UI: Dual-supervised mixture of gaussian mixture models
for uncertainty inference in image recognition. <em>TIP</em>,
<em>30</em>, 9208–9219. (<a
href="https://doi.org/10.1109/TIP.2021.3123555">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a dual-supervised uncertainty inference (DS-UI) framework for improving Bayesian estimation-based UI in DNN-based image recognition. In the DS-UI, we combine the classifier of a DNN, i.e. , the last fully-connected (FC) layer, with a mixture of Gaussian mixture models (MoGMM) to obtain an MoGMM-FC layer. Unlike existing UI methods for DNNs, which only calculate the means or modes of the DNN outputs’ distributions, the proposed MoGMM-FC layer acts as a probabilistic interpreter for the features that are inputs of the classifier to directly calculate the probabilities of them for the DS-UI. In addition, we propose a dual-supervised stochastic gradient-based variational Bayes (DS-SGVB) algorithm for the MoGMM-FC layer optimization. Unlike conventional SGVB and optimization algorithms in other UI methods, the DS-SGVB not only models the samples in the specific class for each Gaussian mixture model (GMM) in the MoGMM, but also considers the negative samples from other classes for the GMM to reduce the intra-class distances and enlarge the inter-class margins simultaneously for enhancing the learning ability of the MoGMM-FC layer in the DS-UI. Experimental results show the DS-UI outperforms the state-of-the-art UI methods in misclassification detection. We further evaluate the DS-UI in open-set out-of-domain/-distribution detection and find statistically significant improvements. Visualizations of the feature spaces demonstrate the superiority of the DS-UI. Codes are available at https://github.com/PRIS-CV/DS-UI .},
  archive      = {J_TIP},
  author       = {Jiyang Xie and Zhanyu Ma and Jing-Hao Xue and Guoqiang Zhang and Jian Sun and Yinhe Zheng and Jun Guo},
  doi          = {10.1109/TIP.2021.3123555},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {9208-9219},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {DS-UI: Dual-supervised mixture of gaussian mixture models for uncertainty inference in image recognition},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Memorize, associate and match: Embedding enhancement via
fine-grained alignment for image-text retrieval. <em>TIP</em>,
<em>30</em>, 9193–9207. (<a
href="https://doi.org/10.1109/TIP.2021.3123553">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image-text retrieval aims to capture the semantic correlation between images and texts. Existing image-text retrieval methods can be roughly categorized into embedding learning paradigm and pair-wise learning paradigm. The former paradigm fails to capture the fine-grained correspondence between images and texts. The latter paradigm achieves fine-grained alignment between regions and words, but the high cost of pair-wise computation leads to slow retrieval speed. In this paper, we propose a novel method named MEMBER by using Memory-based EMBedding Enhancement for image-text Retrieval (MEMBER), which introduces global memory banks to enable fine-grained alignment and fusion in embedding learning paradigm. Specifically, we enrich image ( resp. , text) features with relevant text ( resp. , image) features stored in the text ( resp. , image) memory bank. In this way, our model not only accomplishes mutual embedding enhancement across two modalities, but also maintains the retrieval efficiency. Extensive experiments demonstrate that our MEMBER remarkably outperforms state-of-the-art approaches on two large-scale benchmark datasets.},
  archive      = {J_TIP},
  author       = {Jiangtong Li and Liu Liu and Li Niu and Liqing Zhang},
  doi          = {10.1109/TIP.2021.3123553},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {9193-9207},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Memorize, associate and match: Embedding enhancement via fine-grained alignment for image-text retrieval},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dynamic selective network for RGB-d salient object
detection. <em>TIP</em>, <em>30</em>, 9179–9192. (<a
href="https://doi.org/10.1109/TIP.2021.3123548">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {RGB-D saliency detection is receiving more and more attention in recent years. There are many efforts have been devoted to this area, where most of them try to integrate the multi-modal information, i.e. RGB images and depth maps, via various fusion strategies. However, some of them ignore the inherent difference between the two modalities, which leads to the performance degradation when handling some challenging scenes. Therefore, in this paper, we propose a novel RGB-D saliency model, namely Dynamic Selective Network (DSNet), to perform salient object detection (SOD) in RGB-D images by taking full advantage of the complementarity between the two modalities. Specifically, we first deploy a cross-modal global context module (CGCM) to acquire the high-level semantic information, which can be used to roughly locate salient objects. Then, we design a dynamic selective module (DSM) to dynamically mine the cross-modal complementary information between RGB images and depth maps, and to further optimize the multi-level and multi-scale information by executing the gated and pooling based selection, respectively. Moreover, we conduct the boundary refinement to obtain high-quality saliency maps with clear boundary details. Extensive experiments on eight public RGB-D datasets show that the proposed DSNet achieves a competitive and excellent performance against the current 17 state-of-the-art RGB-D SOD models.},
  archive      = {J_TIP},
  author       = {Hongfa Wen and Chenggang Yan and Xiaofei Zhou and Runmin Cong and Yaoqi Sun and Bolun Zheng and Jiyong Zhang and Yongjun Bao and Guiguang Ding},
  doi          = {10.1109/TIP.2021.3123548},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {9179-9192},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Dynamic selective network for RGB-D salient object detection},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Depth privileged scene recognition via dual attention
hallucination. <em>TIP</em>, <em>30</em>, 9164–9178. (<a
href="https://doi.org/10.1109/TIP.2021.3122955">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {RGB-D scene recognition has achieved promising performance because depth could provide complementary geometric information to RGB images. However, the inaccessibility of depth sensors severely limits RGB-D applications. In this paper, we focus on depth privileged setting, in which depth information is only available during training but not available during testing. Considering that the information obtained from RGB and depth images are complementary while attention is informative and transferable, our idea is using RGB input to hallucinate depth attention. We build our model upon modulated deformable convolutional layer and hallucinate dual attention: post-hoc importance weight and trainable spatial transformation. Specifically, we use modulation ( resp. , offset) learned from RGB to mimic Grad-CAM ( resp. , offset) learned from depth, to combine the strength of dual attention. We also design a weighted loss to avoid negative transfer according to the quality of depth attention. Extensive experiments on two benchmarks, i.e. , SUN RGB-D and NYUDv2, demonstrate that our method outperforms the state-of-the-art methods for depth privileged scene recognition.},
  archive      = {J_TIP},
  author       = {Junjie Chen and Li Niu and Liqing Zhang},
  doi          = {10.1109/TIP.2021.3122955},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {9164-9178},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Depth privileged scene recognition via dual attention hallucination},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). ACP++: Action co-occurrence priors for human-object
interaction detection. <em>TIP</em>, <em>30</em>, 9150–9163. (<a
href="https://doi.org/10.1109/TIP.2021.3113563">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A common problem in the task of human-object interaction (HOI) detection is that numerous HOI classes have only a small number of labeled examples, resulting in training sets with a long-tailed distribution. The lack of positive labels can lead to low classification accuracy for these classes. Towards addressing this issue, we observe that there exist natural correlations and anti-correlations among human-object interactions. In this paper, we model the correlations as action co-occurrence matrices and present techniques to learn these priors and leverage them for more effective training, especially on rare classes. The efficacy of our approach is demonstrated experimentally, where the performance of our approach consistently improves over the state-of-the-art methods on both of the two leading HOI detection benchmark datasets, HICO-Det and V-COCO.},
  archive      = {J_TIP},
  author       = {Dong-Jin Kim and Xiao Sun and Jinsoo Choi and Stephen Lin and In So Kweon},
  doi          = {10.1109/TIP.2021.3113563},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {9150-9163},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {ACP++: Action co-occurrence priors for human-object interaction detection},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Non-homogeneous haze removal via artificial scene prior and
bidimensional graph reasoning. <em>TIP</em>, <em>30</em>, 9136–9149. (<a
href="https://doi.org/10.1109/TIP.2021.3122806">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the lack of natural scene and haze prior information, it is greatly challenging to completely remove the haze from a single image without distorting its visual content. Fortunately, the real-world haze usually presents non-homogeneous distribution, which provides us with many valuable clues in partial well-preserved regions. In this paper, we propose a Non-Homogeneous Haze Removal Network (NHRN) via artificial scene prior and bidimensional graph reasoning. Firstly, we employ the gamma correction iteratively to simulate artificial multiple shots under different exposure conditions, whose haze degrees are different and enrich the underlying scene prior. Secondly, beyond utilizing the local neighboring relationship, we build a bidimensional graph reasoning module to conduct non-local filtering in the spatial and channel dimensions of feature maps, which models their long-range dependency and propagates the natural scene prior between the well-preserved nodes and the nodes contaminated by haze. To the best of our knowledge, this is the first exploration to remove non-homogeneous haze via the graph reasoning based framework. We evaluate our method on different benchmark datasets. The results demonstrate that our method achieves superior performance over many state-of-the-art algorithms for both the single image dehazing and hazy image understanding tasks. The source code of the proposed NHRN is available on https://github.com/whrws/NHRNet .},
  archive      = {J_TIP},
  author       = {Haoran Wei and Qingbo Wu and Hui Li and King Ngi Ngan and Hongliang Li and Fanman Meng and Linfeng Xu},
  doi          = {10.1109/TIP.2021.3122806},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {9136-9149},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Non-homogeneous haze removal via artificial scene prior and bidimensional graph reasoning},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Semi-supervised dual relation learning for multi-label
classification. <em>TIP</em>, <em>30</em>, 9125–9135. (<a
href="https://doi.org/10.1109/TIP.2021.3122003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a real-world scenario, an object could contain multiple tags instead of a single categorical label. To this end, multi-label learning (MLL) emerged. In MLL, the feature distributions are long-tailed and the complex semantic label relation and the long-tailed training samples are the main challenges. Semi-supervised learning is a potential solution. While, existing methods are mainly designed for single class scenario while ignoring the latent label relations. In addition, they cannot well handle the distribution shift commonly existing across source and target domains. To this end, a Semi-supervised Dual Relation Learning (SDRL) framework for multi-label classification is proposed. SDRL utilizes a few labeled samples as well as large scale unlabeled samples in the training stage. It jointly explores the inter-instance feature-level relation and the intra-instance label-level relation even from the unlabeled samples. In our model, a dual-classifier structure is deployed to obtain domain invariant representations. The prediction results from the classifiers are further compared and the most confident predictions are extracted as pseudo labels. A trainable label relation tensor is designed to explicitly explore the pairwise latent label relations and refine the predicted labels. SDRL is able to effectively and efficiently explore the feature-label relation as well as the label-label relation knowledge without any extra semantic knowledge. We evaluated SDRL in general and zero-shot multi-label classification tasks and we concluded that SDRL is superior to other SOTA baselines. Furthermore, extensive ablation studies have been done which reveal the effectiveness of each component in our framework.},
  archive      = {J_TIP},
  author       = {Lichen Wang and Yunyu Liu and Hang Di and Can Qin and Gan Sun and Yun Fu},
  doi          = {10.1109/TIP.2021.3122003},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {9125-9135},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Semi-supervised dual relation learning for multi-label classification},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Texture memory-augmented deep patch-based image inpainting.
<em>TIP</em>, <em>30</em>, 9112–9124. (<a
href="https://doi.org/10.1109/TIP.2021.3122930">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Patch-based methods and deep networks have been employed to tackle image inpainting problem, with their own strengths and weaknesses. Patch-based methods are capable of restoring a missing region with high-quality texture through searching nearest neighbor patches from the unmasked regions. However, these methods bring problematic contents when recovering large missing regions. Deep networks, on the other hand, show promising results in completing large regions. Nonetheless, the results often lack faithful and sharp details that resemble the surrounding area. By bringing together the best of both paradigms, we propose a new deep inpainting framework where texture generation is guided by a texture memory of patch samples extracted from unmasked regions. The framework has a novel design that allows texture memory retrieval to be trained end-to-end with the deep inpainting network. In addition, we introduce a patch distribution loss to encourage high-quality patch synthesis. The proposed method shows superior performance both qualitatively and quantitatively on three challenging image benchmarks, i.e., Places, CelebA-HQ, and Paris Street-View datasets (Code will be made publicly available in https://github.com/open-mmlab/mmediting ).},
  archive      = {J_TIP},
  author       = {Rui Xu and Minghao Guo and Jiaqi Wang and Xiaoxiao Li and Bolei Zhou and Chen Change Loy},
  doi          = {10.1109/TIP.2021.3122930},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {9112-9124},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Texture memory-augmented deep patch-based image inpainting},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Parallel residual bi-fusion feature pyramid network for
accurate single-shot object detection. <em>TIP</em>, <em>30</em>,
9099–9111. (<a href="https://doi.org/10.1109/TIP.2021.3118953">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes the Parallel Residual Bi-Fusion Feature Pyramid Network (PRB-FPN) for fast and accurate single-shot object detection. Feature Pyramid (FP) is widely used in recent visual detection, however the top-down pathway of FP cannot preserve accurate localization due to pooling shifting. The advantage of FP is weakened as deeper backbones with more layers are used. In addition, it cannot keep up accurate detection of both small and large objects at the same time. To address these issues, we propose a new parallel FP structure with bi-directional (top-down and bottom-up) fusion and associated improvements to retain high-quality features for accurate localization. We provide the following design improvements: 1) parallel bifusion FP structure with a bottom-up fusion module (BFM) to detect both small and large objects at once with high accuracy; 2) concatenation and re-organization (CORE) module provides a bottom-up pathway for feature fusion, which leads to the bi-directional fusion FP that can recover lost information from lower-layer feature maps; 3) CORE feature is further purified to retain richer contextual information. Such CORE purification in both top-down and bottom-up pathways can be finished in only a few iterations; 4) adding of a residual design to CORE leads to a new Re-CORE module that enables easy training and integration with a wide range of deeper or lighter backbones. The proposed network achieves state-of-the-art performance on the UAVDT17 and MS COCO datasets.},
  archive      = {J_TIP},
  author       = {Ping-Yang Chen and Ming-Ching Chang and Jun-Wei Hsieh and Yong-Sheng Chen},
  doi          = {10.1109/TIP.2021.3118953},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {9099-9111},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Parallel residual bi-fusion feature pyramid network for accurate single-shot object detection},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Night-time scene parsing with a large real dataset.
<em>TIP</em>, <em>30</em>, 9085–9098. (<a
href="https://doi.org/10.1109/TIP.2021.3122004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although huge progress has been made on scene analysis in recent years, most existing works assume the input images to be in day-time with good lighting conditions. In this work, we aim to address the night-time scene parsing (NTSP) problem, which has two main challenges: 1) labeled night-time data are scarce, and 2) over- and under-exposures may co-occur in the input night-time images and are not explicitly modeled in existing pipelines. To tackle the scarcity of night-time data, we collect a novel labeled dataset, named NightCity , of 4,297 real night-time images with ground truth pixel-level semantic annotations. To our knowledge, NightCity is the largest dataset for NTSP. In addition, we also propose an exposure-aware framework to address the NTSP problem through augmenting the segmentation process with explicitly learned exposure features. Extensive experiments show that training on NightCity can significantly improve NTSP performances and that our exposure-aware model outperforms the state-of-the-art methods, yielding top performances on our dataset as well as existing datasets.},
  archive      = {J_TIP},
  author       = {Xin Tan and Ke Xu and Ying Cao and Yiheng Zhang and Lizhuang Ma and Rynson W. H. Lau},
  doi          = {10.1109/TIP.2021.3122004},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {9085-9098},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Night-time scene parsing with a large real dataset},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-target pan-class intrinsic relevance driven model for
improving semantic segmentation in autonomous driving. <em>TIP</em>,
<em>30</em>, 9069–9084. (<a
href="https://doi.org/10.1109/TIP.2021.3122293">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {At present, most semantic segmentation models rely on the excellent feature extraction capabilities of a deep learning network structure. Although these models can achieve excellent performance on multiple datasets, ways of refining the target main body segmentation and overcoming the performance limitation of deep learning networks are still a research focus. We discovered a pan-class intrinsic relevance phenomenon among targets that can link the targets cross-class. This cross-class strategy is different from the latest semantic segmentation model via context where targets are divided into an intra-class and inter-class. This paper proposes a model for refining the target main body segmentation using multi-target pan-class intrinsic relevance. The main contributions of the proposed model can be summarized as follows: a) The multi-target pan-class intrinsic relevance prior knowledge establishment (RPK-Est) module builds the prior knowledge of the intrinsic relevance to lay the foundation for the following extraction of the pan-class intrinsic relevance feature. b) The multi-target pan-class intrinsic relevance feature extraction (RF-Ext) module is designed to extract the pan-class intrinsic relevance feature based on the proposed multi-target node graph and graph convolution network. c) The multi-target pan-class intrinsic relevance feature integration (RF-Int) module is proposed to integrate the intrinsic relevance features and semantic features by a generative adversarial learning strategy at the gradient level, which can make intrinsic relevance features play a role in semantic segmentation. The proposed model achieved outstanding performance in semantic segmentation testing on four authoritative datasets compared to other state-of-the-art models.},
  archive      = {J_TIP},
  author       = {Yingfeng Cai and Lei Dai and Hai Wang and Zhixiong Li},
  doi          = {10.1109/TIP.2021.3122293},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {9069-9084},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Multi-target pan-class intrinsic relevance driven model for improving semantic segmentation in autonomous driving},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Meta-knowledge learning and domain adaptation for unseen
background subtraction. <em>TIP</em>, <em>30</em>, 9058–9068. (<a
href="https://doi.org/10.1109/TIP.2021.3122102">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background subtraction is a classic video processing task pervading in numerous visual applications such as video surveillance and traffic monitoring. Given the diversity and variability of real application scenes, an ideal background subtraction model should be robust to various scenarios. Even though deep-learning approaches have demonstrated unprecedented improvements, they often fail to generalize to unseen scenarios, thereby less suitable for extensive deployment. In this work, we propose to tackle cross-scene background subtraction via a two-phase framework that includes meta-knowledge learning and domain adaptation. Specifically, as we observe that meta-knowledge (i.e., scene-independent common knowledge) is the cornerstone for generalizing to unseen scenes, we draw on traditional frame differencing algorithms and design a deep difference network (DDN) to encode meta-knowledge especially temporal change knowledge from various cross-scene data (source domain) without intermittent foreground motion pattern. In addition, we explore a self-training domain adaptation strategy based on iterative evolution. With iteratively updated pseudo-labels, the DDN is continuously fine-tuned and evolves progressively toward unseen scenes (target domain) in an unsupervised fashion. Our framework could be easily deployed on unseen scenes without relying on their annotations. As evidenced by our experiments on the CDnet2014 dataset, it brings a significant improvement to background subtraction. Our method has a favorable processing speed (70 fps) and outperforms the best unsupervised algorithm and top supervised algorithm designed for unseen scenes by 9\% and 3\%, respectively.},
  archive      = {J_TIP},
  author       = {Jin Zhang and Xi Zhang and Yanyan Zhang and Yexin Duan and Yang Li and Zhisong Pan},
  doi          = {10.1109/TIP.2021.3122102},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {9058-9068},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Meta-knowledge learning and domain adaptation for unseen background subtraction},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). IDRLP: Image dehazing using region line prior. <em>TIP</em>,
<em>30</em>, 9043–9057. (<a
href="https://doi.org/10.1109/TIP.2021.3122088">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, a novel and ultra-robust single image dehazing method called IDRLP is proposed. It is observed that when an image is divided into $n$ regions, with each region having a similar scene depth, the brightness of both the hazy image and its haze-free correspondence are positively related with the scene depth. Based on this observation, this work determines that the hazy input and its haze-free correspondence exhibit a quasi-linear relationship after performing this region segmentation, which is named as region line prior (RLP). By combining RLP and the atmospheric scattering model (ASM), a recovery formula (RF) can be easily obtained with only two unknown parameters, i.e., the slope of the linear function and the atmospheric light. A 2D joint optimization function considering two constraints is then designed to seek the solution of RF. Unlike other comparable works, this “joint optimization” strategy makes efficient use of the information across the entire image, leading to more accurate results with ultra-high robustness. Finally, a guided filter is introduced in RF to eliminate the adverse interference caused by the region segmentation. The proposed RLP and IDRLP are evaluated from various perspectives and compared with related state-of-the-art techniques. Extensive analysis verifies the superiority of IDRLP over state-of-the-art image dehazing techniques in terms of both the recovery quality and efficiency. A software release is available at https://sites.google.com/site/renwenqi888/ .},
  archive      = {J_TIP},
  author       = {Mingye Ju and Can Ding and Charles A. Guo and Wenqi Ren and Dacheng Tao},
  doi          = {10.1109/TIP.2021.3122088},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {9043-9057},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {IDRLP: Image dehazing using region line prior},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Rethinking the u-shape structure for salient object
detection. <em>TIP</em>, <em>30</em>, 9030–9042. (<a
href="https://doi.org/10.1109/TIP.2021.3122093">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The U-shape structure has shown its advantage in salient object detection for efficiently combining multi-scale features. However, most existing U-shape-based methods focused on improving the bottom-up and top-down pathways while ignoring the connections between them. This paper shows that we can achieve the cross-scale information interaction by centralizing these connections, hence obtaining semantically stronger and positionally more precise features. To inspire the newly proposed strategy’s potential, we further design a relative global calibration module that can simultaneously process multi-scale inputs without spatial interpolation. Our approach can aggregate features more effectively while introducing only a few additional parameters. Our approach can cooperate with various existing U-shape-based salient object detection methods by substituting the connections between the bottom-up and top-down pathways. Experimental results demonstrate that our proposed approach performs favorably against the previous state-of-the-arts on five widely used benchmarks with less computational complexity. The source code will be publicly available.},
  archive      = {J_TIP},
  author       = {Jiang-Jiang Liu and Zhi-Ang Liu and Pai Peng and Ming-Ming Cheng},
  doi          = {10.1109/TIP.2021.3122093},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {9030-9042},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Rethinking the U-shape structure for salient object detection},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A fully spiking hybrid neural network for energy-efficient
object detection. <em>TIP</em>, <em>30</em>, 9014–9029. (<a
href="https://doi.org/10.1109/TIP.2021.3122092">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a Fully Spiking Hybrid Neural Network (FSHNN) for energy-efficient and robust object detection in resource-constrained platforms. The network architecture is based on a Spiking Convolutional Neural Network using leaky-integrate-fire neuron models. The model combines unsupervised Spike Time-Dependent Plasticity (STDP) learning with back-propagation (STBP) learning methods and also uses Monte Carlo Dropout to get an estimate of the uncertainty error. FSHNN provides better accuracy compared to DNN based object detectors while being more energy-efficient. It also outperforms these object detectors, when subjected to noisy input data and less labeled training data with a lower uncertainty error.},
  archive      = {J_TIP},
  author       = {Biswadeep Chakraborty and Xueyuan She and Saibal Mukhopadhyay},
  doi          = {10.1109/TIP.2021.3122092},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {9014-9029},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A fully spiking hybrid neural network for energy-efficient object detection},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Spatial-angular attention network for light field
reconstruction. <em>TIP</em>, <em>30</em>, 8999–9013. (<a
href="https://doi.org/10.1109/TIP.2021.3122089">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Typical learning-based light field reconstruction methods demand in constructing a large receptive field by deepening their networks to capture correspondences between input views. In this paper, we propose a spatial-angular attention network to perceive non-local correspondences in the light field, and reconstruct high angular resolution light field in an end-to-end manner. Motivated by the non-local attention mechanism (Wang et al. , 2018; Zhang et al. , 2019), a spatial-angular attention module specifically for the high-dimensional light field data is introduced to compute the response of each query pixel from all the positions on the epipolar plane, and generate an attention map that captures correspondences along the angular dimension. Then a multi-scale reconstruction structure is proposed to efficiently implement the non-local attention in the low resolution feature space, while also preserving the high frequency components in the high-resolution feature space. Extensive experiments demonstrate the superior performance of the proposed spatial-angular attention network for reconstructing sparsely-sampled light fields with Non-Lambertian effects.},
  archive      = {J_TIP},
  author       = {Gaochang Wu and Yingqian Wang and Yebin Liu and Lu Fang and Tianyou Chai},
  doi          = {10.1109/TIP.2021.3122089},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8999-9013},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Spatial-angular attention network for light field reconstruction},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). No-reference physics-based quality assessment of
polarization images and its application to demosaicking. <em>TIP</em>,
<em>30</em>, 8983–8998. (<a
href="https://doi.org/10.1109/TIP.2021.3122085">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Assessing the quality of polarization images is of significance for recovering reliable polarization information. Widely used quality assessment methods including peak signal-to-noise ratio and structural similarity index require reference data that is usually not available in practice. We introduce a simple and effective physics-based quality assessment method for polarization images that does not require any reference. This metric, based on the self-consistency of redundant linear polarization measurements, can thus be used to evaluate the quality of polarization images degraded by noise, misalignment, or demosaicking errors even in the absence of ground-truth. Based on this new metric, we propose a novel processing algorithm that significantly improves demosaicking of division-of-focal-plane polarization images by enabling efficient fusion between demosaicking algorithms and edge-preserving image filtering. Experimental results obtained on public databases and homemade polarization images show the effectiveness of the proposed method.},
  archive      = {J_TIP},
  author       = {Ning Li and Benjamin Le Teurnier and Matthieu Boffety and François Goudail and Yongqiang Zhao and Quan Pan},
  doi          = {10.1109/TIP.2021.3122085},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8983-8998},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {No-reference physics-based quality assessment of polarization images and its application to demosaicking},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Light-DehazeNet: A novel lightweight CNN architecture for
single image dehazing. <em>TIP</em>, <em>30</em>, 8968–8982. (<a
href="https://doi.org/10.1109/TIP.2021.3116790">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the rapid development of artificial intelligence technology, industrial sectors are revolutionizing in automation, reliability, and robustness, thereby significantly increasing quality and productivity. Most of the surveillance and industrial sectors are monitored by visual sensor networks capturing different surrounding environment images. However, during tempestuous weather conditions, the visual quality of the images is reduced due to contaminated suspended atmospheric particles that affect the overall surveillance systems. To tackle these challenges, this article presents a computationally efficient lightweight convolutional neural network referred to as Light-DehazeNet (LD-Net) for the reconstruction of hazy images. Unlike other learning-based approaches, which separately measure the transmission map and the atmospheric light, our proposed LD-Net jointly estimates both the transmission map and the atmospheric light using a transformed atmospheric scattering model. Furthermore, a color visibility restoration method is proposed to evade the color distortion in the dehaze image. Finally, we conduct extensive experiments using synthetic and natural hazy images. The quantitative and qualitative evaluation on different benchmark hazy datasets verify the superiority of the proposed method over other state-of-the-art image dehazing techniques. Moreover, additional experimentation validates the applicability of the proposed method in the object detection tasks. Considering the lightweight architecture with minimal computational cost, the proposed system is encouraged to be incorporated as an integral part of the vision-based monitoring systems to improve the overall performance.},
  archive      = {J_TIP},
  author       = {Hayat Ullah and Khan Muhammad and Muhammad Irfan and Saeed Anwar and Muhammad Sajjad and Ali Shariq Imran and Victor Hugo C. de Albuquerque},
  doi          = {10.1109/TIP.2021.3116790},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8968-8982},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Light-DehazeNet: A novel lightweight CNN architecture for single image dehazing},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Progressive diversified augmentation for general robustness
of DNNs: A unified approach. <em>TIP</em>, <em>30</em>, 8955–8967. (<a
href="https://doi.org/10.1109/TIP.2021.3121150">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adversarial images are imperceptible perturbations to mislead deep neural networks (DNNs), which have attracted great attention in recent years. Although several defense strategies achieved encouraging robustness against adversarial samples, most of them still failed to consider the robustness on common corruptions (e.g. noise, blur, and weather/digital effects). To address this problem, we propose a simple yet effective method, named Progressive Diversified Augmentation (PDA), which improves the robustness of DNNs by progressively injecting diverse adversarial noises during training. In other words, DNNs trained with PDA achieve better general robustness against both adversarial attacks and common corruptions than other strategies. In addition, PDA also enjoys the advantages of spending less training time and keeping high standard accuracy on clean examples. Further, we theoretically prove that PDA can control the perturbation bound and guarantee better robustness. Extensive results on CIFAR-10, SVHN, ImageNet, CIFAR-10-C and ImageNet-C have demonstrated that PDA comprehensively outperforms its counterparts on the robustness against adversarial examples and common corruptions as well as clean images. More experiments on the frequency-based perturbations and visualized gradients further prove that PDA achieves general robustness and is more aligned with the human visual system.},
  archive      = {J_TIP},
  author       = {Hang Yu and Aishan Liu and Gengchao Li and Jichen Yang and Chongzhi Zhang},
  doi          = {10.1109/TIP.2021.3121150},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8955-8967},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Progressive diversified augmentation for general robustness of DNNs: A unified approach},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SSSIC: Semantics-to-signal scalable image coding with
learned structural representations. <em>TIP</em>, <em>30</em>,
8939–8954. (<a href="https://doi.org/10.1109/TIP.2021.3121131">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We address the requirement of image coding for joint human-machine vision, i.e., the decoded image serves both human observation and machine analysis/understanding. Previously, human vision and machine vision have been extensively studied by image (signal) compression and (image) feature compression, respectively. Recently, for joint human-machine vision, several studies have been devoted to joint compression of images and features, but the correlation between images and features is still unclear. We identify the deep network as a powerful toolkit for generating structural image representations. From the perspective of information theory, the deep features of an image naturally form an entropy decreasing series: a scalable bitstream is achieved by compressing the features backward from a deeper layer to a shallower layer until culminating with the image signal. Moreover, we can obtain learned representations by training the deep network for a given semantic analysis task or multiple tasks and acquire deep features that are related to semantics. With the learned structural representations, we propose SSSIC, a framework to obtain an embedded bitstream that can be either partially decoded for semantic analysis or fully decoded for human vision. We implement an exemplar SSSIC scheme using coarse-to-fine image classification as the driven semantic analysis task. We also extend the scheme for object detection and instance segmentation tasks. The experimental results demonstrate the effectiveness of the proposed SSSIC framework and establish that the exemplar scheme achieves higher compression efficiency than separate compression of images and features.},
  archive      = {J_TIP},
  author       = {Ning Yan and Changsheng Gao and Dong Liu and Houqiang Li and Li Li and Feng Wu},
  doi          = {10.1109/TIP.2021.3121131},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8939-8954},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {SSSIC: Semantics-to-signal scalable image coding with learned structural representations},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-branch tensor network structure for tensor-train
discriminant analysis. <em>TIP</em>, <em>30</em>, 8926–8938. (<a
href="https://doi.org/10.1109/TIP.2021.3120871">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Higher-order data with high dimensionality arise in a diverse set of application areas such as computer vision, video analytics and medical imaging. Tensors provide a natural tool for representing these types of data. Two major challenges that confound current tensor based supervised learning algorithms are storage complexity and computational efficiency. In this paper, we address these problems by introducing a multi-branch tensor network structure. The multi-branch structure is a general tensor decomposition that includes Tucker and tensor-train (TT) as special cases and takes advantage of the flexibility of the tensor network to provide a better balance between storage and computational complexity. We then introduce a supervised discriminative tensor-train subspace learning approach referred to as tensor-train discriminant analysis (TTDA), and its implementations using the multi-branch tensor network structure. Multi-branch implementations of TTDA are shown to achieve lower storage and computational complexity while providing improved classification performance with respect to both Tucker and TT based supervised learning methods.},
  archive      = {J_TIP},
  author       = {Seyyid Emre Sofuoglu and Selin Aviyente},
  doi          = {10.1109/TIP.2021.3120871},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8926-8938},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Multi-branch tensor network structure for tensor-train discriminant analysis},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep high-resolution representation learning for
cross-resolution person re-identification. <em>TIP</em>, <em>30</em>,
8913–8925. (<a href="https://doi.org/10.1109/TIP.2021.3120054">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person re-identification (re-ID) tackles the problem of matching person images with the same identity from different cameras. In practical applications, due to the differences in camera performance and distance between cameras and persons of interest, captured person images usually have various resolutions. This problem, named Cross-Resolution Person Re-identification, presents a great challenge for the accurate person matching. In this paper, we propose a Deep High-Resolution Pseudo-Siamese Framework (PS-HRNet) to solve the above problem. Specifically, we first improve the VDSR by introducing existing channel attention (CA) mechanism and harvest a new module, i.e., VDSR-CA, to restore the resolution of low-resolution images and make full use of the different channel information of feature maps. Then we reform the HRNet by designing a novel representation head, HRNet-ReID, to extract discriminating features. In addition, a pseudo-siamese framework is developed to reduce the difference of feature distributions between low-resolution images and high-resolution images. The experimental results on five cross-resolution person datasets verify the effectiveness of our proposed approach. Compared with the state-of-the-art methods, the proposed PS-HRNet improves the Rank-1 accuracy by 3.4\%, 6.2\%, 2.5\%,1.1\% and 4.2\% on MLR-Market-1501, MLR-CUHK03, MLR-VIPeR, MLR-DukeMTMC-reID, and CAVIAR datasets, respectively, which demonstrates the superiority of our method in handling the Cross-Resolution Person Re-ID task. Our code is available at https://github.com/zhguoqing .},
  archive      = {J_TIP},
  author       = {Guoqing Zhang and Yu Ge and Zhicheng Dong and Hao Wang and Yuhui Zheng and Shengyong Chen},
  doi          = {10.1109/TIP.2021.3120054},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8913-8925},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Deep high-resolution representation learning for cross-resolution person re-identification},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptive LiDAR sampling and depth completion using ensemble
variance. <em>TIP</em>, <em>30</em>, 8900–8912. (<a
href="https://doi.org/10.1109/TIP.2021.3120042">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work considers the problem of depth completion, with or without image data, where an algorithm may measure the depth of a prescribed limited number of pixels. The algorithmic challenge is to choose pixel positions strategically and dynamically to maximally reduce overall depth estimation error. This setting is realized in daytime or nighttime depth completion for autonomous vehicles with a programmable LiDAR. Our method uses an ensemble of predictors to define a sampling probability over pixels. This probability is proportional to the variance of the predictions of ensemble members, thus highlighting pixels that are difficult to predict. By additionally proceeding in several prediction phases, we effectively reduce redundant sampling of similar pixels. Our ensemble-based method may be implemented using any depth-completion learning algorithm, such as a state-of-the-art neural network, treated as a black box. In particular, we also present a simple and effective Random Forest-based algorithm, and similarly use its internal ensemble in our design. We conduct experiments on the KITTI dataset, using the neural network algorithm of Ma et al. and our Random Forest-based learner for implementing our method. The accuracy of both implementations exceeds the state of the art. Compared with a random or grid sampling pattern, our method allows a reduction by a factor of 4–10 in the number of measurements required to attain the same accuracy.},
  archive      = {J_TIP},
  author       = {Eyal Gofer and Shachar Praisler and Guy Gilboa},
  doi          = {10.1109/TIP.2021.3120042},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8900-8912},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Adaptive LiDAR sampling and depth completion using ensemble variance},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Text-based localization of moments in a video corpus.
<em>TIP</em>, <em>30</em>, 8886–8899. (<a
href="https://doi.org/10.1109/TIP.2021.3120038">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prior works on text-based video moment localization focus on temporally grounding the textual query in an untrimmed video. These works assume that the relevant video is already known and attempt to localize the moment on that relevant video only. Different from such works, we relax this assumption and address the task of localizing moments in a corpus of videos for a given sentence query. This task poses a unique challenge as the system is required to perform: 2) retrieval of the relevant video where only a segment of the video corresponds with the queried sentence, 2) temporal localization of moment in the relevant video based on sentence query. Towards overcoming this challenge, we propose Hierarchical Moment Alignment Network (HMAN) which learns an effective joint embedding space for moments and sentences. In addition to learning subtle differences between intra-video moments, HMAN focuses on distinguishing inter-video global semantic concepts based on sentence queries. Qualitative and quantitative results on three benchmark text-based video moment retrieval datasets - Charades-STA, DiDeMo, and ActivityNet Captions - demonstrate that our method achieves promising performance on the proposed task of temporal localization of moments in a corpus of videos.},
  archive      = {J_TIP},
  author       = {Sudipta Paul and Niluthpol Chowdhury Mithun and Amit K. Roy-Chowdhury},
  doi          = {10.1109/TIP.2021.3120038},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8886-8899},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Text-based localization of moments in a video corpus},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Loop closure detection by using global and local features
with photometric and viewpoint invariance. <em>TIP</em>, <em>30</em>,
8873–8885. (<a href="https://doi.org/10.1109/TIP.2021.3116898">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Loop closure detection plays an important role in many Simultaneous Localization and Mapping (SLAM) systems, while the main challenge lies in the photometric and viewpoint variance. This paper presents a novel loop closure detection algorithm that is more robust to the variance by using both global and local features. Specifically, the global feature with the consolidation of photometric and viewpoint invariance is learned by a Siamese Network from the intensity, depth, gradient and normal vectors distribution. The local feature with rotation invariance is based on the histogram of relative pixel intensity and geometric information like curvature and coplanarity. Then, these two types of features are jointly leveraged for the robust detection of loop closures. The extensive experiments have been conducted on the publicly available RGB-D benchmark datasets like TUM and KITTI. The results demonstrate that our algorithm can effectively address challenging scenarios with large photometric and viewpoint variance, which outperforms other state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Mingfei Yu and Lei Zhang and Wufan Wang and Hua Huang},
  doi          = {10.1109/TIP.2021.3116898},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8873-8885},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Loop closure detection by using global and local features with photometric and viewpoint invariance},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Attend to the difference: Cross-modality person
re-identification via contrastive correlation. <em>TIP</em>,
<em>30</em>, 8861–8872. (<a
href="https://doi.org/10.1109/TIP.2021.3120881">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of cross-modality person re-identification has been receiving increasing attention recently, due to its practical significance. Motivated by the fact that human usually attend to the difference when they compare two similar objects, we propose a dual-path cross-modality feature learning framework which preserves intrinsic spatial structures and attends to the difference of input cross-modality image pairs. Our framework is composed by two main components: a Dual-path Spatial-structure-preserving Common Space Network (DSCSN) and a Contrastive Correlation Network (CCN). The former embeds cross-modality images into a common 3D tensor space without losing spatial structures, while the latter extracts contrastive features by dynamically comparing input image pairs. Note that the representations generated for the input RGB and Infrared images are mutually dependant to each other. We conduct extensive experiments on two public available RGB-IR ReID datasets, SYSU-MM01 and RegDB, and our proposed method outperforms state-of-the-art algorithms by a large margin with both full and simplified evaluation modes.},
  archive      = {J_TIP},
  author       = {Shizhou Zhang and Yifei Yang and Peng Wang and Guoqiang Liang and Xiuwei Zhang and Yanning Zhang},
  doi          = {10.1109/TIP.2021.3120881},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8861-8872},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Attend to the difference: Cross-modality person re-identification via contrastive correlation},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). GPU-based supervoxel generation with a novel anisotropic
metric. <em>TIP</em>, <em>30</em>, 8847–8860. (<a
href="https://doi.org/10.1109/TIP.2021.3120878">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video over-segmentation into supervoxels is an important pre-processing technique for many computer vision tasks. Videos are an order of magnitude larger than images. Most existing methods for generating supervovels are either memory- or time-inefficient, which limits their application in subsequent video processing tasks. In this paper, we present an anisotropic supervoxel method, which is memory-efficient and can be executed on the graphics processing unit (GPU). Therefore, our algorithm achieves good balance among segmentation quality, memory usage and processing time. In order to provide accurate segmentation for moving objects in video, we use the optical flow information to design a brand new non-Euclidean metric to calculate the anisotropic distances between seeds and voxels. To efficiently compute the anisotropic metric, we adjust the classic jump flooding algorithm (which is designed for parallel execution on the GPU) to generate anisotropic Voronoi tessellation in the combined color and spatio-temporal space. We evaluate our method and the representative supervoxel algorithms for their capability on segmentation performance, computation speed and memory efficiency. We also apply supervoxel results to the application of foreground propagation in videos to test the performance on solving practical problems. Experiments show that our algorithm is much faster than the existing methods, and achieves good balance on segmentation quality and efficiency.},
  archive      = {J_TIP},
  author       = {Xiao Dong and Zhonggui Chen and Yong-Jin Liu and Junfeng Yao and Xiaohu Guo},
  doi          = {10.1109/TIP.2021.3120878},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8847-8860},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {GPU-based supervoxel generation with a novel anisotropic metric},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). High frequency detail accentuation in CNN image restoration.
<em>TIP</em>, <em>30</em>, 8836–8846. (<a
href="https://doi.org/10.1109/TIP.2021.3120678">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given its nature of statistical inference, machine learning methods incline to downplay relatively rare events. But in many applications statistical outliers carry disproportional significance; they can, if being left without special treatment as of now, cause CNNs to perform unsatisfactorily on instances of interests. This is the reason why existing CNN image restoration methods all suffer from the problem of blurred details. To overcome this weakness, we advocate a new training methodology to sensitize the CNNs to desired events even they are atypical. Specifically for image restoration, we propose a so-called high frequency feature accentuation space that promotes image sharpness and clarity by maximally discriminating the ground truth image and the CNN-restored image in atypical but semantically important features. Then we force the restored image to agree with the ground truth image in the feature accentuation space by including an auxiliary loss term in the training process. This aims at a high degree of agreement of the two images on high frequency constructs such as sharp edges and fine textures, i.e., penalizes image blurs. The new CNN design method is implemented and tested for tasks of image super-resolution and denoising. Experimental results demonstrate the achievement of our design objective.},
  archive      = {J_TIP},
  author       = {Seyed Mehdi Ayyoubzadeh and Xiaolin Wu},
  doi          = {10.1109/TIP.2021.3120678},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8836-8846},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {High frequency detail accentuation in CNN image restoration},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Superpixel-guided discriminative low-rank representation of
hyperspectral images for classification. <em>TIP</em>, <em>30</em>,
8823–8835. (<a href="https://doi.org/10.1109/TIP.2021.3120675">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel classification scheme for the remotely sensed hyperspectral image (HSI), namely SP-DLRR, by comprehensively exploring its unique characteristics, including the local spatial information and low-rankness. SP-DLRR is mainly composed of two modules, i.e., the classification-guided superpixel segmentation and the discriminative low-rank representation, which are iteratively conducted. Specifically, by utilizing the local spatial information and incorporating the predictions from a typical classifier, the first module segments pixels of an input HSI (or its restoration generated by the second module) into superpixels. According to the resulting superpixels, the pixels of the input HSI are then grouped into clusters and fed into our novel discriminative low-rank representation model with an effective numerical solution. Such a model is capable of increasing the intra-class similarity by suppressing the spectral variations locally while promoting the inter-class discriminability globally, leading to a restored HSI with more discriminative pixels. Experimental results on three benchmark datasets demonstrate the significant superiority of SP-DLRR over state-of-the-art methods, especially for the case with an extremely limited number of training pixels.},
  archive      = {J_TIP},
  author       = {Shujun Yang and Junhui Hou and Yuheng Jia and Shaohui Mei and Qian Du},
  doi          = {10.1109/TIP.2021.3120675},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8823-8835},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Superpixel-guided discriminative low-rank representation of hyperspectral images for classification},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-scale spatial attention-guided monocular depth
estimation with semantic enhancement. <em>TIP</em>, <em>30</em>,
8811–8822. (<a href="https://doi.org/10.1109/TIP.2021.3120670">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Depth estimation from single monocular image is a vital but challenging task in 3D vision and scene understanding. Previous unsupervised methods have yielded impressive results, but the predicted depth maps still have several disadvantages such as missing small objects and object edge blurring. To address these problems, a multi-scale spatial attention guided monocular depth estimation method with semantic enhancement is proposed. Specifically, we first construct a multi-scale spatial attention-guided block based on atrous spatial pyramid pooling and spatial attention. Then, the correlation between the left and right views is fully explored by mutual information to obtain a more robust feature representation. Finally, we design a double-path prediction network to simultaneously generate depth maps and semantic labels. The proposed multi-scale spatial attention-guided block can focus more on the objects, especially on small objects. Moreover, the additional semantic information also enables the objects edge in the predicted depth maps more sharper. We conduct comprehensive evaluations on public benchmark datasets, such as KITTI and Make3D. The experiment results well demonstrate the effectiveness of the proposed method and achieve better performance than other self-supervised methods.},
  archive      = {J_TIP},
  author       = {Xianfa Xu and Zhe Chen and Fuliang Yin},
  doi          = {10.1109/TIP.2021.3120670},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8811-8822},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Multi-scale spatial attention-guided monocular depth estimation with semantic enhancement},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Controllable sketch-to-image translation for robust face
synthesis. <em>TIP</em>, <em>30</em>, 8797–8810. (<a
href="https://doi.org/10.1109/TIP.2021.3120669">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel controllable sketch-to-image translation framework that allows users to interactively and robustly synthesize and edit face images with hand-drawn sketches. Inspired by the coarse-to-fine painting process of human artists, we propose a novel dilation-based sketch refinement method to refine sketches at varied coarse levels without the need for real sketch training data. We further investigate multi-level refinement that enables users to flexibly define how “reliable” the input sketch should be considered for the final output through a refinement level control parameter, which helps balance between the realism of the output and its structural consistency with the input sketch. It is realized by leveraging scale-aware style transfer to model and adjust the style features of sketches at different coarse levels. Moreover, advanced user controllability in terms of the editing region control, facial attribute editing, and spatially non-uniform refinement is further explored for fine-grained and semantic editing. We demonstrate the effectiveness of the proposed method in terms of visual quality and user controllability through extensive experiments including qualitative and quantitative comparison with state-of-the-art methods, ablation studies and various applications.},
  archive      = {J_TIP},
  author       = {Shuai Yang and Zhangyang Wang and Jiaying Liu and Zongming Guo},
  doi          = {10.1109/TIP.2021.3120669},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8797-8810},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Controllable sketch-to-image translation for robust face synthesis},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning to rank proposals for siamese visual tracking.
<em>TIP</em>, <em>30</em>, 8785–8796. (<a
href="https://doi.org/10.1109/TIP.2021.3120305">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, Siamese network based trackers with region proposal networks(RPN) decompose the visual tracking task into classification and regression, and have drawn much attention. However, previous Siamese trackers process all the training samples equally to learn the desired network, and only take the classification scores of proposals to locate the tracked target at the inference stage. To address the above issues, we propose a simple, yet effective strategy to rank the importance of training samples, and pay more attention to the important samples, which can facilitate the classification optimization. Moreover, we propose a lightweight ranking network to generate the ranking scores for proposals. Higher scores are assigned to proposals whose Intersection over Union(IoU) with the ground-truth are larger. The combination of classification and ranking scores serves as a new proposal selection criterion for online tracking, and can boost the tracking performance significantly. Our proposed method could be easily integrated into existing RPN-based Siamese networks in an end-to-end fashion. Extensive experiments are conducted on 10 tracking benchmarks, including NFS, UAV123, OTB2015, Temple-Color, VOT2016, VOT2017, VOT2019, TrackingNet, GOT-10K and LaSOT. The proposed method achieves a state-of-the-art tracking accuracy with a real-time speed.},
  archive      = {J_TIP},
  author       = {Feng Tang and Qiang Ling},
  doi          = {10.1109/TIP.2021.3120305},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8785-8796},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Learning to rank proposals for siamese visual tracking},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A deep learning-based model that reduces speed of sound
aberrations for improved in vivo photoacoustic imaging. <em>TIP</em>,
<em>30</em>, 8773–8784. (<a
href="https://doi.org/10.1109/TIP.2021.3120053">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Photoacoustic imaging (PAI) has attracted great attention as a medical imaging method. Typically, photoacoustic (PA) images are reconstructed via beamforming, but many factors still hinder the beamforming techniques in reconstructing optimal images in terms of image resolution, imaging depth, or processing speed. Here, we demonstrate a novel deep learning PAI that uses multiple speed of sound (SoS) inputs. With this novel method, we achieved SoS aberration mitigation, streak artifact removal, and temporal resolution improvement all at once in structural and functional in vivo PA images of healthy human limbs and melanoma patients. The presented method produces high-contrast PA images in vivo with reduced distortion, even in adverse conditions where the medium is heterogeneous and/or the data sampling is sparse. Thus, we believe that this new method can achieve high image quality with fast data acquisition and can contribute to the advance of clinical PAI.},
  archive      = {J_TIP},
  author       = {Seungwan Jeon and Wonseok Choi and Byullee Park and Chulhong Kim},
  doi          = {10.1109/TIP.2021.3120053},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8773-8784},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A deep learning-based model that reduces speed of sound aberrations for improved in vivo photoacoustic imaging},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DotFAN: A domain-transferred face augmentation net.
<em>TIP</em>, <em>30</em>, 8759–8772. (<a
href="https://doi.org/10.1109/TIP.2021.3120313">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The performance of a convolutional neural network (CNN) based face recognition model largely relies on the richness of labeled training data. However, it is expensive to collect a training set with large variations of a face identity under different poses and illumination changes, so the diversity of within-class face images becomes a critical issue in practice. In this paper, we propose a 3D model-assisted domain-transferred face augmentation network (DotFAN) that can generate a series of variants of an input face based on the knowledge distilled from existing rich face datasets of other domains. Extending from StarGAN’s architecture, DotFAN integrates with two additional subnetworks, i.e., face expert model (FEM) and face shape regressor (FSR), for latent facial code control. While FSR aims to extract face attributes, FEM is designed to capture a face identity. With their aid, DotFAN can separately learn facial feature codes and effectively generate face images of various facial attributes while keeping the identity of augmented faces unaltered. Experiments show that DotFAN is beneficial for augmenting small face datasets to improve their within-class diversity so that a better face recognition model can be learned from the augmented dataset.},
  archive      = {J_TIP},
  author       = {Hao-Chiang Shao and Kang-Yu Liu and Weng-Tai Su and Chia-Wen Lin and Jiwen Lu},
  doi          = {10.1109/TIP.2021.3120313},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8759-8772},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {DotFAN: A domain-transferred face augmentation net},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). LR-net: Low-rank spatial-spectral network for hyperspectral
image denoising. <em>TIP</em>, <em>30</em>, 8743–8758. (<a
href="https://doi.org/10.1109/TIP.2021.3120037">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the physical limitations of the imaging devices, hyperspectral images (HSIs) are commonly distorted by a mixture of Gaussian noise, impulse noise, stripes, and dead lines, leading to the decline in the performance of unmixing, classification, and other subsequent applications. In this paper, we propose a novel end-to-end low-rank spatial-spectral network (LR-Net) for the removal of the hybrid noise in HSIs. By integrating the low-rank physical property into a deep convolutional neural network (DCNN), the proposed LR-Net simultaneously enjoys the strong feature representation ability from DCNN and the implicit physical constraint of clean HSIs. Firstly, spatial-spectral atrous blocks (SSABs) are built to exploit spatial-spectral features of HSIs. Secondly, these spatial-spectral features are forwarded to a multi-atrous block (MAB) to aggregate the context in different receptive fields. Thirdly, the contextual features and spatial-spectral features from different levels are concatenated before being fed into a plug-and-play low-rank module (LRM) for feature reconstruction. With the help of the LRM, the workflow of low-rank matrix reconstruction can be streamlined in a differentiable manner. Finally, the low-rank features are utilized to capture the latent semantic relationships of the HSIs to recover clean HSIs. Extensive experiments on both simulated and real-world datasets were conducted. The experimental results show that the LR-Net outperforms other state-of-the-art denoising methods in terms of evaluation metrics and visual assessments. Particularly, through the collaborative integration of DCNNs and the low-rank property, the LR-Net shows strong stability and capacity for generalization.},
  archive      = {J_TIP},
  author       = {Hongyan Zhang and Hongyu Chen and Guangyi Yang and Liangpei Zhang},
  doi          = {10.1109/TIP.2021.3120037},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8743-8758},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {LR-net: Low-rank spatial-spectral network for hyperspectral image denoising},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bifurcated backbone strategy for RGB-d salient object
detection. <em>TIP</em>, <em>30</em>, 8727–8742. (<a
href="https://doi.org/10.1109/TIP.2021.3116793">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-level feature fusion is a fundamental topic in computer vision. It has been exploited to detect, segment and classify objects at various scales. When multi-level features meet multi-modal cues, the optimal feature aggregation and multi-modal learning strategy become a hot potato. In this paper, we leverage the inherent multi-modal and multi-level nature of RGB-D salient object detection to devise a novel Bifurcated Backbone Strategy Network ( BBS-Net ). Our architecture, is simple, efficient, and backbone-independent. In particular, first, we propose to regroup the multi-level features into teacher and student features using a bifurcated backbone strategy (BBS). Second, we introduce a depth-enhanced module (DEM) to excavate informative depth cues from the channel and spatial views. Then, RGB and depth modalities are fused in a complementary way. Extensive experiments show that BBS-Net significantly outperforms 18 state-of-the-art (SOTA) models on eight challenging datasets under five evaluation measures, demonstrating the superiority of our approach (~4\% improvement in S-measure $vs$ . the top-ranked model: DMRA). In addition, we provide a comprehensive analysis on the generalization ability of different RGB-D datasets and provide a powerful training set for future research. The complete algorithm, benchmark results, and post-processing toolbox are publicly available at https://github.com/zyjwuyan/BBS-Net .},
  archive      = {J_TIP},
  author       = {Yingjie Zhai and Deng-Ping Fan and Jufeng Yang and Ali Borji and Ling Shao and Junwei Han and Liang Wang},
  doi          = {10.1109/TIP.2021.3116793},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8727-8742},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Bifurcated backbone strategy for RGB-D salient object detection},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Simulation of atmospheric visibility impairment.
<em>TIP</em>, <em>30</em>, 8713–8726. (<a
href="https://doi.org/10.1109/TIP.2021.3120044">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Changes in aerosol composition and its proportions can cause changes in atmospheric visibility. Vision systems deployed outdoors must take into account the negative effects brought by visibility impairment. In order to develop vision algorithms that can adapt to low atmospheric visibility conditions, a large-scale dataset containing pairs of clear images and their visibility-impaired versions (along with other annotations if necessary) is usually indispensable. However, it is almost impossible to collect large amounts of such image pairs in a real physical environment. A natural and reasonable solution is to use virtual simulation technologies, which is also the focus of this paper. In this paper, we first deeply analyze the limitations and irrationalities of the existing work specializing on simulation of atmospheric visibility impairment. We point out that many simulation schemes actually even violate the assumptions of the Koschmieder’s law. Second, more importantly, based on a thorough investigation of the relevant studies in the field of atmospheric science, we present simulation strategies for five most commonly encountered visibility impairment phenomena, including mist, fog, natural haze, smog, and Asian dust. Our work establishes a direct link between the fields of atmospheric science and computer vision. In addition, as a byproduct, with the proposed simulation schemes, a large-scale synthetic dataset is established, comprising 40,000 clear source images and their 800,000 visibility-impaired versions. To make our work reproducible, source codes and the dataset have been released at https://cslinzhang.github.io/AVID/ .},
  archive      = {J_TIP},
  author       = {Lin Zhang and Anqi Zhu and Shiyu Zhao and Yicong Zhou},
  doi          = {10.1109/TIP.2021.3120044},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8713-8726},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Simulation of atmospheric visibility impairment},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Exploring spatial diversity for region-based active
learning. <em>TIP</em>, <em>30</em>, 8702–8712. (<a
href="https://doi.org/10.1109/TIP.2021.3120041">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {State-of-the-art methods for semantic segmentation are based on deep neural networks trained on large-scale labeled datasets. Acquiring such datasets would incur large annotation costs, especially for dense pixel-level prediction tasks like semantic segmentation. We consider region-based active learning as a strategy to reduce annotation costs while maintaining high performance. In this setting, batches of informative image regions instead of entire images are selected for labeling. Importantly, we propose that enforcing local spatial diversity is beneficial for active learning in this case, and to incorporate spatial diversity along with the traditional active selection criterion, e.g., data sample uncertainty, in a unified optimization framework for region-based active learning. We apply this framework to the Cityscapes and PASCAL VOC datasets and demonstrate that the inclusion of spatial diversity effectively improves the performance of uncertainty-based and feature diversity-based active learning methods. Our framework achieves 95\% performance of fully supervised methods with only 5 – 9\% of the labeled pixels, outperforming all state-of-the-art region-based active learning methods for semantic segmentation.},
  archive      = {J_TIP},
  author       = {Lile Cai and Xun Xu and Lining Zhang and Chuan-Sheng Foo},
  doi          = {10.1109/TIP.2021.3120041},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8702-8712},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Exploring spatial diversity for region-based active learning},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SOLVER: Scene-object interrelated visual emotion reasoning
network. <em>TIP</em>, <em>30</em>, 8686–8701. (<a
href="https://doi.org/10.1109/TIP.2021.3118983">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual Emotion Analysis (VEA) aims at finding out how people feel emotionally towards different visual stimuli, which has attracted great attention recently with the prevalence of sharing images on social networks. Since human emotion involves a highly complex and abstract cognitive process, it is difficult to infer visual emotions directly from holistic or regional features in affective images. It has been demonstrated in psychology that visual emotions are evoked by the interactions between objects as well as the interactions between objects and scenes within an image. Inspired by this, we propose a novel Scene-Object interreLated Visual Emotion Reasoning network (SOLVER) to predict emotions from images. To mine the emotional relationships between distinct objects, we first build up an Emotion Graph based on semantic concepts and visual features. Then, we conduct reasoning on the Emotion Graph using Graph Convolutional Network (GCN), yielding emotion-enhanced object features. We also design a Scene-Object Fusion Module to integrate scenes and objects, which exploits scene features to guide the fusion process of object features with the proposed scene-based attention mechanism. Extensive experiments and comparisons are conducted on eight public visual emotion datasets, and the results demonstrate that the proposed SOLVER consistently outperforms the state-of-the-art methods by a large margin. Ablation studies verify the effectiveness of our method and visualizations prove its interpretability, which also bring new insight to explore the mysteries in VEA. Notably, we further discuss SOLVER on three other potential datasets with extended experiments, where we validate the robustness of our method and notice some limitations of it.},
  archive      = {J_TIP},
  author       = {Jingyuan Yang and Xinbo Gao and Leida Li and Xiumei Wang and Jinshan Ding},
  doi          = {10.1109/TIP.2021.3118983},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8686-8701},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {SOLVER: Scene-object interrelated visual emotion reasoning network},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Self-attention context network: Addressing the threat of
adversarial attacks for hyperspectral image classification.
<em>TIP</em>, <em>30</em>, 8671–8685. (<a
href="https://doi.org/10.1109/TIP.2021.3118977">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning models have shown their great capability for the hyperspectral image (HSI) classification task in recent years. Nevertheless, their vulnerability towards adversarial attacks could not be neglected. In this study, we systematically analyze the influence of adversarial attacks on the HSI classification task for the first time. While existing research of adversarial attacks focuses on the generation of adversarial examples in the RGB domain, the experiments in this study show such adversarial examples could also exist in the hyperspectral domain. Although the difference between the generated adversarial image and the original hyperspectral data is imperceptible to the human visual system, most of the existing state-of-the-art deep learning models could be fooled by the adversarial image to make wrong predictions. To address this challenge, a novel self-attention context network (SACNet) is further proposed. We discover that the global context information contained in HSI can significantly improve the robustness of deep neural networks when confronted with adversarial attacks. Extensive experiments on three benchmark HSI datasets demonstrate that the proposed SACNet possesses stronger resistibility towards adversarial examples compared with the existing state-of-the-art deep learning models.},
  archive      = {J_TIP},
  author       = {Yonghao Xu and Bo Du and Liangpei Zhang},
  doi          = {10.1109/TIP.2021.3118977},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8671-8685},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Self-attention context network: Addressing the threat of adversarial attacks for hyperspectral image classification},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). F³A-GAN: Facial flow for face animation with generative
adversarial networks. <em>TIP</em>, <em>30</em>, 8658–8670. (<a
href="https://doi.org/10.1109/TIP.2021.3112059">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Formulated as a conditional generation problem, face animation aims at synthesizing continuous face images from a single source image driven by a set of conditional face motion. Previous works mainly model the face motion as conditions with 1D or 2D representation ( e.g., action units, emotion codes, landmark ), which often leads to low-quality results in some complicated scenarios such as continuous generation and large-pose transformation. To tackle this problem, the conditions are supposed to meet two requirements, i.e., motion information preserving and geometric continuity. To this end, we propose a novel representation based on a 3D geometric flow, termed facial flow, to represent the natural motion of the human face at any pose. Compared with other previous conditions, the proposed facial flow well controls the continuous changes to the face. After that, in order to utilize the facial flow for face editing, we build a synthesis framework generating continuous images with conditional facial flows. To fully take advantage of the motion information of facial flows, a hierarchical conditional framework is designed to combine the extracted multi-scale appearance features from images and motion features from flows in a hierarchical manner. The framework then decodes multiple fused features back to images progressively. Experimental results demonstrate the effectiveness of our method compared to other state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Xintian Wu and Qihang Zhang and Yiming Wu and Huanyu Wang and Songyuan Li and Lingyun Sun and Xi Li},
  doi          = {10.1109/TIP.2021.3112059},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8658-8670},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {F³A-GAN: Facial flow for face animation with generative adversarial networks},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MW-GAN: Multi-warping GAN for caricature generation with
multi-style geometric exaggeration. <em>TIP</em>, <em>30</em>,
8644–8657. (<a href="https://doi.org/10.1109/TIP.2021.3118984">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given an input face photo, the goal of caricature generation is to produce stylized, exaggerated caricatures that share the same identity as the photo. It requires simultaneous style transfer and shape exaggeration with rich diversity, and meanwhile preserving the identity of the input. To address this challenging problem, we propose a novel framework called Multi-Warping GAN (MW-GAN), including a style network and a geometric network that are designed to conduct style transfer and geometric exaggeration respectively. We bridge the gap between the style/landmark space and their corresponding latent code spaces by a dual way design, so as to generate caricatures with arbitrary styles and geometric exaggeration, which can be specified either through random sampling of latent code or from a given caricature sample. Besides, we apply identity preserving loss to both image space and landmark space, leading to a great improvement in quality of generated caricatures. Experiments show that caricatures generated by MW-GAN have better quality than existing methods.},
  archive      = {J_TIP},
  author       = {Haodi Hou and Jing Huo and Jing Wu and Yu-Kun Lai and Yang Gao},
  doi          = {10.1109/TIP.2021.3118984},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8644-8657},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {MW-GAN: Multi-warping GAN for caricature generation with multi-style geometric exaggeration},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Uncertainty learning for noise resistant sketch-based 3D
shape retrieval. <em>TIP</em>, <em>30</em>, 8632–8643. (<a
href="https://doi.org/10.1109/TIP.2021.3118979">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, sketch-based 3D shape retrieval has received growing attention in the community of computer graphics and computer vision. Most previous works focus on the problem of how to reduce the large cross-modality difference between 2D sketch and 3D shape data and make significant progress. Nevertheless, little attention has been paid to another important problem of how to deal with noise in the sketch data . For the first time, this work investigates the problem of noisy sketch data. It firstly provides qualitative and insightful analysis on the impact of noise, revealing that the noisy data are a key factor for unsatisfactory retrieval performance, as they cause severe over fitting and impair feature learning. Thus, the issue is worthy of serious treatment. Then, we propose to estimate sketch noise as data uncertainty, motivated by existing ideas that model data uncertainty with a distributional representation. We present methods with simple network structure and loss functions. They achieve strong results and establish new state-of-the-art on two benchmarks. Comprehensive experiment results, ablation studies, and insightful analysis validate the effectiveness of our methods, revealing that sketch feature learning with uncertainty is crucial for noise resistant sketch based 3D shape retrieval.},
  archive      = {J_TIP},
  author       = {Shuang Liang and Weidong Dai and Yichen Wei},
  doi          = {10.1109/TIP.2021.3118979},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8632-8643},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Uncertainty learning for noise resistant sketch-based 3D shape retrieval},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Supervised domain adaptation: A graph embedding perspective
and a rectified experimental protocol. <em>TIP</em>, <em>30</em>,
8619–8631. (<a href="https://doi.org/10.1109/TIP.2021.3118978">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain Adaptation is the process of alleviating distribution gaps between data from different domains. In this paper, we show that Domain Adaptation methods using pair-wise relationships between source and target domain data can be formulated as a Graph Embedding in which the domain labels are incorporated into the structure of the intrinsic and penalty graphs. Specifically, we analyse the loss functions of three existing state-of-the-art Supervised Domain Adaptation methods and demonstrate that they perform Graph Embedding. Moreover, we highlight some generalisation and reproducibility issues related to the experimental setup commonly used to demonstrate the few-shot learning capabilities of these methods. To assess and compare Supervised Domain Adaptation methods accurately, we propose a rectified evaluation protocol, and report updated benchmarks on the standard datasets Office31 (Amazon, DSLR, and Webcam), Digits (MNIST, USPS, SVHN, and MNIST-M) and VisDA (Synthetic, Real).},
  archive      = {J_TIP},
  author       = {Lukas Hedegaard and Omar Ali Sheikh-Omar and Alexandros Iosifidis},
  doi          = {10.1109/TIP.2021.3118978},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8619-8631},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Supervised domain adaptation: A graph embedding perspective and a rectified experimental protocol},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-view feature selection for PolSAR image classification
via l₂,₁ sparsity regularization and manifold regularization.
<em>TIP</em>, <em>30</em>, 8607–8618. (<a
href="https://doi.org/10.1109/TIP.2021.3118976">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature is a crucial element of polarimetric synthetic aperture radar (PolSAR) image classification. Multiple types of Features, such as polarimetric features (PF) generated from the PolSAR data and various polarimetric target decompositions, texture features (TF) of the Pauli color-coded PolSAR images are used as features for PolSAR image classification. The obtained PF and TF often form the high-dimensional data, which leads to high computational complexity. Moreover, some features are irrelative and do nothing to improve the classification performance. Therefore, it is fairly indispensable to select a subset of useful features for PolSAR image classification. This paper proposes a multi-view feature selection method for PolSAR image classification. Firstly, two types of features, PF and TF are generated separately. Then the optimization model is built to pursue the feature selection matrices. Specifically, in order to maintain the consistency of different types of features, we search for the common representation of multiple types of features in the optimization problem. The $l_{2,1}$ norm sparsity regularization is imposed on the feature selection matrices to achieve feature selection. In addition, the manifold regularization on the common representation is utilized to preserve the structure information of the data. The effectiveness of the proposed method is evaluated on three real PolSAR data sets. Experimental results demonstrate the superiority of the proposed method.},
  archive      = {J_TIP},
  author       = {Xiayuan Huang and Xiangli Nie},
  doi          = {10.1109/TIP.2021.3118976},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8607-8618},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Multi-view feature selection for PolSAR image classification via l₂,₁ sparsity regularization and manifold regularization},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Toward fine-grained sketch-based 3D shape retrieval.
<em>TIP</em>, <em>30</em>, 8595–8606. (<a
href="https://doi.org/10.1109/TIP.2021.3118975">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we study, for the first time, the problem of fine-grained sketch-based 3D shape retrieval. We advocate the use of sketches as a fine-grained input modality to retrieve 3D shapes at instance-level – e.g., given a sketch of a chair, we set out to retrieve a specific chair from a gallery of all chairs. Fine-grained sketch-based 3D shape retrieval (FG-SBSR) has not been possible till now due to a lack of datasets that exhibit one-to-one sketch-3D correspondences. The first key contribution of this paper is two new datasets, consisting a total of 4,680 sketch-3D pairings from two object categories. Even with the datasets, FG-SBSR is still highly challenging because (i) the inherent domain gap between 2D sketch and 3D shape is large, and (ii) retrieval needs to be conducted at the instance level instead of the coarse category level matching as in traditional SBSR. Thus, the second contribution of the paper is the first cross-modal deep embedding model for FG-SBSR, which specifically tackles the unique challenges presented by this new problem. Core to the deep embedding model is a novel cross-modal view attention module which automatically computes the optimal combination of 2D projections of a 3D shape given a query sketch.},
  archive      = {J_TIP},
  author       = {Anran Qi and Yulia Gryaditskaya and Jifei Song and Yongxin Yang and Yonggang Qi and Timothy M. Hospedales and Tao Xiang and Yi-Zhe Song},
  doi          = {10.1109/TIP.2021.3118975},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8595-8606},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Toward fine-grained sketch-based 3D shape retrieval},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Graph-theoretic post-processing of segmentation with
application to dense biofilms. <em>TIP</em>, <em>30</em>, 8580–8594. (<a
href="https://doi.org/10.1109/TIP.2021.3116792">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent deep learning methods have provided successful initial segmentation results for generalized cell segmentation in microscopy. However, for dense arrangements of small cells with limited ground truth for training, the deep learning methods produce both over-segmentation and under-segmentation errors. Post-processing attempts to balance the trade-off between the global goal of cell counting for instance segmentation, and local fidelity to the morphology of identified cells. The need for post-processing is especially evident for segmenting 3D bacterial cells in densely-packed communities called biofilms. A graph-based recursive clustering approach, m-LCuts , is proposed to automatically detect collinearly structured clusters and applied to post-process unsolved cells in 3D bacterial biofilm segmentation. Construction of outlier-removed graphs to extract the collinearity feature in the data adds additional novelty to m-LCuts . The superiority of m-LCuts is observed by the evaluation in cell counting with over 90\% of cells correctly identified, while a lower bound of 0.8 in terms of average single-cell segmentation accuracy is maintained. This proposed method does not need manual specification of the number of cells to be segmented. Furthermore, the broad adaptation for working on various applications, with the presence of data collinearity, also makes m-LCuts stand out from the other approaches.},
  archive      = {J_TIP},
  author       = {Jie Wang and Mingxing Zhang and Ji Zhang and Yibo Wang and Andreas Gahlmann and Scott T. Acton},
  doi          = {10.1109/TIP.2021.3116792},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8580-8594},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Graph-theoretic post-processing of segmentation with application to dense biofilms},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multitask identity-aware image steganography via minimax
optimization. <em>TIP</em>, <em>30</em>, 8567–8579. (<a
href="https://doi.org/10.1109/TIP.2021.3107999">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-capacity image steganography, aimed at concealing a secret image in a cover image, is a technique to preserve sensitive data, e.g., faces and fingerprints. Previous methods focus on the security during transmission and subsequently run a risk of privacy leakage after the restoration of secret images at the receiving end. To address this issue, we propose a framework, called Multitask Identity-Aware Image Steganography (MIAIS), to achieve direct recognition on container images without restoring secret images. The key issue of the direct recognition is to preserve identity information of secret images into container images and make container images look similar to cover images at the same time. Thus, we introduce a simple content loss to preserve the identity information, and design a minimax optimization to deal with the contradictory aspects. We demonstrate that the robustness results can be transferred across different cover datasets. In order to be flexible for the secret image restoration in some cases, we incorporate an optional restoration network into our method, providing a multitask framework. The experiments under the multitask scenario show the effectiveness of our framework compared with other visual information hiding methods and state-of-the-art high-capacity image steganography methods. The code is available at https://github.com/jiabaocui/MIAIS .},
  archive      = {J_TIP},
  author       = {Jiabao Cui and Pengyi Zhang and Songyuan Li and Liangli Zheng and Cuizhu Bao and Jupeng Xia and Xi Li},
  doi          = {10.1109/TIP.2021.3107999},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8567-8579},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Multitask identity-aware image steganography via minimax optimization},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Toward accurate pixelwise object tracking via attention
retrieval. <em>TIP</em>, <em>30</em>, 8553–8566. (<a
href="https://doi.org/10.1109/TIP.2021.3117077">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pixelwise single object tracking is challenging due to the competition of running speeds and segmentation accuracy. Current state-of-the-art real-time approaches seamlessly connect tracking and segmentation by sharing computation of the backbone network, e.g. , SiamMask and D3S fork a light branch from the tracking model to predict segmentation mask. Although efficient, directly reusing features from tracking networks may harm the segmentation accuracy, since background clutter in the backbone feature tends to introduce false positives in segmentation. To mitigate this problem, we propose a unified tracking-retrieval-segmentation framework consisting of an attention retrieval network (ARN) and an iterative feedback network (IFN). Instead of segmenting the target inside the bounding box, the proposed framework performs soft spatial constraints on backbone features to obtain an accurate global segmentation map. Concretely, in ARN, a look-up-table (LUT) is first built by sufficiently using the information of the first frame. By retrieving it, a target-aware attention map is generated to suppress the negative influence of background clutter. To ulteriorly refine the contour of the segmentation, IFN iteratively enhances the features at different resolutions by taking the predicted mask as feedback guidance. Our framework sets a new state of the art on the recent pixelwise tracking benchmark VOT2020 and runs at 40 fps. Notably, the proposed model surpasses SiamMask by 11.7/4.2/5.5 points on VOT2020, DAVIS2016, and DAVIS2017, respectively. Code is available at https://github.com/JudasDie/SOTS .},
  archive      = {J_TIP},
  author       = {Zhipeng Zhang and Yufan Liu and Bing Li and Weiming Hu and Houwen Peng},
  doi          = {10.1109/TIP.2021.3117077},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8553-8566},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Toward accurate pixelwise object tracking via attention retrieval},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). CoANet: Connectivity attention network for road extraction
from satellite imagery. <em>TIP</em>, <em>30</em>, 8540–8552. (<a
href="https://doi.org/10.1109/TIP.2021.3117076">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extracting roads from satellite imagery is a promising approach to update the dynamic changes of road networks efficiently and timely. However, it is challenging due to the occlusions caused by other objects and the complex traffic environment, the pixel-based methods often generate fragmented roads and fail to predict topological correctness. In this paper, motivated by the road shapes and connections in the graph network, we propose a connectivity attention network (CoANet) to jointly learn the segmentation and pair-wise dependencies. Since the strip convolution is more aligned with the shape of roads, which are long-span, narrow, and distributed continuously. We develop a strip convolution module (SCM) that leverages four strip convolutions to capture long-range context information from different directions and avoid interference from irrelevant regions. Besides, considering the occlusions in road regions caused by buildings and trees, a connectivity attention module (CoA) is proposed to explore the relationship between neighboring pixels. The CoA module incorporates the graphical information and enables the connectivity of roads are better preserved. Extensive experiments on the popular benchmarks (SpaceNet and DeepGlobe datasets) demonstrate that our proposed CoANet establishes new state-of-the-art results. The source code will be made publicly available at: https://mmcheng.net/coanet/ .},
  archive      = {J_TIP},
  author       = {Jie Mei and Rou-Jing Li and Wang Gao and Ming-Ming Cheng},
  doi          = {10.1109/TIP.2021.3117076},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8540-8552},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {CoANet: Connectivity attention network for road extraction from satellite imagery},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Globally and locally semantic colorization via
exemplar-based broad-GAN. <em>TIP</em>, <em>30</em>, 8526–8539. (<a
href="https://doi.org/10.1109/TIP.2021.3117061">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given a target grayscale image and a reference color image, exemplar-based image colorization aims to generate a visually natural-looking color image by transforming meaningful color information from the reference image to the target image. It remains a challenging problem due to the differences in semantic content between the target image and the reference image. In this paper, we present a novel globally and locally semantic colorization method called exemplar-based conditional broad-GAN, a broad generative adversarial network (GAN) framework, to deal with this limitation. Our colorization framework is composed of two sub-networks: the match sub-net and the colorization sub-net. We reconstruct the target image with a dictionary-based sparse representation in the match sub-net, where the dictionary consists of features extracted from the reference image. To enforce global-semantic and local-structure self-similarity constraints, global-local affinity energy is explored to constrain the sparse representation for matching consistency. Then, the matching information of the match sub-net is fed into the colorization sub-net as the perceptual information of the conditional broad-GAN to facilitate the personalized results. Finally, inspired by the observation that a broad learning system is able to extract semantic features efficiently, we further introduce a broad learning system into the conditional GAN and propose a novel loss, which substantially improves the training stability and the semantic similarity between the target image and the ground truth. Extensive experiments have shown that our colorization approach outperforms the state-of-the-art methods, both perceptually and semantically.},
  archive      = {J_TIP},
  author       = {Haoxuan Li and Bin Sheng and Ping Li and Riaz Ali and C. L. Philip Chen},
  doi          = {10.1109/TIP.2021.3117061},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8526-8539},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Globally and locally semantic colorization via exemplar-based broad-GAN},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Singularity-exponent-domain image feature transform.
<em>TIP</em>, <em>30</em>, 8510–8525. (<a
href="https://doi.org/10.1109/TIP.2021.3117055">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Combining the generalized fractal theory and the time-frequency distribution, the image feature decomposition in the singularity exponent domain is studied in this paper. With the theoretical derivation and quantitative analysis, the singularity-exponent-domain image feature transform (SIFT) method is proposed to analyze and process images from new feature dimensions. If one derives from the generalized fractal characteristics of the image, the two-dimensional frequency variables of the 2D time-frequency transform of the image can be used to estimate the two-dimensional singularity power spectrum (SPS) in the space dimension. As a consequence, it leads to the SPS distribution of the original image in the spatial domain, i.e., SIFT images. Based on the SIFT, the feature transform images with different singularity exponent and feature curves of singularity power spectrum with respect to different physical regions can thus be obtained. The SIFT is rigorously derived from the 2D-SPS and the Pseudo Wigner-Ville distribution (PWVD). In addition, the feature images based on the SIFT is proved to be the SNR independence in the GWN background. In order to validate the effectiveness of feature extraction, the proposed methodology is tested on the breast ultrasound images, the visual images, and the synthetic aperture radar (SAR) images. Furthermore, the SAR target detection method based on the SIFT images is proposed, and the experiment results indicate that the proposed algorithm is superior in performance to the traditional CFAR or 2D-SPS method. In fact, this new SIFT is promising to provide a technical approach for image feature extraction, target detection, and recognition.},
  archive      = {J_TIP},
  author       = {Gang Xiong and Fang Wang and Wenxian Yu and Trieu-Kien Truong},
  doi          = {10.1109/TIP.2021.3117055},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8510-8525},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Singularity-exponent-domain image feature transform},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Intensity-aware single-image deraining with semantic and
color regularization. <em>TIP</em>, <em>30</em>, 8497–8509. (<a
href="https://doi.org/10.1109/TIP.2021.3116794">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rain degrades image visual quality and disrupts object structures, obscuring their details and erasing their colors. Existing deraining methods are primarily based on modeling either visual appearances of rain or its physical characteristics ( e.g. , rain direction and density), and thus suffer from two common problems. First, due to the stochastic nature of rain, they tend to fail in recognizing rain streaks correctly, and wrongly remove image structures and details. Second, they fail to recover the image colors erased by heavy rain. In this paper, we address these two problems with the following three contributions. First, we propose a novel PHP block to aggregate comprehensive spatial and hierarchical information for removing rain streaks of different sizes. Second, we propose a novel network to first remove rain streaks, then recover objects structures/colors, and finally enhance details. Third, to train the network, we prepare a new dataset, and propose a novel loss function to introduce semantic and color regularization for deraining. Extensive experiments demonstrate the superiority of the proposed method over state-of-the-art deraining methods on both synthesized and real-world data, in terms of visual quality, quantitative accuracy, and running speed.},
  archive      = {J_TIP},
  author       = {Ke Xu and Xin Tian and Xin Yang and Baocai Yin and Rynson W. H. Lau},
  doi          = {10.1109/TIP.2021.3116794},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8497-8509},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Intensity-aware single-image deraining with semantic and color regularization},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). AutoPedestrian: An automatic data augmentation and loss
function search scheme for pedestrian detection. <em>TIP</em>,
<em>30</em>, 8483–8496. (<a
href="https://doi.org/10.1109/TIP.2021.3115672">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pedestrian detection is a challenging and hot research topic in the field of computer vision, especially for the crowded scenes where occlusion happens frequently. In this paper, we propose a novel AutoPedestrian scheme that automatically augments the pedestrian data and searches for suitable loss functions, aiming for better performance of pedestrian detection especially in crowded scenes. To our best knowledge, it is the first work to automatically search the optimal policy of data augmentation and loss function jointly for the pedestrian detection. To achieve the goal of searching the optimal augmentation scheme and loss function jointly, we first formulate the data augmentation policy and loss function as probability distributions based on different hyper-parameters. Then, we apply a double-loop scheme with importance-sampling to solve the optimization problem of data augmentation and loss function types efficiently. Comprehensive experiments on two popular benchmarks of CrowdHuman and CityPersons show the effectiveness of our proposed method. In particular, we achieve 40.58\% in MR on CrowdHuman datasets and 11.3\% in MR on CityPersons reasonable subset, yielding new state-of-the-art results on these two datasets.},
  archive      = {J_TIP},
  author       = {Yi Tang and Baopu Li and Min Liu and Boyu Chen and Yaonan Wang and Wanli Ouyang},
  doi          = {10.1109/TIP.2021.3115672},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8483-8496},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {AutoPedestrian: An automatic data augmentation and loss function search scheme for pedestrian detection},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Learning dense correspondences for non-rigid point clouds
with two-stage regression. <em>TIP</em>, <em>30</em>, 8468–8482. (<a
href="https://doi.org/10.1109/TIP.2021.3116786">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel deep learning method to predict dense correspondences for partial point clouds of non-rigidly deformable targets. Dense correspondences are learned in the form of vertex displacements of a template mesh towards the point clouds. A two-stage regression framework is proposed to estimate accurate displacement vectors, including the global and local regression networks. Specifically, the global regression network estimates global displacements from the global features of the template mesh and point clouds through a graph CNN based hierarchical encoder-decoder network. Based on the initial displacements, a mesh can be generated that fits to the point clouds roughly. In the local regression network, a local feature embedding layer fuses local features of point clouds with graph features on the generated mesh through an attention mechanism. Consequently, the embedded local features are employed to refine the correspondences in local regions of the targets by predicting the increments of vertex displacements. Our method is further generalized to correspondence estimation on unseen real data with a robust fine-tuning method. The experimental results on diverse datasets of various deformable subjects (e.g., human bodies, animals, and hands) demonstrate that the proposed approach can accurately and robustly estimate dense correspondences from non-rigid point clouds.},
  archive      = {J_TIP},
  author       = {Kangkan Wang and Guofeng Zhang and Huayu Zheng and Jian Yang},
  doi          = {10.1109/TIP.2021.3116786},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8468-8482},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Learning dense correspondences for non-rigid point clouds with two-stage regression},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Is heuristic sampling necessary in training deep object
detectors? <em>TIP</em>, <em>30</em>, 8454–8467. (<a
href="https://doi.org/10.1109/TIP.2021.3106802">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To train accurate deep object detectors under the extreme foreground-background imbalance, heuristic sampling methods are always necessary, which either re-sample a subset of all training samples (hard sampling methods, e.g. biased sampling, OHEM), or use all training samples but re-weight them discriminatively (soft sampling methods, e.g. Focal Loss, GHM). In this paper, we challenge the necessity of such hard/soft sampling methods for training accurate deep object detectors. While previous studies have shown that training detectors without heuristic sampling methods would significantly degrade accuracy, we reveal that this degradation comes from an unreasonable classification gradient magnitude caused by the imbalance, rather than a lack of re-sampling/re-weighting. Motivated by our discovery, we propose a simple yet effective Sampling-Free mechanism to achieve a reasonable classification gradient magnitude by initialization and loss scaling. Unlike heuristic sampling methods with multiple hyperparameters, our Sampling-Free mechanism is fully data diagnostic, without laborious hyperparameters searching. We verify the effectiveness of our method in training anchor-based and anchor-free object detectors, where our method always achieves higher detection accuracy than heuristic sampling methods on COCO and PASCAL VOC datasets. Our Sampling-Free mechanism provides a new perspective to address the foreground-background imbalance. Our code is released at https://github.com/ChenJoya/sampling-free .},
  archive      = {J_TIP},
  author       = {Joya Chen and Dong Liu and Tong Xu and Shiwei Wu and Yifei Cheng and Enhong Chen},
  doi          = {10.1109/TIP.2021.3106802},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8454-8467},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Is heuristic sampling necessary in training deep object detectors?},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust texture-aware computer-generated image forensic:
Benchmark and algorithm. <em>TIP</em>, <em>30</em>, 8439–8453. (<a
href="https://doi.org/10.1109/TIP.2021.3114989">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With advances in rendering techniques and generative adversarial networks, computer-generated (CG) images tend to be indistinguishable from photographic (PG) images. Revisiting previous works towards CG image forensic, we observed that existing datasets are constructed years ago and limited in both quantity and diversity. Besides, current algorithms only consider the global visual features for forensic, ignoring finer differences between CG and PG images. To mitigate these problems, we first contribute a Large-Scale CG images Benchmark (LSCGB), and then propose a simple yet strong baseline model to address the forensic task. On the one hand, the introduced benchmark has three superior properties, 1) large-scale: the benchmark contains 71168 CG and 71168 PG images with the corresponding expert-annotated labels. It is orders of magnitude bigger than previous datasets. 2) high diversity: we collect CG images from 4 different scenes generated by various rendering techniques. The PG images are varied in terms of image content, camera models, and photographer styles. 3) small bias: we carefully filter the collected images to ensure that the distributions of color, brightness, tone and saturation between CG and PG images are close. Furthermore, inspired by an empirical study on texture difference between CG and PG images, an effective texture-aware network is proposed to improve forensic accuracy. Concretely, we first strengthen texture information of multilevel features extracted from a backbone. Then, the relations among feature channels are explored by learning its gram matrix. Each feature channel represents a specific texture pattern. The gram matrix is thus able to embed the finer texture differences. Experimental results demonstrate that this baseline surpasses the existing methods. The benchmark is publically available at https://github.com/wmbai/LSCGB .},
  archive      = {J_TIP},
  author       = {Weiming Bai and Zhipeng Zhang and Bing Li and Pei Wang and Yangxi Li and Congxuan Zhang and Weiming Hu},
  doi          = {10.1109/TIP.2021.3114989},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8439-8453},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Robust texture-aware computer-generated image forensic: Benchmark and algorithm},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Progressive self-guided loss for salient object detection.
<em>TIP</em>, <em>30</em>, 8426–8438. (<a
href="https://doi.org/10.1109/TIP.2021.3113794">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a simple yet effective progressive self-guided loss function to facilitate deep learning-based salient object detection (SOD) in images. The saliency maps produced by the most relevant works still suffer from incomplete predictions due to the internal complexity of salient objects. Our proposed progressive self-guided loss simulates a morphological closing operation on the model predictions for progressively creating auxiliary training supervisions to step-wisely guide the training process. We demonstrate that this new loss function can guide the SOD model to highlight more complete salient objects step-by-step and meanwhile help to uncover the spatial dependencies of the salient object pixels in a region growing manner. Moreover, a new feature aggregation module is proposed to capture multi-scale features and aggregate them adaptively by a branch-wise attention mechanism. Benefiting from this module, our SOD framework takes advantage of adaptively aggregated multi-scale features to locate and detect salient objects effectively. Experimental results on several benchmark datasets show that our loss function not only advances the performance of existing SOD models without architecture modification but also helps our proposed framework to achieve state-of-the-art performance.},
  archive      = {J_TIP},
  author       = {Sheng Yang and Weisi Lin and Guosheng Lin and Qiuping Jiang and Zichuan Liu},
  doi          = {10.1109/TIP.2021.3113794},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8426-8438},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Progressive self-guided loss for salient object detection},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fine-grained fashion similarity prediction by
attribute-specific embedding learning. <em>TIP</em>, <em>30</em>,
8410–8425. (<a href="https://doi.org/10.1109/TIP.2021.3115658">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper strives to predict fine-grained fashion similarity. In this similarity paradigm, one should pay more attention to the similarity in terms of a specific design/attribute between fashion items. For example, whether the collar designs of the two clothes are similar. It has potential value in many fashion related applications, such as fashion copyright protection. To this end, we propose an Attribute-Specific Embedding Network (ASEN) to jointly learn multiple attribute-specific embeddings, thus measure the fine-grained similarity in the corresponding space. The proposed ASEN is comprised of a global branch and a local branch. The global branch takes the whole image as input to extract features from a global perspective, while the local branch takes as input the zoomed-in region-of-interest (RoI) w.r.t. the specified attribute thus able to extract more fine-grained features. As the global branch and the local branch extract the features from different perspectives, they are complementary to each other. Additionally, in each branch, two attention modules, i.e., Attribute-aware Spatial Attention and Attribute-aware Channel Attention , are integrated to make ASEN be able to locate the related regions and capture the essential patterns under the guidance of the specified attribute, thus make the learned attribute-specific embeddings better reflect the fine-grained similarity. Extensive experiments on three fashion-related datasets, i.e., FashionAI, DARN, and DeepFashion, show the effectiveness of ASEN for fine-grained fashion similarity prediction and its potential for fashion reranking. Code and data are available at https://github.com/maryeon/asenpp .},
  archive      = {J_TIP},
  author       = {Jianfeng Dong and Zhe Ma and Xiaofeng Mao and Xun Yang and Yuan He and Richang Hong and Shouling Ji},
  doi          = {10.1109/TIP.2021.3115658},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8410-8425},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Fine-grained fashion similarity prediction by attribute-specific embedding learning},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-view learning a decomposable affinity matrix via
tensor self-representation on grassmann manifold. <em>TIP</em>,
<em>30</em>, 8396–8409. (<a
href="https://doi.org/10.1109/TIP.2021.3114995">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view clustering aims to partition objects into potential categories by utilizing cross-view information. One of the core issues is to sufficiently leverage different views to learn a latent subspace, within which the clustering task is performed. Recently, it has been shown that representing the multi-view data by a tensor and then learning a latent self-expressive tensor is effective. However, early works mainly focus on learning essential tensor representation from multi-view data and the resulted affinity matrix is considered as a byproduct or is computed by a simple average in Euclidean space, thereby destroying the intrinsic clustering structure. To that end, here we proposed a novel multi-view clustering method to directly learn a well-structured affinity matrix driven by the clustering task on Grassmann manifold. Specifically, we firstly employed a tensor learning model to unify multiple feature spaces into a latent low-rank tensor space. Then each individual view was merged on Grassmann manifold to obtain both an integrative subspace and a consensus affinity matrix, driven by clustering task. The two parts are modeled by a unified objective function and optimized jointly to mine a decomposable affinity matrix. Extensive experiments on eight real-world datasets show that our method achieves superior performances over other popular methods.},
  archive      = {J_TIP},
  author       = {Haiyan Wang and Guoqiang Han and Bin Zhang and Guihua Tao and Hongmin Cai},
  doi          = {10.1109/TIP.2021.3114995},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8396-8409},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Multi-view learning a decomposable affinity matrix via tensor self-representation on grassmann manifold},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Progressive feature enhancement for person
re-identification. <em>TIP</em>, <em>30</em>, 8384–8395. (<a
href="https://doi.org/10.1109/TIP.2021.3113183">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most of person Re-Identification (ReID) works extract features from the top CNN layer for person image matching. The top CNN layer commonly corresponds to large receptive fields, thus is not effective in depicting visual cues at multiple scales, e.g. , both global appearance and local details. This work proposes a Progressive Feature Enhancement (PFE) algorithm to spot and fuse multi-scale discriminative cues from different CNN layers into a single feature vector. The basic idea is to progressively learn complementary features with a layer-specific supervision from deep to shallow layers. The layer-specific supervision is inferred by the proposed Masked Feature Augmentation (MFA) module. For each CNN layer, MFA indicates cues that have been captured in its deeper layers. MFA hence supervises each layer to depict additional visual cues missed by its deeper layers. This framework effectively learns multi-scale features without requiring extra part annotations or dividing body parts. To further facilitate the layer-specific feature generation, a Two-Stage Attention Module (TSAM) is proposed to filter pixel-wise and channel-wise noises on intermediate feature maps. Extensive experiments on four ReID datasets show that our approach achieves competitive performance, e.g. , with ResNet50 backbone, it achieves rank1 accuracy of 95.1\%, 88.2\%, 79.1\% and 71.6\% on Market-1501 , DukeMTMC-ReID , MSMT17 and CUHK03 Detected , respectively, outperforming many state-of-the-art works.},
  archive      = {J_TIP},
  author       = {Yingji Zhong and Yaowei Wang and Shiliang Zhang},
  doi          = {10.1109/TIP.2021.3113183},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8384-8395},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Progressive feature enhancement for person re-identification},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep inference networks for reliable vehicle lateral
position estimation in congested urban environments. <em>TIP</em>,
<em>30</em>, 8368–8383. (<a
href="https://doi.org/10.1109/TIP.2021.3115454">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reliable estimation of vehicle lateral position plays an essential role in enhancing the safety of autonomous vehicles. However, it remains a challenging problem due to the frequently occurred road occlusion and the unreliability of employed reference objects (e.g., lane markings, curbs, etc.). Most existing works can only solve part of the problem, resulting in unsatisfactory performance. This paper proposes a novel deep inference network (DINet) to estimate vehicle lateral position, which can adequately address the challenges. DINet integrates three deep neural network (DNN)-based components in a human-like manner. A road area detection and occluding object segmentation (RADOOS) model focuses on detecting road areas and segmenting occluding objects on the road. A road area reconstruction (RAR) model tries to reconstruct the corrupted road area to a complete one as realistic as possible, by inferring missing road regions conditioned on the occluding objects segmented before. A lateral position estimator (LPE) model estimates the position from the reconstructed road area. To verify the effectiveness of DINet, road-test experiments were carried out in the scenarios with different degrees of occlusion. The experimental results demonstrate that DINet can obtain reliable and accurate (centimeter-level) lateral position even in severe road occlusion.},
  archive      = {J_TIP},
  author       = {Zhiyong Zheng and Xu Li and Qimin Xu and Xiang Song},
  doi          = {10.1109/TIP.2021.3115454},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8368-8383},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Deep inference networks for reliable vehicle lateral position estimation in congested urban environments},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep magnification-flexible upsampling over 3D point clouds.
<em>TIP</em>, <em>30</em>, 8354–8367. (<a
href="https://doi.org/10.1109/TIP.2021.3115385">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the problem of generating dense point clouds from given sparse point clouds to model the underlying geometric structures of objects/scenes. To tackle this challenging issue, we propose a novel end-to-end learning-based framework. Specifically, by taking advantage of the linear approximation theorem, we first formulate the problem explicitly, which boils down to determining the interpolation weights and high-order approximation errors. Then, we design a lightweight neural network to adaptively learn unified and sorted interpolation weights as well as the high-order refinements, by analyzing the local geometry of the input point cloud. The proposed method can be interpreted by the explicit formulation, and thus is more memory-efficient than existing ones. In sharp contrast to the existing methods that work only for a pre-defined and fixed upsampling factor, the proposed framework only requires a single neural network with one-time training to handle various upsampling factors within a typical range, which is highly desired in real-world applications. In addition, we propose a simple yet effective training strategy to drive such a flexible ability. In addition, our method can handle non-uniformly distributed and noisy data well. Extensive experiments on both synthetic and real-world data demonstrate the superiority of the proposed method over state-of-the-art methods both quantitatively and qualitatively. The code will be publicly available at https://github.com/ninaqy/Flexible-PU .},
  archive      = {J_TIP},
  author       = {Yue Qian and Junhui Hou and Sam Kwong and Ying He},
  doi          = {10.1109/TIP.2021.3115385},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8354-8367},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Deep magnification-flexible upsampling over 3D point clouds},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Resolution learning in deep convolutional networks using
scale-space theory. <em>TIP</em>, <em>30</em>, 8342–8353. (<a
href="https://doi.org/10.1109/TIP.2021.3115001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Resolution in deep convolutional neural networks (CNNs) is typically bounded by the receptive field size through filter sizes, and subsampling layers or strided convolutions on feature maps. The optimal resolution may vary significantly depending on the dataset. Modern CNNs hard-code their resolution hyper-parameters in the network architecture which makes tuning such hyper-parameters cumbersome. We propose to do away with hard-coded resolution hyper-parameters and aim to learn the appropriate resolution from data. We use scale-space theory to obtain a self-similar parametrization of filters and make use of the N-Jet: a truncated Taylor series to approximate a filter by a learned combination of Gaussian derivative filters. The parameter $\sigma $ of the Gaussian basis controls both the amount of detail the filter encodes and the spatial extent of the filter. Since $\sigma $ is a continuous parameter, we can optimize it with respect to the loss. The proposed N-Jet layer achieves comparable performance when used in state-of-the art architectures, while learning the correct resolution in each layer automatically. We evaluate our N-Jet layer on both classification and segmentation, and we show that learning $\sigma $ is especially beneficial when dealing with inputs at multiple sizes.},
  archive      = {J_TIP},
  author       = {Silvia L. Pintea and Nergis Tömen and Stanley F. Goes and Marco Loog and Jan C. van Gemert},
  doi          = {10.1109/TIP.2021.3115001},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8342-8353},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Resolution learning in deep convolutional networks using scale-space theory},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Unsupervised abstract reasoning for raven’s problem
matrices. <em>TIP</em>, <em>30</em>, 8332–8341. (<a
href="https://doi.org/10.1109/TIP.2021.3114987">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Raven’s Progressive Matrices (RPM) is highly correlated with human intelligence, and it has been widely used to measure the abstract reasoning ability of humans. In this paper, to study the abstract reasoning capability of deep neural networks, we propose the first unsupervised learning method for solving RPM problems. Since the ground truth labels are not allowed, we design a pseudo target based on the prior constraints of the RPM formulation to approximate the ground-truth label, which effectively converts the unsupervised learning strategy into a supervised one. However, the correct answer is wrongly labelled by the pseudo target, and thus the noisy contrast will lead to inaccurate model training. To alleviate this issue, we propose to improve the model performance with negative answers. Moreover, we develop a decentralization method to adapt the feature representation to different RPM problems. Extensive experiments on three datasets demonstrate that our method even outperforms some of the supervised approaches. Our code is available at https://github.com/visiontao/ncd .},
  archive      = {J_TIP},
  author       = {Tao Zhuo and Qiang Huang and Mohan Kankanhalli},
  doi          = {10.1109/TIP.2021.3114987},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8332-8341},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Unsupervised abstract reasoning for raven’s problem matrices},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Scene adaptive online surveillance video synopsis via
dynamic tube rearrangement using octree. <em>TIP</em>, <em>30</em>,
8318–8331. (<a href="https://doi.org/10.1109/TIP.2021.3114986">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual surveillance produces a significant amount of raw video data that can be time consuming to browse and analyze. In this work, we present a video synopsis methodology called “scene adaptive online video synopsis via dynamic tube rearrangement using octree (SSOcT)” that can effectively condense input surveillance videos. Our method entailed summarizing the input video by analyzing scene characteristics and determining an effective spatio-temporal 3D structure for video synopsis. For this purpose, we first analyzed the attributes of each extracted tube with respect to scene geometry and complexity. Then, we adaptively grouped the tubes using an online grouping algorithm that exploits these scene characteristics. Finally, the tube groups were dynamically rearranged using the proposed octree-based algorithm that efficiently inserted and refined tubes containing high spatio-temporal movements in real time. Extensive video synopsis experimental results are provided, demonstrating the effectiveness and efficiency of our method in summarizing real-world surveillance videos with diverse scene characteristics.},
  archive      = {J_TIP},
  author       = {Yoonsik Yang and Haksub Kim and Heeseung Choi and Seungho Chae and Ig-Jae Kim},
  doi          = {10.1109/TIP.2021.3114986},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8318-8331},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Scene adaptive online surveillance video synopsis via dynamic tube rearrangement using octree},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hierarchical reasoning network for human-object interaction
detection. <em>TIP</em>, <em>30</em>, 8306–8317. (<a
href="https://doi.org/10.1109/TIP.2021.3093784">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human-object interaction detection that aims at detecting triplets is critical for the holistic human-centric scene understanding. Existing approaches ignore the modeling of correlations among hierarchical human parts and objects. In this work, we introduce a Hierarchical Reasoning Network (HRNet) to capture relations among human parts at multiple scales (including the holistic human, human region, and human keypoint levels) and objects via a unified graph. In particular, HRNet first constructs one multi-level human parts graph, each level of which consists of human parts at one specific scale, objects, and the unions of human part-object pairs as nodes, and their mutual visual and spatial layout relations as intra-level reasoning. To also capture the relations across scales, we further introduce inter-level reasoning between the nodes of two consecutive levels based on the prior of human body structure. The representations of graph nodes are propagated along intra-level and inter-level reasoning in turn during reasoning. Extensive experiments demonstrate our HRNet obtains new state-of-the-art results on three challenging HICO-DET, V-COCO and HOI-A benchmarks, validating the compelling effectiveness of the proposed method.},
  archive      = {J_TIP},
  author       = {Yiming Gao and Zhanghui Kuang and Guanbin Li and Wayne Zhang and Liang Lin},
  doi          = {10.1109/TIP.2021.3093784},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8306-8317},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Hierarchical reasoning network for human-object interaction detection},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Predicting task-driven attention via integrating bottom-up
stimulus and top-down guidance. <em>TIP</em>, <em>30</em>, 8293–8305.
(<a href="https://doi.org/10.1109/TIP.2021.3113799">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Task-free attention has gained intensive interest in the computer vision community while relatively few works focus on task-driven attention (TDAttention). Thus this paper handles the problem of TDAttention prediction in daily scenarios where a human is doing a task. Motivated by the cognition mechanism that human attention allocation is jointly controlled by the top-down guidance and bottom-up stimulus, this paper proposes a cognitively-explanatory deep neural network model to predict TDAttention. Given an image sequence, bottom-up features, such as human pose and motion, are firstly extracted. At the same time, the coarse-grained task information and fine-grained task information are embedded as a top-down feature. The bottom-up features are then fused with the top-down feature to guide the model to predict TDAttention. Two public datasets are re-annotated to make them qualified for TDAttention prediction, and our model is widely compared with other models on the two datasets. In addition, some ablation studies are conducted to evaluate the individual modules in our model. Experiment results demonstrate the effectiveness of our model.},
  archive      = {J_TIP},
  author       = {Zhixiong Nan and Jingjing Jiang and Xiaofeng Gao and Sanping Zhou and Weiliang Zuo and Ping Wei and Nanning Zheng},
  doi          = {10.1109/TIP.2021.3113799},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8293-8305},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Predicting task-driven attention via integrating bottom-up stimulus and top-down guidance},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Investigating customization strategies and convergence
behaviors of task-specific ADMM. <em>TIP</em>, <em>30</em>, 8278–8292.
(<a href="https://doi.org/10.1109/TIP.2021.3113796">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Alternating Direction Method of Multiplier (ADMM) has been a popular algorithmic framework for separable optimization problems with linear constraints. For numerical ADMM fail to exploit the particular structure of the problem at hand nor the input data information, leveraging task-specific modules (e.g., neural networks and other data-driven architectures) to extend ADMM is a significant but challenging task. This work focuses on designing a flexible algorithmic framework to incorporate various task-specific modules (with no additional constraints) to improve the performance of ADMM in real-world applications. Specifically, we propose Guidance from Optimality (GO), a new customization strategy, to embed task-specific modules into ADMM (GO-ADMM). By introducing an optimality-based criterion to guide the propagation, GO-ADMM establishes an updating scheme agnostic to the choice of additional modules. The existing task-specific methods just plug their task-specific modules into the numerical iterations in a straightforward manner. Even with some restrictive constraints on the plug-in modules, they can only obtain some relatively weaker convergence properties for the resulted ADMM iterations. Fortunately, without any restrictions on the embedded modules, we prove the convergence of GO-ADMM regarding objective values and constraint violations, and derive the worst-case convergence rate measured by iteration complexity. Extensive experiments are conducted to verify the theoretical results and demonstrate the efficiency of GO-ADMM.},
  archive      = {J_TIP},
  author       = {Risheng Liu and Pan Mu and Jin Zhang},
  doi          = {10.1109/TIP.2021.3113796},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8278-8292},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Investigating customization strategies and convergence behaviors of task-specific ADMM},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-modal interaction graph convolutional network for
temporal language localization in videos. <em>TIP</em>, <em>30</em>,
8265–8277. (<a href="https://doi.org/10.1109/TIP.2021.3113791">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper focuses on tackling the problem of temporal language localization in videos, which aims to identify the start and end points of a moment described by a natural language sentence in an untrimmed video. However, it is non-trivial since it requires not only the comprehensive understanding of the video and sentence query, but also the accurate semantic correspondence capture between them. Existing efforts are mainly centered on exploring the sequential relation among video clips and query words to reason the video and sentence query, neglecting the other intra-modal relations ( e.g. , semantic similarity among video clips and syntactic dependency among the query words). Towards this end, in this work, we propose a Multi-modal Interaction Graph Convolutional Network (MIGCN), which jointly explores the complex intra-modal relations and inter-modal interactions residing in the video and sentence query to facilitate the understanding and semantic correspondence capture of the video and sentence query. In addition, we devise an adaptive context-aware localization method, where the context information is taken into the candidate moments and the multi-scale fully connected layers are designed to rank and adjust the boundary of the generated coarse candidate moments with different lengths. Extensive experiments on Charades-STA and ActivityNet datasets demonstrate the promising performance and superior efficiency of our model.},
  archive      = {J_TIP},
  author       = {Zongmeng Zhang and Xianjing Han and Xuemeng Song and Yan Yan and Liqiang Nie},
  doi          = {10.1109/TIP.2021.3113791},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8265-8277},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Multi-modal interaction graph convolutional network for temporal language localization in videos},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Direct unsupervised super-resolution using generative
adversarial network (DUS-GAN) for real-world data. <em>TIP</em>,
<em>30</em>, 8251–8264. (<a
href="https://doi.org/10.1109/TIP.2021.3113783">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The deep learning models for the Single Image Super-Resolution (SISR) task have found success in recent years. However, one of the prime limitations of existing deep learning-based SISR approaches is that they need supervised training. Specifically, the Low-Resolution (LR) images are obtained through known degradation (for instance, bicubic downsampling) from the High-Resolution (HR) images to provide supervised data as an LR-HR pair. Such training results in a domain shift of learnt models when real-world data is provided with multiple degradation factors not present in the training set. To address this challenge, we propose an unsupervised approach for the SISR task using Generative Adversarial Network (GAN), which we refer to hereafter as DUS-GAN . The novel design of the proposed method accomplishes the SR task without degradation estimation of real-world LR data. In addition, a new human perception-based quality assessment loss, i.e., Mean Opinion Score (MOS), has also been introduced to boost the perceptual quality of SR results. The pertinence of the proposed method is validated with numerous experiments on different reference-based (i.e., NTIRE Real-world SR Challenge validation dataset) and no-reference based (i.e., NTIRE Real-world SR Challenge Track-1 and Track-2) testing datasets. The experimental analysis demonstrates committed improvement from the proposed method over the other state-of-the-art unsupervised SR approaches, both in terms of subjective and quantitative evaluations on different reference metrics (i.e., LPIPS, PI-RMSE graph) and no-reference quality measures such as NIQE, BRISQUE and PIQE. We also provide the implementation of the proposed approach ( https://github.com/kalpeshjp89/DUSGAN ) to support reproducible research.},
  archive      = {J_TIP},
  author       = {Kalpesh Prajapati and Vishal Chudasama and Heena Patel and Kishor Upla and Kiran Raja and Raghavendra Ramachandra and Christoph Busch},
  doi          = {10.1109/TIP.2021.3113783},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8251-8264},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Direct unsupervised super-resolution using generative adversarial network (DUS-GAN) for real-world data},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). AGRNet: Adaptive graph representation learning and reasoning
for face parsing. <em>TIP</em>, <em>30</em>, 8236–8250. (<a
href="https://doi.org/10.1109/TIP.2021.3113780">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face parsing infers a pixel-wise label to each facial component, which has drawn much attention recently. Previous methods have shown their success in face parsing, which however overlook the correlation among facial components. As a matter of fact, the component-wise relationship is a critical clue in discriminating ambiguous pixels in facial area. To address this issue, we propose adaptive graph representation learning and reasoning over facial components, aiming to learn representative vertices that describe each component, exploit the component-wise relationship and thereby produce accurate parsing results against ambiguity. In particular, we devise an adaptive and differentiable graph abstraction method to represent the components on a graph via pixel-to-vertex projection under the initial condition of a predicted parsing map, where pixel features within a certain facial region are aggregated onto a vertex. Further, we explicitly incorporate the image edge as a prior in the model, which helps to discriminate edge and non-edge pixels during the projection, thus leading to refined parsing results along the edges. Then, our model learns and reasons over the relations among components by propagating information across vertices on the graph. Finally, the refined vertex features are projected back to pixel grids for the prediction of the final parsing map. To train our model, we propose a discriminative loss to penalize small distances between vertices in the feature space, which leads to distinct vertices with strong semantics. Experimental results show the superior performance of the proposed model on multiple face parsing datasets, along with the validation on the human parsing task to demonstrate the generalizability of our model.},
  archive      = {J_TIP},
  author       = {Gusi Te and Wei Hu and Yinglu Liu and Hailin Shi and Tao Mei},
  doi          = {10.1109/TIP.2021.3113780},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8236-8250},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {AGRNet: Adaptive graph representation learning and reasoning for face parsing},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Tracking beyond detection: Learning a global response map
for end-to-end multi-object tracking. <em>TIP</em>, <em>30</em>,
8222–8235. (<a href="https://doi.org/10.1109/TIP.2021.3113169">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most of the existing Multi-Object Tracking (MOT) approaches follow the Tracking-by-Detection and Data Association paradigm, in which objects are firstly detected and then associated in the tracking process. In recent years, deep neural network has been utilized to obtain more discriminative appearance features for cross-frame association, and noticeable performance improvement has been reported. On the other hand, the Tracking-by-Detection framework is yet not completely end-to-end, which leads to huge computation and limited performance especially in the inference (tracking) process. To address this problem, we present an effective end-to-end deep learning framework which can directly take image-sequence/video as input and output the located and tracked objects of learned types. Specifically, a novel global response network is learned to project multiple objects in the image-sequence/video into a continuous response map, and the trajectory of each tracked object can then be easily picked out. The overall process is similar to how a detector inputs an image and outputs the bounding boxes of each detected object. Experimental results based on the MOT16 and MOT17 benchmarks show that our proposed on-line tracker achieves state-of-the-art performance on several tracking metrics.},
  archive      = {J_TIP},
  author       = {Xingyu Wan and Jiakai Cao and Sanping Zhou and Jinjun Wang and Nanning Zheng},
  doi          = {10.1109/TIP.2021.3113169},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8222-8235},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Tracking beyond detection: Learning a global response map for end-to-end multi-object tracking},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Semi-supervised pixel-level scene text segmentation by
mutually guided network. <em>TIP</em>, <em>30</em>, 8212–8221. (<a
href="https://doi.org/10.1109/TIP.2021.3113157">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we present a new data-driven method for pixel-level scene text segmentation from a single natural image. Although scene text detection, i.e. producing a text region mask, has been well studied in the past decade, pixel-level text segmentation is still an open problem due to the lack of massive pixel-level labeled data for supervised training. To tackle this issue, we incorporate text region mask as an auxiliary data into this task, considering acquiring large-scale of labeled text region mask is commonly less expensive and time-consuming. To be specific, we propose a mutually guided network which produces a polygon-level mask in one branch and a pixel-level text mask in the other. The two branches’ outputs serve as guidance for each other and the whole network is trained via a semi-supervised learning strategy. Extensive experiments are conducted to demonstrate the effectiveness of our mutually guided network, and experimental results show our network outperforms the state-of-the-art in pixel-level scene text segmentation. We also demonstrate the mask produced by our network could improve the text recognition performance besides the trivial image editing application.},
  archive      = {J_TIP},
  author       = {Chuan Wang and Shan Zhao and Li Zhu and Kunming Luo and Yanwen Guo and Jue Wang and Shuaicheng Liu},
  doi          = {10.1109/TIP.2021.3113157},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8212-8221},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Semi-supervised pixel-level scene text segmentation by mutually guided network},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Towards fair knowledge transfer for imbalanced domain
adaptation. <em>TIP</em>, <em>30</em>, 8200–8211. (<a
href="https://doi.org/10.1109/TIP.2021.3113576">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain adaptation (DA) becomes an up-and-coming technique to address the insufficient or no annotation issue by exploiting external source knowledge. Existing DA algorithms mainly focus on practical knowledge transfer through domain alignment. Unfortunately, they ignore the fairness issue when the auxiliary source is extremely imbalanced across different categories, which results in severe under-presented knowledge adaptation of minority source set. To this end, we propose a Towards Fair Knowledge Transfer (TFKT) framework to handle the fairness challenge in imbalanced cross-domain learning. Specifically, a novel cross-domain knowledge propagation technique is proposed with the guidance of within-source and cross-domain structure graphs to smooth the manifold of the minority source set. Besides, a cross-domain fulfillment augmentation strategy is exploited achieve domain adaptation. Moreover, hybrid distinct classifiers and cross-domain prototype alignment are adopted to seek a more robust classifier boundary and mitigate the domain shift. Such three strategies are formulated into a unified framework to address the fairness issue and domain shift challenge. Extensive experiments over two popular benchmarks have verified the effectiveness of our proposed model by comparing to existing state-of-the-art DA models, and especially our model significantly improves over 20\% on two benchmarks in terms of the overall accuracy.},
  archive      = {J_TIP},
  author       = {Taotao Jing and Bingrong Xu and Zhengming Ding},
  doi          = {10.1109/TIP.2021.3113576},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8200-8211},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Towards fair knowledge transfer for imbalanced domain adaptation},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Relational reasoning for group activity recognition via
self-attention augmented conditional random field. <em>TIP</em>,
<em>30</em>, 8184–8199. (<a
href="https://doi.org/10.1109/TIP.2021.3113570">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a new relational network for group activity recognition. The essence of the network is to integrate conditional random fields (CRFs) with self-attention to infer the temporal dependencies and spatial relationships of the actors. This combination can take advantage of the capability of CRFs in modelling the actors’ features that depend on each other and the capability of self-attention in learning the temporal evolution and spatial relational contexts of every actor in videos. Additionally, there are two distinct facets of our CRF and self-attention. First, the pairwise energy of the new CRF relies on both of the temporal self-attention and spatial self-attention, which apply the self-attention mechanism to the features in time and space, respectively. Second, to address both local and non-local relationships in group activities, the spatial self-attention takes account of a collection of cliques with different scales of spatial locality. The associated mean-field inference thereafter can thus be reformulated as a self-attention network to generate the relational contexts of the actors and their individual action labels. Lastly, a bidirectional universal transformer encoder (UTE) is utilized to aggregate the forward and backward temporal context information, scene information and relational contexts for group activity recognition. A new loss function is also employed, consisting of not only the cost for the classification of individual actions and group activities, but also a contrastive loss to address the miscellaneous relational contexts between actors. Simulations show that the new approach can surpass previous works on four commonly used datasets.},
  archive      = {J_TIP},
  author       = {Rizard Renanda Adhi Pramono and Wen-Hsien Fang and Yie-Tarng Chen},
  doi          = {10.1109/TIP.2021.3113570},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8184-8199},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Relational reasoning for group activity recognition via self-attention augmented conditional random field},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-task learning framework for motion estimation and
dynamic scene deblurring. <em>TIP</em>, <em>30</em>, 8170–8183. (<a
href="https://doi.org/10.1109/TIP.2021.3113185">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motion blur, which disturbs human and machine perceptions of a scene, has been considered an unnecessary artifact that should be removed. However, the blur can be a useful clue to understanding the dynamic scene, since various sources of motion generate different types of artifacts. Motivated by the relationship between motion and blur, we propose a motion-aware feature learning framework for dynamic scene deblurring through multi-task learning. Our multi-task framework simultaneously estimates a deblurred image and a motion field from a blurred image. We design the encoder-decoder architectures for two tasks, and the encoder part is shared between them. Our motion estimation network could effectively distinguish between different types of blur, which facilitates image deblurring. Understanding implicit motion information through image deblurring could improve the performance of motion estimation. In addition to sharing the network between two tasks, we propose a reblurring loss function to optimize the overall parameters in our multi-task architecture. We provide an intensive analysis of complementary tasks to show the effectiveness of our multi-task framework. Furthermore, the experimental results demonstrate that the proposed method outperforms the state-of-the-art deblurring methods with respect to both qualitative and quantitative evaluations.},
  archive      = {J_TIP},
  author       = {Hyungjoo Jung and Youngjung Kim and Hyunsung Jang and Namkoo Ha and Kwanghoon Sohn},
  doi          = {10.1109/TIP.2021.3113185},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8170-8183},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Multi-task learning framework for motion estimation and dynamic scene deblurring},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Joint multisource saliency and exemplar mechanism for weakly
supervised video object segmentation. <em>TIP</em>, <em>30</em>,
8155–8169. (<a href="https://doi.org/10.1109/TIP.2021.3113166">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weakly supervised video object segmentation (WSVOS) is a vital yet challenging task in which the aim is to segment pixel-level masks with only category labels. Existing methods still have certain limitations, e.g., difficulty in comprehending appropriate spatiotemporal knowledge and an inability to explore common semantic information with category labels. To overcome these challenges, we formulate a novel framework by integrating multisource saliency and incorporating an exemplar mechanism for WSVOS. Specifically, we propose a multisource saliency module to comprehend spatiotemporal knowledge by integrating spatial and temporal saliency as bottom-up cues, which can effectively eliminate disruptions due to confusing regions and identify attractive regions. Moreover, to our knowledge, we make the first attempt to incorporate an exemplar mechanism into WSVOS by proposing an adaptive exemplar module to process top-down cues, which can provide reliable guidance for co-occurring objects in intraclass videos and identify attentive regions. Our framework, which comprises the two aforementioned modules, offers a new perspective on directly constructing the correspondence between bottom-up cues and top-down cues when ground-truth information for the reference frames is lacking. Comprehensive experiments demonstrate that the proposed framework achieves state-of-the-art performance.},
  archive      = {J_TIP},
  author       = {Qing En and Lijuan Duan and Zhaoxiang Zhang},
  doi          = {10.1109/TIP.2021.3113166},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8155-8169},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Joint multisource saliency and exemplar mechanism for weakly supervised video object segmentation},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Graph attention layer evolves semantic segmentation for road
pothole detection: A benchmark and algorithms. <em>TIP</em>,
<em>30</em>, 8144–8154. (<a
href="https://doi.org/10.1109/TIP.2021.3112316">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing road pothole detection approaches can be classified as computer vision-based or machine learning-based. The former approaches typically employ 2D image analysis/ understanding or 3D point cloud modeling and segmentation algorithms to detect ( i.e. , recognize and localize) road potholes from vision sensor data, e.g. , RGB images and/or depth/disparity images. The latter approaches generally address road pothole detection using convolutional neural networks (CNNs) in an end-to-end manner. However, road potholes are not necessarily ubiquitous and it is challenging to prepare a large well-annotated dataset for CNN training. In this regard, while computer vision-based methods were the mainstream research trend in the past decade, machine learning-based methods were merely discussed. Recently, we published the first stereo vision-based road pothole detection dataset and a novel disparity transformation algorithm, whereby the damaged and undamaged road areas can be highly distinguished. However, there are no benchmarks currently available for state-of-the-art (SoTA) CNNs trained using either disparity images or transformed disparity images. Therefore, in this paper, we first discuss the SoTA CNNs designed for semantic segmentation and evaluate their performance for road pothole detection with extensive experiments. Additionally, inspired by graph neural network (GNN), we propose a novel CNN layer, referred to as graph attention layer (GAL), which can be easily deployed in any existing CNN to optimize image feature representations for semantic segmentation. Our experiments compare GAL-DeepLabv3+, our best-performing implementation, with nine SoTA CNNs on three modalities of training data: RGB images, disparity images, and transformed disparity images. The experimental results suggest that our proposed GAL-DeepLabv3+ achieves the best overall pothole detection accuracy on all training data modalities. The source code, dataset, and benchmark are publicly available at mias.group/GAL-Pothole-Detection.},
  archive      = {J_TIP},
  author       = {Rui Fan and Hengli Wang and Yuan Wang and Ming Liu and Ioannis Pitas},
  doi          = {10.1109/TIP.2021.3112316},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8144-8154},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Graph attention layer evolves semantic segmentation for road pothole detection: A benchmark and algorithms},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning to match anchor-target video pairs with dual
attentional holographic networks. <em>TIP</em>, <em>30</em>, 8130–8143.
(<a href="https://doi.org/10.1109/TIP.2021.3113165">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video hyperlinking is the task of linking two video fragments/clips based on their multi-modal contents. Specifically, given an anchor video as a query, machine techniques automatically generate links between the anchor and target videos by modeling and comparing their content aboutness. The term “aboutness” specifically refers to contextually relevant multimedia content, i.e., a fragment is on or of something. Since video contents are multi-modal (e.g., audio and vision), the content aboutness may be reflected across different modalities. Existing approaches regard hyperlinking as a retrieval task, by embedding multi-modal video contents into one or multiple common video representation space(s) for cross-modal comparison. As a result, the aboutness between videos is scored by computing the vector-distance based similarity in the learnt common feature space. However, these methods suffer from two main limitations: (1) the video modality descriptors/features are treated equally in representation learning, which hinders the effective modeling of their respective capabilities in linking; and (2) directly using the vector-distance based similarity to measure aboutness bears the risk of returning more duplicates. This paper focuses on addressing these two problems. Specifically, we firstly build attentional neural networks to learn a compact fragment-level representation, assigning different importance weights to different descriptor/feature contents by an attention mechanism. We believe that the potentially interesting content(s) should be highlighted in the representation. Furthermore, instead of directly computing the similarity of two representation embeddings, we secondly build a holographic composition network to model the aboutness for link establishment, with the core use of circular correlation. The two networks string together to form the final hyperlinking matching system. The entire model is trained in an end-to-end fashion. We examine its effectiveness by creating four train/validate/test partitioning schemes on the Blip10000 dataset and employing two video fragmentation methods.},
  archive      = {J_TIP},
  author       = {Yanbin Hao and Chong-Wah Ngo and Bin Zhu},
  doi          = {10.1109/TIP.2021.3113165},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8130-8143},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Learning to match anchor-target video pairs with dual attentional holographic networks},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Action anticipation using pairwise human-object interactions
and transformers. <em>TIP</em>, <em>30</em>, 8116–8129. (<a
href="https://doi.org/10.1109/TIP.2021.3113114">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ability to anticipate future actions of humans is useful in application areas such as automated driving, robot-assisted manufacturing, and smart homes. These applications require representing and anticipating human actions involving the use of objects. Existing methods that use human-object interactions for anticipation require object affordance labels for every relevant object in the scene that match the ongoing action. Hence, we propose to represent every pairwise human-object (HO) interaction using only their visual features. Next, we use cross-correlation to capture the second-order statistics across human-object pairs in a frame. Cross-correlation produces a holistic representation of the frame that can also handle a variable number of human-object pairs in every frame of the observation period. We show that cross-correlation based frame representation is more suited for action anticipation than attention-based and other second-order approaches. Furthermore, we observe that using a transformer model for temporal aggregation of frame-wise HO representations results in better action anticipation than other temporal networks. So, we propose two approaches for constructing an end-to-end trainable multi-modal transformer (MM-Transformer; code at https://github.com/debadityaroy/MM-Transformer_ActAnt ) model that combines the evidence across spatio-temporal, motion, and HO representations. We show the performance of MM-Transformer on procedural datasets like 50 Salads and Breakfast, and an unscripted dataset like EPIC-KITCHENS55. Finally, we demonstrate that the combination of human-object representation and MM-Transformers is effective even for long-term anticipation.},
  archive      = {J_TIP},
  author       = {Debaditya Roy and Basura Fernando},
  doi          = {10.1109/TIP.2021.3113114},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8116-8129},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Action anticipation using pairwise human-object interactions and transformers},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fs-DSM: Few-shot diagram-sentence matching via cross-modal
attention graph model. <em>TIP</em>, <em>30</em>, 8102–8115. (<a
href="https://doi.org/10.1109/TIP.2021.3112294">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diagram-sentence matching is a valuable academic research because it can help learners effectively understand the diagrams with the assisted by sentences. However, there are many uncommon objects, i.e. few-shot contents in diagrams and sentences. The existing methods for image-sentence matching have great limitations when applied to diagrams. Because they focus on the high-frequency objects during training and ignore the uncommon objects. In addition, the specialty leads to the semantic non-intuition of the diagram itself. In this work, we propose a cross-modal attention graph model for the few-shot diagram-sentence matching task named Fs-DSM. Specifically, it is composed of three modules. The graph initialization module regards the region-level diagram features and word-level sentence features as the nodes of Fs-DSM, and edges are represented as similarity between nodes. The information propagation module is a key point of Fs-DSM, in which the few-shot contents are recognized by an uncommon object recognition strategy, and then the nodes are updated by a neighborhood aggregation procedure with cross-modal propagation between all visual and textual nodes, while the edges are recomputed based on the new node features. The global association module integrates the features of regions and words to represent the global diagrams and sentences. By conducting comprehensive experiments in terms of few-shot and conventional image-sentence matching, we demonstrate that Fs-DSM achieves superior performances over the competitors on the AI2D $^\sharp $ diagram dataset and two public benchmark datasets with nature images.},
  archive      = {J_TIP},
  author       = {Xin Hu and Lingling Zhang and Jun Liu and Qinghua Zheng and Jianlong Zhou},
  doi          = {10.1109/TIP.2021.3112294},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8102-8115},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Fs-DSM: Few-shot diagram-sentence matching via cross-modal attention graph model},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sparse coding driven deep decision tree ensembles for
nucleus segmentation in digital pathology images. <em>TIP</em>,
<em>30</em>, 8088–8101. (<a
href="https://doi.org/10.1109/TIP.2021.3112057">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automating generalized nucleus segmentation has proven to be non-trivial and challenging in digital pathology. Most existing techniques in the field rely either on deep neural networks or on shallow learning-based cascading models. The former lacks theoretical understanding and tends to degrade performance when only limited amounts of training data are available while the latter often suffers from limitations for generalization. To address these issues, we propose sparse coding driven deep decision tree ensembles (ScD 2 TE), an easily trained yet powerful representation learning approach with performance highly competitive to deep neural networks in the generalized nucleus segmentation task. We explore the possibility of stacking several layers based on fast convolutional sparse coding–decision tree ensemble pairwise modules and generate a layer-wise encoder–decoder architecture with intra-decoder and inter-encoder dense connectivity patterns. Under this architecture, all the encoders share the same assumption across the different layers to represent images and interact with their decoders to give fast convergence. Compared with deep neural networks, our proposed ScD 2 TE does not require back-propagation computation and depends on less hyper-parameters. ScD 2 TE is able to achieve a fast end-to-end pixel-wise training in a layer-wise manner. We demonstrated the superiority of our segmentation method by evaluating it on the multi-disease state and multi-organ dataset where consistently higher performances were obtained for comparison against other state-of-the-art deep learning techniques and cascading methods with various connectivity patterns.},
  archive      = {J_TIP},
  author       = {Jie Song and Liang Xiao and Mohsen Molaei and Zhichao Lian},
  doi          = {10.1109/TIP.2021.3112057},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8088-8101},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Sparse coding driven deep decision tree ensembles for nucleus segmentation in digital pathology images},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Semantically adversarial learnable filters. <em>TIP</em>,
<em>30</em>, 8075–8087. (<a
href="https://doi.org/10.1109/TIP.2021.3112290">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present an adversarial framework to craft perturbations that mislead classifiers by accounting for the image content and the semantics of the labels. The proposed framework combines a structure loss and a semantic adversarial loss in a multi-task objective function to train a fully convolutional neural network. The structure loss helps generate perturbations whose type and magnitude are defined by a target image processing filter. The semantic adversarial loss considers groups of (semantic) labels to craft perturbations that prevent the filtered image from being classified with a label in the same group. We validate our framework with three different target filters, namely detail enhancement, log transformation and gamma correction filters; and evaluate the adversarially filtered images against three classifiers, ResNet50, ResNet18 and AlexNet, pre-trained on ImageNet. We show that the proposed framework generates filtered images with a high success rate, robustness, and transferability to unseen classifiers. We also discuss objective and subjective evaluations of the adversarial perturbations.},
  archive      = {J_TIP},
  author       = {Ali Shahin Shamsabadi and Changjae Oh and Andrea Cavallaro},
  doi          = {10.1109/TIP.2021.3112290},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8075-8087},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Semantically adversarial learnable filters},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). ChipQA: No-reference video quality prediction via space-time
chips. <em>TIP</em>, <em>30</em>, 8059–8074. (<a
href="https://doi.org/10.1109/TIP.2021.3112055">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new model for no-reference video quality assessment (VQA). Our approach uses a new idea of highly-localized space-time (ST) slices called Space-Time Chips (ST Chips). ST Chips are localized cuts of video data along directions that implicitly capture motion. We use perceptually-motivated bandpass and normalization models to first process the video data, and then select oriented ST Chips based on how closely they fit parametric models of natural video statistics. We show that the parameters that describe these statistics can be used to reliably predict the quality of videos, without the need for a reference video. The proposed method implicitly models ST video naturalness, and deviations from naturalness. We train and test our model on several large VQA databases, and show that our model achieves state-of-the-art performance at reduced cost, without requiring motion computation.},
  archive      = {J_TIP},
  author       = {Joshua Peter Ebenezer and Zaixi Shang and Yongjun Wu and Hai Wei and Sriram Sethuraman and Alan C. Bovik},
  doi          = {10.1109/TIP.2021.3112055},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8059-8074},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {ChipQA: No-reference video quality prediction via space-time chips},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An automatic and optimal MPA design method. <em>TIP</em>,
<em>30</em>, 8046–8058. (<a
href="https://doi.org/10.1109/TIP.2021.3112047">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Raw polarimetric images are captured by a focal plane polarimeter which is covered by a micro-polarizer array (MPA). The design of the MPA plays a crucial role in polarimetric imaging. MPAs are predominantly designed according to expert engineering experience and rules of thumb. Typically, only one optimization criterion, maximizing bandwidth, is used to design the MPA. To select a design, an exhaustive search is usually performed on a very limited set of available polarizing patterns, which must be constrained in order to make the search tractable. In contrast, this paper proposes a fully automated and optimal MPA design method (AO-MPA) which generates significantly improved MPAs. Instead of the single criterion of bandwidth, we propose six design principles, and show how they can be utilized to mutually optimize the MPA design by formulating a tri-objective optimization problem with multiple constraints. A much larger set of possible MPA patterns is rapidly and automatically searched by applying advanced multi-objective optimization techniques. We have tested AO-MPA using two groups of experiments, in which AO-MPA is compared against several other leading MPA design methods, and the patterns generated by AO-MPA are compared against state-of-the-art patterns from the literature. The results, obtained using a public benchmark dataset, show that the AO-MPA method is very computationally efficient, and can find all optimal MPA patterns for all array sizes. Moreover, for each size, AO-MPA obtains all optimal layouts simultaneously. AO-MPA generates designs which require fewer polarization orientations, while also yielding better performance in estimating intensity measurements, Stokes vector and the degree of linear polarization. This results in MPAs which are easier to manufacture while also being more robust to noise.},
  archive      = {J_TIP},
  author       = {Lin Li and Lingchen Sun and Bin Feng and Rustam Stolkin and Zhunga Liu},
  doi          = {10.1109/TIP.2021.3112047},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8046-8058},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {An automatic and optimal MPA design method},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Joint multi-dimension pruning via numerical gradient
update. <em>TIP</em>, <em>30</em>, 8034–8045. (<a
href="https://doi.org/10.1109/TIP.2021.3112041">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present joint multi-dimension pruning (abbreviated as JointPruning), an effective method of pruning a network on three crucial aspects: spatial, depth and channel simultaneously. To tackle these three naturally different dimensions, we proposed a general framework by defining pruning as seeking the best pruning vector (i.e., the numerical value of layer-wise channel number, spatial size, depth) and construct a unique mapping from the pruning vector to the pruned network structures. Then we optimize the pruning vector with gradient update and model joint pruning as a numerical gradient optimization process. To overcome the challenge that there is no explicit function between the loss and the pruning vectors, we proposed self-adapted stochastic gradient estimation to construct a gradient path through network loss to pruning vectors and enable efficient gradient update. We show that the joint strategy discovers a better status than previous studies that focused on individual dimensions solely, as our method is optimized collaboratively across the three dimensions in a single end-to-end training and it is more efficient than the previous exhaustive methods. Extensive experiments on large-scale ImageNet dataset across a variety of network architectures MobileNet V1&amp;V2&amp;V3 and ResNet demonstrate the effectiveness of our proposed method. For instance, we achieve significant margins of 2.5\% and 2.6\% improvement over the state-of-the-art approach on the already compact MobileNet V1&amp;V2 under an extremely large compression ratio.},
  archive      = {J_TIP},
  author       = {Zechun Liu and Xiangyu Zhang and Zhiqiang Shen and Yichen Wei and Kwang-Ting Cheng and Jian Sun},
  doi          = {10.1109/TIP.2021.3112041},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8034-8045},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Joint multi-dimension pruning via numerical gradient update},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning modal-invariant angular metric by cyclic projection
network for VIS-NIR person re-identification. <em>TIP</em>, <em>30</em>,
8019–8033. (<a href="https://doi.org/10.1109/TIP.2021.3112035">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person re-identification across visible and near-infrared cameras (VIS-NIR Re-ID) has widespread applications. The challenge of this task lies in heterogeneous image matching. Existing methods attempt to learn discriminative features via complex feature extraction strategies. Nevertheless, the distributions of visible and near-infrared features are disparate caused by modal gap, which significantly affects feature metric and makes the performance of the existing models poor. To address this problem, we propose a novel approach from the perspective of metric learning. We conduct metric learning on a well-designed angular space. Geometrically, features are mapped from the original space to the hypersphere manifold, which eliminates the variations of feature norm and concentrates on the angle between the feature and the target category. Specifically, we propose a cyclic projection network (CPN) that transforms features into an angle-related space while identity information is preserved. Furthermore, we proposed three kinds of loss functions, AICAL, LAL and DAL, in angular space for angular metric learning. Multiple experiments on two existing public datasets, SYSU-MM01 and RegDB, show that performance of our method greatly exceeds the SOTA performance.},
  archive      = {J_TIP},
  author       = {Quan Zhang and Jianhuang Lai and Xiaohua Xie},
  doi          = {10.1109/TIP.2021.3112035},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8019-8033},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Learning modal-invariant angular metric by cyclic projection network for VIS-NIR person re-identification},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Domain adaptive ensemble learning. <em>TIP</em>,
<em>30</em>, 8008–8018. (<a
href="https://doi.org/10.1109/TIP.2021.3112012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of generalizing deep neural networks from multiple source domains to a target one is studied under two settings: When unlabeled target data is available, it is a multi-source unsupervised domain adaptation (UDA) problem, otherwise a domain generalization (DG) problem. We propose a unified framework termed domain adaptive ensemble learning (DAEL) to address both problems. A DAEL model is composed of a CNN feature extractor shared across domains and multiple classifier heads each trained to specialize in a particular source domain. Each such classifier is an expert to its own domain but a non-expert to others. DAEL aims to learn these experts collaboratively so that when forming an ensemble, they can leverage complementary information from each other to be more effective for an unseen target domain. To this end, each source domain is used in turn as a pseudo-target-domain with its own expert providing supervisory signal to the ensemble of non-experts learned from the other sources. To deal with unlabeled target data under the UDA setting where real expert does not exist, DAEL uses pseudo labels to supervise the ensemble learning. Extensive experiments on three multi-source UDA datasets and two DG datasets show that DAEL improves the state of the art on both problems, often by significant margins.},
  archive      = {J_TIP},
  author       = {Kaiyang Zhou and Yongxin Yang and Yu Qiao and Tao Xiang},
  doi          = {10.1109/TIP.2021.3112012},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {8008-8018},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Domain adaptive ensemble learning},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep multiple instance hashing for fast multi-object image
search. <em>TIP</em>, <em>30</em>, 7995–8007. (<a
href="https://doi.org/10.1109/TIP.2021.3112011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-keyword query is widely supported in text search engines. However, an analogue in image retrieval systems, multi-object query, is rarely studied. Meanwhile, traditional object-based image retrieval methods often involve multiple steps separately. In this work, we propose a weakly-supervised Deep Multiple Instance Hashing (DMIH) approach for multi-object image retrieval. Our DMIH approach, which leverages a popular CNN model to build the end-to-end relation between a raw image and the binary hash codes of its multiple objects, can support multi-object queries effectively and integrate object detection with hashing learning seamlessly. We treat object detection as a binary multiple instance learning (MIL) problem and such instances are automatically extracted from multi-scale convolutional feature maps. We also design a conditional random field (CRF) module to capture both the semantic and spatial relations among different class labels. For hashing training, we sample image pairs to learn their semantic relationships in terms of hash codes of the most probable proposals for owned labels as guided by object predictors. The two objectives benefit each other in a multi-task learning scheme. Finally, a two-level inverted index method is proposed to further speed up the retrieval of multi-object queries. Our DMIH approach outperforms state-of-the-arts on public benchmarks for object-based image retrieval and achieves promising results for multi-object queries.},
  archive      = {J_TIP},
  author       = {Wanqing Zhao and Ziyu Guan and Hangzai Luo and Jinye Peng and Jianping Fan},
  doi          = {10.1109/TIP.2021.3112011},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7995-8007},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Deep multiple instance hashing for fast multi-object image search},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Bilateral asymmetry guided counterfactual generating
network for mammogram classification. <em>TIP</em>, <em>30</em>,
7980–7994. (<a href="https://doi.org/10.1109/TIP.2021.3112053">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mammogram benign or malignant classification with only image-level labels is challenging due to the absence of lesion annotations. Motivated by the symmetric prior that the lesions on one side of breasts rarely appear in the corresponding areas on the other side, we explore to answer a counterfactual question to identify the lesion areas. This counterfactual question means: given an image with lesions, how would the features have behaved if there were no lesions in the image? To answer this question, we derive a new theoretical result based on the symmetric prior. Specifically, by building a causal model that entails such a prior for bilateral images, we identify to optimize the distances in distribution between i) the counterfactual features and the target side’s features in lesion-free areas; and ii) the counterfactual features and the reference side’s features in lesion areas. To realize these optimizations for better benign/malignant classification, we propose a counterfactual generative network, which is mainly composed of Generator Adversarial Network and a prediction feedback mechanism , they are optimized jointly and prompt each other. Specifically, the former can further improve the classi?cation performance by generating counterfactual features to calculate lesion areas. On the other hand, the latter helps counterfactual generation by the supervision of classification loss. The utility of our method and the effectiveness of each module in our model can be verified by state-of-the-art performance on INBreast and an in-house dataset and ablation studies.},
  archive      = {J_TIP},
  author       = {Churan Wang and Jing Li and Fandong Zhang and Xinwei Sun and Hao Dong and Yizhou Yu and Yizhou Wang},
  doi          = {10.1109/TIP.2021.3112053},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7980-7994},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Bilateral asymmetry guided counterfactual generating network for mammogram classification},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep learning-based forgery attack on document images.
<em>TIP</em>, <em>30</em>, 7964–7979. (<a
href="https://doi.org/10.1109/TIP.2021.3112048">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the ongoing popularization of online services, the digital document images have been used in various applications. Meanwhile, there have emerged some deep learning-based text editing algorithms which alter the textual information of an image in an end-to-end fashion. In this work, we present a low-cost document forgery algorithm by the existing deep learning-based technologies to edit practical document images. To achieve this goal, the limitations of existing text editing algorithms towards complicated characters and complex background are addressed by a set of network design strategies. First, the unnecessary confusion in the supervision data is avoided by disentangling the textual and background information in the source images. Second, to capture the structure of some complicated components, the text skeleton is provided as auxiliary information and the continuity in texture is considered explicitly in the loss function. Third, the forgery traces induced by the text editing operation are mitigated by some post-processing operations which consider the distortions from the print-and-scan channel. Quantitative comparisons of the proposed method and the exiting approach have shown the advantages of our design by reducing the about 2/3 reconstruction error measured in MSE, improving reconstruction quality measured in PSNR and in SSIM by 4 dB and 0.21, respectively. Qualitative experiments have confirmed that the reconstruction results of the proposed method are visually better than the existing approach in both complicated characters and complex texture. More importantly, we have demonstrated the performance of the proposed document forgery algorithm under a practical scenario where an attacker is able to alter the textual information in an identity document using only one sample in the target domain. The forged-and-recaptured samples created by the proposed text editing attack and recapturing operation have successfully fooled some existing document authentication systems.},
  archive      = {J_TIP},
  author       = {Lin Zhao and Changsheng Chen and Jiwu Huang},
  doi          = {10.1109/TIP.2021.3112048},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7964-7979},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Deep learning-based forgery attack on document images},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). TEST: Triplet ensemble student-teacher model for
unsupervised person re-identification. <em>TIP</em>, <em>30</em>,
7952–7963. (<a href="https://doi.org/10.1109/TIP.2021.3112039">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The self-ensembling methods have achieved amazing performance for semi-supervised representation learning and domain adaptation. However, the disadvantage of these methods is that the teacher network is tightly coupled with the student network, which limits the descriptive ability of the self-ensembling model. To overcome the coupling effect between the teacher network and the student network, we propose a novel Triplet Ensemble Student-Teacher (TEST) model for unsupervised person re-identification, which consists of one teacher network $T$ and two student networks $S1$ and $S2$ . Similar to the traditional self-ensembling model, the student network $S1$ is applied to update the teacher network $T$ . Furthermore, a closed-loop learning mechanism is built in the TEST model by imposing an ensemble consistent constraint between $T$ and $S2$ , and performing a heterogeneous co-teaching procedure between $S1$ and $S2$ . With the closed-loop learning mechanism, the TEST model can loosen the constraint between the teacher $T$ and the student $S1$ , and enhance the descriptive ability of $S1$ . Besides, the knowledge exchange between $S1$ and $S2$ can ensure that the two student networks can elegantly deal with the noisy labels and avoid coupling. By training the TEST model with the clustering-generated pseudo labels, we can achieve effective and robust representation learning for unsupervised person re-identification. The evaluations on three widely-used benchmarks show that our approach can achieve significant performance compared with state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Yaoyu Li and Hantao Yao and Changsheng Xu},
  doi          = {10.1109/TIP.2021.3112039},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7952-7963},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {TEST: Triplet ensemble student-teacher model for unsupervised person re-identification},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). TDIOT: Target-driven inference for deep video object
tracking. <em>TIP</em>, <em>30</em>, 7938–7951. (<a
href="https://doi.org/10.1109/TIP.2021.3112010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent tracking-by-detection approaches use deep object detectors as target detection baseline, because of their high performance on still images. For effective video object tracking, object detection is integrated with a data association step performed by either a custom design inference architecture or an end-to-end joint training for tracking purpose. In this work, we adopt the former approach and use the pre-trained Mask R-CNN deep object detector as the baseline. We introduce a novel inference architecture placed on top of FPN-ResNet101 backbone of Mask R-CNN to jointly perform detection and tracking, without requiring additional training for tracking purpose. The proposed single object tracker, TDIOT, applies an appearance similarity-based temporal matching for data association. To tackle tracking discontinuities, we incorporate a local search and matching module into the inference head layer that exploits SiamFC. Moreover, to improve robustness to scale changes, we introduce a scale adaptive region proposal network that enables to search for the target at an adaptively enlarged spatial neighborhood specified by the trace of the target. In order to meet long term tracking requirements, a low cost verification layer is incorporated into the inference architecture to monitor presence of the target based on its LBP histogram model. Performance evaluation on videos from VOT2016, VOT2018, and VOT-LT2018 datasets demonstrate that TDIOT achieves higher accuracy compared to the state-of-the-art short-term trackers while it provides comparable performance in long term tracking. We also compare our tracker on LaSOT dataset where we observe that TDIOT provides comparable performance with other methods that are trained on LaSOT. The source code and TDIOT output videos are accessible at https://github.com/msprITU/TDIOT .},
  archive      = {J_TIP},
  author       = {Filiz Gurkan and Llukman Cerkezi and Ozgun Cirakman and Bilge Gunsel},
  doi          = {10.1109/TIP.2021.3112010},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7938-7951},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {TDIOT: Target-driven inference for deep video object tracking},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Joint feature optimization and fusion for compressed action
recognition. <em>TIP</em>, <em>30</em>, 7926–7937. (<a
href="https://doi.org/10.1109/TIP.2021.3112008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent methods including CoViAR and DMC-Net provide a new paradigm for action recognition since they are directly targeted at compressed videos (e.g., MPEG4 files). It avoids the cumbersome decoding procedure of traditional methods, and leverages the pre-encoded motion vectors and residuals in compressed videos to complete recognition efficiently. However, motion vectors and residuals are noisy, sparse and highly correlated information, which cannot be effectively exploited by plain and separated networks. To tackle these issues, we propose a joint feature optimization and fusion framework that better utilizes motion vectors and residuals in the following three aspects. (i) We model the feature optimization problem as a reconstruction process that represents features by a set of bases, and propose a joint feature optimization module that extracts bases in the both modalities. (ii) A low-rank non-local attention module, which combines the non-local operation with the low-rank constraint, is proposed to tackle the noise and sparsity problem during the feature reconstruction process. (iii) A lightweight feature fusion module and a self-adaptive knowledge distillation method are introduced, which use motion vectors and residuals to generate predictions similar to those from networks with optical flows. With these proposed components embedded in a baseline network, the proposed network not only achieves the state-of-the-art performance on HMDB-51 and UCF-101, but also maintains its advantage in computational complexity.},
  archive      = {J_TIP},
  author       = {Hanhui Li and Xudong Jiang and Boliang Guan and Raymond Rui Ming Tan and Ruomei Wang and Nadia Magnenat Thalmann},
  doi          = {10.1109/TIP.2021.3112008},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7926-7937},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Joint feature optimization and fusion for compressed action recognition},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Learning dynamical human-joint affinity for 3D pose
estimation in videos. <em>TIP</em>, <em>30</em>, 7914–7925. (<a
href="https://doi.org/10.1109/TIP.2021.3109517">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Convolution Network (GCN) has been successfully used for 3D human pose estimation in videos. However, it is often built on the fixed human-joint affinity, according to human skeleton. This may reduce adaptation capacity of GCN to tackle complex spatio-temporal pose variations in videos. To alleviate this problem, we propose a novel Dynamical Graph Network (DG-Net), which can dynamically identify human-joint affinity, and estimate 3D pose by adaptively learning spatial/temporal joint relations from videos. Different from traditional graph convolution, we introduce Dynamical Spatial/Temporal Graph convolution (DSG/DTG) to discover spatial/temporal human-joint affinity for each video exemplar, depending on spatial distance/temporal movement similarity between human joints in this video. Hence, they can effectively understand which joints are spatially closer and/or have consistent motion, for reducing depth ambiguity and/or motion uncertainty when lifting 2D pose to 3D pose. We conduct extensive experiments on three popular benchmarks, e.g., Human3.6M, HumanEva-I, and MPI-INF-3DHP, where DG-Net outperforms a number of recent SOTA approaches with fewer input frames and model size.},
  archive      = {J_TIP},
  author       = {Junhao Zhang and Yali Wang and Zhipeng Zhou and Tianyu Luan and Zhe Wang and Yu Qiao},
  doi          = {10.1109/TIP.2021.3109517},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7914-7925},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Learning dynamical human-joint affinity for 3D pose estimation in videos},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Layout-to-image translation with double pooling generative
adversarial networks. <em>TIP</em>, <em>30</em>, 7903–7913. (<a
href="https://doi.org/10.1109/TIP.2021.3109531">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we address the task of layout-to-image translation, which aims to translate an input semantic layout to a realistic image. One open challenge widely observed in existing methods is the lack of effective semantic constraints during the image translation process, leading to models that cannot preserve the semantic information and ignore the semantic dependencies within the same object. To address this issue, we propose a novel Double Pooling GAN (DPGAN) for generating photo-realistic and semantically-consistent results from the input layout. We also propose a novel Double Pooling Module (DPM), which consists of the Square-shape Pooling Module (SPM) and the Rectangle-shape Pooling Module (RPM). Specifically, SPM aims to capture short-range semantic dependencies of the input layout with different spatial scales, while RPM aims to capture long-range semantic dependencies from both horizontal and vertical directions. We then effectively fuse both outputs of SPM and RPM to further enlarge the receptive field of our generator. Extensive experiments on five popular datasets show that the proposed DPGAN achieves better results than state-of-the-art methods. Finally, both SPM and SPM are general and can be seamlessly integrated into any GAN-based architectures to strengthen the feature representation. The code is available at https://github.com/Ha0Tang/DPGAN .},
  archive      = {J_TIP},
  author       = {Hao Tang and Nicu Sebe},
  doi          = {10.1109/TIP.2021.3109531},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7903-7913},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Layout-to-image translation with double pooling generative adversarial networks},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An unified recurrent video object segmentation framework for
various surveillance environments. <em>TIP</em>, <em>30</em>, 7889–7902.
(<a href="https://doi.org/10.1109/TIP.2021.3108405">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Moving object segmentation (MOS) in videos received considerable attention because of its broad security-based applications like robotics, outdoor video surveillance, self-driving cars, etc. The current prevailing algorithms highly depend on additional trained modules for other applications or complicated training procedures or neglect the inter-frame spatio-temporal structural dependencies. To address these issues, a simple, robust, and effective unified recurrent edge aggregation approach is proposed for MOS, in which additional trained modules or fine-tuning on a test video frame(s) are not required. Here, a recurrent edge aggregation module (REAM) is proposed to extract effective foreground relevant features capturing spatio-temporal structural dependencies with encoder and respective decoder features connected recurrently from previous frame. These REAM features are then connected to a decoder through skip connections for comprehensive learning named as temporal information propagation . Further, the motion refinement block with multi-scale dense residual is proposed to combine the features from the optical flow encoder stream and the last REAM module for holistic feature learning. Finally, these holistic features and REAM features are given to the decoder block for segmentation. To guide the decoder block, previous frame output with respective scales is utilized. The different configurations of training-testing techniques are examined to evaluate the performance of the proposed method. Specifically, outdoor videos often suffer from constrained visibility due to different environmental conditions and other small particles in the air that scatter the light in the atmosphere. Thus, comprehensive result analysis is conducted on six benchmark video datasets with different surveillance environments. We demonstrate that the proposed method outperforms the state-of-the-art methods for MOS without any pre-trained module, fine-tuning on the test video frame(s) or complicated training.},
  archive      = {J_TIP},
  author       = {Prashant W. Patil and Akshay Dudhane and Ashutosh Kulkarni and Subrahmanyam Murala and Anil Balaji Gonde and Sunil Gupta},
  doi          = {10.1109/TIP.2021.3108405},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7889-7902},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {An unified recurrent video object segmentation framework for various surveillance environments},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Discriminative cross-modality attention network for temporal
inconsistent audio-visual event localization. <em>TIP</em>, <em>30</em>,
7878–7888. (<a href="https://doi.org/10.1109/TIP.2021.3106814">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is theoretically insufficient to construct a complete set of semantics in the real world using single-modality data. As a typical application of multi-modality perception, the audio-visual event localization task aims to match audio and visual components to identify the simultaneous events of interest. Although some recent methods have been proposed to deal with this task, they cannot handle the practical situation of temporal inconsistency that is widespread in the audio-visual scene. Inspired by the human system which automatically filters out event-unrelated information when performing multi-modality perception, we propose a discriminative cross-modality attention network to simulate such a process. Similar to human mechanism, our network can adaptively select “where” to attend, “when” to attend and “which” to attend for audio-visual event localization. In addition, to prevent our network from getting trivial solutions, a novel eigenvalue-based objective function is proposed to train the whole network to better fuse audio and visual signals, which can obtain discriminative and nonlinear multi-modality representation. In this way, even with large temporal inconsistency between audio and visual sequence, our network is able to adaptively select event-valuable information for audio-visual event localization. Furthermore, we systemically investigate three subtasks of audio-visual event localization, i.e., temporal localization, weakly-supervised spatial localization and cross-modality localization. The visualization results also help us better understand how our network works.},
  archive      = {J_TIP},
  author       = {Hanyu Xuan and Lei Luo and Zhenyu Zhang and Jian Yang and Yan Yan},
  doi          = {10.1109/TIP.2021.3106814},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7878-7888},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Discriminative cross-modality attention network for temporal inconsistent audio-visual event localization},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Generalized MSFA engineering with structural and adaptive
nonlocal demosaicing. <em>TIP</em>, <em>30</em>, 7867–7877. (<a
href="https://doi.org/10.1109/TIP.2021.3108913">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The emerging multispectral-filter-array (MSFA) cameras require generalized demosaicing for MSFA engineering. The existing interpolation, compressive sensing and deep learning based methods suffer from either limited reconstruction accuracy or poor generalization. In this work, we report a generalized demosaicing method with structural and adaptive nonlocal optimization, enabling boosted reconstruction accuracy for different MSFAs. The advantages lie in the following three aspects. First, the nonlocal low-rank optimization is applied and extended to the multiple spatial-spectral-temporal dimensions to exploit more crucial details. Second, the block matching accuracy is promoted by employing a novel structural similarity metric instead of the conventional Euclidean distance. Third, the running efficiency is boosted by an adaptive iteration strategy. We built a prototype system to capture raw mosaic images under different MSFAs, and used the technique as an off-the-shelf tool to demonstrate MSFA engineering. The experiments show that the binary tree (BT) based filter array produces higher accuracy than the random and regular ones for different number of channels.},
  archive      = {J_TIP},
  author       = {Liheng Bian and Yugang Wang and Jun Zhang},
  doi          = {10.1109/TIP.2021.3108913},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7867-7877},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Generalized MSFA engineering with structural and adaptive nonlocal demosaicing},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hierarchical generation of human pose with part-based layer
representation. <em>TIP</em>, <em>30</em>, 7856–7866. (<a
href="https://doi.org/10.1109/TIP.2021.3108023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human pose transfer has been becoming one of the emerging research topics in recent years. However, state-of-the-art results are still far from satisfactory. One main reason is that these end-to-end methods are often blindly trained without the semantic understanding of its content. In this paper, we propose a novel method for human pose transfer with consideration of the semantic part-based representation of a human. In particular, we propose to segment the human body into multiple parts, and each of them represents a semantic region of a human. With the proposed part-based layer generators, a high-quality result is guaranteed for each local semantic region. We design a three-stage hierarchical framework to fuse local representations into the final result in a coarse-to-fine manner, which provides adaptive attention for global consistency and local details, respectively. Via exploiting spatial guidance from 3D human model through the framework, our method can naturally handle the ambiguity of self-occlusions which always causes artifacts in previous methods. With semantic-aware and spatial-aware representations, our method outperforms previous approaches quantitatively and qualitatively in better handling self-occlusions, fine detail preservation/synthesis and a higher resolution result.},
  archive      = {J_TIP},
  author       = {Xian Wu and Chen Li and Shi-Min Hu and Yu-Wing Tai},
  doi          = {10.1109/TIP.2021.3108023},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7856-7866},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Hierarchical generation of human pose with part-based layer representation},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Joint clustering and discriminative feature alignment for
unsupervised domain adaptation. <em>TIP</em>, <em>30</em>, 7842–7855.
(<a href="https://doi.org/10.1109/TIP.2021.3109530">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised Domain Adaptation (UDA) aims to learn a classifier for the unlabeled target domain by leveraging knowledge from a labeled source domain with a different but related distribution. Many existing approaches typically learn a domain-invariant representation space by directly matching the marginal distributions of the two domains. However, they ignore exploring the underlying discriminative features of the target data and align the cross-domain discriminative features, which may lead to suboptimal performance. To tackle these two issues simultaneously, this paper presents a Joint Clustering and Discriminative Feature Alignment (JCDFA) approach for UDA, which is capable of naturally unifying the mining of discriminative features and the alignment of class-discriminative features into one single framework. Specifically, in order to mine the intrinsic discriminative information of the unlabeled target data, JCDFA jointly learns a shared encoding representation for two tasks: supervised classification of labeled source data, and discriminative clustering of unlabeled target data, where the classification of the source domain can guide the clustering learning of the target domain to locate the object category. We then conduct the cross-domain discriminative feature alignment by separately optimizing two new metrics: 1) an extended supervised contrastive learning, i.e. , semi-supervised contrastive learning 2) an extended Maximum Mean Discrepancy (MMD), i.e. , conditional MMD, explicitly minimizing the intra-class dispersion and maximizing the inter-class compactness. When these two procedures, i.e. , discriminative features mining and alignment are integrated into one framework, they tend to benefit from each other to enhance the final performance from a cooperative learning perspective. Experiments are conducted on four real-world benchmarks ( e.g. , Office-31, ImageCLEF-DA, Office-Home and VisDA-C). All the results demonstrate that our JCDFA can obtain remarkable margins over state-of-the-art domain adaptation methods. Comprehensive ablation studies also verify the importance of each key component of our proposed algorithm and the effectiveness of combining two learning strategies into a framework.},
  archive      = {J_TIP},
  author       = {Wanxia Deng and Qing Liao and Lingjun Zhao and Deke Guo and Gangyao Kuang and Dewen Hu and Li Liu},
  doi          = {10.1109/TIP.2021.3109530},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7842-7855},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Joint clustering and discriminative feature alignment for unsupervised domain adaptation},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust single-image super-resolution via CNNs and TV-TV
minimization. <em>TIP</em>, <em>30</em>, 7830–7841. (<a
href="https://doi.org/10.1109/TIP.2021.3108907">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single-image super-resolution is the process of increasing the resolution of an image, obtaining a high-resolution (HR) image from a low-resolution (LR) one. By leveraging large training datasets, convolutional neural networks (CNNs) currently achieve the state-of-the-art performance in this task. Yet, during testing/deployment, they fail to enforce consistency between the HR and LR images: if we downsample the output HR image, it never matches its LR input. Based on this observation, we propose to post-process the CNN outputs with an optimization problem that we call TV-TV minimization , which enforces consistency. As our extensive experiments show, such post-processing not only improves the quality of the images, in terms of PSNR and SSIM, but also makes the super-resolution task robust to operator mismatch, i.e., when the true downsampling operator is different from the one used to create the training dataset.},
  archive      = {J_TIP},
  author       = {Marija Vella and João F. C. Mota},
  doi          = {10.1109/TIP.2021.3108907},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7830-7841},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Robust single-image super-resolution via CNNs and TV-TV minimization},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dual-refinement: Joint label and feature refinement for
unsupervised domain adaptive person re-identification. <em>TIP</em>,
<em>30</em>, 7815–7829. (<a
href="https://doi.org/10.1109/TIP.2021.3104169">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised domain adaptive (UDA) person re-identification (re-ID) is a challenging task due to the missing of labels for the target domain data. To handle this problem, some recent works adopt clustering algorithms to off-line generate pseudo labels, which can then be used as the supervision signal for on-line feature learning in the target domain. However, the off-line generated labels often contain lots of noise that significantly hinders the discriminability of the on-line learned features, and thus limits the final UDA re-ID performance. To this end, we propose a novel approach, called Dual-Refinement, that jointly refines pseudo labels at the off-line clustering phase and features at the on-line training phase, to alternatively boost the label purity and feature discriminability in the target domain for more reliable re-ID. Specifically, at the off-line phase, a new hierarchical clustering scheme is proposed, which selects representative prototypes for every coarse cluster. Thus, labels can be effectively refined by using the inherent hierarchical information of person images. Besides, at the on-line phase, we propose an instant memory spread-out (IM-spread-out) regularization, that takes advantage of the proposed instant memory bank to store sample features of the entire dataset and enable spread-out feature learning over the entire training data instantly. Our Dual-Refinement method reduces the influence of noisy labels and refines the learned features within the alternative training process. Experiments demonstrate that our method outperforms the state-of-the-art methods by a large margin.},
  archive      = {J_TIP},
  author       = {Yongxing Dai and Jun Liu and Yan Bai and Zekun Tong and Ling-Yu Duan},
  doi          = {10.1109/TIP.2021.3104169},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7815-7829},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Dual-refinement: Joint label and feature refinement for unsupervised domain adaptive person re-identification},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Semi-supervised perception augmentation for aerial photo
topologies understanding. <em>TIP</em>, <em>30</em>, 7803–7814. (<a
href="https://doi.org/10.1109/TIP.2021.3079820">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intelligently understanding the sophisticated topological structures from aerial photographs is a useful technique in aerial image analysis. Conventional methods cannot fulfill this task due to the following challenges: 1) the topology number of an aerial photo increases exponentially with the topology size, which requires a fine-grained visual descriptor to discriminatively represent each topology; 2) identifying visually/semantically salient topologies within each aerial photo in a weakly-labeled context, owing to the unaffordable human resources required for pixel-level annotation; and 3) designing a cross-domain knowledge transferal module to augment aerial photo perception, since multi-resolution aerial photos are taken asynchronistically in practice. To handle the above problems, we propose a unified framework to understand aerial photo topologies, focusing on representing each aerial photo by a set of visually/semantically salient topologies based on human visual perception and further employing them for visual categorization. Specifically, we first extract multiple atomic regions from each aerial photo, and thereby graphlets are built to capture the each aerial photo topologically. Then, a weakly-supervised ranking algorithm selects a few semantically salient graphlets by seamlessly encoding multiple image-level attributes. Toward a visualizable and perception-aware framework, we construct gaze shifting path (GSP) by linking the top-ranking graphlets. Finally, we derive the deep GSP representation, and formulate a semi-supervised and cross-domain SVM to partition each aerial photo into multiple categories. The SVM utilizes the global composition from low-resolution counterparts to enhance the deep GSP features from high-resolution aerial photos which are partially-annotated. Extensive visualization results and categorization performance comparisons have demonstrated the competitiveness of our approach.},
  archive      = {J_TIP},
  author       = {Luming Zhang and Zhigeng Pan and Ling Shao},
  doi          = {10.1109/TIP.2021.3079820},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7803-7814},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Semi-supervised perception augmentation for aerial photo topologies understanding},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). GMNet: Graded-feature multilabel-learning network for
RGB-thermal urban scene semantic segmentation. <em>TIP</em>,
<em>30</em>, 7790–7802. (<a
href="https://doi.org/10.1109/TIP.2021.3109518">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic segmentation is a fundamental task in computer vision, and it has various applications in fields such as robotic sensing, video surveillance, and autonomous driving. A major research topic in urban road semantic segmentation is the proper integration and use of cross-modal information for fusion. Here, we attempt to leverage inherent multimodal information and acquire graded features to develop a novel multilabel-learning network for RGB-thermal urban scene semantic segmentation. Specifically, we propose a strategy for graded-feature extraction to split multilevel features into junior, intermediate, and senior levels. Then, we integrate RGB and thermal modalities with two distinct fusion modules, namely a shallow feature fusion module and deep feature fusion module for junior and senior features. Finally, we use multilabel supervision to optimize the network in terms of semantic, binary, and boundary characteristics. Experimental results confirm that the proposed architecture, the graded-feature multilabel-learning network, outperforms state-of-the-art methods for urban scene semantic segmentation, and it can be generalized to depth data.},
  archive      = {J_TIP},
  author       = {Wujie Zhou and Jinfu Liu and Jingsheng Lei and Lu Yu and Jenq-Neng Hwang},
  doi          = {10.1109/TIP.2021.3109518},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7790-7802},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {GMNet: Graded-feature multilabel-learning network for RGB-thermal urban scene semantic segmentation},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Salience-guided iterative asymmetric mutual hashing for fast
person re-identification. <em>TIP</em>, <em>30</em>, 7776–7789. (<a
href="https://doi.org/10.1109/TIP.2021.3109508">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person Re-identification (ReID) aims to retrieve the pedestrian with the same identity across different views. Existing studies mainly focus on improving accuracy, while ignoring their efficiency. Recently, several hash based methods have been proposed. Despite their improvement in efficiency, there still exists an unacceptable gap in accuracy between these methods and real-valued ones. Besides, few attempts have been made to simultaneously explicitly reduce redundancy and improve discrimination of hash codes, especially for short ones. Integrating Mutual learning may be a possible solution to reach this goal. However, it fails to utilize the complementary effect of teacher and student models. Additionally, it will degrade the performance of teacher models by treating two models equally. To address these issues, we propose a salience-guided iterative asymmetric mutual hashing (SIAMH) to achieve high-quality hash code generation and fast feature extraction. Specifically, a salience-guided self-distillation branch (SSB) is proposed to enable SIAMH to generate hash codes based on salience regions, thus explicitly reducing the redundancy between codes. Moreover, a novel iterative asymmetric mutual training strategy (IAMT) is proposed to alleviate drawbacks of common mutual learning, which can continuously refine the discriminative regions for SSB and extract regularized dark knowledge for two models as well. Extensive experiment results on five widely used datasets demonstrate the superiority of the proposed method in efficiency and accuracy when compared with existing state-of-the-art hashing and real-valued approaches. The code is released at https://github.com/Vill-Lab/SIAMH .},
  archive      = {J_TIP},
  author       = {Cairong Zhao and Yuanpeng Tu and Zhihui Lai and Fumin Shen and Heng Tao Shen and Duoqian Miao},
  doi          = {10.1109/TIP.2021.3109508},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7776-7789},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Salience-guided iterative asymmetric mutual hashing for fast person re-identification},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Multiscale spatio-temporal graph neural networks for 3D
skeleton-based motion prediction. <em>TIP</em>, <em>30</em>, 7760–7775.
(<a href="https://doi.org/10.1109/TIP.2021.3108708">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a multiscale spatio-temporal graph neural network (MST-GNN) to predict the future 3D skeleton-based human poses in an action-category-agnostic manner. The core of MST-GNN is a multiscale spatio-temporal graph that explicitly models the relations in motions at various spatial and temporal scales. Different from many previous hierarchical structures, our multiscale spatio-temporal graph is built in a data-adaptive fashion , which captures nonphysical, yet motion-based relations. The key module of MST-GNN is a multiscale spatio-temporal graph computational unit (MST-GCU) based on the trainable graph structure. MST-GCU embeds underlying features at individual scales and then fuses features across scales to obtain a comprehensive representation. The overall architecture of MST-GNN follows an encoder-decoder framework, where the encoder consists of a sequence of MST-GCUs to learn the spatial and temporal features of motions, and the decoder uses a graph-based attention gate recurrent unit (GA-GRU) to generate future poses. Extensive experiments are conducted to show that the proposed MST-GNN outperforms state-of-the-art methods in both short and long-term motion prediction on the datasets of Human 3.6M, CMU Mocap and 3DPW, where MST-GNN outperforms previous works by 5.33\% and 3.67\% of mean angle errors in average for short-term and long-term prediction on Human 3.6M, and by 11.84\% and 4.71\% of mean angle errors for short-term and long-term prediction on CMU Mocap, and by 1.13\% of mean angle errors on 3DPW in average, respectively. We further investigate the learned multiscale graphs for interpretability.},
  archive      = {J_TIP},
  author       = {Maosen Li and Siheng Chen and Yangheng Zhao and Ya Zhang and Yanfeng Wang and Qi Tian},
  doi          = {10.1109/TIP.2021.3108708},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7760-7775},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Multiscale spatio-temporal graph neural networks for 3D skeleton-based motion prediction},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptive streaming of 360 videos with perfect, imperfect,
and unknown FoV viewing probabilities in wireless networks.
<em>TIP</em>, <em>30</em>, 7744–7759. (<a
href="https://doi.org/10.1109/TIP.2021.3099741">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates adaptive streaming of one or multiple tiled 360 videos from a multi-antenna base station (BS) to one or multiple single-antenna users, respectively, in a multi-carrier wireless system. We aim to maximize the video quality while keeping rebuffering time small via encoding rate adaptation at each group of pictures (GOP) and transmission adaptation at each (transmission) slot. To capture the impact of field-of-view (FoV) prediction, we consider three cases of FoV viewing probability distributions, i.e., perfect, imperfect, and unknown FoV viewing probability distributions, and use the average total utility, worst average total utility, and worst total utility as the respective performance metrics. In the single-user scenario, we optimize the encoding rates of the tiles, encoding rates of the FoVs, and transmission beamforming vectors for all subcarriers to maximize the total utility in each case. In the multi-user scenario, we adopt rate splitting with successive decoding and optimize the encoding rates of the tiles, encoding rates of the FoVs, rates of the common and private messages, and transmission beamforming vectors for all subcarriers to maximize the total utility in each case. Then, we separate the challenging optimization problem into multiple tractable problems in each scenario. In the single-user scenario, we obtain a globally optimal solution of each problem using transformation techniques and the Karush-Kuhn-Tucker (KKT) conditions. In the multi-user scenario, we obtain a KKT point of each problem using the concave-convex procedure (CCCP). Finally, numerical results demonstrate that the proposed solutions achieve notable gains in quality, quality variation, and rebuffering time over existing schemes in all three cases. To the best of our knowledge, this is the first work revealing the impact of FoV prediction on the performance of adaptive streaming of tiled 360 videos.},
  archive      = {J_TIP},
  author       = {Lingzhi Zhao and Ying Cui and Zhi Liu and Yunfei Zhang and Sheng Yang},
  doi          = {10.1109/TIP.2021.3099741},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7744-7759},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Adaptive streaming of 360 videos with perfect, imperfect, and unknown FoV viewing probabilities in wireless networks},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Conversational image search. <em>TIP</em>, <em>30</em>,
7732–7743. (<a href="https://doi.org/10.1109/TIP.2021.3108724">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conversational image search, a revolutionary search mode, is able to interactively induce the user response to clarify their intents step by step. Several efforts have been dedicated to the conversation part, namely automatically asking the right question at the right time for user preference elicitation, while few studies focus on the image search part given the well-prepared conversational query. In this paper, we work towards conversational image search, which is much difficult compared to the traditional image search task, due to the following challenges: 1) understanding complex user intents from a multimodal conversational query; 2) utilizing multiform knowledge associated images from a memory network; and 3) enhancing the image representation with distilled knowledge. To address these problems, in this paper, we present a novel contextuaL imAge seaRch sCHeme (LARCH for short), consisting of three components. In the first component, we design a multimodal hierarchical graph-based neural network, which learns the conversational query embedding for better user intent understanding. As to the second one, we devise a multi-form knowledge embedding memory network to unify heterogeneous knowledge structures into a homogeneous base that greatly facilitates relevant knowledge retrieval. In the third component, we learn the knowledge-enhanced image representation via a novel gated neural network, which selects the useful knowledge from retrieved relevant one. Extensive experiments have shown that our LARCH yields significant performance over an extended benchmark dataset. As a side contribution, we have released the data, codes, and parameter settings to facilitate other researchers in the conversational image search community.},
  archive      = {J_TIP},
  author       = {Liqiang Nie and Fangkai Jiao and Wenjie Wang and Yinglong Wang and Qi Tian},
  doi          = {10.1109/TIP.2021.3108724},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7732-7743},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Conversational image search},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). RGB-d salient object detection with ubiquitous target
awareness. <em>TIP</em>, <em>30</em>, 7717–7731. (<a
href="https://doi.org/10.1109/TIP.2021.3108412">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conventional RGB-D salient object detection methods aim to leverage depth as complementary information to find the salient regions in both modalities. However, the salient object detection results heavily rely on the quality of captured depth data which sometimes are unavailable. In this work, we make the first attempt to solve the RGB-D salient object detection problem with a novel depth-awareness framework. This framework only relies on RGB data in the testing phase, utilizing captured depth data as supervision for representation learning. To construct our framework as well as achieving accurate salient detection results, we propose a Ubiquitous Target Awareness (UTA) network to solve three important challenges in RGB-D SOD task: 1) a depth awareness module to excavate depth information and to mine ambiguous regions via adaptive depth-error weights, 2) a spatial-aware cross-modal interaction and a channel-aware cross-level interaction, exploiting the low-level boundary cues and amplifying high-level salient channels, and 3) a gated multi-scale predictor module to perceive the object saliency in different contextual scales. Besides its high performance, our proposed UTA network is depth-free for inference and runs in real-time with 43 FPS. Experimental evidence demonstrates that our proposed network not only surpasses the state-of-the-art methods on five public RGB-D SOD benchmarks by a large margin, but also verifies its extensibility on five public RGB SOD benchmarks.},
  archive      = {J_TIP},
  author       = {Yifan Zhao and Jiawei Zhao and Jia Li and Xiaowu Chen},
  doi          = {10.1109/TIP.2021.3108412},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7717-7731},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {RGB-D salient object detection with ubiquitous target awareness},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Superpixels with content-adaptive criteria. <em>TIP</em>,
<em>30</em>, 7702–7716. (<a
href="https://doi.org/10.1109/TIP.2021.3108403">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Superpixels are widely used in computer vision applications. Most of the existing superpixel methods use established criteria to indiscriminately process all pixels, resulting in superpixel boundary adherence and regularity being unnecessarily inter-inhibitive. This study builds upon a previous work by proposing a new segmentation strategy that classifies image content into meaningful areas containing object boundaries and meaningless parts that include color-homogeneous and texture-rich regions. Based on this classification, we design two distinct criteria to process the pixels in different environments to achieve highly accurate superpixels in content-meaningful areas and keep the regularity of the superpixels in content-meaningless regions. Additionally, we add a group of weights when adopting the color feature, successfully reducing the undersegmentation error. The superior accuracy and the moderate compactness achieved by the proposed method in comparative experiments with several state-of-the-art methods indicate that the content-adaptive criteria efficiently reduce the compromise between boundary adherence and compactness.},
  archive      = {J_TIP},
  author       = {Ye Yuan and Wei Zhang and Hai Yu and Zhiliang Zhu},
  doi          = {10.1109/TIP.2021.3108403},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7702-7716},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Superpixels with content-adaptive criteria},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). TMMF: Temporal multi-modal fusion for single-stage
continuous gesture recognition. <em>TIP</em>, <em>30</em>, 7689–7701.
(<a href="https://doi.org/10.1109/TIP.2021.3108349">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gesture recognition is a much studied research area which has myriad real-world applications including robotics and human-machine interaction. Current gesture recognition methods have focused on recognising isolated gestures, and existing continuous gesture recognition methods are limited to two-stage approaches where independent models are required for detection and classification, with the performance of the latter being constrained by detection performance. In contrast, we introduce a single-stage continuous gesture recognition framework, called Temporal Multi-Modal Fusion (TMMF), that can detect and classify multiple gestures in a video via a single model. This approach learns the natural transitions between gestures and non-gestures without the need for a pre-processing segmentation step to detect individual gestures. To achieve this, we introduce a multi-modal fusion mechanism to support the integration of important information that flows from multi-modal inputs, and is scalable to any number of modes. Additionally, we propose Unimodal Feature Mapping (UFM) and Multi-modal Feature Mapping (MFM) models to map uni-modal features and the fused multi-modal features respectively. To further enhance performance, we propose a mid-point based loss function that encourages smooth alignment between the ground truth and the prediction, helping the model to learn natural gesture transitions. We demonstrate the utility of our proposed framework, which can handle variable-length input videos, and outperforms the state-of-the-art on three challenging datasets: EgoGesture, IPN hand and ChaLearn LAP Continuous Gesture Dataset (ConGD). Furthermore, ablation experiments show the importance of different components of the proposed framework.},
  archive      = {J_TIP},
  author       = {Harshala Gammulle and Simon Denman and Sridha Sridharan and Clinton Fookes},
  doi          = {10.1109/TIP.2021.3108349},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7689-7701},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {TMMF: Temporal multi-modal fusion for single-stage continuous gesture recognition},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). PoT-GAN: Pose transform GAN for person image synthesis.
<em>TIP</em>, <em>30</em>, 7677–7688. (<a
href="https://doi.org/10.1109/TIP.2021.3104183">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pose-based person image synthesis aims to generate a new image containing a person with a target pose conditioned on a source image containing a person with a specified pose. It is challenging as the target pose is arbitrary and often significantly differs from the specified source pose, which leads to large appearance discrepancy between the source and the target images. This paper presents the Pose Transform Generative Adversarial Network (PoT-GAN) for person image synthesis where the generator explicitly learns the transform between the two poses by manipulating the corresponding multi-scale feature maps. By incorporating the learned pose transform information into the multi-scale feature maps of the source image in a GAN architecture, our method reliably transfers the appearance of the person in the source image to the target pose with no need for any hard-coded spatial information depicting the change of pose. According to both qualitative and quantitative results, the proposed PoT-GAN demonstrates a state-of-the-art performance on three publicly available datasets for person image synthesis.},
  archive      = {J_TIP},
  author       = {Tianjiao Li and Wei Zhang and Ran Song and Zhiheng Li and Jun Liu and Xiaolei Li and Shijian Lu},
  doi          = {10.1109/TIP.2021.3104183},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7677-7688},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {PoT-GAN: Pose transform GAN for person image synthesis},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Person re-identification via attention pyramid.
<em>TIP</em>, <em>30</em>, 7663–7676. (<a
href="https://doi.org/10.1109/TIP.2021.3107211">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose an attention pyramid method for person re-identification. Unlike conventional attention-based methods which only learn a global attention map, our attention pyramid exploits the attention regions in a multi-scale manner because human attention varies with different scales. Our attention pyramid imitates the process of human visual perception which tends to notice the foreground person over the cluttered background, and further focus on the specific color of the shirt with close observation. Specifically, we describe our attention pyramid by a “split-attend-merge-stack” principle. We first split the features into multiple local parts and learn the corresponding attentions. Then, we merge local attentions and stack these merged attentions with the residual connection as an attention pyramid. The proposed attention pyramid is a lightweight plug-and-play module that can be applied to off-the-shelf models. We implement our attention pyramid method in two different attention mechanisms including: channel-wise attention and spatial attention. We evaluate our method on four large-scale person re-identification benchmarks including Market-1501, DukeMTMC, CUHK03, and MSMT17. Experimental results demonstrate the superiority of our method, which outperforms the state-of-the-art methods by a large margin with limited computationa cost. Code is available at https://github.com/CHENGY12/APNet .},
  archive      = {J_TIP},
  author       = {Guangyi Chen and Tianpei Gu and Jiwen Lu and Jin-An Bao and Jie Zhou},
  doi          = {10.1109/TIP.2021.3107211},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7663-7676},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Person re-identification via attention pyramid},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021c). Location sensitive network for human instance segmentation.
<em>TIP</em>, <em>30</em>, 7649–7662. (<a
href="https://doi.org/10.1109/TIP.2021.3107210">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Location is an important distinguishing information for instance segmentation. In this paper, we propose a novel model, called Location Sensitive Network (LSNet), for human instance segmentation. LSNet integrates instance-specific location information into one-stage segmentation framework. Specifically, in the segmentation branch, Pose Attention Module (PAM) encodes the location information into the attention regions through coordinates encoding. Based on the location information provided by PAM, the segmentation branch is able to effectively distinguish instances in feature-level. Moreover, we propose a combination operation named Keypoints Sensitive Combination (KSCom) to utilize the location information from multiple sampling points. These sampling points construct the points representation for instances via human keypoints and random points. Human keypoints provide the spatial locations and semantic information of the instances, and random points expand the receptive fields. Based on the points representation for each instance, KSCom effectively reduces the mis-classified pixels. Our method is validated by the experiments on public datasets. LSNet-5 achieves 56.2 mAP at 18.5 FPS on COCOPersons. Besides, the proposed method is significantly superior to its peers in the case of severe occlusion.},
  archive      = {J_TIP},
  author       = {Xiangzhou Zhang and Bingpeng Ma and Hong Chang and Shiguang Shan and Xilin Chen},
  doi          = {10.1109/TIP.2021.3107210},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7649-7662},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Location sensitive network for human instance segmentation},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). AAN-face: Attention augmented networks for face recognition.
<em>TIP</em>, <em>30</em>, 7636–7648. (<a
href="https://doi.org/10.1109/TIP.2021.3107238">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks are capable of extracting powerful representations for face recognition. However, they tend to suffer from poor generalization due to imbalanced data distributions where a small number of classes are over-represented (e.g. frontal or non-occluded faces) and some of the remaining rarely appear (e.g. profile or heavily occluded faces). This is the reason why the performance is dramatically degraded in minority classes. For example, this issue is serious for recognizing masked faces in the scenario of ongoing pandemic of the COVID-19. In this work, we propose an Attention Augmented Network, called AAN-Face, to handle this issue. First, an attention erasing (AE) scheme is proposed to randomly erase units in attention maps. This well prepares models towards occlusions or pose variations. Second, an attention center loss (ACL) is proposed to learn a center for each attention map, so that the same attention map focuses on the same facial part. Consequently, discriminative facial regions are emphasized, while useless or noisy ones are suppressed. Third, the AE and the ACL are incorporated to form the AAN-Face. Since the discriminative parts are randomly removed by the AE, the ACL is encouraged to learn different attention centers, leading to the localization of diverse and complementary facial parts. Comprehensive experiments on various test datasets, especially on masked faces, demonstrate that our AAN-Face models outperform the state-of-the-art methods, showing the importance and effectiveness.},
  archive      = {J_TIP},
  author       = {Qiangchang Wang and Guodong Guo},
  doi          = {10.1109/TIP.2021.3107238},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7636-7648},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {AAN-face: Attention augmented networks for face recognition},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-stream fusion network with generalized smooth l1 loss
for single image dehazing. <em>TIP</em>, <em>30</em>, 7620–7635. (<a
href="https://doi.org/10.1109/TIP.2021.3108022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single image dehazing is an important but challenging computer vision problem. For the problem, an end-to-end convolutional neural network, named multi-stream fusion network (MSFNet), is proposed in this paper. MSFNet is built following the encoder-decoder network structure. The encoder is a three-stream network to produce features at three resolution levels. Residual dense blocks (RDBs) are used for feature extraction. The resizing blocks serve as bridges to connect different streams. The features from different streams are fused in a full connection manner by a feature fusion block, with stream-wise and channel-wise attention mechanisms. The decoder directly regresses the dehazed image from coarse to fine by the use of RDBs and the skip connections. To train the network, we design a generalized smooth $\mathcal {L}_{1}$ loss function, which is a parametric loss family and permits to adjust the insensitivity to the outliers by varying the parameter settings. Moreover, to guide MSFNet to capture the valid features in each stream, we propose the multi-scale supervision learning strategy, where the loss at each resolution level is computed and summed as the final loss. Extensive experimental results demonstrate that the proposed MSFNet achieves superior performance on both synthetic and real-world images, as compared with the state-of-the-art single image dehazing methods.},
  archive      = {J_TIP},
  author       = {Xinshan Zhu and Shuoshi Li and Yongdong Gan and Yun Zhang and Biao Sun},
  doi          = {10.1109/TIP.2021.3108022},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7620-7635},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Multi-stream fusion network with generalized smooth l1 loss for single image dehazing},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dual attention-in-attention model for joint rain streak and
raindrop removal. <em>TIP</em>, <em>30</em>, 7608–7619. (<a
href="https://doi.org/10.1109/TIP.2021.3108019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rain streaks and raindrops are two natural phenomena, which degrade image capture in different ways. Currently, most existing deep deraining networks take them as two distinct problems and individually address one, and thus cannot deal adequately with both simultaneously. To address this, we propose a Dual Attention-in-Attention Model (DAiAM) which includes two DAMs for removing both rain streaks and raindrops. Inside the DAM, there are two attentive maps - each of which attends to the heavy and light rainy regions, respectively, to guide the deraining process differently for applicable regions. In addition, to further refine the result, a Differential-driven Dual Attention-in-Attention Model (D-DAiAM) is proposed with a “heavy-to-light” scheme to remove rain via addressing the unsatisfying deraining regions. Extensive experiments on one public raindrop dataset, one public rain streak and our synthesized joint rain streak and raindrop (JRSRD) dataset have demonstrated that the proposed method not only is capable of removing rain streaks and raindrops simultaneously, but also achieves the state-of-the-art performance on both tasks.},
  archive      = {J_TIP},
  author       = {Kaihao Zhang and Dongxu Li and Wenhan Luo and Wenqi Ren},
  doi          = {10.1109/TIP.2021.3108019},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7608-7619},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Dual attention-in-attention model for joint rain streak and raindrop removal},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Visual explanation for deep metric learning. <em>TIP</em>,
<em>30</em>, 7593–7607. (<a
href="https://doi.org/10.1109/TIP.2021.3107214">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work explores the visual explanation for deep metric learning and its applications. As an important problem for learning representation, metric learning has attracted much attention recently, while the interpretation of the metric learning model is not as well-studied as classification. To this end, we propose an intuitive idea to show where contributes the most to the overall similarity of two input images by decomposing the final activation. Instead of only providing the overall activation map of each image, we propose to generate point-to-point activation intensity between two images so that the relationship between different regions is uncovered. We show that the proposed framework can be directly applied to a wide range of metric learning applications and provides valuable information for model understanding. Both theoretical and empirical analyses are provided to demonstrate the superiority of the proposed overall activation map over existing methods. Furthermore, our experiments validate the effectiveness of the proposed point-specific activation map on two applications, i.e. cross-view pattern discovery and interactive retrieval. Code is available at https://github.com/Jeff-Zilence/Explain_Metric_Learning},
  archive      = {J_TIP},
  author       = {Sijie Zhu and Taojiannan Yang and Chen Chen},
  doi          = {10.1109/TIP.2021.3107214},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7593-7607},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Visual explanation for deep metric learning},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Geometry auxiliary salient object detection for light
fields via graph neural networks. <em>TIP</em>, <em>30</em>, 7578–7592.
(<a href="https://doi.org/10.1109/TIP.2021.3108018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Light field imaging, originated from the availability of light field capture technology, offers a wide range of applications in the field of computational vision. The capability of predicting salient objects of light fields remains technologically challenging due to its complicated geometry structure. In this paper, we propose a light field salient object detection approach that formulates the geometric coherence among multiple views of light fields as graphs, where the angular/central views represent the nodes and their relations compose the edges. The spatial and disparity correlations between multiple views are effectively explored through multi-scale graph neural networks, enabling the more comprehensive understanding of light field content and more representative and discriminative saliency features generation. Moreover, a multi-scale saliency feature consistency learning module is embedded to enhance the saliency features. Finally, an accurate salient object map is produced for the light field based upon the extracted features. In addition, we establish a new light field salient object detection dataset (CITYU-Lytro) that contains 817 light fields with diverse contents and their corresponding annotations, aiming to further promote the research on light field salient object detection. Quantitative and qualitative experiments demonstrate that the proposed method performs favorably compared with the state-of-the-art methods on the benchmark datasets.},
  archive      = {J_TIP},
  author       = {Qiudan Zhang and Shiqi Wang and Xu Wang and Zhenhao Sun and Sam Kwong and Jianmin Jiang},
  doi          = {10.1109/TIP.2021.3108018},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7578-7592},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Geometry auxiliary salient object detection for light fields via graph neural networks},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hierarchical edge refinement network for saliency detection.
<em>TIP</em>, <em>30</em>, 7567–7577. (<a
href="https://doi.org/10.1109/TIP.2021.3106798">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {At present, most saliency detection methods are based on fully convolutional neural networks (FCNs). However, FCNs usually blur the edges of salient objects. Due to that, the multiple convolution and pooling operations of the FCNs will limit the spatial resolution of the feature maps. To alleviate this issue and obtain accurate edges, we propose a hierarchical edge refinement network (HERNet) for accurate saliency detection. In detail, the HERNet is mainly composed of a saliency prediction network and an edge preserving network. Firstly, the saliency prediction network is used to roughly detect the regions of salient objects and is based on a modified U-Net structure. Then, the edge preserving network is used to accurately detect the edges of salient objects, and this network is mainly composed of the atrous spatial pyramid pooling (ASPP) module. Different from the previous indiscriminate supervision strategy, we adopt a new one-to-one hierarchical supervision strategy to supervise the different outputs of the entire network. Experimental results on five traditional benchmark datasets demonstrate that the proposed HERNet performs well when compared with the state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Dawei Song and Yongsheng Dong and Xuelong Li},
  doi          = {10.1109/TIP.2021.3106798},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7567-7577},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Hierarchical edge refinement network for saliency detection},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning transformation-invariant local descriptors with
low-coupling binary codes. <em>TIP</em>, <em>30</em>, 7554–7566. (<a
href="https://doi.org/10.1109/TIP.2021.3106805">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the great success achieved by prevailing binary local descriptors, they are still suffering from two problems: 1) vulnerable to the geometric transformations; 2) lack of an effective treatment to the highly-correlated bits that are generated by directly applying the scheme of image hashing. To tackle both limitations, we propose an unsupervised Transformation-invariant Binary Local Descriptor learning method (TBLD). Specifically, the transformation invariance of binary local descriptors is ensured by projecting the original patches and their transformed counterparts into an identical high-dimensional feature space and an identical low-dimensional descriptor space simultaneously. Meanwhile, it enforces the dissimilar image patches to have distinctive binary local descriptors. Moreover, to reduce high correlations between bits, we propose a bottom-up learning strategy, termed Adversarial Constraint Module , where low-coupling binary codes are introduced externally to guide the learning of binary local descriptors. With the aid of the Wasserstein loss, the framework is optimized to encourage the distribution of the generated binary local descriptors to mimic that of the introduced low-coupling binary codes, eventually making the former more low-coupling. Experimental results on three benchmark datasets well demonstrate the superiority of the proposed method over the state-of-the-art methods. The project page is available at https://github.com/yoqim/TBLD .},
  archive      = {J_TIP},
  author       = {Yunqi Miao and Zijia Lin and Xiao Ma and Guiguang Ding and Jungong Han},
  doi          = {10.1109/TIP.2021.3106805},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7554-7566},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Learning transformation-invariant local descriptors with low-coupling binary codes},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). PSAT-GAN: Efficient adversarial attacks against holistic
scene understanding. <em>TIP</em>, <em>30</em>, 7541–7553. (<a
href="https://doi.org/10.1109/TIP.2021.3106807">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in deep neural networks (DNNs) have facilitated high-end applications, including holistic scene understanding (HSU), in which many tasks run in parallel with the same visual input. Following this trend, various methods have been proposed to use DNNs to perform multiple vision tasks. However, these methods are task-specific and less effective when considering multiple HSU tasks. End-to-end demonstrations of adversarial examples, which generate one-to-many heterogeneous adversarial examples in parallel from the same input, are scarce. Additionally, one-to-many mapping of adversarial examples for HSU usually requires joint representation learning and flexible constraints on magnitude, which can render the prevalent attack methods ineffective. In this paper, we propose PSAT-GAN, an end-to-end framework that follows the pipeline of HSU. It is based on a mixture of generative models and an adversarial classifier that employs partial weight sharing to learn a one-to-many mapping of adversarial examples in parallel, each of which is effective for its corresponding task in HSU attacks. PSAT-GAN is further enhanced by applying novel adversarial and soft-constraint losses to generate effective perturbations and avoid studying transferability. Experimental results indicate that our method is efficient in generating both universal and image-dependent adversarial examples to fool HSU tasks under either targeted or non-targeted settings.},
  archive      = {J_TIP},
  author       = {Lin Wang and Kuk-Jin Yoon},
  doi          = {10.1109/TIP.2021.3106807},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7541-7553},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {PSAT-GAN: Efficient adversarial attacks against holistic scene understanding},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Image denoising based on fractional gradient vector flow and
overlapping group sparsity as priors. <em>TIP</em>, <em>30</em>,
7527–7540. (<a href="https://doi.org/10.1109/TIP.2021.3104181">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a new regularization term in the form of L1-norm based fractional gradient vector flow (LF-GGVF) is presented for the task of image denoising. A fractional order variational method is formulated, which is then utilized for estimating the proposed LF-GGVF. Overlapping group sparsity along with LF-GGVF is used as priors in image denoising optimization framework. The Riemann-Liouville derivative is used for approximating the fractional order derivatives present in the optimization framework. Its role in the framework helps in boosting the denoising performance. The numerical optimization is performed in an alternating manner using the well-known alternating direction method of multipliers (ADMM) and split Bregman techniques. The resulting system of linear equations is then solved using an efficient numerical scheme. A variety of simulated data that includes test images contaminated by additive white Gaussian noise are used for experimental validation. The results of numerical solutions obtained from experimental work demonstrate that the performance of the proposed approach in terms of noise suppression and edge preservation is better when compared with that of several other methods.},
  archive      = {J_TIP},
  author       = {Ahlad Kumar and M. Omair Ahmad and M. N. S. Swamy},
  doi          = {10.1109/TIP.2021.3104181},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7527-7540},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Image denoising based on fractional gradient vector flow and overlapping group sparsity as priors},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Predicting the quality of compressed videos with
pre-existing distortions. <em>TIP</em>, <em>30</em>, 7511–7526. (<a
href="https://doi.org/10.1109/TIP.2021.3107213">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Because of the increasing ease of video capture, many millions of consumers create and upload large volumes of User-Generated-Content (UGC) videos to social and streaming media sites over the Internet. UGC videos are commonly captured by naive users having limited skills and imperfect techniques, and tend to be afflicted by mixtures of highly diverse in-capture distortions. These UGC videos are then often uploaded for sharing onto cloud servers, where they are further compressed for storage and transmission. Our paper tackles the highly practical problem of predicting the quality of compressed videos (perhaps during the process of compression, to help guide it), with only (possibly severely) distorted UGC videos as references. To address this problem, we have developed a novel Video Quality Assessment (VQA) framework that we call 1stepVQA (to distinguish it from two-step methods that we discuss). 1stepVQA overcomes limitations of Full-Reference, Reduced-Reference and No-Reference VQA models by exploiting the statistical regularities of both natural videos and distorted videos. We also describe a new dedicated video database, which was created by applying a realistic VMAF-Guided perceptual rate distortion optimization (RDO) criterion to create realistically compressed versions of UGC source videos, which typically have pre-existing distortions. We show that 1stepVQA is able to more accurately predict the quality of compressed videos, given imperfect reference videos, and outperforms other VQA models in this scenario.},
  archive      = {J_TIP},
  author       = {Xiangxu Yu and Neil Birkbeck and Yilin Wang and Christos G. Bampis and Balu Adsumilli and Alan C. Bovik},
  doi          = {10.1109/TIP.2021.3107213},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7511-7526},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Predicting the quality of compressed videos with pre-existing distortions},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Spatial-aware texture transformer for high-fidelity garment
transfer. <em>TIP</em>, <em>30</em>, 7499–7510. (<a
href="https://doi.org/10.1109/TIP.2021.3107235">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Garment transfer aims to transfer the desired garment from a model image with the desired clothing to a target person, which has attracted a great deal of attention due to its wider potential applications. However, considering the model and target persons are often given at different views, body shapes and poses, realistic garment transfer is facing the following challenges that have not been well addressed: 1) deforming the garment; 2) inferring unobserved appearance; 3) preserving fine texture details. To tackle these challenges, we propose a novel SPatial-Aware Texture Transformer (SPATT) model. Different from existing models, SPATT establishes correspondence and infers unobserved clothing appearance by leveraging the spatial prior information of a UV-space. Specifically, the source image is transformed into a partial UV texture map guided by the extracted dense pose. To better infer the unseen appearance utilizing seen region, we first propose a novel coordinate-prior map that defines the spatial relationship between the coordinates in the UV texture map, and design an algorithm to compute it. Based on the proposed coordinate-prior map, we present a novel spatial-aware texture generation network to complete the partial UV texture. In the second stage, we first transform the completed UV texture to fit the target person. To polish the details and improve realism, we introduce a refinement generative network conditioned on the warped image and source input. Compared with existing frameworks as shown experimentally, the proposed framework can generate more realistic images with better-preserved texture details. Furthermore, difficult cases where two persons have large pose and view differences can also be well handled by SPATT.},
  archive      = {J_TIP},
  author       = {Ting Liu and Jianfeng Zhang and Xuecheng Nie and Yunchao Wei and Shikui Wei and Yao Zhao and Jiashi Feng},
  doi          = {10.1109/TIP.2021.3107235},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7499-7510},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Spatial-aware texture transformer for high-fidelity garment transfer},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). L3DOC: Lifelong 3D object classification. <em>TIP</em>,
<em>30</em>, 7486–7498. (<a
href="https://doi.org/10.1109/TIP.2021.3106799">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D object classification has been widely applied in both academic and industrial scenarios. However, most state-of-the-art algorithms rely on a fixed object classification task set, which cannot tackle the scenario when a new 3D object classification task is coming. Meanwhile, the existing lifelong learning models can easily destroy the learned tasks performance, due to the unordered, large-scale, and irregular 3D geometry data. To address these challenges, we propose a L ifelong 3D O bject C lassification ( i.e., L3DOC) model, which can consecutively learn new 3D object classification tasks via imitating “human learning”. More specifically, the core idea of our model is to capture and store the cross-task common knowledge of 3D geometry data in a 3D neural network, named as point-knowledge , through employing layer-wise point-knowledge factorization architecture. Afterwards, a task-relevant knowledge distillation mechanism is employed to connect the current task to previous relevant tasks and effectively prevent catastrophic forgetting . It consists of a point-knowledge distillation module and a transforming-space distillation module, which transfers the accumulated point-knowledge from previous tasks and soft-transfers the compact factorized representations of the transforming-space, respectively. To our best knowledge, the proposed L3DOC algorithm is the first attempt to perform deep learning on 3D object classification tasks in a lifelong learning way. Extensive experiments on several point cloud benchmarks illustrate the superiority of our L3DOC model over the state-of-the-art lifelong learning methods.},
  archive      = {J_TIP},
  author       = {Yuyang Liu and Yang Cong and Gan Sun and Tao Zhang and Jiahua Dong and Hongsen Liu},
  doi          = {10.1109/TIP.2021.3106799},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7486-7498},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {L3DOC: Lifelong 3D object classification},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Unsharp mask guided filtering. <em>TIP</em>, <em>30</em>,
7472–7485. (<a href="https://doi.org/10.1109/TIP.2021.3106812">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of this paper is guided image filtering, which emphasizes the importance of structure transfer during filtering by means of an additional guidance image. Where classical guided filters transfer structures using hand-designed functions, recent guided filters have been considerably advanced through parametric learning of deep networks. The state-of-the-art leverages deep networks to estimate the two core coefficients of the guided filter. In this work, we posit that simultaneously estimating both coefficients is suboptimal, resulting in halo artifacts and structure inconsistencies. Inspired by unsharp masking, a classical technique for edge enhancement that requires only a single coefficient, we propose a new and simplified formulation of the guided filter. Our formulation enjoys a filtering prior from a low-pass filter and enables explicit structure transfer by estimating a single coefficient. Based on our proposed formulation, we introduce a successive guided filtering network, which provides multiple filtering results from a single network, allowing for a trade-off between accuracy and efficiency. Extensive ablations, comparisons and analysis show the effectiveness and efficiency of our formulation and network, resulting in state-of-the-art results across filtering tasks like upsampling, denoising, and cross-modality filtering. Code is available at https://github.com/shizenglin/Unsharp-Mask-Guided-Filtering.},
  archive      = {J_TIP},
  author       = {Zenglin Shi and Yunlu Chen and Efstratios Gavves and Pascal Mettes and Cees G. M. Snoek},
  doi          = {10.1109/TIP.2021.3106812},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7472-7485},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Unsharp mask guided filtering},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Urban scene LOD vectorized modeling from photogrammetry
meshes. <em>TIP</em>, <em>30</em>, 7458–7471. (<a
href="https://doi.org/10.1109/TIP.2021.3106811">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Urban scene modeling is a challenging task for the photogrammetry and computer vision community due to its large scale, structural complexity, and topological delicacy. This paper presents an efficient multistep modeling framework for large-scale urban scenes from aerial images. It takes aerial images and a textured 3D mesh model generated by an image-based modeling system as the input and outputs compact polygon models with semantics at different levels of detail (LODs). Based on the key observation that urban buildings usually have piecewise planar rooftops and vertical walls, we propose a segment-based modeling method, which consists of three major stages: scene segmentation, roof contour extraction, and building modeling. By combining the deep neural network predictions with geometric constraints of the 3D mesh, the scene is first segmented into three classes. Then, for each building mesh, the 2D line segments are detected and used to slice the ground into polygon cells, followed by assigning each cell a roof plane via a MRF optimization. Finally, the LOD model is obtained by extruding cells to their corresponding planes. Compared with direct modeling in 3D space, we transform the mesh into a uniform 2D image grid representation and most of the modeling work is performed in 2D space, which has the advantages of low computational complexity and high robustness. In addition, our method doesn&#39;t require any global prior, such as the Manhattan or Atlanta world assumption, making it flexible to model scenes with different characteristics and complexity. Experiments on both single buildings and large-scale urban scenes demonstrate that by combining 2D photometric with 3D geometric information, the proposed algorithm is robust and efficient in urban scene LOD vectorized modeling compared with the state-of-the-art approaches.},
  archive      = {J_TIP},
  author       = {Jiali Han and Lingjie Zhu and Xiang Gao and Zhanyi Hu and Liyang Zhou and Hongmin Liu and Shuhan Shen},
  doi          = {10.1109/TIP.2021.3106811},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7458-7471},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Urban scene LOD vectorized modeling from photogrammetry meshes},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). ST-GREED: Space-time generalized entropic differences for
frame rate dependent video quality prediction. <em>TIP</em>,
<em>30</em>, 7446–7457. (<a
href="https://doi.org/10.1109/TIP.2021.3106801">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of conducting frame rate dependent video quality assessment (VQA) on videos of diverse frame rates, including high frame rate (HFR) videos. More generally, we study how perceptual quality is affected by frame rate, and how frame rate and compression combine to affect perceived quality. We devise an objective VQA model called Space-Time GeneRalized Entropic Difference (GREED) which analyzes the statistics of spatial and temporal band-pass video coefficients. A generalized Gaussian distribution (GGD) is used to model band-pass responses, while entropy variations between reference and distorted videos under the GGD model are used to capture video quality variations arising from frame rate changes. The entropic differences are calculated across multiple temporal and spatial subbands, and merged using a learned regressor. We show through extensive experiments that GREED achieves state-of-the-art performance on the LIVE-YT-HFR Database when compared with existing VQA models. The features used in GREED are highly generalizable and obtain competitive performance even on standard, non-HFR VQA databases. The implementation of GREED has been made available online: https://github.com/pavancm/GREED.},
  archive      = {J_TIP},
  author       = {Pavan C. Madhusudana and Neil Birkbeck and Yilin Wang and Balu Adsumilli and Alan C. Bovik},
  doi          = {10.1109/TIP.2021.3106801},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7446-7457},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {ST-GREED: Space-time generalized entropic differences for frame rate dependent video quality prediction},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Stimuli-aware visual emotion analysis. <em>TIP</em>,
<em>30</em>, 7432–7445. (<a
href="https://doi.org/10.1109/TIP.2021.3106813">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual emotion analysis (VEA) has attracted great attention recently, due to the increasing tendency of expressing and understanding emotions through images on social networks. Different from traditional vision tasks, VEA is inherently more challenging since it involves a much higher level of complexity and ambiguity in human cognitive process. Most of the existing methods adopt deep learning techniques to extract general features from the whole image, disregarding the specific features evoked by various emotional stimuli. Inspired by the Stimuli-Organism-Response (S-O-R) emotion model in psychological theory, we proposed a stimuli-aware VEA method consisting of three stages, namely stimuli selection (S), feature extraction (O) and emotion prediction (R). First, specific emotional stimuli ( i . e ., color, object, face) are selected from images by employing the off-the-shelf tools. To the best of our knowledge, it is the first time to introduce stimuli selection process into VEA in an end-to-end network. Then, we design three specific networks, i . e ., Global-Net, Semantic-Net and Expression-Net, to extract distinct emotional features from different stimuli simultaneously. Finally, benefiting from the inherent structure of Mikel’s wheel, we design a novel hierarchical cross-entropy loss to distinguish hard false examples from easy ones in an emotion-specific manner. Experiments demonstrate that the proposed method consistently outperforms the state-of-the-art approaches on four public visual emotion datasets. Ablation study and visualizations further prove the validity and interpretability of our method.},
  archive      = {J_TIP},
  author       = {Jingyuan Yang and Jie Li and Xiumei Wang and Yuxuan Ding and Xinbo Gao},
  doi          = {10.1109/TIP.2021.3106813},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7432-7445},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Stimuli-aware visual emotion analysis},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep dense multi-scale network for snow removal using
semantic and depth priors. <em>TIP</em>, <em>30</em>, 7419–7431. (<a
href="https://doi.org/10.1109/TIP.2021.3104166">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Images captured in snowy days suffer from noticeable degradation of scene visibility, which degenerates the performance of current vision-based intelligent systems. Removing snow from images thus is an important topic in computer vision. In this paper, we propose a Deep Dense Multi-Scale Network (DDMSNet) for snow removal by exploiting semantic and depth priors. As images captured in outdoor often share similar scenes and their visibility varies with depth from camera, such semantic and depth information provides a strong prior for snowy image restoration. We incorporate the semantic and depth maps as input and learn the semantic-aware and geometry-aware representation to remove snow. In particular, we first create a coarse network to remove snow from the input images. Then, the coarsely desnowed images are fed into another network to obtain the semantic and depth labels. Finally, we design a DDMSNet to learn semantic-aware and geometry-aware representation via a self-attention mechanism to produce the final clean images. Experiments evaluated on public synthetic and real-world snowy images verify the superiority of the proposed method, offering better results both quantitatively and qualitatively. https://github.com/HDCVLab/Deep-Dense-Multi-scale-Network https://github.com/HDCVLab/Deep-Dense-Multi-scale-Network .},
  archive      = {J_TIP},
  author       = {Kaihao Zhang and Rongqing Li and Yanjiang Yu and Wenhan Luo and Changsheng Li},
  doi          = {10.1109/TIP.2021.3104166},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7419-7431},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Deep dense multi-scale network for snow removal using semantic and depth priors},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Rain-free and residue hand-in-hand: A progressive coupled
network for real-time image deraining. <em>TIP</em>, <em>30</em>,
7404–7418. (<a href="https://doi.org/10.1109/TIP.2021.3102504">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rainy weather is a challenge for many vision-oriented tasks ( e.g. , object detection and segmentation), which causes performance degradation. Image deraining is an effective solution to avoid performance drop of downstream vision tasks. However, most existing deraining methods either fail to produce satisfactory restoration results or cost too much computation. In this work, considering both effectiveness and efficiency of image deraining, we propose a progressive coupled network (PCNet) to well separate rain streaks while preserving rain-free details. To this end, we investigate the blending correlations between them and particularly devise a novel coupled representation module (CRM) to learn the joint features and the blending correlations. By cascading multiple CRMs, PCNet extracts the hierarchical features of multi-scale rain streaks, and separates the rain-free content and rain streaks progressively. To promote computation efficiency, we employ depth-wise separable convolutions and a U-shaped structure, and construct CRM in an asymmetric architecture to reduce model parameters and memory footprint. Extensive experiments are conducted to evaluate the efficacy of the proposed PCNet in two aspects: (1) image deraining on several synthetic and real-world rain datasets and (2) joint image deraining and downstream vision tasks ( e.g. , object detection and segmentation). Furthermore, we show that the proposed CRM can be easily adopted to similar image restoration tasks including image dehazing and low-light enhancement with competitive performance. The source code is available at https://github.com/kuijiang0802/PCNet .},
  archive      = {J_TIP},
  author       = {Kui Jiang and Zhongyuan Wang and Peng Yi and Chen Chen and Zheng Wang and Xiao Wang and Junjun Jiang and Chia-Wen Lin},
  doi          = {10.1109/TIP.2021.3102504},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7404-7418},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Rain-free and residue hand-in-hand: A progressive coupled network for real-time image deraining},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Kinship verification based on cross-generation feature
interaction learning. <em>TIP</em>, <em>30</em>, 7391–7403. (<a
href="https://doi.org/10.1109/TIP.2021.3104192">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Kinship verification from facial images has been recognized as an emerging yet challenging technique in many potential computer vision applications. In this paper, we propose a novel cross-generation feature interaction learning (CFIL) framework for robust kinship verification. Particularly, an effective collaborative weighting strategy is constructed to explore the characteristics of cross-generation relations by corporately extracting features of both parents and children image pairs. Specifically, we take parents and children as a whole to extract the expressive local and non-local features. Different from the traditional works measuring similarity by distance, we interpolate the similarity calculations as the interior auxiliary weights into the deep CNN architecture to learn the whole and natural features. These similarity weights not only involve corresponding single points but also excavate the multiple relationships cross points, where local and non-local features are calculated by using these two kinds of distance measurements. Importantly, instead of separately conducting similarity computation and feature extraction, we integrate similarity learning and feature extraction into one unified learning process. The integrated representations deduced from local and non-local features can comprehensively express the informative semantics embedded in images and preserve abundant correlation knowledge from image pairs. Extensive experiments demonstrate the efficiency and superiority of the proposed model compared to some state-of-the-art kinship verification methods.},
  archive      = {J_TIP},
  author       = {Guan-Nan Dong and Chi-Man Pun and Zheng Zhang},
  doi          = {10.1109/TIP.2021.3104192},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7391-7403},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Kinship verification based on cross-generation feature interaction learning},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Real-time deep image retouching based on learnt semantics
dependent global transforms. <em>TIP</em>, <em>30</em>, 7378–7390. (<a
href="https://doi.org/10.1109/TIP.2021.3104173">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although artists&#39; actions in photo retouching appear to be highly nonlinear in nature and very difficult to characterize analytically, we find that the net effects of interactively editing a mundane image to a desired appearance can be modeled, in most cases, by a parametric monotonically non-decreasing global tone mapping function in the luminance axis and by a global affine transform in the chrominance plane that are weighted by saliency. This allows us to simplify the machine learning problem of mimicking artists in photo retouching to constructing a deep artful image transform (DAIT) using convolutional neural networks (CNN). The CNN design of DAIT aims to learn the image-dependent parameters of the luminance tone mapping function and the affine chrominance transform, rather than learning the end-to-end pixel level mapping as in the mainstream methods of image restoration and enhancement. The proposed DAIT approach reduces the computation complexity of the neural network by two orders of magnitude, which also, as a side benefit, improves the robustness and generalization capability at the inference stage. The high throughput and robustness of DAIT lend itself readily to real-time video enhancement as well after a simple temporal processing. Experiments and a Turing-type test are conducted to evaluate the proposed method and its competitors.},
  archive      = {J_TIP},
  author       = {Qifan Gao and Xiaolin Wu},
  doi          = {10.1109/TIP.2021.3104173},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7378-7390},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Real-time deep image retouching based on learnt semantics dependent global transforms},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Cross-dataset point cloud recognition using deep-shallow
domain adaptation network. <em>TIP</em>, <em>30</em>, 7364–7377. (<a
href="https://doi.org/10.1109/TIP.2021.3092818">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we propose a new two-view domain adaptation network named Deep-Shallow Domain Adaptation Network (DSDAN) for 3D point cloud recognition. Different from the traditional 2D image recognition task, the valuable texture information is often absent in point cloud data, making point cloud recognition a challenging task, especially in the cross-dataset scenario where the training and testing data exhibit a considerable distribution mismatch. In our DSDAN method, we tackle the challenging cross-dataset 3D point cloud recognition task from two aspects. On one hand, we propose a two-view learning framework, such that we can effectively leverage multiple feature representations to improve the recognition performance. To this end, we propose a simple and efficient Bag-of-Points feature method, as a complementary view to the deep representation. Moreover, we also propose a cross view consistency loss to boost the two-view learning framework. On the other hand, we further propose a two-level adaptation strategy to effectively address the domain distribution mismatch issue. Specifically, we apply a feature-level distribution alignment module for each view, and also propose an instance-level adaptation approach to select highly confident pseudo-labeled target samples for adapting the model to the target domain, based on which a co-training scheme is used to integrate the learning and adaptation process on the two views. Extensive experiments on the benchmark dataset show that our newly proposed DSDAN method outperforms the existing state-of-the-art methods for the cross-dataset point cloud recognition task.},
  archive      = {J_TIP},
  author       = {Feiyu Wang and Wen Li and Dong Xu},
  doi          = {10.1109/TIP.2021.3092818},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7364-7377},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Cross-dataset point cloud recognition using deep-shallow domain adaptation network},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). 3D-guided face manipulation of 2D images for the prediction
of post-operative outcome after cranio-maxillofacial surgery.
<em>TIP</em>, <em>30</em>, 7349–7363. (<a
href="https://doi.org/10.1109/TIP.2021.3096081">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cranio-maxillofacial surgery often alters the aesthetics of the face which can be a heavy burden for patients to decide whether or not to undergo surgery. Today, physicians can predict the post-operative face using surgery planning tools to support the patient’s decision-making. While these planning tools allow a simulation of the post-operative face, the facial texture must usually be captured by another 3D texture scan and subsequently mapped on the simulated face. This approach often results in face predictions that do not appear realistic or lively looking and are therefore ill-suited to guide the patient’s decision-making. Instead, we propose a method using a generative adversarial network to modify a facial image according to a 3D soft-tissue estimation of the post-operative face. To circumvent the lack of available data pairs between pre- and post-operative measurements we propose a semi-supervised training strategy using cycle losses that only requires paired open-source data of images and 3D surfaces of the face’s shape. After training on “in-the-wild” images we show that our model can realistically manipulate local regions of a face in a 2D image based on a modified 3D shape. We then test our model on four clinical examples where we predict the post-operative face according to a 3D soft-tissue prediction of surgery outcome, which was simulated by a surgery planning tool. As a result, we aim to demonstrate the potential of our approach to predict realistic post-operative images of faces without the need of paired clinical data, physical models, or 3D texture scans.},
  archive      = {J_TIP},
  author       = {Robin Andlauer and Andreas Wachter and Matthias Schaufelberger and Frederic Weichel and Reinald Kühle and Christian Freudlsperger and Werner Nahm},
  doi          = {10.1109/TIP.2021.3096081},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7349-7363},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {3D-guided face manipulation of 2D images for the prediction of post-operative outcome after cranio-maxillofacial surgery},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Extremely lightweight skeleton-based action recognition with
ShiftGCN++. <em>TIP</em>, <em>30</em>, 7333–7348. (<a
href="https://doi.org/10.1109/TIP.2021.3104182">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In skeleton-based action recognition, graph convolutional networks (GCNs) have achieved remarkable success. However, there are two shortcomings of current GCN-based methods. Firstly, the computation cost is pretty heavy, typically over 15 GFLOPs for one action sample. Some recent works even reach ~100 GFLOPs. Secondly, the receptive fields of both spatial graph and temporal graph are inflexible. Although recent works introduce incremental adaptive modules to enhance the expressiveness of spatial graph, their efficiency is still limited by regular GCN structures. In this paper, we propose a shift graph convolutional network (ShiftGCN) to overcome both shortcomings. ShiftGCN is composed of novel shift graph operations and lightweight point-wise convolutions, where the shift graph operations provide flexible receptive fields for both spatial graph and temporal graph. To further boost the efficiency, we introduce four techniques and build a more lightweight skeleton-based action recognition model named ShiftGCN++. ShiftGCN++ is an extremely computation-efficient model, which is designed for low-power and low-cost devices with very limited computing power. On three datasets for skeleton-based action recognition, ShiftGCN notably exceeds the state-of-the-art methods with over 10× less FLOPs and 4× practical speedup. ShiftGCN++ further boosts the efficiency of ShiftGCN, which achieves comparable performance with 6× less FLOPs and 2× practical speedup.},
  archive      = {J_TIP},
  author       = {Ke Cheng and Yifan Zhang and Xiangyu He and Jian Cheng and Hanqing Lu},
  doi          = {10.1109/TIP.2021.3104182},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7333-7348},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Extremely lightweight skeleton-based action recognition with ShiftGCN++},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). TPSSI-net: Fast and enhanced two-path iterative network for
3D SAR sparse imaging. <em>TIP</em>, <em>30</em>, 7317–7332. (<a
href="https://doi.org/10.1109/TIP.2021.3104168">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The emerging field of combining compressed sensing (CS) and three-dimensional synthetic aperture radar (3D SAR) imaging has shown significant potential to reduce sampling rate and improve image quality. However, the conventional CS-driven algorithms are always limited by huge computational costs and non-trivial tuning of parameters. In this article, to address this problem, we propose a two-path iterative framework dubbed TPSSI-Net for 3D SAR sparse imaging. By mapping the AMP into a layer-fixed deep neural network, each layer of TPSSI-Net consists of four modules in cascade corresponding to four steps of the AMP optimization. Differently, the Onsager terms in TPSSI-Net are modified to be differentiable and scaled by learnable coefficients. Rather than manually choosing a sparsifying basis, a two-path convolutional neural network (CNN) is developed and embedded in TPSSI-Net for nonlinear sparse representation in the complex-valued domain. All parameters are layer-varied and optimized by end-to-end training based on a channel-wise loss function, bounding both symmetry constraint and measurement fidelity. Finally, extensive SAR imaging experiments, including simulations and real-measured tests, demonstrate the effectiveness and high efficiency of the proposed TPSSI-Net.},
  archive      = {J_TIP},
  author       = {Mou Wang and Shunjun Wei and Jiadian Liang and Zichen Zhou and Qizhe Qu and Jun Shi and Xiaoling Zhang},
  doi          = {10.1109/TIP.2021.3104168},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7317-7332},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {TPSSI-net: Fast and enhanced two-path iterative network for 3D SAR sparse imaging},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Sub-sampled cross-component prediction for emerging video
coding standards. <em>TIP</em>, <em>30</em>, 7305–7316. (<a
href="https://doi.org/10.1109/TIP.2021.3104191">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-component linear model (CCLM) prediction has been repeatedly proven to be effective in reducing the inter-channel redundancies in video compression. Essentially speaking, the linear model is identically trained by employing accessible luma and chroma reference samples at both encoder and decoder, elevating the level of operational complexity due to the least square regression or max-min based model parameter derivation. In this paper, we investigate the capability of the linear model in the context of sub-sampled based cross-component correlation mining, as a means of significantly releasing the operation burden and facilitating the hardware and software design for both encoder and decoder. In particular, the sub-sampling ratios and positions are elaborately designed by exploiting the spatial correlation and the inter-channel correlation. Extensive experiments verify that the proposed method is characterized by its simplicity in operation and robustness in terms of rate-distortion performance, leading to the adoption by Versatile Video Coding (VVC) standard and the third generation of Audio Video Coding Standard (AVS3).},
  archive      = {J_TIP},
  author       = {Junru Li and Meng Wang and Li Zhang and Shiqi Wang and Kai Zhang and Shanshe Wang and Siwei Ma and Wen Gao},
  doi          = {10.1109/TIP.2021.3104191},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7305-7316},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Sub-sampled cross-component prediction for emerging video coding standards},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dual color space guided sketch colorization. <em>TIP</em>,
<em>30</em>, 7292–7304. (<a
href="https://doi.org/10.1109/TIP.2021.3104190">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic sketch colorization is a challenging task in both computer graphics and computer vision since all the color, texture, shading generation have to be created based on the abstract sketch. Besides, it is a subjective task in painting process, which needs illustrators to comprehend drawing priori (DP), such as hue variation, saturation contrast and gray contrast and utilize them in the HSV color space which is closer to human visual cognition system. As such, incorporating supplementary supervision in the HSV color space may be beneficial to sketch colorization. However, previous methods improve the colorization quality only in the RGB color space without considering the HSV color space, often causing results with dull color, inappropriate saturation contrast, and artifacts. To address this issue, we propose a novel sketch colorization method, dual color space guided generative adversarial network (DCSGAN), that considers the complementary information contained in both the RGB and HSV color space. Specifically, we incorporate the HSV color space to construct dual color spaces for supervising our method with a color space transformation (CST) network that learns transformation from the RGB to HSV color space. Then, we propose a DP loss that enables the DCSGAN to generate vivid color images with pixel level supervision. Additionally, a novel dual color space adversarial (DCSA) loss is designed to guide the generator at global level to reduce the artifacts to meet audiences’ aesthetic expectations. Extensive experiments and ablation studies demonstrate the superiority of the proposed method over previous state-of-the-art (SOTA) methods.},
  archive      = {J_TIP},
  author       = {Zhi Dou and Ning Wang and Baopu Li and Zhihui Wang and Haojie Li and Bin Liu},
  doi          = {10.1109/TIP.2021.3104190},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7292-7304},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Dual color space guided sketch colorization},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Polarization guided specular reflection separation.
<em>TIP</em>, <em>30</em>, 7280–7291. (<a
href="https://doi.org/10.1109/TIP.2021.3104188">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since specular reflection often exists in the real captured images and causes deviation between the recorded color and intrinsic color, specular reflection separation can bring advantages to multiple applications that require consistent object surface appearance. However, due to the color of an object is significantly influenced by the color of the illumination, the existing researches still suffer from the near-duplicate challenge, that is, the separation becomes unstable when the illumination color is close to the surface color. In this paper, we derive a polarization guided model to incorporate the polarization information into a designed iteration optimization separation strategy to separate the specular reflection. Based on the analysis of polarization, we propose a polarization guided model to generate a polarization chromaticity image, which is able to reveal the geometrical profile of the input image in complex scenarios, e.g., diversity of illumination. The polarization chromaticity image can accurately cluster the pixels with similar diffuse color. We further use the specular separation of all these clusters as an implicit prior to ensure that the diffuse component will not be mistakenly separated as the specular component. With the polarization guided model, we reformulate the specular reflection separation into a unified optimization function which can be solved by the ADMM strategy. The specular reflection will be detected and separated jointly by RGB and polarimetric information. Both qualitative and quantitative experimental results have shown that our method can faithfully separate the specular reflection, especially in some challenging scenarios.},
  archive      = {J_TIP},
  author       = {Sijia Wen and Yinqiang Zheng and Feng Lu},
  doi          = {10.1109/TIP.2021.3104188},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7280-7291},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Polarization guided specular reflection separation},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Rotation awareness based self-supervised learning for SAR
target recognition with limited training samples. <em>TIP</em>,
<em>30</em>, 7266–7279. (<a
href="https://doi.org/10.1109/TIP.2021.3104179">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The scattering signatures of a synthetic aperture radar (SAR) target image will be highly sensitive to different azimuth angles/poses, which aggravates the demand for training samples in learning-based SAR image automatic target recognition (ATR) algorithms, and makes SAR ATR a more challenging task. This paper develops a novel rotation awareness-based learning framework termed RotANet for SAR ATR under the condition of limited training samples. First, we propose an encoding scheme to characterize the rotational pattern of pose variations among intra-class targets. These targets will constitute several ordered sequences with different rotational patterns via permutations. By further exploiting the intrinsic relation constraints among these sequences as the supervision, we develop a novel self-supervised task which makes RotANet learn to predict the rotational pattern of a baseline sequence and then autonomously generalize this ability to the others without external supervision. Therefore, this task essentially contains a learning and self-validation process to achieve human-like rotation awareness, and it serves as a task-induced prior to regularize the learned feature domain of RotANet in conjunction with an individual target recognition task to improve the generalization ability of the features. Extensive experiments on moving and stationary target acquisition and recognition benchmark database demonstrate the effectiveness of our proposed framework. Compared with other state-of-the-art SAR ATR algorithms, RotANet will remarkably improve the recognition accuracy especially in the case of very limited training samples without performing any other data augmentation strategy.},
  archive      = {J_TIP},
  author       = {Zaidao Wen and Zhunga Liu and Shuai Zhang and Quan Pan},
  doi          = {10.1109/TIP.2021.3104179},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7266-7279},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Rotation awareness based self-supervised learning for SAR target recognition with limited training samples},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Spectral super-resolution network guided by intrinsic
properties of hyperspectral imagery. <em>TIP</em>, <em>30</em>,
7256–7265. (<a href="https://doi.org/10.1109/TIP.2021.3104177">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperspectral imagery (HSI) contains rich spectral information, which is beneficial to many tasks. However, acquiring HSI is difficult because of the limitations of current imaging technology. As an alternative method, spectral super-resolution aims at reconstructing HSI from its corresponding RGB image. Recently, deep learning has shown its power to this task, but most of the used networks are transferred from other domains, such as spatial super-resolution. In this paper, we attempt to design a spectral super-resolution network by taking advantage of two intrinsic properties of HSI. The first one is the spectral correlation. Based on this property, a decomposition subnetwork is designed to reconstruct HSI. The other one is the projection property, i.e., RGB image can be regarded as a three-dimensional projection of HSI. Inspired from it, a self-supervised subnetwork is constructed as a constraint to the decomposition subnetwork. These two subnetworks constitute our end-to-end super-resolution network. In order to test the effectiveness of it, we conduct experiments on three widely used HSI datasets (i.e., CAVE, NUS, and NTIRE2018). Experimental results show that our proposed network can achieve competitive reconstruction performance in comparison with several state-of-the-art networks.},
  archive      = {J_TIP},
  author       = {Renlong Hang and Qingshan Liu and Zhu Li},
  doi          = {10.1109/TIP.2021.3104177},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7256-7265},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Spectral super-resolution network guided by intrinsic properties of hyperspectral imagery},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Approximate intrinsic voxel structure for point cloud
simplification. <em>TIP</em>, <em>30</em>, 7241–7255. (<a
href="https://doi.org/10.1109/TIP.2021.3104174">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A point cloud as an information-intensive 3D representation usually requires a large amount of transmission, storage and computing resources, which seriously hinder its usage in many emerging fields. In this paper, we propose a novel point cloud simplification method, Approximate Intrinsic Voxel Structure (AIVS), to meet the diverse demands in real-world application scenarios. The method includes point cloud pre-processing (denoising and down-sampling), AIVS-based realization for isotropic simplification and flexible simplification with intrinsic control of point distance. To demonstrate the effectiveness of the proposed AIVS-based method, we conducted extensive experiments by comparing it with several relevant point cloud simplification methods on three public datasets, including Stanford, SHREC, and RGB-D scene models. The experimental results indicate that AIVS has great advantages over peers in terms of moving least squares (MLS) surface approximation quality, curvature-sensitive sampling, sharp-feature keeping and processing speed. The source code of the proposed method is publicly available. ( https://github.com/vvvwo/AIVS-project ).},
  archive      = {J_TIP},
  author       = {Chenlei Lv and Weisi Lin and Baoquan Zhao},
  doi          = {10.1109/TIP.2021.3104174},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7241-7255},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Approximate intrinsic voxel structure for point cloud simplification},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). 2D-LCoLBP: A learning two-dimensional co-occurrence local
binary pattern for image recognition. <em>TIP</em>, <em>30</em>,
7228–7240. (<a href="https://doi.org/10.1109/TIP.2021.3104163">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rotation, scale and translation invariance of extracted features have a high significance in image recognition. Local binary pattern (LBP) and LBP-based descriptors have been widely used in image recognition due to feature discrimination and computational efficiency. However, most of the existing LBP-based descriptors have been designed to achieve rotation invariance while fail to achieve scale invariance. Moreover, it is usually difficult to achieve a good trade-off between the feature discrimination and the feature dimension. In this work, a learning 2D co-occurrence LBP termed 2D-LCoLBP is proposed to address these issues. Firstly, a weighted joint histogram is constructed in different neighborhoods and scales of an image to represent the multi-neighborhood and multi-scale LBP (2D-MLBP) and achieve the rotation invariance. A feature learning strategy is then designed to learn the compact and robust descriptor (2D-LCoLBP) from LBP pattern pairs across different scales in the extracted 2D-MLBP to characterize the most stable local structures and achieve the scale invariance, as well as decrease the feature dimension and improve the noise robustness. Finally, a linear SVM classifier is employed for recognition. We applied the proposed 2D-LCoLBP on four image recognition tasks—texture, object, face and food recognition with ten image databases. Experimental results show that 2D-LCoLBP has obviously low feature dimension but outperforms the state-of-the-art LBP-based descriptors in terms of recognition accuracy under noise-free, Gaussian noise and JPEG compression conditions.},
  archive      = {J_TIP},
  author       = {Xiuli Bi and Yuan Yuan and Bin Xiao and Weisheng Li and Xinbo Gao},
  doi          = {10.1109/TIP.2021.3104163},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7228-7240},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {2D-LCoLBP: A learning two-dimensional co-occurrence local binary pattern for image recognition},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust focus volume regularization in shape from focus.
<em>TIP</em>, <em>30</em>, 7215–7227. (<a
href="https://doi.org/10.1109/TIP.2021.3100268">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Shape from focus (SFF) reconstructs 3D shape of the scene from a sequence of multi-focus images, and the quality of reconstructed shape mainly depends on the accuracy of image focus volume (FV). Traditional SFF techniques exhibit poor performance in preserving structural edges and fine details while removing noisy artifacts, and mostly they do not incorporate any additional shape prior. Therefore, in this paper, we propose to refine FV by formulating an energy minimization framework that employs a nonconvex regularizer and incorporates two types of shape priors. The proposed regularizer is robust against noisy focus values. The first proposed shape prior is input image sequence and it is a single and static shape prior. While, the second shape prior corresponds to a series of shape priors. These shape priors are FVs which are iteratively obtained on-the-fly. Both of these shape priors constrain the solution space for output FV. We optimize nonconvex energy function through majorize-minimization algorithm which iteratively guarantees a local minimum and converges quickly. Experiments have been conducted to evaluate accuracy and convergence properties of the proposed method. Experimental results of synthetic and real image sequences demonstrate that our method achieves superior results in terms of ability to reconstruct accurate 3D shapes as compared to existing approaches.},
  archive      = {J_TIP},
  author       = {Usman Ali and Muhammad Tariq Mahmood},
  doi          = {10.1109/TIP.2021.3100268},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7215-7227},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Robust focus volume regularization in shape from focus},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MGSeg: Multiple granularity-based real-time semantic
segmentation network. <em>TIP</em>, <em>30</em>, 7200–7214. (<a
href="https://doi.org/10.1109/TIP.2021.3102509">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent works on semantic segmentation witness significant performance improvement by utilizing global contextual information. In this paper, an efficient multi-granularity based semantic segmentation network (MGSeg) is proposed for real-time semantic segmentation, by modeling the latent relevance between multi-scale geometric details and high-level semantics for fine granularity segmentation. In particular, a light-weight backbone ResNet-18 is first adopted to produce the hierarchical features. Hybrid Attention Feature Aggregation (HAFA) is designed to filter the noisy spatial details of features, acquire the scale-invariance representation, and alleviate the gradient vanishing problem of the early-stage feature learning. After aggregating the learned features, Fine Granularity Refinement (FGR) module is employed to explicitly model the relationship between the multi-level features and categories, generating proper weights for fusion. More importantly, to meet the real-time processing, a series of light-weight strategies and simplified structures are applied to accelerate the efficiency, including light-weight backbone, channel compression, narrow neck structure, and so on. Extensive experiments conducted on benchmark datasets Cityscapes and CamVid demonstrate that the proposed method achieves the state-of-the-art performance, 77.8\%@50fps and 72.7\%@127fps on Cityscapes and CamVid datasets, respectively, having the capability for real-time applications.},
  archive      = {J_TIP},
  author       = {Jun-Yan He and Shi-Hua Liang and Xiao Wu and Bo Zhao and Lei Zhang},
  doi          = {10.1109/TIP.2021.3102509},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7200-7214},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {MGSeg: Multiple granularity-based real-time semantic segmentation network},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Uncertainty quantification enforced flash radiography
reconstruction by two-level efficient MCMC. <em>TIP</em>, <em>30</em>,
7184–7199. (<a href="https://doi.org/10.1109/TIP.2021.3101929">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Flash Radiography inspections stand to gain from inversion to infer density distribution of object based on X-ray transmission image. It is indispensable to be able to reliably provide uncertainties associated with the inversions. Although many inversion algorithms have been devised, they often perform poorly due to either their sensitivity to regularization parameter chosen in variational optimization or prohibitive computation and noisy results in stochastic simulation. In this paper, we present a gradual reconstruction algorithm, called TLE-Gibbs (two-level efficient Gibbs sampling), for flash radiography. At its core, TLE-Gibbs is a stochastic approach based on efficient Gibbs sampling and reconstruction refinement. A two-level scheme is proposed that enables high-resolution image to be constrained with uncertainty estimation from high-level reconstruction. Furthermore, a splitting variant that increases flexibility and precision is considered in the two-level scheme. An efficient Markov chain Monte Carlo (MCMC) endowed with first-order truncated conjugate gradient (CG) optimizer is developed to achieve minimal cost per sample and to approximate the posterior distribution. Finally, we adopt an effective refinement method to remove noises remained in the sample meanwhile maintaining sharp edges. For performance evaluation, TLE-Gibbs is applied on both synthetic data in which the influence of system blur is specially investigated and real data, and comparison with state-of-the-art reconstruction methods demonstrates the superiority of the proposed method.},
  archive      = {J_TIP},
  author       = {Qingwu Li and Jinxin Xu and Jiayu Wang and Yuefeng Jing and Xiaolin Wang},
  doi          = {10.1109/TIP.2021.3101929},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7184-7199},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Uncertainty quantification enforced flash radiography reconstruction by two-level efficient MCMC},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fast hyperspectral image recovery of dual-camera compressive
hyperspectral imaging via non-iterative subspace-based fusion.
<em>TIP</em>, <em>30</em>, 7170–7183. (<a
href="https://doi.org/10.1109/TIP.2021.3101916">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Coded aperture snapshot spectral imaging (CASSI) is a promising technique for capturing three-dimensional hyperspectral images (HSIs), in which algorithms are used to perform the inverse problem of HSI reconstruction from a single coded two-dimensional (2D) measurement. Due to the ill-posed nature of this problem, various regularizers have been exploited to reconstruct 3D data from 2D measurements. Unfortunately, the accuracy and computational complexity are unsatisfactory. One feasible solution is to utilize additional information such as the RGB measurement in CASSI. Considering the combined CASSI and RGB measurements, in this paper, we propose a fusion model for HSI reconstruction. Specifically, we investigate the low-dimensional spectral subspace property of HSIs composed of a spectral basis and spatial coefficients. In particular, the RGB measurement is utilized to estimate the coefficients, while the CASSI measurement is adopted to provide the spectral basis. We further propose a patch processing strategy to enhance the spectral low-rank property of HSIs. The optimization of the proposed model requires neither iteration nor the spectral sensing matrix of the RGB detector. Extensive experiments on both simulated and real HSI datasets demonstrate that our proposed method not only outperforms previous state-of-the-art (iterative algorithms) methods in quality but also speeds up the reconstruction by more than 5000 times.},
  archive      = {J_TIP},
  author       = {Wei He and Naoto Yokoya and Xin Yuan},
  doi          = {10.1109/TIP.2021.3101916},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7170-7183},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Fast hyperspectral image recovery of dual-camera compressive hyperspectral imaging via non-iterative subspace-based fusion},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Compressed domain deep video super-resolution. <em>TIP</em>,
<em>30</em>, 7156–7169. (<a
href="https://doi.org/10.1109/TIP.2021.3101826">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-world video processing algorithms are often faced with the great challenges of processing the compressed videos instead of pristine videos. Despite the tremendous successes achieved in deep-learning based video super-resolution (SR), much less work has been dedicated to the SR of compressed videos. Herein, we propose a novel approach for compressed domain deep video SR by jointly leveraging the coding priors and deep priors. By exploiting the diverse and ready-made spatial and temporal coding priors ( e.g., partition maps and motion vectors) extracted directly from the video bitstream in an effortless way, the video SR in the compressed domain allows us to accurately reconstruct the high resolution video with high flexibility and substantially economized computational complexity. More specifically, to incorporate the spatial coding prior, the Guided Spatial Feature Transform (GSFT) layer is proposed to modulate features of the prior with the guidance of the video information, making the prior features more fine-grained and content-adaptive. To incorporate the temporal coding prior, a guided soft alignment scheme is designed to generate local attention off-sets to compensate for decoded motion vectors. Our soft alignment scheme combines the merits of explicit and implicit motion modeling methods, rendering the alignment of features more effective for SR in terms of the computational complexity and robustness to inaccurate motion fields. Furthermore, to fully make use of the deep priors, the multi-scale fused features are generated from a scale-wise convolution reconstruction network for final SR video reconstruction. To promote the compressed domain video SR research, we build an extensive Compressed Videos with Coding Prior ( CVCP ) dataset, including compressed videos of diverse content and various coding priors extracted from the bitstream. Extensive experimental results show the effectiveness of coding priors in compressed domain video SR.},
  archive      = {J_TIP},
  author       = {Peilin Chen and Wenhan Yang and Meng Wang and Long Sun and Kangkang Hu and Shiqi Wang},
  doi          = {10.1109/TIP.2021.3101826},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7156-7169},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Compressed domain deep video super-resolution},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning dynamic relationships for facial expression
recognition based on graph convolutional network. <em>TIP</em>,
<em>30</em>, 7143–7155. (<a
href="https://doi.org/10.1109/TIP.2021.3101820">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial action units (AUs) analysis plays an important role in facial expression recognition (FER). Existing deep spectral convolutional networks (DSCNs) have made encouraging performance for FER based on a set of facial local regions and a predefined graph structure. However, these regions do not have close relationships to AUs, and DSCNs cannot model the dynamic spatial dependencies of these regions for estimating different facial expressions. To tackle these issues, we propose a novel double dynamic relationships graph convolutional network (DDRGCN) to learn the strength of the edges in the facial graph by a trainable weighted adjacency matrix. We construct facial graph data by 20 regions of interest (ROIs) guided by different facial AUs. Furthermore, we devise an efficient graph convolutional network in which the inherent dependencies of vertices in the facial graph can be learned automatically during network training. Notably, the proposed model only has 110K parameters and 0.48MB model size, which is significantly less than most existing methods. Experiments on four widely-used FER datasets demonstrate that the proposed dynamic relationships graph network achieves superior results compared to existing light-weight networks, not just in terms of accuracy but also model size and speed.},
  archive      = {J_TIP},
  author       = {Xing Jin and Zhihui Lai and Zhong Jin},
  doi          = {10.1109/TIP.2021.3101820},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7143-7155},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Learning dynamic relationships for facial expression recognition based on graph convolutional network},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-relation attention network for image patch matching.
<em>TIP</em>, <em>30</em>, 7127–7142. (<a
href="https://doi.org/10.1109/TIP.2021.3101414">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep convolutional neural networks attract increasing attention in image patch matching. However, most of them rely on a single similarity learning model, such as feature distance and the correlation of concatenated features. Their performances will degenerate due to the complex relation between matching patches caused by various imagery changes. To tackle this challenge, we propose a multi-relation attention learning network (MRAN) for image patch matching. Specifically, we propose to fuse multiple feature relations (MR) for matching, which can benefit from the complementary advantages between different feature relations and achieve significant improvements on matching tasks. Furthermore, we propose a relation attention learning module to learn the fused relation adaptively. With this module, meaningful feature relations are emphasized and the others are suppressed. Extensive experiments show that our MRAN achieves best matching performances, and has good generalization on multi-modal image patch matching, multi-modal remote sensing image patch matching and image retrieval tasks.},
  archive      = {J_TIP},
  author       = {Dou Quan and Shuang Wang and Yi Li and Bowu Yang and Ning Huyan and Jocelyn Chanussot and Biao Hou and Licheng Jiao},
  doi          = {10.1109/TIP.2021.3101414},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7127-7142},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Multi-relation attention network for image patch matching},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep-learned regularization and proximal operator for image
compressive sensing. <em>TIP</em>, <em>30</em>, 7112–7126. (<a
href="https://doi.org/10.1109/TIP.2021.3088611">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning has recently been intensively studied in the context of image compressive sensing (CS) to discover and represent complicated image structures. These approaches, however, either suffer from nonflexibility for an arbitrary sampling ratio or lack an explicit deep-learned regularization term. This paper aims to solve the CS reconstruction problem by combining the deep-learned regularization term and proximal operator. We first introduce a regularization term using a carefully designed residual-regressive net, which can measure the distance between a corrupted image and a clean image set and accurately identify to which subspace the corrupted image belongs. We then address a proximal operator with a tailored dilated residual channel attention net, which enables the learned proximal operator to map the distorted image into the clean image set. We adopt an adaptive proximal selection strategy to embed the network into the loop of the CS image reconstruction algorithm. Moreover, a self-ensemble strategy is presented to improve CS recovery performance. We further utilize state evolution to analyze the effectiveness of the designed networks. Extensive experiments also demonstrate that our method can yield superior accurate reconstruction (PSNR gain over 1 dB) compared to other competing approaches while achieving the current state-of-the-art image CS reconstruction performance. The test code is available at https://github.com/zjut-gwl/CSDRCANet.},
  archive      = {J_TIP},
  author       = {Zan Chen and Wenlong Guo and Yuanjing Feng and Yongqiang Li and Changchen Zhao and Yi Ren and Ling Shao},
  doi          = {10.1109/TIP.2021.3088611},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7112-7126},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Deep-learned regularization and proximal operator for image compressive sensing},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Blind motion deblurring super-resolution: When dynamic
spatio-temporal learning meets static image understanding. <em>TIP</em>,
<em>30</em>, 7101–7111. (<a
href="https://doi.org/10.1109/TIP.2021.3101402">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single-image super-resolution (SR) and multi-frame SR are two ways to super resolve low-resolution images. Single-Image SR generally handles each image independently, but ignores the temporal information implied in continuing frames. Multi-frame SR is able to model the temporal dependency via capturing motion information. However, it relies on neighbouring frames which are not always available in the real world. Meanwhile, slight camera shake easily causes heavy motion blur on long-distance-shot low-resolution images. To address these problems, a Blind Motion Deblurring Super-Reslution Networks, BMDSRNet, is proposed to learn dynamic spatio-temporal information from single static motion-blurred images. Motion-blurred images are the accumulation over time during the exposure of cameras, while the proposed BMDSRNet learns the reverse process and uses three-streams to learn Bidirectional spatio-temporal information based on well designed reconstruction loss functions to recover clean high-resolution images. Extensive experiments demonstrate that the proposed BMDSRNet outperforms recent state-of-the-art methods, and has the ability to simultaneously deal with image deblurring and SR.},
  archive      = {J_TIP},
  author       = {Wenjia Niu and Kaihao Zhang and Wenhan Luo and Yiran Zhong},
  doi          = {10.1109/TIP.2021.3101402},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7101-7111},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Blind motion deblurring super-resolution: When dynamic spatio-temporal learning meets static image understanding},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DeepFoveaNet: Deep fovea eagle-eye bioinspired model to
detect moving objects. <em>TIP</em>, <em>30</em>, 7090–7100. (<a
href="https://doi.org/10.1109/TIP.2021.3101398">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Birds of prey especially eagles and hawks have a visual acuity two to five times better than humans. Among the peculiar characteristics of their biological vision are that they have two types of foveae; one shallow fovea used in their binocular vision, and a deep fovea for monocular vision. The deep fovea allows these birds to see objects at long distances and to identify them as possible prey. Inspired by the biological functioning of the deep fovea a model called DeepFoveaNet is proposed in this paper. DeepFoveaNet is a convolutional neural network model to detect moving objects in video sequences. DeepFoveaNet emulates the monocular vision of birds of prey through two Encoder-Decoder convolutional neural network modules. This model combines the capacity of magnification of the deep fovea and the context information of the peripheral vision. Unlike algorithms to detect moving objects, ranked in the first places of the Change Detection database (CDnet14), DeepFoveaNet does not depend on previously trained neural networks, neither on a huge number of training images for its training. Besides, its architecture allows it to learn spatiotemporal information of the video. DeepFoveaNet was evaluated in the CDnet14 database achieving high performance and was ranked as one of the ten best algorithms. The characteristics and results of DeepFoveaNet demonstrated that the model is comparable to the state-of-the-art algorithms to detect moving objects, and it can detect very small moving objects through its deep fovea model that other algorithms cannot detect.},
  archive      = {J_TIP},
  author       = {Abimael Guzman-Pando and Mario I. Chacon-Murguia},
  doi          = {10.1109/TIP.2021.3101398},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7090-7100},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {DeepFoveaNet: Deep fovea eagle-eye bioinspired model to detect moving objects},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). WaveCNet: Wavelet integrated CNNs to suppress aliasing
effect for noise-robust image classification. <em>TIP</em>, <em>30</em>,
7074–7089. (<a href="https://doi.org/10.1109/TIP.2021.3101395">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Though widely used in image classification, convolutional neural networks (CNNs) are prone to noise interruptions, i.e. the CNN output can be drastically changed by small image noise. To improve the noise robustness, we try to integrate CNNs with wavelet by replacing the common down-sampling (max-pooling, strided-convolution, and average pooling) with discrete wavelet transform (DWT). We firstly propose general DWT and inverse DWT (IDWT) layers applicable to various orthogonal and biorthogonal discrete wavelets like Haar, Daubechies, and Cohen, etc., and then design wavelet integrated CNNs (WaveCNets) by integrating DWT into the commonly used CNNs (VGG, ResNets, and DenseNet). During the down-sampling, WaveCNets apply DWT to decompose the feature maps into the low-frequency and high-frequency components. Containing the main information including the basic object structures, the low-frequency component is transmitted into the following layers to generate robust high-level features. The high-frequency components are dropped to remove most of the data noises. The experimental results show that WaveCNets achieve higher accuracy on ImageNet than various vanilla CNNs. We have also tested the performance of WaveCNets on the noisy version of ImageNet, ImageNet-C and six adversarial attacks, the results suggest that the proposed DWT/IDWT layers could provide better noise-robustness and adversarial robustness. When applying WaveCNets as backbones, the performance of object detectors (i.e., faster R-CNN and RetinaNet) on COCO detection dataset are consistently improved. We believe that suppression of aliasing effect, i.e. separation of low frequency and high frequency information, is the main advantages of our approach. The code of our DWT/IDWT layer and different WaveCNets are available at https://github.com/CVI-SZU/WaveCNet.},
  archive      = {J_TIP},
  author       = {Qiufu Li and Linlin Shen and Sheng Guo and Zhihui Lai},
  doi          = {10.1109/TIP.2021.3101395},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7074-7089},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {WaveCNet: Wavelet integrated CNNs to suppress aliasing effect for noise-robust image classification},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep dichromatic model estimation under AC light sources.
<em>TIP</em>, <em>30</em>, 7064–7073. (<a
href="https://doi.org/10.1109/TIP.2021.3100550">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The dichromatic reflection model has been popularly exploited for computer vison tasks, such as color constancy and highlight removal. However, dichromatic model estimation is an severely ill-posed problem. Thus, several assumptions have been commonly made to estimate the dichromatic model, such as white-light (highlight removal) and the existence of highlight regions (color constancy). In this paper, we propose a spatio-temporal deep network to estimate the dichromatic parameters under AC light sources. The minute illumination variations can be captured with high-speed camera. The proposed network is composed of two sub-network branches. From high-speed video frames, each branch generates chromaticity and coefficient matrices, which correspond to the dichromatic image model. These two separate branches are jointly learned by spatio-temporal regularization. As far as we know, this is the first work that aims to estimate all dichromatic parameters in computer vision. To validate the model estimation accuracy, it is applied to color constancy and highlight removal. Both experimental results show that the dichromatic model can be estimated accurately via the proposed deep network.},
  archive      = {J_TIP},
  author       = {Jun-Sang Yoo and Chan-Ho Lee and Jong-Ok Kim},
  doi          = {10.1109/TIP.2021.3100550},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7064-7073},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Deep dichromatic model estimation under AC light sources},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Towards efficient scene understanding via squeeze reasoning.
<em>TIP</em>, <em>30</em>, 7050–7063. (<a
href="https://doi.org/10.1109/TIP.2021.3099369">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph-based convolutional model such as non-local block has shown to be effective for strengthening the context modeling ability in convolutional neural networks (CNNs). However, its pixel-wise computational overhead is prohibitive which renders it unsuitable for high resolution imagery. In this paper, we explore the efficiency of context graph reasoning and propose a novel framework called Squeeze Reasoning. Instead of propagating information on the spatial map, we first learn to squeeze the input feature into a channel-wise global vector and perform reasoning within the single vector where the computation cost can be significantly reduced. Specifically, we build the node graph in the vector where each node represents an abstract semantic concept. The refined feature within the same semantic category results to be consistent, which is thus beneficial for downstream tasks. We show that our approach can be modularized as an end-to-end trained block and can be easily plugged into existing networks. Despite its simplicity and being lightweight, the proposed strategy allows us to establish the considerable results on different semantic segmentation datasets and shows significant improvements with respect to strong baselines on various other scene understanding tasks including object detection, instance segmentation and panoptic segmentation. Code is available at https://github.com/lxtGH/SFSegNets.},
  archive      = {J_TIP},
  author       = {Xiangtai Li and Xia Li and Ansheng You and Li Zhang and Guangliang Cheng and Kuiyuan Yang and Yunhai Tong and Zhouchen Lin},
  doi          = {10.1109/TIP.2021.3099369},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7050-7063},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Towards efficient scene understanding via squeeze reasoning},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A comprehensive bi-static amplitude compensated range
migration algorithm (AC-RMA). <em>TIP</em>, <em>30</em>, 7038–7049. (<a
href="https://doi.org/10.1109/TIP.2021.3100679">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a comprehensive form of the range migration algorithm (RMA) is analytically derived for reconstructing the reflectivity function using synthetic aperture imaging techniques. Specifically, amplitude compensation, in addition to the typical phase compensation, is included in the development of the matched filter of the RMA, with the result herein referred to as the amplitude compensated RMA (AC-RMA). To illustrate the improvements offered by the AC-RMA, simulation and measurement (at Ka-band, 26.5 - 40 GHz) are performed to reconstruct the reflectivity function of a target using both RMA and AC-RMA algorithms. The results prove that the AC-RMA is a robust algorithm that can successfully reconstruct the reflectivity function of a target with higher accuracy, regardless of its dielectric properties, including scenarios with low contrast between the dielectric properties of the background and target in the presence of noise. This approach is also independent of the bandwidth of the imaging system and is applicable to multilayer media as well. In addition, while the formulation of the AC-RMA is more complicated than the traditional (phase compensation only) RMA, the processing time necessary for images created with the AC-RMA is just 1.2 times greater than that of the traditional RMA processing time.},
  archive      = {J_TIP},
  author       = {Zahra Manzoor and Mohammad Tayeb Al Qaseer and Kristen M. Donnell},
  doi          = {10.1109/TIP.2021.3100679},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7038-7049},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A comprehensive bi-static amplitude compensated range migration algorithm (AC-RMA)},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust phase unwrapping via deep image prior for
quantitative phase imaging. <em>TIP</em>, <em>30</em>, 7025–7037. (<a
href="https://doi.org/10.1109/TIP.2021.3099956">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantitative phase imaging (QPI) is an emerging label-free technique that produces images containing morphological and dynamical information without contrast agents. Unfortunately, the phase is wrapped in most imaging system. Phase unwrapping is the computational process that recovers a more informative image. It is particularly challenging with thick and complex samples such as organoids. Recent works that rely on supervised training show that deep learning is a powerful method to unwrap the phase; however, supervised approaches require large and representative datasets which are difficult to obtain for complex biological samples. Inspired by the concept of deep image priors, we propose a deep-learning-based method that does not need any training set. Our framework relies on an untrained convolutional neural network to accurately unwrap the phase while ensuring the consistency of the measurements. We experimentally demonstrate that the proposed method faithfully recovers the phase of complex samples on both real and simulated data. Our work paves the way to reliable phase imaging of thick and complex samples with QPI.},
  archive      = {J_TIP},
  author       = {Fangshu Yang and Thanh-An Pham and Nathalie Brandenberg and Matthias P. Lütolf and Jianwei Ma and Michael Unser},
  doi          = {10.1109/TIP.2021.3099956},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7025-7037},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Robust phase unwrapping via deep image prior for quantitative phase imaging},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DPANet: Depth potentiality-aware gated attention network for
RGB-d salient object detection. <em>TIP</em>, <em>30</em>, 7012–7024.
(<a href="https://doi.org/10.1109/TIP.2020.3028289">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There are two main issues in RGB-D salient object detection: (1) how to effectively integrate the complementarity from the cross-modal RGB-D data; (2) how to prevent the contamination effect from the unreliable depth map. In fact, these two problems are linked and intertwined, but the previous methods tend to focus only on the first problem and ignore the consideration of depth map quality, which may yield the model fall into the sub-optimal state. In this paper, we address these two issues in a holistic model synergistically, and propose a novel network named DPANet to explicitly model the potentiality of the depth map and effectively integrate the cross-modal complementarity. By introducing the depth potentiality perception, the network can perceive the potentiality of depth information in a learning-based manner, and guide the fusion process of two modal data to prevent the contamination occurred. The gated multi-modality attention module in the fusion process exploits the attention mechanism with a gate controller to capture long-range dependencies from a cross-modal perspective. Experimental results compared with 16 state-of-the-art methods on 8 datasets demonstrate the validity of the proposed approach both quantitatively and qualitatively. https://github.com/JosephChenHub/DPANet.},
  archive      = {J_TIP},
  author       = {Zuyao Chen and Runmin Cong and Qianqian Xu and Qingming Huang},
  doi          = {10.1109/TIP.2020.3028289},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7012-7024},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {DPANet: Depth potentiality-aware gated attention network for RGB-D salient object detection},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Embedding regularizer learning for multi-view
semi-supervised classification. <em>TIP</em>, <em>30</em>, 6997–7011.
(<a href="https://doi.org/10.1109/TIP.2021.3101917">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classification remains challenging when confronted with the existence of multi-view data with limited labels. In this paper, we propose an embedding regularizer learning scheme for multi-view semi-supervised classification (ERL-MVSC). The proposed framework integrates diversity, sparsity and consensus to dexterously manipulate multi-view data with limited labels. To encourage diversity, ERL-MVSC recasts a linear regression model to derive view-specific embedding regularizers and automatically determines their weights. This is able to tactfully incorporate complementary information of different views. To ensure sparsity, ERL-MVSC imposes $\ell _{2,1}$ -norm on a fused embedding regularizer to exploit the sparse local structure of samples, thereby conveying valuable classification information and enhancing the robustness against noise/outliers. To enhance consensus, ERL-MVSC learns a shared predicted label matrix, which serves as the comment target of multi-view classification. With these techniques, we formulate ERL-MVSC as a joint optimization problem of an embedding regularizer and a predicted label matrix, which can be solved by a coordinate descent method. Extensive experimental results on real-world datasets demonstrate the effectiveness and superiority of the proposed algorithm.},
  archive      = {J_TIP},
  author       = {Aiping Huang and Zheng Wang and Yannan Zheng and Tiesong Zhao and Chia-Wen Lin},
  doi          = {10.1109/TIP.2021.3101917},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6997-7011},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Embedding regularizer learning for multi-view semi-supervised classification},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Resolution-aware knowledge distillation for efficient
inference. <em>TIP</em>, <em>30</em>, 6985–6996. (<a
href="https://doi.org/10.1109/TIP.2021.3101158">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Minimizing the computation complexity is essential for the popularization of deep networks in practical applications. Nowadays, most researches attempt to accelerate deep networks by designing new network structure or compressing the network parameters. Meanwhile, transfer learning techniques such as knowledge distillation are utilized to keep the performance of deep models. In this paper, we focus on accelerating deep models and relieving the computation burden by using low-resolution (LR) images as inputs while maintaining competitive performance, which is rarely researched in the current literature. Deep networks may encounter serious performance degradation when using LR inputs because many details are unavailable from LR images. Besides, the existing approaches may fail to learn discriminative features for LR images because of the dramatic appearance variations between LR and high-resolution (HR) images. To tackle with the above problems, we propose a resolution-aware knowledge distillation (RKD) framework to narrow the cross-resolution variations by transferring knowledge from HR domain to LR domain. The proposed framework consists of a HR teacher network and a LR student network. First, we introduce a discriminator and propose an adversarial learning strategy to shrink the variations between inputs with changing resolution. Then we design a cross-resolution knowledge distillation (CRKD) loss to train discriminative student network by exploiting the knowledge of the teacher network. The CRKD loss is consisted of a resolution-aware distillation loss, a pair-wise constraint, and a maximum mean discrepancy loss. Experimental results on person re-identification, image classification, face recognition, and defect segmentation tasks demonstrate that RKD outperforms traditional knowledge distillation method by achieving better performance with lower computation complexities. Furthermore, CRKD surpasses the state-of-the-art knowledge distillation methods in transferring knowledge across different resolutions under RKD framework, especially when coping with large resolution differences.},
  archive      = {J_TIP},
  author       = {Zhanxiang Feng and Jianhuang Lai and Xiaohua Xie},
  doi          = {10.1109/TIP.2021.3101158},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6985-6996},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Resolution-aware knowledge distillation for efficient inference},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Blind image deblurring using a non-linear channel prior
based on dark and bright channels. <em>TIP</em>, <em>30</em>, 6970–6984.
(<a href="https://doi.org/10.1109/TIP.2021.3101154">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blind image deblurring aims at recovering a clean image from the given blurry image without knowing the blur kernel. Recently proposed dark and extreme channel priors have shown their effectiveness in deblurring various blurry scenarios. However, these two priors fail to help the blur kernel estimation under the particular circumstance that clean images contain neither enough darkest nor brightest pixels. In this paper, we propose a novel and robust non-linear channel (NLC) prior for the blur kernel estimation to fill this gap. It is motivated by a simple idea that the blurring operation will increase the ratio of dark channel to bright channel. This change has been proved to be true both theoretically and empirically. Nonetheless, the presence of the NLC prior introduces a thorny optimization model. To handle it, an efficient algorithm based on projected alternating minimization (PAM) has been established which innovatively combines an approximate strategy, the half-quadratic splitting method, and fast iterative shrinkage-thresholding algorithm (FISTA). Extensive experimental results show that the proposed method achieves state-of-the-art results no matter when it has been applied in synthetic uniform and non-uniform benchmark datasets or in real blurry images.},
  archive      = {J_TIP},
  author       = {Xianyu Ge and Jieqing Tan and Li Zhang},
  doi          = {10.1109/TIP.2021.3101154},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6970-6984},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Blind image deblurring using a non-linear channel prior based on dark and bright channels},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Interleaved deep artifacts-aware attention mechanism for
concrete structural defect classification. <em>TIP</em>, <em>30</em>,
6957–6969. (<a href="https://doi.org/10.1109/TIP.2021.3100556">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic machine classification of concrete structural defects in images poses significant challenges because of multitude of problems arising from the surface texture, such as presence of stains, holes, colors, poster remains, graffiti, marking and painting, along with uncontrolled weather conditions and illuminations. In this paper, we propose an interleaved deep artifacts-aware attention mechanism (iDAAM) to classify multi-target multi-class and single-class defects from structural defect images. Our novel architecture is composed of interleaved fine-grained dense modules (FGDM) and concurrent dual attention modules (CDAM) to extract local discriminative features from concrete defect images. FGDM helps to aggregate multi-layer robust information with wide range of scales to describe visually-similar overlapping defects. On the other hand, CDAM selects multiple representations of highly localized overlapping defect features and encodes the crucial spatial regions from discriminative channels to address variations in texture, viewing angle, shape and size of overlapping defect classes. Within iDAAM, FGDM and CDAM are interleaved to extract salient discriminative features from multiple scales by constructing an end-to-end trainable network without any preprocessing steps, making the process fully automatic. Experimental results and extensive ablation studies on three publicly available large concrete defect datasets show that our proposed approach outperforms the current state-of-the-art methodologies.},
  archive      = {J_TIP},
  author       = {Gaurab Bhattacharya and Bappaditya Mandal and Niladri B. Puhan},
  doi          = {10.1109/TIP.2021.3100556},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6957-6969},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Interleaved deep artifacts-aware attention mechanism for concrete structural defect classification},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An iterative co-training transductive framework for zero
shot learning. <em>TIP</em>, <em>30</em>, 6943–6956. (<a
href="https://doi.org/10.1109/TIP.2021.3100552">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In zero-shot learning (ZSL) community, it is generally recognized that transductive learning performs better than inductive one as the unseen-class samples are also used in its training stage. How to generate pseudo labels for unseen-class samples and how to use such usually noisy pseudo labels are two critical issues in transductive learning. In this work, we introduce an iterative co-training framework which contains two different base ZSL models and an exchanging module. At each iteration, the two different ZSL models are co-trained to separately predict pseudo labels for the unseen-class samples, and the exchanging module exchanges the predicted pseudo labels, then the exchanged pseudo-labeled samples are added into the training sets for the next iteration. By such, our framework can gradually boost the ZSL performance by fully exploiting the potential complementarity of the two models’ classification capabilities. In addition, our co-training framework is also applied to the generalized ZSL (GZSL), in which a semantic-guided OOD detector is proposed to pick out the most likely unseen-class samples before class-level classification to alleviate the bias problem in GZSL. Extensive experiments on three benchmarks show that our proposed methods could significantly outperform about 31 state-of-the-art ones.},
  archive      = {J_TIP},
  author       = {Bo Liu and Lihua Hu and Qiulei Dong and Zhanyi Hu},
  doi          = {10.1109/TIP.2021.3100552},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6943-6956},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {An iterative co-training transductive framework for zero shot learning},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Joint denoising and demosaicking with green channel prior
for real-world burst images. <em>TIP</em>, <em>30</em>, 6930–6942. (<a
href="https://doi.org/10.1109/TIP.2021.3100312">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Denoising and demosaicking are essential yet correlated steps to reconstruct a full color image from the raw color filter array (CFA) data. By learning a deep convolutional neural network (CNN), significant progress has been achieved to perform denoising and demosaicking jointly. However, most existing CNN-based joint denoising and demosaicking (JDD) methods work on a single image while assuming additive white Gaussian noise, which limits their performance on real-world applications. In this work, we study the JDD problem for real-world burst images, namely JDD-B. Considering the fact that the green channel has twice the sampling rate and better quality than the red and blue channels in CFA raw data, we propose to use this green channel prior (GCP) to build a GCP-Net for the JDD-B task. In GCP-Net, the GCP features extracted from green channels are utilized to guide the feature extraction and feature upsampling of the whole image. To compensate for the shift between frames, the offset is also estimated from GCP features to reduce the impact of noise. Our GCP-Net can preserve more image structures and details than other JDD methods while removing noise. Experiments on synthetic and real-world noisy images demonstrate the effectiveness of GCP-Net quantitatively and qualitatively.},
  archive      = {J_TIP},
  author       = {Shi Guo and Zhetong Liang and Lei Zhang},
  doi          = {10.1109/TIP.2021.3100312},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6930-6942},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Joint denoising and demosaicking with green channel prior for real-world burst images},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). HCE: Hierarchical context embedding for region-based object
detection. <em>TIP</em>, <em>30</em>, 6917–6929. (<a
href="https://doi.org/10.1109/TIP.2021.3099733">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {State-of-the-art two-stage object detectors apply a classifier to a sparse set of object proposals, relying on region-wise features extracted by RoIPool or RoIAlign as inputs. The region-wise features, in spite of aligning well with the proposal locations, may still lack the crucial context information which is necessary for filtering out noisy background detections, as well as recognizing objects possessing no distinctive appearances. To address this issue, we present a simple but effective Hierarchical Context Embedding (HCE) framework, which can be applied as a plug-and-play component, to facilitate the classification ability of a series of region-based detectors by mining contextual cues. Specifically, to advance the recognition of context-dependent object categories, we propose an image-level categorical embedding module which leverages the holistic image-level context to learn object-level concepts. Then, novel RoI features are generated by exploiting hierarchically embedded context information beneath both whole images and interested regions, which are also complementary to conventional RoI features. Moreover, to make full use of our hierarchical contextual RoI features, we propose the early-and-late fusion strategies (i.e., feature fusion and confidence fusion), which can be combined to boost the classification accuracy of region-based detectors. Comprehensive experiments demonstrate that our HCE framework is flexible and generalizable, leading to significant and consistent improvements upon various region-based detectors, including FPN, Cascade R-CNN, Mask R-CNN and PA-FPN. With simple modification, our HCE framework can be conveniently adapted to fit the structure of one-stage detectors, and achieve improved performance for SSD, RetinaNet and EfficientDet.},
  archive      = {J_TIP},
  author       = {Zhao-Min Chen and Xin Jin and Bo-Rui Zhao and Xiaoqin Zhang and Yanwen Guo},
  doi          = {10.1109/TIP.2021.3099733},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6917-6929},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {HCE: Hierarchical context embedding for region-based object detection},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Spatially-aware context neural networks. <em>TIP</em>,
<em>30</em>, 6906–6916. (<a
href="https://doi.org/10.1109/TIP.2021.3097917">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A variety of computer vision tasks benefit significantly from increasingly powerful deep convolutional neural networks. However, the inherently local property of convolution operations prevents most existing models from capturing long-range feature interactions for improved performances. In this paper, we propose a novel module, called Spatially-Aware Context (SAC) block, to learn spatially-aware contexts by capturing multi-mode global contextual semantics for sophisticated long-range dependencies modeling. We enable customized non-local feature interactions for each spatial position through re-weighted global context fusion in a non-normalized way. SAC is very lightweight and can be easily plugged into popular backbone models. Extensive experiments on COCO, ImageNet, and HICO-DET benchmarks show that our SAC block achieves significant performance improvements over existing baseline architectures while with a negligible computational burden increase. The results also demonstrate the exceptional effectiveness and scalability of the proposed approach on capturing long-range dependencies for object detection, segmentation, and image classification, outperforming a bank of state-of-the-art attention blocks.},
  archive      = {J_TIP},
  author       = {Dongsheng Ruan and Yu Shi and Jun Wen and Nenggan Zheng and Min Zheng},
  doi          = {10.1109/TIP.2021.3097917},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6906-6916},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Spatially-aware context neural networks},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fast local spatial verification for feature-agnostic
large-scale image retrieval. <em>TIP</em>, <em>30</em>, 6892–6905. (<a
href="https://doi.org/10.1109/TIP.2021.3097175">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Images from social media can reflect diverse viewpoints, heated arguments, and expressions of creativity, adding new complexity to retrieval tasks. Researchers working on Content-Based Image Retrieval (CBIR) have traditionally tuned their algorithms to match filtered results with user search intent. However, we are now bombarded with composite images of unknown origin, authenticity, and even meaning. With such uncertainty, users may not have an initial idea of what the search query results should look like. For instance, hidden people, spliced objects, and subtly altered scenes can be difficult for a user to detect initially in a meme image, but may contribute significantly to its composition. It is pertinent to design systems that retrieve images with these nuanced relationships in addition to providing more traditional results, such as duplicates and near-duplicates - and to do so with enough efficiency at large scale. We propose a new approach for spatial verification that aims at modeling object-level regions using image keypoints retrieved from an image index, which is then used to accurately weight small contributing objects within the results, without the need for costly object detection steps. We call this method the Objects in Scene to Objects in Scene (OS2OS) score, and it is optimized for fast matrix operations, which can run quickly on either CPUs or GPUs. It performs comparably to state-of-the-art methods on classic CBIR problems (Oxford 5K, Paris 6K, and Google-Landmarks), and outperforms them in emerging retrieval tasks such as image composite matching in the NIST MFC2018 dataset and meme-style imagery from Reddit.},
  archive      = {J_TIP},
  author       = {Joel Brogan and Aparna Bharati and Daniel Moreira and Anderson Rocha and Kevin W. Bowyer and Patrick J. Flynn and Walter J. Scheirer},
  doi          = {10.1109/TIP.2021.3097175},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6892-6905},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Fast local spatial verification for feature-agnostic large-scale image retrieval},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MINet: Meta-learning instance identifiers for video object
detection. <em>TIP</em>, <em>30</em>, 6879–6891. (<a
href="https://doi.org/10.1109/TIP.2021.3099409">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in video object detection have characterized the exploration of temporal coherence across frames to enhance object detector. Nevertheless, previous solutions either rely on additional inputs (e.g., optical flow) to guide feature aggregation, or complex post-processing to associate bounding boxes. In this paper, we introduce a simple but effective design that learns instance identifiers for instance association in a meta-learning paradigm, which requires no auxiliary inputs or post-processing. Specifically, we present Meta-Learnt Instance Identifier Networks (namely MINet) that novelly meta-learns instance identifiers to recognize identical instances across frames in a single forward-pass, leading to the robust online linking of instances. Technically, depending on the detection results of previous frames, we teach MINet to learn the weights of an instance identifier on the fly, which can be well applied to up-coming frames. Such meta-learning paradigm enables instance identifiers to be flexibly adapted to novel frames at inference. Furthermore, MINet writes/updates the detection results of previous instances into memory and reads from memory when performing inference to encourage temporal consistency for video object detection. Our MINet is appealing in the sense that it is pluggable to any object detection model. Extensive experiments on ImageNet VID dataset demonstrate the superiority of MINet. More remarkably, by integrating MINet into Faster R-CNN, we achieve 80.2\% mAP on ImageNet VID dataset.},
  archive      = {J_TIP},
  author       = {Jiajun Deng and Yingwei Pan and Ting Yao and Wengang Zhou and Houqiang Li and Tao Mei},
  doi          = {10.1109/TIP.2021.3099409},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6879-6891},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {MINet: Meta-learning instance identifiers for video object detection},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). KFC: An efficient framework for semi-supervised temporal
action localization. <em>TIP</em>, <em>30</em>, 6869–6878. (<a
href="https://doi.org/10.1109/TIP.2021.3099407">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In temporal action localization (TAL), semi-supervised learning is a promising technique to mitigate the cost of precise boundary annotations. Semi-supervised approaches employing consistency regularization (CR), encouraging models to be robust to the perturbed inputs, have achieved great success in image classification problems. The success of CR is largely depended on the perturbations, where instances are perturbed to train a robust model without altering their semantic information. However, the perturbations for image or video classification tasks are not fit to apply to TAL. Since videos in TAL are too long to train the model with raw videos in an end-to-end manner. In this paper, we devise a method named K-farthest crossover to construct perturbations based on video features and apply it to TAL. Motivated by the observation that features in the same action instance become more and more similar during the training process while those in different action instances or backgrounds become more and more divergent, we add perturbations to each feature along temporal axis and adopt CR to encourage the model to retain this observation. Specifically, for a feature, we first find the top-k dissimilar features and average them to form a perturbation. Then, similar to chromosomal crossover, we select a large part of the feature and a small part of the perturbation to recombine a perturbed feature, which preserves the feature semantics yet enough discrepancy.},
  archive      = {J_TIP},
  author       = {Xinpeng Ding and Nannan Wang and Xinbo Gao and Jie Li and Xiaoyu Wang and Tongliang Liu},
  doi          = {10.1109/TIP.2021.3099407},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6869-6878},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {KFC: An efficient framework for semi-supervised temporal action localization},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Salient object detection with purificatory mechanism and
structural similarity loss. <em>TIP</em>, <em>30</em>, 6855–6868. (<a
href="https://doi.org/10.1109/TIP.2021.3099405">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image-based salient object detection has made great progress over the past decades, especially after the revival of deep neural networks. By the aid of attention mechanisms to weight the image features adaptively, recent advanced deep learning-based models encourage the predicted results to approximate the ground-truth masks with as large predictable areas as possible, thus achieving the state-of-the-art performance. However, these methods do not pay enough attention to small areas prone to misprediction. In this way, it is still tough to accurately locate salient objects due to the existence of regions with indistinguishable foreground and background and regions with complex or fine structures. To address these problems, we propose a novel convolutional neural network with purificatory mechanism and structural similarity loss. Specifically, in order to better locate preliminary salient objects, we first introduce the promotion attention, which is based on spatial and channel attention mechanisms to promote attention to salient regions. Subsequently, for the purpose of restoring the indistinguishable regions that can be regarded as error-prone regions of one model, we propose the rectification attention, which is learned from the areas of wrong prediction and guide the network to focus on error-prone regions thus rectifying errors. Through these two attentions, we use the Purificatory Mechanism to impose strict weights with different regions of the whole salient objects and purify results from hard-to-distinguish regions, thus accurately predicting the locations and details of salient objects. In addition to paying different attention to these hard-to-distinguish regions, we also consider the structural constraints on complex regions and propose the Structural Similarity Loss. The proposed loss models the region-level pair-wise relationship between regions to assist these regions to calibrate their own saliency values. In experiments, the proposed purificatory mechanism and structural similarity loss can both effectively improve the performance, and the proposed approach outperforms 19 state-of-the-art methods on six datasets with a notable margin. Also, the proposed method is efficient and runs at over 27FPS on a single NVIDIA 1080Ti GPU.},
  archive      = {J_TIP},
  author       = {Jia Li and Jinming Su and Changqun Xia and Mingcan Ma and Yonghong Tian},
  doi          = {10.1109/TIP.2021.3099405},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6855-6868},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Salient object detection with purificatory mechanism and structural similarity loss},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A two-stage density-aware single image deraining method.
<em>TIP</em>, <em>30</em>, 6843–6854. (<a
href="https://doi.org/10.1109/TIP.2021.3099396">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although advanced single image deraining methods have been proposed, one main challenge remains: the available methods usually perform well on specific rain patterns but can hardly deal with scenarios with dramatically different rain densities, especially when the impacts of rain streaks and the veiling effect caused by rain accumulation are heavily coupled. To tackle this challenge, we propose a two-stage density-aware single image deraining method with gated multi-scale feature fusion. In the first stage, a realistic physics model closer to real rain scenes is leveraged for initial deraining, and a network branch is also trained for rain density estimation to guide the subsequent refinement. The second stage of model-independent refinement is realized using conditional Generative Adversarial Network (cGAN), aiming to eliminate artifacts and improve the restoration quality. In particular, dilated convolutions are applied to extract rain features at multiple scales and gated feature fusion is exploited to better aggregate multi-level contextual information in both stages. Extensive experiments have been conducted on representative synthetic rain datasets and real rain scenes. Quantitative and qualitative results demonstrate the superiority of our method in terms of effectiveness and generalization ability, which outperforms the state-of-the-art.},
  archive      = {J_TIP},
  author       = {Min Cao and Zhi Gao and Bharath Ramesh and Tiancan Mei and Jinqiang Cui},
  doi          = {10.1109/TIP.2021.3099396},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6843-6854},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A two-stage density-aware single image deraining method},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Global aggregation then local distribution for scene
parsing. <em>TIP</em>, <em>30</em>, 6829–6842. (<a
href="https://doi.org/10.1109/TIP.2021.3099366">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modelling long-range contextual relationships is critical for pixel-wise prediction tasks such as semantic segmentation. However, convolutional neural networks (CNNs) are inherently limited to model such dependencies due to the naive structure in its building modules (e.g., local convolution kernel). While recent global aggregation methods are beneficial for long-range structure information modelling, they would oversmooth and bring noise to the regions contain fine details (e.g., boundaries and small objects), which are very much cared in the semantic segmentation task. To alleviate this problem, we propose to explore the local context for making the aggregated long-range relationship being distributed more accurately in local regions. In particular, we design a novel local distribution module which models the affinity map between global and local relationship for each pixel adaptively. Integrating existing global aggregation modules, we show that our approach can be modularized as an end-to-end trainable block and easily plugged into existing semantic segmentation networks, giving rise to the GALD networks. Despite its simplicity and versatility, our approach allows us to build new state of the art on major semantic segmentation benchmarks including Cityscapes, ADE20K, Pascal Context, Camvid and COCO-stuff. Code and trained models are released at https://github.com/lxtGH/GALD-DGCNet to foster further research.},
  archive      = {J_TIP},
  author       = {Xiangtai Li and Li Zhang and Guangliang Cheng and Kuiyuan Yang and Yunhai Tong and Xiatian Zhu and Tao Xiang},
  doi          = {10.1109/TIP.2021.3099366},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6829-6842},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Global aggregation then local distribution for scene parsing},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Cascaded convolutional neural network-based hyperspectral
image resolution enhancement via an auxiliary panchromatic image.
<em>TIP</em>, <em>30</em>, 6815–6828. (<a
href="https://doi.org/10.1109/TIP.2021.3098246">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Owing to the limits of incident energy and hardware system, hyperspectral (HS) images always suffer from low spatial resolution, compared with multispectral (MS) or panchromatic (PAN) images. Therefore, image fusion has emerged as a useful technology that is able to combine the characteristics of high spectral and spatial resolutions of HS and PAN/MS images. In this paper, a novel HS and PAN image fusion method based on convolutional neural network (CNN) is proposed. The proposed method incorporates the ideas of both hyper-sharpening and MS pan-sharpening techniques, thereby employing a two-stage cascaded CNN to reconstruct the anticipated high-resolution HS image. Technically, the proposed CNN architecture consists of two sub-networks, the detail injection sub-network and unmixing sub-network. The former aims at producing a latent high-resolution MS image, whereas the latter estimates the desired high-resolution abundance maps by exploring the spatial and spectral information of both HS and MS images. Moreover, two model-training fashions are presented in this paper for the sake of effectively training our network. Experiments on simulated and real remote sensing data demonstrate that the proposed method can improve the spatial resolution and spectral fidelity of HS image, and achieve better performance than some state-of-the-art HS pan-sharpening algorithms.},
  archive      = {J_TIP},
  author       = {Xiaochen Lu and Junping Zhang and Dezheng Yang and Longting Xu and FengDe Jia},
  doi          = {10.1109/TIP.2021.3098246},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6815-6828},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Cascaded convolutional neural network-based hyperspectral image resolution enhancement via an auxiliary panchromatic image},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). No-reference quality assessment for screen content images
using visual edge model and AdaBoosting neural network. <em>TIP</em>,
<em>30</em>, 6801–6814. (<a
href="https://doi.org/10.1109/TIP.2021.3098245">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a competitive no-reference metric is proposed to assess the perceptive quality of screen content images (SCIs), which uses the human visual edge model and AdaBoosting neural network. Inspired by the existing theory that the edge information which reflects the visual quality of SCI is effectively captured by the human visual difference of the Gaussian (DOG) model, we compute two types of multi-scale edge maps via the DOG operator firstly. Specifically, two types of edge maps contain contour and edge information respectively. Then after locally normalizing edge maps, L-moments distribution estimation is utilized to fit their DOG coefficients, and the fitted L-moments parameters can be regarded as edge features. Finally, to obtain the final perceptive quality score, we use an AdaBoosting back-propagation neural network (ABPNN) to map the quality-aware features to the perceptual quality score of SCIs. The reason why the ABPNN is regarded as the appropriate approach for the visual quality assessment of SCIs is that we abandon the regression network with a shallow structure, try a regression network with a deep architecture, and achieve a good generalization ability. The proposed method delivers highly competitive performance and shows high consistency with the human visual system (HVS) on the public SCI-oriented databases.},
  archive      = {J_TIP},
  author       = {Jiachen Yang and Zilin Bian and Jiacheng Liu and Bin Jiang and Wen Lu and Xinbo Gao and Houbing Song},
  doi          = {10.1109/TIP.2021.3098245},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6801-6814},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {No-reference quality assessment for screen content images using visual edge model and AdaBoosting neural network},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Estimating human pose efficiently by parallel pyramid
networks. <em>TIP</em>, <em>30</em>, 6785–6800. (<a
href="https://doi.org/10.1109/TIP.2021.3097836">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Good performance and high efficiency are both critical for estimating human pose in practice. Recent state-of-the-art methods have greatly boosted the pose detection accuracy through deep convolutional neural networks, however, the strong performance is typically achieved without high efficiency. In this paper, we design a novel network architecture for human pose estimation, which aims to strike a fine balance between speed and accuracy. Two essential tasks for successful pose estimation, preserving spatial location and extracting semantic information, are handled separately in the proposed architecture. Semantic knowledge of joint type is obtained through deep and wide sub-networks with low-resolution input, and high-resolution features indicating joint location are processed by shallow and narrow sub-networks. Because accurate semantic analysis mainly asks for adequate depth and width of the network and precise spatial information mostly requests preserving high-resolution features, good results can be produced by fusing the outputs of the sub-networks. Moreover, the computational cost can be considerably reduced comparing with existing networks, since the main part of the proposed network only deals with low-resolution features. We refer to the architecture as “parallel pyramid” network (PPNet), as features of different resolutions are processed at different levels of the hierarchical model. The superiority of our network is empirically demonstrated on two benchmark datasets: the MPII Human Pose dataset and the COCO keypoint detection dataset. PPNet outcompetes all recent methods by using less computation and memory to achieve better human pose estimation results.},
  archive      = {J_TIP},
  author       = {Lin Zhao and Nannan Wang and Chen Gong and Jian Yang and Xinbo Gao},
  doi          = {10.1109/TIP.2021.3097836},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6785-6800},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Estimating human pose efficiently by parallel pyramid networks},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Joint learning of latent similarity and local embedding for
multi-view clustering. <em>TIP</em>, <em>30</em>, 6772–6784. (<a
href="https://doi.org/10.1109/TIP.2021.3096086">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spectral clustering has been an attractive topic in the field of computer vision due to the extensive growth of applications, such as image segmentation, clustering and representation. In this problem, the construction of the similarity matrix is a vital element affecting clustering performance. In this paper, we propose a multi-view joint learning (MVJL) framework to achieve both a reliable similarity matrix and a latent low-dimensional embedding. Specifically, the similarity matrix to be learned is represented as a convex hull of similarity matrices from different views, where the nuclear norm is imposed to capture the principal information of multiple views and improve robustness against noise/outliers. Moreover, an effective low-dimensional representation is obtained by applying local embedding on the similarity matrix, which preserves the local intrinsic structure of data through dimensionality reduction. With these techniques, we formulate the MVJL as a joint optimization problem and derive its mathematical solution with the alternating direction method of multipliers strategy and the proximal gradient descent method. The solution, which consists of a similarity matrix and a low-dimensional representation, is ultimately integrated with spectral clustering or K-means for multi-view clustering. Extensive experimental results on real-world datasets demonstrate that MVJL achieves superior clustering performance over other state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Aiping Huang and Weiling Chen and Tiesong Zhao and Chang Wen Chen},
  doi          = {10.1109/TIP.2021.3096086},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6772-6784},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Joint learning of latent similarity and local embedding for multi-view clustering},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). PlenoptiCam v1.0: A light-field imaging framework.
<em>TIP</em>, <em>30</em>, 6757–6771. (<a
href="https://doi.org/10.1109/TIP.2021.3095671">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Light-field cameras play a vital role for rich 3D information retrieval in narrow range depth sensing applications. The key obstacle in composing light-fields from exposures taken by a plenoptic camera is to computationally calibrate, align and rearrange four-dimensional image data. Several attempts have been proposed to enhance the overall image quality by tailoring pipelines dedicated to particular plenoptic cameras and improving the consistency across viewpoints at the expense of high computational loads. The framework presented herein advances prior outcomes thanks to its novel micro image scale-space analysis for generic camera calibration independent of the lens specifications and its parallax-invariant, cost-effective viewpoint color equalization from optimal transport theory. Artifacts from the sensor and micro lens grid are compensated in an innovative way to enable superior quality in sub-aperture image extraction, computational refocusing and Scheimpflug rendering with sub-sampling capabilities. Benchmark comparisons using established image metrics suggest that our proposed pipeline outperforms state-of-the-art tool chains in the majority of cases. Results from a Wasserstein distance further show that our color transfer outdoes the existing transport methods. Our algorithms are released under an open-source license, offer cross-platform compatibility with few dependencies and different user interfaces. This makes the reproduction of results and experimentation with plenoptic camera technology convenient for peer researchers, developers, photographers, data scientists and others working in this field.},
  archive      = {J_TIP},
  author       = {Christopher Hahne and Amar Aggoun},
  doi          = {10.1109/TIP.2021.3095671},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6757-6771},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {PlenoptiCam v1.0: A light-field imaging framework},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fast manifold ranking with local bipartite graph.
<em>TIP</em>, <em>30</em>, 6744–6756. (<a
href="https://doi.org/10.1109/TIP.2021.3096082">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {During the past decades, manifold ranking has been widely applied to content-based image retrieval and shown excellent performance. However, manifold ranking is computationally expensive in both graph construction and ranking learning. Much effort has been devoted to improve its performance by introducing approximating techniques. In this paper, we propose a fast manifold ranking method, namely Local Bipartite Manifold Ranking (LBMR). Given a set of images, we first extract multiple regions from each image to form a large image descriptor matrix, and then use the anchor-based strategy to construct a local bipartite graph in which a regional k-means (RKM) is proposed to obtain high quality anchors. We propose an iterative method to directly solve the manifold ranking problem from the local bipartite graph, which monotonically decreases the objective function value in each iteration until the algorithm converges. Experimental results on several real-world image datasets demonstrate the effectiveness and efficiency of our proposed method.},
  archive      = {J_TIP},
  author       = {Xiaojun Chen and Yuzhong Ye and Qingyao Wu and Feiping Nie},
  doi          = {10.1109/TIP.2021.3096082},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6744-6756},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Fast manifold ranking with local bipartite graph},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Re-attention for visual question answering. <em>TIP</em>,
<em>30</em>, 6730–6743. (<a
href="https://doi.org/10.1109/TIP.2021.3097180">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A simultaneous understanding of questions and images is crucial in Visual Question Answering (VQA). While the existing models have achieved satisfactory performance by associating questions with key objects in images, the answers also contain rich information that can be used to describe the visual contents in images. In this paper, we propose a re-attention framework to utilize the information in answers for the VQA task. The framework first learns the initial attention weights for the objects by calculating the similarity of each word-object pair in the feature space. Then, the visual attention map is reconstructed by re-attending the objects in images based on the answer. Through keeping the initial visual attention map and the reconstructed one to be consistent, the learned visual attention map can be corrected by the answer information. Besides, we introduce a gate mechanism to automatically control the contribution of re-attention to model training based on the entropy of the learned initial visual attention maps. We conduct experiments on three benchmark datasets, and the results demonstrate the proposed model performs favorably against state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Wenya Guo and Ying Zhang and Jufeng Yang and Xiaojie Yuan},
  doi          = {10.1109/TIP.2021.3097180},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6730-6743},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Re-attention for visual question answering},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hierarchical connectivity-centered clustering for
unsupervised domain adaptation on person re-identification.
<em>TIP</em>, <em>30</em>, 6715–6729. (<a
href="https://doi.org/10.1109/TIP.2021.3094140">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised domain adaptation (UDA) on person Re-Identification (ReID) aims to transfer the knowledge from a labeled source domain to an unlabeled target domain. Recent works mainly optimize the ReID models with pseudo labels generated by unsupervised clustering on the target domain. However, the pseudo labels generated by the unsupervised clustering methods are often unreliable, due to the severe intra-person variations and complicated cluster structures in the practical application scenarios. In this work, to handle the complicated cluster structures, we propose a novel learnable Hierarchical Connectivity-Centered (HCC) clustering scheme by Graph Convolutional Networks (GCNs) to generate more reliable pseudo labels. Our HCC scheme learns the complicated cluster structure by hierarchically estimating the connectivity among samples from the vertex level to cluster level in a graph representation, and thereby progressively refines the pseudo labels. Additionally, to handle the intra-person variations in clustering, we propose a novel relation feature for HCC clustering, which exploits the identities from the source domain as references to represent target domain samples. Experiments demonstrate that our method is able to achieve state-of-the art performance on three challenging benchmarks.},
  archive      = {J_TIP},
  author       = {Yan Bai and Ce Wang and Yihang Lou and Jun Liu and Ling-Yu Duan},
  doi          = {10.1109/TIP.2021.3094140},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6715-6729},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Hierarchical connectivity-centered clustering for unsupervised domain adaptation on person re-identification},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Interpreting bottom-up decision-making of CNNs via
hierarchical inference. <em>TIP</em>, <em>30</em>, 6701–6714. (<a
href="https://doi.org/10.1109/TIP.2021.3097187">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the great success of convolutional neural networks (CNNs), interpretation of their internal network mechanism has been increasingly critical, while the network decision-making logic is still an open issue. In the bottom-up hierarchical logic of neuroscience, the decision-making process can be deduced from a series of sub-decision-making processes from low to high levels. Inspired by this, we propose the Concept-harmonized HierArchical INference (CHAIN) interpretation scheme. In CHAIN, a network decision-making process from shallow to deep layers is interpreted by the hierarchical backward inference based on visual concepts from high to low semantic levels. Firstly, we learned a general hierarchical visual-concept representation in CNN layered feature space by concept harmonizing model on a large concept dataset. Secondly, for interpreting a specific network decision-making process, we conduct the concept-harmonized hierarchical inference backward from the highest to the lowest semantic level. Specifically, the network learning for a target concept at a deeper layer is disassembled into that for concepts at shallower layers. Finally, a specific network decision-making process is explained as a form of concept-harmonized hierarchical inference, which is intuitively comparable to the bottom-up hierarchical visual recognition way. Quantitative and qualitative experiments demonstrate the effectiveness of the proposed CHAIN at both instance and class levels.},
  archive      = {J_TIP},
  author       = {Dan Wang and Xinrui Cui and Xun Chen and Rabab Ward and Z. Jane Wang},
  doi          = {10.1109/TIP.2021.3097187},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6701-6714},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Interpreting bottom-up decision-making of CNNs via hierarchical inference},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Erratum to “multi-view face synthesis via progressive face
flow.” <em>TIP</em>, <em>30</em>, 6700. (<a
href="https://doi.org/10.1109/TIP.2021.3097165">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the above article [1] , unfortunately, Fig. 5 was not displayed correctly with many empty images. The correct version is supplemented here.},
  archive      = {J_TIP},
  author       = {Yangyang Xu and Xuemiao Xu and Jianbo Jiao and Keke Li and Cheng Xu and Shengfeng He},
  doi          = {10.1109/TIP.2021.3097165},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6700},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Erratum to “Multi-view face synthesis via progressive face flow”},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). FloorLevel-net: Recognizing floor-level lines with
height-attention-guided multi-task learning. <em>TIP</em>, <em>30</em>,
6686–6699. (<a href="https://doi.org/10.1109/TIP.2021.3096090">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ability to recognize the position and order of the floor-level lines that divide adjacent building floors can benefit many applications, for example, urban augmented reality (AR). This work tackles the problem of locating floor-level lines in street-view images, using a supervised deep learning approach. Unfortunately, very little data is available for training such a network - current street-view datasets contain either semantic annotations that lack geometric attributes, or rectified facades without perspective priors. To address this issue, we first compile a new dataset and develop a new data augmentation scheme to synthesize training samples by harassing (i) the rich semantics of existing rectified facades and (ii) perspective priors of buildings in diverse street views. Next, we design FloorLevel-Net, a multi-task learning network that associates explicit features of building facades and implicit floor-level lines, along with a height-attention mechanism to help enforce a vertical ordering of floor-level lines. The generated segmentations are then passed to a second-stage geometry post-processing to exploit self-constrained geometric priors for plausible and consistent reconstruction of floor-level lines. Quantitative and qualitative evaluations conducted on assorted facades in existing datasets and street views from Google demonstrate the effectiveness of our approach. Also, we present context-aware image overlay results and show the potentials of our approach in enriching AR-related applications. Project website: https://wumengyangok.github.io/Project/FloorLevelNet.},
  archive      = {J_TIP},
  author       = {Mengyang Wu and Wei Zeng and Chi-Wing Fu},
  doi          = {10.1109/TIP.2021.3096090},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6686-6699},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {FloorLevel-net: Recognizing floor-level lines with height-attention-guided multi-task learning},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Better compression with deep pre-editing. <em>TIP</em>,
<em>30</em>, 6673–6685. (<a
href="https://doi.org/10.1109/TIP.2021.3096085">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Could we compress images via standard codecs while avoiding visible artifacts? The answer is obvious - this is doable as long as the bit budget is generous enough. What if the allocated bit-rate for compression is insufficient? Then unfortunately, artifacts are a fact of life. Many attempts were made over the years to fight this phenomenon, with various degrees of success. In this work we aim to break the unholy connection between bit-rate and image quality, and propose a way to circumvent compression artifacts by pre-editing the incoming image and modifying its content to fit the given bits. We design this editing operation as a learned convolutional neural network, and formulate an optimization problem for its training. Our loss takes into account a proximity between the original image and the edited one, a bit-budget penalty over the proposed image, and a no-reference image quality measure for forcing the outcome to be visually pleasing. The proposed approach is demonstrated on the popular JPEG compression, showing savings in bits and/or improvements in visual quality, obtained with intricate editing effects.},
  archive      = {J_TIP},
  author       = {Hossein Talebi and Damien Kelly and Xiyang Luo and Ignacio Garcia Dorado and Feng Yang and Peyman Milanfar and Michael Elad},
  doi          = {10.1109/TIP.2021.3096085},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6673-6685},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Better compression with deep pre-editing},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Improving weakly supervised temporal action localization by
exploiting multi-resolution information in temporal domain.
<em>TIP</em>, <em>30</em>, 6659–6672. (<a
href="https://doi.org/10.1109/TIP.2021.3089355">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weakly supervised temporal action localization is a challenging task as only the video-level annotation is available during the training process. To address this problem, we propose a two-stage approach to generate high-quality frame-level pseudo labels by fully exploiting multi-resolution information in the temporal domain and complementary information between the appearance (i.e., RGB) and motion (i.e., optical flow) streams. In the first stage, we propose an Initial Label Generation (ILG) module to generate reliable initial frame-level pseudo labels. Specifically, in this newly proposed module, we exploit temporal multi-resolution consistency and cross-stream consistency to generate high quality class activation sequences (CASs), which consist of a number of sequences with each sequence measuring how likely each video frame belongs to one specific action class. In the second stage, we propose a Progressive Temporal Label Refinement (PTLR) framework to iteratively refine the pseudo labels, in which we use a set of selected frames with highly confident pseudo labels to progressively train two networks and better predict action class scores at each frame. Specifically, in our newly proposed PTLR framework, two networks called Network-OTS and Network-RTS, which are respectively used to generate CASs for the original temporal scale and the reduced temporal scales, are used as two streams (i.e., the OTS stream and the RTS stream) to refine the pseudo labels in turn. By this way, multi-resolution information in the temporal domain is exchanged at the pseudo label level, and our work can help improve each network/stream by exploiting the refined pseudo labels from another network/stream. Comprehensive experiments on two benchmark datasets THUMOS14 and ActivityNet v1.3 demonstrate the effectiveness of our newly proposed method for weakly supervised temporal action localization.},
  archive      = {J_TIP},
  author       = {Rui Su and Dong Xu and Luping Zhou and Wanli Ouyang},
  doi          = {10.1109/TIP.2021.3089355},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6659-6672},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Improving weakly supervised temporal action localization by exploiting multi-resolution information in temporal domain},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adversarial multi-path residual network for image
super-resolution. <em>TIP</em>, <em>30</em>, 6648–6658. (<a
href="https://doi.org/10.1109/TIP.2021.3096089">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, deep convolutional neural networks have demonstrated remarkable progresses on single image super-resolution (SR) problem. However, most of them use more deeper and wider networks to improve SR performance, which is not practical in real-world applications due to large complexity, high computation cost, and low efficiency. In addition, they cannot provide high perception quality and guarantee objective quality simultaneously. To address these limitations, we in this paper propose a novel A dversarial M ulti- p ath R esidual N etwork (AMPRN), which can largely suppress the number of network parameters and achieve a higher SR performance compared with the state-of-the-art methods. More specifically, we propose a multi-path residual block (MPRB) for multi-path residual network (MPRN) with fewer network parameters, which can extract abundant local features by fully using features from different paths generated by channel slices. These hierarchical features from all the MPRBs are then jointly aggregated by global gradual feature fusion. Following MPRN, we construct an adversarial gradient network with a gradient loss to make the gradient distribution of the generated SR images and ground truth image closer. In this way, the generated SR images of our model can provide high perception quality and objective quality. Finally, several experimental results demonstrate that our AMPRN achieves better performance in comparison with fewer parameters than the state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Qianqian Wang and Quanxue Gao and Linlu Wu and Gan Sun and Licheng Jiao},
  doi          = {10.1109/TIP.2021.3096089},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6648-6658},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Adversarial multi-path residual network for image super-resolution},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Reference-based defect detection network. <em>TIP</em>,
<em>30</em>, 6637–6647. (<a
href="https://doi.org/10.1109/TIP.2021.3096067">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The defect detection task can be regarded as a realistic scenario of object detection in the computer vision field and it is widely used in the industrial field. Directly applying vanilla object detector to defect detection task can achieve promising results, while there still exists challenging issues that have not been solved. The first issue is the texture shift which means a trained defect detector model will be easily affected by unseen texture, and the second issue is partial visual confusion which indicates that a partial defect box is visually similar with a complete box. To tackle these two problems, we propose a Reference-based Defect Detection Network (RDDN). Specifically, we introduce template reference and context reference to against those two problems, respectively. Template reference can reduce the texture shift from image, feature or region levels, and encourage the detectors to focus more on the defective area as a result. We can use either well-aligned template images or the outputs of a pseudo template generator as template references in this work, and they are jointly trained with detectors by the supervision of normal samples. To solve the partial visual confusion issue, we propose to leverage the carried context information of context reference, which is the concentric bigger box of each region proposal, to perform more accurate region classification and regression. Experiments on two defect detection datasets demonstrate the effectiveness of our proposed approach.},
  archive      = {J_TIP},
  author       = {Zhaoyang Zeng and Bei Liu and Jianlong Fu and Hongyang Chao},
  doi          = {10.1109/TIP.2021.3096067},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6637-6647},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Reference-based defect detection network},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Reduced reference perceptual quality model with application
to rate control for video-based point cloud compression. <em>TIP</em>,
<em>30</em>, 6623–6636. (<a
href="https://doi.org/10.1109/TIP.2021.3096060">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In rate-distortion optimization, the encoder settings are determined by maximizing a reconstruction quality measure subject to a constraint on the bitrate. One of the main challenges of this approach is to define a quality measure that can be computed with low computational cost and which correlates well with the perceptual quality. While several quality measures that fulfil these two criteria have been developed for images and videos, no such one exists for point clouds. We address this limitation for the video-based point cloud compression (V-PCC) standard by proposing a linear perceptual quality model whose variables are the V-PCC geometry and color quantization step sizes and whose coefficients can easily be computed from two features extracted from the original point cloud. Subjective quality tests with 400 compressed point clouds show that the proposed model correlates well with the mean opinion score, outperforming state-of-the-art full reference objective measures in terms of Spearman rank-order and Pearson linear correlation coefficient. Moreover, we show that for the same target bitrate, rate-distortion optimization based on the proposed model offers higher perceptual quality than rate-distortion optimization based on exhaustive search with a point-to-point objective quality metric. Our datasets are publicly available at https://github.com/qdushl/Waterloo-Point-Cloud-Database-2.0 .},
  archive      = {J_TIP},
  author       = {Qi Liu and Hui Yuan and Raouf Hamzaoui and Honglei Su and Junhui Hou and Huan Yang},
  doi          = {10.1109/TIP.2021.3096060},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6623-6636},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Reduced reference perceptual quality model with application to rate control for video-based point cloud compression},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Self-supervised colorization towards monochrome-color camera
systems using cycle CNN. <em>TIP</em>, <em>30</em>, 6609–6622. (<a
href="https://doi.org/10.1109/TIP.2021.3096385">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Colorization in monochrome-color camera systems aims to colorize the gray image ${{\mathrm{I}}_{\mathrm{G}}}$ from the monochrome camera using the color image ${{\mathrm{R}}_{\mathrm{C}}}$ from the color camera as reference. Since monochrome cameras have better imaging quality than color cameras, the colorization can help obtain higher quality color images. Related learning based methods usually simulate the monochrome-color camera systems to generate the synthesized data for training, due to the lack of ground-truth color information of the gray image in the real data. However, the methods that are trained relying on the synthesized data may get poor results when colorizing real data, because the synthesized data may deviate from the real data. We present a self-supervised CNN model, named Cycle CNN, which can directly use the real data from monochrome-color camera systems for training. In detail, we use the Weighted Average Colorization (WAC) network to do the colorization twice. First, we colorize ${{\mathrm{I}}_{\mathrm{G}}}$ using ${{\mathrm{R}}_{\mathrm{C}}}$ as reference to obtain the first-time colorization result ${{\mathrm{I}}_{\mathrm{C}}}$ . Second, we colorize the de-colored map of ${{\mathrm{R}}_{\mathrm{C}}}$ , i.e. ${{\mathrm{R}}_{\mathrm{G}}}$ , using the concatenated image of ${{\mathrm{I}}_{\mathrm{G}}}$ and Cb/Cr channels of the first-time colorization result ${{\mathrm{I}}_{\mathrm{C}}}$ , i.e. ${{\mathrm{I}}_{\mathrm{C}}^{Cb}}$ and ${{\mathrm{I}}_{\mathrm{C}}^{Cr}}$ , as reference to obtain the second-time colorization result ${\mathrm{R}}_{\mathrm{C}}^{{ {&#39;}}}$ . In this way, for the second-time colorization result ${\mathrm{R}}_{\mathrm{C}}^{{ {&#39;}}}$ , we use the Cb and Cr channels of the original color map ${{\mathrm{R}}_{\mathrm{C}}}$ as ground-truth and introduce the cycle consistency loss to push ${\mathrm{R}}_{\mathrm{C}}^{{ {&#39;}}Cb/Cr} \approx {\mathrm{R}}_{\mathrm{C}}^{Cb/Cr}$ . Also, for the $Y$ channel of the first-time colorization result ${{\mathrm{I}}_{\mathrm{C}}^{Y}}$ , we propose the Global Curve Adjustment (GCA) network and the structure similarity loss to encourage the structure similarity between ${{\mathrm{I}}_{\mathrm{C}}^{Y}}$ and ${{\mathrm{I}}_{\mathrm{G}}}$ . In addition, we introduce a spatial smoothness loss within the WAC network to encourage spatial smoothness of the colorization result. Combining all these losses, we could train the Cycle CNN using the real data in the absence of the ground-truth color information of ${{\mathrm{I}}_{\mathrm{G}}}$ . Experimental results show that we can outperform related methods largely for colorizing real data.},
  archive      = {J_TIP},
  author       = {Xuan Dong and Chang Liu and Weixin Li and Xiaoyan Hu and Xiaojie Wang and Yunhong Wang},
  doi          = {10.1109/TIP.2021.3096385},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6609-6622},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Self-supervised colorization towards monochrome-color camera systems using cycle CNN},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Global and local texture randomization for synthetic-to-real
semantic segmentation. <em>TIP</em>, <em>30</em>, 6594–6608. (<a
href="https://doi.org/10.1109/TIP.2021.3096334">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic segmentation is a crucial image understanding task, where each pixel of image is categorized into a corresponding label. Since the pixel-wise labeling for ground-truth is tedious and labor intensive, in practical applications, many works exploit the synthetic images to train the model for real-word image semantic segmentation, i.e., Synthetic-to-Real Semantic Segmentation (SRSS). However, Deep Convolutional Neural Networks (CNNs) trained on the source synthetic data may not generalize well to the target real-world data. To address this problem, there has been rapidly growing interest in Domain Adaption technique to mitigate the domain mismatch between the synthetic and real-world images. Besides, Domain Generalization technique is another solution to handle SRSS. In contrast to Domain Adaption, Domain Generalization seeks to address SRSS without accessing any data of the target domain during training. In this work, we propose two simple yet effective texture randomization mechanisms, Global Texture Randomization (GTR) and Local Texture Randomization (LTR), for Domain Generalization based SRSS. GTR is proposed to randomize the texture of source images into diverse unreal texture styles. It aims to alleviate the reliance of the network on texture while promoting the learning of the domain-invariant cues. In addition, we find the texture difference is not always occurred in entire image and may only appear in some local areas. Therefore, we further propose a LTR mechanism to generate diverse local regions for partially stylizing the source images. Finally, we implement a regularization of Consistency between GTR and LTR (CGL) aiming to harmonize the two proposed mechanisms during training. Extensive experiments on five publicly available datasets (i.e., GTA5, SYNTHIA, Cityscapes, BDDS and Mapillary) with various SRSS settings (i.e., GTA5/SYNTHIA to Cityscapes/BDDS/Mapillary) demonstrate that the proposed method is superior to the state-of-the-art methods for domain generalization based SRSS.},
  archive      = {J_TIP},
  author       = {Duo Peng and Yinjie Lei and Lingqiao Liu and Pingping Zhang and Jun Liu},
  doi          = {10.1109/TIP.2021.3096334},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6594-6608},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Global and local texture randomization for synthetic-to-real semantic segmentation},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). IPGN: Interactiveness proposal graph network for
human-object interaction detection. <em>TIP</em>, <em>30</em>,
6583–6593. (<a href="https://doi.org/10.1109/TIP.2021.3096333">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human-Object Interaction (HOI) Detection is an important task to understand how humans interact with objects. Most of the existing works treat this task as an exhaustive triplet 〈human, verb, object 〉 classification problem. In this paper, we decompose it and propose a novel two-stage graph model to learn the knowledge of interactiveness and interaction in one network, namely, Interactiveness Proposal Graph Network (IPGN). In the first stage, we design a fully connected graph for learning the interactiveness, which distinguishes whether a pair of human and object is interactive or not. Concretely, it generates the interactiveness features to encode high-level semantic interactiveness knowledge for each pair. The class-agnostic interactiveness is a more general and simpler objective, which can be used to provide reasonable proposals for the graph construction in the second stage. In the second stage, a sparsely connected graph is constructed with all interactive pairs selected by the first stage. Specifically, we use the interactiveness knowledge to guide the message passing. By contrast with the feature similarity, it explicitly represents the connections between the nodes. Benefiting from the valid graph reasoning, the node features are well encoded for interaction learning. Experiments show that the proposed method achieves state-of-the-art performance on both V-COCO and HICO-DET datasets.},
  archive      = {J_TIP},
  author       = {Haoran Wang and Licheng Jiao and Fang Liu and Lingling Li and Xu Liu and Deyi Ji and Weihao Gan},
  doi          = {10.1109/TIP.2021.3096333},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6583-6593},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {IPGN: Interactiveness proposal graph network for human-object interaction detection},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Semi-supervised image deraining using gaussian processes.
<em>TIP</em>, <em>30</em>, 6570–6582. (<a
href="https://doi.org/10.1109/TIP.2021.3096323">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent CNN-based methods for image deraining have achieved excellent performance in terms of reconstruction error as well as visual quality. However, these methods are limited in the sense that they can be trained only on fully labeled data. Due to various challenges in obtaining real world fully-labeled image deraining datasets, existing methods are trained only on synthetically generated data and hence, generalize poorly to real-world images. The use of real-world data in training image deraining networks is relatively less explored in the literature. We propose a Gaussian Process-based semi-supervised learning framework which enables the network in learning to derain using synthetic dataset while generalizing better using unlabeled real-world images. More specifically, we model the latent space vectors of unlabeled data using Gaussian Processes, which is then used to compute pseudo-ground-truth for supervising the network on unlabeled data. The pseudo ground-truth is further used to supervise the network at the intermediate level for the unlabeled data. Through extensive experiments and ablations on several challenging datasets (such as Rain800, Rain200L and DDN-SIRR), we show that the proposed method is able to effectively leverage unlabeled data thereby resulting in significantly better performance as compared to labeled-only training. Additionally, we demonstrate that using unlabeled real-world images in the proposed GP-based framework results in superior performance as compared to the existing methods. Code is available at: https://github.com/rajeevyasarla/Syn2Real.},
  archive      = {J_TIP},
  author       = {Rajeev Yasarla and Vishwanath A. Sindagi and Vishal M. Patel},
  doi          = {10.1109/TIP.2021.3096323},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6570-6582},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Semi-supervised image deraining using gaussian processes},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bayer CFA pattern compression with JPEG XS. <em>TIP</em>,
<em>30</em>, 6557–6569. (<a
href="https://doi.org/10.1109/TIP.2021.3095421">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While traditional image compression algorithms take a full three-component color representation of an image as input, capturing of such images is done in many applications with Bayer CFA pattern sensors that provide only a single color information per sensor element and position. In order to avoid additional complexity at the encoder side, such CFA pattern images can be compressed directly without prior conversion to a full color image. In this paper, we describe a recent activity of the JPEG committee (ISO SC 29 WG 1) to develop such a compression algorithm in the framework of JPEG XS. It turns out that it is important to understand the “development process” from CFA patterns to full color images in order to optimize the image quality of such a compression algorithm, which we will also describe shortly. We introduce (1) a novel decorrelation step upfront processing (the so-called Star-Tetrix transform), along with (2) a pre-emphasis function to improve the compression efficiency of the subsequent compression algorithm (here, JPEG XS). Our experiments clearly indicate a gain over a RGB compression workflow in terms of complexity and quality (between 1.5dB and more than 4dB depending on the target bitrate). A comparison is also made with other state-of-the-art CFA compression techniques.},
  archive      = {J_TIP},
  author       = {Thomas Richter and Siegfried Fößel and Antonin Descampe and Gaël Rouvroy},
  doi          = {10.1109/TIP.2021.3095421},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6557-6569},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Bayer CFA pattern compression with JPEG XS},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning deep global multi-scale and local attention
features for facial expression recognition in the wild. <em>TIP</em>,
<em>30</em>, 6544–6556. (<a
href="https://doi.org/10.1109/TIP.2021.3093397">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial expression recognition (FER) in the wild received broad concerns in which occlusion and pose variation are two key issues. This paper proposed a global multi-scale and local attention network (MA-Net) for FER in the wild. Specifically, the proposed network consists of three main components: a feature pre-extractor, a multi-scale module, and a local attention module. The feature pre-extractor is utilized to pre-extract middle-level features, the multi-scale module to fuse features with different receptive fields, which reduces the susceptibility of deeper convolution towards occlusion and variant pose, while the local attention module can guide the network to focus on local salient features, which releases the interference of occlusion and non-frontal pose problems on FER in the wild. Extensive experiments demonstrate that the proposed MA-Net achieves the state-of-the-art results on several in-the-wild FER benchmarks: CAER-S, AffectNet-7, AffectNet-8, RAFDB, and SFEW with accuracies of 88.42\%, 64.53\%, 60.29\%, 88.40\%, and 59.40\% respectively. The codes and training logs are publicly available at https://github.com/zengqunzhao/MA-Net.},
  archive      = {J_TIP},
  author       = {Zengqun Zhao and Qingshan Liu and Shanmin Wang},
  doi          = {10.1109/TIP.2021.3093397},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6544-6556},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Learning deep global multi-scale and local attention features for facial expression recognition in the wild},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Non-contact heart rate estimation via adaptive RGB/NIR
signal fusion. <em>TIP</em>, <em>30</em>, 6528–6543. (<a
href="https://doi.org/10.1109/TIP.2021.3094739">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a non-contact heart rate (HR) estimation method that is robust to various situations, such as bright, low-light, and varying illumination scenes. We utilize a camera that records red, green, and blue (RGB) and near-infrared (NIR) information to capture the subtle skin color changes induced by the cardiac pulse of a person. The key novelty of our method is the adaptive fusion of RGB and NIR signals for HR estimation based on the analysis of background illumination variations. RGB signals are suitable indicators for HR estimation in bright scenes. Conversely, NIR signals are more reliable than RGB signals in scenes with more complex illumination, as they can be captured independently of the changes in background illumination. By measuring the correlations between the lights reflected from the background and facial regions, we adaptively utilize RGB and NIR observations for HR estimation. The experiments demonstrate the effectiveness of the proposed method.},
  archive      = {J_TIP},
  author       = {Kosuke Kurihara and Daisuke Sugimura and Takayuki Hamamoto},
  doi          = {10.1109/TIP.2021.3094739},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6528-6543},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Non-contact heart rate estimation via adaptive RGB/NIR signal fusion},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). BiSPL: Bidirectional self-paced learning for recognition
from web data. <em>TIP</em>, <em>30</em>, 6512–6527. (<a
href="https://doi.org/10.1109/TIP.2021.3094744">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning (DL) is inherently subject to the requirement of a large amount of well-labeled data, which is expensive and time-consuming to obtain manually. In order to broaden the reach of DL, leveraging free web data becomes an attractive strategy to alleviate the issue of data scarcity. However, directly utilizing collected web data to train a deep model is ineffective because of the mixed noisy data. To address such problems, we develop a novel bidirectional self-paced learning (BiSPL) framework which reduces the effect of noise by learning from web data in a meaningful order. Technically, the BiSPL framework consists of two essential steps. Relying on distances defined between web samples and labeled source samples, first, the web samples with short distances are sampled and combined to form a new training set. Second, based on the new training set, both easy and hard samples are initially employed to train deep models for higher stability, and hard samples are gradually dropped to reduce the noise as the training progresses. By iteratively alternating such steps, deep models converge to a better solution. We mainly focus on the fine-grained visual classification (FGVC) tasks because their corresponding datasets are generally small and therefore face a more significant data scarcity problem. Experiments conducted on six public FGVC tasks demonstrate that our proposed method outperforms the state-of-the-art approaches. Especially, BiSPL suffices to achieve the highest stable performance when the scale of the well-labeled training set decreases dramatically.},
  archive      = {J_TIP},
  author       = {Xiaoping Wu and Jianlong Chang and Yu-Kun Lai and Jufeng Yang and Qi Tian},
  doi          = {10.1109/TIP.2021.3094744},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6512-6527},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {BiSPL: Bidirectional self-paced learning for recognition from web data},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Local semantic enhanced ConvNet for aerial scene
recognition. <em>TIP</em>, <em>30</em>, 6498–6511. (<a
href="https://doi.org/10.1109/TIP.2021.3092816">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aerial scene recognition is challenging due to the complicated object distribution and spatial arrangement in a large-scale aerial image. Recent studies attempt to explore the local semantic representation capability of deep learning models, but how to exactly perceive the key local regions remains to be handled. In this paper, we present a local semantic enhanced ConvNet (LSE-Net) for aerial scene recognition, which mimics the human visual perception of key local regions in aerial scenes, in the hope of building a discriminative local semantic representation. Our LSE-Net consists of a context enhanced convolutional feature extractor, a local semantic perception module and a classification layer. Firstly, we design a multi-scale dilated convolution operators to fuse multi-level and multi-scale convolutional features in a trainable manner in order to fully receive the local feature responses in an aerial scene. Then, these features are fed into our two-branch local semantic perception module. In this module, we design a context-aware class peak response (CACPR) measurement to precisely depict the visual impulse of key local regions and the corresponding context information. Also, a spatial attention weight matrix is extracted to describe the importance of each key local region for the aerial scene. Finally, the refined class confidence maps are fed into the classification layer. Exhaustive experiments on three aerial scene classification benchmarks indicate that our LSE-Net achieves the state-of-the-art performance, which validates the effectiveness of our local semantic perception module and CACPR measurement.},
  archive      = {J_TIP},
  author       = {Qi Bi and Kun Qin and Han Zhang and Gui-Song Xia},
  doi          = {10.1109/TIP.2021.3092816},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6498-6511},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Local semantic enhanced ConvNet for aerial scene recognition},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Removing adversarial noise via low-rank completion of
high-sensitivity points. <em>TIP</em>, <em>30</em>, 6485–6497. (<a
href="https://doi.org/10.1109/TIP.2021.3086596">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks are fragile under adversarial attacks. In this work, we propose to develop a new defense method based on image restoration to remove adversarial attack noise. Using the gradient information back-propagated over the network to the input image, we identify high-sensitivity keypoints which have significant contributions to the image classification performance. We then partition the image pixels into the two groups: high-sensitivity and low-sensitivity points. For low-sensitivity pixels, we use a total variation (TV) norm-based image smoothing method to remove adversarial attack noise. For those high-sensitivity keypoints, we develop a structure-preserving low-rank image completion method. Based on matrix analysis and optimization, we derive an iterative solution for this optimization problem. Our extensive experimental results on the CIFAR-10, SVHN, and Tiny-ImageNet datasets have demonstrated that our method significantly outperforms other defense methods which are based on image de-noising or restoration, especially under powerful adversarial attacks.},
  archive      = {J_TIP},
  author       = {Zhiqun Zhao and Hengyou Wang and Hao Sun and Jianhe Yuan and Zhongchao Huang and Zhihai He},
  doi          = {10.1109/TIP.2021.3086596},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6485-6497},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Removing adversarial noise via low-rank completion of high-sensitivity points},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Open-set hypothesis transfer with semantic consistency.
<em>TIP</em>, <em>30</em>, 6473–6484. (<a
href="https://doi.org/10.1109/TIP.2021.3093393">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised open-set domain adaptation (UODA) is a realistic problem where unlabeled target data contain unknown classes. Prior methods rely on the coexistence of both source and target domain data to perform domain alignment, which greatly limits their applications when source domain data are restricted due to privacy concerns. In this paper we address the challenging hypothesis transfer setting for UODA, where data from source domain are no longer available during adaptation on target domain. Specifically, we propose to use pseudo-labels and a novel consistency regularization on target data, where using conventional formulations fails in this open-set setting. Firstly, our method discovers confident predictions on target domain and performs classification with pseudo-labels. Then we enforce the model to output consistent and definite predictions on semantically similar transformed inputs, discovering all latent class semantics. As a result, unlabeled data can be classified into discriminative classes coincided with either source classes or unknown classes. We theoretically prove that under perfect semantic transformation, the proposed objective that enforces consistency can recover the information of true labels in prediction. Experimental results show that our model outperforms state-of-the-art methods on UODA benchmarks.},
  archive      = {J_TIP},
  author       = {Zeyu Feng and Chang Xu and Dacheng Tao},
  doi          = {10.1109/TIP.2021.3093393},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6473-6484},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Open-set hypothesis transfer with semantic consistency},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Patch-wise spatial-temporal quality enhancement for HEVC
compressed video. <em>TIP</em>, <em>30</em>, 6459–6472. (<a
href="https://doi.org/10.1109/TIP.2021.3092949">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, many deep learning based researches are conducted to explore the potential quality improvement of compressed videos. These methods mostly utilize either the spatial or temporal information to perform frame-level video enhancement. However, they fail in combining different spatial-temporal information to adaptively utilize adjacent patches to enhance the current patch and achieve limited enhancement performance especially on scene-changing and strong-motion videos. To overcome these limitations, we propose a patch-wise spatial-temporal quality enhancement network which firstly extracts spatial and temporal features, then recalibrates and fuses the obtained spatial and temporal features. Specifically, we design a temporal and spatial-wise attention-based feature distillation structure to adaptively utilize the adjacent patches for distilling patch-wise temporal features. For adaptively enhancing different patch with spatial and temporal information, a channel and spatial-wise attention fusion block is proposed to achieve patch-wise recalibration and fusion of spatial and temporal features. Experimental results demonstrate our network achieves peak signal-to-noise ratio improvement, 0.55 - 0.69 dB compared with the compressed videos at different quantization parameters, outperforming state-of-the-art approach.},
  archive      = {J_TIP},
  author       = {Qing Ding and Liquan Shen and Liangwei Yu and Hao Yang and Mai Xu},
  doi          = {10.1109/TIP.2021.3092949},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6459-6472},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Patch-wise spatial-temporal quality enhancement for HEVC compressed video},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Removal of micro-doppler effect of ISAR image based on
laplacian regularized nonconvex low-rank representation. <em>TIP</em>,
<em>30</em>, 6446–6458. (<a
href="https://doi.org/10.1109/TIP.2021.3094316">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The micro-Doppler (m-D) effect caused by micro-motion degrades the readability of the inverse synthetic aperture radar (ISAR) image. To achieve well-focused ISAR image of the target with the micro-motion part, this paper proposes a novel approach for the removal of m-D effect of ISAR image. Note that the range profiles of the rigid body are similar to each other, making the respective data matrix low-rank. Those of the micro-motion part, in contrary, generally fluctuate in different range cells, whose data matrix is sparse. Therefore, the removal of m-D effect can be naturally solved by the robust principal component analysis (RPCA)-a convenient convex program to decompose an auxiliary matrix into a low-rank matrix and a sparse one. In RPCA, the rank of a matrix is described by the nuclear norm, which is convex but leads to a suboptimal solution. To address it, we utilize a nonconvex surrogate, i.e., the summation of logistic function of the singular values of a matrix, to approximate the rank. Moreover, the range profiles of the rigid body are generally locally similar. To capture this geometric structured information, we further introduce a Laplacian regularization into the model. Then, the Laplacian regularized nonconvex low-rank (LRNL) model is solved efficiently by the linearized alternating direction method (ADM). Extensive experimental results based on both simulated and measured data demonstrate the effectiveness of the proposed approach on the removal of m-D effect of ISAR image.},
  archive      = {J_TIP},
  author       = {Shuanghui Zhang and Yongxiang Liu and Xiang Li and Dewen Hu},
  doi          = {10.1109/TIP.2021.3094316},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6446-6458},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Removal of micro-doppler effect of ISAR image based on laplacian regularized nonconvex low-rank representation},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SlimConv: Reducing channel redundancy in convolutional
neural networks by features recombining. <em>TIP</em>, <em>30</em>,
6434–6445. (<a href="https://doi.org/10.1109/TIP.2021.3093795">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The channel redundancy of convolutional neural networks (CNNs) results in the large consumption of memories and computational resources. In this work, we design a novel Slim Convolution (SlimConv) module to boost the performance of CNNs by reducing channel redundancies. Our SlimConv consists of three main steps: Reconstruct , Transform , and Fuse . It aims to reorganize and fuse the learned features more efficiently, such that the method can compress the model effectively. Our SlimConv is a plug-and-play architectural unit that can be used to replace convolutional layers in CNNs directly. We validate the effectiveness of SlimConv by conducting comprehensive experiments on various leading benchmarks, such as ImageNet, MS COCO2014, Pascal VOC2012 segmentation, and Pascal VOC2007 detection datasets. The experiments show that SlimConv-equipped models can achieve better performances consistently, less consumption of memory and computation resources than non-equipped counterparts. For example, the ResNet-101 fitted with SlimConv achieves 77.84\% top-1 classification accuracy with 4.87 GFLOPs and 27.96M parameters on ImageNet, which shows almost 0.5\% better performance with about 3 GFLOPs and 38\% parameters reduced.},
  archive      = {J_TIP},
  author       = {Jiaxiong Qiu and Cai Chen and Shuaicheng Liu and Heng-Yu Zhang and Bing Zeng},
  doi          = {10.1109/TIP.2021.3093795},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6434-6445},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {SlimConv: Reducing channel redundancy in convolutional neural networks by features recombining},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). OIFlow: Occlusion-inpainting optical flow estimation by
unsupervised learning. <em>TIP</em>, <em>30</em>, 6420–6433. (<a
href="https://doi.org/10.1109/TIP.2021.3093781">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Occlusion is an inevitable and critical problem in unsupervised optical flow learning. Existing methods either treat occlusions equally as non-occluded regions or simply remove them to avoid incorrectness. However, the occlusion regions can provide effective information for optical flow learning. In this paper, we present OIFlow, an occlusion-inpainting framework to make full use of occlusion regions. Specifically, a new appearance-flow network is proposed to inpaint occluded flows based on the image content. Moreover, a boundary dilated warp is proposed to deal with occlusions caused by displacement beyond the image border. We conduct experiments on multiple leading flow benchmark datasets such as Flying Chairs, KITTI and MPI-Sintel, which demonstrate that the performance is significantly improved by our proposed occlusion handling framework.},
  archive      = {J_TIP},
  author       = {Shuaicheng Liu and Kunming Luo and Nianjin Ye and Chuan Wang and Jue Wang and Bing Zeng},
  doi          = {10.1109/TIP.2021.3093781},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6420-6433},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {OIFlow: Occlusion-inpainting optical flow estimation by unsupervised learning},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A hierarchical superpixel-based approach for DIBR view
synthesis. <em>TIP</em>, <em>30</em>, 6408–6419. (<a
href="https://doi.org/10.1109/TIP.2021.3092817">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {View synthesis allows observers to explore static scenes using aligned color images and depth maps captured in a preset camera path. Among the options, depth-image-based rendering (DIBR) approaches have been effective and efficient since only one pair of color and depth map is required, saving storage and bandwidth. The present work proposes a novel DIBR pipeline for view synthesis that properly tackles the different artifacts that arise from 3D warping, such as cracks, disocclusions, ghosts, and out-of-field areas. A key aspect of our contributions relies on the adaptation and usage of a hierarchical image superpixel algorithm that helps to maintain structural characteristics of the scene during image reconstruction. We compare our approach with state-of-the-art methods and show that it attains the best average results in two common assessment metrics under public still-image and video-sequence datasets. Visual results are also provided, illustrating the potential of our technique in real-world applications.},
  archive      = {J_TIP},
  author       = {Adriano Q. de Oliveira and Thiago L. T. da Silveira and Marcelo Walter and Cláudio R. Jung},
  doi          = {10.1109/TIP.2021.3092817},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6408-6419},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A hierarchical superpixel-based approach for DIBR view synthesis},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Homogeneous-to-heterogeneous: Unsupervised learning for
RGB-infrared person re-identification. <em>TIP</em>, <em>30</em>,
6392–6407. (<a href="https://doi.org/10.1109/TIP.2021.3092578">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {RGB-Infrared (RGB-IR) cross-modality person re-identification (re-ID) is attracting more and more attention due to requirements for 24-h scene surveillance. However, the high cost of labeling person identities of an RGB-IR dataset largely limits the scalability of supervised models in real-world scenarios. In this paper, we study the unsupervised RGB-IR person re-ID problem (or briefly uRGB-IR re-ID) in which no identity annotations are available in RGB-IR cross-modality datasets. Considering that intra-modality (i.e., RGB-RGB or IR-IR) re-ID is much easier than cross-modality re-ID and can provide shared knowledge for RGB-IR re-ID, we propose a two-stage method to solve the uRGB-IR re-ID, namely homogeneous-to-heterogeneous learning. In the first stage, the unsupervised self-learning method is conducted to learn the intra-modality feature representation and to generate the pseudo-labeled identities of person images separately for each modality. In the second stage, heterogeneous learning is used to learn a shared discriminative feature representation by distilling the knowledge from intra-modality pseudo-labels, to align two modalities via a modality-based consistent learning module, and finally to target modality-invariant learning via a pseudo-labeled positive instance selection module. With the use of homogeneous-to-heterogeneous learning, the proposed unsupervised framework greatly reduces the modality gap and thus learns a robust feature representation against RGB and infrared modalities, leading to promising accuracy. We also propose a novel cross-modality re-ranking approach that includes a self-modality search and a cycle-modality search to tailor the uRGB-IR re-ID. Unlike conventional re-ranking, the proposed re-ranking method takes a modality-based constraint into re-ranking and thus can select more reliable nearest neighbors, which greatly improves uRGB-IR re-ID. The experimental results demonstrate the superiority of our approach on the SYSU-MM01 and RegDB datasets.},
  archive      = {J_TIP},
  author       = {Wenqi Liang and Guangcong Wang and Jianhuang Lai and Xiaohua Xie},
  doi          = {10.1109/TIP.2021.3092578},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6392-6407},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Homogeneous-to-heterogeneous: Unsupervised learning for RGB-infrared person re-identification},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SMSIR: Spherical measure based spherical image
representation. <em>TIP</em>, <em>30</em>, 6377–6391. (<a
href="https://doi.org/10.1109/TIP.2021.3079797">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a spherical measure based spherical image representation(SMSIR) and sphere-based resampling methods for generating our representation. On this basis, a spherical wavelet transform is also proposed. We first propose a formal recursive definition of the spherical triangle elements of SMSIR and a dyadic index scheme. The index scheme, which supports global random access and needs not to be pre-computed and stored, can efficiently index the elements of SMSIR like planar images. Two resampling methods to generate SMSIR from the most commonly used ERP(Equirectangular Projection) representation are presented. Notably, the spherical measure based resampling, which exploits the mapping between the spherical and the parameter domain, achieves higher computational efficiency than the spherical RBF(Radial Basis Function) based resampling. Finally, we design high-pass and low-pass filters with lifting schemes based on the dyadic index to further verify the efficiency of our index and deal with the spherical isotropy. It provides novel Multi-Resolution Analysis(MRA) for spherical images. Experiments on continuous synthetic spherical images indicate that our representation can recover the original image signals with higher accuracy than the ERP and CMP(Cubemap) representations at the same sampling rate. Besides, the resampling experiments on natural spherical images show that our resampling methods outperform the bilinear and bicubic interpolations concerning the subjective and objective quality. Particularly, as high as 2dB gain in terms of S-PSNR is achieved. Experiments also show that our spherical image transform can capture more geometric features of spherical images than traditional wavelet transform.},
  archive      = {J_TIP},
  author       = {Gang Wu and Yunhui Shi and Xiaoyan Sun and Jin Wang and Baocai Yin},
  doi          = {10.1109/TIP.2021.3079797},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6377-6391},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {SMSIR: Spherical measure based spherical image representation},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Heterogeneous domain adaptation by information capturing and
distribution matching. <em>TIP</em>, <em>30</em>, 6364–6376. (<a
href="https://doi.org/10.1109/TIP.2021.3094137">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heterogeneous domain adaptation (HDA) is a challenging problem because of the different feature representations in the source and target domains. Most HDA methods search for mapping matrices from the source and target domains to discover latent features for learning. However, these methods barely consider the reconstruction error to measure the information loss during the mapping procedure. In this paper, we propose to jointly capture the information and match the source and target domain distributions in the latent feature space. In the learning model, we propose to minimize the reconstruction loss between the original and reconstructed representations to preserve information during transformation and reduce the Maximum Mean Discrepancy between the source and target domains to align their distributions. The resulting minimization problem involves two projection variables with orthogonal constraints that can be solved by the generalized gradient flow method, which can preserve orthogonal constraints in the computational procedure. We conduct extensive experiments on several image classification datasets to demonstrate that the effectiveness and efficiency of the proposed method are better than those of state-of-the-art HDA methods.},
  archive      = {J_TIP},
  author       = {Hanrui Wu and Hong Zhu and Yuguang Yan and Jiaju Wu and Yifan Zhang and Michael K. Ng},
  doi          = {10.1109/TIP.2021.3094137},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6364-6376},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Heterogeneous domain adaptation by information capturing and distribution matching},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MGG: Monocular global geolocation for outdoor long-range
targets. <em>TIP</em>, <em>30</em>, 6349–6363. (<a
href="https://doi.org/10.1109/TIP.2021.3093789">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional monocular vision localization methods are usually suitable for short-range area and indoor relative positioning tasks. This paper presents MGG, a novel monocular global geolocation method for outdoor long-range targets. This method takes a single RGB image combined with necessary navigation parameters as input and outputs targets&#39; GPS information under the Global Navigation Satellite System (GNSS). In MGG, we first design a camera pose correction method via pixel mapping to correct the pose of the camera. Then, we use anchor-based methods to improve the detection ability for long-range targets with small image regions. Next, the local monocular vision model (LMVM) with a local structure coefficient is proposed to establish an accurate 2D-to-3D mapping relationship. Subsequently, a soft correspondence constraint (SCC) is presented to solve the local structure coefficient, which can weaken the coupling degree between detection and localization. Finally, targets can be geolocated through optimization theory-based methods and a series of coordinate transformations. Furthermore, we demonstrate the importance of focal length on solving the error explosion problem in locating long-range targets with monocular vision. Extensive experiments on the challenging KITTI dataset as well as applications in outdoor environments with targets located at a long range of up to 150 meters show the superiority of our method.},
  archive      = {J_TIP},
  author       = {Feng Gao and Fang Deng and Linhan Li and Lele Zhang and Jiaqi Zhu and Chengpu Yu},
  doi          = {10.1109/TIP.2021.3093789},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6349-6363},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {MGG: Monocular global geolocation for outdoor long-range targets},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A non-local superpatch-based algorithm exploiting low rank
prior for restoration of hyperspectral images. <em>TIP</em>,
<em>30</em>, 6335–6348. (<a
href="https://doi.org/10.1109/TIP.2021.3093780">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel algorithm for the restoration of a degraded hyperspectral image. The proposed algorithm exploits the spatial as well as the spectral redundancy of a degraded hyperspectral image in order to restore it without having any prior knowledge about the type of degradation present. Our work uses superpatches to exploit the spatial and spectral redundancies. We formulate a restoration algorithm incorporating structural similarity index measure as the data fidelity term and nuclear norm as the regularization term. The proposed algorithm is able to cope with additive Gaussian noise, signal dependent Poisson noise, mixed Poisson-Gaussian noise and can restore a hyperspectral image corrupted by dead lines and stripes. As we demonstrate with the aid of extensive experiments, our algorithm is capable of recovering the spectra even in the case of severe degradation. A comparison with the state-of-the-art low rank hyperspectral image restoration methods via experiments with real world and simulated data establishes the competitiveness of the proposed algorithm with the existing methods.},
  archive      = {J_TIP},
  author       = {Sourish Sarkar and Rajiv Ranjan Sahay},
  doi          = {10.1109/TIP.2021.3093780},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6335-6348},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A non-local superpatch-based algorithm exploiting low rank prior for restoration of hyperspectral images},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning efficient hash codes for fast graph-based data
similarity retrieval. <em>TIP</em>, <em>30</em>, 6321–6334. (<a
href="https://doi.org/10.1109/TIP.2021.3093387">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional operations, e.g. graph edit distance (GED), are no longer suitable for processing the massive quantities of graph-structured data now available, due to their irregular structures and high computational complexities. With the advent of graph neural networks (GNNs), the problems of graph representation and graph similarity search have drawn particular attention in the field of computer vision. However, GNNs have been less studied for efficient and fast retrieval after graph representation. To represent graph-based data, and maintain fast retrieval while doing so, we introduce an efficient hash model with graph neural networks (HGNN) for a newly designed task (i.e. fast graph-based data retrieval). Due to its flexibility, HGNN can be implemented in both an unsupervised and supervised manner. Specifically, by adopting a graph neural network and hash learning algorithms, HGNN can effectively learn a similarity-preserving graph representation and compute pair-wise similarity or provide classification via low-dimensional compact hash codes. To the best of our knowledge, our model is the first to address graph hashing representation in the Hamming space. Our experimental results reach comparable prediction accuracy to full-precision methods and can even outperform traditional models in some cases. In real-world applications, using hash codes can greatly benefit systems with smaller memory capacities and accelerate the retrieval speed of graph-structured data. Hence, we believe the proposed HGNN has great potential in further research.},
  archive      = {J_TIP},
  author       = {Jinbao Wang and Shuo Xu and Feng Zheng and Ke Lu and Jingkuan Song and Ling Shao},
  doi          = {10.1109/TIP.2021.3093387},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6321-6334},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Learning efficient hash codes for fast graph-based data similarity retrieval},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Task-driven semantic coding via reinforcement learning.
<em>TIP</em>, <em>30</em>, 6307–6320. (<a
href="https://doi.org/10.1109/TIP.2021.3091909">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Task-driven semantic video/image coding has drawn considerable attention with the development of intelligent media applications, such as license plate detection, face detection, and medical diagnosis, which focuses on maintaining the semantic information of videos/images. Deep neural network (DNN)-based codecs have been studied for this purpose due to their inherent end-to-end optimization mechanism. However, the traditional hybrid coding framework cannot be optimized in an end-to-end manner, which makes task-driven semantic fidelity metric unable to be automatically integrated into the rate-distortion optimization process. Therefore, it is still attractive and challenging to implement task-driven semantic coding with the traditional hybrid coding framework, which should still be widely used in practical industry for a long time. To solve this challenge, we design semantic maps for different tasks to extract the pixelwise semantic fidelity for videos/images. Instead of directly integrating the semantic fidelity metric into traditional hybrid coding framework, we implement task-driven semantic coding by implementing semantic bit allocation based on reinforcement learning (RL). We formulate the semantic bit allocation problem as a Markov decision process (MDP) and utilize one RL agent to automatically determine the quantization parameters (QPs) for different coding units (CUs) according to the task-driven semantic fidelity metric. Extensive experiments on different tasks, such as classification, detection and segmentation, have demonstrated the superior performance of our approach by achieving an average bitrate saving of 34.39\% to 52.62\% over the High Efficiency Video Coding (H.265/HEVC) anchor under equivalent task-related semantic fidelity.},
  archive      = {J_TIP},
  author       = {Xin Li and Jun Shi and Zhibo Chen},
  doi          = {10.1109/TIP.2021.3091909},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6307-6320},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Task-driven semantic coding via reinforcement learning},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Optimal pre-filtering for improving facebook shared images.
<em>TIP</em>, <em>30</em>, 6292–6306. (<a
href="https://doi.org/10.1109/TIP.2021.3093794">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online Social Networks (OSNs) have attracted a huge number of users, who store and share various images on a daily basis. As a well-known fact, most OSN platforms apply a series of lossy operations on the uploaded images, which could severely degrade the quality of the shared images, negatively affecting the user experiences. In this work, we consider the problem of significantly improving OSN-shared images through applying an optimal pre-filtering prior to image sharing, without any cooperation from the OSN platform itself. Facebook, as one of the most popular and representative OSNs, is chosen as the platform to present our designed pre-filtering strategy. We first treat Facebook as a black box, and thoroughly recover its mechanism of processing color images. Based on the precise knowledge on the image processing pipeline on Facebook, we design the pre-filter under an optimization framework, minimizing the end-to-end distortion between the shared image and the original one. Compared with the directly shared images, our proposed pre-filtering-then-sharing strategy brings significant improvements in terms of both quantitative and qualitative metrics. Extensive experimental results are provided to show the superiority of our proposed method. Finally, we discuss the strategy on how to extend our proposed technique to other OSN platforms.},
  archive      = {J_TIP},
  author       = {Weiwei Sun and Jiantao Zhou and Li Dong and Jinyu Tian and Jun Liu},
  doi          = {10.1109/TIP.2021.3093794},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6292-6306},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Optimal pre-filtering for improving facebook shared images},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Iterative robust graph for unsupervised change detection of
heterogeneous remote sensing images. <em>TIP</em>, <em>30</em>,
6277–6291. (<a href="https://doi.org/10.1109/TIP.2021.3093766">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work presents a robust graph mapping approach for the unsupervised heterogeneous change detection problem in remote sensing imagery. To address the challenge that heterogeneous images cannot be directly compared due to different imaging mechanisms, we take advantage of the fact that the heterogeneous images share the same structure information for the same ground object, which is imaging modality-invariant. The proposed method first constructs a robust K-nearest neighbor graph to represent the structure of each image, and then compares the graphs within the same image domain by means of graph mapping to calculate the forward and backward difference images, which can avoid the confusion of heterogeneous data. Finally, it detects the changes through a Markovian co-segmentation model that can fuse the forward and backward difference images in the segmentation process, which can be solved by the co-graph cut. Once the changed areas are detected by the Markovian co-segmentation, they will be propagated back into the graph construction process to reduce the influence of changed neighbors. This iterative framework makes the graph more robust and thus improves the final detection performance. Experimental results on different data sets confirm the effectiveness of the proposed method. Source code of the proposed method is made available at https://github.com/yulisun/IRG-McS.},
  archive      = {J_TIP},
  author       = {Yuli Sun and Lin Lei and Dongdong Guan and Gangyao Kuang},
  doi          = {10.1109/TIP.2021.3093766},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6277-6291},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Iterative robust graph for unsupervised change detection of heterogeneous remote sensing images},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A two-stream dynamic pyramid representation model for
video-based person re-identification. <em>TIP</em>, <em>30</em>,
6266–6276. (<a href="https://doi.org/10.1109/TIP.2021.3093759">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video-based person re-identification (Re-ID) leverages rich spatio-temporal information embedded in sequence data to further improve the retrieval accuracy comparing with single image Re-ID. However, it also brings new difficulties. 1) Both spatial and temporal information should be considered simultaneously. 2) Pedestrian video data often contains redundant information and 3) suffers from data quality problems such as occlusion, background clutter. To solve the above problems, we propose a novel two-stream Dynamic Pyramid Representation Model (DPRM) . DPRM mainly consists of three sub-models, i.e. , Pyramidal Distribution Sampling Method (PDSM), Dynamic Pyramid Dilated Convolution (DPDC) and Pyramid Attention Pooling (PAP). PDSM is applied for more effective data pre-processing according to sequence semantic distribution. DPDC and PAP can be considered as two streams to describe the motion context and static appearance of a video sequence, respectively. By fusing the two-stream features together, we finally achieve comprehensive spatio-temporal representation. Notably, dynamic pyramid strategy is applied throughout the whole model. This strategy exploits multi-scale features under attention mechanism to maximally capture the most discriminative features and mitigate the impact of video data quality problems such as partial occlusion. Extensive experiments demonstrate the outperformance of DPRM. For instance, it achieves 83.0\% mAP and 89.0\% Rank-1 accuracy on MARS dataset and reaches state of the art.},
  archive      = {J_TIP},
  author       = {Xi Yang and Liangchen Liu and Nannan Wang and Xinbo Gao},
  doi          = {10.1109/TIP.2021.3093759},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6266-6276},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A two-stream dynamic pyramid representation model for video-based person re-identification},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Accurate and fast image denoising via attention guided
scaling. <em>TIP</em>, <em>30</em>, 6255–6265. (<a
href="https://doi.org/10.1109/TIP.2021.3093396">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image denoising is a classical topic yet still a challenging problem, especially for reducing noise from the texture information. Feature scaling (e.g., downscale and upscale) is a widely practice in image denoising to enlarge receptive field size and save resources. However, such a common operation would lose some visual informative details. To address those problems, we propose fast and accurate image denoising via attention guided scaling (AGS). We find that the main informative feature channel and visual primitives during the scaling should keep similar. We then propose to extract the global channel-wise attention to maintain main channel information. Moreover, we propose to collect global descriptors by considering the entire spatial feature. And we then distribute the global descriptors to local positions of the scaled feature, based on their specific needs. We further introduce AGS for adversarial training, resulting in a more powerful discriminator. Extensive experiments show the effectiveness of our proposed method, where we clearly surpass all the state-of-the-art methods on most popular synthetic and real-world denoising benchmarks quantitatively and visually. We further show that our network contributes to other high-level vision applications and improves their performances significantly.},
  archive      = {J_TIP},
  author       = {Yulun Zhang and Kunpeng Li and Kai Li and Gan Sun and Yu Kong and Yun Fu},
  doi          = {10.1109/TIP.2021.3093396},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6255-6265},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Accurate and fast image denoising via attention guided scaling},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Human interaction understanding with joint graph
decomposition and node labeling. <em>TIP</em>, <em>30</em>, 6240–6254.
(<a href="https://doi.org/10.1109/TIP.2021.3093383">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The task of human interaction understanding involves both recognizing the action of each individual in the scene and decoding the interaction relationship among people, which is useful to a series of vision applications such as camera surveillance, video-based sports analysis and event retrieval. This paper divides the task into two problems including grouping people into clusters and assigning labels to each of them, and presents an approach to solving these problems in a joint manner. Our method does not assume the number of groups is known beforehand as this will substantially restrict its application. With the observation that the two challenges are highly correlated, the key idea is to model the pairwise interacting relations among people via a complete graph and its associated energy function such that the labeling and grouping problems are translated into the minimization of the energy function. We implement this joint framework by fusing both deep features and rich contextual cues, and learn the fusion parameters from data. An alternating search algorithm is developed in order to efficiently solve the associated inference problem. By combining the grouping and labeling results obtained with our method, we are able to achieve the semantic-level understanding of human interactions. Extensive experiments are performed to qualitatively and quantitatively evaluate the effectiveness of our approach, which outperforms state-of-the-art methods on several important benchmarks. An ablation study is also performed to verify the effectiveness of different modules within our approach.},
  archive      = {J_TIP},
  author       = {Zhenhua Wang and Jinchao Ge and Dongyan Guo and Jianhua Zhang and Yanjing Lei and Shengyong Chen},
  doi          = {10.1109/TIP.2021.3093383},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6240-6254},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Human interaction understanding with joint graph decomposition and node labeling},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Decomposition and completion network for salient object
detection. <em>TIP</em>, <em>30</em>, 6226–6239. (<a
href="https://doi.org/10.1109/TIP.2021.3093380">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, fully convolutional networks (FCNs) have made great progress in the task of salient object detection and existing state-of-the-arts methods mainly focus on how to integrate edge information in deep aggregation models. In this paper, we propose a novel Decomposition and Completion Network (DCN), which integrates edge and skeleton as complementary information and models the integrity of salient objects in two stages. In the decomposition network, we propose a cross multi-branch decoder, which iteratively takes advantage of cross-task aggregation and cross-layer aggregation to integrate multi-level multi-task features and predict saliency, edge, and skeleton maps simultaneously. In the completion network, edge and skeleton maps are further utilized to fill flaws and suppress noises in saliency maps via hierarchical structure-aware feature learning and multi-scale feature completion. Through jointly learning with edge and skeleton information for localizing boundaries and interiors of salient objects respectively, the proposed network generates precise saliency maps with uniformly and completely segmented salient objects. Experiments conducted on five benchmark datasets demonstrate that the proposed model outperforms existing networks. Furthermore, we extend the proposed model to the task of RGB-D salient object detection, and it also achieves state-of-the-art performance. The code is available at https://github.com/wuzhe71/DCN.},
  archive      = {J_TIP},
  author       = {Zhe Wu and Li Su and Qingming Huang},
  doi          = {10.1109/TIP.2021.3093380},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6226-6239},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Decomposition and completion network for salient object detection},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). I understand you: Blind 3D human attention inference from
the perspective of third-person. <em>TIP</em>, <em>30</em>, 6212–6225.
(<a href="https://doi.org/10.1109/TIP.2021.3092842">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inferring object-wise human attention in 3D space from the third-person perspective (e.g., a camera) is crucial to many visual tasks and applications, including human-robot collaboration, unmanned vehicle driving, etc. Challenges arise from classical human attention when human eyes are not visible to cameras, gaze point is outside the field of vision, or the gazed object is occluded by others in the 3D space. In this case, blind 3D human attention inference brings a new paradigm to the community. In this paper, we address these challenges by proposing a scene-behavior associated mechanism, in which both 3D scene and temporal behavior of human are adopted to infer object-wise human attention and its transition. Specifically, point cloud is reconstructed and used for the spatial representation of 3D scene, which is beneficial to handle the blind problem from the perspective of a camera. Based on this, in order to address the blind human attention inference without eye information, we propose a Sequential Skeleton Based Attention Network (S 2 BAN) for behavior-based attention modeling. As is embedded in the scene-behavior associated mechanism, the proposed S 2 BAN is built under the temporal architecture of Long-Short-Term-Memory (LSTM). Our network employs human skeleton as behavior representation, and maps it to the attention direction frame by frame, which makes attention inference a temporal-correlated issue. With the help of S 2 BAN, 3D gaze spot and further the attended objects can be obtained frame by frame via intersection and segmentation on the previously reconstructed point cloud. Finally, we conduct experiments from various aspects to verify the object-wise attention localization accuracy, the angular error of attention direction calculation, as well as the subjective results. The experimental results show that the proposed outperforms other competitors.},
  archive      = {J_TIP},
  author       = {Xiang Shi and You Yang and Qiong Liu},
  doi          = {10.1109/TIP.2021.3092842},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6212-6225},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {I understand you: Blind 3D human attention inference from the perspective of third-person},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A domain-guided noise-optimization-based inversion method
for facial image manipulation. <em>TIP</em>, <em>30</em>, 6198–6211. (<a
href="https://doi.org/10.1109/TIP.2021.3089905">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A style-based architecture (StyleGAN2) yields outstanding results in data-driven unconditional generative image modeling. This work proposes a Domain-guided Noise-optimization-based Inversion (DNI) method to perform facial image manipulation. It works based on an inverse code that includes: 1) a novel domain-guided encoder called Image2latent to project the image to StyleGAN2 latent space, which can reconstruct an input image with high-quality and maintain its semantic meaning well; 2) a noise optimization mechanism in which a set of noise vectors are used to capture the high-frequency details such as image edges, further improving image reconstruction quality; and 3) a mask for seamless image fusion and local style migration. We further propose a novel semantic alignment evaluation pipeline. It evaluates the semantic alignment with an inverse code by using different attribute boundaries. Extensive qualitative and quantitative comparisons show that DNI can capture rich semantic information and achieve a satisfactory image reconstruction. It can realize a variety of facial image manipulation tasks and outperform state of the art.},
  archive      = {J_TIP},
  author       = {Nan Yang and Zeyu Zheng and MengChu Zhou and Xiwang Guo and Liang Qi and Tianran Wang},
  doi          = {10.1109/TIP.2021.3089905},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6198-6211},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A domain-guided noise-optimization-based inversion method for facial image manipulation},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Unsupervised deep image stitching: Reconstructing stitched
features to images. <em>TIP</em>, <em>30</em>, 6184–6197. (<a
href="https://doi.org/10.1109/TIP.2021.3092828">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional feature-based image stitching technologies rely heavily on feature detection quality, often failing to stitch images with few features or low resolution. The learning-based image stitching solutions are rarely studied due to the lack of labeled data, making the supervised methods unreliable. To address the above limitations, we propose an unsupervised deep image stitching framework consisting of two stages: unsupervised coarse image alignment and unsupervised image reconstruction. In the first stage, we design an ablation-based loss to constrain an unsupervised homography network, which is more suitable for large-baseline scenes. Moreover, a transformer layer is introduced to warp the input images in the stitching-domain space. In the second stage, motivated by the insight that the misalignments in pixel-level can be eliminated to a certain extent in feature-level, we design an unsupervised image reconstruction network to eliminate the artifacts from features to pixels. Specifically, the reconstruction network can be implemented by a low-resolution deformation branch and a high-resolution refined branch, learning the deformation rules of image stitching and enhancing the resolution simultaneously. To establish an evaluation benchmark and train the learning framework, a comprehensive real-world image dataset for unsupervised deep image stitching is presented and released. Extensive experiments well demonstrate the superiority of our method over other state-of-the-art solutions. Even compared with the supervised solutions, our image stitching quality is still preferred by users.},
  archive      = {J_TIP},
  author       = {Lang Nie and Chunyu Lin and Kang Liao and Shuaicheng Liu and Yao Zhao},
  doi          = {10.1109/TIP.2021.3092828},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6184-6197},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Unsupervised deep image stitching: Reconstructing stitched features to images},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dynamic point cloud denoising via manifold-to-manifold
distance. <em>TIP</em>, <em>30</em>, 6168–6183. (<a
href="https://doi.org/10.1109/TIP.2021.3092826">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D dynamic point clouds provide a natural discrete representation of real-world objects or scenes in motion, with a wide range of applications in immersive telepresence, autonomous driving, surveillance, etc. Nevertheless, dynamic point clouds are often perturbed by noise due to hardware, software or other causes. While a plethora of methods have been proposed for static point cloud denoising, few efforts are made for the denoising of dynamic point clouds, which is quite challenging due to the irregular sampling patterns both spatially and temporally. In this paper, we represent dynamic point clouds naturally on spatial-temporal graphs, and exploit the temporal consistency with respect to the underlying surface (manifold). In particular, we define a manifold-to-manifold distance and its discrete counterpart on graphs to measure the variation-based intrinsic distance between surface patches in the temporal domain, provided that graph operators are discrete counterparts of functionals on Riemannian manifolds. Then, we construct the spatial-temporal graph connectivity between corresponding surface patches based on the temporal distance and between points in adjacent patches in the spatial domain. Leveraging the initial graph representation, we formulate dynamic point cloud denoising as the joint optimization of the desired point cloud and underlying graph representation, regularized by both spatial smoothness and temporal consistency. We reformulate the optimization and present an efficient algorithm. Experimental results show that the proposed method significantly outperforms independent denoising of each frame from state-of-the-art static point cloud denoising approaches, on both Gaussian noise and simulated LiDAR noise.},
  archive      = {J_TIP},
  author       = {Wei Hu and Qianjiang Hu and Zehua Wang and Xiang Gao},
  doi          = {10.1109/TIP.2021.3092826},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6168-6183},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Dynamic point cloud denoising via manifold-to-manifold distance},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Toward visual distortion in black-box attacks. <em>TIP</em>,
<em>30</em>, 6156–6167. (<a
href="https://doi.org/10.1109/TIP.2021.3092822">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Constructing adversarial examples in a black-box threat model injures the original images by introducing visual distortion. In this paper, we propose a novel black-box attack approach that can directly minimize the induced distortion by learning the noise distribution of the adversarial example, assuming only loss-oracle access to the black-box network. To quantify visual distortion, the perceptual distance between the adversarial example and the original image, is introduced in our loss. We first approximate the gradient of the corresponding non-differentiable loss function by sampling noise from the learned noise distribution. Then the distribution is updated using the estimated gradient to reduce visual distortion. The learning continues until an adversarial example is found. We validate the effectiveness of our attack on ImageNet. Our attack results in much lower distortion when compared to the state-of-the-art black-box attacks and achieves 100\% success rate on InceptionV3, ResNet50 and VGG16bn. Furthermore, we theoretically prove the convergence of our model. The code is publicly available at https://github.com/Alina-1997/visual-distortion-in-attack .},
  archive      = {J_TIP},
  author       = {Nannan Li and Zhenzhong Chen},
  doi          = {10.1109/TIP.2021.3092822},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6156-6167},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Toward visual distortion in black-box attacks},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Structure-aware motion deblurring using multi-adversarial
optimized CycleGAN. <em>TIP</em>, <em>30</em>, 6142–6155. (<a
href="https://doi.org/10.1109/TIP.2021.3092814">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, Convolutional Neural Networks (CNNs) have achieved great improvements in blind image motion deblurring. However, most existing image deblurring methods require a large amount of paired training data and fail to maintain satisfactory structural information, which greatly limits their application scope. In this paper, we present an unsupervised image deblurring method based on a multi-adversarial optimized cycle-consistent generative adversarial network (CycleGAN). Although original CycleGAN can handle unpaired training data well, the generated high-resolution images are probable to lose content and structure information. To solve this problem, we utilize a multi-adversarial mechanism based on CycleGAN for blind motion deblurring to generate high-resolution images iteratively. In this multi-adversarial manner, the hidden layers of the generator are gradually supervised, and the implicit refinement is carried out to generate high-resolution images continuously. Meanwhile, we also introduce the structure-aware mechanism to enhance the structure and detail retention ability of the multi-adversarial network for deblurring by taking the edge map as guidance information and adding multi-scale edge constraint functions. Our approach not only avoids the strict need for paired training data and the errors caused by blur kernel estimation, but also maintains the structural information better with multi-adversarial learning and structure-aware mechanism. Comprehensive experiments on several benchmarks have shown that our approach prevails the state-of-the-art methods for blind image motion deblurring.},
  archive      = {J_TIP},
  author       = {Yang Wen and Jie Chen and Bin Sheng and Zhihua Chen and Ping Li and Ping Tan and Tong-Yee Lee},
  doi          = {10.1109/TIP.2021.3092814},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6142-6155},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Structure-aware motion deblurring using multi-adversarial optimized CycleGAN},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Unsupervised discrete hashing with affinity similarity.
<em>TIP</em>, <em>30</em>, 6130–6141. (<a
href="https://doi.org/10.1109/TIP.2021.3091895">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, supervised hashing has been validated to greatly boost the performance of image retrieval. However, the label-hungry property requires massive label collection, making it intractable in practical scenarios. To liberate the model training procedure from laborious manual annotations, some unsupervised methods are proposed. However, the following two factors make unsupervised algorithms inferior to their supervised counterparts: (1) Without manually-defined labels, it is difficult to capture the semantic information across data, which is of crucial importance to guide robust binary code learning. (2) The widely adopted relaxation on binary constraints results in quantization error accumulation in the optimization procedure. To address the above-mentioned problems, in this paper, we propose a novel Unsupervised Discrete Hashing method (UDH). Specifically, to capture the semantic information, we propose a balanced graph-based semantic loss which explores the affinity priors in the original feature space. Then, we propose a novel self-supervised loss, termed orthogonal consistent loss, which can leverage semantic loss of instance and impose independence of codes. Moreover, by integrating the discrete optimization into the proposed unsupervised framework, the binary constraints are consistently preserved, alleviating the influence of quantization errors. Extensive experiments demonstrate that UDH outperforms state-of-the-art unsupervised methods for image retrieval.},
  archive      = {J_TIP},
  author       = {Sheng Jin and Hongxun Yao and Qin Zhou and Yao Liu and Jianqiang Huang and Xiansheng Hua},
  doi          = {10.1109/TIP.2021.3091895},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6130-6141},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Unsupervised discrete hashing with affinity similarity},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Defense against adversarial attacks by reconstructing
images. <em>TIP</em>, <em>30</em>, 6117–6129. (<a
href="https://doi.org/10.1109/TIP.2021.3092582">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks (CNNs) are vulnerable to being deceived by adversarial examples generated by adding small, human-imperceptible perturbations to a clean image. In this paper, we propose an image reconstruction network that reconstructs an input adversarial example into a clean output image to defend against such adversarial attacks. Due to the powerful learning capabilities of the residual block structure, our model can learn a precise mapping from adversarial examples to reconstructed examples. The use of a perceptual loss greatly suppresses the error amplification effect and improves the performance of our reconstruction network. In addition, by adding randomization layers to the end of the network, the effects of additional noise are further suppressed, especially for iterative attacks. Our model has the following four advantages. 1) It greatly reduces the impact of adversarial perturbations while having little influence on the prediction performance of clean images. 2) During inference phase, it performs better than most existing model-agnostic defense methods. 3) It has better generalization capability. 4) It can be flexibly combined with other methods, such as adversarially trained models.},
  archive      = {J_TIP},
  author       = {Shudong Zhang and Haichang Gao and Qingxun Rao},
  doi          = {10.1109/TIP.2021.3092582},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6117-6129},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Defense against adversarial attacks by reconstructing images},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). UniFaceGAN: A unified framework for temporally consistent
facial video editing. <em>TIP</em>, <em>30</em>, 6107–6116. (<a
href="https://doi.org/10.1109/TIP.2021.3089909">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent research has witnessed advances in facial image editing tasks including face swapping and face reenactment. However, these methods are confined to dealing with one specific task at a time. In addition, for video facial editing, previous methods either simply apply transformations frame by frame or utilize multiple frames in a concatenated or iterative fashion, which leads to noticeable visual flickers. In this paper, we propose a unified temporally consistent facial video editing framework termed UniFaceGAN. Based on a 3D reconstruction model and a simple yet efficient dynamic training sample selection mechanism, our framework is designed to handle face swapping and face reenactment simultaneously. To enforce the temporal consistency, a novel 3D temporal loss constraint is introduced based on the barycentric coordinate interpolation. Besides, we propose a region-aware conditional normalization layer to replace the traditional AdaIN or SPADE to synthesize more context-harmonious results. Compared with the state-of-the-art facial image editing methods, our framework generates video portraits that are more photo-realistic and temporally smooth.},
  archive      = {J_TIP},
  author       = {Meng Cao and Haozhi Huang and Hao Wang and Xuan Wang and Li Shen and Sheng Wang and Linchao Bao and Zhifeng Li and Jiebo Luo},
  doi          = {10.1109/TIP.2021.3089909},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6107-6116},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {UniFaceGAN: A unified framework for temporally consistent facial video editing},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). PVRED: A position-velocity recurrent encoder-decoder for
human motion prediction. <em>TIP</em>, <em>30</em>, 6096–6106. (<a
href="https://doi.org/10.1109/TIP.2021.3089380">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human motion prediction, which aims to predict future human poses given past poses, has recently seen increased interest. Many recent approaches are based on Recurrent Neural Networks (RNN) which model human poses with exponential maps. These approaches neglect the pose velocity as well as temporal relation of different poses, and tend to converge to the mean pose or fail to generate natural-looking poses. We therefore propose a novel Position-Velocity Recurrent Encoder-Decoder (PVRED) for human motion prediction, which makes full use of pose velocities and temporal positional information. A temporal position embedding method is presented and a Position-Velocity RNN (PVRNN) is proposed. We also emphasize the benefits of quaternion parameterization of poses and design a novel trainable Quaternion Transformation (QT) layer, which is combined with a robust loss function during training. We provide quantitative results for both short-term prediction in the future 0.5 seconds and long-term prediction in the future 0.5 to 1 seconds. Experiments on several benchmarks show that our approach considerably outperforms the state-of-the-art methods. In addition, qualitative visualizations in the future 4 seconds show that our approach could predict future human-like and meaningful poses in very long time horizons. Code is publicly available on GitHub: https://github.com/hongsong-wang/PVRNN.},
  archive      = {J_TIP},
  author       = {Hongsong Wang and Jian Dong and Bin Cheng and Jiashi Feng},
  doi          = {10.1109/TIP.2021.3089380},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6096-6106},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {PVRED: A position-velocity recurrent encoder-decoder for human motion prediction},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Invertible image decolorization. <em>TIP</em>, <em>30</em>,
6081–6095. (<a href="https://doi.org/10.1109/TIP.2021.3091902">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Invertible image decolorization is a useful color compression technique to reduce the cost in multimedia systems. Invertible decolorization aims to synthesize faithful grayscales from color images, which can be fully restored to the original color version. In this paper, we propose a novel color compression method to produce invertible grayscale images using invertible neural networks (INNs). Our key idea is to separate the color information from color images, and encode the color information into a set of Gaussian distributed latent variables via INNs. By this means, we force the color information lost in grayscale generation to be independent of the input color image. Therefore, the original color version can be efficiently recovered by randomly re-sampling a new set of Gaussian distributed variables, together with the synthetic grayscale, through the reverse mapping of INNs. To effectively learn the invertible grayscale, we introduce the wavelet transformation into a UNet-like INN architecture, and further present a quantization embedding to prevent the information omission in format conversion, which improves the generalizability of the framework in real-world scenarios. Extensive experiments on three widely used benchmarks demonstrate that the proposed method achieves a state-of-the-art performance in terms of both qualitative and quantitative results, which shows its superiority in multimedia communication and storage systems.},
  archive      = {J_TIP},
  author       = {Rui Zhao and Tianshan Liu and Jun Xiao and Daniel P. K. Lun and Kin-Man Lam},
  doi          = {10.1109/TIP.2021.3091902},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6081-6095},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Invertible image decolorization},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). COAST: COntrollable arbitrary-sampling NeTwork for
compressive sensing. <em>TIP</em>, <em>30</em>, 6066–6080. (<a
href="https://doi.org/10.1109/TIP.2021.3091834">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent deep network-based compressive sensing (CS) methods have achieved great success. However, most of them regard different sampling matrices as different independent tasks and need to train a specific model for each target sampling matrix. Such practices give rise to inefficiency in computing and suffer from poor generalization ability. In this paper, we propose a novel COntrollable Arbitrary-Sampling neTwork, dubbed COAST, to solve CS problems of arbitrary-sampling matrices (including unseen sampling matrices) with one single model. Under the optimization-inspired deep unfolding framework, our COAST exhibits good interpretability. In COAST, a random projection augmentation (RPA) strategy is proposed to promote the training diversity in the sampling space to enable arbitrary sampling, and a controllable proximal mapping module (CPMM) and a plug-and-play deblocking (PnP-D) strategy are further developed to dynamically modulate the network features and effectively eliminate the blocking artifacts, respectively. Extensive experiments on widely used benchmark datasets demonstrate that our proposed COAST is not only able to handle arbitrary sampling matrices with one single model but also to achieve state-of-the-art performance with fast speed.},
  archive      = {J_TIP},
  author       = {Di You and Jian Zhang and Jingfen Xie and Bin Chen and Siwei Ma},
  doi          = {10.1109/TIP.2021.3091834},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6066-6080},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {COAST: COntrollable arbitrary-sampling NeTwork for compressive sensing},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-scale low-discriminative feature reactivation for
weakly supervised object localization. <em>TIP</em>, <em>30</em>,
6050–6065. (<a href="https://doi.org/10.1109/TIP.2021.3091833">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For weakly supervised object localization (WSOL), how to avoid the network focusing only on some small discriminative parts is a main challenge needed to solve. The widely-used Class Activation Mapping (CAM) based paradigm usually employs Adversarial Learning (AL) strategy to search more object parts by constantly hiding discovered object features, but the adversarial process is difficult to control. In this paper, we propose a novel CAM-based framework with Multi-scale Low-Discriminative Feature Reactivation (mLDFR) for WSOL. The mLDFR framework reactivates the low-discriminative object parts via bottom-up continuous feature maps recalibration and multi-scale object category mapping. Compared with the AL-based methods, our method fully improves the localization power of the network without damaging the classification power and can perform multi-instance localization, which are hard to achieve under the AL-based framework. Moreover, the mLDFR framework is flexible, and can be built on the top of various classical CNN backbones. Experimental results demonstrate the superiority of our method. With VGG16 as backbone, we achieve 46.96\% Cls-Loc top1 err and 66.12\% CorLoc on ILSVRC2014, 38.07\% Cls-Loc top1 err and 75.04\% CorLoc on CUB200-2011, surpassing the state-of-the-arts by a large margin.},
  archive      = {J_TIP},
  author       = {Bo Wang and Chunfeng Yuan and Bing Li and Xinmiao Ding and Zeya Li and Ying Wu and Weiming Hu},
  doi          = {10.1109/TIP.2021.3091833},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6050-6065},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Multi-scale low-discriminative feature reactivation for weakly supervised object localization},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SDOF-GAN: Symmetric dense optical flow estimation with
generative adversarial networks. <em>TIP</em>, <em>30</em>, 6036–6049.
(<a href="https://doi.org/10.1109/TIP.2021.3084073">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is a growing consensus in computer vision that symmetric optical flow estimation constitutes a better model than a generic asymmetric one for its independence of the selection of source/target image. Yet, convolutional neural networks (CNNs), that are considered the de facto standard vision model, deal with the asymmetric case only in most cutting-edge CNNs-based optical flow techniques. We bridge this gap by introducing a novel model named SDOF-GAN: symmetric dense optical flow with generative adversarial networks (GANs). SDOF-GAN realizes a consistency between the forward mapping (source-to-target) and the backward one (target-to-source) by ensuring that they are inverse of each other with an inverse network. In addition, SDOF-GAN leverages a GAN model for which the generator estimates symmetric optical flow fields while the discriminator differentiates the “real” ground-truth flow field from a “fake” estimation by assessing the flow warping error. Finally, SDOF-GAN is trained in a semi-supervised fashion to enable both the precious labeled data and large amounts of unlabeled data to be fully-exploited. We demonstrate significant performance benefits of SDOF-GAN on five publicly-available datasets in contrast to several representative state-of-the-art models for optical flow estimation.},
  archive      = {J_TIP},
  author       = {Tongtong Che and Yuanjie Zheng and Yunshuai Yang and Sujuan Hou and Weikuan Jia and Jie Yang and Chen Gong},
  doi          = {10.1109/TIP.2021.3084073},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6036-6049},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {SDOF-GAN: Symmetric dense optical flow estimation with generative adversarial networks},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021c). Multi-view face synthesis via progressive face flow.
<em>TIP</em>, <em>30</em>, 6024–6035. (<a
href="https://doi.org/10.1109/TIP.2021.3090658">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing GAN-based multi-view face synthesis methods rely heavily on “creating” faces, and thus they struggle in reproducing the faithful facial texture and fail to preserve identity when undergoing a large angle rotation. In this paper, we combat this problem by dividing the challenging large-angle face synthesis into a series of easy small-angle rotations, and each of them is guided by a face flow to maintain faithful facial details. In particular, we propose a Face Flow-guided Generative Adversarial Network (FFlowGAN) that is specifically trained for small-angle synthesis. The proposed network consists of two modules, a face flow module that aims to compute a dense correspondence between the input and target faces. It provides strong guidance to the second module, face synthesis module, for emphasizing salient facial texture. We apply FFlowGAN multiple times to progressively synthesize different views, and therefore facial features can be propagated to the target view from the very beginning. All these multiple executions are cascaded and trained end-to-end with a unified back-propagation, and thus we ensure each intermediate step contributes to the final result. Extensive experiments demonstrate the proposed divide-and-conquer strategy is effective, and our method outperforms the state-of-the-art on four benchmark datasets qualitatively and quantitatively.},
  archive      = {J_TIP},
  author       = {Yangyang Xu and Xuemiao Xu and Jianbo Jiao and Keke Li and Cheng Xu and Shengfeng He},
  doi          = {10.1109/TIP.2021.3090658},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6024-6035},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Multi-view face synthesis via progressive face flow},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning category- and instance-aware pixel embedding for
fast panoptic segmentation. <em>TIP</em>, <em>30</em>, 6013–6023. (<a
href="https://doi.org/10.1109/TIP.2021.3090522">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Panoptic segmentation (PS) is a complex scene understanding task that requires providing high-quality segmentation for both thing objects and stuff regions. Previous methods handle these two classes with semantic and instance segmentation modules separately, following with heuristic fusion or additional modules to resolve the conflicts between the two outputs. This work simplifies this pipeline of PS by consistently modeling the two classes with a novel PS framework, which extends a detection model with an extra module to predict category- and instance-aware pixel embedding (CIAE). CIAE is a novel pixel-wise embedding feature that encodes both semantic-classification and instance-distinction information. At the inference process, PS results are simply derived by assigning each pixel to a detected instance or a stuff class according to the learned embedding. Our method not only demonstrates fast inference speed but also the first one-stage method to achieve comparable performance to two-stage methods on the challenging COCO benchmark.},
  archive      = {J_TIP},
  author       = {Naiyu Gao and Yanhu Shan and Xin Zhao and Kaiqi Huang},
  doi          = {10.1109/TIP.2021.3090522},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6013-6023},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Learning category- and instance-aware pixel embedding for fast panoptic segmentation},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Blind decomposition of multispectral document images using
orthogonal nonnegative matrix factorization. <em>TIP</em>, <em>30</em>,
5997–6012. (<a href="https://doi.org/10.1109/TIP.2021.3088266">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the challenge of Multispectral (MS) document image segmentation, which is an essential step for subsequent document image analysis. Most previous studies have focused only on binary (text/non-text) separation. They also rely on handcrafted features and techniques dedicated to conventional images that do not take advantage of MS images&#39; spectral richness. In this work, we reformulate this task as a source separation problem, whereby we target the blind decomposition of entire MS document images via a new orthogonal nonnegative matrix factorization (ONMF). On the one hand, we incorporate orthogonality constraint as a Riemannian optimization on the Stiefel manifold. On the other hand, based on which factor we impose the orthogonality constraint, i.e., either on the endmember matrix, abundance matrix, or both, we propose three ONMF models to investigate this issue and determine which model is more suitable for this study. Minimizing the three models subject to nonnegativity and orthogonality constraints simultaneously is very challenging. Therefore, we extend the alternating direction method of multipliers scheme to solve them. We evaluated our models on synthetic Hyperspectral (HS) images and real-world MS document images. The experimental results confirm the effectiveness of the proposed models and demonstrate their generalization power compared to state-of-the-art techniques.},
  archive      = {J_TIP},
  author       = {Abderrahmane Rahiche and Mohamed Cheriet},
  doi          = {10.1109/TIP.2021.3088266},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5997-6012},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Blind decomposition of multispectral document images using orthogonal nonnegative matrix factorization},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Delving deep into label smoothing. <em>TIP</em>,
<em>30</em>, 5984–5996. (<a
href="https://doi.org/10.1109/TIP.2021.3089942">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Label smoothing is an effective regularization tool for deep neural networks (DNNs), which generates soft labels by applying a weighted average between the uniform distribution and the hard label. It is often used to reduce the overfitting problem of training DNNs and further improve classification performance. In this paper, we aim to investigate how to generate more reliable soft labels. We present an Online Label Smoothing (OLS) strategy, which generates soft labels based on the statistics of the model prediction for the target category. The proposed OLS constructs a more reasonable probability distribution between the target categories and non-target categories to supervise DNNs. Experiments demonstrate that based on the same classification models, the proposed approach can effectively improve the classification performance on CIFAR-100, ImageNet, and fine-grained datasets. Additionally, the proposed method can significantly improve the robustness of DNN models to noisy labels compared to current label smoothing approaches. The source code is available at our project page: https://mmcheng.net/ols/},
  archive      = {J_TIP},
  author       = {Chang-Bin Zhang and Peng-Tao Jiang and Qibin Hou and Yunchao Wei and Qi Han and Zhen Li and Ming-Ming Cheng},
  doi          = {10.1109/TIP.2021.3089942},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5984-5996},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Delving deep into label smoothing},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Speckle-free SAR image ship detection. <em>TIP</em>,
<em>30</em>, 5969–5983. (<a
href="https://doi.org/10.1109/TIP.2021.3089936">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ship detection is one of important applications for synthetic aperture radar (SAR). Speckle effects usually make SAR image understanding difficult and speckle reduction becomes a necessary pre-processing step for majority SAR applications. This work examines different speckle reduction methods on SAR ship detection performances. It is found out that the influences of different speckle filters are significant which can be positive or negative. However, how to select a suitable combination of speckle filters and ship detectors is lack of theoretical basis and is also data-orientated. To overcome this limitation, a speckle-free SAR ship detection approach is proposed. A similar pixel number (SPN) indicator which can effectively identify salient target is derived, during the similar pixel selection procedure with the context covariance matrix (CCM) similarity test. The underlying principle lies in that ship and sea clutter candidates show different properties of homogeneity within a moving window and the SPN indicator can clearly reflect their differences. The sensitivity and efficiency of the SPN indicator is examined and demonstrated. Then, a speckle-free SAR ship detection approach is established based on the SPN indicator. The detection flowchart is also given. Experimental and comparison studies are carried out with three kinds of spaceborne SAR datasets in terms of different polarizations. The proposed method achieves the best SAR ship detection performances with the highest figures of merits (FoM) of 97.14\%, 90.32\% and 93.75\% for the used Radarsat-2, GaoFen-3 and Sentinel-1 datasets, accordingly.},
  archive      = {J_TIP},
  author       = {Si-Wei Chen and Xing-Chao Cui and Xue-Song Wang and Shun-Ping Xiao},
  doi          = {10.1109/TIP.2021.3089936},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5969-5983},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Speckle-free SAR image ship detection},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). End-to-end light field spatial super-resolution network
using multiple epipolar geometry. <em>TIP</em>, <em>30</em>, 5956–5968.
(<a href="https://doi.org/10.1109/TIP.2021.3079805">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Light Field (LF) cameras are considered to have many potential applications since angular and spatial information is captured simultaneously. However, the limited spatial resolution has brought lots of difficulties in developing related applications and becomes the main bottleneck of LF cameras. In this paper, an end-to-end learning-based method is proposed to simultaneously reconstruct all view images in LFs with higher spatial resolution. Based on the epipolar geometry, view images in one LF are first grouped into several image stacks and fed into different network branches to learn sub-pixel details for each view image. Since LFs have dense sampling in angular domain, sub-pixel details in multiple spatial directions are learned from corresponding angular directions in multiple branches, respectively. Then, sub-pixel details from different directions are further integrated to generate global high-frequency residual details. Combined with the spatially upsampled LF, the final LF with high spatial resolution is obtained. Experimental results on synthetic and real-world datasets demonstrate that the proposed method outperforms other state-of-the-art methods in both visual and numerical evaluations. We also implement the proposed method on LFs with different angular resolution and experiments show that the proposed method achieves superior results than others, especially for LFs with small angular resolution. Furthermore, since the epipolar geometry is fully considered, the proposed network shows good performances in preserving the inherent epipolar property in LF images.},
  archive      = {J_TIP},
  author       = {Shuo Zhang and Song Chang and Youfang Lin},
  doi          = {10.1109/TIP.2021.3079805},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5956-5968},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {End-to-end light field spatial super-resolution network using multiple epipolar geometry},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep k-SVD denoising. <em>TIP</em>, <em>30</em>, 5944–5955.
(<a href="https://doi.org/10.1109/TIP.2021.3090531">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work considers noise removal from images, focusing on the well-known K-SVD denoising algorithm. This sparsity-based method was proposed in 2006, and for a short while it was considered as state-of-the-art. However, over the years it has been surpassed by other methods, including the recent deep-learning-based newcomers. The question we address in this paper is whether K-SVD was brought to its peak in its original conception, or whether it can be made competitive again. The approach we take in answering this question is to redesign the algorithm to operate in a supervised manner. More specifically, we propose an end-to-end deep architecture with the exact K-SVD computational path, and train it for optimized denoising. Our work shows how to overcome difficulties arising in turning the K-SVD scheme into a differentiable, and thus learnable, machine. With a small number of parameters to learn and while preserving the original K-SVD essence, the proposed architecture is shown to outperform the classical K-SVD algorithm substantially, and getting closer to recent state-of-the-art learning-based denoising methods. Adopting a broader context, this work touches on themes around the design of deep-learning solutions for image processing tasks, while paving a bridge between classic methods and novel deep-learning-based ones.},
  archive      = {J_TIP},
  author       = {Meyer Scetbon and Michael Elad and Peyman Milanfar},
  doi          = {10.1109/TIP.2021.3090531},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5944-5955},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Deep K-SVD denoising},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Coarse-to-fine semantic alignment for cross-modal moment
localization. <em>TIP</em>, <em>30</em>, 5933–5943. (<a
href="https://doi.org/10.1109/TIP.2021.3090521">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video moment localization, as an important branch of video content analysis, has attracted extensive attention in recent years. However, it is still in its infancy due to the following challenges: cross-modal semantic alignment and localization efficiency. To address these impediments, we present a cross-modal semantic alignment network. To be specific, we first design a video encoder to generate moment candidates, learn their representations, as well as model their semantic relevance. Meanwhile, we design a query encoder for diverse query intention understanding. Thereafter, we introduce a multi-granularity interaction module to deeply explore the semantic correlation between multi-modalities. Thereby, we can effectively complete target moment localization via sufficient cross-modal semantic understanding. Moreover, we introduce a semantic pruning strategy to reduce cross-modal retrieval overhead, improving localization efficiency. Experimental results on two benchmark datasets have justified the superiority of our model over several state-of-the-art competitors.},
  archive      = {J_TIP},
  author       = {Yupeng Hu and Liqiang Nie and Meng Liu and Kun Wang and Yinglong Wang and Xian-Sheng Hua},
  doi          = {10.1109/TIP.2021.3090521},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5933-5943},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Coarse-to-fine semantic alignment for cross-modal moment localization},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning to discover multi-class attentional regions for
multi-label image recognition. <em>TIP</em>, <em>30</em>, 5920–5932. (<a
href="https://doi.org/10.1109/TIP.2021.3088605">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-label image recognition is a practical and challenging task compared to single-label image classification. However, previous works may be suboptimal because of a great number of object proposals or complex attentional region generation modules. In this paper, we propose a simple but efficient two-stream framework to recognize multi-category objects from global image to local regions, similar to how human beings perceive objects. To bridge the gap between global and local streams, we propose a multi-class attentional region module which aims to make the number of attentional regions as small as possible and keep the diversity of these regions as high as possible. Our method can efficiently and effectively recognize multi-class objects with an affordable computation cost and a parameter-free region localization module. Over three benchmarks on multi-label image classification, our method achieves new state-of-the-art results with a single model only using image semantics without label dependency. In addition, the effectiveness of the proposed method is extensively demonstrated under different factors such as global pooling strategy, input size and network architecture. Code has been made available at https://github.com/gaobb/MCAR.},
  archive      = {J_TIP},
  author       = {Bin-Bin Gao and Hong-Yu Zhou},
  doi          = {10.1109/TIP.2021.3088605},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5920-5932},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Learning to discover multi-class attentional regions for multi-label image recognition},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Subjective and objective quality assessment of 2D and 3D
foveated video compression in virtual reality. <em>TIP</em>,
<em>30</em>, 5905–5919. (<a
href="https://doi.org/10.1109/TIP.2021.3087322">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Virtual Reality (VR), the requirements of much higher resolution and smooth viewing experiences under rapid and often real-time changes in viewing direction, leads to significant challenges in compression and communication. To reduce the stresses of very high bandwidth consumption, the concept of foveated video compression is being accorded renewed interest. By exploiting the space-variant property of retinal visual acuity, foveation has the potential to substantially reduce video resolution in the visual periphery, with hardly noticeable perceptual quality degradations. Accordingly, foveated image / video quality predictors are also becoming increasingly important, as a practical way to monitor and control future foveated compression algorithms. Towards advancing the development of foveated image / video quality assessment (FIQA / FVQA) algorithms, we have constructed 2D and (stereoscopic) 3D VR databases of foveated / compressed videos, and conducted a human study of perceptual quality on each database. Each database includes 10 reference videos and 180 foveated videos, which were processed by 3 levels of foveation on the reference videos. Foveation was applied by increasing compression with increased eccentricity. In the 2D study, each video was of resolution 7680×3840 and was viewed and quality-rated by 36 subjects, while in the 3D study, each video was of resolution 5376×5376 and rated by 34 subjects. Both studies were conducted on top of a foveated video player having low motion-to-photon latency (~50ms). We evaluated different objective image and video quality assessment algorithms, including both FIQA / FVQA algorithms and non-foveated algorithms, on our so called LIVE-Facebook Technologies Foveation-Compressed Virtual Reality (LIVE-FBT-FCVR) databases. We also present a statistical evaluation of the relative performances of these algorithms. The LIVE-FBT-FCVR databases have been made publicly available and can be accessed at https://live.ece.utexas.edu/research/LIVEFBTFCVR/index.html.},
  archive      = {J_TIP},
  author       = {Yize Jin and Meixu Chen and Todd Goodall and Anjul Patney and Alan C. Bovik},
  doi          = {10.1109/TIP.2021.3087322},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5905-5919},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Subjective and objective quality assessment of 2D and 3D foveated video compression in virtual reality},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Shape-preserving stereo object remapping via
object-consistent grid warping. <em>TIP</em>, <em>30</em>, 5889–5904.
(<a href="https://doi.org/10.1109/TIP.2021.3089947">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Viewing various stereo images under different viewing conditions has escalated the need for effective object-level remapping techniques. In this paper, we propose a new object spatial mapping scheme, which adjusts the depth and size of the selected object to match user preference and viewing conditions. Existing warping-based methods often distort the shape of important objects or cannot faithfully adjust the depth/size of the selected object due to improper warping such as local rotations. In this paper, by explicitly reducing the transformation freedom degree of warping, we propose an optimization model based on axis-aligned warping for object spatial remapping. The proposed axis-aligned warping based optimization model can simultaneously adjust the depths and sizes of selected objects to their target values without introducing severe shape distortions. Moreover, we propose object consistency constraints to ensure the size/shape of parts inside a selected object to be consistently adjusted. Such constraints improve the size/shape adjustment performance while remaining robust to some extent to incomplete object extraction. Experimental results demonstrate that the proposed method achieves high flexibility and effectiveness in adjusting the size and depth of objects compared with existing methods.},
  archive      = {J_TIP},
  author       = {Bing Li and Chia-Wen Lin and Cheng Zheng and Shan Liu and Bernard Ghanem and Wen Gao and C.-C. Jay Kuo},
  doi          = {10.1109/TIP.2021.3089947},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5889-5904},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Shape-preserving stereo object remapping via object-consistent grid warping},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). LayerCAM: Exploring hierarchical class activation maps for
localization. <em>TIP</em>, <em>30</em>, 5875–5888. (<a
href="https://doi.org/10.1109/TIP.2021.3089943">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The class activation maps are generated from the final convolutional layer of CNN. They can highlight discriminative object regions for the class of interest. These discovered object regions have been widely used for weakly-supervised tasks. However, due to the small spatial resolution of the final convolutional layer, such class activation maps often locate coarse regions of the target objects, limiting the performance of weakly-supervised tasks that need pixel-accurate object locations. Thus, we aim to generate more fine-grained object localization information from the class activation maps to locate the target objects more accurately. In this paper, by rethinking the relationships between the feature maps and their corresponding gradients, we propose a simple yet effective method, called LayerCAM. It can produce reliable class activation maps for different layers of CNN. This property enables us to collect object localization information from coarse (rough spatial localization) to fine (precise fine-grained details) levels. We further integrate them into a high-quality class activation map, where the object-related pixels can be better highlighted. To evaluate the quality of the class activation maps produced by LayerCAM, we apply them to weakly-supervised object localization and semantic segmentation. Experiments demonstrate that the class activation maps generated by our method are more effective and reliable than those by the existing attention methods. The code will be made publicly available.},
  archive      = {J_TIP},
  author       = {Peng-Tao Jiang and Chang-Bin Zhang and Qibin Hou and Ming-Ming Cheng and Yunchao Wei},
  doi          = {10.1109/TIP.2021.3089943},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5875-5888},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {LayerCAM: Exploring hierarchical class activation maps for localization},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SCG: Saliency and contour guided salient instance
segmentation. <em>TIP</em>, <em>30</em>, 5862–5874. (<a
href="https://doi.org/10.1109/TIP.2021.3088282">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Different from conventional instance segmentation, salient instance segmentation (SIS) faces two difficulties. The first is that it involves segmenting salient instances only while ignoring background, and the second is that it targets generic object instances without pre-defined object categories. In this paper, based on the state-of-the-art Mask R-CNN model, we propose to leverage complementary saliency and contour information to handle these two challenges. We first improve Mask R-CNN by introducing an interleaved execution strategy and proposing a novel mask head network to incorporate global context within each RoI. Then we add two branches to Mask R-CNN for saliency and contour detection, respectively. We fuse the Mask R-CNN features with the saliency and contour features, where the former supply pixel-wise saliency information to help with identifying salient regions and the latter provide a generic object contour prior to help detect and segment generic objects. We also propose a novel multiscale global attention model to generate attentive global features from multiscale representative features for feature fusion. Experimental results demonstrate that all our proposed model components can improve SIS performance. Finally, our overall model outperforms state-of-the-art SIS methods and Mask R-CNN by more than 6\% and 3\%, respectively. By using additional multitask training data, we can further improve the model performance on the ILSO dataset.},
  archive      = {J_TIP},
  author       = {Nian Liu and Wangbo Zhao and Ling Shao and Junwei Han},
  doi          = {10.1109/TIP.2021.3088282},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5862-5874},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {SCG: Saliency and contour guided salient instance segmentation},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-scale structure-aware network for weakly supervised
temporal action detection. <em>TIP</em>, <em>30</em>, 5848–5861. (<a
href="https://doi.org/10.1109/TIP.2021.3089361">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weakly supervised temporal action detection has better scalability and practicability than fully supervised action detection in reality deployment. However, it is difficult to learn a robust model without temporal action boundary annotations. In this paper, we propose an en-to-end Multi-Scale Structure-Aware Network (MSA-Net) for weakly supervised temporal action detection by exploring both the global structure information of a video and the local structure information of actions. The proposed SA-Net enjoys several merits. First, to localize actions with different durations, each video is encoded into feature representations with different temporal scales. Second, based on the multi-scale feature representation, the proposed model has designed two effective structure modeling mechanisms including global structure modeling and local structure modeling, which can effectively learn discriminative structure aware representations for robust and complete action detection. To the best of our knowledge, this is the first work to fully explore the global and local structure information in a unified deep model for weakly supervised action detection. And extensive experimental results on two benchmark datasets demonstrate that the proposed MSA-Net performs favorably against state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Wenfei Yang and Tianzhu Zhang and Zhendong Mao and Yongdong Zhang and Qi Tian and Feng Wu},
  doi          = {10.1109/TIP.2021.3089361},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5848-5861},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Multi-scale structure-aware network for weakly supervised temporal action detection},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Detail preserving coarse-to-fine matching for stereo
matching and optical flow. <em>TIP</em>, <em>30</em>, 5835–5847. (<a
href="https://doi.org/10.1109/TIP.2021.3088635">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Coarse-To-Fine (CTF) matching scheme has been widely applied to reduce computational complexity and matching ambiguity in stereo matching and optical flow tasks by converting image pairs into multi-scale representations and performing matching from coarse to fine levels. Despite its efficiency, it suffers from several weaknesses, such as tending to blur the edges and miss small structures like thin bars and holes. We find that the pixels of small structures and edges are often assigned with wrong disparity/flow in the upsampling process of the CTF framework, introducing errors to the fine levels and leading to such weaknesses. We observe that these wrong disparity/flow values can be avoided if we select the best-matched value among their neighborhood, which inspires us to propose a novel differentiable Neighbor-Search Upsampling (NSU) module. The NSU module first estimates the matching scores and then selects the best-matched disparity/flow for each pixel from its neighbors. It effectively preserves finer structure details by exploiting the information from the finer level while upsampling the disparity/flow. The proposed module can be a drop-in replacement of the naive upsampling in the CTF matching framework and allows the neural networks to be trained end-to-end. By integrating the proposed NSU module into a baseline CTF matching network, we design our Detail Preserving Coarse-To-Fine (DPCTF) matching network. Comprehensive experiments demonstrate that our DPCTF can boost performances for both stereo matching and optical flow tasks. Notably, our DPCTF achieves new state-of-the-art performances for both tasks - it outperforms the competitive baseline (Bi3D) by 28.8\% (from 0.73 to 0.52) on EPE of the FlyingThings3D stereo dataset, and ranks first in KITTI flow 2012 benchmark. The code is available at https://github.com/Deng-Y/DPCTF.},
  archive      = {J_TIP},
  author       = {Yong Deng and Jimin Xiao and Steven Zhiying Zhou and Jiashi Feng},
  doi          = {10.1109/TIP.2021.3088635},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5835-5847},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Detail preserving coarse-to-fine matching for stereo matching and optical flow},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Triply complementary priors for image restoration.
<em>TIP</em>, <em>30</em>, 5819–5834. (<a
href="https://doi.org/10.1109/TIP.2021.3086049">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent works that utilized deep models have achieved superior results in various image restoration (IR) applications. Such approach is typically supervised, which requires a corpus of training images with distributions similar to the images to be recovered. On the other hand, the shallow methods, which are usually unsupervised remain promising performance in many inverse problems, e.g., image deblurring and image compressive sensing (CS), as they can effectively leverage nonlocal self-similarity priors of natural images. However, most of such methods are patch-based leading to the restored images with various artifacts due to naive patch aggregation in addition to the slow speed. Using either approach alone usually limits performance and generalizability in IR tasks. In this paper, we propose a joint low-rank and deep (LRD) image model, which contains a pair of triply complementary priors, namely, internal and external, shallow and deep, and non-local and local priors. We then propose a novel hybrid plug-and-play (H-PnP) framework based on the LRD model for IR. Following this, a simple yet effective algorithm is developed to solve the proposed H-PnP based IR problems. Extensive experimental results on several representative IR tasks, including image deblurring, image CS and image deblocking, demonstrate that the proposed H-PnP algorithm achieves favorable performance compared to many popular or state-of-the-art IR methods in terms of both objective and visual perception.},
  archive      = {J_TIP},
  author       = {Zhiyuan Zha and Bihan Wen and Xin Yuan and Joey Tianyi Zhou and Jiantao Zhou and Ce Zhu},
  doi          = {10.1109/TIP.2021.3086049},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5819-5834},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Triply complementary priors for image restoration},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Knowledge exchange between domain-adversarial and private
networks improves open set image classification. <em>TIP</em>,
<em>30</em>, 5807–5818. (<a
href="https://doi.org/10.1109/TIP.2021.3088642">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Both target-specific and domain-invariant features can facilitate Open Set Domain Adaptation (OSDA). To exploit these features, we propose a Knowledge Exchange (KnowEx) model which jointly trains two complementary constituent networks: (1) a Domain-Adversarial Network (DAdvNet) learning the domain-invariant representation, through which the supervision in source domain can be exploited to infer the class information of unlabeled target data; (2) a Private Network (PrivNet) exclusive for target domain, which is beneficial for discriminating between instances from known and unknown classes. The two constituent networks exchange training experience in the learning process. Toward this end, we exploit an adversarial perturbation process against DAdvNet to regularize PrivNet. This enhances the complementarity between the two networks. At the same time, we incorporate an adaptation layer into DAdvNet to address the unreliability of the PrivNet&#39;s experience. Therefore, DAdvNet and PrivNet are able to mutually reinforce each other during training. We have conducted thorough experiments on multiple standard benchmarks to verify the effectiveness and superiority of KnowEx in OSDA.},
  archive      = {J_TIP},
  author       = {Haohong Zhou and Mohamed Azzam and Jian Zhong and Cheng Liu and Si Wu and Hau-San Wong},
  doi          = {10.1109/TIP.2021.3088642},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5807-5818},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Knowledge exchange between domain-adversarial and private networks improves open set image classification},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SADRNet: Self-aligned dual face regression networks for
robust 3D dense face alignment and reconstruction. <em>TIP</em>,
<em>30</em>, 5793–5806. (<a
href="https://doi.org/10.1109/TIP.2021.3087397">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Three-dimensional face dense alignment and reconstruction in the wild is a challenging problem as partial facial information is commonly missing in occluded and large pose face images. Large head pose variations also increase the solution space and make the modeling more difficult. Our key idea is to model occlusion and pose to decompose this challenging task into several relatively more manageable subtasks. To this end, we propose an end-to-end framework, termed as Self-aligned Dual face Regression Network (SADRNet), which predicts a pose-dependent face, a pose-independent face. They are combined by an occlusion-aware self-alignment to generate the final 3D face. Extensive experiments on two popular benchmarks, AFLW2000-3D and Florence, demonstrate that the proposed method achieves significant superior performance over existing state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Zeyu Ruan and Changqing Zou and Longhai Wu and Gangshan Wu and Limin Wang},
  doi          = {10.1109/TIP.2021.3087397},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5793-5806},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {SADRNet: Self-aligned dual face regression networks for robust 3D dense face alignment and reconstruction},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Training robust object detectors from noisy category labels
and imprecise bounding boxes. <em>TIP</em>, <em>30</em>, 5782–5792. (<a
href="https://doi.org/10.1109/TIP.2021.3085208">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object detection has gained great improvements with the advances of convolutional neural networks and the availability of large amounts of accurate training data. Though the amount of data is increasing significantly, the quality of data annotations is not guaranteed from the existing crowd-sourcing labeling platforms. In addition to noisy category labels, imprecise bounding box annotations are commonly existed for object detection data. When the quality of training data degenerates, the performance of the typical object detectors is severely impaired. In this paper, we propose a Meta-Refine-Net (MRNet) to train object detectors from noisy category labels and imprecise bounding boxes. First, MRNet learns to adaptively assign lower weights to proposals with incorrect labels so as to suppress large loss values generated by these proposals on the classification branch. Second, MRNet learns to dynamically generate more accurate bounding box annotations to overcome the misleading of imprecisely annotated bounding boxes. Thus, the imprecise bounding boxes could impose positive impacts on the regression branch rather than simply be ignored. Third, we propose to refine the imprecise bounding box annotations by jointly learning from both the category and the localization information. By doing this, the approximation of ground-truth bounding boxes is more accurate while the misleading would be further alleviated. Our MRNet is model-agnostic and is capable of learning from noisy object detection data with only a few clean examples (less than 2\%). Extensive experiments on PASCAL VOC 2012 and MS COCO 2017 demonstrate the effectiveness and efficiency of our method.},
  archive      = {J_TIP},
  author       = {Youjiang Xu and Linchao Zhu and Yi Yang and Fei Wu},
  doi          = {10.1109/TIP.2021.3085208},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5782-5792},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Training robust object detectors from noisy category labels and imprecise bounding boxes},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Training robust deep neural networks via adversarial noise
propagation. <em>TIP</em>, <em>30</em>, 5769–5781. (<a
href="https://doi.org/10.1109/TIP.2021.3082317">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In practice, deep neural networks have been found to be vulnerable to various types of noise, such as adversarial examples and corruption. Various adversarial defense methods have accordingly been developed to improve adversarial robustness for deep models. However, simply training on data mixed with adversarial examples, most of these models still fail to defend against the generalized types of noise. Motivated by the fact that hidden layers play a highly important role in maintaining a robust model, this paper proposes a simple yet powerful training algorithm, named Adversarial Noise Propagation (ANP), which injects noise into the hidden layers in a layer-wise manner. ANP can be implemented efficiently by exploiting the nature of the backward-forward training style. Through thorough investigations, we determine that different hidden layers make different contributions to model robustness and clean accuracy, while shallow layers are comparatively more critical than deep layers. Moreover, our framework can be easily combined with other adversarial training methods to further improve model robustness by exploiting the potential of hidden layers. Extensive experiments on MNIST, CIFAR-10, CIFAR-10-C, CIFAR-10-P, and ImageNet demonstrate that ANP enables the strong robustness for deep models against both adversarial and corrupted ones, and also significantly outperforms various adversarial defense methods.},
  archive      = {J_TIP},
  author       = {Aishan Liu and Xianglong Liu and Hang Yu and Chongzhi Zhang and Qiang Liu and Dacheng Tao},
  doi          = {10.1109/TIP.2021.3082317},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5769-5781},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Training robust deep neural networks via adversarial noise propagation},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Model-guided deep hyperspectral image super-resolution.
<em>TIP</em>, <em>30</em>, 5754–5768. (<a
href="https://doi.org/10.1109/TIP.2021.3078058">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The trade-off between spatial and spectral resolution is one of the fundamental issues in hyperspectral images (HSI). Given the challenges of directly acquiring high-resolution hyperspectral images (HR-HSI), a compromised solution is to fuse a pair of images: one has high-resolution (HR) in the spatial domain but low-resolution (LR) in spectral-domain and the other vice versa. Model-based image fusion methods including pan-sharpening aim at reconstructing HR-HSI by solving manually designed objective functions. However, such hand-crafted prior often leads to inevitable performance degradation due to a lack of end-to-end optimization. Although several deep learning-based methods have been proposed for hyperspectral pan-sharpening, HR-HSI related domain knowledge has not been fully exploited, leaving room for further improvement. In this paper, we propose an iterative Hyperspectral Image Super-Resolution (HSISR) algorithm based on a deep HSI denoiser to leverage both domain knowledge likelihood and deep image prior. By taking the observation matrix of HSI into account during the end-to-end optimization, we show how to unfold an iterative HSISR algorithm into a novel model-guided deep convolutional network (MoG-DCN). The representation of the observation matrix by subnetworks also allows the unfolded deep HSISR network to work with different HSI situations, which enhances the flexibility of MoG-DCN. Extensive experimental results are reported to demonstrate that the proposed MoG-DCN outperforms several leading HSISR methods in terms of both implementation cost and visual quality. The code is available at https://see.xidian.edu.cn/faculty/wsdong/Projects/MoG-DCN.htm.},
  archive      = {J_TIP},
  author       = {Weisheng Dong and Chen Zhou and Fangfang Wu and Jinjian Wu and Guangming Shi and Xin Li},
  doi          = {10.1109/TIP.2021.3078058},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5754-5768},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Model-guided deep hyperspectral image super-resolution},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Active subdivision surfaces for the semiautomatic
segmentation of biomedical volumes. <em>TIP</em>, <em>30</em>,
5739–5753. (<a href="https://doi.org/10.1109/TIP.2021.3087947">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a new family of active surfaces for the semiautomatic segmentation of volumetric objects in 3D biomedical images. We represent our deformable model by a subdivision surface encoded by a small set of control points and generated through a geometric refinement process. The subdivision operator confers important properties to the surface such as smoothness, reproduction of desirable shapes and interpolation of the control points. We deform the subdivision surface through the minimization of suitable gradient-based and region-based energy terms that we have designed for that purpose. In addition, we provide an easy way to combine these energies with convolutional neural networks. Our active subdivision surface satisfies the property of multiresolution, which allows us to adopt a coarse-to-fine optimization strategy. This speeds up the computations and decreases its dependence on initialization compared to singleresolution active surfaces. Performance evaluations on both synthetic and real biomedical data show that our active subdivision surface is robust in the presence of noise and outperforms current state-of-the-art methods. In addition, we provide a software that gives full control over the active subdivision surface via an intuitive manipulation of the control points.},
  archive      = {J_TIP},
  author       = {Anaïs Badoual and Lucia Romani and Michael Unser},
  doi          = {10.1109/TIP.2021.3087947},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5739-5753},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Active subdivision surfaces for the semiautomatic segmentation of biomedical volumes},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-resolution aitchison geometry image denoising for
low-light photography. <em>TIP</em>, <em>30</em>, 5724–5738. (<a
href="https://doi.org/10.1109/TIP.2021.3087943">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the low-photon imaging regime, noise in the image sensors is dominated by shot noise, best modeled statistically as Poisson distribution. In this work, we show that the Poisson likelihood function is very well matched with the Bayesian estimation of the “difference of log of contrast of pixel intensities.” More specifically, our work is rooted in statistical compositional data analysis, whereby we reinterpret the Aitchison geometry as a multi-resolution analysis in the log-pixel domain. We demonstrate that the difference-log-contrast has wavelet-like properties that correspond well with the human visual system, while being robust to illumination variations. We derive a denoising technique based on an approximate conjugate prior for the latent Aitchison variable that gives rise to an explicit minimum mean squared error estimation. The resulting denoising technique preserves image contrast details that are arguably more meaningful to human vision than the pixel intensity values themselves.},
  archive      = {J_TIP},
  author       = {Sarah Miller and Chen Zhang and Keigo Hirakawa},
  doi          = {10.1109/TIP.2021.3087943},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5724-5738},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Multi-resolution aitchison geometry image denoising for low-light photography},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Combining appearance and gradient information for image
symmetry detection. <em>TIP</em>, <em>30</em>, 5708–5723. (<a
href="https://doi.org/10.1109/TIP.2021.3085202">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work addresses the challenging problem of reflection symmetry detection in unconstrained environments. Starting from the understanding on how the visual cortex manages planar symmetry detection, it is proposed to treat the problem in two stages: i) the design of a stable metric that extracts subsets of consistently oriented candidate segments, whenever the underlying 2D signal appearance exhibits definite near symmetric correspondences; ii) the ranking of such segments on the basis of the surrounding gradient orientation specularity, in order to reflect real symmetric object boundaries. Since these operations are related to the way the human brain performs planar symmetry detection, a better correspondence can be established between the outcomes of the proposed algorithm and a human-constructed ground truth. When compared to the testing sets used in recent symmetry detection competitions, a remarkable performance gain can be observed. In additional, further validation has been achieved by conducting perceptual validation experiments with users on a newly built dataset.},
  archive      = {J_TIP},
  author       = {Alessandro Gnutti and Fabrizio Guerrini and Riccardo Leonardi},
  doi          = {10.1109/TIP.2021.3085202},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5708-5723},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Combining appearance and gradient information for image symmetry detection},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Quantization parameter cascading for surveillance video
coding considering all inter reference frames. <em>TIP</em>,
<em>30</em>, 5692–5707. (<a
href="https://doi.org/10.1109/TIP.2021.3087413">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video surveillance and its applications have become increasingly ubiquitous in modern daily life. In video surveillance system, video coding as a critical enabling technology determines the effective transmission and storage of surveillance videos. In order to meet the real-time or time-critical transmission requirements of video surveillance systems, the low-delay (LD) configuration of the advanced high efficiency video coding (HEVC) standard is usually used to encode surveillance videos. The coding efficiency of the LD configuration is closely related to the quantization parameter (QP) cascading technique which selects or determines the QPs for encoding. However, the quantization parameter cascading (QPC) technique currently adopted for the LD configuration in HEVC test model (i.e., HM) is not optimized since it has not taken full account of the reference dependency in coding. In this paper, an efficient QPC technique for surveillance video coding, referred to as QPC-SV, is proposed, considering all inter reference frames under the LD configuration. Experimental results demonstrate the efficacy of the proposed QPC-SV. Compared with the default configuration of QPC in the HM, the QPC-SV achieves significant rate-distortion performance gain with average BD-rates of −9.35\% and −9.76\% for the LDP and LDB configurations, respectively.},
  archive      = {J_TIP},
  author       = {Yanchao Gong and Kaifang Yang and Ying Liu and Keng-Pang Lim and Nam Ling and Hong Ren Wu},
  doi          = {10.1109/TIP.2021.3087413},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5692-5707},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Quantization parameter cascading for surveillance video coding considering all inter reference frames},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-interactive dual-decoder for RGB-thermal salient
object detection. <em>TIP</em>, <em>30</em>, 5678–5691. (<a
href="https://doi.org/10.1109/TIP.2021.3087412">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {RGB-thermal salient object detection (SOD) aims to segment the common prominent regions of visible image and corresponding thermal infrared image that we call it RGBT SOD. Existing methods don&#39;t fully explore and exploit the potentials of complementarity of different modalities and multi-type cues of image contents, which play a vital role in achieving accurate results. In this paper, we propose a multi-interactive dual-decoder to mine and model the multi-type interactions for accurate RGBT SOD. In specific, we first encode two modalities into multi-level multi-modal feature representations. Then, we design a novel dual-decoder to conduct the interactions of multi-level features, two modalities and global contexts. With these interactions, our method works well in diversely challenging scenarios even in the presence of invalid modality. Finally, we carry out extensive experiments on public RGBT and RGBD SOD datasets, and the results show that the proposed method achieves the outstanding performance against state-of-the-art algorithms. The source code has been released at: https://github.com/lz118/Multi-interactive-Dual-decoder.},
  archive      = {J_TIP},
  author       = {Zhengzheng Tu and Zhun Li and Chenglong Li and Yang Lang and Jin Tang},
  doi          = {10.1109/TIP.2021.3087412},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5678-5691},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Multi-interactive dual-decoder for RGB-thermal salient object detection},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Diversity image coding using irregular interpolation.
<em>TIP</em>, <em>30</em>, 5665–5677. (<a
href="https://doi.org/10.1109/TIP.2021.3087404">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diversity “multiple description” (MD) source coding promises graceful degradation in the presence of a priori unknown number of erased packets in the channel. A simple coding scheme for the case of two packets consists of oversampling the source by a factor of two and delta-sigma quantization. This approach was applied successfully to JPEG-based image coding over a lossy packet network, where the interpolation and splitting into two descriptions are done in the discrete cosine transform (DCT) domain. Moreover, unlike the classical source-channel separation approach - which is designed for a predetermined number of erasures (say, K out of N), hence its distortion does not improve when the channel behaves better than expected - an MD coding scheme aims to achieve a better reconstruction quality when more or all the N descriptions are received at the decoder side. The extension to a larger number of descriptions, however, suffers from noise amplification whenever the received descriptions form a non-uniform sampling pattern. In this work, we examine inter- and intra-block interpolation methods, and show how noise amplification can be reduced by redesigning the interpolation filter at the encoder. Specifically, for a given total coding rate, we demonstrate that an “irregular” interpolation filter is robust to the pattern of received packets over all ( K out of N) patterns, with some degradation relative to low-pass (LP) interpolation in the case where all N packets arrived. We provide experimental results comparing LP and irregular interpolation filters, and examine the effect of noise shaping on the trade-off between the central distortion (receiving all packets) and side distortion (receiving K packets).},
  archive      = {J_TIP},
  author       = {Mor Goren and Ram Zamir},
  doi          = {10.1109/TIP.2021.3087404},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5665-5677},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Diversity image coding using irregular interpolation},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). CycleSegNet: Object co-segmentation with cycle refinement
and region correspondence. <em>TIP</em>, <em>30</em>, 5652–5664. (<a
href="https://doi.org/10.1109/TIP.2021.3087401">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image co-segmentation is an active computer vision task that aims to segment the common objects from a set of images. Recently, researchers design various learning-based algorithms to undertake the co-segmentation task. The main difficulty in this task is how to effectively transfer information between images to make conditional predictions. In this paper, we present CycleSegNet, a novel framework for the co-segmentation task. Our network design has two key components: a region correspondence module which is the basic operation for exchanging information between local image regions, and a cycle refinement module, which utilizes ConvLSTMs to progressively update image representations and exchange information in a cycle and iterative manner. Extensive experiments demonstrate that our proposed method significantly outperforms the state-of-the-art methods on four popular benchmark datasets - PASCAL VOC dataset, MSRC dataset, Internet dataset, and iCoseg dataset, by 2.6\%, 7.7\%, 2.2\%, and 2.9\%, respectively.},
  archive      = {J_TIP},
  author       = {Chi Zhang and Guankai Li and Guosheng Lin and Qingyao Wu and Rui Yao},
  doi          = {10.1109/TIP.2021.3087401},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5652-5664},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {CycleSegNet: Object co-segmentation with cycle refinement and region correspondence},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Efficient face detection in the fisheye image domain.
<em>TIP</em>, <em>30</em>, 5641–5651. (<a
href="https://doi.org/10.1109/TIP.2021.3087400">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Significant progress has been made for face detection from normal images in recent years; however, accurate and fast face detection from fisheye images remains a challenging issue because of serious fisheye distortion in the peripheral region of the image. To improve face detection accuracy, we propose a light-weight location-aware network to distinguish the peripheral region from the central region in the feature learning stage. To match the face detector, the shape and scale of the anchor (bounding box) is made location dependent. The overall face detection system performs directly in the fisheye image domain without rectification and calibration and hence is agnostic of the fisheye projection parameters. Experiments on Wider-360 and real-world fisheye images using a single CPU core indeed show that our method is superior to the state-of-the-art real-time face detector RFB Net.},
  archive      = {J_TIP},
  author       = {Cheng-Yun Yang and Homer H. Chen},
  doi          = {10.1109/TIP.2021.3087400},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5641-5651},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Efficient face detection in the fisheye image domain},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Searching multi-rate and multi-modal temporal enhanced
networks for gesture recognition. <em>TIP</em>, <em>30</em>, 5626–5640.
(<a href="https://doi.org/10.1109/TIP.2021.3087348">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gesture recognition has attracted considerable attention owing to its great potential in applications. Although the great progress has been made recently in multi-modal learning methods, existing methods still lack effective integration to fully explore synergies among spatio-temporal modalities effectively for gesture recognition. The problems are partially due to the fact that the existing manually designed network architectures have low efficiency in the joint learning of multi-modalities. In this paper, we propose the first neural architecture search (NAS)-based method for RGB-D gesture recognition. The proposed method includes two key components: 1) enhanced temporal representation via the proposed 3D Central Difference Convolution (3D-CDC) family, which is able to capture rich temporal context via aggregating temporal difference information; and 2) optimized backbones for multi-sampling-rate branches and lateral connections among varied modalities. The resultant multi-modal multi-rate network provides a new perspective to understand the relationship between RGB and depth modalities and their temporal dynamics. Comprehensive experiments are performed on three benchmark datasets (IsoGD, NvGesture, and EgoGesture), demonstrating the state-of-the-art performance in both single- and multi-modality settings. The code is available at https://github.com/ZitongYu/3DCDC-NAS .},
  archive      = {J_TIP},
  author       = {Zitong Yu and Benjia Zhou and Jun Wan and Pichao Wang and Haoyu Chen and Xin Liu and Stan Z. Li and Guoying Zhao},
  doi          = {10.1109/TIP.2021.3087348},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5626-5640},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Searching multi-rate and multi-modal temporal enhanced networks for gesture recognition},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). RGBT tracking via multi-adapter network with hierarchical
divergence loss. <em>TIP</em>, <em>30</em>, 5613–5625. (<a
href="https://doi.org/10.1109/TIP.2021.3087341">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {RGBT tracking has attracted increasing attention since RGB and thermal infrared data have strong complementary advantages, which could make trackers all-day and all-weather work. Existing works usually focus on extracting modality-shared or modality-specific information, but the potentials of these two cues are not well explored and exploited in RGBT tracking. In this paper, we propose a novel multi-adapter network to jointly perform modality-shared, modality-specific and instance-aware target representation learning for RGBT tracking. To this end, we design three kinds of adapters within an end-to-end deep learning framework. In specific, we use the modified VGG-M as the generality adapter to extract the modality-shared target representations. To extract the modality-specific features while reducing the computational complexity, we design a modality adapter, which adds a small block to the generality adapter in each layer and each modality in a parallel manner. Such a design could learn multilevel modality-specific representations with a modest number of parameters as the vast majority of parameters are shared with the generality adapter. We also design instance adapter to capture the appearance properties and temporal variations of a certain target. Moreover, to enhance the shared and specific features, we employ the loss of multiple kernel maximum mean discrepancy to measure the distribution divergence of different modal features and integrate it into each layer for more robust representation learning. Extensive experiments on two RGBT tracking benchmark datasets demonstrate the outstanding performance of the proposed tracker against the state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Andong Lu and Chenglong Li and Yuqing Yan and Jin Tang and Bin Luo},
  doi          = {10.1109/TIP.2021.3087341},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5613-5625},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {RGBT tracking via multi-adapter network with hierarchical divergence loss},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-stage degradation homogenization for super-resolution
of face images with extreme degradations. <em>TIP</em>, <em>30</em>,
5600–5612. (<a href="https://doi.org/10.1109/TIP.2021.3086595">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face Super-Resolution (FSR) aims to infer High-Resolution (HR) face images from the captured Low-Resolution (LR) face image with the assistance of external information. Existing FSR methods are less effective for the LR face images captured with serious low-quality since the huge imaging/degradation gap caused by the different imaging scenarios (i.e., the complex practical imaging scenario that generates test LR images, the simple manual imaging degradation that generates the training LR images) is not considered in these algorithms. In this paper, we propose an image homogenization strategy via re-expression to solve this problem. In contrast to existing methods, we propose a homogenization projection in LR space and HR space as compensation for the classical LR/HR projection to formulate the FSR in a multi-stage framework. We then develop a re-expression process to bridge the gap between the complex degradation and the simple degradation, which can remove the heterogeneous factors such as serious noise and blur. To further improve the accuracy of the homogenization, we extract the image patch set that is invariant to degradation changes as Robust Neighbor Resources (RNR), with which these two homogenization projections re-express the input LR images and the initial inferred HR images successively. Both quantitative and qualitative results on the public datasets demonstrate the effectiveness of the proposed algorithm against the state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Liang Chen and Jinshan Pan and Junjun Jiang and Jiawei Zhang and Zhen Han and Linchao Bao},
  doi          = {10.1109/TIP.2021.3086595},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5600-5612},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Multi-stage degradation homogenization for super-resolution of face images with extreme degradations},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MABAN: Multi-agent boundary-aware network for natural
language moment retrieval. <em>TIP</em>, <em>30</em>, 5589–5599. (<a
href="https://doi.org/10.1109/TIP.2021.3086591">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The amount of videos over the Internet and electronic surveillant cameras is growing dramatically, meanwhile paired sentence descriptions are significant clues to select attentional contents from videos. The task of natural language moment retrieval (NLMR) has drawn great interests from both academia and industry, which aims to associate specific video moments with the text descriptions figuring complex scenarios and multiple activities. In general, NLMR requires temporal context to be properly comprehended, and the existing studies suffer from two problems: (1) limited moment selection and (2) insufficient comprehension of structural context. To address these issues, a multi-agent boundary-aware network (MABAN) is proposed in this work. To guarantee flexible and goal-oriented moment selection, MABAN utilizes multi-agent reinforcement learning to decompose NLMR into localizing the two temporal boundary points for each moment. Specially, MABAN employs a two-phase cross-modal interaction to exploit the rich contextual semantic information. Moreover, temporal distance regression is considered to deduce the temporal boundaries, with which the agents can enhance the comprehension of structural context. Extensive experiments are carried out on two challenging benchmark datasets of ActivityNet Captions and Charades-STA, which demonstrate the effectiveness of the proposed approach as compared to state-of-the-art methods. The project page can be found in https://mic.tongji.edu.cn/e5/23/c9778a189731/page.htm.},
  archive      = {J_TIP},
  author       = {Xiaoyang Sun and Hanli Wang and Bin He},
  doi          = {10.1109/TIP.2021.3086591},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5589-5599},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {MABAN: Multi-agent boundary-aware network for natural language moment retrieval},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Semantics-aware adaptive knowledge distillation for
sensor-to-vision action recognition. <em>TIP</em>, <em>30</em>,
5573–5588. (<a href="https://doi.org/10.1109/TIP.2021.3086590">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing vision-based action recognition is susceptible to occlusion and appearance variations, while wearable sensors can alleviate these challenges by capturing human motion with one-dimensional time-series signals (e.g. acceleration, gyroscope, and orientation). For the same action, the knowledge learned from vision sensors (videos or images) and wearable sensors, may be related and complementary. However, there exists a significantly large modality difference between action data captured by wearable-sensor and vision-sensor in data dimension, data distribution, and inherent information content. In this paper, we propose a novel framework, named Semantics-aware Adaptive Knowledge Distillation Networks (SAKDN), to enhance action recognition in vision-sensor modality (videos) by adaptively transferring and distilling the knowledge from multiple wearable sensors. The SAKDN uses multiple wearable-sensors as teacher modalities and uses RGB videos as student modalities. To preserve the local temporal relationship and facilitate employing visual deep learning models, we transform one-dimensional time-series signals of wearable sensors to two-dimensional images by designing a gramian angular field based virtual image generation model. Then, we introduce a novel Similarity-Preserving Adaptive Multi-modal Fusion Module (SPAMFM) to adaptively fuse intermediate representation knowledge from different teacher networks. Finally, to fully exploit and transfer the knowledge of multiple well-trained teacher networks to the student network, we propose a novel Graph-guided Semantically Discriminative Mapping (GSDM) module, which utilizes graph-guided ablation analysis to produce a good visual explanation to highlight the important regions across modalities and concurrently preserve the interrelations of original data. Experimental results on Berkeley-MHAD, UTD-MHAD, and MMAct datasets well demonstrate the effectiveness of our proposed SAKDN for adaptive knowledge transfer from wearable-sensors modalities to vision-sensors modalities. The code is publicly available at https://github.com/YangLiu9208/SAKDN .},
  archive      = {J_TIP},
  author       = {Yang Liu and Keze Wang and Guanbin Li and Liang Lin},
  doi          = {10.1109/TIP.2021.3086590},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5573-5588},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Semantics-aware adaptive knowledge distillation for sensor-to-vision action recognition},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DanceIt: Music-inspired dancing video synthesis.
<em>TIP</em>, <em>30</em>, 5559–5572. (<a
href="https://doi.org/10.1109/TIP.2021.3086082">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Close your eyes and listen to music, one can easily imagine an actor dancing rhythmically along with the music. These dance movements are usually made up of dance movements you have seen before. In this paper, we propose to reproduce such an inherent capability of the human-being within a computer vision system. The proposed system consists of three modules. To explore the relationship between music and dance movements, we propose a cross-modal alignment module that focuses on dancing video clips, accompanied by pre-designed music, to learn a system that can judge the consistency between the visual features of pose sequences and the acoustic features of music. The learned model is then used in the imagination module to select a pose sequence for the given music. Such pose sequence selected from the music, however, is usually discontinuous. To solve this problem, in the spatial-temporal alignment module we develop a spatial alignment algorithm based on the tendency and periodicity of dance movements to predict dance movements between discontinuous fragments. In addition, the selected pose sequence is often misaligned with the music beat. To solve this problem, we further develop a temporal alignment algorithm to align the rhythm of music and dance. Finally, the processed pose sequence is used to synthesize realistic dancing videos in the imagination module. The generated dancing videos match the content and rhythm of the music. Experimental results and subjective evaluations show that the proposed approach can perform the function of generating promising dancing videos by inputting music.},
  archive      = {J_TIP},
  author       = {Xin Guo and Yifan Zhao and Jia Li},
  doi          = {10.1109/TIP.2021.3086082},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5559-5572},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {DanceIt: Music-inspired dancing video synthesis},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Image stitching based on semantic planar region consensus.
<em>TIP</em>, <em>30</em>, 5545–5558. (<a
href="https://doi.org/10.1109/TIP.2021.3086079">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image stitching for two images without a global transformation between them is notoriously difficult. In this paper, noticing the importance of semantic planar structures under perspective geometry, we propose a new image stitching method which stitches images by allowing for the alignment of a set of matched dominant semantic planar regions. Clearly different from previous methods resorting to plane segmentation, the key to our approach is to utilize rich semantic information directly from RGB images to extract semantic planar image regions with a deep Convolutional Neural Network (CNN). We specifically design a module implementing our newly proposed clustering loss to make full use of existing semantic segmentation networks to accommodate region segmentation. To train the network, a dataset for semantic planar region segmentation is constructed. With the prior of semantic planar region, a set of local transformation models can be obtained by constraining matched regions, enabling more precise alignment in the overlapping area. We also use this prior to estimate a transformation field over the whole image. The final mosaic is obtained by mesh-based optimization which maintains high alignment accuracy and relaxes similarity transformation at the same time. Extensive experiments with both qualitative and quantitative comparisons show that our method can deal with different situations and outperforms the state-of-the-arts on challenging scenes.},
  archive      = {J_TIP},
  author       = {Aocheng Li and Jie Guo and Yanwen Guo},
  doi          = {10.1109/TIP.2021.3086079},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5545-5558},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Image stitching based on semantic planar region consensus},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust segmentation-free algorithm for homogeneity
quantification in images. <em>TIP</em>, <em>30</em>, 5533–5544. (<a
href="https://doi.org/10.1109/TIP.2021.3086053">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Objective: Homogeneity is a notion used to describe images in various fields and is often linked to critical aspects of those fields. However, this term is rarely defined in the literature and no gold standard exists for its quantification. A few quantification algorithms have been proposed, but they lack both simplicity and robustness. As a result, the scientific community uses the notion of homogeneity in subjective analysis, preventing objective comparison of a large number of data or of different studies. The main objectives of this manuscript are to propose a definition of homogeneity and an algorithm for its quantification. Method: This algorithm, called MASQH, rely on a multi-scale, statistical and segmentation-free approach and outputs a single homogeneity index, which makes it robust and easy to use. Results: The performance and reliability of the method are demonstrated through three case studies: Firstly, on synthetic images to study the behavior and assess the relevance of the algorithm in diverse situations and hence, in various potential fields. Secondly, on histological images derived from experimental chitosan-platelet-rich-plasma hybrid biomaterial, where the quantitative results are compared to a qualitative classification provided by an expert in the field. Thirdly, on experimental nanocomposites images for which results are compared to two other homogeneity quantification algorithms from the field of nanocomposites. Conclusion and significance: By quantifying homogeneity, the MASQH method may help to compare disparate studies in the literature and quantitatively demonstrate the impact of homogeneity in various fields. The MASQH method is freely available online.},
  archive      = {J_TIP},
  author       = {Fiona Milano and Anik Chevrier and Gregory De Crescenzo and Marc Lavertu},
  doi          = {10.1109/TIP.2021.3086053},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5533-5544},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Robust segmentation-free algorithm for homogeneity quantification in images},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Rate-distortion optimized graph coarsening and partitioning
for light field coding. <em>TIP</em>, <em>30</em>, 5518–5532. (<a
href="https://doi.org/10.1109/TIP.2021.3085203">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph-based transforms are powerful tools for signal representation and energy compaction. However, their use for high dimensional signals such as light fields poses obvious problems of complexity. To overcome this difficulty, one can consider local graph transforms defined on supports of limited dimension, which may however not allow us to fully exploit long-term signal correlation. In this paper, we present methods to optimize local graph supports in a rate distortion sense for efficient light field compression. A large graph support can be well adapted for compression efficiency, however at the expense of high complexity. In this case, we use graph reduction techniques to make the graph transform feasible. We also consider spectral clustering to reduce the dimension of the graph supports while controlling both rate and complexity. We derive the distortion and rate models which are then used to guide the graph optimization. We describe a complete light field coding scheme based on the proposed graph optimization tools. Experimental results show rate-distortion performance gains compared to the use of fixed graph support. The method also provides competitive results when compared against HEVC-based and the JPEG Pleno light field coding schemes. We also assess the method against a homography-based low rank approximation and a Fourier disparity layer based coding method.},
  archive      = {J_TIP},
  author       = {Mira Rizkallah and Thomas Maugey and Christine Guillemot},
  doi          = {10.1109/TIP.2021.3085203},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5518-5532},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Rate-distortion optimized graph coarsening and partitioning for light field coding},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Domain shift preservation for zero-shot domain adaptation.
<em>TIP</em>, <em>30</em>, 5505–5517. (<a
href="https://doi.org/10.1109/TIP.2021.3084354">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In learning-based image processing a model that is learned in one domain often performs poorly in another since the image samples originate from different sources and thus have different distributions. Domain adaptation techniques alleviate the problem of domain shift by learning transferable knowledge from the source domain to the target domain. Zero-shot domain adaptation (ZSDA) refers to a category of challenging tasks in which no target-domain sample for the task of interest is accessible for training. To address this challenge, we propose a simple but effective method that is based on the strategy of domain shift preservation across tasks. First, we learn the shift between the source domain and the target domain from an irrelevant task for which sufficient data samples from both domains are available. Then, we transfer the domain shift to the task of interest under the hypothesis that different tasks may share the domain shift for a specified pair of domains. Via this strategy, we can learn a model for the unseen target domain of the task of interest. Our method uses two coupled generative adversarial networks (CoGANs) to capture the joint distribution of data samples in dual-domains and another generative adversarial network (GAN) to explicitly model the domain shift. The experimental results on image classification and semantic segmentation demonstrate the satisfactory performance of our method in transferring various kinds of domain shifts across tasks.},
  archive      = {J_TIP},
  author       = {Jinghua Wang and Ming-Ming Cheng and Jianmin Jiang},
  doi          = {10.1109/TIP.2021.3084354},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5505-5517},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Domain shift preservation for zero-shot domain adaptation},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-view mouse social behaviour recognition with deep
graphic model. <em>TIP</em>, <em>30</em>, 5490–5504. (<a
href="https://doi.org/10.1109/TIP.2021.3083079">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Home-cage social behaviour analysis of mice is an invaluable tool to assess therapeutic efficacy of neurodegenerative diseases. Despite tremendous efforts made within the research community, single-camera video recordings are mainly used for such analysis. Because of the potential to create rich descriptions for mouse social behaviors, the use of multi-view video recordings for rodent observations is increasingly receiving much attention. However, identifying social behaviours from various views is still challenging due to the lack of correspondence across data sources. To address this problem, we here propose a novel multi-view latent-attention and dynamic discriminative model that jointly learns view-specific and view-shared sub-structures, where the former captures unique dynamics of each view whilst the latter encodes the interaction between the views. Furthermore, a novel multi-view latent-attention variational autoencoder model is introduced in learning the acquired features, enabling us to learn discriminative features in each view. Experimental results on the standard CRMI13 and our multi-view Parkinson&#39;s Disease Mouse Behaviour (PDMB) datasets demonstrate that our proposed model outperforms the other state of the arts technologies, has lower computational cost than the other graphical models and effectively deals with the imbalanced data problem.},
  archive      = {J_TIP},
  author       = {Zheheng Jiang and Feixiang Zhou and Aite Zhao and Xin Li and Ling Li and Dacheng Tao and Xuelong Li and Huiyu Zhou},
  doi          = {10.1109/TIP.2021.3083079},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5490-5504},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Multi-view mouse social behaviour recognition with deep graphic model},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptive spatio-temporal graph enhanced vision-language
representation for video QA. <em>TIP</em>, <em>30</em>, 5477–5489. (<a
href="https://doi.org/10.1109/TIP.2021.3076556">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vision-language research has become very popular, which focuses on understanding of visual contents, language semantics and relationships between them. Video question answering (Video QA) is one of the typical tasks. Recently, several BERT style pre-training methods have been proposed and shown effectiveness on various vision-language tasks. In this work, we leverage the successful vision-language transformer structure to solve the Video QA problem. However, we do not pre-train it with any video data, because video pre-training requires massive computing resources and is hard to perform with only a few GPUs. Instead, our work aims to leverage image-language pre-training to help with video-language modeling, by sharing a common module design. We further introduce an adaptive spatio-temporal graph to enhance the vision-language representation learning. That is, we adaptively refine the spatio-temporal tubes of salient objects according to their spatio-temporal relations learned through a hierarchical graph convolution process. Finally, we can obtain a number of fine-grained tube-level video object representations, as the visual inputs of the vision-language transformer module. Experiments on three widely used Video QA datasets show that our model achieves the new state-of-the-art results.},
  archive      = {J_TIP},
  author       = {Weike Jin and Zhou Zhao and Xiaochun Cao and Jieming Zhu and Xiuqiang He and Yueting Zhuang},
  doi          = {10.1109/TIP.2021.3076556},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5477-5489},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Adaptive spatio-temporal graph enhanced vision-language representation for video QA},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). No-reference screen content image quality assessment with
unsupervised domain adaptation. <em>TIP</em>, <em>30</em>, 5463–5476.
(<a href="https://doi.org/10.1109/TIP.2021.3084750">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we quest the capability of transferring the quality of natural scene images to the images that are not acquired by optical cameras (e.g., screen content images, SCIs), rooted in the widely accepted view that the human visual system has adapted and evolved through the perception of natural environment. Here, we develop the first unsupervised domain adaptation based no reference quality assessment method for SCIs, leveraging rich subjective ratings of the natural images (NIs). In general, it is a non-trivial task to directly transfer the quality prediction model from NIs to a new type of content (i.e., SCIs) that holds dramatically different statistical characteristics. Inspired by the transferability of pair-wise relationship, the proposed quality measure operates based on the philosophy of improving the transferability and discriminability simultaneously. In particular, we introduce three types of losses which complementarily and explicitly regularize the feature space of ranking in a progressive manner. Regarding feature discriminatory capability enhancement, we propose a center based loss to rectify the classifier and improve its prediction capability not only for source domain (NI) but also the target domain (SCI). For feature discrepancy minimization, the maximum mean discrepancy (MMD) is imposed on the extracted ranking features of NIs and SCIs. Furthermore, to further enhance the feature diversity, we introduce the correlation penalization between different feature dimensions, leading to the features with lower rank and higher diversity. Experiments show that our method can achieve higher performance on different source-target settings based on a light-weight convolution neural network. The proposed method also sheds light on learning quality assessment measures for unseen application-specific content without the cumbersome and costing subjective evaluations.},
  archive      = {J_TIP},
  author       = {Baoliang Chen and Haoliang Li and Hongfei Fan and Shiqi Wang},
  doi          = {10.1109/TIP.2021.3084750},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5463-5476},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {No-reference screen content image quality assessment with unsupervised domain adaptation},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Pixel-wise wasserstein autoencoder for highly generative
dehazing. <em>TIP</em>, <em>30</em>, 5452–5462. (<a
href="https://doi.org/10.1109/TIP.2021.3084743">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a highly generative dehazing method based on pixel-wise Wasserstein autoencoders. In contrast to existing dehazing methods based on generative adversarial networks, our method can produce a variety of dehazed images with different styles. It significantly improves the dehazing accuracy via pixel-wise matching from hazy to dehazed images through 2-dimensional latent tensors of the Wasserstein autoencoder. In addition, we present an advanced feature fusion technique to deliver rich information to the latent space. For style transfer, we introduce a mapping function that transforms existing latent spaces to new ones. Thus, our method can produce highly generative haze-free images with various tones, illuminations, and moods, which induces several interesting applications, including low-light enhancement, daytime dehazing, nighttime dehazing, and underwater image enhancement. Experimental results demonstrate that our method quantitatively outperforms existing state-of-the-art methods for synthetic and real-world datasets, and simultaneously generates highly generative haze-free images, which are qualitatively diverse.},
  archive      = {J_TIP},
  author       = {Guisik Kim and Sung Woo Park and Junseok Kwon},
  doi          = {10.1109/TIP.2021.3084743},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5452-5462},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Pixel-wise wasserstein autoencoder for highly generative dehazing},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptive deep reinforcement learning-based in-loop filter
for VVC. <em>TIP</em>, <em>30</em>, 5439–5451. (<a
href="https://doi.org/10.1109/TIP.2021.3084345">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning-based in-loop filters have recently demonstrated great improvement for both coding efficiency and subjective quality in video coding. However, most existing deep learning-based in-loop filters tend to develop a sophisticated model in exchange for good performance, and they employ a single network structure to all reconstructed samples, which lack sufficient adaptiveness to the various video content, limiting their performances to some extent. In contrast, this paper proposes an adaptive deep reinforcement learning-based in-loop filter (ARLF) for versatile video coding (VVC). Specifically, we treat the filtering as a decision-making process and employ an agent to select an appropriate network by leveraging recent advances in deep reinforcement learning. To this end, we develop a lightweight backbone and utilize it to design a network set $\mathcal {S}$ containing networks with different complexities. Then a simple but efficient agent network is designed to predict the optimal network from $\mathcal {S}$ , which makes the model adaptive to various video contents. To improve the robustness of our model, a two-stage training scheme is further proposed to train the agent and tune the network set. The coding tree unit (CTU) is seen as the basic unit for the in-loop filtering processing. A CTU level control flag is applied in the sense of rate-distortion optimization (RDO). Extensive experimental results show that our ARLF approach obtains on average 2.17\%, 2.65\%, 2.58\%, 2.51\% under all-intra, low-delay P, low-delay, and random access configurations, respectively. Compared with other deep learning-based methods, the proposed approach can achieve better performance with low computation complexity.},
  archive      = {J_TIP},
  author       = {Zhijie Huang and Jun Sun and Xiaopeng Guo and Mingyu Shang},
  doi          = {10.1109/TIP.2021.3084345},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5439-5451},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Adaptive deep reinforcement learning-based in-loop filter for VVC},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Defocus blur detection via boosting diversity of deep
ensemble networks. <em>TIP</em>, <em>30</em>, 5426–5438. (<a
href="https://doi.org/10.1109/TIP.2021.3084101">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing defocus blur detection (DBD) methods usually explore multi-scale and multi-level features to improve performance. However, defocus blur regions normally have incomplete semantic information, which will reduce DBD&#39;s performance if it can&#39;t be used properly. In this paper, we address the above problem by exploring deep ensemble networks, where we boost diversity of defocus blur detectors to force the network to generate diverse results that some rely more on high-level semantic information while some ones rely more on low-level information. Then, diverse result ensemble makes detection errors cancel out each other. Specifically, we propose two deep ensemble networks (e.g., adaptive ensemble network (AENet) and encoder-feature ensemble network (EFENet)), which focus on boosting diversity while costing less computation. AENet constructs different light-weight sequential adapters for one backbone network to generate diverse results without introducing too many parameters and computation. AENet is optimized only by the self- negative correlation loss. On the other hand, we propose EFENet by exploring the diversity of multiple encoded features and ensemble strategies of features (e.g., group-channel uniformly weighted average ensemble and self-gate weighted ensemble). Diversity is represented by encoded features with less parameters, and a simple mean squared error loss can achieve the superior performance. Experimental results demonstrate the superiority over the state-of-the-arts in terms of accuracy and speed. Codes and models are available at: https://github.com/wdzhao123/DENets.},
  archive      = {J_TIP},
  author       = {Wenda Zhao and Xueqing Hou and You He and Huchuan Lu},
  doi          = {10.1109/TIP.2021.3084101},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5426-5438},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Defocus blur detection via boosting diversity of deep ensemble networks},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Age-oriented face synthesis with conditional discriminator
pool and adversarial triplet loss. <em>TIP</em>, <em>30</em>, 5413–5425.
(<a href="https://doi.org/10.1109/TIP.2021.3084106">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The vanilla Generative Adversarial Networks (GANs) are commonly used to generate realistic images depicting aged and rejuvenated faces. However, the performance of such vanilla GANs in the age-oriented face synthesis task is often compromised by the mode collapse issue, which may produce poorly synthesized faces with indistinguishable visual variations. In addition, recent age-oriented face synthesis methods use the L1 or L2 constraint to preserve the identity information in synthesized faces, which implicitly limits the identity permanence capabilities when these constraints are associated with a trivial weighting factor. In this paper, we propose a method for the age-oriented face synthesis task that achieves high synthesis accuracy with strong identity permanence capabilities. Specifically, to achieve high synthesis accuracy, our method tackles the mode collapse issue with a novel Conditional Discriminator Pool, which consists of multiple discriminators, each targeting one particular age category. To achieve strong identity permanence capabilities, our method uses a novel Adversarial Triplet loss. This loss, which is based on the Triplet loss, adds a ranking operation to further pull the positive embedding towards the anchor embedding to significantly reduce intra-class variances in the feature space. Through extensive experiments, we show that our proposed method outperforms state-of-the-art methods in terms of synthesis accuracy and identity permanence capabilities, both qualitatively and quantitatively.},
  archive      = {J_TIP},
  author       = {Haoyi Wang and Victor Sanchez and Chang-Tsun Li},
  doi          = {10.1109/TIP.2021.3084106},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5413-5425},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Age-oriented face synthesis with conditional discriminator pool and adversarial triplet loss},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A contour co-tracking method for image pairs. <em>TIP</em>,
<em>30</em>, 5402–5412. (<a
href="https://doi.org/10.1109/TIP.2021.3079798">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We proposed a contour co-tracking method for co-segmentation of image pairs based on active contour model. Our method comprehensively re-models objects and backgrounds signified by level set functions, and leverages Hellinger distance to measure the similarity between image regions encoded by probability distributions. The main contribution are as follows. 1) The new energy functional, combining a rewarding and a penalty term, relaxes the assumptions of co-segmentation methods. 2) Hellinger distance, fulfilling the triangle inequality, ensures a coherence measurement between probability distributions in metric space, and contributes to finding a unique solution to the energy functional. The proposed contour co-tracking method was carefully verified against five representative methods on four popular datasets, i.e. , the images pair dataset (105 pairs), MSRC dataset (30 pairs), iCoseg dataset (66 pairs) and Coseg-rep dataset (25 pairs). The comparison experiments suggest that our method achieves the competitive and even better performance compared to the state-of-the-art co-segmentation methods.},
  archive      = {J_TIP},
  author       = {Bin Wang and Dapeng Tao and Rui Dong and Yuanyan Tang and Xinbo Gao},
  doi          = {10.1109/TIP.2021.3079798},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5402-5412},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A contour co-tracking method for image pairs},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A context-based image contrast enhancement using energy
equalization with clipping limit. <em>TIP</em>, <em>30</em>, 5391–5401.
(<a href="https://doi.org/10.1109/TIP.2021.3083448">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a new context-based image contrast enhancement process using energy curve equalization (ECE) with a clipping limit has been proposed. In a fundamental anomaly to the existing contrast enhancement practice using histogram equalization, the projected method uses the energy curve. The computation of the energy curve utilizes a modified Hopfield neural network architecture. This process embraces the image&#39;s spatial adjacency information to the energy curve. For each intensity level, the energy value is calculated and the overall energy curve appears to be smoother than the histogram. A clipping limit applies to evade the over enhancement and is chosen as the average of the mean and median value. The clipped energy curve is subdivided into three regions based on the standard deviation value. Each part of the subdivided energy curve is equalized individually, and the final enhanced image is produced by combining transfer functions computed by the equalization process. The projected scheme&#39;s qualitative and quantitative efficiency is assessed by comparing it with the conventional histogram equalization techniques with and without the clipping limit.},
  archive      = {J_TIP},
  author       = {Kankanala Srinivas and Ashish Kumar Bhandari and Puli Kishore Kumar},
  doi          = {10.1109/TIP.2021.3083448},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5391-5401},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A context-based image contrast enhancement using energy equalization with clipping limit},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). DeepQTMT: A deep learning approach for fast QTMT-based CU
partition of intra-mode VVC. <em>TIP</em>, <em>30</em>, 5377–5390. (<a
href="https://doi.org/10.1109/TIP.2021.3083447">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Versatile Video Coding (VVC), as the latest standard, significantly improves the coding efficiency over its predecessor standard High Efficiency Video Coding (HEVC), but at the expense of sharply increased complexity. In VVC, the quad-tree plus multi-type tree (QTMT) structure of the coding unit (CU) partition accounts for over 97\% of the encoding time, due to the brute-force search for recursive rate-distortion (RD) optimization. Instead of the brute-force QTMT search, this paper proposes a deep learning approach to predict the QTMT-based CU partition, for drastically accelerating the encoding process of intra-mode VVC. First, we establish a large-scale database containing sufficient CU partition patterns with diverse video content, which can facilitate the data-driven VVC complexity reduction. Next, we propose a multi-stage exit CNN (MSE-CNN) model with an early-exit mechanism to determine the CU partition, in accord with the flexible QTMT structure at multiple stages. Then, we design an adaptive loss function for training the MSE-CNN model, synthesizing both the uncertain number of split modes and the target on minimized RD cost. Finally, a multi-threshold decision scheme is developed, achieving a desirable trade-off between complexity and RD performance. The experimental results demonstrate that our approach can reduce the encoding time of VVC by 44.65\%~66.88\% with a negligible Bjøntegaard delta bit-rate (BD-BR) of 1.322\%~3.188\%, significantly outperforming other state-of-the-art approaches.},
  archive      = {J_TIP},
  author       = {Tianyi Li and Mai Xu and Runzhi Tang and Ying Chen and Qunliang Xing},
  doi          = {10.1109/TIP.2021.3083447},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5377-5390},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {DeepQTMT: A deep learning approach for fast QTMT-based CU partition of intra-mode VVC},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Double similarity distillation for semantic image
segmentation. <em>TIP</em>, <em>30</em>, 5363–5376. (<a
href="https://doi.org/10.1109/TIP.2021.3083113">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The balance between high accuracy and high speed has always been a challenging task in semantic image segmentation. Compact segmentation networks are more widely used in the case of limited resources, while their performances are constrained. In this paper, motivated by the residual learning and global aggregation, we propose a simple yet general and effective knowledge distillation framework called double similarity distillation (DSD) to improve the classification accuracy of all existing compact networks by capturing the similarity knowledge in pixel and category dimensions, respectively. Specifically, we propose a pixel-wise similarity distillation (PSD) module that utilizes residual attention maps to capture more detailed spatial dependencies across multiple layers. Compared with exiting methods, the PSD module greatly reduces the amount of calculation and is easy to expand. Furthermore, considering the differences in characteristics between semantic segmentation task and other computer vision tasks, we propose a category-wise similarity distillation (CSD) module, which can help the compact segmentation network strengthen the global category correlation by constructing the correlation matrix. Combining these two modules, DSD framework has no extra parameters and only a minimal increase in FLOPs. Extensive experiments on four challenging datasets, including Cityscapes, CamVid, ADE20K, and Pascal VOC 2012, show that DSD outperforms current state-of-the-art methods, proving its effectiveness and generality. The code and models will be publicly available.},
  archive      = {J_TIP},
  author       = {Yingchao Feng and Xian Sun and Wenhui Diao and Jihao Li and Xin Gao},
  doi          = {10.1109/TIP.2021.3083113},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5363-5376},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Double similarity distillation for semantic image segmentation},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep spectral representation learning from multi-view data.
<em>TIP</em>, <em>30</em>, 5352–5362. (<a
href="https://doi.org/10.1109/TIP.2021.3083072">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view representation learning (MvRL) aims to learn a consensus representation from diverse sources or domains to facilitate downstream tasks such as clustering, retrieval, and classification. Due to the limited representative capacity of the adopted shallow models, most existing MvRL methods may yield unsatisfactory results, especially when the labels of data are unavailable. To enjoy the representative capacity of deep learning, this paper proposes a novel multi-view unsupervised representation learning method, termed as Multi-view Laplacian Network (MvLNet), which could be the first deep version of the multi-view spectral representation learning method. Note that, such an attempt is nontrivial because simply combining Laplacian embedding (i.e., spectral representation) with neural networks will lead to trivial solutions. To solve this problem, MvLNet enforces an orthogonal constraint and reformulates it as a layer with the help of Cholesky decomposition. The orthogonal layer is stacked on the embedding network so that a common space could be learned for consensus representation. Compared with numerous recent-proposed approaches, extensive experiments on seven challenging datasets demonstrate the effectiveness of our method in three multi-view tasks including clustering, recognition, and retrieval. The source code could be found at www.pengxi.me.},
  archive      = {J_TIP},
  author       = {Zhenyu Huang and Joey Tianyi Zhou and Hongyuan Zhu and Changqing Zhang and Jiancheng Lv and Xi Peng},
  doi          = {10.1109/TIP.2021.3083072},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5352-5362},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Deep spectral representation learning from multi-view data},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Graph regularized flow attention network for video animal
counting from drones. <em>TIP</em>, <em>30</em>, 5339–5351. (<a
href="https://doi.org/10.1109/TIP.2021.3082297">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a large-scale video based animal counting dataset collected by drones (AnimalDrone) for agriculture and wildlife protection. The dataset consists of two subsets, i.e., PartA captured on site by drones and PartB collected from the Internet, with rich annotations of more than 4 million objects in 53, 644 frames and corresponding attributes in terms of density, altitude and view. Moreover, we develop a new graph regularized flow attention network (GFAN) to perform density map estimation in dense crowds of video clips with arbitrary crowd density, perspective, and flight altitude. Specifically, our GFAN method leverages optical flow to warp the multi-scale feature maps in sequential frames to exploit the temporal relations, and then combines the enhanced features to predict the density maps. Moreover, we introduce the multi-granularity loss function including pixel-wise density loss and region-wise count loss to enforce the network to concentrate on discriminative features for different scales of objects. Meanwhile, the graph regularizer is imposed on the density maps of multiple consecutive frames to maintain temporal coherency. Extensive experiments are conducted to demonstrate the effectiveness of the proposed method, compared with several state-of-the-art counting algorithms. The AnimalDrone dataset is available at https://github.com/VisDrone/AnimalDrone.},
  archive      = {J_TIP},
  author       = {Pengfei Zhu and Tao Peng and Dawei Du and Hongtao Yu and Libo Zhang and Qinghua Hu},
  doi          = {10.1109/TIP.2021.3082297},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5339-5351},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Graph regularized flow attention network for video animal counting from drones},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-scale representation learning on hypergraph for 3D
shape retrieval and recognition. <em>TIP</em>, <em>30</em>, 5327–5338.
(<a href="https://doi.org/10.1109/TIP.2021.3082765">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Effective 3D shape retrieval and recognition are challenging but important tasks in computer vision research field, which have attracted much attention in recent decades. Although recent progress has shown significant improvement of deep learning methods on 3D shape retrieval and recognition performance, it is still under investigated of how to jointly learn an optimal representation of 3D shapes considering their relationships. To tackle this issue, we propose a multi-scale representation learning method on hypergraph for 3D shape retrieval and recognition, called multi-scale hypergraph neural network (MHGNN). In this method, the correlation among 3D shapes is formulated in a hypergraph and a hypergraph convolution process is conducted to learn the representations. Here, multiple representations can be obtained through different convolution layers, leading to multi-scale representations of 3D shapes. A fusion module is then introduced to combine these representations for 3D shape retrieval and recognition. The main advantages of our method lie in 1) the high-order correlation among 3D shapes can be investigated in the framework and 2) the joint multi-scale representation can be more robust for comparison. Comparisons with state-of-the-art methods on the public ModelNet40 dataset demonstrate remarkable performance improvement of our proposed method on the 3D shape retrieval task. Meanwhile, experiments on recognition tasks also show better results of our proposed method, which indicate the superiority of our method on learning better representation for retrieval and recognition.},
  archive      = {J_TIP},
  author       = {Junjie Bai and Biao Gong and Yining Zhao and Fuqiang Lei and Chenggang Yan and Yue Gao},
  doi          = {10.1109/TIP.2021.3082765},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5327-5338},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Multi-scale representation learning on hypergraph for 3D shape retrieval and recognition},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Structure-coherent deep feature learning for robust face
alignment. <em>TIP</em>, <em>30</em>, 5313–5326. (<a
href="https://doi.org/10.1109/TIP.2021.3082319">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a structure-coherent deep feature learning method for face alignment. Unlike most existing face alignment methods which overlook the facial structure cues, we explicitly exploit the relation among facial landmarks to make the detector robust to hard cases such as occlusion and large pose. Specifically, we leverage a landmark-graph relational network to enforce the structural relationships among landmarks. We consider the facial landmarks as structural graph nodes and carefully design the neighborhood to passing features among the most related nodes. Our method dynamically adapts the weights of node neighborhood to eliminate distracted information from noisy nodes, such as occluded landmark point. Moreover, different from most previous works which only tend to penalize the landmarks absolute position during the training, we propose a relative location loss to enhance the information of relative location of landmarks. This relative location supervision further regularizes the facial structure. Our approach considers the interactions among facial landmarks and can be easily implemented on top of any convolutional backbone to boost the performance. Extensive experiments on three popular benchmarks, including WFLW, COFW and 300W, demonstrate the effectiveness of the proposed method. In particular, due to explicit structure modeling, our approach is especially robust to challenging cases resulting in impressive low failure rate on COFW and WFLW datasets. The model and code are publicly available at https://github.com/BeierZhu/Sturcture-Coherency-Face-Alignment.},
  archive      = {J_TIP},
  author       = {Chunze Lin and Beier Zhu and Quan Wang and Renjie Liao and Chen Qian and Jiwen Lu and Jie Zhou},
  doi          = {10.1109/TIP.2021.3082319},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5313-5326},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Structure-coherent deep feature learning for robust face alignment},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-view 3D shape recognition via correspondence-aware
deep learning. <em>TIP</em>, <em>30</em>, 5299–5312. (<a
href="https://doi.org/10.1109/TIP.2021.3082310">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, multi-view learning has emerged as a promising approach for 3D shape recognition, which identifies a 3D shape based on its 2D views taken from different viewpoints. Usually, the correspondences inside a view or across different views encode the spatial arrangement of object parts and the symmetry of the object, which provide useful geometric cues for recognition. However, such view correspondences have not been explicitly and fully exploited in existing work. In this paper, we propose a correspondence-aware representation (CAR) module, which explicitly finds potential intra-view correspondences and cross-view correspondences via $k$ NN search in semantic space and then aggregates the shape features from the correspondences via learned transforms. Particularly, the spatial relations of correspondences in terms of their viewpoint positions and intra-view locations are taken into account for learning correspondence-aware features. Incorporating the CAR module into a ResNet-18 backbone, we propose an effective deep model called CAR-Net for 3D shape classification and retrieval. Extensive experiments have demonstrated the effectiveness of the CAR module as well as the excellent performance of the CAR-Net.},
  archive      = {J_TIP},
  author       = {Yong Xu and Chaoda Zheng and Ruotao Xu and Yuhui Quan and Haibin Ling},
  doi          = {10.1109/TIP.2021.3082310},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5299-5312},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Multi-view 3D shape recognition via correspondence-aware deep learning},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Self-training with progressive representation enhancement
for unsupervised cross-domain person re-identification. <em>TIP</em>,
<em>30</em>, 5287–5298. (<a
href="https://doi.org/10.1109/TIP.2021.3082298">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, person re-identification (re-ID) has achieved relatively good performance, benefiting from the revival of deep neural networks. However, due to the existence of domain bias which refers to the different data distributions between two domains, it remains challenging to directly deploy a model trained on a labeled source domain to a target domain only with unlabeled data available. In this paper, a Self-Training with Progressive Representation Enhancement (PREST) framework, which comprises a multi-scale self-training method and a view-invariant representation learning module, is proposed to promote re-ID performance on the target domain in an unsupervised manner. More specifically, multi-scale representations, including the global body and local parts of pedestrian images, are utilized to obtain pseudo-labels. Then, some images are selected according to the pseudo-labels to create a new dataset for supervising the fine-tuning process, which is operated iteratively to progressively promote the performance. Furthermore, to mitigate the influence of different styles among sub-domains, in cases where a single sub-domain is captured by one camera, a classifier with a gradient reverse layer is first employed to learn view-invariant representation for pedestrian images with the same identity taken by different cameras; this can further enhance the reliability of the predicted labels and improve the cross-domain re-ID performance. Extensive experiments on three large-scale re-ID datasets demonstrate that our framework achieves significantly better performance than existing approaches.},
  archive      = {J_TIP},
  author       = {Hang Zhang and Huanhuan Cao and Xu Yang and Cheng Deng and Dacheng Tao},
  doi          = {10.1109/TIP.2021.3082298},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5287-5298},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Self-training with progressive representation enhancement for unsupervised cross-domain person re-identification},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Non-greedy l21-norm maximization for principal component
analysis. <em>TIP</em>, <em>30</em>, 5277–5286. (<a
href="https://doi.org/10.1109/TIP.2021.3073282">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Principal Component Analysis (PCA) is one of the most important unsupervised methods to handle high-dimensional data. However, due to the high computational complexity of its eigen-decomposition solution, it is hard to apply PCA to the large-scale data with high dimensionality, e.g., millions of data points with millions of variables. Meanwhile, the squared L2-norm based objective makes it sensitive to data outliers. In recent research, the L1-norm maximization based PCA method was proposed for efficient computation and being robust to outliers. However, this work used a greedy strategy to solve the eigenvectors. Moreover, the L1-norm maximization based objective may not be the correct robust PCA formulation, because it loses the theoretical connection to the minimization of data reconstruction error, which is one of the most important intuitions and goals of PCA. In this paper, we propose to maximize the L21-norm based robust PCA objective, which is theoretically connected to the minimization of reconstruction error. More importantly, we propose the efficient non-greedy optimization algorithms to solve our objective and the more general L21-norm maximization problem with theoretically guaranteed convergence. Experimental results on real world data sets show the effectiveness of the proposed method for principal component analysis.},
  archive      = {J_TIP},
  author       = {Feiping Nie and Lai Tian and Heng Huang and Chris Ding},
  doi          = {10.1109/TIP.2021.3073282},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5277-5286},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Non-greedy l21-norm maximization for principal component analysis},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptive context-aware multi-modal network for depth
completion. <em>TIP</em>, <em>30</em>, 5264–5276. (<a
href="https://doi.org/10.1109/TIP.2021.3079821">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Depth completion aims to recover a dense depth map from the sparse depth data and the corresponding single RGB image. The observed pixels provide the significant guidance for the recovery of the unobserved pixels&#39; depth. However, due to the sparsity of the depth data, the standard convolution operation, exploited by most of existing methods, is not effective to model the observed contexts with depth values. To address this issue, we propose to adopt the graph propagation to capture the observed spatial contexts. Specifically, we first construct multiple graphs at different scales from observed pixels. Since the graph structure varies from sample to sample, we then apply the attention mechanism on the propagation, which encourages the network to model the contextual information adaptively. Furthermore, considering the mutli-modality of input data, we exploit the graph propagation on the two modalities respectively to extract multi-modal representations. Finally, we introduce the symmetric gated fusion strategy to exploit the extracted multi-modal features effectively. The proposed strategy preserves the original information for one modality and also absorbs complementary information from the other through learning the adaptive gating weights. Our model, named Adaptive Context-Aware Multi-Modal Network (ACMNet), achieves the state-of-the-art performance on two benchmarks, i.e., KITTI and NYU-v2, and at the same time has fewer parameters than latest models. Our code is available at: https://github.com/sshan-zhao/ACMNet.},
  archive      = {J_TIP},
  author       = {Shanshan Zhao and Mingming Gong and Huan Fu and Dacheng Tao},
  doi          = {10.1109/TIP.2021.3079821},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5264-5276},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Adaptive context-aware multi-modal network for depth completion},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Pseudo-supervised deep subspace clustering. <em>TIP</em>,
<em>30</em>, 5252–5263. (<a
href="https://doi.org/10.1109/TIP.2021.3079800">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Auto-Encoder (AE)-based deep subspace clustering (DSC) methods have achieved impressive performance due to the powerful representation extracted using deep neural networks while prioritizing categorical separability. However, self-reconstruction loss of an AE ignores rich useful relation information and might lead to indiscriminative representation, which inevitably degrades the clustering performance. It is also challenging to learn high-level similarity without feeding semantic labels. Another unsolved problem facing DSC is the huge memory cost due to n×n similarity matrix, which is incurred by the self-expression layer between an encoder and decoder. To tackle these problems, we use pairwise similarity to weigh the reconstruction loss to capture local structure information, while a similarity is learned by the self-expression layer. Pseudo-graphs and pseudo-labels, which allow benefiting from uncertain knowledge acquired during network training, are further employed to supervise similarity learning. Joint learning and iterative training facilitate to obtain an overall optimal solution. Extensive experiments on benchmark datasets demonstrate the superiority of our approach. By combining with the k-nearest neighbors algorithm, we further show that our method can address the large-scale and out-of-sample problems. The source code of our method is available: https://github.com/sckangz/SelfsupervisedSC.},
  archive      = {J_TIP},
  author       = {Juncheng Lv and Zhao Kang and Xiao Lu and Zenglin Xu},
  doi          = {10.1109/TIP.2021.3079800},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5252-5263},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Pseudo-supervised deep subspace clustering},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Image-guided human reconstruction via multi-scale graph
transformation networks. <em>TIP</em>, <em>30</em>, 5239–5251. (<a
href="https://doi.org/10.1109/TIP.2021.3080177">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D human reconstruction from a single image is a challenging problem. Existing methods have difficulties to infer 3D clothed human models with consistent topologies for various poses. In this paper, we propose an efficient and effective method using a hierarchical graph transformation network. To deal with large deformations and avoid distorted geometries, rather than using Euclidean coordinates directly, 3D human shapes are represented by a vertex-based deformation representation that effectively encodes the deformation and copes well with large deformations. To infer a 3D human mesh consistent with the input real image, we also use a perspective projection layer to incorporate perceptual image features into the deformation representation. Our model is easy to train and fast to converge with short test time. Besides, we present the $D^{2}Human$ (Dynamic Detailed Human) dataset, including variously posed 3D human meshes with consistent topologies and rich geometry details, together with the captured color images and SMPL models, which is useful for training and evaluation of deep frameworks, particularly for graph neural networks. Experimental results demonstrate that our method achieves more plausible and complete 3D human reconstruction from a single image, compared with several state-of-the-art methods. The code and dataset are available for research purposes at http://cic.tju.edu.cn/faculty/likun/projects/MGTnet .},
  archive      = {J_TIP},
  author       = {Kun Li and Hao Wen and Qiao Feng and Yuxiang Zhang and Xiongzheng Li and Jing Huang and Cunkuan Yuan and Yu-Kun Lai and Yebin Liu},
  doi          = {10.1109/TIP.2021.3080177},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5239-5251},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Image-guided human reconstruction via multi-scale graph transformation networks},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Image restoration via reconciliation of group sparsity and
low-rank models. <em>TIP</em>, <em>30</em>, 5223–5238. (<a
href="https://doi.org/10.1109/TIP.2021.3078329">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image nonlocal self-similarity (NSS) property has been widely exploited via various sparsity models such as joint sparsity (JS) and group sparse coding (GSC). However, the existing NSS-based sparsity models are either too restrictive, e.g., JS enforces the sparse codes to share the same support, or too general, e.g., GSC imposes only plain sparsity on the group coefficients, which limit their effectiveness for modeling real images. In this paper, we propose a novel NSS-based sparsity model, namely, low-rank regularized group sparse coding (LR-GSC), to bridge the gap between the popular GSC and JS. The proposed LR-GSC model simultaneously exploits the sparsity and low-rankness of the dictionary-domain coefficients for each group of similar patches. An alternating minimization with an adaptive adjusted parameter strategy is developed to solve the proposed optimization problem for different image restoration tasks, including image denoising, image deblocking, image inpainting, and image compressive sensing. Extensive experimental results demonstrate that the proposed LR-GSC algorithm outperforms many popular or state-of-the-art methods in terms of objective and perceptual metrics.},
  archive      = {J_TIP},
  author       = {Zhiyuan Zha and Bihan Wen and Xin Yuan and Jiantao Zhou and Ce Zhu},
  doi          = {10.1109/TIP.2021.3078329},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5223-5238},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Image restoration via reconciliation of group sparsity and low-rank models},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Single image dehazing via dual-path recurrent network.
<em>TIP</em>, <em>30</em>, 5211–5222. (<a
href="https://doi.org/10.1109/TIP.2021.3078319">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An image can be decomposed into two parts: the basic content and details, which usually correspond to the low-frequency and high-frequency information of the image. For a hazy image, these two parts are often affected by haze in different levels, e.g., high-frequency parts are often affected more serious than low-frequency parts. In this paper, we approach the single image dehazing problem as two restoration problems of recovering basic content and image details, and propose a Dual-Path Recurrent Network (DPRN) to simultaneously tackle these two problems. Specifically, the core structure of DPRN is a dual-path block, which uses two parallel branches to learn the characteristics of the basic content and details of hazy images. Each branch consists of several Convolutional LSTM blocks and convolution layers. Moreover, a parallel interaction function is incorporated into the dual-path block, thus enables each branch to dynamically fuse the intermediate features of both the basic content and image details. In this way, both branches can benefit from each other, and recover the basic content and image details alternately, therefore alleviating the color distortion problem in the dehazing process. Experimental results show that the proposed DPRN outperforms state-of-the-art image dehazing methods in terms of both quantitative accuracy and qualitative visual effect.},
  archive      = {J_TIP},
  author       = {Xiaoqin Zhang and Runhua Jiang and Tao Wang and Wenhan Luo},
  doi          = {10.1109/TIP.2021.3078319},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5211-5222},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Single image dehazing via dual-path recurrent network},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-target multi-camera tracking of vehicles using
metadata-aided re-ID and trajectory-based camera link model.
<em>TIP</em>, <em>30</em>, 5198–5210. (<a
href="https://doi.org/10.1109/TIP.2021.3078124">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel framework for multi-target multi-camera tracking (MTMCT) of vehicles based on metadata-aided re-identification (MA-ReID) and the trajectory-based camera link model (TCLM). Given a video sequence and the corresponding frame-by-frame vehicle detections, we first address the isolated tracklets issue from single camera tracking (SCT) by the proposed traffic-aware single-camera tracking (TSCT). Then, after automatically constructing the TCLM, we solve MTMCT by the MA-ReID. The TCLM is generated from camera topological configuration to obtain the spatial and temporal information to improve the performance of MTMCT by reducing the candidate search of ReID. We also use the temporal attention model to create more discriminative embeddings of trajectories from each camera to achieve robust distance measures for vehicle ReID. Moreover, we train a metadata classifier for MTMCT to obtain the metadata feature, which is concatenated with the temporal attention based embeddings. Finally, the TCLM and hierarchical clustering are jointly applied for global ID assignment. The proposed method is evaluated on the CityFlow dataset, achieving IDF1 76.77\%, which outperforms the state-of-the-art MTMCT methods.},
  archive      = {J_TIP},
  author       = {Hung-Min Hsu and Jiarui Cai and Yizhou Wang and Jenq-Neng Hwang and Kwang-Ju Kim},
  doi          = {10.1109/TIP.2021.3078124},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5198-5210},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Multi-target multi-camera tracking of vehicles using metadata-aided re-ID and trajectory-based camera link model},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Towards perceptually optimized adaptive video streaming-a
realistic quality of experience database. <em>TIP</em>, <em>30</em>,
5182–5197. (<a href="https://doi.org/10.1109/TIP.2021.3073294">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Measuring Quality of Experience (QoE) and integrating these measurements into video streaming algorithms is a multi-faceted problem that fundamentally requires the design of comprehensive subjective QoE databases and objective QoE prediction models. To achieve this goal, we have recently designed the LIVE-NFLX-II database, a highly-realistic database which contains subjective QoE responses to various design dimensions, such as bitrate adaptation algorithms, network conditions and video content. Our database builds on recent advancements in content-adaptive encoding and incorporates actual network traces to capture realistic network variations on the client device. The new database focuses on low bandwidth conditions which are more challenging for bitrate adaptation algorithms, which often must navigate tradeoffs between rebuffering and video quality. Using our database, we study the effects of multiple streaming dimensions on user experience and evaluate video quality and quality of experience models and analyze their strengths and weaknesses. We believe that the tools introduced here will help inspire further progress on the development of perceptually-optimized client adaptation and video streaming strategies. The database is publicly available at http://live.ece.utexas.edu/research/LIVE_NFLX_II/live_nflx_plus.html.},
  archive      = {J_TIP},
  author       = {Christos G. Bampis and Zhi Li and Ioannis Katsavounidis and Te-Yuan Huang and Chaitanya Ekanadham and Alan C. Bovik},
  doi          = {10.1109/TIP.2021.3073294},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5182-5197},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Towards perceptually optimized adaptive video streaming-A realistic quality of experience database},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hierarchical attention learning of scene flow in 3D point
clouds. <em>TIP</em>, <em>30</em>, 5168–5181. (<a
href="https://doi.org/10.1109/TIP.2021.3079796">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scene flow represents the 3D motion of every point in the dynamic environments. Like the optical flow that represents the motion of pixels in 2D images, 3D motion representation of scene flow benefits many applications, such as autonomous driving and service robot. This paper studies the problem of scene flow estimation from two consecutive 3D point clouds. In this paper, a novel hierarchical neural network with double attention is proposed for learning the correlation of point features in adjacent frames and refining scene flow from coarse to fine layer by layer. The proposed network has a new more-for-less hierarchical architecture. The more-for-less means that the number of input points is greater than the number of output points for scene flow estimation, which brings more input information and balances the precision and resource consumption. In this hierarchical architecture, scene flow of different levels is generated and supervised respectively. A novel attentive embedding module is introduced to aggregate the features of adjacent points using a double attention method in a patch-to-patch manner. The proper layers for flow embedding and flow supervision are carefully considered in our network designment. Experiments show that the proposed network outperforms the state-of-the-art performance of 3D scene flow estimation on the FlyingThings3D and KITTI Scene Flow 2015 datasets. We also apply the proposed network to the realistic LiDAR odometry task, which is a key problem in autonomous driving. The experiment results demonstrate that our proposed network can outperform the ICP-based method and shows good practical application ability. The source codes will be released on https://github.com/IRMVLab/HALFlow.},
  archive      = {J_TIP},
  author       = {Guangming Wang and Xinrui Wu and Zhe Liu and Hesheng Wang},
  doi          = {10.1109/TIP.2021.3079796},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5168-5181},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Hierarchical attention learning of scene flow in 3D point clouds},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Modeling sub-actions for weakly supervised temporal action
localization. <em>TIP</em>, <em>30</em>, 5154–5167. (<a
href="https://doi.org/10.1109/TIP.2021.3078324">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a challenging task of high-level video understanding, weakly supervised temporal action localization has attracted more attention recently. Due to the usage of video-level category labels, this task is usually formulated as the task of classification, which always suffers from the contradiction between classification and detection. In this paper, we describe a novel approach to alleviate the contradiction for detecting more complete action instances by explicitly modeling sub-actions. Our method makes use of three innovations to model the latent sub-actions. First, our framework uses prototypes to represent sub-actions, which can be automatically learned in an end-to-end way. Second, we regard the relations among sub-actions as a graph, and construct the correspondences between sub-actions and actions by the graph pooling operation. Doing so not only makes the sub-actions inter-dependent to facilitate the multi-label setting, but also naturally use the video-level labels as weak supervision. Third, we devise three complementary loss functions, namely, representation loss, balance loss and relation loss to ensure the learned sub-actions are diverse and have clear semantic meanings. Experimental results on THUMOS14 and ActivityNet1.3 datasets demonstrate the effectiveness of our method and superior performance over state-of-the-art approaches.},
  archive      = {J_TIP},
  author       = {Linjiang Huang and Yan Huang and Wanli Ouyang and Liang Wang},
  doi          = {10.1109/TIP.2021.3078324},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5154-5167},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Modeling sub-actions for weakly supervised temporal action localization},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Geodesic paths for image segmentation with implicit
region-based homogeneity enhancement. <em>TIP</em>, <em>30</em>,
5138–5153. (<a href="https://doi.org/10.1109/TIP.2021.3078106">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Minimal paths are regarded as a powerful and efficient tool for boundary detection and image segmentation due to its global optimality and the well-established numerical solutions such as fast marching method. In this paper, we introduce a flexible interactive image segmentation model based on the Eikonal partial differential equation (PDE) framework in conjunction with region-based homogeneity enhancement. A key ingredient in the introduced model is the construction of local geodesic metrics, which are capable of integrating anisotropic and asymmetric edge features, implicit region-based homogeneity features and/or curvature regularization. The incorporation of the region-based homogeneity features into the metrics considered relies on an implicit representation of these features, which is one of the contributions of this work. Moreover, we also introduce a way to build simple closed contours as the concatenation of two disjoint open curves. Experimental results prove that the proposed model indeed outperforms state-of-the-art minimal paths-based image segmentation approaches.},
  archive      = {J_TIP},
  author       = {Da Chen and Jian Zhu and Xinxin Zhang and Minglei Shu and Laurent D. Cohen},
  doi          = {10.1109/TIP.2021.3078106},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5138-5153},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Geodesic paths for image segmentation with implicit region-based homogeneity enhancement},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Gradient-based feature extraction from raw bayer pattern
images. <em>TIP</em>, <em>30</em>, 5122–5137. (<a
href="https://doi.org/10.1109/TIP.2021.3067166">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, the impact of demosaicing on gradient extraction is studied and a gradient-based feature extraction pipeline based on raw Bayer pattern images is proposed. It is shown both theoretically and experimentally that the Bayer pattern images are applicable to the central difference gradient-based feature extraction algorithms with negligible performance degradation, as long as the arrangement of color filter array (CFA) patterns matches the gradient operators. The color difference constancy assumption, which is widely used in various demosaicing algorithms, is applied in the proposed Bayer pattern image-based gradient extraction pipeline. Experimental results show that the gradients extracted from Bayer pattern images are robust enough to be used in histogram of oriented gradients (HOG)-based pedestrian detection algorithms and shift-invariant feature transform (SIFT)-based matching algorithms. By skipping most of the steps in the image signal processing (ISP) pipeline, the computational complexity and power consumption of a computer vision system can be reduced significantly.},
  archive      = {J_TIP},
  author       = {Wei Zhou and Ling Zhang and Shengyu Gao and Xin Lou},
  doi          = {10.1109/TIP.2021.3067166},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5122-5137},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Gradient-based feature extraction from raw bayer pattern images},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). VMAF oriented perceptual coding based on piecewise metric
coupling. <em>TIP</em>, <em>30</em>, 5109–5121. (<a
href="https://doi.org/10.1109/TIP.2021.3078622">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It has been recognized that videos have to be encoded in a rate-distortion optimized manner for high coding performance. Therefore, operational coding methods have been developed for conventional distortion metrics such as Sum of Squared Error (SSE). Nowadays, with the rapid development of machine learning, the state-of-the-art learning based metric Video Multimethod Assessment Fusion (VMAF) has been proven to outperform conventional ones in terms of the correlation with human perception, and thus deserves integration into the coding framework. However, unlike conventional metrics, VMAF has no specific computational formulas and may be frequently updated by new training data, which invalidates the existing coding methods and makes it highly desired to develop a rate-distortion optimized method for VMAF. Moreover, VMAF is designed to operate at the frame level, which leads to further difficulties in its application to today&#39;s block based coding. In this paper, we propose a VMAF oriented perceptual coding method based on piecewise metric coupling. Firstly, we explore the correlation between VMAF and SSE in the neighborhood of a benchmark distortion. Then a rate-distortion optimization model is formulated based on the correlation, and an optimized block based coding method is presented for VMAF. Experimental results show that 3.61\% and 2.67\% bit saving on average can be achieved for VMAF under the low_delay_p and the random_access_main configurations of HEVC coding respectively.},
  archive      = {J_TIP},
  author       = {Zhengyi Luo and Chen Zhu and Yan Huang and Rong Xie and Li Song and C.-C. Jay Kuo},
  doi          = {10.1109/TIP.2021.3078622},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5109-5121},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {VMAF oriented perceptual coding based on piecewise metric coupling},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptive linear span network for object skeleton detection.
<em>TIP</em>, <em>30</em>, 5096–5108. (<a
href="https://doi.org/10.1109/TIP.2021.3078079">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conventional networks for object skeleton detection are usually hand-crafted. Despite the effectiveness, hand-crafted network architectures lack the theoretical basis and require intensive prior knowledge to implement representation complementarity for objects/parts in different granularity. In this paper, we propose an adaptive linear span network (AdaLSN), driven by neural architecture search (NAS), to automatically configure and integrate scale-aware features for object skeleton detection. AdaLSN is formulated with the theory of linear span, which provides one of the earliest explanations for multi-scale deep feature fusion. AdaLSN is materialized by defining a mixed unit-pyramid search space, which goes beyond many existing search spaces using unit-level or pyramid-level features. Within the mixed space, we apply genetic architecture search to jointly optimize unit-level operations and pyramid-level connections for adaptive feature space expansion. AdaLSN substantiates its versatility by achieving significantly higher accuracy and latency trade-off compared with the state-of-the-arts. It also demonstrates general applicability to image-to-mask tasks such as edge detection and road extraction. Code is available at https://github.com/sunsmarterjie/SDL-Skeletongithub.com/sunsmarterjie/SDL-Skeleton.},
  archive      = {J_TIP},
  author       = {Chang Liu and Yunjie Tian and Zhiwen Chen and Jianbin Jiao and Qixiang Ye},
  doi          = {10.1109/TIP.2021.3078079},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5096-5108},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Adaptive linear span network for object skeleton detection},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Angular-driven feedback restoration networks for imperfect
sketch recognition. <em>TIP</em>, <em>30</em>, 5085–5095. (<a
href="https://doi.org/10.1109/TIP.2021.3071711">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic hand-drawn sketch recognition is an important task in computer vision. However, the vast majority of prior works focus on exploring the power of deep learning to achieve better accuracy on complete and clean sketch images, and thus fail to achieve satisfactory performance when applied to incomplete or destroyed sketch images. To address this problem, we first develop two datasets that contain different levels of scrawl and incomplete sketches. Then, we propose an angular-driven feedback restoration network (ADFRNet), which first detects the imperfect parts of a sketch and then refines them into high quality images, to boost the performance of sketch recognition. By introducing a novel “feedback restoration loop” to deliver information between the middle stages, the proposed model can improve the quality of generated sketch images while avoiding the extra memory cost associated with popular cascading generation schemes. In addition, we also employ a novel angular-based loss function to guide the refinement of sketch images and learn a powerful discriminator in the angular space. Extensive experiments conducted on the proposed imperfect sketch datasets demonstrate that the proposed model is able to efficiently improve the quality of sketch images and achieve superior performance over the current state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Jia Wan and Kaihao Zhang and Hongdong Li and Antoni B. Chan},
  doi          = {10.1109/TIP.2021.3071711},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5085-5095},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Angular-driven feedback restoration networks for imperfect sketch recognition},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Efficient center voting for object detection and 6D pose
estimation in 3D point cloud. <em>TIP</em>, <em>30</em>, 5072–5084. (<a
href="https://doi.org/10.1109/TIP.2021.3078109">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel and efficient approach to estimate 6D object poses of known objects in complex scenes represented by point clouds. Our approach is based on the well-known point pair feature (PPF) matching, which utilizes self-similar point pairs to compute potential matches and thereby cast votes for the object pose by a voting scheme. The main contribution of this paper is to present an improved PPF-based recognition framework, especially a new center voting strategy based on the relative geometric relationship between the object center and point pair features. Using this geometric relationship, we first generate votes to object centers resulting in vote clusters near real object centers. Then we group and aggregate these votes to generate a set of pose hypotheses. Finally, a pose verification operator is performed to filter out false positives and predict appropriate 6D poses of the target object. Our approach is also suitable to solve the multi-instance and multi-object detection tasks. Extensive experiments on a variety of challenging benchmark datasets demonstrate that the proposed algorithm is discriminative and robust towards similar-looking distractors, sensor noise, and geometrically simple shapes. The advantage of our work is further verified by comparing to the state-of-the-art approaches.},
  archive      = {J_TIP},
  author       = {Jianwei Guo and Xuejun Xing and Weize Quan and Dong-Ming Yan and Qingyi Gu and Yang Liu and Xiaopeng Zhang},
  doi          = {10.1109/TIP.2021.3078109},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5072-5084},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Efficient center voting for object detection and 6D pose estimation in 3D point cloud},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A generalized asymmetric dual-front model for active
contours and image segmentation. <em>TIP</em>, <em>30</em>, 5056–5071.
(<a href="https://doi.org/10.1109/TIP.2021.3078102">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Voronoi diagram-based dual-front scheme is known as a powerful and efficient technique for addressing the image segmentation and domain partitioning problems. In the basic formulation of existing dual-front approaches, the evolving contour can be considered as the interfaces of adjacent Voronoi regions. Among these dual-front models, a crucial ingredient is regarded as the geodesic metrics by which the geodesic distances and the corresponding Voronoi diagram can be estimated. In this paper, we introduce a new dual-front model based on asymmetric quadratic metrics. These metrics considered are built by the integration of the image features and a vector field derived from the evolving contour. The use of the asymmetry enhancement can reduce the risk for the segmentation contours being stuck at false positions, especially when the initial curves are far away from the target boundaries or the images have complicated intensity distributions. Moreover, the proposed dual-front model can be applied for image segmentation in conjunction with various region-based homogeneity terms. The numerical experiments on both synthetic and real images show that the proposed dual-front model indeed achieves encouraging results.},
  archive      = {J_TIP},
  author       = {Da Chen and Jack Spencer and Jean-Marie Mirebeau and Ke Chen and Minglei Shu and Laurent D. Cohen},
  doi          = {10.1109/TIP.2021.3078102},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5056-5071},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A generalized asymmetric dual-front model for active contours and image segmentation},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Reversible data hiding based on dual pairwise
prediction-error expansion. <em>TIP</em>, <em>30</em>, 5045–5055. (<a
href="https://doi.org/10.1109/TIP.2021.3078088">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reversible data hiding generally exploits the redundancy of the cover medium and prediction-error expansion (PEE) has become the most effective mechanism. However, although the pairwise PEE technique has been proposed to jointly modify the prediction-errors to achieve less degradation, there is still room for improvement. In this paper, a dual pairwise PEE strategy is proposed to fully exploit the potential of pairwise PEE. The key observation behind dual pairwise PEE lies in that most capacity is provided by individually expanding only one pairing error. For such separable error-pairs, we propose to recalculate and collect the rest pairing error to form an error sequence after shifting any one pairing error. Next, by considering every two neighboring errors of the sequence together, a new set of error-pairs for double pairwise PEE can be obtained. Compared with original pairwise PEE, dual pairwise PEE significantly better exploits the correlation of errors such that it leads to better capacity-distortion performance. Experimental results also demonstrate that the proposed scheme outperforms several state-of-the-art schemes.},
  archive      = {J_TIP},
  author       = {Wenguang He and Zhanchuan Cai},
  doi          = {10.1109/TIP.2021.3078088},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5045-5055},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Reversible data hiding based on dual pairwise prediction-error expansion},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A systematic IoU-related method: Beyond simplified
regression for better localization. <em>TIP</em>, <em>30</em>,
5032–5044. (<a href="https://doi.org/10.1109/TIP.2021.3077144">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Four-variable-independent-regression localization losses, such as Smooth- l 1 Loss, are used by default in modern detectors. Nevertheless, this kind of loss is oversimplified so that it is inconsistent with the final evaluation metric, intersection over union (IoU). Directly employing the standard IoU is also not infeasible, since the constant-zero plateau in the case of non-overlapping boxes and the non-zero gradient at the minimum may make it not trainable. Accordingly, we propose a systematic method to address these problems. Firstly, we propose a new metric, the extended IoU (EIoU), which is well-defined when two boxes are not overlapping and reduced to the standard IoU when overlapping. Secondly, we present the convexification technique (CT) to construct a loss on the basis of EIoU, which can guarantee the gradient at the minimum to be zero. Thirdly, we propose a steady optimization technique (SOT) to make the fractional EIoU loss approaching the minimum more steadily and smoothly. Fourthly, to fully exploit the capability of the EIoU based loss, we introduce an interrelated IoU-predicting head to further boost localization accuracy. With the proposed contributions, the new method incorporated into Faster R-CNN with ResNet50+FPN as the backbone yields 4.2 mAP gain on VOC2007 and 2.3 mAP gain on COCO2017 over the baseline Smooth- l 1 Loss, at almost no training and inferencing computational cost. Specifically, the stricter the metric is, the more notable the gain is, improving 8.2 mAP on VOC2007 and 5.4 mAP on COCO2017 at metric AP 90 .},
  archive      = {J_TIP},
  author       = {Hanyang Peng and Shiqi Yu},
  doi          = {10.1109/TIP.2021.3077144},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5032-5044},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A systematic IoU-related method: Beyond simplified regression for better localization},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust subspace detectors based on α-divergence with
application to detection in imaging. <em>TIP</em>, <em>30</em>,
5017–5031. (<a href="https://doi.org/10.1109/TIP.2021.3077139">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robust variants of Wald, Rao and likelihood ratio (LR) tests for the detection of a signal subspace in a signal interference subspace corrupted by contaminated Gaussian noise are proposed in this paper. They are derived using the α-divergence, and the trade-off between the robustness and the power (the probability of detection) of the tests is adjustable using a single hyperparameter α. It is shown that when α→ 1, these tests are equivalent to their well known classical counterparts. For example the robust LR test coincides with the LR test or the matched subspace detector (MSD). Asymptotic results are provided to support the proposed tests and robustness to outliers is obtained using values of α &lt;; 1. Numerical experiments illustrating the performance of these tests on simulated, real functional magnetic resonance imaging (fMRI), hyperspectral and synthetic aperture radar (SAR) data are also presented.},
  archive      = {J_TIP},
  author       = {Aref Miri Rekavandi and Abd-Krim Seghouane and Robin J. Evans},
  doi          = {10.1109/TIP.2021.3077139},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5017-5031},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Robust subspace detectors based on α-divergence with application to detection in imaging},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Ghost-free deep high-dynamic-range imaging using focus
pixels for complex motion scenes. <em>TIP</em>, <em>30</em>, 5001–5016.
(<a href="https://doi.org/10.1109/TIP.2021.3077137">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-exposure image fusion inevitably causes ghost artifacts owing to inaccurate image registration. In this study, we propose a deep learning technique for the seamless fusion of multi-exposed low dynamic range (LDR) images using a focus-pixel sensor. For auto-focusing in mobile cameras, a focus-pixel sensor originally provides left (L) and right (R) luminance images simultaneously with a full-resolution RGB image. These L/R images are less saturated than the RGB images because they are summed up to be a normal pixel value in the RGB image of the focus pixel sensor. These two features of the focus pixel image, namely, relatively short exposure and perfect alignment are utilized in this study to provide fusion cues for high dynamic range (HDR) imaging. To minimize fusion artifacts, luminance and chrominance fusions are performed separately in two sub-nets. In a luminance recovery network, two heterogeneous images, the focus pixel image and the corresponding overexposed LDR image, are first fused by joint learning to produce an HDR luminance image. Subsequently, a chrominance network fuses the color components of the misaligned underexposed LDR input to obtain a 3-channel HDR image. Existing deep-neural-network-based HDR fusion methods fuse misaligned multi-exposed inputs directly. They suffer from visual artifacts that are observed mostly in saturated regions because pixel values are clipped out. Meanwhile, the proposed method reconstructs missing luminance with aligned unsaturated focus pixel image first, and thus, the luma-recovered image provides the cues for accurate color fusion. The experimental results show that the proposed method not only accurately restores fine details in saturated areas, but also produce ghost-free high-quality HDR images without pre-alignment.},
  archive      = {J_TIP},
  author       = {Sung-Min Woo and Je-Ho Ryu and Jong-Ok Kim},
  doi          = {10.1109/TIP.2021.3077137},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5001-5016},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Ghost-free deep high-dynamic-range imaging using focus pixels for complex motion scenes},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Underwater image enhancement via medium transmission-guided
multi-color space embedding. <em>TIP</em>, <em>30</em>, 4985–5000. (<a
href="https://doi.org/10.1109/TIP.2021.3076367">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Underwater images suffer from color casts and low contrast due to wavelength- and distance-dependent attenuation and scattering. To solve these two degradation issues, we present an underwater image enhancement network via medium transmission-guided multi-color space embedding, called Ucolor. Concretely, we first propose a multi-color space encoder network, which enriches the diversity of feature representations by incorporating the characteristics of different color spaces into a unified structure. Coupled with an attention mechanism, the most discriminative features extracted from multiple color spaces are adaptively integrated and highlighted. Inspired by underwater imaging physical models, we design a medium transmission (indicating the percentage of the scene radiance reaching the camera)-guided decoder network to enhance the response of network towards quality-degraded regions. As a result, our network can effectively improve the visual quality of underwater images by exploiting multiple color spaces embedding and the advantages of both physical model-based and learning-based methods. Extensive experiments demonstrate that our Ucolor achieves superior performance against state-of-the-art methods in terms of both visual quality and quantitative metrics. The code is publicly available at: https://li-chongyi.github.io/Proj_Ucolor.html.},
  archive      = {J_TIP},
  author       = {Chongyi Li and Saeed Anwar and Junhui Hou and Runmin Cong and Chunle Guo and Wenqi Ren},
  doi          = {10.1109/TIP.2021.3076367},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4985-5000},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Underwater image enhancement via medium transmission-guided multi-color space embedding},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Backward attentive fusing network with local aggregation
classifier for 3D point cloud semantic segmentation. <em>TIP</em>,
<em>30</em>, 4973–4984. (<a
href="https://doi.org/10.1109/TIP.2021.3073660">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a Backward Attentive Fusing Network with Local Aggregation Classifier (BAF-LAC) is proposed to improve the performance of 3D point cloud semantic segmentation. It consists of a Backward Attentive Fusing Encoder-Decoder (BAF-ED) to learn semantic features and a Local Aggregation Classifier (LAC) to maintain the context-awareness of points. BAF-ED narrows the semantic gap between the encoder and the decoder via fusing multi-layer encoder features with the decoder features. High-level encoder features are transformed into an attention map to modulate low-level encoder features backward. LAC adaptively enhances the intermediate features in point-wise MLPs via aggregating the features of neighboring points into the center point. It takes the place of commonly used post-processing techniques and retains context consistency into the classifier. Equipped with these modules, BAF-LAC can extract discriminative semantic features and predict smoother results. Extensive experiments on Semantic3D, SemanticKITTI, and S3DIS demonstrate that the proposed method can achieve competitive results against the state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Hui Shuai and Xiang Xu and Qingshan Liu},
  doi          = {10.1109/TIP.2021.3073660},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4973-4984},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Backward attentive fusing network with local aggregation classifier for 3D point cloud semantic segmentation},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Visible-NIR image fusion based on top-hat transform.
<em>TIP</em>, <em>30</em>, 4962–4972. (<a
href="https://doi.org/10.1109/TIP.2021.3077310">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The near-infrared band of the electromagnetic spectrum has become an important tool for enhancing image quality. Commonly, outdoor color images are degraded by bad weather conditions that lead to a loss of contrast and fine details in color images since light scattering produces attenuation and smoothing effects. Despite the fact that current Visible-NIR fusion methods achieve image enhancement features, some issues like edge preservation and color oversaturation still need to be addressed. In this work, a method that performs a selective Visible-NIR fusion of the most relevant image structures through top-hat transform is proposed. The performance of the method is evaluated by quantifying the new information added to the image and the change in color. Experimental results show a high degree of detail in preserving the edges while maintaining the color of the image. Moreover, the proposed method demonstrates that the image quality improvements were not significantly affected by a change in the color space.},
  archive      = {J_TIP},
  author       = {María Herrera-Arellano and Hayde Peregrina-Barreto and Iván Terol-Villalobos},
  doi          = {10.1109/TIP.2021.3077310},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4962-4972},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Visible-NIR image fusion based on top-hat transform},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Reasoning graph networks for kinship verification: From
star-shaped to hierarchical. <em>TIP</em>, <em>30</em>, 4947–4961. (<a
href="https://doi.org/10.1109/TIP.2021.3077111">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we investigate the problem of facial kinship verification by learning hierarchical reasoning graph networks. Conventional methods usually focus on learning discriminative features for each facial image of a paired sample and neglect how to fuse the obtained two facial image features and reason about the relations between them. To address this, we propose a Star-shaped Reasoning Graph Network (S-RGN). Our S-RGN first constructs a star-shaped graph where each surrounding node encodes the information of comparisons in a feature dimension and the central node is employed as the bridge for the interaction of surrounding nodes. Then we perform relational reasoning on this star graph with iterative message passing. The proposed S-RGN uses only one central node to analyze and process information from all surrounding nodes, which limits its reasoning capacity. We further develop a Hierarchical Reasoning Graph Network (H-RGN) to exploit more powerful and flexible capacity. More specifically, our H-RGN introduces a set of latent reasoning nodes and constructs a hierarchical graph with them. Then bottom-up comparative information abstraction and top-down comprehensive signal propagation are iteratively performed on the hierarchical graph to update the node features. Extensive experimental results on four widely used kinship databases show that the proposed methods achieve very competitive results.},
  archive      = {J_TIP},
  author       = {Wanhua Li and Jiwen Lu and Abudukelimu Wuerkaixi and Jianjiang Feng and Jie Zhou},
  doi          = {10.1109/TIP.2021.3077111},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4947-4961},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Reasoning graph networks for kinship verification: From star-shaped to hierarchical},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Consistency graph modeling for semantic correspondence.
<em>TIP</em>, <em>30</em>, 4932–4946. (<a
href="https://doi.org/10.1109/TIP.2021.3077138">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To establish robust semantic correspondence between images covering different objects belonging to the same category, there are three important types of information including inter-image relationship, intra-image relationship and cycle consistency. Most existing methods only exploit one or two types of the above information and cannot make them enhance and complement each other. Different from existing methods, we propose a novel end-to-end Consistency Graph Modeling Network (CGMNet) for semantic correspondence by modeling inter-image relationship, intra-image relationship and cycle consistency jointly in a unified deep model. The proposed CGMNet enjoys several merits. First, to the best of our knowledge, this is the first work to jointly model the three kinds of information in a deep model for semantic correspondence. Second, our model has designed three effective modules including cross-graph module, intra-graph module and cycle consistency module, which can jointly learn more discriminative feature representations robust to local ambiguities and background clutter for semantic correspondence. Extensive experimental results show that our algorithm performs favorably against state-of-the-art methods on four challenging datasets including PF-PASCAL, PF-WILLOW, Caltech-101 and TSS.},
  archive      = {J_TIP},
  author       = {Jianfeng He and Tianzhu Zhang and Yuhui Zheng and Mingliang Xu and Yongdong Zhang and Feng Wu},
  doi          = {10.1109/TIP.2021.3077138},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4932-4946},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Consistency graph modeling for semantic correspondence},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning from images: A distillation learning framework for
event cameras. <em>TIP</em>, <em>30</em>, 4919–4931. (<a
href="https://doi.org/10.1109/TIP.2021.3077136">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Event cameras have recently drawn massive attention in the computer vision community because of their low power consumption and high response speed. These cameras produce sparse and non-uniform spatiotemporal representations of a scene. These characteristics of representations make it difficult for event-based models to extract discriminative cues (such as textures and geometric relationships). Consequently, event-based methods usually perform poorly compared to their conventional image counterparts. Considering that traditional images and event signals share considerable visual information, this paper aims to improve the feature extraction ability of event-based models by using knowledge distilled from the image domain to additionally provide explicit feature-level supervision for the learning of event data. Specifically, we propose a simple yet effective distillation learning framework, including multi-level customized knowledge distillation constraints. Our framework can significantly boost the feature extraction process for event data and is applicable to various downstream tasks. We evaluate our framework on high-level and low-level tasks, i.e., object classification and optical flow prediction. Experimental results show that our framework can effectively improve the performance of event-based models on both tasks by a large margin. Furthermore, we present a 10K dataset (CEP-DVS) for event-based object classification. This dataset consists of samples recorded under random motion trajectories that can better evaluate the motion robustness of the event-based model and is compatible with multi-modality vision tasks.},
  archive      = {J_TIP},
  author       = {Yongjian Deng and Hao Chen and Huiying Chen and Youfu Li},
  doi          = {10.1109/TIP.2021.3077136},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4919-4931},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Learning from images: A distillation learning framework for event cameras},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SRGAT: Single image super-resolution with graph attention
network. <em>TIP</em>, <em>30</em>, 4905–4918. (<a
href="https://doi.org/10.1109/TIP.2021.3077135">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks have demonstrated remarkable reconstruction for single-image super-resolution (SISR). However, most existing CNN-based SISR methods directly learn the relation between low-resolution (LR) and high-resolution (HR) images, neglecting to explore the recurrence of internal patches, hence hindering the representational power of CNNs. In this paper, we propose a novel single image Super-Resolution network based on Graph ATtention network (SRGAT) to make full use of the internal patch-recurrence in a natural image. The proposed model employs a feature mapping block with a recurrent structure to refine low-level representations with high-level information. Especifically, the feature mapping block contains a parallel graph similarity branch and a content branch, where the graph similarity branch aims at exploiting the similarity and symmetry across different image patches in low-resolution feature space and provides additional priors for the content branch to enhance texture details. Specifically, we consider the internal patch-recurrence of an image by constructing a graph network on image feature patches. In this way, the information from neighboring patches can be interacted using graph attention network (GAT) to help it recover additional textures, which complements the textures learned from the content branch. Extensive quantitative and qualitative evaluations on five benchmark datasets demonstrate that the proposed algorithm performs favorably against the state-of-the-art super-resolution methods.},
  archive      = {J_TIP},
  author       = {Yanyang Yan and Wenqi Ren and Xiaobin Hu and Kun Li and Haifeng Shen and Xiaochun Cao},
  doi          = {10.1109/TIP.2021.3077135},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4905-4918},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {SRGAT: Single image super-resolution with graph attention network},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Diversifying inference path selection: Moving-mobile-network
for landmark recognition. <em>TIP</em>, <em>30</em>, 4894–4904. (<a
href="https://doi.org/10.1109/TIP.2021.3076275">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep convolutional neural networks have largely benefited computer vision tasks. However, the high computational complexity limits their real-world applications. To this end, many methods have been proposed for efficient network learning, and applications in portable mobile devices. In this paper, we propose a novel Moving-Mobile-Network, named M 2 Net, for landmark recognition, equipped each landmark image with located geographic information. We intuitively find that M 2 Net can essentially promote the diversity of the inference path (selected blocks subset) selection, so as to enhance the recognition accuracy. The above intuition is achieved by our proposed reward function with the input of geo-location and landmarks. We also find that the performance of other portable networks can be improved via our architecture. We construct two landmark image datasets, with each landmark associated with geographic information, over which we conduct extensive experiments to demonstrate that M 2 Net achieves improved recognition accuracy with comparable complexity.},
  archive      = {J_TIP},
  author       = {Biao Qian and Yang Wang and Richang Hong and Meng Wang and Ling Shao},
  doi          = {10.1109/TIP.2021.3076275},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4894-4904},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Diversifying inference path selection: Moving-mobile-network for landmark recognition},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Simultaneous direct depth estimation and synthesis stereo
for single image plant root reconstruction. <em>TIP</em>, <em>30</em>,
4883–4893. (<a href="https://doi.org/10.1109/TIP.2021.3069578">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Plant roots are the main conduit to its interaction with the physical and biological environment. A 3D root system architecture can provide fundamental and applied knowledge of a plant&#39;s ability to thrive, but the construction of 3D structures for thin and complicated plant roots is challenging. Existing methods such as structure-from-motion and shape-from-silhouette require multiple images, as input, under a complicated optimization process, which is usually not convenient in fieldwork. Little effort has been put into investigating the applications of deep neural network methods to reconstruct thin objects, like plant root systems, from a single image. We propose an unsupervised learning scheme to estimate the root depth from only one image as input, which is further applied to reconstruct the complete root system. The boundaries of the reconstructed object usually contain large errors, which is a significant problem for roots with many thin branches. To reduce reconstruction errors, we integrate a cross-view GAN-based network into the reconstruction process, which predicts the root image from a different perspective. Based on the predicted view, we reconstruct the root system using stereo reconstruction, which helps to identify the accurately reconstructed points by enforcing their consistency. The results on both the real plant root dataset and the synthetic dataset demonstrate the effectiveness of the proposed algorithm compared with state-of-the-art single image 3D reconstruction models on plant roots.},
  archive      = {J_TIP},
  author       = {Yawen Lu and Yuxing Wang and Devarth Parikh and Awais Khan and Guoyu Lu},
  doi          = {10.1109/TIP.2021.3069578},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4883-4893},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Simultaneous direct depth estimation and synthesis stereo for single image plant root reconstruction},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep-masking generative network: A unified framework for
background restoration from superimposed images. <em>TIP</em>,
<em>30</em>, 4867–4882. (<a
href="https://doi.org/10.1109/TIP.2021.3076589">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Restoring the clean background from the superimposed images containing a noisy layer is the common crux of a classical category of tasks on image restoration such as image reflection removal, image deraining and image dehazing. These tasks are typically formulated and tackled individually due to diverse and complicated appearance patterns of noise layers within the image. In this work we present the Deep-Masking Generative Network ( DMGN ), which is a unified framework for background restoration from the superimposed images and is able to cope with different types of noise. Our proposed DMGN follows a coarse-to-fine generative process: a coarse background image and a noise image are first generated in parallel, then the noise image is further leveraged to refine the background image to achieve a higher-quality background image. In particular, we design the novel Residual Deep-Masking Cell as the core operating unit for our DMGN to enhance the effective information and suppress the negative information during image generation via learning a gating mask to control the information flow. By iteratively employing this Residual Deep-Masking Cell, our proposed DMGN is able to generate both high-quality background image and noisy image progressively. Furthermore, we propose a two-pronged strategy to effectively leverage the generated noise image as contrasting cues to facilitate the refinement of the background image. Extensive experiments across three typical tasks for image background restoration, including image reflection removal, image rain steak removal and image dehazing, show that our DMGN consistently outperforms state-of-the-art methods specifically designed for each single task.},
  archive      = {J_TIP},
  author       = {Xin Feng and Wenjie Pei and Zihui Jia and Fanglin Chen and David Zhang and Guangming Lu},
  doi          = {10.1109/TIP.2021.3076589},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4867-4882},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Deep-masking generative network: A unified framework for background restoration from superimposed images},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Image inpainting by end-to-end cascaded refinement with mask
awareness. <em>TIP</em>, <em>30</em>, 4855–4866. (<a
href="https://doi.org/10.1109/TIP.2021.3076310">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inpainting arbitrary missing regions is challenging because learning valid features for various masked regions is nontrivial. Though U-shaped encoder-decoder frameworks have been witnessed to be successful, most of them share a common drawback of mask unawareness in feature extraction because all convolution windows (or regions), including those with various shapes of missing pixels, are treated equally and filtered with fixed learned kernels. To this end, we propose our novel mask-aware inpainting solution. Firstly, a Mask-Aware Dynamic Filtering (MADF) module is designed to effectively learn multi-scale features for missing regions in the encoding phase. Specifically, filters for each convolution window are generated from features of the corresponding region of the mask. The second fold of mask awareness is achieved by adopting Point-wise Normalization (PN) in our decoding phase, considering that statistical natures of features at masked points differentiate from those of unmasked points. The proposed PN can tackle this issue by dynamically assigning point-wise scaling factor and bias. Lastly, our model is designed to be an end-to-end cascaded refinement one. Supervision information such as reconstruction loss, perceptual loss and total variation loss is incrementally leveraged to boost the inpainting results from coarse to fine. Effectiveness of the proposed framework is validated both quantitatively and qualitatively via extensive experiments on three public datasets including Places2, CelebA and Paris StreetView.},
  archive      = {J_TIP},
  author       = {Manyu Zhu and Dongliang He and Xin Li and Chao Li and Fu Li and Xiao Liu and Errui Ding and Zhaoxiang Zhang},
  doi          = {10.1109/TIP.2021.3076310},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4855-4866},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Image inpainting by end-to-end cascaded refinement with mask awareness},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). VolumeNet: A lightweight parallel network for
super-resolution of MR and CT volumetric data. <em>TIP</em>,
<em>30</em>, 4840–4854. (<a
href="https://doi.org/10.1109/TIP.2021.3076285">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning-based super-resolution (SR) techniques have generally achieved excellent performance in the computer vision field. Recently, it has been proven that three-dimensional (3D) SR for medical volumetric data delivers better visual results than conventional two-dimensional (2D) processing. However, deepening and widening 3D networks increases training difficulty significantly due to the large number of parameters and small number of training samples. Thus, we propose a 3D convolutional neural network (CNN) for SR of magnetic resonance (MR) and computer tomography (CT) volumetric data called ParallelNet using parallel connections. We construct a parallel connection structure based on the group convolution and feature aggregation to build a 3D CNN that is as wide as possible with a few parameters. As a result, the model thoroughly learns more feature maps with larger receptive fields. In addition, to further improve accuracy, we present an efficient version of ParallelNet (called VolumeNet), which reduces the number of parameters and deepens ParallelNet using a proposed lightweight building block module called the Queue module. Unlike most lightweight CNNs based on depthwise convolutions, the Queue module is primarily constructed using separable 2D cross-channel convolutions. As a result, the number of network parameters and computational complexity can be reduced significantly while maintaining accuracy due to full channel fusion. Experimental results demonstrate that the proposed VolumeNet significantly reduces the number of model parameters and achieves high precision results compared to state-of-the-art methods in tasks of brain MR image SR, abdomen CT image SR, and reconstruction of super-resolution 7T-like images from their 3T counterparts.},
  archive      = {J_TIP},
  author       = {Yinhao Li and Yutaro Iwamoto and Lanfen Lin and Rui Xu and Ruofeng Tong and Yen-Wei Chen},
  doi          = {10.1109/TIP.2021.3076285},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4840-4854},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {VolumeNet: A lightweight parallel network for super-resolution of MR and CT volumetric data},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Uncertainty guided multi-scale attention network for
raindrop removal from a single image. <em>TIP</em>, <em>30</em>,
4828–4839. (<a href="https://doi.org/10.1109/TIP.2021.3076283">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Raindrops adhered to a glass window or camera lens appear in various blurring degrees and resolutions due to the difference in the degrees of raindrops aggregation. The removal of raindrops from a rainy image remains a challenging task because of the density and diversity of raindrops. The abundant location and blur level information are strong prior guide to the task of raindrop removal. However, existing methods use a binary mask to locate and estimate the raindrop with the value 1 (adhesion of raindrops) and 0 (no adhesion), which ignores the diversity of raindrops. Meanwhile, it is noticed that different scale versions of a rainy image have similar raindrop patterns, which makes it possible to employ such complementary information to represent raindrops. In this work, we first propose a soft mask with the value in [-1,1] indicating the blurring level of the raindrops on the background, and explore the positive effect of the blur degree attribute of raindrops on the task of raindrop removal. Secondly, we explore the multi-scale fusion representation for raindrops based on the deep features of the input multi-scale images. The framework is termed uncertainty guided multi-scale attention network (UMAN). Specifically, we construct a multi-scale pyramid structure and introduce an iterative mechanism to extract blur-level information about raindrops to guide the removal of raindrops at different scales. We further introduce the attention mechanism to fuse the input image with the blur-level information, which will highlight raindrop information and reduce the effects of redundant noise. Our proposed method is extensively evaluated on several benchmark datasets and obtains convincing results.},
  archive      = {J_TIP},
  author       = {Ming-Wen Shao and Le Li and De-Yu Meng and Wang-Meng Zuo},
  doi          = {10.1109/TIP.2021.3076283},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4828-4839},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Uncertainty guided multi-scale attention network for raindrop removal from a single image},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning deep lucas-kanade siamese network for visual
tracking. <em>TIP</em>, <em>30</em>, 4814–4827. (<a
href="https://doi.org/10.1109/TIP.2021.3076272">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In most recent years, Siamese trackers have drawn great attention because of their well-balanced accuracy and efficiency. Although these approaches have achieved great success, the discriminative power of the conventional Siamese trackers is still limited by the insufficient template-candidate representation. Most of the existing approaches take non-aligned features to learn a similarity function for template-candidate matching, while the target object’s geometrical transformation is seldom explored. To address this problem, we propose a novel Siamese tracking framework, which enables to dynamically transform the template-candidate features to a more discriminative viewpoint for similarity matching. Specifically, we reformulate the template-candidate matching problem of the conventional Siamese tracker from the perspective of Lucas-Kanade (LK) image alignment approach. A Lucas-Kanade network (LKNet) is proposed and incorporated to the Siamese architecture to learn aligned feature representations in data-driven trainable manner, which is able to enhance the model adaptability in challenging scenarios. Within this framework, we propose two Siamese trackers named LK-Siam and LK-SiamRPN to validate the effectiveness. Extensive experiments conducted on the prevalent datasets show that the proposed method is more competitive over a number of state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Siyuan Yao and Xiaoguang Han and Hua Zhang and Xiao Wang and Xiaochun Cao},
  doi          = {10.1109/TIP.2021.3076272},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4814-4827},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Learning deep lucas-kanade siamese network for visual tracking},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On plug-and-play regularization using linear denoisers.
<em>TIP</em>, <em>30</em>, 4802–4813. (<a
href="https://doi.org/10.1109/TIP.2021.3075092">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In plug-and-play (PnP) regularization, the knowledge of the forward model is combined with a powerful denoiser to obtain state-of-the-art image reconstructions. This is typically done by taking a proximal algorithm such as FISTA or ADMM, and formally replacing the proximal map associated with a regularizer by nonlocal means, BM3D or a CNN denoiser. Each iterate of the resulting PnP algorithm involves some kind of inversion of the forward model followed by denoiser-induced regularization. A natural question in this regard is that of optimality, namely, do the PnP iterations minimize some f+g, where f is a loss function associated with the forward model and g is a regularizer? This has a straightforward solution if the denoiser can be expressed as a proximal map, as was shown to be the case for a class of linear symmetric denoisers. However, this result excludes kernel denoisers such as nonlocal means that are inherently non-symmetric. In this paper, we prove that a broader class of linear denoisers (including symmetric denoisers and kernel denoisers) can be expressed as a proximal map of some convex regularizer g. An algorithmic implication of this result for non-symmetric denoisers is that it necessitates appropriate modifications in the PnP updates to ensure convergence to a minimum of f+g. Apart from the convergence guarantee, the modified PnP algorithms are shown to produce good restorations.},
  archive      = {J_TIP},
  author       = {Ruturaj G. Gavaskar and Chirayu D. Athalye and Kunal N. Chaudhury},
  doi          = {10.1109/TIP.2021.3075092},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4802-4813},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {On plug-and-play regularization using linear denoisers},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DerainCycleGAN: Rain attentive CycleGAN for single image
deraining and rainmaking. <em>TIP</em>, <em>30</em>, 4788–4801. (<a
href="https://doi.org/10.1109/TIP.2021.3074804">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single Image Deraining (SID) is a relatively new and still challenging topic in emerging vision applications, and most of the recently emerged deraining methods use the supervised manner depending on the ground-truth (i.e., using paired data). However, in practice it is rather common to encounter unpaired images in real deraining task. In such cases, how to remove the rain streaks in an unsupervised way will be a challenging task due to lack of constraints between images and hence suffering from low-quality restoration results. In this paper, we therefore explore the unsupervised SID issue using unpaired data, and propose a new unsupervised framework termed DerainCycleGAN for single image rain removal and generation, which can fully utilize the constrained transfer learning ability and circulatory structures of CycleGAN. In addition, we design an unsupervised rain attentive detector (UARD) for enhancing the rain information detection by paying attention to both rainy and rain-free images. Besides, we also contribute a new synthetic way of generating the rain streak information, which is different from the previous ones. Specifically, since the generated rain streaks have diverse shapes and directions, existing derianing methods trained on the generated rainy image by this way can perform much better for processing real rainy images. Extensive experimental results on synthetic and real datasets show that our DerainCycleGAN is superior to current unsupervised and semi-supervised methods, and is also highly competitive to the fully-supervised ones.},
  archive      = {J_TIP},
  author       = {Yanyan Wei and Zhao Zhang and Yang Wang and Mingliang Xu and Yi Yang and Shuicheng Yan and Meng Wang},
  doi          = {10.1109/TIP.2021.3074804},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4788-4801},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {DerainCycleGAN: Rain attentive CycleGAN for single image deraining and rainmaking},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Salient object detection based on visual perceptual
saturation and two-stream hybrid networks. <em>TIP</em>, <em>30</em>,
4773–4787. (<a href="https://doi.org/10.1109/TIP.2021.3074796">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inspired by the perceived saturation of human visual system, this paper proposes a two-stream hybrid networks to simulate binocular vision for salient object detection (SOD). Each stream in our system consists of unsupervised and supervised methods to form a two-branch module, so as to model the interaction between human intuition and memory. The two-branch module parallel processes visual information with bottom-up and top-down SODs, and output two initial saliency maps. Then a polyharmonic neural network with random-weight (PNNRW) is utilized to fuse two-branch’s perception and refine the salient objects by learning online via multi-source cues. Depend on visual perceptual saturation, we can select optimal parameter of superpixel for unsupervised branch, locate sampling regions for PNNRW, and construct a positive feedback loop to facilitate perception saturated after the perception fusion. By comparing the binary outputs of the two-stream, the pixel annotation of predicted object with high saturation degree could be taken as new training samples. The presented method constitutes a semi-supervised learning framework actually. Supervised branches only need to be pre-trained initial, the system can collect the training samples with high confidence level and then train new models by itself. Extensive experiments show that the new framework can improve performance of the existing SOD methods, that exceeds the state-of-the-art methods in six popular benchmarks.},
  archive      = {J_TIP},
  author       = {Chen Pan and Jianfeng Liu and Wei Qi Yan and Feilong Cao and Wei He and Yongxia Zhou},
  doi          = {10.1109/TIP.2021.3074796},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4773-4787},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Salient object detection based on visual perceptual saturation and two-stream hybrid networks},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Meta-VOS: Learning to adapt online target-specific
segmentation. <em>TIP</em>, <em>30</em>, 4760–4772. (<a
href="https://doi.org/10.1109/TIP.2021.3075086">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The task of video object segmentation is a fundamental but challenging problem in the field of computer vision. To deal with large variations in target objects and background clutter, we propose an online adaptive video object segmentation (VOS) framework, named Meta-VOS, that learns to adapt the target-specific segmentation. Meta-VOS builds an online adaptive learning process by exploiting cumulative expertise after searching for confidence patterns across different videos/frames, and then dynamically improves the model learning from two aspects: Meta-seg learner (i.e., module updating) and Meta-seg criterion (i.e., rule of expertise). As our goal is to rapidly determine which patterns best represent the essential characteristics of specific targets in a video, Meta-seg learner is introduced to adaptively learn to update the parameters and hyperparameters of segmentation network in very few gradient descent steps. Furthermore, a Meta-seg criterion of learned expertise, which is constructed to evaluate the Meta-seg learner for the online adaptation of the segmentation network, can confidently online update positive/negative patterns under the guidance of motion cues, object appearances and learned knowledge. Comprehensive evaluations on several benchmark datasets demonstrate the superiority of our proposed Meta-VOS when compared with other state-of-the-art methods applied to the VOS problem.},
  archive      = {J_TIP},
  author       = {Chunyan Xu and Li Wei and Zhen Cui and Tong Zhang and Jian Yang},
  doi          = {10.1109/TIP.2021.3075086},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4760-4772},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Meta-VOS: Learning to adapt online target-specific segmentation},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fast and robust cascade model for multiple degradation
single image super-resolution. <em>TIP</em>, <em>30</em>, 4747–4759. (<a
href="https://doi.org/10.1109/TIP.2021.3074821">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single Image Super-Resolution (SISR) is one of the low-level computer vision problems that has received increased attention in the last few years. Current approaches are primarily based on harnessing the power of deep learning models and optimization techniques to reverse the degradation model. Owing to its hardness, isotropic blurring or Gaussians with small anisotropic deformations have been mainly considered. Here, we widen this scenario by including large non-Gaussian blurs that arise in real camera movements. Our approach leverages the degradation model and proposes a new formulation of the Convolutional Neural Network (CNN) cascade model, where each network sub-module is constrained to solve a specific degradation: deblurring or upsampling. A new densely connected CNN-architecture is proposed where the output of each sub-module is restricted using some external knowledge to focus it on its specific task. As far we know, this use of domain-knowledge to module-level is a novelty in SISR. To fit the finest model, a final sub-module takes care of the residual errors propagated by the previous sub-modules. We check our model with three state-of-the-art (SOTA) datasets in SISR and compare the results with the SOTA models. The results show that our model is the only one able to manage our wider set of deformations. Furthermore, our model overcomes all current SOTA methods for a standard set of deformations. In terms of computational load, our model also improves on the two closest competitors in terms of efficiency. Although the approach is non-blind and requires an estimation of the blur kernel, it shows robustness to blur kernel estimation errors, making it a good alternative to blind models.},
  archive      = {J_TIP},
  author       = {Santiago López-Tapia and Nicolás Pérez de la Blanca},
  doi          = {10.1109/TIP.2021.3074821},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4747-4759},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Fast and robust cascade model for multiple degradation single image super-resolution},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). ResKD: Residual-guided knowledge distillation. <em>TIP</em>,
<em>30</em>, 4735–4746. (<a
href="https://doi.org/10.1109/TIP.2021.3066051">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge distillation, aimed at transferring the knowledge from a heavy teacher network to a lightweight student network, has emerged as a promising technique for compressing neural networks. However, due to the capacity gap between the heavy teacher and the lightweight student, there still exists a significant performance gap between them. In this article, we see knowledge distillation in a fresh light, using the knowledge gap, or the residual , between a teacher and a student as guidance to train a much more lightweight student, called a res-student. We combine the student and the res-student into a new student, where the res-student rectifies the errors of the former student. Such a residual-guided process can be repeated until the user strikes the balance between accuracy and cost. At inference time, we propose a sample-adaptive strategy to decide which res-students are not necessary for each sample, which can save computational cost. Experimental results show that we achieve competitive performance with 18.04\%, 23.14\%, 53.59\%, and 56.86\% of the teachers’ computational costs on the CIFAR-10, CIFAR-100, Tiny-ImageNet, and ImageNet datasets. Finally, we do thorough theoretical and empirical analysis for our method.},
  archive      = {J_TIP},
  author       = {Xuewei Li and Songyuan Li and Bourahla Omar and Fei Wu and Xi Li},
  doi          = {10.1109/TIP.2021.3066051},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4735-4746},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {ResKD: Residual-guided knowledge distillation},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Composited FishNet: Fish detection and species recognition
from low-quality underwater videos. <em>TIP</em>, <em>30</em>,
4719–4734. (<a href="https://doi.org/10.1109/TIP.2021.3074738">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The automatic detection and identification of fish from underwater videos is of great significance for fishery resource assessment and ecological environment monitoring. However, due to the poor quality of underwater images and unconstrained fish movement, traditional hand-designed feature extraction methods or convolutional neural network (CNN)-based object detection algorithms cannot meet the detection requirements in real underwater scenes. Therefore, to realize fish recognition and localization in a complex underwater environment, this paper proposes a novel composite fish detection framework based on a composite backbone and an enhanced path aggregation network called Composited FishNet. By improving the residual network (ResNet), a new composite backbone network (CBresnet) is designed to learn the scene change information (source domain style), which is caused by the differences in the image brightness, fish orientation, seabed structure, aquatic plant movement, fish species shape and texture differences. Thus, the interference of underwater environmental information on the object characteristics is reduced, and the output of the main network to the object information is strengthened. In addition, to better integrate the high and low feature information output from CBresnet, the enhanced path aggregation network (EPANet) is also designed to solve the insufficient utilization of semantic information caused by linear upsampling. The experimental results show that the average precision (AP) 0.5:0.95 , AP 50 and average recall (AR) max=10 of the proposed Composited FishNet are 75.2\%, 92.8\% and 81.1\%, respectively. The composite backbone network enhances the characteristic information output of the detected object and improves the utilization of characteristic information. This method can be used for fish detection and identification in complex underwater environments such as oceans and aquaculture.},
  archive      = {J_TIP},
  author       = {Zhenxi Zhao and Yang Liu and Xudong Sun and Jintao Liu and Xinting Yang and Chao Zhou},
  doi          = {10.1109/TIP.2021.3074738},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4719-4734},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Composited FishNet: Fish detection and species recognition from low-quality underwater videos},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-objective optimization of quality in VVC rate control
for low-delay video coding. <em>TIP</em>, <em>30</em>, 4706–4718. (<a
href="https://doi.org/10.1109/TIP.2021.3072225">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a multi-objective optimization of quality based coding tree unit (CTU) level rate control method, named MORC, for low-delay video coding of Versatile Video Coding (VVC) is proposed. Unlike the traditional rate control algorithms, objectives of minimizing average distortion and minimizing quality fluctuation are introduced and considered simultaneously to obtain the optimal solution. Weighting method is then used to convert the multi-objective optimization problem into a simplistic single-objective problem. Accordingly, given the target rate constraint, rate allocation is formulated as a convex optimization problem to achieve better trade-off among three multiple objectives: minimizing average video distortion, meeting rate constraint and minimizing video quality fluctuation. Based on the D- $\lambda $ model, a two-stage method is further proposed to solve the optimization problem. Experiments have been conducted on the test model of VVC, which is VTM 3.0. Compared with the state-of-the-art rate control algorithms, the proposed method can obtain the Pareto optimal solution, which dominates the solution of other methods and is able to achieve better trade-off among multiple objectives.},
  archive      = {J_TIP},
  author       = {Feiyang Liu and Zhenzhong Chen},
  doi          = {10.1109/TIP.2021.3072225},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4706-4718},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Multi-objective optimization of quality in VVC rate control for low-delay video coding},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MLDA-net: Multi-level dual attention-based network for
self-supervised monocular depth estimation. <em>TIP</em>, <em>30</em>,
4691–4705. (<a href="https://doi.org/10.1109/TIP.2021.3074306">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The success of supervised learning-based single image depth estimation methods critically depends on the availability of large-scale dense per-pixel depth annotations, which requires both laborious and expensive annotation process. Therefore, the self-supervised methods are much desirable, which attract significant attention recently. However, depth maps predicted by existing self-supervised methods tend to be blurry with many depth details lost. To overcome these limitations, we propose a novel framework, named MLDA-Net, to obtain per-pixel depth maps with shaper boundaries and richer depth details. Our first innovation is a multi-level feature extraction (MLFE) strategy which can learn rich hierarchical representation. Then, a dual-attention strategy, combining global attention and structure attention, is proposed to intensify the obtained features both globally and locally, resulting in improved depth maps with sharper boundaries. Finally, a reweighted loss strategy based on multi-level outputs is proposed to conduct effective supervision for self-supervised depth estimation. Experimental results demonstrate that our MLDA-Net framework achieves state-of-the-art depth prediction results on the KITTI benchmark for self-supervised monocular depth estimation with different input modes and training modes. Extensive experiments on other benchmark datasets further confirm the superiority of our proposed approach.},
  archive      = {J_TIP},
  author       = {Xibin Song and Wei Li and Dingfu Zhou and Yuchao Dai and Jin Fang and Hongdong Li and Liangjun Zhang},
  doi          = {10.1109/TIP.2021.3074306},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4691-4705},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {MLDA-net: Multi-level dual attention-based network for self-supervised monocular depth estimation},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Micro-doppler effects removed sparse aperture ISAR imaging
via low-rank and double sparsity constrained ADMM and linearized ADMM.
<em>TIP</em>, <em>30</em>, 4678–4690. (<a
href="https://doi.org/10.1109/TIP.2021.3074271">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inverse synthetic aperture radar (ISAR) imaging for the target with micro-motion parts is influenced by the micro-Doppler (m-D) effects. In this case, the radar echo is generally decomposed into the components from the main body and micro-motion parts of target, respectively, to remove the m-D effects and derive a focused ISAR image of the main body. For the sparse aperture data, however, the radar echo is intentionally or occasionally under-sampled, which defocuses the ISAR image by introducing considerable interference, and deteriorates the performance of signal decomposition for the removal of m-D effects. To address this issue, this paper proposes a novel m-D effects removed sparse aperture ISAR (SA-ISAR) imaging algorithm. Note that during a short interval of ISAR imaging, the range profiles of the main body of target from different pulses are similar, resulting in a low-rank matrix of range profile sequence of main body. For the range profiles of the micro-motion parts, they either spread in different range cells or glint in a single range cell, which results in a sparse matrix of range profile sequence. From this perspective, the low-rank and sparse properties are utilized to decompose the range profiles of the main body and micro-motion parts, respectively. Moreover, the sparsity of ISAR image is also utilized as a constraint to eliminate the interference caused by sparse aperture. Hence, SA-ISAR imaging with the removal of m-D effects is modeled as a triply constrained underdetermined optimization problem. The alternating direction method of multipliers (ADMM) and linearized ADMM (L-ADMM) are further utilized to solve the problem with high efficiency. Experimental results based on both simulated and measured data validate the effectiveness of the proposed algorithm.},
  archive      = {J_TIP},
  author       = {Shuanghui Zhang and Yongxiang Liu and Xiang Li},
  doi          = {10.1109/TIP.2021.3074271},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4678-4690},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Micro-doppler effects removed sparse aperture ISAR imaging via low-rank and double sparsity constrained ADMM and linearized ADMM},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Video moment localization via deep cross-modal hashing.
<em>TIP</em>, <em>30</em>, 4667–4677. (<a
href="https://doi.org/10.1109/TIP.2021.3073867">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the continuous booming of surveillance and Web videos, video moment localization, as an important branch of video content analysis, has attracted wide attention from both industry and academia in recent years. It is, however, a non-trivial task due to the following challenges: temporal context modeling, intelligent moment candidate generation, as well as the necessary efficiency and scalability in practice. To address these impediments, we present a deep end-to-end cross-modal hashing network. To be specific, we first design a video encoder relying on a bidirectional temporal convolutional network to simultaneously generate moment candidates and learn their representations. Considering that the video encoder characterizes temporal contextual structures at multiple scales of time windows, we can thus obtain enhanced moment representations. As a counterpart, we design an independent query encoder towards user intention understanding. Thereafter, a cross-model hashing module is developed to project these two heterogeneous representations into a shared isomorphic Hamming space for compact hash code learning. After that, we can effectively estimate the relevance score of each “ moment-query ” pair via the Hamming distance. Besides effectiveness, our model is far more efficient and scalable since the hash codes of videos can be learned offline. Experimental results on real-world datasets have justified the superiority of our model over several state-of-the-art competitors.},
  archive      = {J_TIP},
  author       = {Yupeng Hu and Meng Liu and Xiaobin Su and Zan Gao and Liqiang Nie},
  doi          = {10.1109/TIP.2021.3073867},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4667-4677},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Video moment localization via deep cross-modal hashing},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Effective blind image deblurring using matrix-variable
optimization. <em>TIP</em>, <em>30</em>, 4653–4666. (<a
href="https://doi.org/10.1109/TIP.2021.3073856">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blind image deblurring has been a challenging issue due to the unknown blur and computation problem. Recently, the matrix-variable optimization method successfully demonstrates its potential advantages in computation. This paper proposes an effective matrix-variable optimization method for blind image deblurring. Blur kernel matrix is exactly decomposed by a direct SVD technique. The blur kernel and original image are well estimated by minimizing a matrix-variable optimization problem with blur kernel constraints. A matrix-type alternative iterative algorithm is proposed to solve the matrix-variable optimization problem. Finally, experimental results show that the proposed blind image deblurring method is much superior to the state-of-the-art blind image deblurring algorithms in terms of image quality and computation time.},
  archive      = {J_TIP},
  author       = {Liqing Huang and Youshen Xia and Tiantian Ye},
  doi          = {10.1109/TIP.2021.3073856},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4653-4666},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Effective blind image deblurring using matrix-variable optimization},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Simultaneous video stabilization and rolling shutter
removal. <em>TIP</em>, <em>30</em>, 4637–4652. (<a
href="https://doi.org/10.1109/TIP.2021.3073865">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the delay in the row-wise exposure and the lack of stable support when a photographer holds a CMOS camera, video jitter and rolling shutter distortion are closely coupled degradations in the captured videos. However, previous methods have rarely considered both phenomena and usually treat them separately, with stabilization approaches that are unable to handle the rolling shutter effect and rolling shutter removal algorithms that are incapable of addressing motion shake. To tackle this problem, we propose a novel method that simultaneously stabilizes and rectifies a rolling shutter shaky video. The key issue is to estimate both inter-frame motion and intra-frame motion. Specifically, for each pair of adjacent frames, we first estimate a set of spatially variant inter-frame motions using a neighbor-motion-aware local motion model, where the classical mesh-based model is improved by introducing a new constraint to enhance the neighbor motion consistency. Then, different from other 2D rolling shutter removal methods that assume the pixels in the same row have a single intra-frame motion, we build a novel mesh-based intra-frame motion calculation model to cope with the depth variation in a mesh row and obtain more faithful estimation results. Finally, temporal and spatial motion constraints and an adaptive weight assignment strategy are considered together to generate the optimal warping transformations for different motion situations. Experimental results demonstrate the effectiveness and superiority of the proposed method when compared with other state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Huicong Wu and Liang Xiao and Zhihui Wei},
  doi          = {10.1109/TIP.2021.3073865},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4637-4652},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Simultaneous video stabilization and rolling shutter removal},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Graph learning based head movement prediction for
interactive 360 video streaming. <em>TIP</em>, <em>30</em>, 4622–4636.
(<a href="https://doi.org/10.1109/TIP.2021.3073283">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ultra-high definition (UHD) 360 videos encoded in fine quality are typically too large to stream in its entirety over bandwidth (BW)-constrained networks. One popular approach is to interactively extract and send a spatial sub-region corresponding to a viewer’s current field-of-view (FoV) in a head-mounted display (HMD) for more BW-efficient streaming. Due to the non-negligible round-trip-time (RTT) delay between server and client, accurate head movement prediction foretelling a viewer’s future FoVs is essential. In this paper, we cast the head movement prediction task as a sparse directed graph learning problem: three sources of relevant information—collected viewers’ head movement traces, a 360 image saliency map, and a biological human head model—are distilled into a view transition Markov model. Specifically, we formulate a constrained maximum a posteriori (MAP) problem with likelihood and prior terms defined using the three information sources. We solve the MAP problem alternately using a hybrid iterative reweighted least square (IRLS) and Frank-Wolfe (FW) optimization strategy. In each FW iteration, a linear program (LP) is solved, whose runtime is reduced thanks to warm start initialization. Having estimated a Markov model from data, we employ it to optimize a tile-based 360 video streaming system. Extensive experiments show that our head movement prediction scheme noticeably outperformed existing proposals, and our optimized tile-based streaming scheme outperformed competitors in rate-distortion performance.},
  archive      = {J_TIP},
  author       = {Xue Zhang and Gene Cheung and Yao Zhao and Patrick Le Callet and Chunyu Lin and Jack Z. G. Tan},
  doi          = {10.1109/TIP.2021.3073283},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4622-4636},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Graph learning based head movement prediction for interactive 360 video streaming},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Explicit facial expression transfer via fine-grained
representations. <em>TIP</em>, <em>30</em>, 4610–4621. (<a
href="https://doi.org/10.1109/TIP.2021.3073857">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial expression transfer between two unpaired images is a challenging problem, as fine-grained expression is typically tangled with other facial attributes. Most existing methods treat expression transfer as an application of expression manipulation, and use predicted global expression, landmarks or action units (AUs) as a guidance. However, the prediction may be inaccurate, which limits the performance of transferring fine-grained expression. Instead of using an intermediate estimated guidance, we propose to explicitly transfer facial expression by directly mapping two unpaired input images to two synthesized images with swapped expressions. Specifically, considering AUs semantically describe fine-grained expression details, we propose a novel multi-class adversarial training method to disentangle input images into two types of fine-grained representations: AU-related feature and AU-free feature. Then, we can synthesize new images with preserved identities and swapped expressions by combining AU-free features with swapped AU-related features. Moreover, to obtain reliable expression transfer results of the unpaired input, we introduce a swap consistency loss to make the synthesized images and self-reconstructed images indistinguishable. Extensive experiments show that our approach outperforms the state-of-the-art expression manipulation methods for transferring fine-grained expressions while preserving other attributes including identity and pose.},
  archive      = {J_TIP},
  author       = {Zhiwen Shao and Hengliang Zhu and Junshu Tang and Xuequan Lu and Lizhuang Ma},
  doi          = {10.1109/TIP.2021.3073857},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4610-4621},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Explicit facial expression transfer via fine-grained representations},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). GSDet: Object detection in aerial images based on scale
reasoning. <em>TIP</em>, <em>30</em>, 4599–4609. (<a
href="https://doi.org/10.1109/TIP.2021.3073319">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Variations in both object scale and style under different capture scenes (e.g., downtown, port) greatly enhance the difficulties associated with object detection in aerial images. Although ground sample distance (GSD) provides an apparent clue to address this issue, no existing object detection methods have considered utilizing this useful prior knowledge. In this paper, we propose the first object detection network to incorporate GSD into the object detection modeling process. More specifically, built on a two-stage detection framework, we adopt a GSD identification subnet converting the GSD regression into a probability estimation process, then combine the GSD information with the sizes of Regions of Interest (RoIs) to determine the physical size of objects. The estimated physical size can provide a powerful prior for detection by reweighting the weights from the classification layer of each category to produce RoI-wise enhanced features. Furthermore, to improve the discriminability among categories of similar size and make the inference process more adaptive, the scene information is also considered. The pipeline is flexible enough to be stacked on any two-stage modern detection framework. The improvement over the existing two-stage object detection methods on the DOTA dataset demonstrates the effectiveness of our method.},
  archive      = {J_TIP},
  author       = {Wei Li and Wei Wei and Lei Zhang},
  doi          = {10.1109/TIP.2021.3073319},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4599-4609},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {GSDet: Object detection in aerial images based on scale reasoning},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Cross-layer feature pyramid network for salient object
detection. <em>TIP</em>, <em>30</em>, 4587–4598. (<a
href="https://doi.org/10.1109/TIP.2021.3072811">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature pyramid network (FPN) based models, which fuse the semantics and salient details in a progressive manner, have been proven highly effective in salient object detection. However, it is observed that these models often generate saliency maps with incomplete object structures or unclear object boundaries, due to the indirect information propagation among distant layers that makes such fusion structure less effective. In this work, we propose a novel Cross-layer Feature Pyramid Network (CFPN), in which direct cross-layer communication is enabled to improve the progressive fusion in salient object detection. Specifically, the proposed network first aggregates multi-scale features from different layers into feature maps that have access to both the high- and low- level information. Then, it distributes the aggregated features to all the involved layers to gain access to richer context. In this way, the distributed features per layer own both semantics and salient details from all other layers simultaneously, and suffer reduced loss of important information during the progressive feature fusion. At last, CFPN fuses the distributed features of each layer stage-by-stage. This way, the high-level features that contain context useful for locating complete objects are preserved until the final output layer, and the low-level features that contain spatial structure details are embedded into each layer to preserve spatial structural details. Extensive experimental results over six widely used salient object detection benchmarks and with three popular backbones clearly demonstrate that CFPN can accurately locate fairly complete salient regions and effectively segment the object boundaries.},
  archive      = {J_TIP},
  author       = {Zun Li and Congyan Lang and Jun Hao Liew and Yidong Li and Qibin Hou and Jiashi Feng},
  doi          = {10.1109/TIP.2021.3072811},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4587-4598},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Cross-layer feature pyramid network for salient object detection},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Petersen graph multi-orientation based multi-scale ternary
pattern (PGMO-MSTP): An efficient descriptor for texture and material
recognition. <em>TIP</em>, <em>30</em>, 4571–4586. (<a
href="https://doi.org/10.1109/TIP.2021.3070188">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classifying and modeling texture images, especially those with significant rotation, illumination, scale, and view-point variations, is a hot topic in the computer vision field. Inspired by local graph structure (LGS), local ternary patterns (LTP), and their variants, this paper proposes a novel image feature descriptor for texture and material classification, which we call Petersen Graph Multi-Orientation based Multi-Scale Ternary Pattern (PGMO-MSTP). PGMO-MSTP is a histogram representation that efficiently encodes the joint information within an image across feature and scale spaces, exploiting the concepts of both LTP-like and LGS-like descriptors, in order to overcome the shortcomings of these approaches. We first designed two single-scale horizontal and vertical Petersen Graph-based Ternary Pattern descriptors ( PGTP h and PGTP v ). The essence of PGTP h and PGTP v is to encode each 5×5 image patch, extending the ideas of the LTP and LGS concepts, according to relationships between pixels sampled in a variety of spatial arrangements (i.e., up, down, left, and right) of Petersen graph-shaped oriented sampling structures. The histograms obtained from the single-scale descriptors PGTP h and PGTP v are then combined, in order to build the effective multi-scale PGMO-MSTP model. Extensive experiments are conducted on sixteen challenging texture data sets, demonstrating that PGMO-MSTP can outperform state-of-the-art handcrafted texture descriptors and deep learning-based feature extraction approaches. Moreover, a statistical comparison based on the Wilcoxon signed rank test demonstrates that PGMO-MSTP performed the best over all tested data sets.},
  archive      = {J_TIP},
  author       = {Issam El Khadiri and Youssef El Merabet and Ahmad S. Tarawneh and Yassine Ruichek and Dmitry Chetverikov and Raja Touahni and Ahmad B. Hassanat},
  doi          = {10.1109/TIP.2021.3070188},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4571-4586},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Petersen graph multi-orientation based multi-scale ternary pattern (PGMO-MSTP): An efficient descriptor for texture and material recognition},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A framework for hexagonal image processing using hexagonal
pixel-perfect approximations in subpixel resolution. <em>TIP</em>,
<em>30</em>, 4555–4570. (<a
href="https://doi.org/10.1109/TIP.2021.3073328">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image processing in hexagonal lattice has many advantages rather than square lattice. Researchers have addressed benefits of hexagonal structure in applications such as binarization, rotation, scaling and edge detection. Approximately all existing hardwares for capturing and displaying images are based on square lattice. Therefore, the best way for using advantages of hexagonal lattice is to find a proper software approach to convert square pixels to hexagonal ones. This paper presents a hexagonal platform based on interpolation which addresses three existing hexagonal challenges including imperfect hexagonal shape, inaccurate intensity level of hexagonal pixels and lower resolution in hexagonal space. The proposed interpolation is computed by overlaps between square and hexagonal pixels. Overlap types are formulated mathematically in 8 separate cases. Each overlap case is detected automatically and used to compute final gray-level intensity of hexagonal pixels. It is mathematically and experimentally shown that the proposed method satisfies necessary conditions for square-to-hexagonal conversion. The proposed scheme is evaluated on synthetic and real images with 10 different levels of noise in interpolation and edge detection applications. In synthetic images, the proposed method achieves the best figure of merit (FOM) 99.92\% and 98.67\% in high and low SNRs 100 and 20, respectively. Also, the proposed method outperforms existing state of the art hexagonal lattices with interclass correlation coefficient (ICC) 84.18\% and mean rating 7.7 (out of 9) in real images.},
  archive      = {J_TIP},
  author       = {Sadegh Fadaei and Abdolreza Rashno},
  doi          = {10.1109/TIP.2021.3073328},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4555-4570},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A framework for hexagonal image processing using hexagonal pixel-perfect approximations in subpixel resolution},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). HSGAN: Hierarchical graph learning for point cloud
generation. <em>TIP</em>, <em>30</em>, 4540–4554. (<a
href="https://doi.org/10.1109/TIP.2021.3073318">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point clouds are the most general data representations of real and abstract objects, and have a wide variety of applications in many science and engineering fields. Point clouds also provide the most scalable multi-resolution composition for geometric structures. Although point cloud learning has shown remarkable results in shape estimation and semantic segmentation, the unsupervised generation of 3D object parts still pose significant challenges in the 3D shape understanding problem. We address this problem by proposing a novel Generative Adversarial Network (GAN), named HSGAN, or Hierarchical Self-Attention GAN, with remarkable properties for 3D shape generation. Our generative model takes a random code and hierarchically transforms it into a representation graph by incorporating both Graph Convolution Network (GCN) and self-attention. With embedding the global graph topology in shape generation, the proposed model takes advantage of the latent topological information to fully construct the geometry of 3D object shapes. Different from the existing generative pipelines, our deep learning architecture articulates three significant properties HSGAN effectively deploys the compact latent topology information as a graph representation in the generative learning process and generates realistic point clouds, HSGAN avoids multiple discriminator updates per generator update, and HSGAN preserves the most dominant geometric structures of 3D shapes in the same hierarchical sampling process. We demonstrate the performance of our new approach with both quantitative and qualitative evaluations. We further present a new adversarial loss to maintain the training stability and overcome the potential mode collapse of traditional GANs. Finally, we explore the use of HSGAN as a plug-and-play decoder in the auto-encoding architecture.},
  archive      = {J_TIP},
  author       = {Yushi Li and George Baciu},
  doi          = {10.1109/TIP.2021.3073318},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4540-4554},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {HSGAN: Hierarchical graph learning for point cloud generation},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning methodologies to generate kernel-learning-based
image downscaler for arbitrary scaling factors. <em>TIP</em>,
<em>30</em>, 4526–4539. (<a
href="https://doi.org/10.1109/TIP.2021.3073316">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Displays and content have various resolutions and aspect ratios, requiring an image downscaler to adaptively reduce the image resolution. However, research on downscaling has garnered less attention than upscaling, including super-resolution. In practical display systems, simple interpolation, such as a bicubic filter that cannot preserve image details well, is still widely used for image downscaling rather than frame optimization-based or learning-based methods because of following reasons: frame optimization-based methods can effectively preserve image details after downscaling but are difficult to implement due to hardware costs. Learning-based methods have not been developed because defining a target downscaled image for training is difficult and training all downscaling factors is impossible. We propose a novel kernel-learning-based image downscaler to improve detail-preservation quality while supporting arbitrary downscaling factors using simple linear mapping. For this, a method to produce the ideal target downscaling result considering aliasing artifacts and detail preservation after downscaling is proposed. Then, we propose a training technique using the positional relationship between input and output pixels and a hierarchical region analysis to reproduce target images through simple kernel-based linear mapping. Lastly, a kernel-sharing technique is proposed to generate downscaling results for downscaling factors using a minimum number of trained kernels. In the simulation results, the proposed method demonstrated excellent edge preservation by improving the recall, precision, and F1 score, measuring the edge consistency between input and downscaled images, by up to 0.141, 0.079, 0.053, respectively, compared to benchmark methods. In a paired-comparison-based user study, the proposed method obtained the highest preference among benchmark methods using simple operations.},
  archive      = {J_TIP},
  author       = {Sung In Cho and Suk-Ju Kang},
  doi          = {10.1109/TIP.2021.3073316},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4526-4539},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Learning methodologies to generate kernel-learning-based image downscaler for arbitrary scaling factors},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Neutral cross-entropy loss based unsupervised domain
adaptation for semantic segmentation. <em>TIP</em>, <em>30</em>,
4516–4525. (<a href="https://doi.org/10.1109/TIP.2021.3073285">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The generalization performance for semantic segmentation remains a major challenge when the data distributions between the source and target domain mismatch. Unsupervised domain adaptation (UDA) approaches are proposed to mitigate the problem above, among which entropy-minimization-based methods have gained more and more attention. However, the methods merely follow the cluster assumption sharpening the prediction distribution, thus have limited performance improvement. Without additional priors, the entropy loss can easily over-sharpen the prediction distribution, which brings noisy information into the learning process. On the other hand, the gradient of the entropy loss is strongly biased toward easy samples, also leading to limited generalization advances. In this paper, we firstly propose a pixel-level consistency regularization method, which introduces the smoothness prior to the UDA problem. Furthermore, we propose the neutral cross-entropy loss based on the consistency regularization, and reveal that its internal neutralization mechanism mitigates the over-sharpness of entropy minimization via the flatness effect of consistency regularization. We also demonstrate that the gradient bias toward easy samples is inherently tackled via the neutral cross-entropy loss. The experiments show that the proposed method has outperformed state-of-the-art methods in two synthetic-to-real experiments, only using the lightweight network.},
  archive      = {J_TIP},
  author       = {Hanqing Xu and Ming Yang and Liuyuan Deng and Yeqiang Qian and Chunxiang Wang},
  doi          = {10.1109/TIP.2021.3073285},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4516-4525},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Neutral cross-entropy loss based unsupervised domain adaptation for semantic segmentation},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Localizing anomalies from weakly-labeled videos.
<em>TIP</em>, <em>30</em>, 4505–4515. (<a
href="https://doi.org/10.1109/TIP.2021.3072863">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video anomaly detection under video-level labels is currently a challenging task. Previous works have made progresses on discriminating whether a video sequence contains anomalies. However, most of them fail to accurately localize the anomalous events within videos in the temporal domain. In this paper, we propose a Weakly Supervised Anomaly Localization (WSAL) method focusing on temporally localizing anomalous segments within anomalous videos. Inspired by the appearance difference in anomalous videos, the evolution of adjacent temporal segments is evaluated for the localization of anomalous segments. To this end, a high-order context encoding model is proposed to not only extract semantic representations but also measure the dynamic variations so that the temporal context could be effectively utilized. In addition, in order to fully utilize the spatial context information, the immediate semantics are directly derived from the segment representations. The dynamic variations as well as the immediate semantics, are efficiently aggregated to obtain the final anomaly scores. An enhancement strategy is further proposed to deal with noise interference and the absence of localization guidance in anomaly detection. Moreover, to facilitate the diversity requirement for anomaly detection benchmarks, we also collect a new traffic anomaly (TAD) dataset which specifies in the traffic conditions, differing greatly from the current popular anomaly detection evaluation benchmarks. Thedataset and the benchmark test codes, as well as experimental results, are made public on http://vgg-ai.cn/pages/Resource/ and https://github.com/ktr-hubrt/WSAL . Extensive experiments are conducted to verify the effectiveness of different components, and our proposed method achieves new state-of-the-art performance on the UCF-Crime and TAD datasets.},
  archive      = {J_TIP},
  author       = {Hui Lv and Chuanwei Zhou and Zhen Cui and Chunyan Xu and Yong Li and Jian Yang},
  doi          = {10.1109/TIP.2021.3072863},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4505-4515},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Localizing anomalies from weakly-labeled videos},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Unsupervised monocular depth estimation via recursive stereo
distillation. <em>TIP</em>, <em>30</em>, 4492–4504. (<a
href="https://doi.org/10.1109/TIP.2021.3072215">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing unsupervised monocular depth estimation methods resort to stereo image pairs instead of ground-truth depth maps as supervision to predict scene depth. Constrained by the type of monocular input in testing phase, they fail to fully exploit the stereo information through the network during training, leading to the unsatisfactory performance of depth estimation. Therefore, we propose a novel architecture which consists of a monocular network (Mono-Net) that infers depth maps from monocular inputs, and a stereo network (Stereo-Net) that further excavates the stereo information by taking stereo pairs as input. During training, the sophisticated Stereo-Net guides the learning of Mono-Net and devotes to enhance the performance of Mono-Net without changing its network structure and increasing its computational burden. Thus, monocular depth estimation with superior performance and fast runtime can be achieved in testing phase by only using the lightweight Mono-Net. For the proposed framework, our core idea lies in: 1) how to design the Stereo-Net so that it can accurately estimate depth maps by fully exploiting the stereo information; 2) how to use the sophisticated Stereo-Net to improve the performance of Mono-Net. To this end, we propose a recursive estimation and refinement strategy for Stereo-Net to boost its performance of depth estimation. Meanwhile, a multi-space knowledge distillation scheme is designed to help Mono-Net amalgamate the knowledge and master the expertise from Stereo-Net in a multi-scale fashion. Experiments demonstrate that our method achieves the superior performance of monocular depth estimation in comparison with other state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Xinchen Ye and Xin Fan and Mingliang Zhang and Rui Xu and Wei Zhong},
  doi          = {10.1109/TIP.2021.3072215},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4492-4504},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Unsupervised monocular depth estimation via recursive stereo distillation},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep dynamic scene deblurring for unconstrained dual-lens
cameras. <em>TIP</em>, <em>30</em>, 4479–4491. (<a
href="https://doi.org/10.1109/TIP.2021.3072856">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dual-lens (DL) cameras capture depth information, and hence enable several important vision applications. Most present-day DL cameras employ unconstrained settings in the two views in order to support extended functionalities. But a natural hindrance to their working is the ubiquitous motion blur encountered due to camera motion, object motion, or both. However, there exists not a single work for the prospective unconstrained DL cameras that addresses this problem (so called dynamic scene deblurring). Due to the unconstrained settings, degradations in the two views need not be the same, and consequently, naive deblurring approaches produce inconsistent left-right views and disrupt scene-consistent disparities. In this paper, we address this problem using Deep Learning and make three important contributions. First, we address the root cause of view-inconsistency in standard deblurring architectures using a Coherent Fusion Module. Second, we address an inherent problem in unconstrained DL deblurring that disrupts scene-consistent disparities by introducing a memory-efficient Adaptive Scale-space Approach. This signal processing formulation allows accommodation of different image-scales in the same network without increasing the number of parameters. Finally, we propose a module to address the Space-variant and Image-dependent nature of dynamic scene blur. We experimentally show that our proposed techniques have substantial practical merit.},
  archive      = {J_TIP},
  author       = {M. R. Mahesh Mohan and G. K. Nithin and A. N. Rajagopalan},
  doi          = {10.1109/TIP.2021.3072856},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4479-4491},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Deep dynamic scene deblurring for unconstrained dual-lens cameras},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Principled design and implementation of steerable detectors.
<em>TIP</em>, <em>30</em>, 4465–4478. (<a
href="https://doi.org/10.1109/TIP.2021.3072499">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We provide a complete pipeline for the detection of patterns of interest in an image. In our approach, the patterns are assumed to be adequately modeled by a known template, and are located at unknown positions and orientations that we aim at retrieving. We propose a continuous-domain additive image model, where the analyzed image is the sum of the patterns to localize and a background with self-similar isotropic power-spectrum. We are then able to compute the optimal filter fulfilling the SNR criterion based on one single template and background pair: it strongly responds to the template while being optimally decoupled from the background model. In addition, we constrain our filter to be steerable, which allows for a fast template detection together with orientation estimation. In practice, the implementation requires to discretize a continuous-domain formulation on polar grids, which is performed using quadratic radial B-splines. We demonstrate the practical usefulness of our method on a variety of template approximation and pattern detection experiments. We show that the detection performance drastically improves when we exploit the statistics of the background via its power-spectrum decay, which we refer to as spectral-shaping. The proposed scheme outperforms state-of-the-art steerable methods by up to 50\% of absolute detection performance.},
  archive      = {J_TIP},
  author       = {Julien Fageot and Virginie Uhlmann and Zsuzsanna Püspöki and Benjamin Beck and Michael Unser and Adrien Depeursinge},
  doi          = {10.1109/TIP.2021.3072499},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4465-4478},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Principled design and implementation of steerable detectors},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). UGC-VQA: Benchmarking blind video quality assessment for
user generated content. <em>TIP</em>, <em>30</em>, 4449–4464. (<a
href="https://doi.org/10.1109/TIP.2021.3072221">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years have witnessed an explosion of user-generated content (UGC) videos shared and streamed over the Internet, thanks to the evolution of affordable and reliable consumer capture devices, and the tremendous popularity of social media platforms. Accordingly, there is a great need for accurate video quality assessment (VQA) models for UGC/consumer videos to monitor, control, and optimize this vast content. Blind quality prediction of in-the-wild videos is quite challenging, since the quality degradations of UGC videos are unpredictable, complicated, and often commingled. Here we contribute to advancing the UGC-VQA problem by conducting a comprehensive evaluation of leading no-reference/blind VQA (BVQA) features and models on a fixed evaluation architecture, yielding new empirical insights on both subjective video quality studies and objective VQA model design. By employing a feature selection strategy on top of efficient BVQA models, we are able to extract 60 out of 763 statistical features used in existing methods to create a new fusion-based model, which we dub the VIDeo quality EVALuator (VIDEVAL), that effectively balances the trade-off between VQA performance and efficiency. Our experimental results show that VIDEVAL achieves state-of-the-art performance at considerably lower computational cost than other leading models. Our study protocol also defines a reliable benchmark for the UGC-VQA problem, which we believe will facilitate further research on deep learning-based VQA modeling, as well as perceptually-optimized efficient UGC video processing, transcoding, and streaming. To promote reproducible research and public evaluation, an implementation of VIDEVAL has been made available online: https://github.com/vztu/VIDEVAL.},
  archive      = {J_TIP},
  author       = {Zhengzhong Tu and Yilin Wang and Neil Birkbeck and Balu Adsumilli and Alan C. Bovik},
  doi          = {10.1109/TIP.2021.3072221},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4449-4464},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {UGC-VQA: Benchmarking blind video quality assessment for user generated content},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). PRA-net: Point relation-aware network for 3D point cloud
analysis. <em>TIP</em>, <em>30</em>, 4436–4448. (<a
href="https://doi.org/10.1109/TIP.2021.3072214">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning intra-region contexts and inter-region relations are two effective strategies to strengthen feature representations for point cloud analysis. However, unifying the two strategies for point cloud representation is not fully emphasized in existing methods. To this end, we propose a novel framework named Point Relation-Aware Network (PRA-Net), which is composed of an Intra-region Structure Learning (ISL) module and an Inter-region Relation Learning (IRL) module. The ISL module can dynamically integrate the local structural information into the point features, while the IRL module captures inter-region relations adaptively and efficiently via a differentiable region partition scheme and a representative point-based strategy. Extensive experiments on several 3D benchmarks covering shape classification, keypoint estimation, and part segmentation have verified the effectiveness and the generalization ability of PRA-Net. Code will be available at https://github.com/XiwuChen/PRA-Net.},
  archive      = {J_TIP},
  author       = {Silin Cheng and Xiwu Chen and Xinwei He and Zhe Liu and Xiang Bai},
  doi          = {10.1109/TIP.2021.3072214},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4436-4448},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {PRA-net: Point relation-aware network for 3D point cloud analysis},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Weakly-supervised salient object detection with saliency
bounding boxes. <em>TIP</em>, <em>30</em>, 4423–4435. (<a
href="https://doi.org/10.1109/TIP.2021.3071691">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel form of weak supervision for salient object detection (SOD) based on saliency bounding boxes, which are minimum rectangular boxes enclosing the salient objects. Based on this idea, we propose a novel weakly-supervised SOD method, by predicting pixel-level pseudo ground truth saliency maps from just saliency bounding boxes. Our method first takes advantage of the unsupervised SOD methods to generate initial saliency maps and addresses the over/under prediction problems, to obtain the initial pseudo ground truth saliency maps. We then iteratively refine the initial pseudo ground truth by learning a multi-task map refinement network with saliency bounding boxes. Finally, the final pseudo saliency maps are used to supervise the training of a salient object detector. Experimental results show that our method outperforms state-of-the-art weakly-supervised methods.},
  archive      = {J_TIP},
  author       = {Yuxuan Liu and Pengjie Wang and Ying Cao and Zijian Liang and Rynson W. H. Lau},
  doi          = {10.1109/TIP.2021.3071691},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4423-4435},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Weakly-supervised salient object detection with saliency bounding boxes},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A gated recurrent network with dual classification
assistance for smoke semantic segmentation. <em>TIP</em>, <em>30</em>,
4409–4422. (<a href="https://doi.org/10.1109/TIP.2021.3069318">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Smoke has semi-transparency property leading to highly complicated mixture of background and smoke. Sparse or small smoke is visually inconspicuous, and its boundary is often ambiguous. These reasons result in a very challenging task of separating smoke from a single image. To solve these problems, we propose a Classification-assisted Gated Recurrent Network (CGRNet) for smoke semantic segmentation. To discriminate smoke and smoke-like objects, we present a smoke segmentation strategy with dual classification assistance. Our classification module outputs two prediction probabilities for smoke. The first assistance is to use one probability to explicitly regulate the segmentation module for accuracy improvement by supervising a cross-entropy classification loss. The second one is to multiply the segmentation result by another probability for further refinement. This dual classification assistance greatly improves performance at image level. In the segmentation module, we design an Attention Convolutional GRU module (Att-ConvGRU) to learn the long-range context dependence of features. To perceive small or inconspicuous smoke, we design a Multi-scale Context Contrasted Local Feature structure (MCCL) and a Dense Pyramid Pooling Module (DPPM) for improving the representation ability of our network. Extensive experiments validate that our method significantly outperforms existing state-of-art algorithms on smoke datasets, and also obtain satisfactory results on challenging images with inconspicuous smoke and smoke-like objects.},
  archive      = {J_TIP},
  author       = {Feiniu Yuan and Lin Zhang and Xue Xia and Qinghua Huang and Xuelong Li},
  doi          = {10.1109/TIP.2021.3069318},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4409-4422},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A gated recurrent network with dual classification assistance for smoke semantic segmentation},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bilateral upsampling network for single image
super-resolution with arbitrary scaling factors. <em>TIP</em>,
<em>30</em>, 4395–4408. (<a
href="https://doi.org/10.1109/TIP.2021.3071708">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single Image Super-Resolution (SISR) is essential for many computer vision tasks. In some real-world applications, such as object recognition and image classification, the captured image size can be arbitrary while the required image size is fixed, which necessitates SISR with arbitrary scaling factors. It is a challenging problem to take a single model to accomplish the SISR task under arbitrary scaling factors. To solve that problem, this paper proposes a bilateral upsampling network which consists of a bilateral upsampling filter and a depthwise feature upsampling convolutional layer. The bilateral upsampling filter is made up of two upsampling filters, including a spatial upsampling filter and a range upsampling filter. With the introduction of the range upsampling filter, the weights of the bilateral upsampling filter can be adaptively learned under different scaling factors and different pixel values. The output of the bilateral upsampling filter is then provided to the depthwise feature upsampling convolutional layer, which upsamples the low-resolution (LR) feature map to the high-resolution (HR) feature space depthwisely and well recovers the structural information of the HR feature map. The depthwise feature upsampling convolutional layer can not only efficiently reduce the computational cost of the weight prediction of the bilateral upsampling filter, but also accurately recover the textual details of the HR feature map. Experiments on benchmark datasets demonstrate that the proposed bilateral upsampling network can achieve better performance than some state-of-the-art SISR methods.},
  archive      = {J_TIP},
  author       = {Menglei Zhang and Qiang Ling},
  doi          = {10.1109/TIP.2021.3071708},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4395-4408},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Bilateral upsampling network for single image super-resolution with arbitrary scaling factors},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning multi-modal nonlinear embeddings: Performance
bounds and an algorithm. <em>TIP</em>, <em>30</em>, 4384–4394. (<a
href="https://doi.org/10.1109/TIP.2021.3071688">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While many approaches exist in the literature to learn low-dimensional representations for data collections in multiple modalities, the generalizability of multi-modal nonlinear embeddings to previously unseen data is a rather overlooked subject. In this work, we first present a theoretical analysis of learning multi-modal nonlinear embeddings in a supervised setting. Our performance bounds indicate that for successful generalization in multi-modal classification and retrieval problems, the regularity of the interpolation functions extending the embedding to the whole data space is as important as the between-class separation and cross-modal alignment criteria. We then propose a multi-modal nonlinear representation learning algorithm that is motivated by these theoretical findings, where the embeddings of the training samples are optimized jointly with the Lipschitz regularity of the interpolators. Experimental comparison to recent multi-modal and single-modal learning algorithms suggests that the proposed method yields promising performance in multi-modal image classification and cross-modal image-text retrieval applications.},
  archive      = {J_TIP},
  author       = {Semih Kaya and Elif Vural},
  doi          = {10.1109/TIP.2021.3071688},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4384-4394},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Learning multi-modal nonlinear embeddings: Performance bounds and an algorithm},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DAN: Deep-attention network for 3D shape recognition.
<em>TIP</em>, <em>30</em>, 4371–4383. (<a
href="https://doi.org/10.1109/TIP.2021.3071687">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the wide applications in a rapidly increasing number of different fields, 3D shape recognition has become a hot topic in the computer vision field. Many approaches have been proposed in recent years. However, there remain huge challenges in two aspects: exploring the effective representation of 3D shapes and reducing the redundant complexity of 3D shapes. In this paper, we propose a novel deep-attention network (DAN) for 3D shape representation based on multiview information. More specifically, we introduce the attention mechanism to construct a deep multiattention network that has advantages in two aspects: 1) information selection, in which DAN utilizes the self-attention mechanism to update the feature vector of each view, effectively reducing the redundant information, and 2) information fusion, in which DAN applies attention mechanism that can save more effective information by considering the correlations among views. Meanwhile, deep network structure can fully consider the correlations to continuously fuse effective information. To validate the effectiveness of our proposed method, we conduct experiments on the public 3D shape datasets: ModelNet40, ModelNet10, and ShapeNetCore55. Experimental results and comparison with state-of-the-art methods demonstrate the superiority of our proposed method. Code is released on https://github.com/RiDang/DANN.},
  archive      = {J_TIP},
  author       = {Weizhi Nie and Yue Zhao and Dan Song and Yue Gao},
  doi          = {10.1109/TIP.2021.3071687},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4371-4383},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {DAN: Deep-attention network for 3D shape recognition},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Exploring chromatic aberration and defocus blur for relative
depth estimation from monocular hyperspectral image. <em>TIP</em>,
<em>30</em>, 4357–4370. (<a
href="https://doi.org/10.1109/TIP.2021.3071682">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates spectral chromatic and spatial defocus aberration in a monocular hyperspectral image (HSI) and proposes methods on how these cues can be utilized for relative depth estimation. The main aim of this work is to develop a framework by exploring intrinsic and extrinsic reflectance properties in HSI that can be useful for depth estimation. Depth estimation from a monocular image is a challenging task. An additional level of difficulty is added due to low resolution and noises in hyperspectral data. Our contribution to handling depth estimation in HSI is threefold. Firstly, we propose that change in focus across band images of HSI due to chromatic aberration and band-wise defocus blur can be integrated for depth estimation. Novel methods are developed to estimate sparse depth maps based on different integration models. Secondly, by adopting manifold learning, an effective objective function is developed to combine all sparse depth maps into a final optimized sparse depth map. Lastly, a new dense depth map generation approach is proposed, which extrapolate sparse depth cues by using material-based properties on graph Laplacian. Experimental results show that our methods successfully exploit HSI properties to generate depth cues. We also compare our method with state-of-the-art RGB image-based approaches, which shows that our methods produce better sparse and dense depth maps than those from the benchmark methods.},
  archive      = {J_TIP},
  author       = {Ali Zia and Jun Zhou and Yongsheng Gao},
  doi          = {10.1109/TIP.2021.3071682},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4357-4370},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Exploring chromatic aberration and defocus blur for relative depth estimation from monocular hyperspectral image},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hyperspectral texture metrology based on joint probability
of spectral and spatial distribution. <em>TIP</em>, <em>30</em>,
4341–4356. (<a href="https://doi.org/10.1109/TIP.2021.3071557">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Texture characterization from the metrological point of view is addressed in order to establish a physically relevant and directly interpretable feature. In this regard, a generic formulation is proposed to simultaneously capture the spectral and spatial complexity in hyperspectral images. The feature, named relative spectral difference occurrence matrix (RSDOM) is thus constructed in a multireference, multidirectional, and multiscale context. As validation, its performance is assessed in three versatile tasks. In texture classification on HyTexiLa, content-based image retrieval (CBIR) on ICONES-HSI, and land cover classification on Salinas, RSDOM registers 98.5\% accuracy, 80.3\% precision (for the top 10 retrieved images), and 96.0\% accuracy (after post-processing) respectively, outcompeting GLCM, Gabor filter, LBP, SVM, CCF, CNN, and GCN. Analysis shows the advantage of RSDOM in terms of feature size (a mere 126, 30, and 20 scalars using GMM in order of the three tasks) as well as metrological validity in texture representation regardless of the spectral range, resolution, and number of bands.},
  archive      = {J_TIP},
  author       = {Rui Jian Chu and Noël Richard and Hermine Chatoux and Christine Fernandez-Maloigne and Jon Yngve Hardeberg},
  doi          = {10.1109/TIP.2021.3071557},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4341-4356},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Hyperspectral texture metrology based on joint probability of spectral and spatial distribution},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Together recognizing, localizing and summarizing actions in
egocentric videos. <em>TIP</em>, <em>30</em>, 4330–4340. (<a
href="https://doi.org/10.1109/TIP.2021.3070732">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Analysis of egocentric video has recently drawn attention of researchers in the computer vision as well as multimedia communities. In this paper, we propose a weakly supervised superpixel level joint framework for localization, recognition and summarization of actions in an egocentric video. We first recognize and localize single as well as multiple action(s) in each frame of an egocentric video and then construct a summary of these detected actions. The superpixel level solution helps in precise localization of actions in addition to improving the recognition accuracy. Superpixels are extracted within the central regions of the egocentric video frames; these central regions being determined through a previously developed center-surround model. A sparse spatio-temporal video representation graph is constructed in the deep feature space with the superpixels as nodes. A weakly supervised solution using random walks yields action labels for each superpixel. After determining action label(s) for each frame from its constituent superpixels, we apply a fractional knapsack type formulation for obtaining a summary (of actions). Experimental comparisons on publicly available ADL, GTEA, EGTEA Gaze+, EgoGesture, and EPIC-Kitchens datasets show the effectiveness of the proposed solution.},
  archive      = {J_TIP},
  author       = {Abhimanyu Sahu and Ananda S. Chowdhury},
  doi          = {10.1109/TIP.2021.3070732},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4330-4340},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Together recognizing, localizing and summarizing actions in egocentric videos},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). VMAN: A virtual mainstay alignment network for transductive
zero-shot learning. <em>TIP</em>, <em>30</em>, 4316–4329. (<a
href="https://doi.org/10.1109/TIP.2021.3070231">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transductive zero-shot learning (TZSL) extends conventional ZSL by leveraging (unlabeled) unseen images for model training. A typical method for ZSL involves learning embedding weights from the feature space to the semantic space. However, the learned weights in most existing methods are dominated by seen images, and can thus not be adapted to unseen images very well. In this paper, to align the (embedding) weights for better knowledge transfer between seen/unseen classes, we propose the virtual mainstay alignment network (VMAN), which is tailored for the transductive ZSL task. Specifically, VMAN is casted as a tied encoder-decoder net, thus only one linear mapping weights need to be learned. To explicitly learn the weights in VMAN, for the first time in ZSL, we propose to generate virtual mainstay (VM) samples for each seen class, which serve as new training data and can prevent the weights from being shifted to seen images, to some extent. Moreover, a weighted reconstruction scheme is proposed and incorporated into the model training phase, in both the semantic/feature spaces. In this way, the manifold relationships of the VM samples are well preserved. To further align the weights to adapt to more unseen images, a novel instance-category matching regularization is proposed for model re-training. VMAN is thus modeled as a nested minimization problem and is solved by a Taylor approximate optimization paradigm. In comprehensive evaluations on four benchmark datasets, VMAN achieves superior performances under the (Generalized) TZSL setting.},
  archive      = {J_TIP},
  author       = {Guo-Sen Xie and Xu-Yao Zhang and Yazhou Yao and Zheng Zhang and Fang Zhao and Ling Shao},
  doi          = {10.1109/TIP.2021.3070231},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4316-4329},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {VMAN: A virtual mainstay alignment network for transductive zero-shot learning},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dynamic image quantization using leaky integrate-and-fire
neurons. <em>TIP</em>, <em>30</em>, 4305–4315. (<a
href="https://doi.org/10.1109/TIP.2021.3070193">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a novel coding/decoding mechanism that mimics one of the most important properties of the human visual system: its ability to enhance the visual perception quality in time. In other words, the brain takes advantage of time to process and clarify the details of the visual scene. This characteristic is yet to be considered by the state-of-the-art quantization mechanisms that process the visual information regardless the duration of time it appears in the visual scene. We propose a compression architecture built of neuroscience models; it first uses the leaky integrate-and-fire (LIF) model to transform the visual stimulus into a spike train and then it combines two different kinds of spike interpretation mechanisms (SIM), the time-SIM and the rate-SIM for the encoding of the spike train. The time-SIM allows a high quality interpretation of the neural code and the rate-SIM allows a simple decoding mechanism by counting the spikes. For that reason, the proposed mechanisms is called Dual-SIM quantizer (Dual-SIMQ). We show that (i) the time-dependency of Dual-SIMQ automatically controls the reconstruction accuracy of the visual stimulus, (ii) the numerical comparison of Dual-SIMQ to the state-of-the-art shows that the performance of the proposed algorithm is similar to the uniform quantization schema while it approximates the optimal behavior of the non-uniform quantization schema and (iii) from the perceptual point of view the reconstruction quality using the Dual-SIMQ is higher than the state-of-the-art.},
  archive      = {J_TIP},
  author       = {Effrosyni Doutsi and Lionel Fillatre and Marc Antonini and Panagiotis Tsakalides},
  doi          = {10.1109/TIP.2021.3070193},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4305-4315},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Dynamic image quantization using leaky integrate-and-fire neurons},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Enhancing ISAR image efficiently via convolutional
reweighted l1 minimization. <em>TIP</em>, <em>30</em>, 4291–4304. (<a
href="https://doi.org/10.1109/TIP.2021.3070442">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inverse synthetic aperture radar (ISAR) imaging for the sparse aperture data is affected by considerable artifacts, because under-sampling of data produces high-level grating and side lobes. Noting the ISAR image generally exhibits strong sparsity, it is often obtained by sparse signal recovery (SSR) in case of sparse aperture. The image obtained by SSR, however, is often dominated by strong isolated scatterers, resulting in difficulty to recognize the structure of target. This paper proposes a novel approach to enhance the ISAR image obtained from the sparse aperture data. Although the scatterers of target are isolated in the ISAR image, they should be associated with the neighborhood to reflect some intrinsic structural information of the target. A convolutional reweighted l 1 minimization model, therefore, is proposed to model the structural sparsity of ISAR image. Specifically, the ISAR image is reconstructed by solving a sequence of reweighted l 1 problems, where the weight of each pixel used for the next iteration is calculated from the convolution of its neighbor values in the current solution. The problem is solved by the alternating direction of multipliers (ADMM) and linearized approximation, respectively, to improve the computational efficiency. Experimental results based on both simulated and measured data validate that the proposed algorithm is effective to enhance the ISAR image, robust to noise, and more impressively, very efficient to implement.},
  archive      = {J_TIP},
  author       = {Shuanghui Zhang and Yongxiang Liu and Xiang Li and Dewen Hu},
  doi          = {10.1109/TIP.2021.3070442},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4291-4304},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Enhancing ISAR image efficiently via convolutional reweighted l1 minimization},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hand pose understanding with large-scale photo-realistic
rendering dataset. <em>TIP</em>, <em>30</em>, 4275–4290. (<a
href="https://doi.org/10.1109/TIP.2021.3070439">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hand pose understanding is essential to applications such as human computer interaction and augmented reality. Recently, deep learning based methods achieve great progress in this problem. However, the lack of high-quality and large-scale dataset prevents the further improvement of hand pose related tasks such as 2D/3D hand pose from color and depth from color. In this paper, we develop a large-scale and high-quality synthetic dataset, PBRHand. The dataset contains millions of photo-realistic rendered hand images and various ground truths including pose, semantic segmentation, and depth. Based on the dataset, we firstly investigate the effect of rendering methods and used databases on the performance of three hand pose related tasks: 2D/3D hand pose from color, depth from color and 3D hand pose from depth. This study provides insights that photo-realistic rendering dataset is worthy of synthesizing and shows that our new dataset can improve the performance of the state-of-the-art on these tasks. This synthetic data also enables us to explore multi-task learning, while it is expensive to have all the ground truth available on real data. Evaluations show that our approach can achieve state-of-the-art or competitive performance on several public datasets.},
  archive      = {J_TIP},
  author       = {Xiaoming Deng and Yinda Zhang and Jian Shi and Yuying Zhu and Dachuan Cheng and Dexin Zuo and Zhaopeng Cui and Ping Tan and Liang Chang and Hongan Wang},
  doi          = {10.1109/TIP.2021.3070439},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4275-4290},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Hand pose understanding with large-scale photo-realistic rendering dataset},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Latent correlation representation learning for brain tumor
segmentation with missing MRI modalities. <em>TIP</em>, <em>30</em>,
4263–4274. (<a href="https://doi.org/10.1109/TIP.2021.3070752">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Magnetic Resonance Imaging (MRI) is a widely used imaging technique to assess brain tumor. Accurately segmenting brain tumor from MR images is the key to clinical diagnostics and treatment planning. In addition, multi-modal MR images can provide complementary information for accurate brain tumor segmentation. However, it&#39;s common to miss some imaging modalities in clinical practice. In this paper, we present a novel brain tumor segmentation algorithm with missing modalities. Since it exists a strong correlation between multi-modalities, a correlation model is proposed to specially represent the latent multi-source correlation. Thanks to the obtained correlation representation, the segmentation becomes more robust in the case of missing modality. First, the individual representation produced by each encoder is used to estimate the modality independent parameter. Then, the correlation model transforms all the individual representations to the latent multi-source correlation representations. Finally, the correlation representations across modalities are fused via attention mechanism into a shared representation to emphasize the most important features for segmentation. We evaluate our model on BraTS 2018 and BraTS 2019 dataset, it outperforms the current state-of-the-art methods and produces robust results when one or more modalities are missing.},
  archive      = {J_TIP},
  author       = {Tongxue Zhou and Stéphane Canu and Pierre Vera and Su Ruan},
  doi          = {10.1109/TIP.2021.3070752},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4263-4274},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Latent correlation representation learning for brain tumor segmentation with missing MRI modalities},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Few-shot common-object reasoning using common-centric
localization network. <em>TIP</em>, <em>30</em>, 4253–4262. (<a
href="https://doi.org/10.1109/TIP.2021.3070733">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the few-shot common-localization task, given few support images without bounding box annotations at each episode, the goal is to localize the common object in the query image of unseen categories. The few-shot common-localization task involves common object reasoning from the given images, predicting the spatial locations of the object with different shapes, sizes, and orientations. In this work, we propose a common-centric localization (CCL) network for few-shot common-localization. The motivation of our common-centric localization network is to learn the common object features by dynamic feature relation reasoning via a graph convolutional network with conditional feature aggregation. First, we propose a local common object region generation pipeline to reduce background noises due to feature misalignment. Each support image predicts more accurate object spatial locations by replacing the query with the images in the support set. Second, we introduce a graph convolutional network with dynamic feature transformation to enforce the common object reasoning. To enhance the discriminability during feature matching and enable a better generalization in unseen scenarios, we leverage a conditional feature encoding function to alter visual features according to the input query adaptively. Third, we introduce a common-centric relation structure to model the correlation between the common features and the query image feature. The generated common features guide the query image feature towards a more common object-related representation. We evaluate our common-centric localization network on four datasets, i.e., CL-VOC-07, CL-VOC-12, CL-COCO, CL-VID. We obtain significant improvements compared to state-of-the-art. Our quantitative results confirm the effectiveness of our network.},
  archive      = {J_TIP},
  author       = {Linchao Zhu and Hehe Fan and Yawei Luo and Mingliang Xu and Yi Yang},
  doi          = {10.1109/TIP.2021.3070733},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4253-4262},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Few-shot common-object reasoning using common-centric localization network},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Rethinking image salient object detection: Object-level
semantic saliency reranking first, pixelwise saliency refinement later.
<em>TIP</em>, <em>30</em>, 4238–4252. (<a
href="https://doi.org/10.1109/TIP.2021.3068649">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human attention is an interactive activity between our visual system and our brain, using both low-level visual stimulus and high-level semantic information. Previous image salient object detection (SOD) studies conduct their saliency predictions via a multitask methodology in which pixelwise saliency regression and segmentation-like saliency refinement are conducted simultaneously. However, this multitask methodology has one critical limitation: the semantic information embedded in feature backbones might be degenerated during the training process. Our visual attention is determined mainly by semantic information, which is evidenced by our tendency to pay more attention to semantically salient regions even if these regions are not the most perceptually salient at first glance. This fact clearly contradicts the widely used multitask methodology mentioned above. To address this issue, this paper divides the SOD problem into two sequential steps. First, we devise a lightweight, weakly supervised deep network to coarsely locate the semantically salient regions. Next, as a postprocessing refinement, we selectively fuse multiple off-the-shelf deep models on the semantically salient regions identified by the previous step to formulate a pixelwise saliency map. Compared with the state-of-the-art (SOTA) models that focus on learning the pixelwise saliency in single images using only perceptual clues, our method aims at investigating the object-level semantic ranks between multiple images, of which the methodology is more consistent with the human attention mechanism. Our method is simple yet effective, and it is the first attempt to consider salient object detection as mainly an object-level semantic reranking problem.},
  archive      = {J_TIP},
  author       = {Guangxiao Ma and Shuai Li and Chenglizhao Chen and Aimin Hao and Hong Qin},
  doi          = {10.1109/TIP.2021.3068649},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4238-4252},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Rethinking image salient object detection: Object-level semantic saliency reranking first, pixelwise saliency refinement later},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Semantic perceptual image compression with a laplacian
pyramid of convolutional networks. <em>TIP</em>, <em>30</em>, 4225–4237.
(<a href="https://doi.org/10.1109/TIP.2021.3065244">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The existing image compression methods usually choose or optimize low-level representation manually. Actually, these methods struggle for the texture restoration at low bit rates. Recently, deep neural network (DNN)-based image compression methods have achieved impressive results. To achieve better perceptual quality, generative models are widely used, especially generative adversarial networks (GAN). However, training GAN is intractable, especially for high-resolution images, with the challenges of unconvincing reconstructions and unstable training. To overcome these problems, we propose a novel DNN-based image compression framework in this paper. The key point is decomposing an image into multi-scale sub-images using the proposed Laplacian pyramid based multi-scale networks. For each pyramid scale, we train a specific DNN to exploit the compressive representation. Meanwhile, each scale is optimized with different aspects, including pixel, semantics, distribution and entropy, for a good “rate-distortion-perception” trade-off. By independently optimizing each pyramid scale, we make each stage manageable and make each sub-image plausible. Experimental results demonstrate that our method achieves state-of-the-art performance, with advantages over existing methods in providing improved visual quality. Additionally, a better performance in the down-stream visual analysis tasks which are conducted on the reconstructed images, validates the excellent semantics-preserving ability of the proposed method.},
  archive      = {J_TIP},
  author       = {Juan Wang and Yiping Duan and Xiaoming Tao and Mai Xu and Jianhua Lu},
  doi          = {10.1109/TIP.2021.3065244},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4225-4237},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Semantic perceptual image compression with a laplacian pyramid of convolutional networks},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Incremental generative occlusion adversarial suppression
network for person ReID. <em>TIP</em>, <em>30</em>, 4212–4224. (<a
href="https://doi.org/10.1109/TIP.2021.3070182">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person re-identification (re-id) suffers from the significant challenge of occlusion, where an image contains occlusions and less discriminative pedestrian information. However, certain work consistently attempts to design complex modules to capture implicit information (including human pose landmarks, mask maps, and spatial information). The network, consequently, focuses on discriminative features learning on human non-occluded body regions and realizes effective matching under spatial misalignment. Few studies have focused on data augmentation, given that existing single-based data augmentation methods bring limited performance improvement. To address the occlusion problem, we propose a novel Incremental Generative Occlusion Adversarial Suppression (IGOAS) network. It consists of 1) an incremental generative occlusion block, generating easy-to-hard occlusion data, that makes the network more robust to occlusion by gradually learning harder occlusion instead of hardest occlusion directly. And 2) a global-adversarial suppression (G&amp;A) framework with a global branch and an adversarial suppression branch. The global branch extracts steady global features of the images. The adversarial suppression branch, embedded with two occlusion suppression module, minimizes the generated occlusion’s response and strengthens attentive feature representation on human non-occluded body regions. Finally, we get a more discriminative pedestrian feature descriptor by concatenating two branches’ features, which is robust to the occlusion problem. The experiments on the occluded dataset show the competitive performance of IGOAS. On Occluded-DukeMTMC, it achieves 60.1\% Rank-1 accuracy and 49.4\% mAP.},
  archive      = {J_TIP},
  author       = {Cairong Zhao and Xinbi Lv and Shuguang Dou and Shanshan Zhang and Jun Wu and Liang Wang},
  doi          = {10.1109/TIP.2021.3070182},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4212-4224},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Incremental generative occlusion adversarial suppression network for person ReID},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Combining progressive rethinking and collaborative learning:
A deep framework for in-loop filtering. <em>TIP</em>, <em>30</em>,
4198–4211. (<a href="https://doi.org/10.1109/TIP.2021.3068638">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we aim to address issues of (1) joint spatial-temporal modeling and (2) side information injection for deep-learning based in-loop filter. For (1), we design a deep network with both progressive rethinking and collaborative learning mechanisms to improve quality of the reconstructed intra-frames and inter-frames, respectively. For intra coding, a Progressive Rethinking Network (PRN) is designed to simulate the human decision mechanism for effective spatial modeling. Our designed block introduces an additional inter-block connection to bypass a high-dimensional informative feature before the bottleneck module across blocks to review the complete past memorized experiences and rethinks progressively. For inter coding, the current reconstructed frame interacts with reference frames (peak quality frame and the nearest adjacent frame) collaboratively at the feature level. For (2), we extract both intra-frame and inter-frame side information for better context modeling. A coarse-to-fine partition map based on HEVC partition trees is built as the intra-frame side information. Furthermore, the warped features of the reference frames are offered as the inter-frame side information. Our PRN with intra-frame side information provides 9.0\% BD-rate reduction on average compared to HEVC baseline under All-intra (AI) configuration. While under Low-Delay B (LDB), Low-Delay P (LDP) and Random Access (RA) configuration, our PRN with inter-frame side information provides 9.0\%, 10.6\% and 8.0\% BD-rate reduction on average respectively. Our project webpage is https://dezhao-wang.github.io/PRN-v2/.},
  archive      = {J_TIP},
  author       = {Dezhao Wang and Sifeng Xia and Wenhan Yang and Jiaying Liu},
  doi          = {10.1109/TIP.2021.3068638},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4198-4211},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Combining progressive rethinking and collaborative learning: A deep framework for in-loop filtering},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-stream attention-aware graph convolution network for
video salient object detection. <em>TIP</em>, <em>30</em>, 4183–4197.
(<a href="https://doi.org/10.1109/TIP.2021.3070200">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in deep convolution neural networks (CNNs) boost the development of video salient object detection (SOD), and many remarkable deep-CNNs video SOD models have been proposed. However, many existing deep-CNNs video SOD models still suffer from coarse boundaries of the salient object, which may be attributed to the loss of high-frequency information. The traditional graph-based video SOD models can preserve object boundaries well by conducting superpixels/supervoxels segmentation in advance, but they perform weaker in highlighting the whole object than the latest deep-CNNs models, limited by heuristic graph clustering algorithms. To tackle this problem, we find a new way to address this issue under the framework of graph convolution networks (GCNs), taking advantage of graph model and deep neural network. Specifically, a superpixel-level spatiotemporal graph is first constructed among multiple frame-pairs by exploiting the motion cues implied in the frame-pairs. Then the graph data is imported into the devised multi-stream attention-aware GCN, where a novel Edge-Gated graph convolution (GC) operation is proposed to boost the saliency information aggregation on the graph data. A novel attention module is designed to encode the spatiotemporal sematic information via adaptive selection of graph nodes and fusion of the static-specific and the motion-specific graph embedding. Finally, a smoothness-aware regularization term is proposed to enhance the uniformity of salient object. Graph nodes (superpixels) inherently belonging to the same class will be ideally clustered together in the learned embedding space. Extensive experiments have been conducted on three widely used datasets. Compared with fourteen state-of-the-art video SOD models, our proposed method can well retain the salient object boundaries and possess a strong learning ability, which shows that this work is a good practice for designing GCNs for video SOD.},
  archive      = {J_TIP},
  author       = {Mingzhu Xu and Ping Fu and Bing Liu and Junbao Li},
  doi          = {10.1109/TIP.2021.3070200},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4183-4197},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Multi-stream attention-aware graph convolution network for video salient object detection},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). A sparse representation based joint demosaicing method for
single-chip polarized color sensor. <em>TIP</em>, <em>30</em>,
4171–4182. (<a href="https://doi.org/10.1109/TIP.2021.3069190">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The emergence of the single-chip polarized color sensor now allows for simultaneously capturing chromatic and polarimetric information of the scene on a monochromatic image plane. However, unlike the usual camera with an embedded demosaicing method, the latest polarized color camera is not delivered with an in-built demosaicing tool. For demosaicing, the users have to down-sample the captured images or to use traditional interpolation techniques. Neither of them can perform well since the polarization and color are interdependent. Therefore, joint chromatic and polarimetric demosaicing is the key to obtaining high-quality polarized color images. In this paper, we propose a joint chromatic and polarimetric demosaicing model to address this challenging problem. Instead of mechanically demosaicing for the multi-channel polarized color image, we further present a sparse representation-based optimization strategy that utilizes chromatic information and polarimetric information to jointly optimize the model. To avoid the interaction between color and polarization during demosaicing, we separately construct the corresponding dictionaries. We also build an optical data acquisition system to collect a dataset, which contains various sources of polarization, such as illumination, reflectance and birefringence. Results of both qualitative and quantitative experiments have shown that our method is capable of faithfully recovering full RGB information of four polarization angles for each pixel from a single mosaic input image. Moreover, the proposed method can perform well not only on the synthetic data but the real captured data.},
  archive      = {J_TIP},
  author       = {Sijia Wen and Yinqiang Zheng and Feng Lu},
  doi          = {10.1109/TIP.2021.3069190},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4171-4182},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A sparse representation based joint demosaicing method for single-chip polarized color sensor},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Features guided face super-resolution via hybrid model of
deep learning and random forests. <em>TIP</em>, <em>30</em>, 4157–4170.
(<a href="https://doi.org/10.1109/TIP.2021.3069554">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face hallucination or super-resolution is a practical application of general image super-resolution which has been recently studied by many researchers. The challenge of good face hallucination comes from a variety of poses, illuminations, facial expressions, and other degradations. In many proposed methods, researchers resolve it by using a generative neural network to reduce the perceptual loss so we can generate a photo-realistic image. The problem is that researchers usually overlook the fidelity of the super-resolved image which could affect further facial image processing. Meanwhile, many CNN based approaches cascade multiple networks to extract facial prior information to improve super-resolution quality. Because of the end-to-end design, the details are missing for investigation. In this paper, we combine new techniques in convolutional neural network and random forests to a Hierarchical CNN based Random Forests (HCRF) approach for face super-resolution in a coarse-to-fine manner. In the proposed approach, we focus on a general approach that can handle facial images with various conditions without pre-processing. To the best of our knowledge, this is the first paper that combines the advantages of deep learning with random forests for face super-resolution. To achieve superior performance, we propose two novel CNN models for coarse facial image super-resolution and segmentation and then apply new random forests to target on local facial features refinement making use of the segmentation results. Extensive benchmark experiments on subjective and objective evaluation show that HCRF can achieve comparable speed and competitive performance compared with state-of-the-art super-resolution approaches for very low-resolution images.},
  archive      = {J_TIP},
  author       = {Zhi-Song Liu and Wan-Chi Siu and Yui-Lam Chan},
  doi          = {10.1109/TIP.2021.3069554},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4157-4170},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Features guided face super-resolution via hybrid model of deep learning and random forests},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Flexible multi-view unsupervised graph embedding.
<em>TIP</em>, <em>30</em>, 4143–4156. (<a
href="https://doi.org/10.1109/TIP.2021.3062692">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Faced with the increasing data diversity and dimensionality, multi-view dimensionality reduction has been an important technique in computer vision, data mining and multi-media applications. Since collecting labeled data is difficult and costly, unsupervised learning is of great significance. Generally, it is crucial to explore the complementarity or independence of different feature spaces in multi-view learning. How to find a low-dimensional subspace to preserve the intrinsic structure of original unlabeled high-dimensional multi-view data is still challenging. In addition, noises and outliers always appear in real data. In this study, we propose a novel model called flexible multi-view unsupervised graph embedding (FMUGE). A flexible regression residual term is introduced so that the strict linear mapping is relaxed, new-coming data and noises are better handled, and the raw data negotiate with the learned low-dimensional representation in the procedure. To ensure the consistency among multiple views, FMUGE adaptively weights different features and fuses them to get an optimal multi-view consensus similarity graph, which assists high-quality graph embedding. We propose an efficient alternating iterative algorithm to optimize the proposed model. Finally, experimental results on synthetic and benchmark datasets show the significant improvement of FMUGE over the state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Bin Zhang and Qianyao Qiang and Fei Wang and Feiping Nie},
  doi          = {10.1109/TIP.2021.3062692},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4143-4156},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Flexible multi-view unsupervised graph embedding},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep shearlet residual learning network for single image
super-resolution. <em>TIP</em>, <em>30</em>, 4129–4142. (<a
href="https://doi.org/10.1109/TIP.2021.3069317">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, the residual learning strategy has been integrated into the convolutional neural network (CNN) for single image super-resolution (SISR), where the CNN is trained to estimate the residual images. Recognizing that a residual image usually consists of high-frequency details and exhibits cartoon-like characteristics, in this paper, we propose a deep shearlet residual learning network (DSRLN) to estimate the residual images based on the shearlet transform. The proposed network is trained in the shearlet transform-domain which provides an optimal sparse approximation of the cartoon-like image. Specifically, to address the large statistical variation among the shearlet coefficients, a dual-path training strategy and a data weighting technique are proposed. Extensive evaluations on general natural image datasets as well as remote sensing image datasets show that the proposed DSRLN scheme achieves close results in PSNR to the state-of-the-art deep learning methods, using much less network parameters.},
  archive      = {J_TIP},
  author       = {Tianyu Geng and Xiao-Yang Liu and Xiaodong Wang and Guiling Sun},
  doi          = {10.1109/TIP.2021.3069317},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4129-4142},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Deep shearlet residual learning network for single image super-resolution},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Light field super-resolution via adaptive feature remixing.
<em>TIP</em>, <em>30</em>, 4114–4128. (<a
href="https://doi.org/10.1109/TIP.2021.3069291">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel light field super-resolution algorithm to improve the spatial and angular resolutions of light field images is proposed in this work. We develop spatial and angular super-resolution (SR) networks, which can faithfully interpolate images in the spatial and angular domains regardless of the angular coordinates. For each input image, we feed adjacent images into the SR networks to extract multi-view features using a trainable disparity estimator. We concatenate the multi-view features and remix them through the proposed adaptive feature remixing (AFR) module, which performs channel-wise pooling. Finally, the remixed feature is used to augment the spatial or angular resolution. Experimental results demonstrate that the proposed algorithm outperforms the state-of-the-art algorithms on various light field datasets. The source codes and pre-trained models are available at https://github.com/keunsoo-ko/ LFSR-AFR},
  archive      = {J_TIP},
  author       = {Keunsoo Ko and Yeong Jun Koh and Soonkeun Chang and Chang-Su Kim},
  doi          = {10.1109/TIP.2021.3069291},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4114-4128},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Light field super-resolution via adaptive feature remixing},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Designing interpretable recurrent neural networks for video
reconstruction via deep unfolding. <em>TIP</em>, <em>30</em>, 4099–4113.
(<a href="https://doi.org/10.1109/TIP.2021.3069296">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep unfolding methods design deep neural networks as learned variations of optimization algorithms through the unrolling of their iterations. These networks have been shown to achieve faster convergence and higher accuracy than the original optimization methods. In this line of research, this paper presents novel interpretable deep recurrent neural networks (RNNs), designed by the unfolding of iterative algorithms that solve the task of sequential signal reconstruction (in particular, video reconstruction). The proposed networks are designed by accounting that video frames’ patches have a sparse representation and the temporal difference between consecutive representations is also sparse. Specifically, we design an interpretable deep RNN (coined reweighted-RNN) by unrolling the iterations of a proximal method that solves a reweighted version of the $\ell _{1}$ - $\ell _{1}$ minimization problem. Due to the underlying minimization model, our reweighted-RNN has a different thresholding function (alias, different activation function) for each hidden unit in each layer. In this way, it has higher network expressivity than existing deep unfolding RNN models. We also present the derivative $\ell _{1}$ - $\ell _{1}$ -RNN model, which is obtained by unfolding a proximal method for the $\ell _{1}$ - $\ell _{1}$ minimization problem. We apply the proposed interpretable RNNs to the task of video frame reconstruction from low-dimensional measurements, that is, sequential video frame reconstruction. The experimental results on various datasets demonstrate that the proposed deep RNNs outperform various RNN models.},
  archive      = {J_TIP},
  author       = {Huynh Van Luong and Boris Joukovsky and Nikos Deligiannis},
  doi          = {10.1109/TIP.2021.3069296},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4099-4113},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Designing interpretable recurrent neural networks for video reconstruction via deep unfolding},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SC-RPN: A strong correlation learning framework for region
proposal. <em>TIP</em>, <em>30</em>, 4084–4098. (<a
href="https://doi.org/10.1109/TIP.2021.3069547">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current state-of-the-art two-stage detectors heavily rely on region proposals to guide the accurate detection for objects. In previous region proposal approaches, the interaction between different functional modules is correlated weakly, which limits or decreases the performance of region proposal approaches. In this paper, we propose a novel two-stage strong correlation learning framework, abbreviated as SC-RPN, which aims to set up stronger relationship among different modules in the region proposal task. Firstly, we propose a Light-weight IoU-Mask branch to predict intersection-over-union (IoU) mask and refine region classification scores as well, it is used to prevent high-quality region proposals from being filtered. Furthermore, a sampling strategy named Size-Aware Dynamic Sampling (SADS) is proposed to ensure sampling consistency between different stages. In addition, point-based representation is exploited to generate region proposals with stronger fitting ability. Without bells and whistles, SC-RPN achieves AR 1000 14.5\% higher than that of Region Proposal Network (RPN), surpassing all the existing region proposal approaches. We also integrate SC-RPN into Fast R-CNN and Faster R-CNN to test its effectiveness on object detection task, the experimental results achieve a gain of 3.2\% and 3.8\% in terms of mAP compared to the original ones.},
  archive      = {J_TIP},
  author       = {Wenbin Zou and Zhengyu Zhang and Yingqing Peng and Canqun Xiang and Shishun Tian and Lu Zhang},
  doi          = {10.1109/TIP.2021.3069547},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4084-4098},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {SC-RPN: A strong correlation learning framework for region proposal},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Different input resolutions and arbitrary output resolution:
A meta learning-based deep framework for infrared and visible image
fusion. <em>TIP</em>, <em>30</em>, 4070–4083. (<a
href="https://doi.org/10.1109/TIP.2021.3069339">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Infrared and visible image fusion has gained ever-increasing attention in recent years due to its great significance in a variety of vision-based applications. However, existing fusion methods suffer from some limitations in terms of the spatial resolutions of both input source images and output fused image, which prevents their practical usage to a great extent. In this paper, we propose a meta learning-based deep framework for the fusion of infrared and visible images. Unlike most existing methods, the proposed framework can accept the source images of different resolutions and generate the fused image of arbitrary resolution just with a single learned model. In the proposed framework, the features of each source image are first extracted by a convolutional network and upscaled by a meta-upscale module with an arbitrary appropriate factor according to practical requirements. Then, a dual attention mechanism-based feature fusion module is developed to combine features from different source images. Finally, a residual compensation module, which can be iteratively adopted in the proposed framework, is designed to enhance the capability of our method in detail extraction. In addition, the loss function is formulated in a multi-task learning manner via simultaneous fusion and super-resolution, aiming to improve the effect of feature learning. And, a new contrast loss inspired by a perceptual contrast enhancement approach is proposed to further improve the contrast of the fused image. Extensive experiments on widely-used fusion datasets demonstrate the effectiveness and superiority of the proposed method. The code of the proposed method is publicly available at https://github.com/yuliu316316/MetaLearning-Fusion .},
  archive      = {J_TIP},
  author       = {Huafeng Li and Yueliang Cen and Yu Liu and Xun Chen and Zhengtao Yu},
  doi          = {10.1109/TIP.2021.3069339},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4070-4083},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Different input resolutions and arbitrary output resolution: A meta learning-based deep framework for infrared and visible image fusion},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Cross-modal knowledge adaptation for language-based person
search. <em>TIP</em>, <em>30</em>, 4057–4069. (<a
href="https://doi.org/10.1109/TIP.2021.3068825">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present a method named Cross-Modal Knowledge Adaptation (CMKA) for language-based person search. We argue that the image and text information are not equally important in determining a person’s identity. In other words, image carries image-specific information such as lighting condition and background, while text contains more modal agnostic information that is more beneficial to cross-modal matching. Based on this consideration, we propose CMKA to adapt the knowledge of image to the knowledge of text. Specially, text-to-image guidance is obtained at different levels: individuals, lists, and classes. By combining these levels of knowledge adaptation, the image-specific information is suppressed, and the common space of image and text is better constructed. We conduct experiments on the CUHK-PEDES dataset. The experimental results show that the proposed CMKA outperforms the state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Yucheng Chen and Rui Huang and Hong Chang and Chuanqi Tan and Tao Xue and Bingpeng Ma},
  doi          = {10.1109/TIP.2021.3068825},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4057-4069},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Cross-modal knowledge adaptation for language-based person search},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). AFAN: Augmented feature alignment network for cross-domain
object detection. <em>TIP</em>, <em>30</em>, 4046–4056. (<a
href="https://doi.org/10.1109/TIP.2021.3066046">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised domain adaptation for object detection is a challenging problem with many real-world applications. Unfortunately, it has received much less attention than supervised object detection. Models that try to address this task tend to suffer from a shortage of annotated training samples. Moreover, existing methods of feature alignments are not sufficient to learn domain-invariant representations. To address these limitations, we propose a novel augmented feature alignment network (AFAN) which integrates intermediate domain image generation and domain-adversarial training into a unified framework. An intermediate domain image generator is proposed to enhance feature alignments by domain-adversarial training with automatically generated soft domain labels. The synthetic intermediate domain images progressively bridge the domain divergence and augment the annotated source domain training data. A feature pyramid alignment is designed and the corresponding feature discriminator is used to align multi-scale convolutional features of different semantic levels. Last but not least, we introduce a region feature alignment and an instance discriminator to learn domain-invariant features for object proposals. Our approach significantly outperforms the state-of-the-art methods on standard benchmarks for both similar and dissimilar domain adaptations. Further extensive experiments verify the effectiveness of each component and demonstrate that the proposed network can learn domain-invariant representations.},
  archive      = {J_TIP},
  author       = {Hongsong Wang and Shengcai Liao and Ling Shao},
  doi          = {10.1109/TIP.2021.3066046},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4046-4056},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {AFAN: Augmented feature alignment network for cross-domain object detection},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Inspirational adversarial image generation. <em>TIP</em>,
<em>30</em>, 4036–4045. (<a
href="https://doi.org/10.1109/TIP.2021.3065845">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The task of image generation started receiving some attention from artists and designers, providing inspiration for new creations. However, exploiting the results of deep generative models such as Generative Adversarial Networks can be long and tedious given the lack of existing tools. In this work, we propose a simple strategy to inspire creators with new generations learned from a dataset of their choice, while providing some control over the output. We design a simple optimization method to find the optimal latent parameters corresponding to the closest generation to any input inspirational image. Specifically, we allow the generation given an inspirational image of the user’s choosing by performing several optimization steps to recover optimal parameters from the model’s latent space. We tested several exploration methods from classical gradient descents to gradient-free optimizers. Many gradient-free optimizers just need comparisons (better/worse than another image), so they can even be used without numerical criterion nor inspirational image, only with human preferences. Thus, by iterating on one’s preferences we can make robust facial composite or fashion generation algorithms. Our results on four datasets of faces, fashion images, and textures show that satisfactory images are effectively retrieved in most cases.},
  archive      = {J_TIP},
  author       = {Baptiste Rozière and Morgane Riviere and Olivier Teytaud and Jérémy Rapin and Yann LeCun and Camille Couprie},
  doi          = {10.1109/TIP.2021.3065845},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4036-4045},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Inspirational adversarial image generation},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Generalized nonconvex low-rank tensor approximation for
multi-view subspace clustering. <em>TIP</em>, <em>30</em>, 4022–4035.
(<a href="https://doi.org/10.1109/TIP.2021.3068646">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The low-rank tensor representation (LRTR) has become an emerging research direction to boost the multi-view clustering performance. This is because LRTR utilizes not only the pairwise relation between data points, but also the view relation of multiple views. However, there is one significant challenge: LRTR uses the tensor nuclear norm as the convex approximation but provides a biased estimation of the tensor rank function. To address this limitation, we propose the generalized nonconvex low-rank tensor approximation (GNLTA) for multi-view subspace clustering. Instead of the pairwise correlation, GNLTA adopts the low-rank tensor approximation to capture the high-order correlation among multiple views and proposes the generalized nonconvex low-rank tensor norm to well consider the physical meanings of different singular values. We develop a unified solver to solve the GNLTA model and prove that under mild conditions, any accumulation point is a stationary point of GNLTA. Extensive experiments on seven commonly used benchmark databases have demonstrated that the proposed GNLTA achieves better clustering performance over state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Yongyong Chen and Shuqin Wang and Chong Peng and Zhongyun Hua and Yicong Zhou},
  doi          = {10.1109/TIP.2021.3068646},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4022-4035},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Generalized nonconvex low-rank tensor approximation for multi-view subspace clustering},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Joint hand-object 3D reconstruction from a single image with
cross-branch feature fusion. <em>TIP</em>, <em>30</em>, 4008–4021. (<a
href="https://doi.org/10.1109/TIP.2021.3068645">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate 3D reconstruction of the hand and object shape from a hand-object image is important for understanding human-object interaction as well as human daily activities. Different from bare hand pose estimation, hand-object interaction poses a strong constraint on both the hand and its manipulated object, which suggests that hand configuration may be crucial contextual information for the object, and vice versa. However, current approaches address this task by training a two-branch network to reconstruct the hand and object separately with little communication between the two branches. In this work, we propose to consider hand and object jointly in feature space and explore the reciprocity of the two branches. We extensively investigate cross-branch feature fusion architectures with MLP or LSTM units. Among the investigated architectures, a variant with LSTM units that enhances object feature with hand feature shows the best performance gain. Moreover, we employ an auxiliary depth estimation module to augment the input RGB image with the estimated depth map, which further improves the reconstruction accuracy. Experiments conducted on public datasets demonstrate that our approach significantly outperforms existing approaches in terms of the reconstruction accuracy of objects.},
  archive      = {J_TIP},
  author       = {Yujin Chen and Zhigang Tu and Di Kang and Ruizhi Chen and Linchao Bao and Zhengyou Zhang and Junsong Yuan},
  doi          = {10.1109/TIP.2021.3068645},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4008-4021},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Joint hand-object 3D reconstruction from a single image with cross-branch feature fusion},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Exploring rich and efficient spatial temporal interactions
for real-time video salient object detection. <em>TIP</em>, <em>30</em>,
3995–4007. (<a href="https://doi.org/10.1109/TIP.2021.3068644">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We have witnessed a growing interest in video salient object detection (VSOD) techniques in today’s computer vision applications. In contrast with temporal information (which is still considered a rather unstable source thus far), the spatial information is more stable and ubiquitous, thus it could influence our vision system more. As a result, the current main-stream VSOD approaches have inferred and obtained their saliency primarily from the spatial perspective, still treating temporal information as subordinate. Although the aforementioned methodology of focusing on the spatial aspect is effective in achieving a numeric performance gain, it still has two critical limitations. First, to ensure the dominance by the spatial information, its temporal counterpart remains inadequately used, though in some complex video scenes, the temporal information may represent the only reliable data source, which is critical to derive the correct VSOD. Second, both spatial and temporal saliency cues are often computed independently in advance and then integrated later on, while the interactions between them are omitted completely, resulting in saliency cues with limited quality. To combat these challenges, this paper advocates a novel spatiotemporal network, where the key innovation is the design of its temporal unit. Compared with other existing competitors (e.g., convLSTM), the proposed temporal unit exhibits an extremely lightweight design that does not degrade its strong ability to sense temporal information. Furthermore, it fully enables the computation of temporal saliency cues that interact with their spatial counterparts, ultimately boosting the overall VSOD performance and realizing its full potential towards mutual performance improvement for each. The proposed method is easy to implement yet still effective, achieving high-quality VSOD at 50 FPS in real-time applications.},
  archive      = {J_TIP},
  author       = {Chenglizhao Chen and Guotao Wang and Chong Peng and Yuming Fang and Dingwen Zhang and Hong Qin},
  doi          = {10.1109/TIP.2021.3068644},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3995-4007},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Exploring rich and efficient spatial temporal interactions for real-time video salient object detection},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fast kNN search in weighted hamming space with multiple
tables. <em>TIP</em>, <em>30</em>, 3985–3994. (<a
href="https://doi.org/10.1109/TIP.2021.3066907">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hashing methods have been widely used in Approximate Nearest Neighbor (ANN) search for big data due to low storage requirements and high search efficiency. These methods usually map the ANN search for big data into the $k$ -Nearest Neighbor ( $k$ NN) search problem in Hamming space. However, Hamming distance calculation ignores the bit-level distinction, leading to confusing ranking. In order to further increase search accuracy, various bit-level weights have been proposed to rank hash codes in weighted Hamming space. Nevertheless, existing ranking methods in weighted Hamming space are almost based on exhaustive linear scan, which is time consuming and not suitable for large datasets. Although Multi-Index hashing that is a sub-linear search method has been proposed, it relies on Hamming distance rather than weighted Hamming distance. To address this issue, we propose an exact $k$ NN search approach with Multiple Tables in Weighted Hamming space named WHMT, in which the distribution of bit-level weights is incorporated into the multi-index building. By WHMT, we can get the optimal candidate set for exact $k$ NN search in weighted Hamming space without exhaustive linear scan. Experimental results show that WHMT can achieve dramatic speedup up to 69.8 times over linear scan baseline without losing accuracy in weighted Hamming space.},
  archive      = {J_TIP},
  author       = {Jie Gui and Yuan Cao and Heng Qi and Keqiu Li and Jieping Ye and Chao Liu and Xiaowei Xu},
  doi          = {10.1109/TIP.2021.3066907},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3985-3994},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Fast kNN search in weighted hamming space with multiple tables},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sequential instance refinement for cross-domain object
detection in images. <em>TIP</em>, <em>30</em>, 3970–3984. (<a
href="https://doi.org/10.1109/TIP.2021.3066904">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-domain object detection in images has attracted increasing attention in the past few years, which aims at adapting the detection model learned from existing labeled images (source domain) to newly collected unlabeled ones (target domain). Existing methods usually deal with the cross-domain object detection problem through direct feature alignment between the source and target domains at the image level, the instance level ( i.e. , region proposals) or both. However, we have observed that directly aligning features of all object instances from the two domains often results in the problem of negative transfer, due to the existence of (1) outlier target instances that contain confusing objects not belonging to any category of the source domain and thus are hard to be captured by detectors and (2) low-relevance source instances that are considerably statistically different from target instances although their contained objects are from the same category. With this in mind, we propose a reinforcement learning based method, coined as sequential instance refinement, where two agents are learned to progressively refine both source and target instances by taking sequential actions to remove both outlier target instances and low-relevance source instances step by step. Extensive experiments on several benchmark datasets demonstrate the superior performance of our method over existing state-of-the-art baselines for cross-domain object detection.},
  archive      = {J_TIP},
  author       = {Jin Chen and Xinxiao Wu and Lixin Duan and Lin Chen},
  doi          = {10.1109/TIP.2021.3066904},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3970-3984},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Sequential instance refinement for cross-domain object detection in images},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MESNet: A convolutional neural network for spotting
multi-scale micro-expression intervals in long videos. <em>TIP</em>,
<em>30</em>, 3956–3969. (<a
href="https://doi.org/10.1109/TIP.2021.3064258">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Micro-expression spotting is a fundamental step in the micro-expression analysis. This paper proposes a novel network based convolutional neural network (CNN) for spotting multi-scale spontaneous micro-expression intervals in long videos. We named the network as Micro-Expression Spotting Network (MESNet). It is composed of three modules. The first module is a 2+1D Spatiotemporal Convolutional Network, which uses 2D convolution to extract spatial features and 1D convolution to extract temporal features. The second module is a Clip Proposal Network, which gives some proposed micro-expression clips. The last module is a Classification Regression Network, which classifies the proposed clips to micro-expression or not, and further regresses their temporal boundaries. We also propose a novel evaluation metric for spotting micro-expression. Extensive experiments have been conducted on the two long video datasets: CAS(ME) 2 and SAMM, and the leave-one-subject-out cross-validation is used to evaluate the spotting performance. Results show that the proposed MESNet effectively enhances the F1-score metric. And comparative results show the proposed MESNet has achieved a good performance, which outperforms other state-of-the-art methods, especially in the SAMM dataset.},
  archive      = {J_TIP},
  author       = {Su-Jing Wang and Ying He and Jingting Li and Xiaolan Fu},
  doi          = {10.1109/TIP.2021.3064258},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3956-3969},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {MESNet: A convolutional neural network for spotting multi-scale micro-expression intervals in long videos},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Progressive transfer learning for face anti-spoofing.
<em>TIP</em>, <em>30</em>, 3946–3955. (<a
href="https://doi.org/10.1109/TIP.2021.3066912">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face anti-spoofing (FAS) techniques play an important role in defending face recognition systems against spoofing attacks. Existing FAS methods often require a large number of annotated spoofing face data to train effective anti-spoofing models. Considering the attacking nature of spoofing data and its diverse variants, obtaining all the spoofing types in advance is difficult. This would limit the performance of FAS networks in practice. Thus, an online learning FAS method is highly desirable. In this paper, we present a semi-supervised learning based framework to tackle face spoofing attacks with only a few labeled training data (e.g., ~ 50 face images). Specifically, we progressively adopt the unlabeled data with reliable pseudo labels during training to enrich the variety of training data. We observed that face spoofing data are naturally presented in the format of video streams. Thus, we exploit the temporal consistency to consolidate the reliability of a pseudo label for a selected image. Furthermore, we propose an adaptive transfer mechanism to ameliorate the influence of unseen spoofing data. Benefiting from the progressively-labeling nature of our method, we are able to train our network on not only data of seen spoofing types (i.e., the source domain) but also unlabeled data of unseen attacking types (i.e., the target domain). In this way, our method can reduce the domain gap and is more practical in real-world anti-spoofing scenarios. Extensive experiments in both the intra-database and inter-database scenarios demonstrate that our method is on par with the state-of-the-art methods but employs remarkably less labeled data (less than 0.1\% labeled spoofing data in a dataset). Moreover, our method significantly outperforms fully-supervised methods on cross-domain testing scenarios with the help of our progressive learning fashion.},
  archive      = {J_TIP},
  author       = {Ruijie Quan and Yu Wu and Xin Yu and Yi Yang},
  doi          = {10.1109/TIP.2021.3066912},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3946-3955},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Progressive transfer learning for face anti-spoofing},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Embarrassingly simple binarization for deep single imagery
super-resolution networks. <em>TIP</em>, <em>30</em>, 3934–3945. (<a
href="https://doi.org/10.1109/TIP.2021.3066906">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep convolutional neural networks (DCCNs) have shown pleasing performance in single image super-resolution (SISR). To deploy them onto real devices with limited storage and computational resources, a promising solution is to binarize the network, i.e., quantize each float-point weight and activation into 1 bit. However, existing works on binarizing DCNNs still suffer from severe performance degradation in SISR. To mitigate this problem, we argue that the performance degradation mainly comes from no appropriate constraint on the network weights, which causes it difficult to sensitively reverse the binarization results of these weights using the backpropagated gradient during training and thus limits the flexibility of network in respect of fitting extensive training samples. Inspired by this, we present an embarrassingly simple but effective binarization scheme for SISR, which can obviously relieve the performance degeneration resulted from network binarization and is applicable to different DCNN architectures. Specifically, we force each weight to follow a compact uniform prior, with which the weight will be given a very small absolute value close to zero and its binarization result can be straightforwardly reversed even by a small backpropagated gradient. By doing this, the flexibility and the generalization performance of the binarized network can be improved. Moreover, such a prior performs much better when introducing real identity shortcuts into the network. In addition, to avoid falling into bad local minima during training, we employ a pixel-wise curriculum learning strategy to learn the constrained weights in an easy-to-hard manner. Experiments on four SISR benchmark datasets demonstrate the effectiveness of the proposed binarization method in terms of binarizing different SISR network architectures, e.g., it even achieves performance comparable to the baseline with 5 quantization bits.},
  archive      = {J_TIP},
  author       = {Lei Zhang and Zhiqiang Lang and Wei Wei and Yanning Zhang},
  doi          = {10.1109/TIP.2021.3066906},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3934-3945},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Embarrassingly simple binarization for deep single imagery super-resolution networks},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust text image recognition via adversarial
sequence-to-sequence domain adaptation. <em>TIP</em>, <em>30</em>,
3922–3933. (<a href="https://doi.org/10.1109/TIP.2021.3066903">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robust text reading is a very challenging problem, due to the distribution of text images changing significantly in real-world scenarios. One effective solution is to align the distribution between different domains by domain adaptation methods. However, we found that these methods might struggle when dealing sequence-like text images. An important reason is that conventional domain adaptation methods strive to align images as a whole, while text images consist of variable-length fine-grained character information. To address this issue, we propose a novel Adversarial Sequence-to-Sequence Domain Adaptation (ASSDA) method to learn “where to adapt” and “how to align” the sequential image. Our key idea is to mine the local regions that contain characters, and focus on aligning them across domains in an adversarial manner. Extensive text recognition experiments show the ASSDA could efficiently transfer sequence knowledge and validate the promising power towards the various domain shift in the real world applications.},
  archive      = {J_TIP},
  author       = {Yaping Zhang and Shuai Nie and Shan Liang and Wenju Liu},
  doi          = {10.1109/TIP.2021.3066903},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3922-3933},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Robust text image recognition via adversarial sequence-to-sequence domain adaptation},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Light field view synthesis via aperture disparity and
warping confidence map. <em>TIP</em>, <em>30</em>, 3908–3921. (<a
href="https://doi.org/10.1109/TIP.2021.3066293">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a learning-based approach to synthesize the view from an arbitrary camera position given a sparse set of images. A key challenge for this novel view synthesis arises from the reconstruction process, when the views from different input images may not be consistent due to obstruction in the light path. We overcome this by jointly modeling the epipolar property and occlusion in designing a convolutional neural network. We start by defining and computing the aperture disparity map, which approximates the parallax and measures the pixel-wise shift between two views. While this relates to free-space rendering and can fail near the object boundaries, we further develop a warping confidence map to address pixel occlusion in these challenging regions. The proposed method is evaluated on diverse real-world and synthetic light field scenes, and it shows better performance over several state-of-the-art techniques.},
  archive      = {J_TIP},
  author       = {Nan Meng and Kai Li and Jianzhuang Liu and Edmund Y. Lam},
  doi          = {10.1109/TIP.2021.3066293},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3908-3921},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Light field view synthesis via aperture disparity and warping confidence map},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Regularized densely-connected pyramid network for salient
instance segmentation. <em>TIP</em>, <em>30</em>, 3897–3907. (<a
href="https://doi.org/10.1109/TIP.2021.3065822">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Much of the recent efforts on salient object detection (SOD) have been devoted to producing accurate saliency maps without being aware of their instance labels. To this end, we propose a new pipeline for end-to-end salient instance segmentation (SIS) that predicts a class-agnostic mask for each detected salient instance. To better use the rich feature hierarchies in deep networks and enhance the side predictions, we propose the regularized dense connections, which attentively promote informative features and suppress non-informative ones from all feature pyramids. A novel multi-level RoIAlign based decoder is introduced to adaptively aggregate multi-level features for better mask predictions. Such strategies can be well-encapsulated into the Mask R-CNN pipeline. Extensive experiments on popular benchmarks demonstrate that our design significantly outperforms existing state-of-the-art competitors by 6.3\% (58.6\% vs. 52.3\%) in terms of the AP metric. The code is available at https://github.com/yuhuan-wu/RDPNet.},
  archive      = {J_TIP},
  author       = {Yu-Huan Wu and Yun Liu and Le Zhang and Wang Gao and Ming-Ming Cheng},
  doi          = {10.1109/TIP.2021.3065822},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3897-3907},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Regularized densely-connected pyramid network for salient instance segmentation},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). HDR-GAN: HDR image reconstruction from multi-exposed LDR
images with large motions. <em>TIP</em>, <em>30</em>, 3885–3896. (<a
href="https://doi.org/10.1109/TIP.2021.3064433">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Synthesizing high dynamic range (HDR) images from multiple low-dynamic range (LDR) exposures in dynamic scenes is challenging. There are two major problems caused by the large motions of foreground objects. One is the severe misalignment among the LDR images. The other is the missing content due to the over-/under-saturated regions caused by the moving objects, which may not be easily compensated for by the multiple LDR exposures. Thus, it requires the HDR generation model to be able to properly fuse the LDR images and restore the missing details without introducing artifacts. To address these two problems, we propose in this paper a novel GAN-based model, HDR-GAN, for synthesizing HDR images from multi-exposed LDR images. To our best knowledge, this work is the first GAN-based approach for fusing multi-exposed LDR images for HDR reconstruction. By incorporating adversarial learning, our method is able to produce faithful information in the regions with missing content. In addition, we also propose a novel generator network, with a reference-based residual merging block for aligning large object motions in the feature domain, and a deep HDR supervision scheme for eliminating artifacts of the reconstructed HDR images. Experimental results demonstrate that our model achieves state-of-the-art reconstruction performance over the prior HDR methods on diverse scenes.},
  archive      = {J_TIP},
  author       = {Yuzhen Niu and Jianbin Wu and Wenxi Liu and Wenzhong Guo and Rynson W. H. Lau},
  doi          = {10.1109/TIP.2021.3064433},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3885-3896},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {HDR-GAN: HDR image reconstruction from multi-exposed LDR images with large motions},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Mask-guided attention network and occlusion-sensitive hard
example mining for occluded pedestrian detection. <em>TIP</em>,
<em>30</em>, 3872–3884. (<a
href="https://doi.org/10.1109/TIP.2020.3040854">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pedestrian detection relying on deep convolution neural networks has made significant progress. Though promising results have been achieved on standard pedestrians, the performance on heavily occluded pedestrians remains far from satisfactory. The main culprits are intra-class occlusions involving other pedestrians and inter-class occlusions caused by other objects, such as cars and bicycles. These result in a multitude of occlusion patterns. We propose an approach for occluded pedestrian detection with the following contributions. First, we introduce a novel mask-guided attention network that fits naturally into popular pedestrian detection pipelines. Our attention network emphasizes on visible pedestrian regions while suppressing the occluded ones by modulating full body features. Second, we propose the occlusion-sensitive hard example mining method and occlusion-sensitive loss that mines hard samples according to the occlusion level and assigns higher weights to the detection errors occurring at highly occluded pedestrians. Third, we empirically demonstrate that weak box-based segmentation annotations provide reasonable approximation to their dense pixel-wise counterparts. Experiments are performed on CityPersons, Caltech and ETH datasets. Our approach sets a new state-of-the-art on all three datasets. Our approach obtains an absolute gain of 10.3\% in log-average miss rate, compared with the best reported results on the heavily occluded HO pedestrian set of the CityPersons test set. Code and models are available at: https://github.com/Leotju/MGAN .},
  archive      = {J_TIP},
  author       = {Jin Xie and Yanwei Pang and Muhammad Haris Khan and Rao Muhammad Anwer and Fahad Shahbaz Khan and Ling Shao},
  doi          = {10.1109/TIP.2020.3040854},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3872-3884},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Mask-guided attention network and occlusion-sensitive hard example mining for occluded pedestrian detection},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Weighted feature histogram of multi-scale local patch using
multi-bit binary descriptor for face recognition. <em>TIP</em>,
<em>30</em>, 3858–3871. (<a
href="https://doi.org/10.1109/TIP.2021.3065843">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most face recognition methods employ single-bit binary descriptors for face representation. The information from these methods is lost in the process of quantization from real-valued descriptors to binary descriptors, which greatly limits their robustness for face recognition. In this study, we propose a novel weighted feature histogram (WFH) method of multi-scale local patches using multi-bit binary descriptors for face recognition. First, to obtain multi-scale information of the face image, the local patches are extracted using a multi-scale local patch generation (MSLPG) method. Second, with the goal of reducing the quantization information loss of binary descriptors, a novel multi-bit local binary descriptor learning (MBLBDL) method is proposed to extract multi-bit local binary descriptors (MBLBDs). In MBLBDL, a learned mapping matrix and novel multi-bit coding rules are employed to project pixel difference vectors (PDVs) into the MBLBDs in each local patch. Finally, a novel robust weight learning (RWL) method is proposed to learn a set of robust weights for each patch to integrate the MBLBDs into the final face representation. In RWL, a codebook is first constructed by clustering MBLBDs on each local patch to extract a feature histogram. Then, considering that different parts of the face have different degrees of robustness to local changes, a set of weights is learned to concatenate the feature histograms of all local patches into the final representation of a face image. In addition, to further improve the performance for heterogeneous face recognition, a coupled WFH (C-WFH) method is proposed. C-WFH maintains the similarity of the corresponding MBLBDs and feature histograms for a pair of heterogeneous face images by means of a novel coupled feature learning (CFL) method to reduce the modality gap. A series of experiments are conducted on widely used face datasets to analyze the performance of WFH and C-WFH. Extensive experimental results show that WFH and C-WFH outperform state-of-the-art face recognition methods.},
  archive      = {J_TIP},
  author       = {Hua Yang and Chenting Gong and Kaiji Huang and Kaiyou Song and Zhouping Yin},
  doi          = {10.1109/TIP.2021.3065843},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3858-3871},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Weighted feature histogram of multi-scale local patch using multi-bit binary descriptor for face recognition},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Real-time 3D facial tracking via cascaded compositional
learning. <em>TIP</em>, <em>30</em>, 3844–3857. (<a
href="https://doi.org/10.1109/TIP.2021.3065819">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose to learn a cascade of globally-optimized modular boosted ferns (GoMBF) to solve multi-modal facial motion regression for real-time 3D facial tracking from a monocular RGB camera. GoMBF is a deep composition of multiple regression models with each is a boosted ferns initially trained to predict partial motion parameters of the same modality, and then concatenated together via a global optimization step to form a singular strong boosted ferns that can effectively handle the whole regression target. It can explicitly cope with the modality variety in output variables, while manifesting increased fitting power and a faster learning speed comparing against the conventional boosted ferns. By further cascading a sequence of GoMBFs (GoMBF-Cascade) to regress facial motion parameters, we achieve competitive tracking performance on a variety of in-the-wild videos comparing to the state-of-the-art methods which either have higher computational complexity or require much more training data. It provides a robust and highly elegant solution to real-time 3D facial tracking using a small set of training data and hence makes it more practical in real-world applications. We further deeply investigate the effect of synthesized facial images on training non-deep learning methods such as GoMBF-Cascade for 3D facial tracking. We apply three types synthetic images with various naturalness levels for training two different tracking methods, and compare the performance of the tracking models trained on real data, on synthetic data and on a mixture of data. The experimental results indicate that, i) the model trained purely on synthetic facial imageries can hardly generalize well to unconstrained real-world data, ii) involving synthetic faces into training benefits tracking in some certain scenarios but degrades the tracking model&#39;s generalization ability. These two insights could benefit a range of non-deep learning facial image analysis tasks where the labelled real data is difficult to acquire.},
  archive      = {J_TIP},
  author       = {Jianwen Lou and Xiaoxu Cai and Junyu Dong and Hui Yu},
  doi          = {10.1109/TIP.2021.3065819},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3844-3857},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Real-time 3D facial tracking via cascaded compositional learning},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust ellipse fitting using hierarchical gaussian mixture
models. <em>TIP</em>, <em>30</em>, 3828–3843. (<a
href="https://doi.org/10.1109/TIP.2021.3065799">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fitting ellipses from unrecognized data is a fundamental problem in computer vision and pattern recognition. Classic least-squares based methods are sensitive to outliers. To address this problem, in this paper, we present a novel and effective method called hierarchical Gaussian mixture models (HGMM) for ellipse fitting in noisy, outliers-contained, and occluded settings on the basis of Gaussian mixture models (GMM). This method is crafted into two layers to significantly improve its fitting accuracy and robustness for data containing outliers/noise and has been proven to effectively narrow down the iterative interval of the kernel bandwidth, thereby speeding up ellipse fitting. Extensive experiments are conducted on synthetic data including substantial outliers (up to 60\%) and strong noise (up to 200\%) as well as on real images including complex benchmark images with heavy occlusion and images from versatile applications. We compare our results with those of representative state-of-the-art methods and demonstrate that our proposed method has several salient advantages, such as its high robustness against outliers and noise, high fitting accuracy, and improved performance.},
  archive      = {J_TIP},
  author       = {Mingyang Zhao and Xiaohong Jia and Lubin Fan and Yuan Liang and Dong-Ming Yan},
  doi          = {10.1109/TIP.2021.3065799},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3828-3843},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Robust ellipse fitting using hierarchical gaussian mixture models},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). 3D face from x: Learning face shape from diverse sources.
<em>TIP</em>, <em>30</em>, 3815–3827. (<a
href="https://doi.org/10.1109/TIP.2021.3065798">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel method to jointly learn a 3D face parametric model and 3D face reconstruction from diverse sources. Previous methods usually learn 3D face modeling from one kind of source, such as scanned data or in-the-wild images. Although 3D scanned data contain accurate geometric information of face shapes, the capture system is expensive and such datasets usually contain a small number of subjects. On the other hand, in-the-wild face images are easily obtained and there are a large number of facial images. However, facial images do not contain explicit geometric information. In this paper, we propose a method to learn a unified face model from diverse sources. Besides scanned face data and face images, we also utilize a large number of RGB-D images captured with an iPhone X to bridge the gap between the two sources. Experimental results demonstrate that with training data from more sources, we can learn a more powerful face model.},
  archive      = {J_TIP},
  author       = {Yudong Guo and Lin Cai and Juyong Zhang},
  doi          = {10.1109/TIP.2021.3065798},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3815-3827},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {3D face from x: Learning face shape from diverse sources},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SAMNet: Stereoscopically attentive multi-scale network for
lightweight salient object detection. <em>TIP</em>, <em>30</em>,
3804–3814. (<a href="https://doi.org/10.1109/TIP.2021.3065239">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent progress on salient object detection (SOD) mostly benefits from the explosive development of Convolutional Neural Networks (CNNs). However, much of the improvement comes with the larger network size and heavier computation overhead, which, in our view, is not mobile-friendly and thus difficult to deploy in practice. To promote more practical SOD systems, we introduce a novel Stereoscopically Attentive Multi-scale (SAM) module, which adopts a stereoscopic attention mechanism to adaptively fuse the features of various scales. Embarking on this module, we propose an extremely lightweight network, namely SAMNet, for SOD. Extensive experiments on popular benchmarks demonstrate that the proposed SAMNet yields comparable accuracy with state-of-the-art methods while running at a GPU speed of 343fps and a CPU speed of 5fps for $336 \times 336$ inputs with only 1.33M parameters. Therefore, SAMNet paves a new path towards SOD. The source code is available on the project page https://mmcheng.net/SAMNet/ .},
  archive      = {J_TIP},
  author       = {Yun Liu and Xin-Yu Zhang and Jia-Wang Bian and Le Zhang and Ming-Ming Cheng},
  doi          = {10.1109/TIP.2021.3065239},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3804-3814},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {SAMNet: Stereoscopically attentive multi-scale network for lightweight salient object detection},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Attention-based multi-source domain adaptation.
<em>TIP</em>, <em>30</em>, 3793–3803. (<a
href="https://doi.org/10.1109/TIP.2021.3065254">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-source domain adaptation (MSDA) aims to transfer knowledge from multi-source domains to one target domain. Inspired by single-source domain adaptation, existing methods solve MSDA by aligning the data distributions between the target domain and each source domain. However, aligning the target domain with the dissimilar source domain would harm the representation learning. To address the above issue, an intuitive motivation of MSDA is using the attention mechanism to enhance the positive effects of the similar domains, and suppress the negative effects of the dissimilar domains. Therefore, we propose Attention-Based Multi-Source Domain Adaptation (ABMSDA) by considering the domain correlations to alleviate the effects caused by dissimilar domains. To obtain the domain correlations between source and target domains, ABMSDA firstly trains a domain recognition model to calculate the probability that the target images belong to each source domain. Based on the domain correlations, Weighted Moment Distance (WMD) is proposed to pay more attention on the source domains with higher similarities. Furthermore, Attentive Classification Loss (ACL) is developed to constrain that the feature extractor can generate the alignment and discriminative visual representations. The evaluations on two benchmarks demonstrate the effectiveness of the proposed model, e.g., an average of 6.1\% improvement on the challenging DomainNet dataset.},
  archive      = {J_TIP},
  author       = {Yukun Zuo and Hantao Yao and Changsheng Xu},
  doi          = {10.1109/TIP.2021.3065254},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3793-3803},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Attention-based multi-source domain adaptation},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The role of redundant bases and shrinkage functions in image
denoising. <em>TIP</em>, <em>30</em>, 3778–3792. (<a
href="https://doi.org/10.1109/TIP.2021.3065226">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wavelet denoising is a classical and effective approach for reducing noise in images and signals. Suggested in 1994, this approach is carried out by rectifying the coefficients of a noisy image, in the transform domain, using a set of shrinkage functions (SFs). A plethora of papers deals with the optimal shape of the SFs and the transform used. For example, it is widely known that applying SFs in a redundant basis improves the results. However, it is barely known that the shape of the SFs should be changed when the transform used is redundant. In this paper, we introduce a complete picture of the interrelations between the transform used, the optimal shrinkage functions, and the domains in which they are optimized. We suggest three schemes for optimizing the SFs and provide bounds of the remaining noise, in each scheme, with respect to the other alternatives. In particular, we show that for subband optimization, where each SF is optimized independently for a particular band, optimizing the SFs in the spatial domain is always better than or equal to optimizing the SFs in the transform domain. Furthermore, for redundant bases, we provide the expected denoising gain that can be achieved, relative to the unitary basis, as a function of the redundancy rate.},
  archive      = {J_TIP},
  author       = {Yacov Hel-Or and Gil Ben-Artzi},
  doi          = {10.1109/TIP.2021.3065226},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3778-3792},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {The role of redundant bases and shrinkage functions in image denoising},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Towards cross-dataset palmprint recognition via joint pixel
and feature alignment. <em>TIP</em>, <em>30</em>, 3764–3777. (<a
href="https://doi.org/10.1109/TIP.2021.3065220">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning-based palmprint recognition algorithms have shown great potential. Most of them are mainly focused on identifying samples from the same dataset. However, they may be not suitable for a more convenient case that the images for training and test are from different datasets, such as collected by embedded terminals and smartphones. Therefore, we propose a novel Joint Pixel and Feature Alignment (JPFA) framework for such cross-dataset palmprint recognition scenarios. Two-stage alignment is applied to obtain adaptive features in source and target datasets. 1) Deep style transfer model is adopted to convert source images into fake images to reduce the dataset gaps and perform data augmentation on pixel level. 2) A new deep domain adaptation model is proposed to extract adaptive features by aligning the dataset-specific distributions of target-source and target-fake pairs on feature level. Adequate experiments are conducted on several benchmarks including constrained and unconstrained palmprint databases. The results demonstrate that our JPFA outperforms other models to achieve the state-of-the-arts. Compared with baseline, the accuracy of cross-dataset identification is improved by up to 28.10\% and the Equal Error Rate (EER) of cross-dataset verification is reduced by up to 4.69\%. To make our results reproducible, the codes are publicly available at http://gr.xjtu.edu.cn/web/bell/resource.},
  archive      = {J_TIP},
  author       = {Huikai Shao and Dexing Zhong},
  doi          = {10.1109/TIP.2021.3065220},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3764-3777},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Towards cross-dataset palmprint recognition via joint pixel and feature alignment},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Layer-output guided complementary attention learning for
image defocus blur detection. <em>TIP</em>, <em>30</em>, 3748–3763. (<a
href="https://doi.org/10.1109/TIP.2021.3065171">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Defocus blur detection (DBD), which has been widely applied to various fields, aims to detect the out-of-focus or in-focus pixels from a single image. Despite the fact that the deep learning based methods applied to DBD have outperformed the hand-crafted feature based methods, the performance cannot still meet our requirement. In this paper, a novel network is established for DBD. Unlike existing methods which only learn the projection from the in-focus part to the ground-truth, both in-focus and out-of-focus pixels, which are completely and symmetrically complementary, are taken into account. Specifically, two symmetric branches are designed to jointly estimate the probability of focus and defocus pixels, respectively. Due to their complementary constraint, each layer in a branch is affected by an attention obtained from another branch, effectively learning the detailed information which may be ignored in one branch. The feature maps from these two branches are then passed through a unique fusion block to simultaneously get the two-channel output measured by a complementary loss. Additionally, instead of estimating only one binary map from a specific layer, each layer is encouraged to estimate the ground truth to guide the binary map estimation in its linked shallower layer followed by a top-to-bottom combination strategy, gradually exploiting the global and local information. Experimental results on released datasets demonstrate that our proposed method remarkably outperforms state-of-the-art algorithms.},
  archive      = {J_TIP},
  author       = {Jinxing Li and Dandan Fan and Lingxiao Yang and Shuhang Gu and Guangming Lu and Yong Xu and David Zhang},
  doi          = {10.1109/TIP.2021.3065171},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3748-3763},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Layer-output guided complementary attention learning for image defocus blur detection},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Attentive feature refinement network for single rainy image
restoration. <em>TIP</em>, <em>30</em>, 3734–3747. (<a
href="https://doi.org/10.1109/TIP.2021.3064229">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the fact that great progress has been made on single image deraining tasks, it is still challenging for existing models to produce satisfactory results directly, and it often requires a single or multiple refinement stages to gradually improve the quality. However, in this paper, we demonstrate that existing image-level refinement with a stage-independent learning design is problematic with the side effect of over/under-deraining. To resolve this issue, we for the first time propose the mechanism of learning to carry out refinement on the unsatisfactory features, and propose a novel attentive feature refinement (AFR) module. Specifically, AFR is designed as a two-branched network for simultaneous rain-distribution-aware attention map learning and attention guided hierarchy-preserving feature refinement. Guided by task-specific attention, coarse features are progressively refined to better model the diversified rainy effects. By using a separable convolution as the basic component, our AFR module introduces little computation overhead and can be readily integrated into most rainy-to-clean image translation networks for achieving better deraining results. By incorporating a series of AFR modules into a general encoder-decoder network, AFR-Net is constructed for deraining and it achieves new state-of-the-art results on both synthetic and real images. Furthermore, by using AFR-Net as a teacher model, we explore the use of knowledge distillation to successfully learn a student model that is also able to achieve state-of-the-art results but with a much faster inference speed (i.e., it only takes 0.08 second to process a 512×512 rainy image). Code and pre-trained models are available at (https://github.com/RobinCSIRO/AFR-Net).},
  archive      = {J_TIP},
  author       = {Guoqing Wang and Changming Sun and Arcot Sowmya},
  doi          = {10.1109/TIP.2021.3064229},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3734-3747},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Attentive feature refinement network for single rainy image restoration},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021c). Thanka mural inpainting based on multi-scale adaptive
partial convolution and stroke-like mask. <em>TIP</em>, <em>30</em>,
3720–3733. (<a href="https://doi.org/10.1109/TIP.2021.3064268">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Thanka murals are important cultural heritages of Tibet, but many precious murals were damaged during history. Thanka mural restoration is very important for the protection of Tibetan cultural heritage. Partial convolution has great potential for Thanka mural restoration due to its outstanding performance for inpainting irregular holes. However, three challenges prevent the existing partial convolution-based methods from solving Thanka restoration problems: 1) the features of multi-scale objects in Thanka murals cannot be extracted correctly because of single-scale partial convolution; 2) the stroke-like Thanka inpainting mode cannot be effectively simulated and learned by existing rectangular or arbitrary masks; and 3) the original content of damaged Thanka murals cannot be restored. To resolve these problems, we propose a Thanka mural inpainting method based on multi-scale adaptive partial convolution and stroke-like masks. The proposed method consists of three parts: 1) a kernel-level multi-scale adaptive partial convolution (MAPConv) to accurately discriminate valid pixels from invalid pixels, and to extract the features of multi-scale objects; 2) a parameter-configurable stroke-like mask generation method to simulate and learn the stroke-like Thanka inpainting mode; and 3) a 2-phase learning framework based on MAPConv Unet and different loss functions to restore the original content of Thanka murals. Experiments on both simulated and real damages of Thanka murals demonstrated that our approach works well on a small dataset (N=2780), generates realistic mural content, and restores the damaged Thanka murals with high speed (600 ms for multiple holes in 512×512 images). The proposed end-to-end method can be applied to other small datasets-based inpainting tasks.},
  archive      = {J_TIP},
  author       = {Nianyi Wang and Weilan Wang and Wenjin Hu and Aaron Fenster and Shuo Li},
  doi          = {10.1109/TIP.2021.3064268},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3720-3733},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Thanka mural inpainting based on multi-scale adaptive partial convolution and stroke-like mask},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hybrid regularization of diffusion process for visual
re-ranking. <em>TIP</em>, <em>30</em>, 3705–3719. (<a
href="https://doi.org/10.1109/TIP.2021.3064265">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To improve the retrieval result obtained from a pairwise dissimilarity, many variants of diffusion process have been applied in visual re-ranking. In the framework of diffusion process, various contextual similarities can be obtained by solving an optimization problem, and the objective function consists of a smoothness constraint and a fitting constraint. And many improvements on the smoothness constraint have been made to reveal the underlying manifold structure. However, little attention has been paid to the fitting constraint, and how to build an effective fitting constraint still remains unclear. In this article, by deeply analyzing the role of fitting constraint, we firstly propose a novel variant of diffusion process named Hybrid Regularization of Diffusion Process (HyRDP). In HyRDP, we introduce a hybrid regularization framework containing a two-part fitting constraint, and the contextual dissimilarities can be learned from either a closed-form solution or an iterative solution. Furthermore, this article indicates that the basic idea of HyRDP is closely related to the mechanism behind Generalized Mean First-passage Time (GMFPT). GMFPT denotes the mean time-steps for the state transition from one state to any one in the given state set, and is firstly introduced as the contextual dissimilarity in this article. Finally, based on the semi-supervised learning framework, an iterative re-ranking process is developed. With this approach, the relevant objects on the manifold can be iteratively retrieved and labeled within finite iterations. The proposed algorithms are validated on various challenging databases, and the experimental performances demonstrate that retrieval results obtained from different types of measures can be effectively improved by using our methods.},
  archive      = {J_TIP},
  author       = {Danchen Zheng and Jianchao Fan and Min Han},
  doi          = {10.1109/TIP.2021.3064265},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3705-3719},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Hybrid regularization of diffusion process for visual re-ranking},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Attend and guide (AG-net): A keypoints-driven
attention-based deep network for image recognition. <em>TIP</em>,
<em>30</em>, 3691–3704. (<a
href="https://doi.org/10.1109/TIP.2021.3064256">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a novel keypoints-based attention mechanism for visual recognition in still images. Deep Convolutional Neural Networks (CNNs) for recognizing images with distinctive classes have shown great success, but their performance in discriminating fine-grained changes is not at the same level. We address this by proposing an end-to-end CNN model, which learns meaningful features linking fine-grained changes using our novel attention mechanism. It captures the spatial structures in images by identifying semantic regions (SRs) and their spatial distributions, and is proved to be the key to modeling subtle changes in images. We automatically identify these SRs by grouping the detected keypoints in a given image. The “usefulness” of these SRs for image recognition is measured using our innovative attentional mechanism focusing on parts of the image that are most relevant to a given task. This framework applies to traditional and fine-grained image recognition tasks and does not require manually annotated regions (e.g. bounding-box of body parts, objects, etc.) for learning and prediction. Moreover, the proposed keypoints-driven attention mechanism can be easily integrated into the existing CNN models. The framework is evaluated on six diverse benchmark datasets. The model outperforms the state-of-the-art approaches by a considerable margin using Distracted Driver V1 (Acc: 3.39\%), Distracted Driver V2 (Acc: 6.58\%), Stanford-40 Actions (mAP: 2.15\%), People Playing Musical Instruments (mAP: 16.05\%), Food-101 (Acc: 6.30\%) and Caltech-256 (Acc: 2.59\%) datasets.},
  archive      = {J_TIP},
  author       = {Asish Bera and Zachary Wharton and Yonghuai Liu and Nik Bessis and Ardhendu Behera},
  doi          = {10.1109/TIP.2021.3064256},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3691-3704},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Attend and guide (AG-net): A keypoints-driven attention-based deep network for image recognition},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Recovering surface normal and arbitrary images: A dual
regression network for photometric stereo. <em>TIP</em>, <em>30</em>,
3676–3690. (<a href="https://doi.org/10.1109/TIP.2021.3064230">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Photometric stereo recovers three-dimensional (3D) object surface normal from multiple images under different illumination directions. Traditional photometric stereo methods suffer from the problem of non-Lambertian surfaces with general reflectance. By leveraging deep neural networks, learning-based methods are capable of improving the surface normal estimation under general non-Lambertian surfaces. These state-of-the-art learning-based methods however do not associate surface normal with reconstructed images and, therefore, they cannot explore the beneficial effect of such association on the estimation of the surface normal. In this paper, we specifically exploit the positive impact of this association and propose a novel dual regression network for both fine surface normals and arbitrary reconstructed images in calibrated photometric stereo. Our work unifies the 3D reconstruction and rendering tasks in a deep learning framework, with the explorations including: 1. generating specified reconstructed images under arbitrary illumination directions, which provides more intuitive perception of the reflectance and is extremely useful for visual applications, such as virtual reality, and 2. our dual regression scheme introduces an additional constraint on observed images and reconstructed images, which forms a closed-loop to provide additional supervision. Experiments show that our proposed method achieves accurate reconstructed images under arbitrarily specified illumination directions and it significantly outperforms the state-of-the-art learning-based single regression methods in calibrated photometric stereo.},
  archive      = {J_TIP},
  author       = {Yakun Ju and Junyu Dong and Sheng Chen},
  doi          = {10.1109/TIP.2021.3064230},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3676-3690},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Recovering surface normal and arbitrary images: A dual regression network for photometric stereo},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Distributed connected component filtering and analysis in 2D
and 3D tera-scale data sets. <em>TIP</em>, <em>30</em>, 3664–3675. (<a
href="https://doi.org/10.1109/TIP.2021.3064223">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Connected filters and multi-scale tools are region-based operators acting on the connected components of an image. Component trees are image representations to efficiently perform these operations as they represent the inclusion relationship of the connected components hierarchically. This paper presents disccofan (DIStributed Connected COmponent Filtering and ANalysis), a new method that extends the previous 2D implementation of the Distributed Component Forests (DCFs) to handle 3D processing and higher dynamic range data sets. disccofan combines shared and distributed memory techniques to efficiently compute component trees, user-defined attributes filters, and multi-scale analysis. Compared to similar methods, disccofan is faster and scales better on low and moderate dynamic range images, and is the only method with a speed-up larger than 1 on a realistic, astronomical floating-point data set. It achieves a speed-up of 11.20 using 48 processes to compute the DCF of a 162 Gigapixels, single-precision floating-point 3D data set, while reducing the memory used by a factor of 22. This approach is suitable to perform attribute filtering and multi-scale analysis on very large 2D and 3D data sets, up to single-precision floating-point value.},
  archive      = {J_TIP},
  author       = {Simon Gazagnes and Michael H. F Wilkinson},
  doi          = {10.1109/TIP.2021.3064223},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3664-3675},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Distributed connected component filtering and analysis in 2D and 3D tera-scale data sets},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Blind image quality assessment with active inference.
<em>TIP</em>, <em>30</em>, 3650–3663. (<a
href="https://doi.org/10.1109/TIP.2021.3064195">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blind image quality assessment (BIQA) is a useful but challenging task. It is a promising idea to design BIQA methods by mimicking the working mechanism of human visual system (HVS). The internal generative mechanism (IGM) indicates that the HVS actively infers the primary content (i.e., meaningful information) of an image for better understanding. Inspired by that, this paper presents a novel BIQA metric by mimicking the active inference process of IGM. Firstly, an active inference module based on the generative adversarial network (GAN) is established to predict the primary content, in which the semantic similarity and the structural dissimilarity (i.e., semantic consistency and structural completeness) are both considered during the optimization. Then, the image quality is measured on the basis of its primary content. Generally, the image quality is highly related to three aspects, i.e., the scene information (content-dependency), the distortion type (distortion-dependency), and the content degradation (degradation-dependency). According to the correlation between the distorted image and its primary content, the three aspects are analyzed and calculated respectively with a multi-stream convolutional neural network (CNN) based quality evaluator. As a result, with the help of the primary content obtained from the active inference and the comprehensive quality degradation measurement from the multi-stream CNN, our method achieves competitive performance on five popular IQA databases. Especially in cross-database evaluations, our method achieves significant improvements.},
  archive      = {J_TIP},
  author       = {Jupo Ma and Jinjian Wu and Leida Li and Weisheng Dong and Xuemei Xie and Guangming Shi and Weisi Lin},
  doi          = {10.1109/TIP.2021.3064195},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3650-3663},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Blind image quality assessment with active inference},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust sparse representation in quaternion space.
<em>TIP</em>, <em>30</em>, 3637–3649. (<a
href="https://doi.org/10.1109/TIP.2021.3064193">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sparse representation has achieved great success across various fields including signal processing, machine learning and computer vision. However, most existing sparse representation methods are confined to the real valued data. This largely limit their applicability to the quaternion valued data, which has been widely used in numerous applications such as color image processing. Another critical issue is that their performance may be severely hampered due to the data noise or outliers in practice. To tackle the problems above, in this work we propose a robust quaternion valued sparse representation (RQVSR) method in a fully quaternion valued setting. To handle the quaternion noises, we first define a new robust estimator referred as quaternion Welsch estimator to measure the quaternion residual error. Compared to the conventional quaternion mean square error, it can largely suppress the impact of large data corruption and outliers. To implement RQVSR, we have overcome the difficulties raised by the noncommutativity of quaternion multiplication and developed an effective algorithm by leveraging the half-quadratic theory and the alternating direction method of multipliers framework. The experimental results show the effectiveness and robustness of the proposed method for quaternion sparse signal recovery and color image reconstruction.},
  archive      = {J_TIP},
  author       = {Yulong Wang and Kit Ian Kou and Cuiming Zou and Yuan Yan Tang},
  doi          = {10.1109/TIP.2021.3064193},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3637-3649},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Robust sparse representation in quaternion space},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep dichromatic guided learning for illuminant estimation.
<em>TIP</em>, <em>30</em>, 3623–3636. (<a
href="https://doi.org/10.1109/TIP.2021.3062729">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A new dichromatic illuminant estimation method using a deep neural network is proposed. Previous methods based on the dichromatic reflection model commonly suffer from inaccurate separation of specularity, thus being limited in their use in a real-world. Recent deep neural network-based methods have shown a significant improvement in the estimation of the illuminant color. However, why they succeed or fail is not explainable easily, because most of them estimate the illuminant color at the network output directly. To tackle these problems, the proposed architecture is designed to learn dichromatic planes and their confidences using a deep neural network with novel losses function. The illuminant color is estimated by a weighted least mean square of these planes. The proposed dichromatic guided learning not only achieves compelling results among state-of-the-art color constancy methods in standard real-world benchmark evaluations, but also provides a map to include color and regional contributions for illuminant estimation, which allow for an in-depth analysis of success and failure cases of illuminant estimation.},
  archive      = {J_TIP},
  author       = {Sung-Min Woo and Jong-Ok Kim},
  doi          = {10.1109/TIP.2021.3062729},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3623-3636},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Deep dichromatic guided learning for illuminant estimation},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Human skin gloss perception based on texture statistics.
<em>TIP</em>, <em>30</em>, 3610–3622. (<a
href="https://doi.org/10.1109/TIP.2021.3061276">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose objective, image-based techniques for quantitative evaluation of facial skin gloss that is consistent with human judgments. We use polarization photography to obtain separate images of surface and subsurface reflections, and rely on psychophysical studies to uncover and separate the influence of the two components on skin gloss perception. We capture images of facial skin at two levels, macro-scale (whole face) and meso-scale (skin patch), before and after cleansing. To generate a broad range of skin appearances for each subject, we apply photometric image transformations to the surface and subsurface reflection images. We then use linear regression to link statistics of the surface and subsurface reflections to the perceived gloss obtained in our empirical studies. The focus of this paper is on within-subject gloss perception, that is, on visual differences among images of the same subject. Our analysis shows that the contrast of the surface reflection has a strong positive influence on skin gloss perception, while the darkness of the subsurface reflection (skin tone) has a weaker positive effect on perceived gloss. We show that a regression model based on the concatenation of statistics from the two reflection images can successfully predict relative gloss differences.},
  archive      = {J_TIP},
  author       = {Jing Wang and Carla Kuesten and Jim Mayne and Gopa Majmudar and Thrasyvoulos N. Pappas},
  doi          = {10.1109/TIP.2021.3061276},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3610-3622},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Human skin gloss perception based on texture statistics},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SiamCAN: Real-time visual tracking based on siamese
center-aware network. <em>TIP</em>, <em>30</em>, 3597–3609. (<a
href="https://doi.org/10.1109/TIP.2021.3060905">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we present a novel Siamese center-aware network (SiamCAN) for visual tracking, which consists of the Siamese feature extraction subnetwork, followed by the classification, regression, and localization branches in parallel. The classification branch is used to distinguish the target from background, and the regression branch is introduced to regress the bounding box of the target. To reduce the impact of manually designed anchor boxes to adapt to different target motion patterns, we design the localization branch to localize the target center directly to assist the regression branch generating accurate results. Meanwhile, we introduce the global context module into the localization branch to capture long-range dependencies for more robustness to large displacements of the target. A multi-scale learnable attention module is used to guide these three branches to exploit discriminative features for better performance. Extensive experiments on 9 challenging benchmarks, namely VOT2016, VOT2018, VOT2019, OTB100, LTB35, LaSOT, TC128, UAV123 and VisDrone-SOT2019 demonstrate that SiamCAN achieves leading accuracy with high efficiency. Our source code is available at https://isrc.iscas.ac.cn/gitlab/research/siamcan.},
  archive      = {J_TIP},
  author       = {Wenzhang Zhou and Longyin Wen and Libo Zhang and Dawei Du and Tiejian Luo and Yanjun Wu},
  doi          = {10.1109/TIP.2021.3060905},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3597-3609},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {SiamCAN: Real-time visual tracking based on siamese center-aware network},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-dimensional visual data completion via low-rank tensor
representation under coupled transform. <em>TIP</em>, <em>30</em>,
3581–3596. (<a href="https://doi.org/10.1109/TIP.2021.3062995">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the tensor completion problem, which aims to recover missing information of multi-dimensional images. How to represent a low-rank structure embedded in the underlying data is the key issue in tensor completion. In this work, we suggest a novel low-rank tensor representation based on coupled transform, which fully exploits the spatial multi-scale nature and redundancy in spatial and spectral/temporal dimensions, leading to a better low tensor multi-rank approximation. More precisely, this representation is achieved by using two-dimensional framelet transform for the two spatial dimensions, one/two-dimensional Fourier transform for the temporal/spectral dimension, and then Karhunen-Loéve transform (via singular value decomposition) for the transformed tensor. Based on this low-rank tensor representation, we formulate a novel low-rank tensor completion model for recovering missing information in multi-dimensional visual data, which leads to a convex optimization problem. To tackle the proposed model, we develop the alternating directional method of multipliers (ADMM) algorithm tailored for the structured optimization problem. Numerical examples on color images, multispectral images, and videos illustrate that the proposed method outperforms many state-of-the-art methods in qualitative and quantitative aspects.},
  archive      = {J_TIP},
  author       = {Jian-Li Wang and Ting-Zhu Huang and Xi-Le Zhao and Tai-Xiang Jiang and Michael K. Ng},
  doi          = {10.1109/TIP.2021.3062995},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3581-3596},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Multi-dimensional visual data completion via low-rank tensor representation under coupled transform},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bayesian low rank tensor ring for image recovery.
<em>TIP</em>, <em>30</em>, 3568–3580. (<a
href="https://doi.org/10.1109/TIP.2021.3062195">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low rank tensor ring based data recovery can recover missing image entries in signal acquisition and transformation. The recently proposed tensor ring (TR) based completion algorithms generally solve the low rank optimization problem by alternating least squares method with predefined ranks, which may easily lead to overfitting when the unknown ranks are set too large and only a few measurements are available. In this article, we present a Bayesian low rank tensor ring completion method for image recovery by automatically learning the low-rank structure of data. A multiplicative interaction model is developed for low rank tensor ring approximation, where sparsity-inducing hierarchical prior is placed over horizontal and frontal slices of core factors. Compared with most of the existing methods, the proposed one is free of parameter-tuning, and the TR ranks can be obtained by Bayesian inference. Numerical experiments, including synthetic data, real-world color images and YaleFace dataset, show that the proposed method outperforms state-of-the-art ones, especially in terms of recovery accuracy.},
  archive      = {J_TIP},
  author       = {Zhen Long and Ce Zhu and Jiani Liu and Yipeng Liu},
  doi          = {10.1109/TIP.2021.3062195},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3568-3580},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Bayesian low rank tensor ring for image recovery},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-task deep learning for image segmentation using
recursive approximation tasks. <em>TIP</em>, <em>30</em>, 3555–3567. (<a
href="https://doi.org/10.1109/TIP.2021.3062726">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fully supervised deep neural networks for segmentation usually require a massive amount of pixel-level labels which are manually expensive to create. In this work, we develop a multi-task learning method to relax this constraint. We regard the segmentation problem as a sequence of approximation subproblems that are recursively defined and in increasing levels of approximation accuracy. The subproblems are handled by a framework that consists of 1) a segmentation task that learns from pixel-level ground truth segmentation masks of a small fraction of the images, 2) a recursive approximation task that conducts partial object regions learning and data-driven mask evolution starting from partial masks of each object instance, and 3) other problem oriented auxiliary tasks that are trained with sparse annotations and promote the learning of dedicated features. Most training images are only labeled by (rough) partial masks, which do not contain exact object boundaries, rather than by their full segmentation masks. During the training phase, the approximation task learns the statistics of these partial masks, and the partial regions are recursively increased towards object boundaries aided by the learned information from the segmentation task in a fully data-driven fashion. The network is trained on an extremely small amount of precisely segmented images and a large set of coarse labels. Annotations can thus be obtained in a cheap way. We demonstrate the efficiency of our approach in three applications with microscopy images and ultrasound images.},
  archive      = {J_TIP},
  author       = {Rihuan Ke and Aurélie Bugeau and Nicolas Papadakis and Mark Kirkland and Peter Schuetz and Carola-Bibiane Schönlieb},
  doi          = {10.1109/TIP.2021.3062726},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3555-3567},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Multi-task deep learning for image segmentation using recursive approximation tasks},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An image contrast measure based on retinex principles.
<em>TIP</em>, <em>30</em>, 3543–3554. (<a
href="https://doi.org/10.1109/TIP.2021.3062724">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The image contrast is a feature capturing the variation of the image signal across the space. Such a feature is very useful to describe the local image structure at different scales and thus it is relevant to many computer vision applications, like image/texture retrieval and object recognition. In this work, we present MiRCo, a novel measure of image contrast derived from the Retinex theory. MiRCo is robust against in-plane rotations and light changes at multiple scales. Thanks to these properties, MiRCo enables an accurate and robust description of the local image structure. Here we describe and discuss the mathematical insights of MiRCo also in comparison with other popular contrast measures.},
  archive      = {J_TIP},
  author       = {Michela Lecca and Alessandro Rizzi and Raul Paolo Serapioni},
  doi          = {10.1109/TIP.2021.3062724},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3543-3554},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {An image contrast measure based on retinex principles},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hierarchical alternate interaction network for RGB-d salient
object detection. <em>TIP</em>, <em>30</em>, 3528–3542. (<a
href="https://doi.org/10.1109/TIP.2021.3062689">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing RGB-D Salient Object Detection (SOD) methods take advantage of depth cues to improve the detection accuracy, while pay insufficient attention to the quality of depth information. In practice, a depth map is often with uneven quality and sometimes suffers from distractors, due to various factors in the acquisition procedure. In this article, to mitigate distractors in depth maps and highlight salient objects in RGB images, we propose a Hierarchical Alternate Interactions Network (HAINet) for RGB-D SOD. Specifically, HAINet consists of three key stages: feature encoding, cross-modal alternate interaction, and saliency reasoning. The main innovation in HAINet is the Hierarchical Alternate Interaction Module (HAIM), which plays a key role in the second stage for cross-modal feature interaction. HAIM first uses RGB features to filter distractors in depth features, and then the purified depth features are exploited to enhance RGB features in turn. The alternate RGB-depth-RGB interaction proceeds in a hierarchical manner, which progressively integrates local and global contexts within a single feature scale. In addition, we adopt a hybrid loss function to facilitate the training of HAINet. Extensive experiments on seven datasets demonstrate that our HAINet not only achieves competitive performance as compared with 19 relevant state-of-the-art methods, but also reaches a real-time processing speed of 43 fps on a single NVIDIA Titan X GPU. The code and results of our method are available at https://github.com/MathLee/HAINet .},
  archive      = {J_TIP},
  author       = {Gongyang Li and Zhi Liu and Minyu Chen and Zhen Bai and Weisi Lin and Haibin Ling},
  doi          = {10.1109/TIP.2021.3062689},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3528-3542},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Hierarchical alternate interaction network for RGB-D salient object detection},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning causal temporal relation and feature discrimination
for anomaly detection. <em>TIP</em>, <em>30</em>, 3513–3527. (<a
href="https://doi.org/10.1109/TIP.2021.3062192">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weakly supervised anomaly detection is a challenging task since frame-level labels are not given in the training phase. Previous studies generally employ neural networks to learn features and produce frame-level predictions and then use multiple instance learning (MIL)-based classification loss to ensure the interclass separability of the learned features; all operations simply take into account the current time information as input and ignore the historical observations. According to investigations, these solutions are universal but ignore two essential factors, i.e., the temporal cue and feature discrimination. The former introduces temporal context to enhance the current time feature, and the latter enforces the samples of different categories to be more separable in the feature space. In this article, we propose a method that consists of four modules to leverage the effect of these two ignored factors. The causal temporal relation (CTR) module captures local-range temporal dependencies among features to enhance features. The classifier (CL) projects enhanced features to the category space using the causal convolution and further expands the temporal modeling range. Two additional modules, namely, compactness (CP) and dispersion (DP) modules, are designed to learn the discriminative power of features, where the compactness module ensures the intraclass compactness of normal features, and the dispersion module enhances the interclass dispersion. Extensive experiments on three public benchmarks demonstrate the significance of causal temporal relations and feature discrimination for anomaly detection and the superiority of our proposed method.},
  archive      = {J_TIP},
  author       = {Peng Wu and Jing Liu},
  doi          = {10.1109/TIP.2021.3062192},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3513-3527},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Learning causal temporal relation and feature discrimination for anomaly detection},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A novel visual representation on text using diverse
conditional GAN for visual recognition. <em>TIP</em>, <em>30</em>,
3499–3512. (<a href="https://doi.org/10.1109/TIP.2021.3061927">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic image visual recognition can make full use of largely available images with text descriptions on social media platforms to build large-scale image labeled datasets. In this paper, we propose a novel visual text representation, named DG-VRT (Diverse GAN-Visual Representation on Text), which extracts visual features from synthetic images generated by a diverse conditional Generative Adversarial Network (DCGAN) on the text, for visual recognition. The DCGAN incorporates the current state-of-the-art text-to-image GANs and generates multiple synthetic images with various prior noises conditioned on a text. Then we extract deep visual features from the generated synthetic images to explore the underlying visual concepts and provide a visual transformation on text in feature space. Finally, we combine image-level visual features, text-level features and visual features based on synthetic images together to recognize the images, and we also extend the proposed work to semantic segmentation. We conduct extensive experiments on two benchmark datasets and the experimental results demonstrate the efficacy of our proposed representation on text for visual recognition.},
  archive      = {J_TIP},
  author       = {Tao Hu and Chengjiang Long and Chunxia Xiao},
  doi          = {10.1109/TIP.2021.3061927},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3499-3512},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A novel visual representation on text using diverse conditional GAN for visual recognition},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Complementary, heterogeneous and adversarial networks for
image-to-image translation. <em>TIP</em>, <em>30</em>, 3487–3498. (<a
href="https://doi.org/10.1109/TIP.2021.3061286">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image-to-image translation is to transfer images from a source domain to a target domain. Conditional Generative Adversarial Networks (GANs) have enabled a variety of applications. Initial GANs typically conclude one single generator for generating a target image. Recently, using multiple generators has shown promising results in various tasks. However, generators in these works are typically of homogeneous architectures. In this paper, we argue that heterogeneous generators are complementary to each other and will benefit the generation of images. By heterogeneous, we mean that generators are of different architectures, focus on diverse positions, and perform over multiple scales. To this end, we build two generators by using a deep U-Net and a shallow residual network, respectively. The former concludes a series of down-sampling and up-sampling layers, which typically have large perception field and great spatial locality. In contrast, the residual network has small perceptual fields and works well in characterizing details, especially textures and local patterns. Afterwards, we use a gated fusion network to combine these two generators for producing a final output. The gated fusion unit automatically induces heterogeneous generators to focus on different positions and complement each other. Finally, we propose a novel approach to integrate multi-level and multi-scale features in the discriminator. This multi-layer integration discriminator encourages generators to produce realistic details from coarse to fine scales. We quantitatively and qualitatively evaluate our model on various benchmark datasets. Experimental results demonstrate that our method significantly improves the quality of transferred images, across a variety of image-to-image translation tasks. We have made our code and results publicly available: http://aiart.live/chan/.},
  archive      = {J_TIP},
  author       = {Fei Gao and Xingxin Xu and Jun Yu and Meimei Shang and Xiang Li and Dacheng Tao},
  doi          = {10.1109/TIP.2021.3061286},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3487-3498},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Complementary, heterogeneous and adversarial networks for image-to-image translation},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Uncertainty-aware blind image quality assessment in the
laboratory and wild. <em>TIP</em>, <em>30</em>, 3474–3486. (<a
href="https://doi.org/10.1109/TIP.2021.3061932">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Performance of blind image quality assessment (BIQA) models has been significantly boosted by end-to-end optimization of feature engineering and quality regression. Nevertheless, due to the distributional shift between images simulated in the laboratory and captured in the wild, models trained on databases with synthetic distortions remain particularly weak at handling realistic distortions (and vice versa). To confront the cross-distortion-scenario challenge, we develop a unified BIQA model and an approach of training it for both synthetic and realistic distortions. We first sample pairs of images from individual IQA databases, and compute a probability that the first image of each pair is of higher quality. We then employ the fidelity loss to optimize a deep neural network for BIQA over a large number of such image pairs. We also explicitly enforce a hinge constraint to regularize uncertainty estimation during optimization. Extensive experiments on six IQA databases show the promise of the learned method in blindly assessing image quality in the laboratory and wild. In addition, we demonstrate the universality of the proposed training strategy by using it to improve existing BIQA models.},
  archive      = {J_TIP},
  author       = {Weixia Zhang and Kede Ma and Guangtao Zhai and Xiaokang Yang},
  doi          = {10.1109/TIP.2021.3061932},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3474-3486},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Uncertainty-aware blind image quality assessment in the laboratory and wild},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Band representation-based semi-supervised low-light image
enhancement: Bridging the gap between signal fidelity and perceptual
quality. <em>TIP</em>, <em>30</em>, 3461–3473. (<a
href="https://doi.org/10.1109/TIP.2021.3062184">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It has been widely acknowledged that under-exposure causes a variety of visual quality degradation because of intensive noise, decreased visibility, biased color, etc . To alleviate these issues, a novel semi-supervised learning approach is proposed in this paper for low-light image enhancement. More specifically, we propose a deep recursive band network (DRBN) to recover a linear band representation of an enhanced normal-light image based on the guidance of the paired low/normal-light images. Such design philosophy enables the principled network to generate a quality improved one by reconstructing the given bands based upon another learnable linear transformation which is perceptually driven by an image quality assessment neural network. On one hand, the proposed network is delicately developed to obtain a variety of coarse-to-fine band representations, of which the estimations benefit each other in a recursive process mutually. On the other hand, the extracted band representation of the enhanced image in the recursive band learning stage of DRBN is capable of bridging the gap between the restoration knowledge of paired data and the perceptual quality preference to high-quality images. Subsequently, the band recomposition learns to recompose the band representation towards fitting perceptual regularization of high-quality images with the perceptual guidance. The proposed architecture can be flexibly trained with both paired and unpaired data. Extensive experiments demonstrate that our method produces better enhanced results with visually pleasing contrast and color distributions, as well as well-restored structural details.},
  archive      = {J_TIP},
  author       = {Wenhan Yang and Shiqi Wang and Yuming Fang and Yue Wang and Jiaying Liu},
  doi          = {10.1109/TIP.2021.3062184},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3461-3473},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Band representation-based semi-supervised low-light image enhancement: Bridging the gap between signal fidelity and perceptual quality},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). RelightGAN: Instance-level generative adversarial network
for face illumination transfer. <em>TIP</em>, <em>30</em>, 3450–3460.
(<a href="https://doi.org/10.1109/TIP.2021.3061933">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face illumination perception and processing is a significantly difficult issue especially due to asymmetric shadings, local highlights, and local shadows. This study focuses on the face illumination transfer problem, which is to transfer the illumination style from a reference face image to a target face image while preserving other attributes. Such an instance-level transfer task is more challenging than the domain-level one that only considers the pre-defined lighting categories. To tackle this problem, we develop an instance-level conditional Generative Adversarial Networks (GAN). Specifically, face identifier is integrated into GAN learning, which enables an individual-specific low-level visual generation. Moreover, the illumination-inspired attention mechanism is conducted to allow GAN to well handle the local lighting effect. Our method requires neither lighting categorization, 3D information, nor strict face alignment, which are often employed by traditional methods. Experiments demonstrate that our method achieves significantly better results than previous methods.},
  archive      = {J_TIP},
  author       = {Weihong Xu and Xiaohua Xie and Jianhuang Lai},
  doi          = {10.1109/TIP.2021.3061933},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3450-3460},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {RelightGAN: Instance-level generative adversarial network for face illumination transfer},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Logarithmic norm regularized low-rank factorization for
matrix and tensor completion. <em>TIP</em>, <em>30</em>, 3434–3449. (<a
href="https://doi.org/10.1109/TIP.2021.3061908">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Matrix and tensor completion aim to recover the incomplete two- and higher-dimensional observations using the low-rank property. Conventional techniques usually minimize the convex surrogate of rank (such as the nuclear norm), which, however, leads to the suboptimal solution for the low-rank recovery. In this paper, we propose a new definition of matrix/tensor logarithmic norm to induce a sparsity-driven surrogate for rank. More importantly, the factor matrix/tensor norm surrogate theorems are derived, which are capable of factoring the norm of large-scale matrix/tensor into those of small-scale matrices/tensors equivalently. Based upon surrogate theorems, we propose two new algorithms called Logarithmic norm Regularized Matrix Factorization (LRMF) and Logarithmic norm Regularized Tensor Factorization (LRTF). These two algorithms incorporate the logarithmic norm regularization with the matrix/tensor factorization and hence achieve more accurate low-rank approximation and high computational efficiency. The resulting optimization problems are solved using the framework of alternating minimization with the proof of convergence. Simulation results on both synthetic and real-world data demonstrate the superior performance of the proposed LRMF and LRTF algorithms over the state-of-the-art algorithms in terms of accuracy and efficiency.},
  archive      = {J_TIP},
  author       = {Lin Chen and Xue Jiang and Xingzhao Liu and Zhixin Zhou},
  doi          = {10.1109/TIP.2021.3061908},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3434-3449},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Logarithmic norm regularized low-rank factorization for matrix and tensor completion},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Joint depth and defocus estimation from a single image using
physical consistency. <em>TIP</em>, <em>30</em>, 3419–3433. (<a
href="https://doi.org/10.1109/TIP.2021.3061901">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimating depth and defocus maps are two fundamental tasks in computer vision. Recently, many methods explore these two tasks separately with the help of the powerful feature learning ability of deep learning and these methods have achieved impressive progress. However, due to the difficulty in densely labeling depth and defocus on real images, these methods are mostly based on synthetic training dataset, and the performance of learned network degrades significantly on real images. In this paper, we tackle a new task that jointly estimates depth and defocus from a single image. We design a dual network with two subnets respectively for estimating depth and defocus. The network is jointly trained on synthetic dataset with a physical constraint to enforce the physical consistency between depth and defocus. Moreover, we design a simple method to label depth and defocus order on real image dataset, and design two novel metrics to measure accuracies of depth and defocus estimation on real images. Comprehensive experiments demonstrate that joint training for depth and defocus estimation using physical consistency constraint enables these two subnets to guide each other, and effectively improves their depth and defocus estimation performance on real defocused image dataset.},
  archive      = {J_TIP},
  author       = {Anmei Zhang and Jian Sun},
  doi          = {10.1109/TIP.2021.3061901},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3419-3433},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Joint depth and defocus estimation from a single image using physical consistency},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Batch coherence-driven network for part-aware person
re-identification. <em>TIP</em>, <em>30</em>, 3405–3418. (<a
href="https://doi.org/10.1109/TIP.2021.3060909">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing part-aware person re-identification methods typically employ two separate steps: namely, body part detection and part-level feature extraction. However, part detection introduces an additional computational cost and is inherently challenging for low-quality images. Accordingly, in this work, we propose a simple framework named Batch Coherence-Driven Network (BCD-Net) that bypasses body part detection during both the training and testing phases while still learning semantically aligned part features. Our key observation is that the statistics in a batch of images are stable, and therefore that batch-level constraints are robust. First, we introduce a batch coherence-guided channel attention (BCCA) module that highlights the relevant channels for each respective part from the output of a deep backbone model. We investigate channel-part correspondence using a batch of training images, then impose a novel batch-level supervision signal that helps BCCA to identify part-relevant channels. Second, the mean position of a body part is robust and consequently coherent between batches throughout the training process. Accordingly, we introduce a pair of regularization terms based on the semantic consistency between batches. The first term regularizes the high responses of BCD-Net for each part on one batch in order to constrain it within a predefined area, while the second encourages the aggregate of BCD-Net’s responses for all parts covering the entire human body. The above constraints guide BCD-Net to learn diverse, complementary, and semantically aligned part-level features. Extensive experimental results demonstrate that BCD-Net consistently achieves state-of-the-art performance on four large-scale ReID benchmarks.},
  archive      = {J_TIP},
  author       = {Kan Wang and Pengfei Wang and Changxing Ding and Dacheng Tao},
  doi          = {10.1109/TIP.2021.3060909},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3405-3418},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Batch coherence-driven network for part-aware person re-identification},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). RefineDNet: A weakly supervised refinement framework for
single image dehazing. <em>TIP</em>, <em>30</em>, 3391–3404. (<a
href="https://doi.org/10.1109/TIP.2021.3060873">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Haze-free images are the prerequisites of many vision systems and algorithms, and thus single image dehazing is of paramount importance in computer vision. In this field, prior-based methods have achieved initial success. However, they often introduce annoying artifacts to outputs because their priors can hardly fit all situations. By contrast, learning-based methods can generate more natural results. Nonetheless, due to the lack of paired foggy and clear outdoor images of the same scenes as training samples, their haze removal abilities are limited. In this work, we attempt to merge the merits of prior-based and learning-based approaches by dividing the dehazing task into two sub-tasks, i.e., visibility restoration and realness improvement. Specifically, we propose a two-stage weakly supervised dehazing framework, RefineDNet. In the first stage, RefineDNet adopts the dark channel prior to restore visibility. Then, in the second stage, it refines preliminary dehazing results of the first stage to improve realness via adversarial learning with unpaired foggy and clear images. To get more qualified results, we also propose an effective perceptual fusion strategy to blend different dehazing outputs. Extensive experiments corroborate that RefineDNet with the perceptual fusion has an outstanding haze removal capability and can also produce visually pleasing results. Even implemented with basic backbone networks, RefineDNet can outperform supervised dehazing approaches as well as other state-of-the-art methods on indoor and outdoor datasets. To make our results reproducible, relevant code and data are available at https://github.com/xiaofeng94/RefineDNet-for-dehazing.},
  archive      = {J_TIP},
  author       = {Shiyu Zhao and Lin Zhang and Ying Shen and Yicong Zhou},
  doi          = {10.1109/TIP.2021.3060873},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3391-3404},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {RefineDNet: A weakly supervised refinement framework for single image dehazing},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). CDNet: Complementary depth network for RGB-d salient object
detection. <em>TIP</em>, <em>30</em>, 3376–3390. (<a
href="https://doi.org/10.1109/TIP.2021.3060167">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current RGB-D salient object detection (SOD) methods utilize the depth stream as complementary information to the RGB stream. However, the depth maps are usually of low-quality in existing RGB-D SOD datasets. Most RGB-D SOD networks trained with these datasets would produce error-prone results. In this paper, we propose a novel Complementary Depth Network (CDNet) to well exploit saliency-informative depth features for RGB-D SOD. To alleviate the influence of low-quality depth maps to RGB-D SOD, we propose to select saliency-informative depth maps as the training targets and leverage RGB features to estimate meaningful depth maps. Besides, to learn robust depth features for accurate prediction, we propose a new dynamic scheme to fuse the depth features extracted from the original and estimated depth maps with adaptive weights. What&#39;s more, we design a two-stage cross-modal feature fusion scheme to well integrate the depth features with the RGB ones, further improving the performance of our CDNet on RGB-D SOD. Experiments on seven benchmark datasets demonstrate that our CDNet outperforms state-of-the-art RGB-D SOD methods. The code is publicly available at https://github.com/blanclist/CDNet.},
  archive      = {J_TIP},
  author       = {Wen-Da Jin and Jun Xu and Qi Han and Yi Zhang and Ming-Ming Cheng},
  doi          = {10.1109/TIP.2021.3060167},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3376-3390},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {CDNet: Complementary depth network for RGB-D salient object detection},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A deep ordinal distortion estimation approach for distortion
rectification. <em>TIP</em>, <em>30</em>, 3362–3375. (<a
href="https://doi.org/10.1109/TIP.2021.3061283">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Radial distortion has widely existed in the images captured by popular wide-angle cameras and fisheye cameras. Despite the long history of distortion rectification, accurately estimating the distortion parameters from a single distorted image is still challenging. The main reason is that these parameters are implicit to image features, influencing the networks to learn the distortion information fully. In this work, we propose a novel distortion rectification approach that can obtain more accurate parameters with higher efficiency. Our key insight is that distortion rectification can be cast as a problem of learning an ordinal distortion from a single distorted image. To solve this problem, we design a local-global associated estimation network that learns the ordinal distortion to approximate the realistic distortion distribution. In contrast to the implicit distortion parameters, the proposed ordinal distortion has a more explicit relationship with image features, and significantly boosts the distortion perception of neural networks. Considering the redundancy of distortion information, our approach only uses a patch of the distorted image for the ordinal distortion estimation, showing promising applications in efficient distortion rectification. In the distortion rectification field, we are the first to unify the heterogeneous distortion parameters into a learning-friendly intermediate representation through ordinal distortion, bridging the gap between image feature and distortion rectification. The experimental results demonstrate that our approach outperforms the state-of-the-art methods by a significant margin, with approximately 23\% improvement on the quantitative evaluation while displaying the best performance on visual appearance.},
  archive      = {J_TIP},
  author       = {Kang Liao and Chunyu Lin and Yao Zhao},
  doi          = {10.1109/TIP.2021.3061283},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3362-3375},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A deep ordinal distortion estimation approach for distortion rectification},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Pareto-optimal bit allocation for collaborative
intelligence. <em>TIP</em>, <em>30</em>, 3348–3361. (<a
href="https://doi.org/10.1109/TIP.2021.3060875">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent studies, collaborative intelligence (CI) has emerged as a promising framework for deployment of Artificial Intelligence (AI)-based services on mobile/edge devices. In CI, the AI model (a deep neural network) is split between the edge and the cloud, and intermediate features are sent from the edge sub-model to the cloud sub-model. In this article, we study bit allocation for feature coding in multi-stream CI systems. We model task distortion as a function of rate using convex surfaces similar to those found in distortion-rate theory. Using such models, we are able to provide closed-form bit allocation solutions for single-task systems and scalarized multi-task systems. Moreover, we provide analytical characterization of the full Pareto set for 2-stream k-task systems, and bounds on the Pareto set for 3-stream 2-task systems. Analytical results are examined on a variety of DNN models from the literature to demonstrate wide applicability of the results.},
  archive      = {J_TIP},
  author       = {Saeed Ranjbar Alvar and Ivan V. Bajić},
  doi          = {10.1109/TIP.2021.3060875},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3348-3361},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Pareto-optimal bit allocation for collaborative intelligence},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Jointly modeling motion and appearance cues for robust RGB-t
tracking. <em>TIP</em>, <em>30</em>, 3335–3347. (<a
href="https://doi.org/10.1109/TIP.2021.3060862">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we propose a novel RGB-T tracking framework by jointly modeling both appearance and motion cues. First, to obtain a robust appearance model, we develop a novel late fusion method to infer the fusion weight maps of both RGB and thermal (T) modalities. The fusion weights are determined by using offline-trained global and local multimodal fusion networks, and then adopted to linearly combine the response maps of RGB and T modalities. Second, when the appearance cue is unreliable, we comprehensively take motion cues, i.e., target and camera motions, into account to make the tracker robust. We further propose a tracker switcher to switch the appearance and motion trackers flexibly. Numerous results on three recent RGB-T tracking datasets show that the proposed tracker performs significantly better than other state-of-the-art algorithms.},
  archive      = {J_TIP},
  author       = {Pengyu Zhang and Jie Zhao and Chunjuan Bo and Dong Wang and Huchuan Lu and Xiaoyun Yang},
  doi          = {10.1109/TIP.2021.3060862},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3335-3347},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Jointly modeling motion and appearance cues for robust RGB-T tracking},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Affine transformation-based deep frame prediction.
<em>TIP</em>, <em>30</em>, 3321–3334. (<a
href="https://doi.org/10.1109/TIP.2021.3060803">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a neural network model to estimate the current frame from two reference frames, using affine transformation and adaptive spatially-varying filters. The estimated affine transformation allows for using shorter filters compared to existing approaches for deep frame prediction. The predicted frame is used as a reference for coding the current frame. Since the proposed model is available at both encoder and decoder, there is no need to code or transmit motion information for the predicted frame. By making use of dilated convolutions and reduced filter length, our model is significantly smaller, yet more accurate, than any of the neural networks in prior works on this topic. Two versions of the proposed model - one for unidirectional, and one for bi-directional prediction - are trained using a combination of Discrete Cosine Transform (DCT)-based ℓ 1 -loss with various transform sizes, multi-scale Mean Squared Error (MSE) loss, and an object context reconstruction loss. The trained models are integrated with the HEVC video coding pipeline. The experiments show that the proposed models achieve about 7.3\%, 5.4\%, and 4.2\% bit savings for the luminance component on average in the Low delay P, Low delay, and Random access configurations, respectively.},
  archive      = {J_TIP},
  author       = {Hyomin Choi and Ivan V. Bajić},
  doi          = {10.1109/TIP.2021.3060803},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3321-3334},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Affine transformation-based deep frame prediction},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning wavefront coding for extended depth of field
imaging. <em>TIP</em>, <em>30</em>, 3307–3320. (<a
href="https://doi.org/10.1109/TIP.2021.3060166">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Depth of field is an important factor of imaging systems that highly affects the quality of the acquired spatial information. Extended depth of field (EDoF) imaging is a challenging ill-posed problem and has been extensively addressed in the literature. We propose a computational imaging approach for EDoF, where we employ wavefront coding via a diffractive optical element (DOE) and we achieve deblurring through a convolutional neural network. Thanks to the end-to-end differentiable modeling of optical image formation and computational post-processing, we jointly optimize the optical design, i.e., DOE, and the deblurring through standard gradient descent methods. Based on the properties of the underlying refractive lens and the desired EDoF range, we provide an analytical expression for the search space of the DOE, which is instrumental in the convergence of the end-to-end network. We achieve superior EDoF imaging performance compared to the state of the art, where we demonstrate results with minimal artifacts in various scenarios, including deep 3D scenes and broadband imaging.},
  archive      = {J_TIP},
  author       = {Ugur Akpinar and Erdem Sahin and Monjurul Meem and Rajesh Menon and Atanas Gotchev},
  doi          = {10.1109/TIP.2021.3060166},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3307-3320},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Learning wavefront coding for extended depth of field imaging},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Progressive modality cooperation for multi-modality domain
adaptation. <em>TIP</em>, <em>30</em>, 3293–3306. (<a
href="https://doi.org/10.1109/TIP.2021.3052083">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we propose a new generic multi-modality domain adaptation framework called Progressive Modality Cooperation (PMC) to transfer the knowledge learned from the source domain to the target domain by exploiting multiple modality clues ( e.g. , RGB and depth) under the multi-modality domain adaptation (MMDA) and the more general multi-modality domain adaptation using privileged information (MMDA-PI) settings. Under the MMDA setting, the samples in both domains have all the modalities. Through effective collaboration among multiple modalities, the two newly proposed modules in our PMC can select the reliable pseudo-labeled target samples, which captures the modality-specific information and modality-integrated information, respectively. Under the MMDA-PI setting, some modalities are missing in the target domain. Hence, to better exploit the multi-modality data in the source domain, we further propose the PMC with privileged information (PMC-PI) method by proposing a new multi-modality data generation (MMG) network. MMG generates the missing modalities in the target domain based on the source domain data by considering both domain distribution mismatch and semantics preservation, which are respectively achieved by using adversarial learning and conditioning on weighted pseudo semantic class labels. Extensive experiments on three image datasets and eight video datasets for various multi-modality cross-domain visual recognition tasks under both MMDA and MMDA-PI settings clearly demonstrate the effectiveness of our proposed PMC framework.},
  archive      = {J_TIP},
  author       = {Weichen Zhang and Dong Xu and Jing Zhang and Wanli Ouyang},
  doi          = {10.1109/TIP.2021.3052083},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3293-3306},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Progressive modality cooperation for multi-modality domain adaptation},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Temporal reasoning guided QoE evaluation for mobile live
video broadcasting. <em>TIP</em>, <em>30</em>, 3279–3292. (<a
href="https://doi.org/10.1109/TIP.2021.3060255">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quality of experience (QoE) that serves as a direct evaluation of viewing experience from the end users is of vital importance for network optimization, and should be constantly monitored. Unlike existing video-on-demand streaming services, real-time interactivity is critical to the mobile live broadcasting experience for both broadcasters and their audiences. While existing QoE metrics that are validated on limited video contents and synthetic stall patterns have shown effectiveness in their trained QoE benchmarks, a common caveat is that they often encounter challenges in practical live broadcasting scenarios, where one needs to accurately understand the activity in the video with fluctuating QoE and figure out what is going to happen to support the real-time feedback to the broadcaster. In this paper, we propose a temporal relational reasoning guided QoE evaluation approach for mobile live video broadcasting, namely TRR-QoE, which explicitly attends to the temporal relationships between consecutive frames to achieve a more comprehensive understanding of the distortion-aware variation. In our design, video frames are first processed by deep neural network (DNN) to extract quality-indicative features. Afterwards, besides explicitly integrating features of individual frames to account for the spatial distortion information, multi-scale temporal relational information corresponding to diverse temporal resolutions are made full use of to capture temporal-distortion-aware variation. As a result, the overall QoE prediction could be derived by combining both aspects. The results of experiments conducted on a number of benchmark databases demonstrate the superiority of TRR-QoE over the representative state-of-the-art metrics.},
  archive      = {J_TIP},
  author       = {Pengfei Chen and Leida Li and Jinjian Wu and Yabin Zhang and Weisi Lin},
  doi          = {10.1109/TIP.2021.3060255},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3279-3292},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Temporal reasoning guided QoE evaluation for mobile live video broadcasting},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Kalman filter for spatial-temporal regularized correlation
filters. <em>TIP</em>, <em>30</em>, 3263–3278. (<a
href="https://doi.org/10.1109/TIP.2021.3060164">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider visual tracking in numerous applications of computer vision and seek to achieve optimal tracking accuracy and robustness based on various evaluation criteria for applications in intelligent monitoring during disaster recovery activities. We propose a novel framework to integrate a Kalman filter (KF) with spatial-temporal regularized correlation filters (STRCF) for visual tracking to overcome the instability problem due to large-scale application variation. To solve the problem of target loss caused by sudden acceleration and steering, we present a stride length control method to limit the maximum amplitude of the output state of the framework, which provides a reasonable constraint based on the laws of motion of objects in real-world scenarios. Moreover, we analyze the attributes influencing the performance of the proposed framework in large-scale experiments. The experimental results illustrate that the proposed framework outperforms STRCF on OTB-2013, OTB-2015 and Temple-Color datasets for some specific attributes and achieves optimal visual tracking for computer vision. Compared with STRCF, our framework achieves AUC gains of 2.8\%, 2\%, 1.8\%, 1.3\%, and 2.4\% for the background clutter, illumination variation, occlusion, out-of-plane rotation, and out-of-view attributes on the OTB-2015 datasets, respectively. For sporting events, our framework presents much better performance and greater robustness than its competitors.},
  archive      = {J_TIP},
  author       = {Sheng Feng and Keli Hu and En Fan and Liping Zhao and Chengdong Wu},
  doi          = {10.1109/TIP.2021.3060164},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3263-3278},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Kalman filter for spatial-temporal regularized correlation filters},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Local correspondence network for weakly supervised temporal
sentence grounding. <em>TIP</em>, <em>30</em>, 3252–3262. (<a
href="https://doi.org/10.1109/TIP.2021.3058614">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weakly supervised temporal sentence grounding has better scalability and practicability than fully supervised methods in real-world application scenarios. However, most of existing methods cannot model the fine-grained video-text local correspondences well and do not have effective supervision information for correspondence learning, thus yielding unsatisfying performance. To address the above issues, we propose an end-to-end Local Correspondence Network (LCNet) for weakly supervised temporal sentence grounding. The proposed LCNet enjoys several merits. First, we represent video and text features in a hierarchical manner to model the fine-grained video-text correspondences. Second, we design a self-supervised cycle-consistent loss as a learning guidance for video and text matching. To the best of our knowledge, this is the first work to fully explore the fine-grained correspondences between video and text for temporal sentence grounding by using self-supervised learning. Extensive experimental results on two benchmark datasets demonstrate that the proposed LCNet significantly outperforms existing weakly supervised methods.},
  archive      = {J_TIP},
  author       = {Wenfei Yang and Tianzhu Zhang and Yongdong Zhang and Feng Wu},
  doi          = {10.1109/TIP.2021.3058614},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3252-3262},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Local correspondence network for weakly supervised temporal sentence grounding},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Boosting single image super-resolution learnt from implicit
multi-image prior. <em>TIP</em>, <em>30</em>, 3240–3251. (<a
href="https://doi.org/10.1109/TIP.2021.3059507">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning-based single image super-resolution (SISR) aims to learn a versatile mapping from low resolution (LR) image to its high resolution (HR) version. The critical challenge is to bias the network training towards continuous and sharp edges. For the first time in this work, we propose an implicit boundary prior learnt from multi-view observations to significantly mitigate the challenge in SISR we outline. Specifically, the multi-image prior that encodes both disparity information and boundary structure of the scene supervise a SISR network for edge-preserving. For simplicity, in the training procedure of our framework, light field (LF) serves as an effective multi-image prior, and a hybrid loss function jointly considers the content, structure, variance as well as disparity information from 4D LF data. Consequently, for inference, such a general training scheme boosts the performance of various SISR networks, especially for the regions along edges. Extensive experiments on representative backbone SISR architectures constantly show the effectiveness of the proposed method, leading to around 0.6 dB gain without modifying the network architecture.},
  archive      = {J_TIP},
  author       = {Dingjian Jin and Mengqi Ji and Lan Xu and Gaochang Wu and Liejun Wang and Lu Fang},
  doi          = {10.1109/TIP.2021.3059507},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3240-3251},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Boosting single image super-resolution learnt from implicit multi-image prior},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Holistic LSTM for pedestrian trajectory prediction.
<em>TIP</em>, <em>30</em>, 3229–3239. (<a
href="https://doi.org/10.1109/TIP.2021.3058599">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate predictions of future pedestrian trajectory could prevent a considerable number of traffic injuries and improve pedestrian safety. It involves multiple sources of information and real-time interactions, e.g., vehicle speed and ego-motion, pedestrian intention and historical locations. Existing methods directly apply a simple concatenation operation to combine multiple cues while their dynamics over time are less studied. In this paper, we propose a novel Long Short-Term Memory (LSTM), namely, to incorporate multiple sources of information from pedestrians and vehicles adaptively. Different from LSTM, our considers mutual interactions and explores intrinsic relations among multiple cues. First, we introduce extra memory cells to improve the transferability of LSTMs in modeling future variations. These extra memory cells include a speed cell to explicitly model vehicle speed dynamics, an intention cell to dynamically analyze pedestrian crossing intentions and a correlation cell to exploit correlations among temporal frames. These three individual cells uncover the future movement of vehicles, pedestrians and global scenes. Second, we propose a gated shifting operation to learn the movement of pedestrians. The intention of crossing the road or not would significantly affect pedestrian&#39;s spatial locations. To this end, global scene dynamics and pedestrian intention information are leveraged to model the spatial shifts. Third, we integrate the speed variations to the output gate and dynamically reweight the output channels via the scaling of vehicle speed. The movement of the vehicle would alter the scale of the predicted pedestrian bounding box: as the vehicle gets closer to the pedestrian, the bounding box is enlarging. Our rescaling process captures the relative movement and updates the size of pedestrian bounding boxes accordingly. Experiments conducted on three pedestrian trajectory forecasting benchmarks show that our achieves state-of-the-art performance.},
  archive      = {J_TIP},
  author       = {Ruijie Quan and Linchao Zhu and Yu Wu and Yi Yang},
  doi          = {10.1109/TIP.2021.3058599},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3229-3239},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Holistic LSTM for pedestrian trajectory prediction},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Spatially constrained online dictionary learning for source
separation. <em>TIP</em>, <em>30</em>, 3217–3228. (<a
href="https://doi.org/10.1109/TIP.2021.3058558">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Whether in medical imaging, astronomy or remote sensing, the data are increasingly complex. In addition to the spatial dimension, the data may contain temporal or spectral information that characterises the different sources present in the image. The compromise between spatial resolution and temporal/spectral resolution is often at the expense of spatial resolution, resulting in a potentially large mixing of sources in the same pixel/voxel. Source separation methods must incorporate spatial information to estimate the contribution and signature of each source in the image. We consider the particular case where the position of the sources is approximately known thanks to external information that may come from another imaging modality or from a priori knowledge. We propose a spatially constrained dictionary learning source separation algorithm that uses e.g. high resolution segmentation map or regions of interest defined by an expert to regularise the source contribution estimation. The originality of the proposed model is the replacement of the sparsity constraint classically expressed in the form of an $\ell _{1}$ penalty on the localisation of sources by an indicator function exploiting the external source localisation information. The model is easily adaptable to different applications by adding or modifying the constraints on the sources properties in the optimisation problem. The performance of this algorithm has been validated on synthetic and quasi-real data, before being applied to real data previously analysed by other methods of the literature in order to compare the results. To illustrate the potential of the approach, different applications have been considered, from scintigraphic data to astronomy or fMRI data.},
  archive      = {J_TIP},
  author       = {Argheesh Bhanot and Céline Meillier and Fabrice Heitz and Laura Harsan},
  doi          = {10.1109/TIP.2021.3058558},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3217-3228},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Spatially constrained online dictionary learning for source separation},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Looking for the detail and context devils: High-resolution
salient object detection. <em>TIP</em>, <em>30</em>, 3204–3216. (<a
href="https://doi.org/10.1109/TIP.2020.3045624">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, Salient Object Detection (SOD) has shown great success with the achievements of large-scale benchmarks and deep learning techniques. However, existing SOD methods mainly focus on natural images with low-resolutions, e.g., $400\times 400$ or less. This drawback hinders them for advanced practical applications, which need high-resolution, detail-aware results. Besides, lacking of the boundary detail and semantic context of salient objects is also a key concern for accurate SOD. To address these issues, in this work we focus on the High-Resolution Salient Object Detection (HRSOD) task. Technically, we propose the first end-to-end learnable framework, named Dual ReFinement Network (DRFNet), for fully automatic HRSOD. More specifically, the proposed DRFNet consists of a shared feature extractor and two effective refinement heads. By decoupling the detail and context information, one refinement head adopts a global-aware feature pyramid. Without increasing too much computational burden, it can boost the spatial detail information, which narrows the gap between high-level semantics and low-level details. In parallel, the other refinement head adopts hybrid dilated convolutional blocks and group-wise upsamplings, which are very efficient in extracting contextual information. Based on the dual refinements, our approach can enlarge receptive fields and obtain more discriminative features from high-resolution images. Experimental results on high-resolution benchmarks (the public DUT-HRSOD and the proposed DAVIS-SOD) demonstrate that our method is not only efficient but also performs more accurate than other state-of-the-arts. Besides, our method generalizes well on typical low-resolution benchmarks.},
  archive      = {J_TIP},
  author       = {Pingping Zhang and Wei Liu and Yi Zeng and Yinjie Lei and Huchuan Lu},
  doi          = {10.1109/TIP.2020.3045624},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3204-3216},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Looking for the detail and context devils: High-resolution salient object detection},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). FASHE: A FrActal based strategy for head pose estimation.
<em>TIP</em>, <em>30</em>, 3192–3203. (<a
href="https://doi.org/10.1109/TIP.2021.3059409">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Head pose estimation (HPE) represents a topic central to many relevant research fields and characterized by a wide application range. In particular, HPE performed using a singular RGB frame is particular suitable to be applied at best-frame-selection problems. This explains a growing interest witnessed by a large number of contributions, most of which exploit deep learning architectures and require extensive training sessions to achieve accuracy and robustness in estimating head rotations on three axes. However, methods alternative to machine learning approaches could be capable of similar if not better performance. To this regard, we present FASHE, an approach based on partitioned iterated function systems (PIFS) to represent auto-similarities within face image through a contractive affine function transforming the domain blocks extracted only once by a single frontal reference image, in a good approximation of the range blocks which the target image has been partitioned into. Pose estimation is achieved by finding the closest match between fractal code of target image and a reference array by means of Hamming distance. The results of experiments conducted exceed the state of the art on both Biwi and Ponting&#39;04 datasets as well as approaching those of the best performing methods on the challenging AFLW2000 database. In addition, the applications to GOTCHA Video Dataset demonstrate that FASHE successfully operates in-the-wild.},
  archive      = {J_TIP},
  author       = {Carmen Bisogni and Michele Nappi and Chiara Pero and Stefano Ricciardi},
  doi          = {10.1109/TIP.2021.3059409},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3192-3203},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {FASHE: A FrActal based strategy for head pose estimation},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). End-to-end learnt image compression via non-local attention
optimization and improved context modeling. <em>TIP</em>, <em>30</em>,
3179–3191. (<a href="https://doi.org/10.1109/TIP.2021.3058615">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes an end-to-end learnt lossy image compression approach, which is built on top of the deep nerual network (DNN)-based variational auto-encoder (VAE) structure with Non-Local Attention optimization and Improved Context modeling (NLAIC). Our NLAIC 1) embeds non-local network operations as non-linear transforms in both main and hyper coders for deriving respective latent features and hyperpriors by exploiting both local and global correlations, 2) applies attention mechanism to generate implicit masks that are used to weigh the features for adaptive bit allocation, and 3) implements the improved conditional entropy modeling of latent features using joint 3D convolutional neural network (CNN)-based autoregressive contexts and hyperpriors. Towards the practical application, additional enhancements are also introduced to speed up the computational processing (e.g., parallel 3D CNN-based context prediction), decrease the memory consumption (e.g., sparse non-local processing) and reduce the implementation complexity (e.g., a unified model for variable rates without re-training). The proposed model outperforms existing learnt and conventional (e.g., BPG, JPEG2000, JPEG) image compression methods, on both Kodak and Tecnick datasets with the state-of-the-art compression efficiency, for both PSNR and MS-SSIM quality measurements. We have made all materials publicly accessible at https://njuvision.github.io/NIC for reproducible research.},
  archive      = {J_TIP},
  author       = {Tong Chen and Haojie Liu and Zhan Ma and Qiu Shen and Xun Cao and Yao Wang},
  doi          = {10.1109/TIP.2021.3058615},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3179-3191},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {End-to-end learnt image compression via non-local attention optimization and improved context modeling},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust content-adaptive global registration for multimodal
retinal images using weakly supervised deep-learning framework.
<em>TIP</em>, <em>30</em>, 3167–3178. (<a
href="https://doi.org/10.1109/TIP.2021.3058570">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal retinal imaging plays an important role in ophthalmology. We propose a content-adaptive multimodal retinal image registration method in this paper that focuses on the globally coarse alignment and includes three weakly supervised neural networks for vessel segmentation, feature detection and description, and outlier rejection. We apply the proposed framework to register color fundus images with infrared reflectance and fluorescein angiography images, and compare it with several conventional and deep learning methods. Our proposed framework demonstrates a significant improvement in robustness and accuracy reflected by a higher success rate and Dice coefficient compared with other methods.},
  archive      = {J_TIP},
  author       = {Yiqian Wang and Junkang Zhang and Melina Cavichini and Dirk-Uwe G. Bartsch and William R. Freeman and Truong Q. Nguyen and Cheolhong An},
  doi          = {10.1109/TIP.2021.3058570},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3167-3178},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Robust content-adaptive global registration for multimodal retinal images using weakly supervised deep-learning framework},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Efficient style-corpus constrained learning for
photorealistic style transfer. <em>TIP</em>, <em>30</em>, 3154–3166. (<a
href="https://doi.org/10.1109/TIP.2021.3058566">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Photorealistic style transfer is a challenging task, which demands the stylized image remains real. Existing methods are still suffering from unrealistic artifacts and heavy computational cost. In this paper, we propose a novel Style-Corpus Constrained Learning (SCCL) scheme to address these issues. The style-corpus with the style-specific and style-agnostic characteristics simultaneously is proposed to constrain the stylized image with the style consistency among different samples, which improves photorealism of stylization output. By using adversarial distillation learning strategy, a simple fast-to-execute network is trained to substitute previous complex feature transforms models, which reduces the computational cost significantly. Experiments demonstrate that our method produces rich-detailed photorealistic images, with 13 ~ 50 times faster than the state-of-the-art method (WCT 2 ).},
  archive      = {J_TIP},
  author       = {Yingxu Qiao and Jiabao Cui and Fuxian Huang and Hongmin Liu and Cuizhu Bao and Xi Li},
  doi          = {10.1109/TIP.2021.3058566},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3154-3166},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Efficient style-corpus constrained learning for photorealistic style transfer},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Harmonic feature activation for few-shot semantic
segmentation. <em>TIP</em>, <em>30</em>, 3142–3153. (<a
href="https://doi.org/10.1109/TIP.2021.3058512">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot semantic segmentation remains an open problem because limited support (training) images are insufficient to represent the diverse semantics within target categories. Conventional methods typically model a target category solely using information from the support image(s), resulting in incomplete semantic activation. In this paper, we propose a novel few-shot segmentation approach, termed harmonic feature activation (HFA), with the aim to implement dense support-to-query semantic transform by incorporating the features of both query and support images. HFA is formulated as a bilinear model, which takes charge of the pixel-wise dense correlation (bilinear feature activation) between query and support images in a systematic way. HFA incorporates a low-rank decomposition procedure, which speeds up bilinear feature activation with negligible performance cost. In addition, a semantic diffusion procedure is fused with HFA, which further improves the global harmony and local consistency of the feature activation. Extensive experiments on commonly used datasets (PASCAL VOC and MS COCO) show that HFA improves the state-of-the-arts with significant margins. Code is available at https://github.com/Bibikiller/HFA.},
  archive      = {J_TIP},
  author       = {Binghao Liu and Jianbin Jiao and Qixiang Ye},
  doi          = {10.1109/TIP.2021.3058512},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3142-3153},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Harmonic feature activation for few-shot semantic segmentation},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust ellipse fitting with laplacian kernel based maximum
correntropy criterion. <em>TIP</em>, <em>30</em>, 3127–3141. (<a
href="https://doi.org/10.1109/TIP.2021.3058785">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The performance of ellipse fitting may significantly degrade in the presence of outliers, which can be caused by occlusion of the object, mirror reflection or other objects in the process of edge detection. In this paper, we propose an ellipse fitting method that is robust against the outliers, and thus maintaining stable performance when outliers can be present. We formulate an optimization problem for ellipse fitting based on the maximum entropy criterion (MCC), having the Laplacian as the kernel function from the well-known fact that the $\ell _{1}$ -norm error measure is robust to outliers. The optimization problem is highly nonlinear and non-convex, and thus is very difficult to solve. To handle this difficulty, we divide it into two subproblems and solve the two subproblems in an alternate manner through iterations. The first subproblem has a closed-form solution and the second one is cast as a convex second-order cone program (SOCP) that can reach the global solution. By so doing, the alternate iterations always converge to an optimal solution, although it can be local instead of global. Furthermore, we propose a procedure to identify failed fitting of the algorithm caused by local convergence to a wrong solution, and thus, it reduces the probability of fitting failure by restarting the algorithm at a different initialization. The proposed robust ellipse fitting method is next extended to the coupled ellipses fitting problem. Both simulated and real data verify the superior performance of the proposed ellipse fitting method over the existing methods.},
  archive      = {J_TIP},
  author       = {Chenlong Hu and Gang Wang and K. C. Ho and Junli Liang},
  doi          = {10.1109/TIP.2021.3058785},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3127-3141},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Robust ellipse fitting with laplacian kernel based maximum correntropy criterion},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). JCS: An explainable COVID-19 diagnosis system by joint
classification and segmentation. <em>TIP</em>, <em>30</em>, 3113–3126.
(<a href="https://doi.org/10.1109/TIP.2021.3058783">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, the coronavirus disease 2019 (COVID-19) has caused a pandemic disease in over 200 countries, influencing billions of humans. To control the infection, identifying and separating the infected people is the most crucial step. The main diagnostic tool is the Reverse Transcription Polymerase Chain Reaction (RT-PCR) test. Still, the sensitivity of the RT-PCR test is not high enough to effectively prevent the pandemic. The chest CT scan test provides a valuable complementary tool to the RT-PCR test, and it can identify the patients in the early-stage with high sensitivity. However, the chest CT scan test is usually time-consuming, requiring about 21.5 minutes per case. This paper develops a novel Joint Classification and Segmentation ( JCS ) system to perform real-time and explainable COVID- 19 chest CT diagnosis. To train our JCS system, we construct a large scale COVID- 19 Classification and Segmentation ( COVID-CS ) dataset, with 144,167 chest CT images of 400 COVID- 19 patients and 350 uninfected cases. 3,855 chest CT images of 200 patients are annotated with fine-grained pixel-level labels of opacifications, which are increased attenuation of the lung parenchyma. We also have annotated lesion counts, opacification areas, and locations and thus benefit various diagnosis aspects. Extensive experiments demonstrate that the proposed JCS diagnosis system is very efficient for COVID-19 classification and segmentation. It obtains an average sensitivity of 95.0\% and a specificity of 93.0\% on the classification test set, and 78.5\% Dice score on the segmentation test set of our COVID-CS dataset. The COVID-CS dataset and code are available at https://github.com/yuhuan-wu/JCS .},
  archive      = {J_TIP},
  author       = {Yu-Huan Wu and Shang-Hua Gao and Jie Mei and Jun Xu and Deng-Ping Fan and Rong-Guo Zhang and Ming-Ming Cheng},
  doi          = {10.1109/TIP.2021.3058783},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3113-3126},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {JCS: An explainable COVID-19 diagnosis system by joint classification and segmentation},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep coupled feedback network for joint exposure fusion and
image super-resolution. <em>TIP</em>, <em>30</em>, 3098–3112. (<a
href="https://doi.org/10.1109/TIP.2021.3058764">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, people are getting used to taking photos to record their daily life, however, the photos are actually not consistent with the real natural scenes. The two main differences are that the photos tend to have low dynamic range (LDR) and low resolution (LR), due to the inherent imaging limitations of cameras. The multi-exposure image fusion (MEF) and image super-resolution (SR) are two widely-used techniques to address these two issues. However, they are usually treated as independent researches. In this paper, we propose a deep Coupled Feedback Network (CF-Net) to achieve MEF and SR simultaneously. Given a pair of extremely over-exposed and under-exposed LDR images with low-resolution, our CF-Net is able to generate an image with both high dynamic range (HDR) and high-resolution. Specifically, the CF-Net is composed of two coupled recursive sub-networks, with LR over-exposed and under-exposed images as inputs, respectively. Each sub-network consists of one feature extraction block (FEB), one super-resolution block (SRB) and several coupled feedback blocks (CFB). The FEB and SRB are to extract high-level features from the input LDR image, which are required to be helpful for resolution enhancement. The CFB is arranged after SRB, and its role is to absorb the learned features from the SRBs of the two sub-networks, so that it can produce a high-resolution HDR image. We have a series of CFBs in order to progressively refine the fused high-resolution HDR image. Extensive experimental results show that our CF-Net drastically outperforms other state-of-the-art methods in terms of both SR accuracy and fusion performance. The software code is available here https://github.com/ytZhang99/CF-Net.},
  archive      = {J_TIP},
  author       = {Xin Deng and Yutong Zhang and Mai Xu and Shuhang Gu and Yiping Duan},
  doi          = {10.1109/TIP.2021.3058764},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3098-3112},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Deep coupled feedback network for joint exposure fusion and image super-resolution},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Spatial-spectral structured sparse low-rank representation
for hyperspectral image super-resolution. <em>TIP</em>, <em>30</em>,
3084–3097. (<a href="https://doi.org/10.1109/TIP.2021.3058590">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperspectral image super-resolution by fusing high-resolution multispectral image (HR-MSI) and low-resolution hyperspectral image (LR-HSI) aims at reconstructing high resolution spatial-spectral information of the scene. Existing methods mostly based on spectral unmixing and sparse representation are often developed from a low-level vision task perspective, they cannot sufficiently make use of the spatial and spectral priors available from higher-level analysis. To this issue, this paper proposes a novel HSI super-resolution method that fully considers the spatial/spectral subspace low-rank relationships between available HR-MSI/LR-HSI and latent HSI. Specifically, it relies on a new subspace clustering method named “structured sparse low-rank representation” (SSLRR), to represent the data samples as linear combinations of the bases in a given dictionary, where the sparse structure is induced by low-rank factorization for the affinity matrix. Then we exploit the proposed SSLRR model to learn the SSLRR along spatial/spectral domain from the MSI/HSI inputs. By using the learned spatial and spectral low-rank structures, we formulate the proposed HSI super-resolution model as a variational optimization problem, which can be readily solved by the ADMM algorithm. Compared with state-of-the-art hyperspectral super-resolution methods, the proposed method shows better performance on three benchmark datasets in terms of both visual and quantitative evaluatiaon.},
  archive      = {J_TIP},
  author       = {Jize Xue and Yong-Qiang Zhao and Yuanyang Bu and Wenzhi Liao and Jonathan Cheung-Wai Chan and Wilfried Philips},
  doi          = {10.1109/TIP.2021.3058590},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3084-3097},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Spatial-spectral structured sparse low-rank representation for hyperspectral image super-resolution},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Distributed learning and inference with compressed images.
<em>TIP</em>, <em>30</em>, 3069–3083. (<a
href="https://doi.org/10.1109/TIP.2021.3058545">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern computer vision requires processing large amounts of data, both while training the model and/or during inference, once the model is deployed. Scenarios where images are captured and processed in physically separated locations are increasingly common (e.g. autonomous vehicles, cloud computing, smartphones). In addition, many devices suffer from limited resources to store or transmit data (e.g. storage space, channel capacity). In these scenarios, lossy image compression plays a crucial role to effectively increase the number of images collected under such constraints. However, lossy compression entails some undesired degradation of the data that may harm the performance of the downstream analysis task at hand, since important semantic information may be lost in the process. Moreover, we may only have compressed images at training time but are able to use original images at inference time (i.e. test), or vice versa, and in such a case, the downstream model suffers from covariate shift. In this paper, we analyze this phenomenon, with a special focus on vision-based perception for autonomous driving as a paradigmatic scenario. We see that loss of semantic information and covariate shift do indeed exist, resulting in a drop in performance that depends on the compression rate. In order to address the problem, we propose dataset restoration, based on image restoration with generative adversarial networks (GANs). Our method is agnostic to both the particular image compression method and the downstream task; and has the advantage of not adding additional cost to the deployed models, which is particularly important in resource-limited devices. The presented experiments focus on semantic segmentation as a challenging use case, cover a broad range of compression rates and diverse datasets, and show how our method is able to significantly alleviate the negative effects of compression on the downstream visual task.},
  archive      = {J_TIP},
  author       = {Sudeep Katakol and Basem Elbarashy and Luis Herranz and Joost van de Weijer and Antonio M. López},
  doi          = {10.1109/TIP.2021.3058545},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3069-3083},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Distributed learning and inference with compressed images},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). HRSiam: High-resolution siamese network, towards space-borne
satellite video tracking. <em>TIP</em>, <em>30</em>, 3056–3068. (<a
href="https://doi.org/10.1109/TIP.2020.3045634">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tracking moving objects from space-borne satellite videos is a new and challenging task. The main difficulty stems from the extremely small size of the target of interest. First, because the target usually occupies only a few pixels, it is hard to obtain discriminative appearance features. Second, the small object can easily suffer from occlusion and illumination variation, making the features of objects less distinguishable from features in surrounding regions. Current state-of-the-art tracking approaches mainly consider high-level deep features of a single frame with low spatial resolution, and hardly benefit from inter-frame motion information inherent in videos. Thus, they fail to accurately locate such small objects and handle challenging scenarios in satellite videos. In this article, we successfully design a lightweight parallel network with a high spatial resolution to locate the small objects in satellite videos. This architecture guarantees real-time and precise localization when applied to the Siamese Trackers. Moreover, a pixel-level refining model based on online moving object detection and adaptive fusion is proposed to enhance the tracking robustness in satellite videos. It models the video sequence in time to detect the moving targets in pixels and has ability to take full advantage of tracking and detecting. We conduct quantitative experiments on real satellite video datasets, and the results show the proposed HIGH-RESOLUTION SIAMESE NETWORK (HRSiam) achieves state-of-the-art tracking performance while running at over 30 FPS.},
  archive      = {J_TIP},
  author       = {Jia Shao and Bo Du and Chen Wu and Mingming Gong and Tongliang Liu},
  doi          = {10.1109/TIP.2020.3045634},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3056-3068},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {HRSiam: High-resolution siamese network, towards space-borne satellite video tracking},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-view gait image generation for cross-view gait
recognition. <em>TIP</em>, <em>30</em>, 3041–3055. (<a
href="https://doi.org/10.1109/TIP.2021.3055936">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gait recognition aims to recognize persons&#39; identities by walking styles. Gait recognition has unique advantages due to its characteristics of non-contact and long-distance compared with face and fingerprint recognition. Cross-view gait recognition is a challenge task because view variance may produce large impact on gait silhouettes. The development of deep learning has promoted cross-view gait recognition performances to a higher level. However, performances of existing deep learning-based cross-view gait recognition methods are limited by lack of gait samples under different views. In this paper, we take a Multi-view Gait Generative Adversarial Network (MvGGAN) to generate fake gait samples to extend existing gait datasets, which provides adequate gait samples for deep learning-based cross-view gait recognition methods. The proposed MvGGAN method trains a single generator for all view pairs involved in single or multiple datasets. Moreover, we perform domain alignment based on projected maximum mean discrepancy to reduce the influence of distribution divergence caused by sample generation. The experimental results on CASIA-B and OUMVLP dataset demonstrate that fake gait samples generated by the proposed MvGGAN method can improve performances of existing state-of-the-art cross-view gait recognition methods obviously on both single-dataset and cross-dataset evaluation settings.},
  archive      = {J_TIP},
  author       = {Xin Chen and Xizhao Luo and Jian Weng and Weiqi Luo and Huiting Li and Qi Tian},
  doi          = {10.1109/TIP.2021.3055936},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3041-3055},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Multi-view gait image generation for cross-view gait recognition},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Pyramidal multiple instance detection network with mask
guided self-correction for weakly supervised object detection.
<em>TIP</em>, <em>30</em>, 3029–3040. (<a
href="https://doi.org/10.1109/TIP.2021.3056887">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weakly supervised object detection has attracted more and more attention as it only needs image-level annotations for training object detectors. A popular solution to this task is to train a multiple instance detection network (MIDN) which integrates multiple instance learning into a deep convolutional neural network. One major issue of the MIDN is that it is prone to be stuck at local discriminative regions. To address this local optimum issue, we propose a pyramidal MIDN (P-MIDN) comprised of a sequence of multiple MIDNs. In particular, one MIDN performs proposal removal for its subsequent MIDN to reduce the exposure of local discriminative proposal regions to the latter during training. In this manner, it allows our MIDNs to focus on proposals which cover objects more completely. Furthermore, we integrate the P-MIDN into an online instance classifier refinement (OICR) framework. Combined with the P-MIDN, a mask guided self-correction (MGSC) method is proposed to generate high-quality pseudo ground-truths for training the OICR. Experimental results on PASCAL VOC 2007, PASCAL VOC 2010, PASCAL VOC 2012, ILSVRC 2013 DET and MS-COCO benchmarks demonstrate that our approach achieves state-of-the-art performance.},
  archive      = {J_TIP},
  author       = {Yunqiu Xu and Chunluan Zhou and Xin Yu and Bin Xiao and Yi Yang},
  doi          = {10.1109/TIP.2021.3056887},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3029-3040},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Pyramidal multiple instance detection network with mask guided self-correction for weakly supervised object detection},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Learning person re-identification models from videos with
weak supervision. <em>TIP</em>, <em>30</em>, 3017–3028. (<a
href="https://doi.org/10.1109/TIP.2021.3056223">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most person re-identification methods, being supervised techniques, suffer from the burden of massive annotation requirement. Unsupervised methods overcome this need for labeled data, but perform poorly compared to the supervised alternatives. In order to cope with this issue, we introduce the problem of learning person re-identification models from videos with weak supervision. The weak nature of the supervision arises from the requirement of video-level labels, i.e. person identities who appear in the video, in contrast to the more precise frame-level annotations. Towards this goal, we propose a multiple instance attention learning framework for person re-identification using such video-level labels. Specifically, we first cast the video person re-identification task into a multiple instance learning setting, in which person images in a video are collected into a bag. The relations between videos with similar labels can be utilized to identify persons, on top of that, we introduce a co-person attention mechanism which mines the similarity correlations between videos with person identities in common. The attention weights are obtained based on all person images instead of person tracklets in a video, making our learned model less affected by noisy annotations. Extensive experiments demonstrate the superiority of the proposed method over the related methods on two weakly labeled person re-identification datasets.},
  archive      = {J_TIP},
  author       = {Xueping Wang and Min Liu and Dripta S. Raychaudhuri and Sujoy Paul and Yaonan Wang and Amit K. Roy-Chowdhury},
  doi          = {10.1109/TIP.2021.3056223},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3017-3028},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Learning person re-identification models from videos with weak supervision},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Semi-supervised scene text recognition. <em>TIP</em>,
<em>30</em>, 3005–3016. (<a
href="https://doi.org/10.1109/TIP.2021.3051485">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scene text recognition has been widely researched with supervised approaches. Most existing algorithms require a large amount of labeled data and some methods even require character-level or pixel-wise supervision information. However, labeled data is expensive, unlabeled data is relatively easy to collect, especially for many languages with fewer resources. In this paper, we propose a novel semi-supervised method for scene text recognition. Specifically, we design two global metrics, i.e., edit reward and embedding reward, to evaluate the quality of generated string and adopt reinforcement learning techniques to directly optimize these rewards. The edit reward measures the distance between the ground truth label and the generated string. Besides, the image feature and string feature are embedded into a common space and the embedding reward is defined by the similarity between the input image and generated string. It is natural that the generated string should be the nearest with the image it is generated from. Therefore, the embedding reward can be obtained without any ground truth information. In this way, we can effectively exploit a large number of unlabeled images to improve the recognition performance without any additional laborious annotations. Extensive experimental evaluations on the five challenging benchmarks, the Street View Text, IIIT5K, and ICDAR datasets demonstrate the effectiveness of the proposed approach, and our method significantly reduces annotation effort while maintaining competitive recognition performance.},
  archive      = {J_TIP},
  author       = {Yunze Gao and Yingying Chen and Jinqiao Wang and Hanqing Lu},
  doi          = {10.1109/TIP.2021.3051485},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3005-3016},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Semi-supervised scene text recognition},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Semantics-aware spatial-temporal binaries for cross-modal
video retrieval. <em>TIP</em>, <em>30</em>, 2989–3004. (<a
href="https://doi.org/10.1109/TIP.2020.3048680">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the current exponential growth of video-based social networks, video retrieval using natural language is receiving ever-increasing attention. Most existing approaches tackle this task by extracting individual frame-level spatial features to represent the whole video, while ignoring visual pattern consistencies and intrinsic temporal relationships across different frames. Furthermore, the semantic correspondence between natural language queries and person-centric actions in videos has not been fully explored. To address these problems, we propose a novel binary representation learning framework, named Semantics-aware Spatial-temporal Binaries ($\text{S}^{2}$ Bin), which simultaneously considers spatial-temporal context and semantic relationships for cross-modal video retrieval. By exploiting the semantic relationships between two modalities, $\text{S}^{2}$ Bin can efficiently and effectively generate binary codes for both videos and texts. In addition, we adopt an iterative optimization scheme to learn deep encoding functions with attribute-guided stochastic training. We evaluate our model on three video datasets and the experimental results demonstrate that $\text{S}^{2}$ Bin outperforms the state-of-the-art methods in terms of various cross-modal video retrieval tasks.},
  archive      = {J_TIP},
  author       = {Mengshi Qi and Jie Qin and Yi Yang and Yunhong Wang and Jiebo Luo},
  doi          = {10.1109/TIP.2020.3048680},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2989-3004},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Semantics-aware spatial-temporal binaries for cross-modal video retrieval},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Graph-embedded lane detection. <em>TIP</em>, <em>30</em>,
2977–2988. (<a href="https://doi.org/10.1109/TIP.2021.3057287">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lane detection on road segments with complex topologies such as lane merge/split and highway ramps is not yet a solved problem. This paper presents a novel graph-embedded solution. It consists of two key parts, a learning-based low-level lane feature extraction algorithm, and a graph-embedded lane inference algorithm. The former reduces the over-reliance on customized annotated/labeled lane data. We leveraged several open-source semantic segmentation datasets (e.g., Cityscape, Vistas, and Apollo) and designed a dedicated network that can be trained across these heterogeneous datasets to extract lane attributes. The latter algorithm constructs a graph to represent the lane geometry and topology. It does not rely on strong geometric assumptions such as lane lines are a set of parallel polynomials. Instead, it constructs a graph based on detected lane nodes. The lane parameters in the world coordinate are inferred by efficient graph-based searching and calculation. The performance of the proposed method is verified on both open source and our own collected data. On-vehicle experiments were also conducted and the comparison with Mobileye EyeQ2 shows favorable results.},
  archive      = {J_TIP},
  author       = {Pingping Lu and Shaobing Xu and Huei Peng},
  doi          = {10.1109/TIP.2021.3057287},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2977-2988},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Graph-embedded lane detection},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Structural knowledge distillation for efficient
skeleton-based action recognition. <em>TIP</em>, <em>30</em>, 2963–2976.
(<a href="https://doi.org/10.1109/TIP.2021.3056895">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Skeleton data have been extensively used for action recognition since they can robustly accommodate dynamic circumstances and complex backgrounds. To guarantee the action-recognition performance, we prefer to use advanced and time-consuming algorithms to get more accurate and complete skeletons from the scene. However, this may not be acceptable in time- and resource-stringent applications. In this paper, we explore the feasibility of using low-quality skeletons, which can be quickly and easily estimated from the scene, for action recognition. While the use of low-quality skeletons will surely lead to degraded action-recognition accuracy, in this paper we propose a structural knowledge distillation scheme to minimize this accuracy degradations and improve recognition model&#39;s robustness to uncontrollable skeleton corruptions. More specifically, a teacher which observes high-quality skeletons obtained from a scene is used to help train a student which only sees low-quality skeletons generated from the same scene. At inference time, only the student network is deployed for processing low-quality skeletons. In the proposed network, a graph matching loss is proposed to distill the graph structural knowledge at an intermediate representation level. We also propose a new gradient revision strategy to seek a balance between mimicking the teacher model and directly improving the student model&#39;s accuracy. Experiments are conducted on Kenetics400, NTU RGB+D and Penn action recognition datasets and the comparison results demonstrate the effectiveness of our scheme.},
  archive      = {J_TIP},
  author       = {Cunling Bian and Wei Feng and Liang Wan and Song Wang},
  doi          = {10.1109/TIP.2021.3056895},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2963-2976},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Structural knowledge distillation for efficient skeleton-based action recognition},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning to zoom-in via learning to zoom-out: Real-world
super-resolution by generating and adapting degradation. <em>TIP</em>,
<em>30</em>, 2947–2962. (<a
href="https://doi.org/10.1109/TIP.2021.3049951">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most learning-based super-resolution (SR) methods aim to recover high-resolution (HR) image from a given low-resolution (LR) image via learning on LR-HR image pairs. The SR methods learned on synthetic data do not perform well in real-world, due to the domain gap between the artificially synthesized and real LR images. Some efforts are thus taken to capture real-world image pairs. However, the captured LR-HR image pairs usually suffer from unavoidable misalignment, which hampers the performance of end- to-end learning. Here, focusing on the real-world SR, we ask a different question: since misalignment is unavoidable, can we propose a method that does not need LR-HR image pairing and alignment at all and utilizes real images as they are? Hence we propose a framework to learn SR from an arbitrary set of unpaired LR and HR images and see how far a step can go in such a realistic and “unsupervised” setting. To do so, we firstly train a degradation generation network to generate realistic LR images and, more importantly, to capture their distribution (i.e., learning to zoom out). Instead of assuming the domain gap has been eliminated, we minimize the discrepancy between the generated data and real data while learning a degradation adaptive SR network (i.e., learning to zoom in). The proposed unpaired method achieves state-of- the-art SR results on real-world images, even in the datasets that favour the paired-learning methods more.},
  archive      = {J_TIP},
  author       = {Wei Sun and Dong Gong and Qinfeng Shi and Anton van den Hengel and Yanning Zhang},
  doi          = {10.1109/TIP.2021.3049951},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2947-2962},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Learning to zoom-in via learning to zoom-out: Real-world super-resolution by generating and adapting degradation},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Unsupervised cross domain person re-identification by
multi-loss optimization learning. <em>TIP</em>, <em>30</em>, 2935–2946.
(<a href="https://doi.org/10.1109/TIP.2021.3056889">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised cross domain (UCD) person re-identification (re-ID) aims to apply a model trained on a labeled source domain to an unlabeled target domain. It faces huge challenges as the identities have no overlap between these two domains. At present, most UCD person re-ID methods perform “supervised learning” by assigning pseudo labels to the target domain, which leads to poor re-ID performance due to the pseudo label noise. To address this problem, a multi-loss optimization learning (MLOL) model is proposed for UCD person re-ID. In addition to using the information of clustering pseudo labels from the perspective of supervised learning, two losses are designed from the view of similarity exploration and adversarial learning to optimize the model. Specifically, in order to alleviate the erroneous guidance brought by the clustering error to the model, a ranking-average-based triplet loss learning and a neighbor-consistency-based loss learning are developed. Combining these losses to optimize the model results in a deep exploration of the intra-domain relation within the target domain. The proposed model is evaluated on three popular person re-ID datasets, Market-1501, DukeMTMC-reID, and MSMT17. Experimental results show that our model outperforms the state-of-the-art UCD re-ID methods with a clear advantage.},
  archive      = {J_TIP},
  author       = {Jia Sun and Yanfeng Li and Houjin Chen and Yahui Peng and Jinlei Zhu},
  doi          = {10.1109/TIP.2021.3056889},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2935-2946},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Unsupervised cross domain person re-identification by multi-loss optimization learning},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-stage feature fusion network for video
super-resolution. <em>TIP</em>, <em>30</em>, 2923–2934. (<a
href="https://doi.org/10.1109/TIP.2021.3056868">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video super-resolution (VSR) is to restore a photo-realistic high-resolution (HR) frame from both its corresponding low-resolution (LR) frame (reference frame) and multiple neighboring frames (supporting frames). An important step in VSR is to fuse the feature of the reference frame with the features of the supporting frames. The major issue with existing VSR methods is that the fusion is conducted in a one-stage manner, and the fused feature may deviate greatly from the visual information in the original LR reference frame. In this paper, we propose an end-to-end Multi-Stage Feature Fusion Network that fuses the temporally aligned features of the supporting frames and the spatial feature of the original reference frame at different stages of a feed-forward neural network architecture. In our network, the Temporal Alignment Branch is designed as an inter-frame temporal alignment module used to mitigate the misalignment between the supporting frames and the reference frame. Specifically, we apply the multi-scale dilated deformable convolution as the basic operation to generate temporally aligned features of the supporting frames. Afterwards, the Modulative Feature Fusion Branch, the other branch of our network accepts the temporally aligned feature map as a conditional input and modulates the feature of the reference frame at different stages of the branch backbone. This enables the feature of the reference frame to be referenced at each stage of the feature fusion process, leading to an enhanced feature from LR to HR. Experimental results on several benchmark datasets demonstrate that our proposed method can achieve state-of-the-art performance on VSR task.},
  archive      = {J_TIP},
  author       = {Huihui Song and Wenjie Xu and Dong Liu and Bo Liu and Qingshan Liu and Dimitris N. Metaxas},
  doi          = {10.1109/TIP.2021.3056868},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2923-2934},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Multi-stage feature fusion network for video super-resolution},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). HOReID: Deep high-order mapping enhances pose alignment for
person re-identification. <em>TIP</em>, <em>30</em>, 2908–2922. (<a
href="https://doi.org/10.1109/TIP.2021.3055952">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the remarkable progress in recent years, person Re-Identification (ReID) approaches frequently fail in cases where the semantic body parts are misaligned between the detected human boxes. To mitigate such cases, we propose a novel High-Order ReID (HOReID) framework that enables semantic pose alignment by aggregating the fine-grained part details of multilevel feature maps. The HOReID adopts a high-order mapping of multilevel feature similarities in order to emphasize the differences of the similarities between aligned and misaligned part pairs in two person images. Since the similarities of misaligned part pairs are reduced, the HOReID enhances pose-robustness within the learned features. We show that our method derives from an intuitive and interpretable motivation and elegantly reduces the misalignment problem without using any prior knowledge from human pose annotations or pose estimation networks. This paper theoretically and experimentally demonstrates the effectiveness of the proposed HOReID, achieving superior performance over the state-of-the-art methods on the four large-scale person ReID datasets.},
  archive      = {J_TIP},
  author       = {Pingyu Wang and Zhicheng Zhao and Fei Su and Xingyu Zu and Nikolaos V. Boulgouris},
  doi          = {10.1109/TIP.2021.3055952},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2908-2922},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {HOReID: Deep high-order mapping enhances pose alignment for person re-identification},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Complementary pseudo labels for unsupervised domain
adaptation on person re-identification. <em>TIP</em>, <em>30</em>,
2898–2907. (<a href="https://doi.org/10.1109/TIP.2021.3056212">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, supervised person re-identification (re-ID) models have received increasing studies. However, these models trained on the source domain always suffer dramatic performance drop when tested on an unseen domain. Existing methods are primary to use pseudo labels to alleviate this problem. One of the most successful approaches predicts neighbors of each unlabeled image and then uses them to train the model. Although the predicted neighbors are credible, they always miss some hard positive samples, which may hinder the model from discovering important discriminative information of the unlabeled domain. In this paper, to complement these low recall neighbor pseudo labels, we propose a joint learning framework to learn better feature embeddings via high precision neighbor pseudo labels and high recall group pseudo labels. The group pseudo labels are generated by transitively merging neighbors of different samples into a group to achieve higher recall. However, the merging operation may cause subgroups in the group due to imperfect neighbor predictions. To utilize these group pseudo labels properly, we propose using a similarity-aggregating loss to mitigate the influence of these subgroups by pulling the input sample towards the most similar embeddings. Extensive experiments on three large-scale datasets demonstrate that our method can achieve state-of-the-art performance under the unsupervised domain adaptation re-ID setting.},
  archive      = {J_TIP},
  author       = {Hao Feng and Minghao Chen and Jinming Hu and Dong Shen and Haifeng Liu and Deng Cai},
  doi          = {10.1109/TIP.2021.3056212},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2898-2907},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Complementary pseudo labels for unsupervised domain adaptation on person re-identification},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An accurate and lightweight method for human body image
super-resolution. <em>TIP</em>, <em>30</em>, 2888–2897. (<a
href="https://doi.org/10.1109/TIP.2021.3055737">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a new method to super-resolve low resolution human body images by learning efficient multi-scale features and exploiting useful human body prior. Specifically, we propose a lightweight multi-scale block (LMSB) as basic module of a coherent framework, which contains an image reconstruction branch and a prior estimation branch. In the image reconstruction branch, the LMSB aggregates features of multiple receptive fields so as to gather rich context information for low-to-high resolution mapping. In the prior estimation branch, we adopt the human parsing maps and nonsubsampled shearlet transform (NSST) sub-bands to represent the human body prior, which is expected to enhance the details of reconstructed human body images. When evaluated on the newly collected HumanSR dataset, our method outperforms state-of-the-art image super-resolution methods with ~8× fewer parameters; moreover, our method significantly improves the performance of human image analysis tasks (e.g. human parsing and pose estimation) for low-resolution inputs.},
  archive      = {J_TIP},
  author       = {Yunan Liu and Shanshan Zhang and Jie Xu and Jian Yang and Yu-Wing Tai},
  doi          = {10.1109/TIP.2021.3055737},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2888-2897},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {An accurate and lightweight method for human body image super-resolution},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A self-training approach for point-supervised object
detection and counting in crowds. <em>TIP</em>, <em>30</em>, 2876–2887.
(<a href="https://doi.org/10.1109/TIP.2021.3055632">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose a novel self-training approach named Crowd-SDNet that enables a typical object detector trained only with point-level annotations (i.e., objects are labeled with points) to estimate both the center points and sizes of crowded objects. Specifically, during training, we utilize the available point annotations to supervise the estimation of the center points of objects directly. Based on a locally-uniform distribution assumption, we initialize pseudo object sizes from the point-level supervisory information, which are then leveraged to guide the regression of object sizes via a crowdedness-aware loss. Meanwhile, we propose a confidence and order-aware refinement scheme to continuously refine the initial pseudo object sizes such that the ability of the detector is increasingly boosted to detect and count objects in crowds simultaneously. Moreover, to address extremely crowded scenes, we propose an effective decoding method to improve the detector&#39;s representation ability. Experimental results on the WiderFace benchmark show that our approach significantly outperforms state-of-the-art point-supervised methods under both detection and counting tasks, i.e., our method improves the average precision by more than 10\% and reduces the counting error by 31.2\%. Besides, our method obtains the best results on the crowd counting and localization datasets (i.e., ShanghaiTech and NWPU-Crowd) and vehicle counting datasets (i.e., CARPK and PUCPR+) compared with state-of-the-art counting-by-detection methods. The code will be publicly available at https://github.com/WangyiNTU/Point-supervised-crowd-detection.},
  archive      = {J_TIP},
  author       = {Yi Wang and Junhui Hou and Xinyu Hou and Lap-Pui Chau},
  doi          = {10.1109/TIP.2021.3055632},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2876-2887},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A self-training approach for point-supervised object detection and counting in crowds},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Decoupled two-stage crowd counting and beyond. <em>TIP</em>,
<em>30</em>, 2862–2875. (<a
href="https://doi.org/10.1109/TIP.2021.3055631">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of appealing approaches to counting dense objects, such as crowd, is density map estimation. Density maps, however, present ambiguous appearance cues in congested scenes, rendering infeasibility in identifying individuals and difficulties in diagnosing errors. Inspired by an observation that counting can be interpreted as a two-stage process, i.e., identifying possible object regions and counting exact object numbers, we introduce a probabilistic intermediate representation termed the probability map that depicts the probability of each pixel being an object. This representation allows us to decouple counting into probability map regression (PMR) and count map regression (CMR). We therefore propose a novel decoupled two-stage counting (D2C) framework that sequentially regresses the probability map and learns a counter conditioned on the probability map. Given the probability map and the count map, a peak point detection algorithm is derived to localize each object with a point under the guidance of local counts. An advantage of D2C is that the counter can be learned reliably with additional synthesized probability maps. This addresses important data deficiency and sample imbalanced problems in counting. Our framework also enables easy diagnoses and analyses of error patterns. For instance, we find that, the counter per se is sufficiently accurate, while the bottleneck appears to be PMR. We further instantiate a network D2CNet in our framework and report state-of-the-art counting and localization performance across 6 crowd counting benchmarks. Since the probability map is a representation independent of visual appearance, D2CNet also exhibits remarkable cross-dataset transferability. Code and pretrained models are made available at: https://git.io/d2cnet.},
  archive      = {J_TIP},
  author       = {Jian Cheng and Haipeng Xiong and Zhiguo Cao and Hao Lu},
  doi          = {10.1109/TIP.2021.3055631},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2862-2875},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Decoupled two-stage crowd counting and beyond},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning steering kernels for guided depth completion.
<em>TIP</em>, <em>30</em>, 2850–2861. (<a
href="https://doi.org/10.1109/TIP.2021.3055629">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the guided depth completion task in which the goal is to predict a dense depth map given a guidance RGB image and sparse depth measurements. Recent advances on this problem nurture hopes that one day we can acquire accurate and dense depth at a very low cost. A major challenge of guided depth completion is to effectively make use of extremely sparse measurements, e.g., measurements covering less than 1\% of the image pixels. In this paper, we propose a fully differentiable model that avoids convolving on sparse tensors by jointly learning depth interpolation and refinement. More specifically, we propose a differentiable kernel regression layer that interpolates the sparse depth measurements via learned kernels. We further refine the interpolated depth map using a residual depth refinement layer which leads to improved performance compared to learning absolute depth prediction using a vanilla network. We provide experimental evidence that our differentiable kernel regression layer not only enables end-to-end training from very sparse measurements using standard convolutional network architectures, but also leads to better depth interpolation results compared to existing heuristically motivated methods. We demonstrate that our method outperforms many state-of-the-art guided depth completion techniques on both NYUv2 and KITTI. We further show the generalization ability of our method with respect to the density and spatial statistics of the sparse depth measurements.},
  archive      = {J_TIP},
  author       = {Lina Liu and Yiyi Liao and Yue Wang and Andreas Geiger and Yong Liu},
  doi          = {10.1109/TIP.2021.3055629},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2850-2861},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Learning steering kernels for guided depth completion},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Giant panda identification. <em>TIP</em>, <em>30</em>,
2837–2849. (<a href="https://doi.org/10.1109/TIP.2021.3055627">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The lack of automatic tools to identify giant panda makes it hard to keep track of and manage giant pandas in wildlife conservation missions. In this paper, we introduce a new Giant Panda Identification (GPID) task, which aims to identify each individual panda based on an image. Though related to the human re-identification and animal classification problem, GPID is extraordinarily challenging due to subtle visual differences between pandas and cluttered global information. In this paper, we propose a new benchmark dataset iPanda-50 for GPID. The iPanda-50 consists of 6, 874 images from 50 giant panda individuals, and is collected from panda streaming videos. We also introduce a new Feature-Fusion Network with Patch Detector (FFN-PD) for GPID. The proposed FFN-PD exploits the patch detector to detect discriminative local patches without using any part annotations or extra location sub-networks, and builds a hierarchical representation by fusing both global and local features to enhance the inter-layer patch feature interactions. Specifically, an attentional cross-channel pooling is embedded in the proposed FFN-PD to improve the identify-specific patch detectors. Experiments performed on the iPanda-50 datasets demonstrate the proposed FFN-PD significantly outperforms competing methods. Besides, experiments on other fine-grained recognition datasets (i.e., CUB-200-2011, Stanford Cars, and FGVC-Aircraft) demonstrate that the proposed FFN-PD outperforms existing state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Le Wang and Rizhi Ding and Yuanhao Zhai and Qilin Zhang and Wei Tang and Nanning Zheng and Gang Hua},
  doi          = {10.1109/TIP.2021.3055627},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2837-2849},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Giant panda identification},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). AP-CNN: Weakly supervised attention pyramid convolutional
neural network for fine-grained visual classification. <em>TIP</em>,
<em>30</em>, 2826–2836. (<a
href="https://doi.org/10.1109/TIP.2021.3055617">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classifying the sub-categories of an object from the same super-category (e.g., bird species and cars) in fine-grained visual classification (FGVC) highly relies on discriminative feature representation and accurate region localization. Existing approaches mainly focus on distilling information from high-level features. In this article, by contrast, we show that by integrating low-level information (e.g., color, edge junctions, texture patterns), performance can be improved with enhanced feature representation and accurately located discriminative regions. Our solution, named Attention Pyramid Convolutional Neural Network (AP-CNN), consists of 1) a dual pathway hierarchy structure with a top-down feature pathway and a bottom-up attention pathway, hence learning both high-level semantic and low-level detailed feature representation, and 2) an ROI-guided refinement strategy with ROI-guided dropblock and ROI-guided zoom-in operation, which refines features with discriminative local regions enhanced and background noises eliminated. The proposed AP-CNN can be trained end-to-end, without the need of any additional bounding box/part annotation. Extensive experiments on three popularly tested FGVC datasets (CUB-200-2011, Stanford Cars, and FGVC-Aircraft) demonstrate that our approach achieves state-of-the-art performance. Models and code are available at https://github.com/PRIS-CV/AP-CNN_Pytorch-master.},
  archive      = {J_TIP},
  author       = {Yifeng Ding and Zhanyu Ma and Shaoguo Wen and Jiyang Xie and Dongliang Chang and Zhongwei Si and Ming Wu and Haibin Ling},
  doi          = {10.1109/TIP.2021.3055617},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2826-2836},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {AP-CNN: Weakly supervised attention pyramid convolutional neural network for fine-grained visual classification},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A supervised segmentation network for hyperspectral image
classification. <em>TIP</em>, <em>30</em>, 2810–2825. (<a
href="https://doi.org/10.1109/TIP.2021.3055613">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, deep learning has drawn broad attention in the hyperspectral image (HSI) classification task. Many works have focused on elaborately designing various spectral-spatial networks, where convolutional neural network (CNN) is one of the most popular structures. To explore the spatial information for HSI classification, pixels with its adjacent pixels are usually directly cropped from hyperspectral data to form HSI cubes in CNN-based methods. However, the spatial land-cover distributions of cropped HSI cubes are usually complicated. The land-cover label of a cropped HSI cube cannot simply be determined by its center pixel. In addition, the spatial land-cover distribution of a cropped HSI cube is fixed and has less diversity. For CNN-based methods, training with cropped HSI cubes will result in poor generalization to the changes of spatial land-cover distributions. In this paper, an end-to-end fully convolutional segmentation network (FCSN) is proposed to simultaneously identify land-cover labels of all pixels in a HSI cube. First, several experiments are conducted to demonstrate that recent CNN-based methods show the weak generalization capabilities. Second, a fine label style is proposed to label all pixels of HSI cubes to provide detailed spatial land-cover distributions of HSI cubes. Third, a HSI cube generation method is proposed to generate plentiful HSI cubes with fine labels to improve the diversity of spatial land-cover distributions. Finally, a FCSN is proposed to explore spectral-spatial features from finely labeled HSI cubes for HSI classification. Experimental results show that FCSN has the superior generalization capability to the changes of spatial land-cover distributions.},
  archive      = {J_TIP},
  author       = {Hao Sun and Xiangtao Zheng and Xiaoqiang Lu},
  doi          = {10.1109/TIP.2021.3055613},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2810-2825},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A supervised segmentation network for hyperspectral image classification},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021c). Multi-sentence auxiliary adversarial networks for
fine-grained text-to-image synthesis. <em>TIP</em>, <em>30</em>,
2798–2809. (<a href="https://doi.org/10.1109/TIP.2021.3055062">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the development of Generative Adversarial Networks (GANs), significant progress has been achieved in text-to-image synthesis task. However, most previous works have only focus on learning the semantic consistency between paired images and sentences, without exploring the semantic correlation between different yet related sentences that describe the same image, which leads to significant visual variation among the synthesized images. Accordingly, in this article, we propose a new method for text-to-image synthesis, dubbed Multi-sentence Auxiliary Generative Adversarial Networks (MA-GAN); this approach not only improves the generation quality but also guarantees the generation similarity of related sentences by exploring the semantic correlation between different sentences describing the same image. More specifically, we propose a Single-sentence Generation and Multi-sentence Discrimination (SGMD) module that explores the semantic correlation between multiple related sentences in order to reduce the variation between their generated images and enhance the reliability of the generated results. Moreover, a Progressive Negative Sample Selection mechanism (PNSS) is designed to mine more suitable negative samples for training, which can effectively promote detailed discrimination ability in the generative model and facilitate the generation of more fine-grained results. Extensive experiments on Oxford-102 and CUB datasets reveal that our MA-GAN significantly outperforms the state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Yanhua Yang and Lei Wang and De Xie and Cheng Deng and Dacheng Tao},
  doi          = {10.1109/TIP.2021.3055062},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2798-2809},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Multi-sentence auxiliary adversarial networks for fine-grained text-to-image synthesis},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Image co-skeletonization via co-segmentation. <em>TIP</em>,
<em>30</em>, 2784–2797. (<a
href="https://doi.org/10.1109/TIP.2021.3054464">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in the joint processing of a set of images have shown its advantages over individual processing. Unlike the existing works geared towards co-segmentation or co-localization, in this article, we explore a new joint processing topic: image co-skeletonization, which is defined as joint skeleton extraction of the foreground objects in an image collection. It is well known that object skeletonization in a single natural image is challenging, because there is hardly any prior knowledge available about the object present in the image. Therefore, we resort to the idea of image co-skeletonization, hoping that the commonness prior that exists across the semantically similar images can be leveraged to have such knowledge, similar to other joint processing problems such as co-segmentation. Moreover, earlier research has found that augmenting a skeletonization process with the object’s shape information is highly beneficial in capturing the image context. Having made these two observations, we propose a coupled framework for co-skeletonization and co-segmentation tasks to facilitate shape information discovery for our co-skeletonization process through the co-segmentation process. While image co-skeletonization is our primary goal, the co-segmentation process might also benefit, in turn, from exploiting skeleton outputs of the co-skeletonization process as central object seeds through such a coupled framework. As a result, both can benefit from each other synergistically. For evaluating image co-skeletonization results, we also construct a novel benchmark dataset by annotating nearly 1.8 K images and dividing them into 38 semantic categories. Although the proposed idea is essentially a weakly supervised method, it can also be employed in supervised and unsupervised scenarios. Extensive experiments demonstrate that the proposed method achieves promising results in all three scenarios.},
  archive      = {J_TIP},
  author       = {Koteswar Rao Jerripothula and Jianfei Cai and Jiangbo Lu and Junsong Yuan},
  doi          = {10.1109/TIP.2021.3054464},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2784-2797},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Image co-skeletonization via co-segmentation},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Geometrically editable face image translation with
adversarial networks. <em>TIP</em>, <em>30</em>, 2771–2783. (<a
href="https://doi.org/10.1109/TIP.2021.3052084">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, image-to-image translation has received increasing attention, which aims to map images in one domain to another specific one. Existing methods mainly solve this task via a deep generative model that they focus on exploring the bi-directional or multi-directional relationship between specific domains. Those domains are often categorized by attribute-level or class-level labels, which do not incorporate any geometric information in learning process. As a result, existing methods are incapable of editing geometric contents during translation. They also neglect to utilize higher-level and instance-specific information to further guide the training process, leading to a great deal of unrealistic synthesized images of low fidelity, especially for face images. To address these challenges, we formulate the general image translation problem as multi-domain mappings in both geometric and attribute directions within an image set that shares a same latent vector. Particularly, we propose a novel Geometrically Editable Generative Adversarial Networks (GEGAN) model to solve this problem for face images by leveraging facial semantic segmentation to explicitly guide its geometric editing. In details, input face images are encoded to their latent representations via a variational autoencoder, a segmentor network is designed to impose semantic information on the generated images, and multi-scale regional discriminators are employed to force the generator to pay attention to the details of key components. We provide both quantitative and qualitative evaluations on CelebA dataset to demonstrate our ability of the geometric modification and our improvement in image fidelity.},
  archive      = {J_TIP},
  author       = {Songyao Jiang and Zhiqiang Tao and Yun Fu},
  doi          = {10.1109/TIP.2021.3052084},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2771-2783},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Geometrically editable face image translation with adversarial networks},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Graph-based multi-interaction network for video question
answering. <em>TIP</em>, <em>30</em>, 2758–2770. (<a
href="https://doi.org/10.1109/TIP.2021.3051756">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video question answering is an important task combining both Natural Language Processing and Computer Vision, which requires a machine to obtain a thorough understanding of the video. Most existing approaches simply capture spatio-temporal information in videos by using a combination of recurrent and convolutional neural networks. Nonetheless, most previous work focus on only salient frames or regions, which normally lacks some significant details, such as potential location and action relations. In this paper, we propose a new method called Graph-based Multi-interaction Network for video question answering. In our model, a new attention mechanism named multi-interaction is designed to capture both element-wise and segment-wise sequence interactions simultaneously, which can be found between and inside the multi-modal inputs. Moreover, we propose a graph-based relation-aware neural network to explore a more fine-grained visual representation, which could explore the relationships and dependencies between objects spatially and temporally. We evaluate our method on TGIF-QA and other two video QA datasets. The qualitative and quantitative experimental results show the effectiveness of our model, which achieves state-of-the-art performance.},
  archive      = {J_TIP},
  author       = {Mao Gu and Zhou Zhao and Weike Jin and Richang Hong and Fei Wu},
  doi          = {10.1109/TIP.2021.3051756},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2758-2770},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Graph-based multi-interaction network for video question answering},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Crowd scene analysis encounters high density and scale
variation. <em>TIP</em>, <em>30</em>, 2745–2757. (<a
href="https://doi.org/10.1109/TIP.2021.3049963">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crowd scene analysis receives growing attention due to its wide applications. Grasping the accurate crowd location is important for identifying high-risk regions. In this article, we propose a Compressed Sensing based Output Encoding (CSOE) scheme, which casts detecting pixel coordinates of small objects into a task of signal regression in encoding signal space. To prevent gradient vanishing, we derive our own sparse reconstruction backpropagation rule that is adaptive to distinct implementations of sparse reconstruction and makes the whole model end-to-end trainable. With the support of CSOE and the backpropagation rule, the proposed method shows more robustness to deep model training error, which is especially harmful to crowd counting and localization. The proposed method achieves state-of-the-art performance across four mainstream datasets, especially achieves excellent results in highly crowded scenes. A series of analysis and experiments support our claim that regression in CSOE space is better than traditionally detecting coordinates of small objects in pixel space for highly crowded scenes.},
  archive      = {J_TIP},
  author       = {Yao Xue and Yonghui Li and Siming Liu and Xingjun Zhang and Xueming Qian},
  doi          = {10.1109/TIP.2021.3049963},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2745-2757},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Crowd scene analysis encounters high density and scale variation},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Condition-aware comparison scheme for gait recognition.
<em>TIP</em>, <em>30</em>, 2734–2744. (<a
href="https://doi.org/10.1109/TIP.2020.3039888">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an important and challenging problem, gait recognition has gained considerable attention. It suffers from confounding conditions, that is, it is sensitive to camera views, dressing types and so on. Interestingly, it is observed that, under different conditions, local body parts contribute differently to recognition performance. In this paper, we propose a condition-aware comparison scheme to measure gait pairs’ similarity via a novel module named Instructor. Also, we present a geometry-guided data augmentation approach (Dresser) to enrich dressing conditions. Furthermore, to enhance the gait representation, we propose to model temporal local information from coarse to fine. Our model is evaluated on two popular benchmarks, CASIA-B and OULP. Results show that our method outperforms current state-of-the-art methods, especially in the cross-condition scenario.},
  archive      = {J_TIP},
  author       = {Haoqian Wu and Jian Tian and Yongjian Fu and Bin Li and Xi Li},
  doi          = {10.1109/TIP.2020.3039888},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2734-2744},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Condition-aware comparison scheme for gait recognition},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). ASK: Adaptively selecting key local features for RGB-d scene
recognition. <em>TIP</em>, <em>30</em>, 2722–2733. (<a
href="https://doi.org/10.1109/TIP.2021.3053459">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Indoor scene images usually contain scattered objects and various scene layouts, which make RGB-D scene classification a challenging task. Existing methods still have limitations for classifying scene images with great spatial variability. Thus, how to extract local patch-level features effectively using only image label is still an open problem for RGB-D scene recognition. In this article, we propose an efficient framework for RGB-D scene recognition, which adaptively selects important local features to capture the great spatial variability of scene images. Specifically, we design a differentiable local feature selection (DLFS) module, which can extract the appropriate number of key local scene-related features. Discriminative local theme-level and object-level representations can be selected with DLFS module from the spatially-correlated multi-modal RGB-D features. We take advantage of the correlation between RGB and depth modalities to provide more cues for selecting local features. To ensure that discriminative local features are selected, the variational mutual information maximization loss is proposed. Additionally, the DLFS module can be easily extended to select local features of different scales. By concatenating the local-orderless and global-structured multi-modal features, the proposed framework can achieve state-of-the-art performance on public RGB-D scene recognition datasets.},
  archive      = {J_TIP},
  author       = {Zhitong Xiong and Yuan Yuan and Qi Wang},
  doi          = {10.1109/TIP.2021.3053459},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2722-2733},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {ASK: Adaptively selecting key local features for RGB-D scene recognition},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Improving single shot object detection with feature scale
unmixing. <em>TIP</em>, <em>30</em>, 2708–2721. (<a
href="https://doi.org/10.1109/TIP.2020.3048630">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the advantages of real-time detection and improved performance, single-shot detectors have gained great attention recently. To solve the complex scale variations, single-shot detectors make scale-aware predictions based on multiple pyramid layers. Typically, small objects are detected on shallow layers while large objects are detected on deep layers. However, the features in the pyramid are not scale-aware enough, which limits the detection performance. Two common problems in single-shot detectors caused by object scale variations can be observed: (1) false negative problem, i.e., small objects are easily missed due to the weak features; (2) part-false positive problem, i.e., the salient part of a large object is sometimes detected as an object. With this observation, a new Neighbor Erasing and Transferring (NET) mechanism is proposed for feature scale-unmixing to explore scale-aware features in this paper. In NET, a Neighbor Erasing Module (NEM) is designed to erase the salient features of large objects and emphasize the features of small objects in shallow layers. A Neighbor Transferring Module (NTM) is introduced to transfer the erased features and highlight large objects in deep layers. With this mechanism, a single-shot network called NETNet is constructed for scale-aware object detection. In addition, we propose to aggregate nearest neighboring pyramid features to enhance our NET. Experiments on MS COCO dataset and UAVDT dataset demonstrate the effectiveness of our method. NETNet obtains 38.5\% AP at a speed of 27 FPS and 32.0\% AP at a speed of 55 FPS on MS COCO dataset. As a result, NETNet achieves a better trade-off for real-time and accurate object detection.},
  archive      = {J_TIP},
  author       = {Yazhao Li and Yanwei Pang and Jiale Cao and Jianbing Shen and Ling Shao},
  doi          = {10.1109/TIP.2020.3048630},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2708-2721},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Improving single shot object detection with feature scale unmixing},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Ellipse recovery from blurred binary images. <em>TIP</em>,
<em>30</em>, 2697–2707. (<a
href="https://doi.org/10.1109/TIP.2020.3026866">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we address the problem of ellipse recovery from blurred shape images. A shape image is a binary-valued (0/1) image in continuous-domain that represents one or multiple shapes. In general, the shapes can also be overlapping. We assume to observe the shape image through finitely many blurred samples, where the 2D blurring kernel is assumed to be known. The samples might also be noisy. Our goal is to detect and locate ellipses within the shape image. Our approach is based on representing an ellipse as the zero-level-set of a bivariate polynomial of degree 2. Indeed, similar to the theory of finite rate of innovation (FRI), we establish a set of linear equations (annihilation filter) between the image moments and the coefficients of the bivariate polynomial. For a single ellipse, we show that the image can be perfectly recovered from only 6 image moments (improving the bound in [Fatemi et al., 2016]). For multiple ellipses, instead of searching for a polynomial of higher degree, we locally search for single ellipses and apply a pooling technique to detect the ellipse. As we always search for a polynomial of degree 2, this approach is more robust against additive noise compared to the strategy of searching for a polynomial of higher degree (detecting multiple ellipses at the same time). Besides, this approach has the advantage of detecting ellipses even when they intersect and some parts of the boundaries are lost. Simulation results using both synthetic and real world images (red blood cells) confirm superiority of the performance of the proposed method against the existing techniques.},
  archive      = {J_TIP},
  author       = {Hojatollah Zamani and Arash Amini},
  doi          = {10.1109/TIP.2020.3026866},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2697-2707},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Ellipse recovery from blurred binary images},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Denoising-based turbo message passing for compressed video
background subtraction. <em>TIP</em>, <em>30</em>, 2682–2696. (<a
href="https://doi.org/10.1109/TIP.2021.3055063">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we consider the compressed video background subtraction problem that separates the background and foreground of a video from its compressed measurements. The background of a video usually lies in a low dimensional space and the foreground is usually sparse. More importantly, each video frame is a natural image that has textural patterns. By exploiting these properties, we develop a message passing algorithm termed offline denoising-based turbo message passing (DTMP). We show that these structural properties can be efficiently handled by the existing denoising techniques under the turbo message passing framework. We further extend the DTMP algorithm to the online scenario where the video data is collected in an online manner. The extension is based on the similarity/continuity between adjacent video frames. We adopt the optical flow method to refine the estimation of the foreground. We also adopt the sliding window based background estimation to reduce complexity. By exploiting the Gaussianity of messages, we develop the state evolution to characterize the per-iteration performance of offline and online DTMP. Comparing to the existing algorithms, DTMP can work at much lower compression rates, and can subtract the background successfully with a lower mean squared error and better visual quality for both offline and online compressed video background subtraction.},
  archive      = {J_TIP},
  author       = {Zhipeng Xue and Xiaojun Yuan and Yang Yang},
  doi          = {10.1109/TIP.2021.3055063},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2682-2696},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Denoising-based turbo message passing for compressed video background subtraction},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep likelihood network for image restoration with multiple
degradation levels. <em>TIP</em>, <em>30</em>, 2669–2681. (<a
href="https://doi.org/10.1109/TIP.2021.3051767">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks have been proven effective in a variety of image restoration tasks. Most state-of-the-art solutions, however, are trained using images with a single particular degradation level, and their performance deteriorates drastically when applied to other degradation settings. In this paper, we propose deep likelihood network (DL-Net), aiming at generalizing off-the-shelf image restoration networks to succeed over a spectrum of degradation levels. We slightly modify an off-the-shelf network by appending a simple recursive module, which is derived from a fidelity term, for disentangling the computation for multiple degradation levels. Extensive experimental results on image inpainting, interpolation, and super-resolution show the effectiveness of our DL-Net.},
  archive      = {J_TIP},
  author       = {Yiwen Guo and Ming Lu and Wangmeng Zuo and Changshui Zhang and Yurong Chen},
  doi          = {10.1109/TIP.2021.3051767},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2669-2681},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Deep likelihood network for image restoration with multiple degradation levels},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Nocal-siam: Refining visual features and response with
advanced non-local blocks for real-time siamese tracking. <em>TIP</em>,
<em>30</em>, 2656–2668. (<a
href="https://doi.org/10.1109/TIP.2021.3049970">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Siamese trackers contain two core stages, i.e., learning the features of both target and search inputs at first and then calculating response maps via the cross-correlation operation, which can also be used for regression and classification to construct typical one-shot detection tracking framework. Although they have drawn continuous interest from the visual tracking community due to the proper trade-off between accuracy and speed, both stages are easily sensitive to the distracters in search branch, thereby inducing unreliable response positions. To fill this gap, we advance Siamese trackers with two novel non-local blocks named Nocal-Siam, which leverages the long-range dependency property of the non-local attention in a supervised fashion from two aspects. First, a target-aware non-local block (T-Nocal) is proposed for learning the target-guided feature weights, which serve to refine visual features of both target and search branches, and thus effectively suppress noisy distracters. This block reinforces the interplay between both target and search branches in the first stage. Second, we further develop a location-aware non-local block (L-Nocal) to associate multiple response maps, which prevents them inducing diverse candidate target positions in the future coming frame. Experiments on five popular benchmarks show that Nocal-Siam performs favorably against well-behaved counterparts both in quantity and quality.},
  archive      = {J_TIP},
  author       = {Huibin Tan and Xiang Zhang and Zhipeng Zhang and Long Lan and Wenju Zhang and Zhigang Luo},
  doi          = {10.1109/TIP.2021.3049970},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2656-2668},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Nocal-siam: Refining visual features and response with advanced non-local blocks for real-time siamese tracking},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Sketch-guided scenery image outpainting. <em>TIP</em>,
<em>30</em>, 2643–2655. (<a
href="https://doi.org/10.1109/TIP.2021.3054477">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The outpainting results produced by existing approaches are often too random to meet users&#39; requirements. In this work, we take the image outpainting one step forward by allowing users to harvest personal custom outpainting results using sketches as the guidance. To this end, we propose an encoder-decoder based network to conduct sketch-guided outpainting, where two alignment modules are adopted to impose the generated content to be realistic and consistent with the provided sketches. First, we apply a holistic alignment module to make the synthesized part be similar to the real one from the global view. Second, we reversely produce the sketches from the synthesized part and encourage them be consistent with the ground-truth ones using a sketch alignment module. In this way, the learned generator will be imposed to pay more attention to fine details and be sensitive to the guiding sketches. To our knowledge, this work is the first attempt to explore the challenging yet meaningful conditional scenery image outpainting. We conduct extensive experiments on two collected benchmarks to qualitatively and quantitatively validate the effectiveness of our approach compared with the other state-of-the-art generative models.},
  archive      = {J_TIP},
  author       = {Yaxiong Wang and Yunchao Wei and Xueming Qian and Li Zhu and Yi Yang},
  doi          = {10.1109/TIP.2021.3054477},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2643-2655},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Sketch-guided scenery image outpainting},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). CapsField: Light field-based face and expression recognition
in the wild using capsule routing. <em>TIP</em>, <em>30</em>, 2627–2642.
(<a href="https://doi.org/10.1109/TIP.2021.3054476">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Light field (LF) cameras provide rich spatio-angular visual representations by sensing the visual scene from multiple perspectives and have recently emerged as a promising technology to boost the performance of human-machine systems such as biometrics and affective computing. Despite the significant success of LF representation for constrained facial image analysis, this technology has never been used for face and expression recognition in the wild. In this context, this paper proposes a new deep face and expression recognition solution, called CapsField, based on a convolutional neural network and an additional capsule network that utilizes dynamic routing to learn hierarchical relations between capsules. CapsField extracts the spatial features from facial images and learns the angular part-whole relations for a selected set of 2D sub-aperture images rendered from each LF image. To analyze the performance of the proposed solution in the wild, the first in the wild LF face dataset, along with a new complementary constrained face dataset captured from the same subjects recorded earlier have been captured and are made available. A subset of the in the wild dataset contains facial images with different expressions, annotated for usage in the context of face expression recognition tests. An extensive performance assessment study using the new datasets has been conducted for the proposed and relevant prior solutions, showing that the CapsField proposed solution achieves superior performance for both face and expression recognition tasks when compared to the state-of-the-art.},
  archive      = {J_TIP},
  author       = {Alireza Sepas-Moghaddam and Ali Etemad and Fernando Pereira and Paulo Lobato Correia},
  doi          = {10.1109/TIP.2021.3054476},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2627-2642},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {CapsField: Light field-based face and expression recognition in the wild using capsule routing},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SSIM compliant modeling framework with denoising and
deblurring applications. <em>TIP</em>, <em>30</em>, 2611–2626. (<a
href="https://doi.org/10.1109/TIP.2021.3053369">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In image processing, it is well known that mean square error criteria is perceptually inadequate. Consequently, image quality assessment (IQA) has emerged as a new branch to overcome this issue, and this has led to the discovery of one of the most popular perceptual measures, namely, the structural similarity index (SSIM). This measure is mathematically simple, yet powerful enough to express the quality of an image. Therefore, it is natural to deploy SSIM in model based applications, such as denoising, restoration, classification, etc. However, the non-convex nature of this measure makes this task difficult. Our attempt in this work is to discuss problems associated with its convex program and take remedial action in the process of obtaining a generalized convex framework. The obtained framework has been seen as a component of an alternative learning scheme for the case of a regularized linear model. Subsequently, we develop a relevant dictionary learning module as a part of alternative learning. This alternative learning scheme with sparsity prior is finally used in denoising and deblurring applications. To further boost the performance, an iterative scheme is developed based on the statistical nature of added noise. Experiments on image denoising and deblurring validate the effectiveness of the proposed scheme. Furthermore, it has been shown that the proposed framework achieves highly competitive performance with respect to other schemes in literature and performs better in natural images in terms of SSIM and visual inspection.},
  archive      = {J_TIP},
  author       = {Rajesh Bhatt and Naren Naik and Venkatesh K. Subramanian},
  doi          = {10.1109/TIP.2021.3053369},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2611-2626},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {SSIM compliant modeling framework with denoising and deblurring applications},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Geometry-based camera calibration using closed-form solution
of principal line. <em>TIP</em>, <em>30</em>, 2599–2610. (<a
href="https://doi.org/10.1109/TIP.2020.3048684">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Camera calibration is a crucial prerequisite in many applications of computer vision. In this paper, a new geometry-based camera calibration technique is proposed, which resolves two main issues associated with the widely used Zhang&#39;s method: (i) the lack of guidelines to avoid outliers in the computation and (ii) the assumption of fixed camera focal length. The proposed approach is based on the closed-form solution of principal lines with their intersection being the principal point while each principal line can concisely represent relative orientation/position (up to one degree of freedom for both) between a special pair of coordinate systems of image plane and calibration pattern. With such analytically tractable image features, computations associated with the calibration are greatly simplified, while the guidelines in (i) can be established intuitively. Experimental results for synthetic and real data show that the proposed approach does compare favorably with Zhang&#39;s method, in terms of correctness, robustness, and flexibility, and addresses issues (i) and (ii) satisfactorily.},
  archive      = {J_TIP},
  author       = {Jen-Hui Chuang and Chih-Hui Ho and Ardian Umam and Hsin-Yi Chen and Jenq-Neng Hwang and Tai-An Chen},
  doi          = {10.1109/TIP.2020.3048684},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2599-2610},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Geometry-based camera calibration using closed-form solution of principal line},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SFace: Sigmoid-constrained hypersphere loss for robust face
recognition. <em>TIP</em>, <em>30</em>, 2587–2598. (<a
href="https://doi.org/10.1109/TIP.2020.3048632">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep face recognition has achieved great success due to large-scale training databases and rapidly developing loss functions. The existing algorithms devote to realizing an ideal idea: minimizing the intra-class distance and maximizing the inter-class distance. However, they may neglect that there are also low quality training images which should not be optimized in this strict way. Considering the imperfection of training databases, we propose that intra-class and inter-class objectives can be optimized in a moderate way to mitigate overfitting problem, and further propose a novel loss function, named sigmoid-constrained hypersphere loss (SFace). Specifically, SFace imposes intra-class and inter-class constraints on a hypersphere manifold, which are controlled by two sigmoid gradient re-scale functions respectively. The sigmoid curves precisely re-scale the intra-class and inter-class gradients so that training samples can be optimized to some degree. Therefore, SFace can make a better balance between decreasing the intra-class distances for clean examples and preventing overfitting to the label noise, and contributes more robust deep face recognition models. Extensive experiments of models trained on CASIA-WebFace, VGGFace2, and MS-Celeb-1M databases, and evaluated on several face recognition benchmarks, such as LFW, MegaFace and IJB-C databases, have demonstrated the superiority of SFace.},
  archive      = {J_TIP},
  author       = {Yaoyao Zhong and Weihong Deng and Jiani Hu and Dongyue Zhao and Xian Li and Dongchao Wen},
  doi          = {10.1109/TIP.2020.3048632},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2587-2598},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {SFace: Sigmoid-constrained hypersphere loss for robust face recognition},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fast multi-view clustering via nonnegative and orthogonal
factorization. <em>TIP</em>, <em>30</em>, 2575–2586. (<a
href="https://doi.org/10.1109/TIP.2020.3045631">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid growth of the number of data brings great challenges to clustering, especially the introduction of multi-view data, which collected from multiple sources or represented by multiple features, makes these challenges more arduous. How to clustering large-scale data efficiently has become the hottest topic of current large-scale clustering tasks. Although several accelerated multi-view methods have been proposed to improve the efficiency of clustering large-scale data, they still cannot be applied to some scenarios that require high efficiency because of the high computational complexity. To cope with the issue of high computational complexity of existing multi-view methods when dealing with large-scale data, a fast multi-view clustering model via nonnegative and orthogonal factorization (FMCNOF) is proposed in this paper. Instead of constraining the factor matrices to be nonnegative as traditional nonnegative and orthogonal factorization (NOF), we constrain a factor matrix of this model to be cluster indicator matrix which can assign cluster labels to data directly without extra post-processing step to extract cluster structures from the factor matrix. Meanwhile, the F-norm instead of the L2-norm is utilized on the FMCNOF model, which makes the model very easy to optimize. Furthermore, an efficient optimization algorithm is proposed to solve the FMCNOF model. Different from the traditional NOF optimization algorithm requiring dense matrix multiplications, our algorithm can divide the optimization problem into three decoupled small size subproblems that can be solved by much less matrix multiplications. Combined with the FMCNOF model and the corresponding fast optimization method, the efficiency of the clustering process can be significantly improved, and the computational complexity is nearly $O(n)$ . Extensive experiments on various benchmark data sets validate our approach can greatly improve the efficiency when achieve acceptable performance.},
  archive      = {J_TIP},
  author       = {Ben Yang and Xuetao Zhang and Feiping Nie and Fei Wang and Weizhong Yu and Rong Wang},
  doi          = {10.1109/TIP.2020.3045631},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2575-2586},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Fast multi-view clustering via nonnegative and orthogonal factorization},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multitask non-autoregressive model for human motion
prediction. <em>TIP</em>, <em>30</em>, 2562–2574. (<a
href="https://doi.org/10.1109/TIP.2020.3038362">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human motion prediction, which aims at predicting future human skeletons given the past ones, is a typical sequence-to-sequence problem. Therefore, extensive efforts have been devoted to exploring different RNN-based encoder-decoder architectures. However, by generating target poses conditioned on the previously generated ones, these models are prone to bringing issues such as error accumulation problem. In this paper, we argue that such issue is mainly caused by adopting autoregressive manner. Hence, a novel Non-AuToregressive model (NAT) is proposed with a complete non-autoregressive decoding scheme, as well as a context encoder and a positional encoding module. More specifically, the context encoder embeds the given poses from temporal and spatial perspectives. The frame decoder is responsible for predicting each future pose independently. The positional encoding module injects positional signal into the model to indicate the temporal order. Besides, a multitask training paradigm is presented for both low-level human skeleton prediction and high-level human action recognition, resulting in the considerable improvement for the prediction task. Our approach is evaluated on Human3.6M and CMU-Mocap benchmarks and outperforms state-of-the-art autoregressive methods.},
  archive      = {J_TIP},
  author       = {Bin Li and Jian Tian and Zhongfei Zhang and Hailin Feng and Xi Li},
  doi          = {10.1109/TIP.2020.3038362},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2562-2574},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Multitask non-autoregressive model for human motion prediction},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Affinity space adaptation for semantic segmentation across
domains. <em>TIP</em>, <em>30</em>, 2549–2561. (<a
href="https://doi.org/10.1109/TIP.2020.3018221">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic segmentation with dense pixel-wise annotation has achieved excellent performance thanks to deep learning. However, the generalization of semantic segmentation in the wild remains challenging. In this paper, we address the problem of unsupervised domain adaptation (UDA) in semantic segmentation. Motivated by the fact that source and target domain have invariant semantic structures, we propose to exploit such invariance across domains by leveraging co-occurring patterns between pairwise pixels in the output of structured semantic segmentation. This is different from most existing approaches that attempt to adapt domains based on individual pixel-wise information in image, feature, or output level. Specifically, we perform domain adaptation on the affinity relationship between adjacent pixels termed affinity space of source and target domain. To this end, we develop two affinity space adaptation strategies: affinity space cleaning and adversarial affinity space alignment. Extensive experiments demonstrate that the proposed method achieves superior performance against some state-of-the-art methods on several challenging benchmarks for semantic segmentation across domains. The code is available at https://github.com/idealwei/ASANet.},
  archive      = {J_TIP},
  author       = {Wei Zhou and Yukang Wang and Jiajia Chu and Jiehua Yang and Xiang Bai and Yongchao Xu},
  doi          = {10.1109/TIP.2020.3018221},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2549-2561},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Affinity space adaptation for semantic segmentation across domains},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Interaction-integrated network for natural language moment
localization. <em>TIP</em>, <em>30</em>, 2538–2548. (<a
href="https://doi.org/10.1109/TIP.2021.3052086">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Natural language moment localization aims at localizing video clips according to a natural language description. The key to this challenging task lies in modeling the relationship between verbal descriptions and visual contents. Existing approaches often sample a number of clips from the video, and individually determine how each of them is related to the query sentence. However, this strategy can fail dramatically, in particular when the query sentence refers to some visual elements that appear outside of, or even are distant from, the target clip. In this paper, we address this issue by designing an Interaction-Integrated Network (I 2 N), which contains a few Interaction-Integrated Cells (I 2 Cs). The idea lies in the observation that the query sentence not only provides a description to the video clip, but also contains semantic cues on the structure of the entire video. Based on this, I 2 Cs go one step beyond modeling short-term contexts in the time domain by encoding long-term video content into every frame feature. By stacking a few I 2 Cs, the obtained network, I 2 N, enjoys an improved ability of inference, brought by both (I) multi-level correspondence between vision and language and (II) more accurate cross-modal alignment. When evaluated on a challenging video moment localization dataset named DiDeMo, I 2 N outperforms the state-of-the-art approach by a clear margin of 1.98\%. On other two challenging datasets, Charades-STA and TACoS, I 2 N also reports competitive performance.},
  archive      = {J_TIP},
  author       = {Ke Ning and Lingxi Xie and Jianzhuang Liu and Fei Wu and Qi Tian},
  doi          = {10.1109/TIP.2021.3052086},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2538-2548},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Interaction-integrated network for natural language moment localization},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Superpixel-based quality assessment of multi-exposure image
fusion for both static and dynamic scenes. <em>TIP</em>, <em>30</em>,
2526–2537. (<a href="https://doi.org/10.1109/TIP.2021.3053465">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-exposure image fusion (MEF) algorithms have been used to merge a stack of low dynamic range images with various exposure levels into a well-perceived image. However, little work has been dedicated to predicting the visual quality of fused images. In this work, we propose a novel and efficient objective image quality assessment (IQA) model for MEF images of both static and dynamic scenes based on superpixels and an information theory adaptive pooling strategy. First, with the help of superpixels, we divide fused images into large- and small-changed regions using the structural inconsistency map between each exposure and fused images. Then, we compute the quality maps based on the Laplacian pyramid for large- and small-changed regions separately. Finally, an information theory induced adaptive pooling strategy is proposed to compute the perceptual quality of the fused image. Experimental results on three public databases of MEF images demonstrate the proposed model achieves promising performance and yields a relatively low computational complexity. Additionally, we also demonstrate the potential application for parameter tuning of MEF algorithms.},
  archive      = {J_TIP},
  author       = {Yuming Fang and Yan Zeng and Wenhui Jiang and Hanwei Zhu and Jiebin Yan},
  doi          = {10.1109/TIP.2021.3053465},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2526-2537},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Superpixel-based quality assessment of multi-exposure image fusion for both static and dynamic scenes},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adversarial training for solving inverse problems in image
processing. <em>TIP</em>, <em>30</em>, 2513–2525. (<a
href="https://doi.org/10.1109/TIP.2021.3053398">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inverse problems are a group of important mathematical problems that aim at estimating source data x and operation parameters z from inadequate observations y. In the image processing field, most recent deep learning-based methods simply deal with such problems under a pixel-wise regression framework (from y to x) while ignoring the physics behind. In this paper, we re-examine these problems under a different viewpoint and propose a novel framework for solving certain types of inverse problems in image processing. Instead of predicting x directly from y, we train a deep neural network to estimate the degradation parameters z under an adversarial training paradigm. We show that if the degradation behind satisfies some certain assumptions, the solution to the problem can be improved by introducing additional adversarial constraints to the parameter space and the training may not even require pair-wise supervision. In our experiment, we apply our method to a variety of real-world problems, including image denoising, image deraining, image shadow removal, non-uniform illumination correction, and underdetermined blind source separation of images or speech signals. The results on multiple tasks demonstrate the effectiveness of our method.},
  archive      = {J_TIP},
  author       = {Zhengxia Zou and Tianyang Shi and Zhenwei Shi and Jieping Ye},
  doi          = {10.1109/TIP.2021.3053398},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2513-2525},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Adversarial training for solving inverse problems in image processing},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning self-supervised space-time CNN for fast video style
transfer. <em>TIP</em>, <em>30</em>, 2501–2512. (<a
href="https://doi.org/10.1109/TIP.2021.3052709">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Style transfer on images has achieved significant advances in recent years, with the deep convolutional neural network (CNN). Directly applying image style transfer algorithms to each frame of a video independently often leads to flickering and unstable results. In this work, we present a self-supervised space-time convolutional neural network (CNN) based method for online video style transfer, named as VTNet, which is end-to-end trained from nearly unlimited unlabeled video data to produce temporally coherent stylized videos in real-time. Specifically, our VTNet transfer the style of a reference image to the source video frames, which is formed by the temporal prediction branch and the stylizing branch. The temporal prediction branch is used to capture discriminative spatiotemporal features for temporal consistency, pretrained in an adversarial manner from unlabeled video data. The stylizing branch is used to transfer the style image to a video frame with the guidance from the temporal prediction branch to ensure temporal consistency. To guide the training of VTNet, we introduce the style-coherence loss net (SCNet), which assembles the content loss, the style loss, and the new designed coherence loss. These losses are computed based on high-level features extracted from a pretrained VGG-16 network. The content loss is used to preserve high-level abstract contents of the input frames, and the style loss introduces new colors and patterns from the style image. Instead of using optical flow to explicitly redress the stylized video frames, we design the coherence loss to make the stylized video inherit the dynamics and motion patterns from the source video to remove temporal flickering. Extensive subjective and objective evaluations on various styles demonstrate that the proposed method achieves favorable results against the state-of-the-arts with high efficiency.},
  archive      = {J_TIP},
  author       = {Kai Xu and Longyin Wen and Guorong Li and Honggang Qi and Liefeng Bo and Qingming Huang},
  doi          = {10.1109/TIP.2021.3052709},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2501-2512},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Learning self-supervised space-time CNN for fast video style transfer},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SPADE-E2VID: Spatially-adaptive denormalization for
event-based video reconstruction. <em>TIP</em>, <em>30</em>, 2488–2500.
(<a href="https://doi.org/10.1109/TIP.2021.3052070">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Event-based cameras have several advantages over traditional cameras that shoot videos in frames. Event cameras have a high temporal resolution, high dynamic range, and almost non-existence of blurriness. The data that is produced by event sensors forms a chain of events when a change in brightness is reported in each pixel. This feature makes it difficult to directly apply existing algorithms and take advantage of the event camera data. Due to the developments in neural networks, important advances were made in event-based image reconstruction. Even though these neural networks achieve precise reconstructions while preserving most of the properties of the event cameras, there is still an initialization time that needs to have the highest possible quality in the reconstructed frames. In this work, we present the SPADE-E2VID neural network model that improves the quality of early frames in an event-based reconstructed video, as well as the overall contrast. The SPADE-E2VID model improves the quality of the first reconstructed frames by 15.87\% for MSE error, 4.15\% for SSIM, and 2.5\% in LPIPS. In addition, the SPADE layer in our model allows training our model to reconstruct videos without a temporal loss function. Another advantage of our model is that it has a faster training time. In a many-to-one training style, we avoid running the loss function at each step, executing the loss function at the end of each loop only once. In the present work, we also carried out experiments with event cameras that do not have polarity data. Our model produces quality video reconstructions with non-polarity events in HD resolution (1200 × 800). The Video, the code, and the datasets will be available at: https://github.com/RodrigoGantier/SPADE_E2VID.},
  archive      = {J_TIP},
  author       = {Pablo Rodrigo Gantier Cadena and Yeqiang Qian and Chunxiang Wang and Ming Yang},
  doi          = {10.1109/TIP.2021.3052070},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2488-2500},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {SPADE-E2VID: Spatially-adaptive denormalization for event-based video reconstruction},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Discriminative feature learning for thorax disease
classification in chest x-ray images. <em>TIP</em>, <em>30</em>,
2476–2487. (<a href="https://doi.org/10.1109/TIP.2021.3052711">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper focuses on the thorax disease classification problem in chest X-ray (CXR) images. Different from the generic image classification task, a robust and stable CXR image analysis system should consider the unique characteristics of CXR images. Particularly, it should be able to: 1) automatically focus on the disease-critical regions, which usually are of small sizes; 2) adaptively capture the intrinsic relationships among different disease features and utilize them to boost the multi-label disease recognition rates jointly. In this paper, we propose to learn discriminative features with a two-branch architecture, named ConsultNet, to achieve those two purposes simultaneously. ConsultNet consists of two components. First, an information bottleneck constrained feature selector extracts critical disease-specific features according to the feature importance. Second, a spatial-and-channel encoding based feature integrator enhances the latent semantic dependencies in the feature space. ConsultNet fuses these discriminative features to improve the performance of thorax disease classification in CXRs. Experiments conducted on the ChestX-ray14 and CheXpert dataset demonstrate the effectiveness of the proposed method.},
  archive      = {J_TIP},
  author       = {Qingji Guan and Yaping Huang and Yawei Luo and Ping Liu and Mingliang Xu and Yi Yang},
  doi          = {10.1109/TIP.2021.3052711},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2476-2487},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Discriminative feature learning for thorax disease classification in chest X-ray images},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Conditional generative ConvNets for exemplar-based texture
synthesis. <em>TIP</em>, <em>30</em>, 2461–2475. (<a
href="https://doi.org/10.1109/TIP.2021.3052075">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of exemplar-based texture synthesis is to generate texture images that are visually similar to a given exemplar. Recently, promising results have been reported by methods relying on convolutional neural networks (ConvNets) pretrained on large-scale image datasets. However, these methods have difficulties in synthesizing image textures with non-local structures and extending to dynamic or sound textures. In this article, we present a conditional generative ConvNet (cgCNN) model which combines deep statistics and the probabilistic framework of generative ConvNet (gCNN) model. Given a texture exemplar, cgCNN defines a conditional distribution using deep statistics of a ConvNet, and synthesizes new textures by sampling from the conditional distribution. In contrast to previous deep texture models, the proposed cgCNN does not rely on pre-trained ConvNets but learns the weights of ConvNets for each input exemplar instead. As a result, cgCNN can synthesize high quality dynamic, sound and image textures in a unified manner. We also explore the theoretical connections between our model and other texture models. Further investigations show that the cgCNN model can be easily generalized to texture expansion and inpainting. Extensive experiments demonstrate that our model can achieve better or at least comparable results than the state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Zi-Ming Wang and Meng-Han Li and Gui-Song Xia},
  doi          = {10.1109/TIP.2021.3052075},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2461-2475},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Conditional generative ConvNets for exemplar-based texture synthesis},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Vocabulary-wide credit assignment for training image
captioning models. <em>TIP</em>, <em>30</em>, 2450–2460. (<a
href="https://doi.org/10.1109/TIP.2021.3051476">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reinforcement learning (RL) algorithms have been shown to be efficient in training image captioning models. A critical step in RL algorithms is to assign credits to appropriate actions. There are mainly two classes of credit assignment methods in existing RL methods for image captioning, assigning a single credit for the whole sentence and assigning a credit to every word in the sentence. In this article, we propose a new credit assignment method which is orthogonal to the above two. It assigns every word in vocabulary an appropriate credit at each generation step. It is called vocabulary-wide credit assignment. Based on this we propose a Vocabulary-Critical Sequence Training (VCST). VCST can be incorporated into existing RL methods for training image captioning models to achieve better results. Extensive experiments with many popular models validated the effectiveness of VCST.},
  archive      = {J_TIP},
  author       = {Han Liu and Shifeng Zhang and Ke Lin and Jing Wen and Jianmin Li and Xiaolin Hu},
  doi          = {10.1109/TIP.2021.3051476},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2450-2460},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Vocabulary-wide credit assignment for training image captioning models},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Gated path selection network for semantic segmentation.
<em>TIP</em>, <em>30</em>, 2436–2449. (<a
href="https://doi.org/10.1109/TIP.2020.3046921">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic segmentation is a challenging task that needs to handle large scale variations, deformations, and different viewpoints. In this paper, we develop a novel network named Gated Path Selection Network (GPSNet), which aims to adaptively select receptive fields while maintaining the dense sampling capability. In GPSNet, we first design a two-dimensional SuperNet, which densely incorporates features from growing receptive fields. And then, a Comparative Feature Aggregation (CFA) module is introduced to dynamically aggregate discriminative semantic context. In contrast to previous works that focus on optimizing sparse sampling locations on regular grids, GPSNet can adaptively harvest free form dense semantic context information. The derived adaptive receptive fields and dense sampling locations are data-dependent and flexible which can model various contexts of objects. On two representative semantic segmentation datasets, i.e ., Cityscapes and ADE20K, we show that the proposed approach consistently outperforms previous methods without bells and whistles.},
  archive      = {J_TIP},
  author       = {Qichuan Geng and Hong Zhang and Xiaojuan Qi and Gao Huang and Ruigang Yang and Zhong Zhou},
  doi          = {10.1109/TIP.2020.3046921},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2436-2449},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Gated path selection network for semantic segmentation},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Towards fine-grained human pose transfer with detail
replenishing network. <em>TIP</em>, <em>30</em>, 2422–2435. (<a
href="https://doi.org/10.1109/TIP.2021.3052364">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human pose transfer (HPT) is an emerging research topic with huge potential in fashion design, media production, online advertising and virtual reality. For these applications, the visual realism of fine-grained appearance details is crucial for production quality and user engagement. However, existing HPT methods often suffer from three fundamental issues: detail deficiency, content ambiguity and style inconsistency, which severely degrade the visual quality and realism of generated images. Aiming towards real-world applications, we develop a more challenging yet practical HPT setting, termed as Fine-grained Human Pose Transfer (FHPT), with a higher focus on semantic fidelity and detail replenishment. Concretely, we analyze the potential design flaws of existing methods via an illustrative example, and establish the core FHPT methodology by combing the idea of content synthesis and feature transfer together in a mutually-guided fashion. Thereafter, we substantiate the proposed methodology with a Detail Replenishing Network (DRN) and a corresponding coarse-to-fine model training scheme. Moreover, we build up a complete suite of fine-grained evaluation protocols to address the challenges of FHPT in a comprehensive manner, including semantic analysis, structural detection and perceptual quality assessment. Extensive experiments on the DeepFashion benchmark dataset have verified the power of proposed benchmark against start-of-the-art works, with 12\%–14\% gain on top-10 retrieval recall, 5\% higher joint localization accuracy, and near 40\% gain on face identity preservation. Our codes, models and evaluation tools will be released at https://github.com/Lotayou/RATE},
  archive      = {J_TIP},
  author       = {Lingbo Yang and Pan Wang and Chang Liu and Zhanning Gao and Peiran Ren and Xinfeng Zhang and Shanshe Wang and Siwei Ma and Xiansheng Hua and Wen Gao},
  doi          = {10.1109/TIP.2021.3052364},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2422-2435},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Towards fine-grained human pose transfer with detail replenishing network},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hierarchical image segmentation based on nonsymmetry and
anti-packing pattern representation model. <em>TIP</em>, <em>30</em>,
2408–2421. (<a href="https://doi.org/10.1109/TIP.2021.3052359">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image segmentation is the foundation of high-level image analysis and image understanding. How to effectively segment an image into regions that are “meaningful” to the human visual perception and ensure that the segmented regions are consistent at different resolutions is still a very challenging issue. Inspired by the idea of the Nonsymmetry and Anti-packing pattern representation Model in the Lab color space (NAMLab) and the “global-first” invariant perceptual theory, in this paper, we propose a novel framework for hierarchical image segmentation. Firstly, by defining the dissimilarity between two pixels in the Lab color space, we propose an NAMLab-based color image representation approach that is more in line with the human visual perception characteristics and can make the image pixels fast and effectively merge into the NAMLab blocks. Then, by defining the dissimilarity between two NAMLab-based regions and iteratively executing NAMLab-based merging algorithm of adjacent regions into larger ones to progressively generate a segmentation dendrogram, we propose a fast NAMLab-based algorithm for hierarchical image segmentation. Finally, the complexities of our proposed NAMLab-based algorithm for hierarchical image segmentation are analyzed in details. The experimental results presented in this paper show that our proposed algorithm when compared with the state-of-the-art algorithms not only can preserve more details of the object boundaries, but also it can better identify the foreground objects with similar color distributions. Also, our proposed algorithm can be executed much faster and takes up less memory and therefore it is a better algorithm for hierarchical image segmentation.},
  archive      = {J_TIP},
  author       = {Yunping Zheng and Bowen Yang and Mudar Sarem},
  doi          = {10.1109/TIP.2021.3052359},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2408-2421},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Hierarchical image segmentation based on nonsymmetry and anti-packing pattern representation model},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). One-class fingerprint presentation attack detection using
auto-encoder network. <em>TIP</em>, <em>30</em>, 2394–2407. (<a
href="https://doi.org/10.1109/TIP.2021.3052341">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automated Fingerprint Recognition Systems (AFRSs) have been threatened by Presentation Attack (PA) since its existence. It is thus desirable to develop effective presentation attack detection (PAD) methods. However, the unpredictable PAs make PAD be a challenging problem. This paper proposes a novel One-Class PAD (OCPAD) method for Optical Coherence Technology (OCT) images based fingerprint PA detection. The proposed OCPAD model is learned from a training set only consists of Bonafides (i.e. real fingerprints). The reconstruction error and latent code obtained from the trained auto-encoder network in the proposed model is taken as the basis for the following spoofness score calculation. To get more accurate reconstruction error, we propose an activation map based weighting model to further refine the accuracy of reconstruction error. We test different statistics and distance measures and finally use a decision level fusion to make the final prediction. Our experiments are performed using a dataset with 93200 bonafide scans and 48400 PA scans. The results show that the proposed OCPAD can achieve a True Positive Rate (TPR) of 99.43\% when the False Positive Rate (FPR) equals to 10\% and a TPR of 96.59\% when FPR=5\%, which significantly outperformed a feature based approach and a supervised learning based model requiring PAs for training.},
  archive      = {J_TIP},
  author       = {Feng Liu and Haozhe Liu and Wentian Zhang and Guojie Liu and Linlin Shen},
  doi          = {10.1109/TIP.2021.3052341},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2394-2407},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {One-class fingerprint presentation attack detection using auto-encoder network},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Low complexity trellis-coded quantization in versatile
video coding. <em>TIP</em>, <em>30</em>, 2378–2393. (<a
href="https://doi.org/10.1109/TIP.2021.3051460">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The forthcoming Versatile Video Coding (VVC) standard adopts the trellis-coded quantization, which leverages the delicate trellis graph to map the quantization candidates within one block into the optimal path. Despite the high compression efficiency, the complex trellis search with soft-decision quantization may hinder the applications due to high complexity and low throughput capacity. To reduce the complexity, in this paper, we propose a low complexity trellis-coded quantization scheme in a scientifically sound way with theoretical modeling of the rate and distortion. As such, the trellis departure point can be adaptively adjusted, and unnecessarily visited branches are accordingly pruned, leading to the shrink of total trellis stages and simplification of transition branches. Extensive experimental results on the VVC test model show that the proposed scheme is effective in reducing the encoding complexity by 11\% and 5\% with all intra and random access configurations, respectively, at the cost of only 0.11\% and 0.05\% BD-Rate increase. Meanwhile, on average 24\% and 27\% quantization time savings can be achieved under all intra and random access configurations. Due to the excellent performance, the VVC test model has adopted one implementation of the proposed scheme.},
  archive      = {J_TIP},
  author       = {Meng Wang and Shiqi Wang and Junru Li and Li Zhang and Yue Wang and Siwei Ma and Sam Kwong},
  doi          = {10.1109/TIP.2021.3051460},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2378-2393},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Low complexity trellis-coded quantization in versatile video coding},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Cubemap-based perception-driven blind quality assessment for
360-degree images. <em>TIP</em>, <em>30</em>, 2364–2377. (<a
href="https://doi.org/10.1109/TIP.2021.3052073">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {image can be represented with different formats, such as the equirectangular projection (ERP) image, viewport images or spherical image, for its different processing procedures and applications. Accordingly, the 360-degree image quality assessment (360-IQA) can be performed on these different formats. However, the performance of 360-IQA with the ERP image is not equivalent with those with the viewport images or spherical image due to the over-sampling and the resulted obvious geometric distortion of ERP image. This imbalance problem brings challenge to ERP image based applications, such as 360-degree image/video compression and assessment. In this paper, we propose a new blind 360-IQA framework to handle this imbalance problem. In the proposed framework, cubemap projection (CMP) with six inter-related faces is used to realize the omnidirectional viewing of 360-degree image. A multi-distortions visual attention quality dataset for 360-degree images is firstly established as the benchmark to analyze the performance of objective 360-IQA methods. Then, the perception-driven blind 360-IQA framework is proposed based on six cubemap faces of CMP for 360-degree image, in which human attention behavior is taken into account to improve the effectiveness of the proposed framework. The cubemap quality feature subset of CMP image is first obtained, and additionally, attention feature matrices and subsets are also calculated to describe the human visual behavior. Experimental results show that the proposed framework achieves superior performances compared with state-of-the-art IQA methods, and the cross dataset validation also verifies the effectiveness of the proposed framework. In addition, the proposed framework can also be combined with new quality feature extraction method to further improve the performance of 360-IQA. All of these demonstrate that the proposed framework is effective in 360-IQA and has a good potential for future applications.},
  archive      = {J_TIP},
  author       = {Hao Jiang and Gangyi Jiang and Mei Yu and Yun Zhang and You Yang and Zongju Peng and Fen Chen and Qingbo Zhang},
  doi          = {10.1109/TIP.2021.3052073},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2364-2377},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Cubemap-based perception-driven blind quality assessment for 360-degree images},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Depth-quality-aware salient object detection. <em>TIP</em>,
<em>30</em>, 2350–2363. (<a
href="https://doi.org/10.1109/TIP.2021.3052069">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The existing fusion-based RGB-D salient object detection methods usually adopt the bistream structure to strike a balance in the fusion trade-off between RGB and depth (D). While the D quality usually varies among the scenes, the state-of-the-art bistream approaches are depth-quality-unaware, resulting in substantial difficulties in achieving complementary fusion status between RGB and D and leading to poor fusion results for low-quality D. Thus, this paper attempts to integrate a novel depth-quality-aware subnet into the classic bistream structure in order to assess the depth quality prior to conducting the selective RGB-D fusion. Compared to the SOTA bistream methods, the major advantage of our method is its ability to lessen the importance of the low-quality, no-contribution, or even negative-contribution D regions during RGB-D fusion, achieving a much improved complementary status between RGB and D. Our source code and data are available online at https://github.com/qdu1995/DQSD.},
  archive      = {J_TIP},
  author       = {Chenglizhao Chen and Jipeng Wei and Chong Peng and Hong Qin},
  doi          = {10.1109/TIP.2021.3052069},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2350-2363},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Depth-quality-aware salient object detection},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). EnlightenGAN: Deep light enhancement without paired
supervision. <em>TIP</em>, <em>30</em>, 2340–2349. (<a
href="https://doi.org/10.1109/TIP.2021.3051462">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning-based methods have achieved remarkable success in image restoration and enhancement, but are they still competitive when there is a lack of paired training data? As one such example, this paper explores the low-light image enhancement problem, where in practice it is extremely challenging to simultaneously take a low-light and a normal-light photo of the same visual scene. We propose a highly effective unsupervised generative adversarial network, dubbed EnlightenGAN , that can be trained without low/normal-light image pairs, yet proves to generalize very well on various real-world test images. Instead of supervising the learning using ground truth data, we propose to regularize the unpaired training using the information extracted from the input itself, and benchmark a series of innovations for the low-light image enhancement problem, including a global-local discriminator structure, a self-regularized perceptual loss fusion, and the attention mechanism. Through extensive experiments, our proposed approach outperforms recent methods under a variety of metrics in terms of visual quality and subjective user study. Thanks to the great flexibility brought by unpaired training, EnlightenGAN is demonstrated to be easily adaptable to enhancing real-world images from various domains. Our codes and pre-trained models are available at: https://github.com/VITA-Group/EnlightenGAN .},
  archive      = {J_TIP},
  author       = {Yifan Jiang and Xinyu Gong and Ding Liu and Yu Cheng and Chen Fang and Xiaohui Shen and Jianchao Yang and Pan Zhou and Zhangyang Wang},
  doi          = {10.1109/TIP.2021.3051462},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2340-2349},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {EnlightenGAN: Deep light enhancement without paired supervision},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Interpretable detail-fidelity attention network for single
image super-resolution. <em>TIP</em>, <em>30</em>, 2325–2339. (<a
href="https://doi.org/10.1109/TIP.2021.3050856">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Benefiting from the strong capabilities of deep CNNs for feature representation and nonlinear mapping, deep-learning-based methods have achieved excellent performance in single image super-resolution. However, most existing SR methods depend on the high capacity of networks that are initially designed for visual recognition, and rarely consider the initial intention of super-resolution for detail fidelity. To pursue this intention, there are two challenging issues that must be solved: (1) learning appropriate operators which is adaptive to the diverse characteristics of smoothes and details; (2) improving the ability of the model to preserve low-frequency smoothes and reconstruct high-frequency details. To solve these problems, we propose a purposeful and interpretable detail-fidelity attention network to progressively process these smoothes and details in a divide-and-conquer manner, which is a novel and specific prospect of image super-resolution for the purpose of improving detail fidelity. This proposed method updates the concept of blindly designing or using deep CNNs architectures for only feature representation in local receptive fields. In particular, we propose a Hessian filtering for interpretable high-profile feature representation for detail inference, along with a dilated encoder-decoder and a distribution alignment cell to improve the inferred Hessian features in a morphological manner and statistical manner respectively. Extensive experiments demonstrate that the proposed method achieves superior performance compared to the state-of-the-art methods both quantitatively and qualitatively. The code is available at github.com/YuanfeiHuang/DeFiAN.},
  archive      = {J_TIP},
  author       = {Yuanfei Huang and Jie Li and Xinbo Gao and Yanting Hu and Wen Lu},
  doi          = {10.1109/TIP.2021.3050856},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2325-2339},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Interpretable detail-fidelity attention network for single image super-resolution},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Spatial information guided convolution for real-time RGBD
semantic segmentation. <em>TIP</em>, <em>30</em>, 2313–2324. (<a
href="https://doi.org/10.1109/TIP.2021.3049332">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D spatial information is known to be beneficial to the semantic segmentation task. Most existing methods take 3D spatial data as an additional input, leading to a two-stream segmentation network that processes RGB and 3D spatial information separately. This solution greatly increases the inference time and severely limits its scope for real-time applications. To solve this problem, we propose Spatial information guided Convolution (S-Conv), which allows efficient RGB feature and 3D spatial information integration. S-Conv is competent to infer the sampling offset of the convolution kernel guided by the 3D spatial information, helping the convolutional layer adjust the receptive field and adapt to geometric transformations. S-Conv also incorporates geometric information into the feature learning process by generating spatially adaptive convolutional weights. The capability of perceiving geometry is largely enhanced without much affecting the amount of parameters and computational cost. Based on S-Conv, we further design a semantic segmentation network, called Spatial information Guided convolutional Network (SGNet), resulting in real-time inference and state-of-the-art performance on NYUDv2 and SUNRGBD datasets.},
  archive      = {J_TIP},
  author       = {Lin-Zhuo Chen and Zheng Lin and Ziqin Wang and Yong-Liang Yang and Ming-Ming Cheng},
  doi          = {10.1109/TIP.2021.3049332},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2313-2324},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Spatial information guided convolution for real-time RGBD semantic segmentation},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). MSB-FCN: Multi-scale bidirectional FCN for object skeleton
extraction. <em>TIP</em>, <em>30</em>, 2301–2312. (<a
href="https://doi.org/10.1109/TIP.2020.3038483">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The performance of state-of-the-art object skeleton detection (OSD) methods have been greatly boosted by Convolutional Neural Networks (CNNs). However, the most existing CNN-based OSD methods rely on a ‘skip-layer’ structure where low-level and high-level features are combined to gather multi-level contextual information. Unfortunately, as shallow features tend to be noisy and lack semantic knowledge, they will cause errors and inaccuracy. Therefore, in order to improve the accuracy of object skeleton detection, we propose a novel network architecture, the Multi-Scale Bidirectional Fully Convolutional Network (MSB-FCN), to better gather and enhance multi-scale high-level contextual information. The advantage is that only deep features are used to construct multi-scale feature representations along with a bidirectional structure for better capturing contextual knowledge. This enables the proposed MSB-FCN to learn semantic-level information from different sub-regions. Moreover, we introduce dense connections into the bidirectional structure to ensure that the learning process at each scale can directly encode information from all other scales. An attention pyramid is also integrated into our MSB-FCN to dynamically control information propagation and reduce unreliable features. Extensive experiments on various benchmarks demonstrate that the proposed MSB-FCN achieves significant improvements over the state-of-the-art algorithms.},
  archive      = {J_TIP},
  author       = {Fan Yang and Xin Li and Jianbing Shen},
  doi          = {10.1109/TIP.2020.3038483},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2301-2312},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {MSB-FCN: Multi-scale bidirectional FCN for object skeleton extraction},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021c). A lightweight depth estimation network for wide-baseline
light fields. <em>TIP</em>, <em>30</em>, 2288–2300. (<a
href="https://doi.org/10.1109/TIP.2021.3051761">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing traditional and ConvNet-based methods for light field depth estimation mainly work on the narrow-baseline scenario. This paper explores the feasibility and capability of ConvNets to estimate depth in another promising scenario: wide-baseline light fields. Due to the deficiency of training samples, a large-scale and diverse synthetic wide-baseline dataset with labelled data is introduced for depth prediction tasks. Considering the practical goal for real-world applications, we design an end-to-end trained lightweight convolutional network to infer depths from light fields, called LLF-Net. The proposed LLF-Net is built by incorporating a cost volume which allows variable angular light field inputs and an attention module that enables to recover details at occlusion areas. Evaluations are made on the synthetic and real-world wide-baseline light fields, and experimental results show that the proposed network achieves the best performance when compared to recent state-of-the-art methods. We also evaluate our LLF-Net on narrow-baseline datasets, and it consequently improves the performance of previous methods.},
  archive      = {J_TIP},
  author       = {Yan Li and Qiong Wang and Lu Zhang and Gauthier Lafruit},
  doi          = {10.1109/TIP.2021.3051761},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2288-2300},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A lightweight depth estimation network for wide-baseline light fields},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Correcting higher order aberrations using image processing.
<em>TIP</em>, <em>30</em>, 2276–2287. (<a
href="https://doi.org/10.1109/TIP.2021.3051499">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Higher Order Aberrations (HOAs) are complex refractive errors in the human eye that cannot be corrected by regular lens systems. Researchers have developed numerous approaches to analyze the effect of these refractive errors; the most popular among these approaches use Zernike polynomial approximation to describe the shape of the wavefront of light exiting the pupil after it has been altered by the refractive errors. We use this wavefront shape to create a linear imaging system that simulates how the eye perceives source images at the retina. With phase information from this system, we create a second linear imaging system to modify source images so that they would be perceived by the retina without distortion. By modifying source images, the visual process cascades two optical systems before the light reaches the retina, a technique that counteracts the effect of the refractive errors. While our method effectively compensates for distortions induced by HOAs, it also introduces blurring and loss of contrast; a problem that we address with Total Variation Regularization. With this technique, we optimize source images so that they are perceived at the retina as close as possible to the original source image. To measure the effectiveness of our methods, we compute the Euclidean error between the source images and the images perceived at the retina. When comparing our results with existing corrective methods that use deconvolution and total variation regularization, we achieve an average of 50\% reduction in error with lower computational costs.},
  archive      = {J_TIP},
  author       = {Olga E. Jumbo and Shihab Asfour and Ahmed M. Sayed and Mohamed Abdel-Mottaleb},
  doi          = {10.1109/TIP.2021.3051499},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2276-2287},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Correcting higher order aberrations using image processing},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hypergraph neural network for skeleton-based action
recognition. <em>TIP</em>, <em>30</em>, 2263–2275. (<a
href="https://doi.org/10.1109/TIP.2021.3051495">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, skeleton-based human action recognition has attracted a lot of research attention in the field of computer vision. Graph convolutional networks (GCNs), which model the human body skeletons as spatial-temporal graphs, have shown excellent results. However, the existing methods only focus on the local physical connection between the joints, and ignore the non-physical dependencies among joints. To address this issue, we propose a hypergraph neural network (Hyper-GNN) to capture both spatial-temporal information and high-order dependencies for skeleton-based action recognition. In particular, to overcome the influence of noise caused by unrelated joints, we design the Hyper-GNN to extract the local and global structure information via the hyperedge (i.e., non-physical connection) constructions. In addition, the hypergraph attention mechanism and improved residual module are induced to further obtain the discriminative feature representations. Finally, a three-stream Hyper-GNN fusion architecture is adopted in the whole framework for action recognition. The experimental results performed on two benchmark datasets demonstrate that our proposed method can achieve the best performance when compared with the state-of-the-art skeleton-based methods.},
  archive      = {J_TIP},
  author       = {Xiaoke Hao and Jie Li and Yingchun Guo and Tao Jiang and Ming Yu},
  doi          = {10.1109/TIP.2021.3051495},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2263-2275},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Hypergraph neural network for skeleton-based action recognition},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). CameraNet: A two-stage framework for effective camera ISP
learning. <em>TIP</em>, <em>30</em>, 2248–2262. (<a
href="https://doi.org/10.1109/TIP.2021.3051486">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional image signal processing (ISP) pipeline consists of a set of cascaded image processing modules onboard a camera to reconstruct a high-quality sRGB image from the sensor raw data. Recently, some methods have been proposed to learn a convolutional neural network (CNN) to improve the performance of traditional ISP. However, in these works usually a CNN is directly trained to accomplish the ISP tasks without considering much the correlation among the different components in an ISP. As a result, the quality of reconstructed images is barely satisfactory in challenging scenarios such as low-light imaging. In this paper, we firstly analyze the correlation among the different tasks in an ISP, and categorize them into two weakly correlated groups: restoration and enhancement. Then we design a two-stage network, called CameraNet, to progressively learn the two groups of ISP tasks. In each stage, a ground truth is specified to supervise the subnetwork learning, and the two subnetworks are jointly fine-tuned to produce the final output. Experiments on three benchmark datasets show that the proposed CameraNet achieves consistently compelling reconstruction quality and outperforms the recently proposed ISP learning methods.},
  archive      = {J_TIP},
  author       = {Zhetong Liang and Jianrui Cai and Zisheng Cao and Lei Zhang},
  doi          = {10.1109/TIP.2021.3051486},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2248-2262},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {CameraNet: A two-stage framework for effective camera ISP learning},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep SAR imaging and motion compensation. <em>TIP</em>,
<em>30</em>, 2232–2247. (<a
href="https://doi.org/10.1109/TIP.2021.3051484">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compressive sensing (CS) and matrix sensing (MS) techniques have been applied to the synthetic aperture radar (SAR) imaging problem to reduce the sampling amount of SAR echo using the sparse or low-rank prior information. To further exploit the redundancy and improve sampling efficiency, we take a different approach, wherein a deep SAR imaging algorithm is proposed. The main idea is to exploit the redundancy of the backscattering coefficient using an auto-encoder structure, wherein the hidden latent layer in auto-encoder has lower dimension and less parameters than the backscattering coefficient layer. Based on the auto-encoder model, the parameters of the auto-encoder structure and the backscattering coefficient are estimated simultaneously by optimizing the reconstruction loss associated with the down-sampled SAR echo. In addition, in order to meet the practical application requirements, a deep SAR motion compensation algorithm is proposed to eliminate the effect of motion errors on imaging results. The effectiveness of the proposed algorithms is verified on both simulated and real SAR data.},
  archive      = {J_TIP},
  author       = {Wei Pu},
  doi          = {10.1109/TIP.2021.3051484},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2232-2247},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Deep SAR imaging and motion compensation},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning diverse models for end-to-end ensemble tracking.
<em>TIP</em>, <em>30</em>, 2220–2231. (<a
href="https://doi.org/10.1109/TIP.2021.3051471">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In visual tracking, how to effectively model the target appearance using limited prior information remains an open problem. In this paper, we leverage an ensemble of diverse models to learn manifold representations for robust object tracking. The proposed ensemble framework includes a shared backbone network for efficient feature extraction and multiple head networks for independent predictions. Trained by the shared data within an identical structure, the mutually correlated head models heavily hinder the potential of ensemble learning. To shrink the representational overlaps among multiple models while encouraging the diversity of individual predictions, we propose the model diversity and response diversity regularization terms during training. By fusing these distinctive prediction results via a fusion module, the tracking variance caused by the distractor objects can be largely restrained. Our whole framework is end-to-end trained in a data-driven manner, avoiding the heuristic designs of multiple base models and fusion strategies. The proposed method achieves state-of-the-art results on seven challenging benchmarks while operating in real-time.},
  archive      = {J_TIP},
  author       = {Ning Wang and Wengang Zhou and Houqiang Li},
  doi          = {10.1109/TIP.2021.3051471},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2220-2231},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Learning diverse models for end-to-end ensemble tracking},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Semi-supervised low-rank semantics grouping for zero-shot
learning. <em>TIP</em>, <em>30</em>, 2207–2219. (<a
href="https://doi.org/10.1109/TIP.2021.3050677">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zero-shot learning has received great interest in visual recognition community. It aims to classify new unobserved classes based on the model learned from observed classes. Most zero-shot learning methods require pre-provided semantic attributes as the mid-level information to discover the intrinsic relationship between observed and unobserved categories. However, it is impractical to annotate the enriched label information of the observed objects in real-world applications, which would extremely hurt the performance of zero-shot learning with limited labeled seen data. To overcome this obstacle, we develop a Low-rank Semantics Grouping (LSG) method for zero-shot learning in a semi-supervised fashion, which attempts to jointly uncover the intrinsic relationship across visual and semantic information and recover the missing label information from seen classes. Specifically, the visual-semantic encoder is utilized as projection model, low-rank semantic grouping scheme is explored to capture the intrinsic attributes correlations and a Laplacian graph is constructed from the visual features to guide the label propagation from labeled instances to unlabeled ones. Experiments have been conducted on several standard zero-shot learning benchmarks, which demonstrate the efficiency of the proposed method by comparing with state-of-the-art methods. Our model is robust to different levels of missing label settings. Also visualized results prove that the LSG can distinguish the test unseen classes more discriminative.},
  archive      = {J_TIP},
  author       = {Bingrong Xu and Zhigang Zeng and Cheng Lian and Zhengming Ding},
  doi          = {10.1109/TIP.2021.3050677},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2207-2219},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Semi-supervised low-rank semantics grouping for zero-shot learning},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Ellipse r-CNN: Learning to infer elliptical object from
clustering and occlusion. <em>TIP</em>, <em>30</em>, 2193–2206. (<a
href="https://doi.org/10.1109/TIP.2021.3050673">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Images of heavily occluded objects in cluttered scenes, such as fruit clusters in trees, are hard to segment. To further retrieve the 3D size and 6D pose of each individual object in such cases, bounding boxes are not reliable from multiple views since only a little portion of the object&#39;s geometry is captured. We introduce the first CNN-based ellipse detector, called Ellipse R-CNN, to represent and infer occluded objects as ellipses. We first propose a robust and compact ellipse regression based on the Mask R-CNN architecture for elliptical object detection. Our method can infer the parameters of multiple elliptical objects even they are occluded by other neighboring objects. For better occlusion handling, we exploit refined feature regions for the regression stage, and integrate the U-Net structure for learning different occlusion patterns to compute the final detection score. The correctness of ellipse regression is validated through experiments performed on synthetic data of clustered ellipses. We further quantitatively and qualitatively demonstrate that our approach outperforms the state-of-the-art model (i.e., Mask R-CNN followed by ellipse fitting) and its three variants on both synthetic and real datasets of occluded and clustered elliptical objects.},
  archive      = {J_TIP},
  author       = {Wenbo Dong and Pravakar Roy and Cheng Peng and Volkan Isler},
  doi          = {10.1109/TIP.2021.3050673},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2193-2206},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Ellipse R-CNN: Learning to infer elliptical object from clustering and occlusion},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). IDE: Image dehazing and exposure using an enhanced
atmospheric scattering model. <em>TIP</em>, <em>30</em>, 2180–2192. (<a
href="https://doi.org/10.1109/TIP.2021.3050643">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Atmospheric scattering model (ASM) is one of the most widely used model to describe the imaging processing of hazy images. However, we found that ASM has an intrinsic limitation which leads to a dim effect in the recovered results. In this paper, by introducing a new parameter, i.e., light absorption coefficient, into ASM, an enhanced ASM (EASM) is attained, which can address the dim effect and better model outdoor hazy scenes. Relying on this EASM, a simple yet effective gray-world-assumption-based technique called IDE is then developed to enhance the visibility of hazy images. Experimental results show that IDE eliminates the dim effect and exhibits excellent dehazing performance. It is worth mentioning that IDE does not require any training process or extra information related to scene depth, which makes it very fast and robust. Moreover, the global stretch strategy used in IDE can effectively avoid some undesirable effects in recovery results, e.g., over-enhancement, over-saturation, and mist residue, etc. Comparison between the proposed IDE and other state-of-the-art techniques reveals the superiority of IDE in terms of both dehazing quality and efficiency over all the comparable techniques.},
  archive      = {J_TIP},
  author       = {Mingye Ju and Can Ding and Wenqi Ren and Yi Yang and Dengyin Zhang and Y. Jay Guo},
  doi          = {10.1109/TIP.2021.3050643},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2180-2192},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {IDE: Image dehazing and exposure using an enhanced atmospheric scattering model},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). 3D object representation learning: A set-to-set matching
perspective. <em>TIP</em>, <em>30</em>, 2168–2179. (<a
href="https://doi.org/10.1109/TIP.2021.3049968">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we tackle the 3D object representation learning from the perspective of set-to-set matching. Given two 3D objects, calculating their similarity is formulated as the problem of set-to-set similarity measurement between two set of local patches. As local convolutional features from convolutional feature maps are natural representations of local patches, the set-to-set matching between sets of local patches is further converted into a local features pooling problem. To highlight good matchings and suppress the bad ones, we exploit two pooling methods: 1) bilinear pooling and 2) VLAD pooling. We analyze their effectiveness in enhancing the set-to-set matching and meanwhile establish their connection. Moreover, to balance different components inherent in a bilinear-pooled feature, we propose the harmonized bilinear pooling operation, which follows the spirits of intra-normalization used in VLAD pooling. To achieve an end-to-end trainable framework, we implement the proposed harmonized bilinear pooling and intra-normalized VLAD as two layers to construct two types of neural network, multi-view harmonized bilinear network (MHBN) and multi-view VLAD network (MVLADN). Systematic experiments conducted on two public benchmark datasets demonstrate the efficacy of the proposed MHBN and MVLADN in 3D object recognition.},
  archive      = {J_TIP},
  author       = {Tan Yu and Jingjing Meng and Ming Yang and Junsong Yuan},
  doi          = {10.1109/TIP.2021.3049968},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2168-2179},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {3D object representation learning: A set-to-set matching perspective},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SAN: Selective alignment network for cross-domain pedestrian
detection. <em>TIP</em>, <em>30</em>, 2155–2167. (<a
href="https://doi.org/10.1109/TIP.2021.3049948">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-domain pedestrian detection, which has been attracting much attention, assumes that the training and test images are drawn from different data distributions. Existing methods focus on aligning the descriptions of whole candidate instances between source and target domains. Since there exists a giant visual difference among the candidate instances, aligning whole candidate instances between two domains cannot overcome the inter-instance difference. Compared with aligning the whole candidate instances, we consider that aligning each type of instances separately is a more reasonable manner. Therefore, we propose a novel Selective Alignment Network for cross-domain pedestrian detection, which consists of three components: a Base Detector, an Image-Level Adaptation Network, and an Instance-Level Adaptation Network. The Image-Level Adaptation Network and Instance-Level Adaptation Network can be regarded as the global-level and local-level alignments, respectively. Similar to the Faster R-CNN, the Base Detector, which is composed of a Feature module, an RPN module and a Detection module, is used to infer a robust pedestrian detector with the annotated source data. Once obtaining the image description extracted by the Feature module, the Image-Level Adaptation Network is proposed to align the image description with an adversarial domain classifier. Given the candidate proposals generated by the RPN module, the Instance-Level Adaptation Network firstly clusters the source candidate proposals into several groups according to their visual features, and thus generates the pseudo label for each candidate proposal. After generating the pseudo labels, we align the source and target domains by maximizing and minimizing the discrepancy between the prediction of two classifiers iteratively. Extensive evaluations on several benchmarks demonstrate the effectiveness of the proposed approach for cross-domain pedestrian detection.},
  archive      = {J_TIP},
  author       = {Yifan Jiao and Hantao Yao and Changsheng Xu},
  doi          = {10.1109/TIP.2021.3049948},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2155-2167},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {SAN: Selective alignment network for cross-domain pedestrian detection},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fixed viewpoint mirror surface reconstruction under an
uncalibrated camera. <em>TIP</em>, <em>30</em>, 2141–2154. (<a
href="https://doi.org/10.1109/TIP.2021.3049946">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the problem of mirror surface reconstruction, and proposes a solution based on observing the reflections of a moving reference plane on the mirror surface. Unlike previous approaches which require tedious calibration, our method can recover the camera intrinsics, the poses of the reference plane, as well as the mirror surface from the observed reflections of the reference plane under at least three unknown distinct poses. We first show that the 3D poses of the reference plane can be estimated from the reflection correspondences established between the images and the reference plane. We then form a bunch of 3D lines from the reflection correspondences, and derive an analytical solution to recover the line projection matrix . We transform the line projection matrix to its equivalent camera projection matrix, and propose a cross-ratio based formulation to optimize the camera projection matrix by minimizing reprojection errors. The mirror surface is then reconstructed based on the optimized cross-ratio constraint. Experimental results on both synthetic and real data are presented, which demonstrate the feasibility and accuracy of our method.},
  archive      = {J_TIP},
  author       = {Kai Han and Miaomiao Liu and Dirk Schnieders and Kwan-Yee K. Wong},
  doi          = {10.1109/TIP.2021.3049946},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2141-2154},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Fixed viewpoint mirror surface reconstruction under an uncalibrated camera},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Exploit camera raw data for video super- resolution via
hidden markov model inference. <em>TIP</em>, <em>30</em>, 2127–2140. (<a
href="https://doi.org/10.1109/TIP.2021.3049974">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To the best of our knowledge, the existing deep-learning-based Video Super-Resolution (VSR) methods exclusively make use of videos produced by the Image Signal Processor (ISP) of the camera system as inputs. Such methods are 1) inherently suboptimal due to information loss incurred by non-invertible operations in ISP, and 2) inconsistent with the real imaging pipeline where VSR in fact serves as a pre-processing unit of ISP. To address this issue, we propose a new VSR method that can directly exploit camera sensor data, accompanied by a carefully built Raw Video Dataset (RawVD) for training, validation, and testing. This method consists of a Successive Deep Inference (SDI) module and a reconstruction module, among others. The SDI module is designed according to the architectural principle suggested by a canonical decomposition result for Hidden Markov Model (HMM) inference; it estimates the target high-resolution frame by repeatedly performing pairwise feature fusion using deformable convolutions. The reconstruction module, built with elaborately designed Attention-based Residual Dense Blocks (ARDBs), serves the purpose of 1) refining the fused feature and 2) learning the color information needed to generate a spatial-specific transformation for accurate color correction. Extensive experiments demonstrate that owing to the informativeness of the camera raw data, the effectiveness of the network architecture, and the separation of super-resolution and color correction processes, the proposed method achieves superior VSR results compared to the state-of-the-art and can be adapted to any specific camera-ISP. Code and dataset are available at https://github.com/proteus1991/RawVSR.},
  archive      = {J_TIP},
  author       = {Xiaohong Liu and Kangdi Shi and Zhe Wang and Jun Chen},
  doi          = {10.1109/TIP.2021.3049974},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2127-2140},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Exploit camera raw data for video super- resolution via hidden markov model inference},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fine-grained crowd counting. <em>TIP</em>, <em>30</em>,
2114–2126. (<a href="https://doi.org/10.1109/TIP.2021.3049938">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current crowd counting algorithms are only concerned about the number of people in an image, which lacks low-level fine-grained information of the crowd. For many practical applications, the total number of people in an image is not as useful as the number of people in each sub-category. For example, knowing the number of people waiting inline or browsing can help retail stores; knowing the number of people standing/sitting can help restaurants/cafeterias; knowing the number of violent/non-violent people can help police in crowd management. In this article, we propose fine-grained crowd counting, which differentiates a crowd into categories based on the low-level behavior attributes of the individuals (e.g. standing/sitting or violent behavior) and then counts the number of people in each category. To enable research in this area, we construct a new dataset of four real-world fine-grained counting tasks: traveling direction on a sidewalk, standing or sitting, waiting in line or not, and exhibiting violent behavior or not. Since the appearance features of different crowd categories are similar, the challenge of fine-grained crowd counting is to effectively utilize contextual information to distinguish between categories. We propose a two branch architecture, consisting of a density map estimation branch and a semantic segmentation branch. We propose two refinement strategies for improving the predictions of the two branches. First, to encode contextual information, we propose feature propagation guided by the density map prediction, which eliminates the effect of background features during propagation. Second, we propose a complementary attention model to share information between the two branches. Experiment results confirm the effectiveness of our method.},
  archive      = {J_TIP},
  author       = {Jia Wan and Nikil Senthil Kumar and Antoni B. Chan},
  doi          = {10.1109/TIP.2021.3049938},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2114-2126},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Fine-grained crowd counting},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). PCG-TAL: Progressive cross-granularity cooperation for
temporal action localization. <em>TIP</em>, <em>30</em>, 2103–2113. (<a
href="https://doi.org/10.1109/TIP.2020.3044218">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There are two major lines of works, i.e., anchor-based and frame-based approaches, in the field of temporal action localization. But each line of works is inherently limited to a certain detection granularity and cannot simultaneously achieve high recall rates with accurate action boundaries. In this work, we propose a progressive cross-granularity cooperation (PCG-TAL) framework to effectively take advantage of complementarity between the anchor-based and frame-based paradigms, as well as between two-view clues (i.e., appearance and motion). Specifically, our new Anchor-Frame Cooperation (AFC) module can effectively integrate both two-granularity and two-stream knowledge at the feature and proposal levels, as well as within each AFC module and across adjacent AFC modules. Specifically, the RGB-stream AFC module and the flow-stream AFC module are stacked sequentially to form a progressive localization framework. The whole framework can be learned in an end-to-end fashion, whilst the temporal action localization performance can be gradually boosted in a progressive manner. Our newly proposed framework outperforms the state-of-the-art methods on three benchmark datasets the THUMOS14, ActivityNet v1.3 and UCF-101-24, which clearly demonstrates the effectiveness of our framework.},
  archive      = {J_TIP},
  author       = {Rui Su and Dong Xu and Lu Sheng and Wanli Ouyang},
  doi          = {10.1109/TIP.2020.3044218},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2103-2113},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {PCG-TAL: Progressive cross-granularity cooperation for temporal action localization},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Saliency prediction on omnidirectional image with generative
adversarial imitation learning. <em>TIP</em>, <em>30</em>, 2087–2102.
(<a href="https://doi.org/10.1109/TIP.2021.3050861">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When watching omnidirectional images (ODIs), subjects can access different viewports by moving their heads. Therefore, it is necessary to predict subjects’ head fixations on ODIs. Inspired by generative adversarial imitation learning (GAIL), this paper proposes a novel approach to predict saliency of head fixations on ODIs, named SalGAIL. First, we establish a dataset for attention on ODIs (AOI). In contrast to traditional datasets, our AOI dataset is large-scale, which contains the head fixations of 30 subjects viewing 600 ODIs. Next, we mine our AOI dataset and discover three findings: (1) the consistency of head fixations are consistent among subjects, and it grows alongside the increased subject number; (2) the head fixations exist with a front center bias (FCB); and (3) the magnitude of head movement is similar across the subjects. According to these findings, our SalGAIL approach applies deep reinforcement learning (DRL) to predict the head fixations of one subject, in which GAIL learns the reward of DRL, rather than the traditional human-designed reward. Then, multi-stream DRL is developed to yield the head fixations of different subjects, and the saliency map of an ODI is generated via convoluting predicted head fixations. Finally, experiments validate the effectiveness of our approach in predicting saliency maps of ODIs, significantly better than 11 state-of-the-art approaches. Our AOI dataset and code of SalGAIL are available online at https://github.com/yanglixiaoshen/SalGAIL .},
  archive      = {J_TIP},
  author       = {Mai Xu and Li Yang and Xiaoming Tao and Yiping Duan and Zulin Wang},
  doi          = {10.1109/TIP.2021.3050861},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2087-2102},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Saliency prediction on omnidirectional image with generative adversarial imitation learning},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sparse gradient regularized deep retinex network for robust
low-light image enhancement. <em>TIP</em>, <em>30</em>, 2072–2086. (<a
href="https://doi.org/10.1109/TIP.2021.3050850">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the absence of a desirable objective for low-light image enhancement, previous data-driven methods may provide undesirable enhanced results including amplified noise, degraded contrast and biased colors. In this work, inspired by Retinex theory, we design an end-to-end signal prior-guided layer separation and data-driven mapping network with layer-specified constraints for single-image low-light enhancement. A Sparse Gradient Minimization sub-Network (SGM-Net) is constructed to remove the low-amplitude structures and preserve major edge information, which facilitates extracting paired illumination maps of low/normal-light images. After the learned decomposition, two sub-networks (Enhance-Net and Restore-Net) are utilized to predict the enhanced illumination and reflectance maps, respectively, which helps stretch the contrast of the illumination map and remove intensive noise in the reflectance map. The effects of all these configured constraints, including the signal structure regularization and losses, combine together reciprocally, which leads to good reconstruction results in overall visual quality. The evaluation on both synthetic and real images, particularly on those containing intensive noise, compression artifacts and their interleaved artifacts, shows the effectiveness of our novel models, which significantly outperforms the state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Wenhan Yang and Wenjing Wang and Haofeng Huang and Shiqi Wang and Jiaying Liu},
  doi          = {10.1109/TIP.2021.3050850},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2072-2086},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Sparse gradient regularized deep retinex network for robust low-light image enhancement},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An end-to-end foreground-aware network for person
re-identification. <em>TIP</em>, <em>30</em>, 2060–2071. (<a
href="https://doi.org/10.1109/TIP.2021.3050839">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person re-identification is a crucial task of identifying pedestrians of interest across multiple surveillance camera views. For person re-identification, a pedestrian is usually represented with features extracted from a rectangular image region that inevitably contains the scene background, which incurs ambiguity to distinguish different pedestrians and degrades the accuracy. Thus, we propose an end-to-end foreground-aware network to discriminate against the foreground from the background by learning a soft mask for person re-identification. In our method, in addition to the pedestrian ID as supervision for the foreground, we introduce the camera ID of each pedestrian image for background modeling. The foreground branch and the background branch are optimized collaboratively. By presenting a target attention loss, the pedestrian features extracted from the foreground branch become more insensitive to backgrounds, which greatly reduces the negative impact of changing backgrounds on pedestrian matching across different camera views. Notably, in contrast to existing methods, our approach does not require an additional dataset to train a human landmark detector or a segmentation model for locating the background regions. The experimental results conducted on three challenging datasets, i.e. , Market-1501, DukeMTMC-reID, and MSMT17, demonstrate the effectiveness of our approach.},
  archive      = {J_TIP},
  author       = {Yiheng Liu and Wengang Zhou and Jianzhuang Liu and Guo-Jun Qi and Qi Tian and Houqiang Li},
  doi          = {10.1109/TIP.2021.3050839},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2060-2071},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {An end-to-end foreground-aware network for person re-identification},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Panoptic feature fusion net: A novel instance segmentation
paradigm for biomedical and biological images. <em>TIP</em>,
<em>30</em>, 2045–2059. (<a
href="https://doi.org/10.1109/TIP.2021.3050668">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Instance segmentation is an important task for biomedical and biological image analysis. Due to the complicated background components, the high variability of object appearances, numerous overlapping objects, and ambiguous object boundaries, this task still remains challenging. Recently, deep learning based methods have been widely employed to solve these problems and can be categorized into proposal-free and proposal-based methods. However, both proposal-free and proposal-based methods suffer from information loss, as they focus on either global-level semantic or local-level instance features. To tackle this issue, we present a Panoptic Feature Fusion Net (PFFNet) that unifies the semantic and instance features in this work. Specifically, our proposed PFFNet contains a residual attention feature fusion mechanism to incorporate the instance prediction with the semantic features, in order to facilitate the semantic contextual information learning in the instance branch. Then, a mask quality sub-branch is designed to align the confidence score of each object with the quality of the mask prediction. Furthermore, a consistency regularization mechanism is designed between the semantic segmentation tasks in the semantic and instance branches, for the robust learning of both tasks. Extensive experiments demonstrate the effectiveness of our proposed PFFNet, which outperforms several state-of-the-art methods on various biomedical and biological datasets.},
  archive      = {J_TIP},
  author       = {Dongnan Liu and Donghao Zhang and Yang Song and Heng Huang and Weidong Cai},
  doi          = {10.1109/TIP.2021.3050668},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2045-2059},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Panoptic feature fusion net: A novel instance segmentation paradigm for biomedical and biological images},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Online rain/snow removal from surveillance videos.
<em>TIP</em>, <em>30</em>, 2029–2044. (<a
href="https://doi.org/10.1109/TIP.2021.3050313">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video rain/snow removal from surveillance videos is an important task in the computer vision community since rain/snow existed in videos can severely degenerate the performance of many surveillance system. Various methods have been investigated extensively, but most only consider consistent rain/snow under stable background scenes. Rain/snow captured from practical surveillance camera, however, is always highly dynamic in time, and those videos also include occasionally transformed background scenes and background motions caused by waving leaves or water surfaces. To this issue, this paper proposes a novel rain/snow removal approach, which fully considers dynamic statistics of both rain/snow and background scenes taken from a video sequence. Specifically, the rain/snow is encoded as an online multi-scale convolutional sparse coding (OMS-CSC) model, which not only finely delivers the sparse scattering and multi-scale shapes of real rain/snow, but also well distinguish the components of background motion from rain/snow layer. The real-time ameliorated parameters in the model well encodes their temporally dynamic configurations. Furthermore, a transformation operator imposed on the background scenes is further embedded into the proposed model, which finely conveys the background transformations, such as rotations, scalings and distortions, inevitably existed in a real video sequence. The approach so constructed can naturally better adapt to the dynamic rain/snow as well as background changes, and also suitable to deal with the streaming video attributed its online learning mode. The proposed model is formulated in a concise maximum a posterior (MAP) framework and is readily solved by the alternating direction method of multipliers (ADMM). Compared with the state-of-the-art online and offline video rain/snow removal methods, the proposed method achieves best performance on synthetic and real videos datasets both visually and quantitatively. Specifically, our method can be implemented in relatively high efficiency, showing its potential to real-time video rain/snow removal. The code page is at: https://github.com/MinghanLi/OTMSCSC_matlab_2020 .},
  archive      = {J_TIP},
  author       = {Minghan Li and Xiangyong Cao and Qian Zhao and Lei Zhang and Deyu Meng},
  doi          = {10.1109/TIP.2021.3050313},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2029-2044},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Online Rain/Snow removal from surveillance videos},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Adaptively learning facial expression representation via
c-f labels and distillation. <em>TIP</em>, <em>30</em>, 2016–2028. (<a
href="https://doi.org/10.1109/TIP.2021.3049955">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial expression recognition is of significant importance in criminal investigation and digital entertainment. Under unconstrained conditions, existing expression datasets are highly class-imbalanced, and the similarity between expressions is high. Previous methods tend to improve the performance of facial expression recognition through deeper or wider network structures, resulting in increased storage and computing costs. In this paper, we propose a new adaptive supervised objective named AdaReg loss, re-weighting category importance coefficients to address this class imbalance and increasing the discrimination power of expression representations. Inspired by human beings’ cognitive mode, an innovative coarse-fine (C-F) labels strategy is designed to guide the model from easy to difficult to classify highly similar representations. On this basis, we propose a novel training framework named the emotional education mechanism (EEM) to transfer knowledge, composed of a knowledgeable teacher network (KTN) and a self-taught student network (STSN). Specifically, KTN integrates the outputs of coarse and fine streams, learning expression representations from easy to difficult. Under the supervision of the pre-trained KTN and existing learning experience, STSN can maximize the potential performance and compress the original KTN. Extensive experiments on public benchmarks demonstrate that the proposed method achieves superior performance compared to current state-of-the-art frameworks with 88.07\% on RAF-DB, 63.97\% on AffectNet and 90.49\% on FERPlus.},
  archive      = {J_TIP},
  author       = {Hangyu Li and Nannan Wang and Xinpeng Ding and Xi Yang and Xinbo Gao},
  doi          = {10.1109/TIP.2021.3049955},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2016-2028},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Adaptively learning facial expression representation via C-F labels and distillation},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Plant disease recognition: A large-scale benchmark dataset
and a visual region and loss reweighting approach. <em>TIP</em>,
<em>30</em>, 2003–2015. (<a
href="https://doi.org/10.1109/TIP.2021.3049334">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Plant disease diagnosis is very critical for agriculture due to its importance for increasing crop production. Recent advances in image processing offer us a new way to solve this issue via visual plant disease analysis. However, there are few works in this area, not to mention systematic researches. In this paper, we systematically investigate the problem of visual plant disease recognition for plant disease diagnosis. Compared with other types of images, plant disease images generally exhibit randomly distributed lesions, diverse symptoms and complex backgrounds, and thus are hard to capture discriminative information. To facilitate the plant disease recognition research, we construct a new large-scale plant disease dataset with 271 plant disease categories and 220,592 images. Based on this dataset, we tackle plant disease recognition via reweighting both visual regions and loss to emphasize diseased parts. We first compute the weights of all the divided patches from each image based on the cluster distribution of these patches to indicate the discriminative level of each patch. Then we allocate the weight to each loss for each patch-label pair during weakly-supervised training to enable discriminative disease part learning. We finally extract patch features from the network trained with loss reweighting, and utilize the LSTM network to encode the weighed patch feature sequence into a comprehensive feature representation. Extensive evaluations on this dataset and another public dataset demonstrate the advantage of the proposed method. We expect this research will further the agenda of plant disease recognition in the community of image processing.},
  archive      = {J_TIP},
  author       = {Xinda Liu and Weiqing Min and Shuhuan Mei and Lili Wang and Shuqiang Jiang},
  doi          = {10.1109/TIP.2021.3049334},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2003-2015},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Plant disease recognition: A large-scale benchmark dataset and a visual region and loss reweighting approach},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust online tracking via contrastive spatio-temporal aware
network. <em>TIP</em>, <em>30</em>, 1989–2002. (<a
href="https://doi.org/10.1109/TIP.2021.3050314">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing tracking-by-detection approaches using deep features have achieved promising results in recent years. However, these methods mainly exploit feature representations learned from individual static frames, thus paying little attention to the temporal smoothness between frames. This easily leads trackers to drift in the presence of large appearance variations and occlusions. To address this issue, we propose a two-stream network to learn discriminative spatio-temporal feature representations to represent the target objects. The proposed network consists of a Spatial ConvNet module and a Temporal ConvNet module. Specifically, the Spatial ConvNet adopts 2D convolutions to encode the target-specific appearance in static frames, while the Temporal ConvNet models the temporal appearance variations using 3D convolutions and learns consistent temporal patterns in a short video clip. Then we propose a proposal refinement module to adjust the predicted bounding box, which can make the target localizing outputs to be more consistent in video sequences. In addition, to improve the model adaptation during online update, we propose a contrastive online hard example mining (OHEM) strategy, which selects hard negative samples and enforces them to be embedded in a more discriminative feature space. Extensive experiments conducted on the OTB, Temple Color and VOT benchmarks demonstrate that the proposed algorithm performs favorably against the state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Siyuan Yao and Hua Zhang and Wenqi Ren and Chao Ma and Xiaoguang Han and Xiaochun Cao},
  doi          = {10.1109/TIP.2021.3050314},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1989-2002},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Robust online tracking via contrastive spatio-temporal aware network},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adversarial attack against deep saliency models powered by
non-redundant priors. <em>TIP</em>, <em>30</em>, 1973–1988. (<a
href="https://doi.org/10.1109/TIP.2021.3050303">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Saliency detection is an effective front-end process to many security-related tasks, e.g. automatic drive and tracking. Adversarial attack serves as an efficient surrogate to evaluate the robustness of deep saliency models before they are deployed in real world. However, most of current adversarial attacks exploit the gradients spanning the entire image space to craft adversarial examples, ignoring the fact that natural images are high-dimensional and spatially over-redundant, thus causing expensive attack cost and poor perceptibility. To circumvent these issues, this paper builds an efficient bridge between the accessible partially-white-box source models and the unknown black-box target models. The proposed method includes two steps: 1) We design a new partially-white-box attack, which defines the cost function in the compact hidden space to punish a fraction of feature activations corresponding to the salient regions, instead of punishing every pixel spanning the entire dense output space. This partially-white-box attack reduces the redundancy of the adversarial perturbation. 2) We exploit the non-redundant perturbations from some source models as the prior cues, and use an iterative zeroth-order optimizer to compute the directional derivatives along the non-redundant prior directions, in order to estimate the actual gradient of the black-box target model. The non-redundant priors boost the update of some “critical” pixels locating at non-zero coordinates of the prior cues, while keeping other redundant pixels locating at the zero coordinates unaffected. Our method achieves the best tradeoff between attack ability and perturbation redundancy. Finally, we conduct a comprehensive experiment to test the robustness of 18 state-of-the-art deep saliency models against 16 malicious attacks, under both of white-box and black-box settings, which contributes a new robustness benchmark to the saliency community for the first time.},
  archive      = {J_TIP},
  author       = {Zhaohui Che and Ali Borji and Guangtao Zhai and Suiyi Ling and Jing Li and Yuan Tian and Guodong Guo and Patrick Le Callet},
  doi          = {10.1109/TIP.2021.3050303},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1973-1988},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Adversarial attack against deep saliency models powered by non-redundant priors},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Digital image noise estimation using DWT coefficients.
<em>TIP</em>, <em>30</em>, 1962–1972. (<a
href="https://doi.org/10.1109/TIP.2021.3049961">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Noise type and strength estimation are important in many image processing applications like denoising, compression, video tracking, etc. There are many existing methods for estimation of the type of noise and its strength in digital images. These methods mostly rely on the transform or spatial domain information of images. We propose a hybrid Discrete Wavelet Transform (DWT) and edge information removal based algorithm to estimate the strength of Gaussian noise in digital images. The wavelet coefficients corresponding to spatial domain edges are excluded from noise estimate calculation using a Sobel edge detector. The accuracy of the proposed algorithm is further increased using polynomial regression. Parseval’s theorem mathematically validates the proposed algorithm. The performance of the proposed algorithm is evaluated on a standard LIVE image dataset. Benchmarking results show that the proposed algorithm outperforms all other state of the art algorithms by a large margin over a wide range of noise.},
  archive      = {J_TIP},
  author       = {Varad A. Pimpalkhute and Rutvik Page and Ashwin Kothari and Kishor M. Bhurchandi and Vipin Milind Kamble},
  doi          = {10.1109/TIP.2021.3049961},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1962-1972},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Digital image noise estimation using DWT coefficients},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bilateral attention network for RGB-d salient object
detection. <em>TIP</em>, <em>30</em>, 1949–1961. (<a
href="https://doi.org/10.1109/TIP.2021.3049959">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {RGB-D salient object detection (SOD) aims to segment the most attractive objects in a pair of cross-modal RGB and depth images. Currently, most existing RGB-D SOD methods focus on the foreground region when utilizing the depth images. However, the background also provides important information in traditional SOD methods for promising performance. To better explore salient information in both foreground and background regions, this paper proposes a Bilateral Attention Network (BiANet) for the RGB-D SOD task. Specifically, we introduce a Bilateral Attention Module (BAM) with a complementary attention mechanism: foreground-first (FF) attention and background-first (BF) attention. The FF attention focuses on the foreground region with a gradual refinement style, while the BF one recovers potentially useful salient information in the background region. Benefited from the proposed BAM module, our BiANet can capture more meaningful foreground and background cues, and shift more attention to refining the uncertain details between foreground and background regions. Additionally, we extend our BAM by leveraging the multi-scale techniques for better SOD performance. Extensive experiments on six benchmark datasets demonstrate that our BiANet outperforms other state-of-the-art RGB-D SOD methods in terms of objective metrics and subjective visual comparison. Our BiANet can run up to 80 fps on 224×224 RGB-D images, with an NVIDIA GeForce RTX 2080Ti GPU. Comprehensive ablation studies also validate our contributions.},
  archive      = {J_TIP},
  author       = {Zhao Zhang and Zheng Lin and Jun Xu and Wen-Da Jin and Shao-Ping Lu and Deng-Ping Fan},
  doi          = {10.1109/TIP.2021.3049959},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1949-1961},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Bilateral attention network for RGB-D salient object detection},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bidirectional interaction network for person
re-identification. <em>TIP</em>, <em>30</em>, 1935–1948. (<a
href="https://doi.org/10.1109/TIP.2021.3049943">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person re-identification (ReID) task aims to retrieve the same person across multiple spatially disjoint camera views. Due to huge image changes caused by various factors such as posture variation and illumination transformation, images of different persons may share the more similar appearances than images of the same one. Learning discriminative representations to distinguish details of different persons is significant for person ReID. Many existing methods learn discriminative representations resorting to a human body part location branch which requires cumbersome expert human annotations or complex network designs. In this article, a novel bidirectional interaction network is proposed to explore discriminative representations for person ReID without any human body part detection. The proposed method regards multiple convolutional features as responses to various body part properties and exploits the inter-layer interaction to mine discriminative representations for person identities. Firstly, an inter-layer bilinear pooling strategy is proposed to feasibly exploit the pairwise feature relations between two convolution layers. Secondly, to explore interaction of multiple layers, an effective bidirectional integration strategy consisting of two different multi-layer interaction processes is designed to aggregate bilinear pooling interaction of multiple convolution layers. The interaction of multiple layers is implemented in a layer-by-layer nesting policy to ensure the two interaction processes are different and complementary. Extensive experiments validate the superiority of the proposed method on four popular person ReID datasets including Market-1501, DukeMTMC-ReID, CUHK03-NP and MSMT17. Specifically, the proposed method achieves a rank-1 accuracy of 95.1\% and 88.2\% on Market-1501 and DukeMTMC-ReID, respectively.},
  archive      = {J_TIP},
  author       = {Xiumei Chen and Xiangtao Zheng and Xiaoqiang Lu},
  doi          = {10.1109/TIP.2021.3049943},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1935-1948},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Bidirectional interaction network for person re-identification},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Revisiting shadow detection: A new benchmark dataset for
complex world. <em>TIP</em>, <em>30</em>, 1925–1934. (<a
href="https://doi.org/10.1109/TIP.2021.3049331">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Shadow detection in general photos is a nontrivial problem, due to the complexity of the real world. Though recent shadow detectors have already achieved remarkable performance on various benchmark data, their performance is still limited for general real-world situations. In this work, we collected shadow images for multiple scenarios and compiled a new dataset of 10,500 shadow images, each with labeled ground-truth mask, for supporting shadow detection in the complex world. Our dataset covers a rich variety of scene categories, with diverse shadow sizes, locations, contrasts, and types. Further, we comprehensively analyze the complexity of the dataset, present a fast shadow detection network with a detail enhancement module to harvest shadow details, and demonstrate the effectiveness of our method to detect shadows in general situations.},
  archive      = {J_TIP},
  author       = {Xiaowei Hu and Tianyu Wang and Chi-Wing Fu and Yitong Jiang and Qiong Wang and Pheng-Ann Heng},
  doi          = {10.1109/TIP.2021.3049331},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1925-1934},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Revisiting shadow detection: A new benchmark dataset for complex world},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Uncertainty class activation map (u-CAM) using gradient
certainty method. <em>TIP</em>, <em>30</em>, 1910–1924. (<a
href="https://doi.org/10.1109/TIP.2020.3046916">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding and explaining deep learning models is an imperative task. Towards this, we propose a method that obtains gradient-based certainty estimates that also provide visual attention maps. Particularly, we solve for visual question answering task. We incorporate modern probabilistic deep learning methods that we further improve by using the gradients for these estimates. These have two-fold benefits: a) improvement in obtaining the certainty estimates that correlate better with misclassified samples and b) improved attention maps that provide state-of-the-art results in terms of correlation with human attention regions. The improved attention maps result in consistent improvement for various methods for visual question answering. Therefore, the proposed technique can be thought of as a tool for obtaining improved certainty estimates and explanations for deep learning models. We provide detailed empirical analysis for the visual question answering task on all standard benchmarks and comparison with state of the art methods.},
  archive      = {J_TIP},
  author       = {Badri Narayana Patro and Mayank Lunayach and Vinay P. Namboodiri},
  doi          = {10.1109/TIP.2020.3046916},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1910-1924},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Uncertainty class activation map (U-CAM) using gradient certainty method},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Pose flow learning from person images for pose guided
synthesis. <em>TIP</em>, <em>30</em>, 1898–1909. (<a
href="https://doi.org/10.1109/TIP.2020.3031108">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pose guided synthesis aims to generate a new image in an arbitrary target pose while preserving the appearance details from the source image. Existing approaches rely on either hard-coded spatial transformations or 3D body modeling. They often overlook complex non-rigid pose deformation or unmatched occluded regions, thus fail to effectively preserve appearance information. In this article, we propose a pose flow learning scheme that learns to transfer the appearance details from the source image without resorting to annotated correspondences. Based on such learned pose flow, we proposed GarmentNet and SynthesisNet, both of which use multi-scale feature-domain alignment for coarse-to-fine synthesis. Experiments on the DeepFashion, MVC dataset and additional real-world datasets demonstrate that our approach compares favorably with the state-of-the-art methods and generalizes to unseen poses and clothing styles.},
  archive      = {J_TIP},
  author       = {Haitian Zheng and Lele Chen and Chenliang Xu and Jiebo Luo},
  doi          = {10.1109/TIP.2020.3031108},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1898-1909},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Pose flow learning from person images for pose guided synthesis},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On data augmentation for GAN training. <em>TIP</em>,
<em>30</em>, 1882–1897. (<a
href="https://doi.org/10.1109/TIP.2021.3049346">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent successes in Generative Adversarial Networks (GAN) have affirmed the importance of using more data in GAN training. Yet it is expensive to collect data in many domains such as medical applications. Data Augmentation (DA) has been applied in these applications. In this work, we first argue that the classical DA approach could mislead the generator to learn the distribution of the augmented data, which could be different from that of the original data. We then propose a principled framework, termed Data Augmentation Optimized for GAN (DAG), to enable the use of augmented data in GAN training to improve the learning of the original distribution. We provide theoretical analysis to show that using our proposed DAG aligns with the original GAN in minimizing the Jensen–Shannon (JS) divergence between the original distribution and model distribution. Importantly, the proposed DAG effectively leverages the augmented data to improve the learning of discriminator and generator. We conduct experiments to apply DAG to different GAN models: unconditional GAN, conditional GAN, self-supervised GAN and CycleGAN using datasets of natural images and medical images. The results show that DAG achieves consistent and considerable improvements across these models. Furthermore, when DAG is used in some GAN models, the system establishes state-of-the-art Fréchet Inception Distance (FID) scores. Our code is available ( https://github.com/tntrung/dag-gans ).},
  archive      = {J_TIP},
  author       = {Ngoc-Trung Tran and Viet-Hung Tran and Ngoc-Bao Nguyen and Trung-Kien Nguyen and Ngai-Man Cheung},
  doi          = {10.1109/TIP.2021.3049346},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1882-1897},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {On data augmentation for GAN training},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Is context-aware CNN ready for the surroundings? Panoramic
semantic segmentation in the wild. <em>TIP</em>, <em>30</em>, 1866–1881.
(<a href="https://doi.org/10.1109/TIP.2020.3048682">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic segmentation, unifying most navigational perception tasks at the pixel level has catalyzed striking progress in the field of autonomous transportation. Modern Convolution Neural Networks (CNNs) are able to perform semantic segmentation both efficiently and accurately, particularly owing to their exploitation of wide context information. However, most segmentation CNNs are benchmarked against pinhole images with limited Field of View (FoV). Despite the growing popularity of panoramic cameras to sense the surroundings, semantic segmenters have not been comprehensively evaluated on omnidirectional wide-FoV data, which features rich and distinct contextual information. In this paper, we propose a concurrent horizontal and vertical attention module to leverage width-wise and height-wise contextual priors markedly available in the panoramas. To yield semantic segmenters suitable for wide-FoV images, we present a multi-source omni-supervised learning scheme with panoramic domain covered in the training via data distillation. To facilitate the evaluation of contemporary CNNs in panoramic imagery, we put forward the Wild PAnoramic Semantic Segmentation (WildPASS) dataset, comprising images from all around the globe, as well as adverse and unconstrained scenes, which further reflects perception challenges of navigation applications in the real world. A comprehensive variety of experiments demonstrates that the proposed methods enable our high-efficiency architecture to attain significant accuracy gains, outperforming the state of the art in panoramic imagery domains.},
  archive      = {J_TIP},
  author       = {Kailun Yang and Xinxin Hu and Rainer Stiefelhagen},
  doi          = {10.1109/TIP.2020.3048682},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1866-1881},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Is context-aware CNN ready for the surroundings? panoramic semantic segmentation in the wild},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Shadow removal by a lightness-guided network with training
on unpaired data. <em>TIP</em>, <em>30</em>, 1853–1865. (<a
href="https://doi.org/10.1109/TIP.2020.3048677">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Shadow removal can significantly improve the image visual quality and has many applications in computer vision. Deep learning methods based on CNNs have become the most effective approach for shadow removal by training on either paired data, where both the shadow and underlying shadow-free versions of an image are known, or unpaired data, where shadow and shadow-free training images are totally different with no correspondence. In practice, CNN training on unpaired data is more preferred given the easiness of training data collection. In this paper, we present a new Lightness-Guided Shadow Removal Network (LG-ShadowNet) for shadow removal by training on unpaired data. In this method, we first train a CNN module to compensate for the lightness and then train a second CNN module with the guidance of lightness information from the first CNN module for final shadow removal. We also introduce a loss function to further utilise the colour prior of existing data. Extensive experiments on widely used ISTD, adjusted ISTD and USR datasets demonstrate that the proposed method outperforms the state-of-the-art methods with training on unpaired data.},
  archive      = {J_TIP},
  author       = {Zhihao Liu and Hui Yin and Yang Mi and Mengyang Pu and Song Wang},
  doi          = {10.1109/TIP.2020.3048677},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1853-1865},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Shadow removal by a lightness-guided network with training on unpaired data},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning aligned image-text representations using graph
attentive relational network. <em>TIP</em>, <em>30</em>, 1840–1852. (<a
href="https://doi.org/10.1109/TIP.2020.3048627">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image-text matching aims to measure the similarities between images and textual descriptions, which has made great progress recently. The key to this cross-modal matching task is to build the latent semantic alignment between visual objects and words. Due to the widespread variations of sentence structures, it is very difficult to learn the latent semantic alignment using only global cross-modal features. Many previous methods attempt to learn the aligned image-text representations by the attention mechanism but generally ignore the relationships within textual descriptions which determine whether the words belong to the same visual object. In this paper, we propose a graph attentive relational network (GARN) to learn the aligned image-text representations by modeling the relationships between noun phrases in a text for the identity-aware image-text matching. In the GARN, we first decompose images and texts into regions and noun phrases, respectively. Then a skip graph neural network (skip-GNN) is proposed to learn effective textual representations which are a mixture of textual features and relational features. Finally, a graph attention network is further proposed to obtain the probabilities that the noun phrases belong to the image regions by modeling the relationships between noun phrases. We perform extensive experiments on the CUHK Person Description dataset (CUHK-PEDES), Caltech-UCSD Birds dataset (CUB), Oxford-102 Flowers dataset and Flickr30K dataset to verify the effectiveness of each component in our model. Experimental results show that our approach achieves the state-of-the-art results on these four benchmark datasets.},
  archive      = {J_TIP},
  author       = {Ya Jing and Wei Wang and Liang Wang and Tieniu Tan},
  doi          = {10.1109/TIP.2020.3048627},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1840-1852},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Learning aligned image-text representations using graph attentive relational network},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Convex and compact superpixels by edge- constrained
centroidal power diagram. <em>TIP</em>, <em>30</em>, 1825–1839. (<a
href="https://doi.org/10.1109/TIP.2020.3045640">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Superpixel segmentation, as a central image processing task, has many applications in computer vision and computer graphics. Boundary alignment and shape compactness are leading indicators to evaluate a superpixel segmentation algorithm. Furthermore, convexity can make superpixels reflect more geometric structures in images and provide a more concise over-segmentation result. In this paper, we consider generating convex and compact superpixels while satisfying the constraints of adhering to the boundary as far as possible. We formulate the new superpixel segmentation into an edge-constrained centroidal power diagram (ECCPD) optimization problem. In the implementation, we optimize the superpixel configurations by repeatedly performing two alternative operations, which include site location updating and weight updating through a weight function defined by image features. Compared with existing superpixel methods, our method can partition an image into fully convex and compact superpixels with better boundary adherence. Extensive experimental results show that our approach outperforms existing superpixel segmentation methods in boundary alignment and compactness for generating convex superpixels.},
  archive      = {J_TIP},
  author       = {Dongyang Ma and Yuanfeng Zhou and Shiqing Xin and Wenping Wang},
  doi          = {10.1109/TIP.2020.3045640},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1825-1839},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Convex and compact superpixels by edge- constrained centroidal power diagram},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Exploring the effects of blur and deblurring to visual
object tracking. <em>TIP</em>, <em>30</em>, 1812–1824. (<a
href="https://doi.org/10.1109/TIP.2020.3045630">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The existence of motion blur can inevitably influence the performance of visual object tracking. However, in contrast to the rapid development of visual trackers, the quantitative effects of increasing levels of motion blur on the performance of visual trackers still remain unstudied. Meanwhile, although image-deblurring can produce visually sharp videos for pleasant visual perception, it is also unknown whether visual object tracking can benefit from image deblurring or not. In this paper, we present a Blurred Video Tracking (BVT) benchmark to address these two problems, which contains a large variety of videos with different levels of motion blurs, as well as ground-truth tracking results. To explore the effects of blur and deblurring to visual object tracking, we extensively evaluate 25 trackers on the proposed BVT benchmark and obtain several new interesting findings. Specifically, we find that light motion blur may improve the accuracy of many trackers, but heavy blur usually hurts the tracking performance. We also observe that image deblurring is helpful to improve tracking accuracy on heavily-blurred videos but hurts the performance of lightly-blurred videos. According to these observations, we then propose a new general GAN-based scheme to improve a tracker&#39;s robustness to motion blur. In this scheme, a fine-tuned discriminator can effectively serve as an adaptive blur assessor to enable selective frames deblurring during the tracking process. We use this scheme to successfully improve the accuracy of 6 state-of-the-art trackers on motion-blurred videos.},
  archive      = {J_TIP},
  author       = {Qing Guo and Wei Feng and Ruijun Gao and Yang Liu and Song Wang},
  doi          = {10.1109/TIP.2020.3045630},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1812-1824},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Exploring the effects of blur and deblurring to visual object tracking},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep outlier handling for image deblurring. <em>TIP</em>,
<em>30</em>, 1799–1811. (<a
href="https://doi.org/10.1109/TIP.2020.3048679">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Outlier handling has attracted considerable attention recently but remains challenging for image deblurring. Existing approaches mainly depend on iterative outlier detection steps to explicitly or implicitly reduce the influence of outliers on image deblurring. However, these outlier detection steps usually involve heuristic operations and iterative optimization processes, which are complex and time-consuming. In contrast, we propose to learn a deep convolutional neural network to directly estimate the confidence map, which can identify reliable inliers and outliers from the blurred image and thus facilitates the following deblurring process. We analyze that the proposed algorithm incorporated with the learned confidence map is effective in handling outliers and does not require ad-hoc outlier detection steps which are critical to existing outlier handling methods. Compared to existing approaches, the proposed algorithm is more efficient and can be applied to both non-blind and blind image deblurring. Extensive experimental results demonstrate that the proposed algorithm performs favorably against state-of-the-art methods in terms of accuracy and efficiency.},
  archive      = {J_TIP},
  author       = {Jiangxin Dong and Jinshan Pan},
  doi          = {10.1109/TIP.2020.3048679},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1799-1811},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Deep outlier handling for image deblurring},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021c). Dynamic selection network for image inpainting.
<em>TIP</em>, <em>30</em>, 1784–1798. (<a
href="https://doi.org/10.1109/TIP.2020.3048629">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image inpainting is a challenging computer vision task that aims to fill in missing regions of corrupted images with realistic contents. With the development of convolutional neural networks, many deep learning models have been proposed to solve image inpainting issues by learning information from a large amount of data. In particular, existing algorithms usually follow an encoding and decoding network architecture in which some operations with standard schemes are employed, such as static convolution, which only considers pixels with fixed grids, and the monotonous normalization style (e.g., batch normalization). However, these techniques are not well-suited for the image inpainting task because the random corrupted regions in the input images tend to mislead the inpainting process and generate unreasonable content. In this paper, we propose a novel dynamic selection network (DSNet) to solve this problem in image inpainting tasks. The principal idea of the proposed DSNet is to distinguish the corrupted region from the valid ones throughout the entire network architecture, which may help make full use of the information in the known area. Specifically, the proposed DSNet has two novel dynamic selection modules, namely, the validness migratable convolution (VMC) and regional composite normalization (RCN) modules, which share a dynamic selection mechanism that helps utilize valid pixels better. By replacing vanilla convolution with the VMC module, spatial sampling locations are dynamically selected in the convolution phase, resulting in a more flexible feature extraction process. Besides, the RCN module not only combines several normalization methods but also normalizes the feature regions selectively. Therefore, the proposed DSNet can illustrate realistic and fine-detailed images by adaptively selecting features and normalization styles. Experimental results on three public datasets show that our proposed method outperforms state-of-the-art methods both quantitatively and qualitatively.},
  archive      = {J_TIP},
  author       = {Ning Wang and Yipeng Zhang and Lefei Zhang},
  doi          = {10.1109/TIP.2020.3048629},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1784-1798},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Dynamic selection network for image inpainting},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Generative partial multi-view clustering with adaptive
fusion and cycle consistency. <em>TIP</em>, <em>30</em>, 1771–1783. (<a
href="https://doi.org/10.1109/TIP.2020.3048626">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, with the rapid development of data collection sources and feature extraction methods, multi-view data are getting easy to obtain and have received increasing research attention in recent years, among which, multi-view clustering (MVC) forms a mainstream research direction and is widely used in data analysis. However, existing MVC methods mainly assume that each sample appears in all the views, without considering the incomplete view case due to data corruption, sensor failure, equipment malfunction, etc. In this study, we design and build a generative partial multi-view clustering model with adaptive fusion and cycle consistency, named as GP-MVC, to solve the incomplete multi-view problem by explicitly generating the data of missing views. The main idea of GP-MVC lies in two-fold. First, multi-view encoder networks are trained to learn common low-dimensional representations, followed by a clustering layer to capture the shared cluster structure across multiple views. Second, view-specific generative adversarial networks with multi-view cycle consistency are developed to generate the missing data of one view conditioning on the shared representation given by other views. These two steps could be promoted mutually, where the learned common representation facilitates data imputation and the generated data could further explores the view consistency. Moreover, an weighted adaptive fusion scheme is implemented to exploit the complementary information among different views. Experimental results on four benchmark datasets are provided to show the effectiveness of the proposed GP-MVC over the state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Qianqian Wang and Zhengming Ding and Zhiqiang Tao and Quanxue Gao and Yun Fu},
  doi          = {10.1109/TIP.2020.3048626},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1771-1783},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Generative partial multi-view clustering with adaptive fusion and cycle consistency},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Single-image real-time rain removal based on depth-guided
non-local features. <em>TIP</em>, <em>30</em>, 1759–1770. (<a
href="https://doi.org/10.1109/TIP.2020.3048625">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rain is a common weather phenomenon that affects environmental monitoring and surveillance systems. According to an established rain model (Garg and Nayar, 2007), the scene visibility in the rain varies with the depth from the camera, where objects faraway are visually blocked more by the fog than by the rain streaks. However, existing datasets and methods for rain removal ignore these physical properties, thus limiting the rain removal efficiency on real photos. In this work, we analyze the visual effects of rain subject to scene depth and formulate a rain imaging model that collectively considers rain streaks and fog. Also, we prepare a dataset called RainCityscapes on real outdoor photos. Furthermore, we design a novel real-time end-to-end deep neural network, for which we train to learn the depth-guided non-local features and to regress a residual map to produce a rain-free output image. We performed various experiments to visually and quantitatively compare our method with several state-of-the-art methods to show its superiority over others.},
  archive      = {J_TIP},
  author       = {Xiaowei Hu and Lei Zhu and Tianyu Wang and Chi-Wing Fu and Pheng-Ann Heng},
  doi          = {10.1109/TIP.2020.3048625},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1759-1770},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Single-image real-time rain removal based on depth-guided non-local features},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fine-grained 3D shape classification with hierarchical
part-view attention. <em>TIP</em>, <em>30</em>, 1744–1758. (<a
href="https://doi.org/10.1109/TIP.2020.3048623">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fine-grained 3D shape classification is important for shape understanding and analysis, which poses a challenging research problem. However, the studies on the fine-grained 3D shape classification have rarely been explored, due to the lack of fine-grained 3D shape benchmarks. To address this issue, we first introduce a new 3D shape dataset (named FG3D dataset) with fine-grained class labels, which consists of three categories including airplane, car and chair. Each category consists of several subcategories at a fine-grained level. According to our experiments under this fine-grained dataset, we find that state-of-the-art methods are significantly limited by the small variance among subcategories in the same category. To resolve this problem, we further propose a novel fine-grained 3D shape classification method named FG3D-Net to capture the fine-grained local details of 3D shapes from multiple rendered views. Specifically, we first train a Region Proposal Network (RPN) to detect the generally semantic parts inside multiple views under the benchmark of generally semantic part detection. Then, we design a hierarchical part-view attention aggregation module to learn a global shape representation by aggregating generally semantic part features, which preserves the local details of 3D shapes. The part-view attention module hierarchically leverages part-level and view-level attention to increase the discriminability of our features. The part-level attention highlights the important parts in each view while the view-level attention highlights the discriminative views among all the views of the same object. In addition, we integrate a Recurrent Neural Network (RNN) to capture the spatial relationships among sequential views from different viewpoints. Our results under the fine-grained 3D shape dataset show that our method outperforms other state-of-the-art methods. The FG3D dataset is available at https://github.com/liuxinhai/FG3D-Net.},
  archive      = {J_TIP},
  author       = {Xinhai Liu and Zhizhong Han and Yu-Shen Liu and Matthias Zwicker},
  doi          = {10.1109/TIP.2020.3048623},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1744-1758},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Fine-grained 3D shape classification with hierarchical part-view attention},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Face hallucination with finishing touches. <em>TIP</em>,
<em>30</em>, 1728–1743. (<a
href="https://doi.org/10.1109/TIP.2020.3046918">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Obtaining a high-quality frontal face image from a low-resolution (LR) non-frontal face image is primarily important for many facial analysis applications. However, mainstreams either focus on super-resolving near-frontal LR faces or frontalizing non-frontal high-resolution (HR) faces. It is desirable to perform both tasks seamlessly for daily-life unconstrained face images. In this paper, we present a novel Vivid Face Hallucination Generative Adversarial Network (VividGAN) for simultaneously super-resolving and frontalizing tiny non-frontal face images. VividGAN consists of coarse-level and fine-level Face Hallucination Networks (FHnet) and two discriminators, i.e., Coarse-D and Fine-D. The coarse-level FHnet generates a frontal coarse HR face and then the fine-level FHnet makes use of the facial component appearance prior, i.e., fine-grained facial components, to attain a frontal HR face image with authentic details. In the fine-level FHnet, we also design a facial component-aware module that adopts the facial geometry guidance as clues to accurately align and merge the frontal coarse HR face and prior information. Meanwhile, two-level discriminators are designed to capture both the global outline of a face image as well as detailed facial characteristics. The Coarse-D enforces the coarsely hallucinated faces to be upright and complete while the Fine-D focuses on the fine hallucinated ones for sharper details. Extensive experiments demonstrate that our VividGAN achieves photo-realistic frontal HR faces, reaching superior performance in downstream tasks, i.e., face recognition and expression classification, compared with other state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Yang Zhang and Ivor W. Tsang and Jun Li and Ping Liu and Xiaobo Lu and Xin Yu},
  doi          = {10.1109/TIP.2020.3046918},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1728-1743},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Face hallucination with finishing touches},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust 3D reconstruction of dynamic scenes from
single-photon lidar using beta-divergences. <em>TIP</em>, <em>30</em>,
1716–1727. (<a href="https://doi.org/10.1109/TIP.2020.3046882">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we present a new algorithm for fast, online 3D reconstruction of dynamic scenes using times of arrival of photons recorded by single-photon detector arrays. One of the main challenges in 3D imaging using single-photon lidar in practical applications is the presence of strong ambient illumination which corrupts the data and can jeopardize the detection of peaks/surface in the signals. This background noise not only complicates the observation model classically used for 3D reconstruction but also the estimation procedure which requires iterative methods. In this work, we consider a new similarity measure for robust depth estimation, which allows us to use a simple observation model and a non-iterative estimation procedure while being robust to mis-specification of the background illumination model. This choice leads to a computationally attractive depth estimation procedure without significant degradation of the reconstruction performance. This new depth estimation procedure is coupled with a spatio-temporal model to capture the natural correlation between neighboring pixels and successive frames for dynamic scene analysis. The resulting online inference process is scalable and well suited for parallel implementation. The benefits of the proposed method are demonstrated through a series of experiments conducted with simulated and real single-photon lidar videos, allowing the analysis of dynamic scenes at 325 m observed under extreme ambient illumination conditions.},
  archive      = {J_TIP},
  author       = {Quentin Legros and Julián Tachella and Rachael Tobin and Aongus Mccarthy and Sylvain Meignen and Gerald S. Buller and Yoann Altmann and Stephen Mclaughlin and Michael E. Davies},
  doi          = {10.1109/TIP.2020.3046882},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1716-1727},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Robust 3D reconstruction of dynamic scenes from single-photon lidar using beta-divergences},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). RR-DnCNN v2.0: Enhanced restoration-reconstruction deep
neural network for down-sampling-based video coding. <em>TIP</em>,
<em>30</em>, 1702–1715. (<a
href="https://doi.org/10.1109/TIP.2020.3046872">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Integrating deep learning techniques into the video coding framework gains significant improvement compared to the standard compression techniques, especially applying super-resolution (up-sampling) to down-sampling based video coding as post-processing. However, besides up-sampling degradation, the various artifacts brought from compression make super-resolution problem more difficult to solve. The straightforward solution is to integrate the artifact removal techniques before super-resolution. However, some helpful features may be removed together, degrading the super-resolution performance. To address this problem, we proposed an end-to-end restoration-reconstruction deep neural network (RR-DnCNN) using the degradation-aware technique, which entirely solves degradation from compression and sub-sampling. Besides, we proved that the compression degradation produced by Random Access configuration is rich enough to cover other degradation types, such as Low Delay P and All Intra, for training. Since the straightforward network RR-DnCNN with many layers as a chain has poor learning capability suffering from the gradient vanishing problem, we redesign the network architecture to let reconstruction leverages the captured features from restoration using up-sampling skip connections. Our novel architecture is called restoration-reconstruction u-shaped deep neural network (RR-DnCNN v2.0). As a result, our RR-DnCNN v2.0 outperforms the previous works and can attain 17.02\% BD-rate reduction on UHD resolution for all-intra anchored by the standard H.265/HEVC. The source code is available at https://minhmanho.github.io/rrdncnn/.},
  archive      = {J_TIP},
  author       = {Man M. Ho and Jinjia Zhou and Gang He},
  doi          = {10.1109/TIP.2020.3046872},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1702-1715},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {RR-DnCNN v2.0: Enhanced restoration-reconstruction deep neural network for down-sampling-based video coding},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SLOAN: Scale-adaptive orientation attention network for
scene text recognition. <em>TIP</em>, <em>30</em>, 1687–1701. (<a
href="https://doi.org/10.1109/TIP.2020.3045602">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scene text recognition, the final step of the scene text reading system, has made impressive progress based on deep neural networks. However, existing recognition methods devote to dealing with the geometrically regular or irregular scene text. They are limited to the semantically arbitrary-orientation scene text. Meanwhile, previous scene text recognizers usually learn the single-scale feature representations for various-scale characters, which cannot model effective contexts for different characters. In this paper, we propose a novel scale-adaptive orientation attention network for arbitrary-orientation scene text recognition, which consists of a dynamic log-polar transformer and a sequence recognition network. Specifically, the dynamic log-polar transformer learns the log-polar origin to adaptively convert the arbitrary rotations and scales of scene texts into the shifts in the log-polar space, which is helpful to generate the rotation-aware and scale-aware visual representation. Next, the sequence recognition network is an encoder-decoder model, which incorporates a novel character-level receptive field attention module to encode more valid contexts for various-scale characters. The whole architecture can be trained in an end-to-end manner, only requiring the word image and its corresponding ground-truth text. Extensive experiments on several public datasets have demonstrated the effectiveness and superiority of our proposed method.},
  archive      = {J_TIP},
  author       = {Pengwen Dai and Hua Zhang and Xiaochun Cao},
  doi          = {10.1109/TIP.2020.3045602},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1687-1701},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {SLOAN: Scale-adaptive orientation attention network for scene text recognition},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). ES-net: Erasing salient parts to learn more in
re-identification. <em>TIP</em>, <em>30</em>, 1676–1686. (<a
href="https://doi.org/10.1109/TIP.2020.3046904">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an instance-level recognition problem, re-identification (re-ID) requires models to capture diverse features. However, with continuous training, re-ID models pay more and more attention to the salient areas. As a result, the model may only focus on few small regions with salient representations and ignore other important information. This phenomenon leads to inferior performance, especially when models are evaluated on small inter-identity variation data. In this paper, we propose a novel network, Erasing-Salient Net (ES-Net), to learn comprehensive features by erasing the salient areas in an image. ES-Net proposes a novel method to locate the salient areas by the confidence of objects and erases them efficiently in a training batch. Meanwhile, to mitigate the over-erasing problem, this paper uses a trainable pooling layer P-pooling that generalizes global max and global average pooling. Experiments are conducted on two specific re-identification tasks (i.e., Person re-ID, Vehicle re-ID). Our ES-Net outperforms state-of-the-art methods on three Person re-ID benchmarks and two Vehicle re-ID benchmarks. Specifically, mAP / Rank-1 rate: 88.6\% / 95.7\% on Market1501, 78.8\% / 89.2\% on DuckMTMC-reID, 57.3\% / 80.9\% on MSMT17, 81.9\% / 97.0\% on Veri-776, respectively. Rank-1 / Rank-5 rate: 83.6\% / 96.9\% on VehicleID (Small), 79.9\% / 93.5\% on VehicleID (Medium), 76.9\% / 90.7\% on VehicleID (Large), respectively. Moreover, the visualized salient areas show human-interpretable visual explanations for the ranking results.},
  archive      = {J_TIP},
  author       = {Dong Shen and Shuai Zhao and Jinming Hu and Hao Feng and Deng Cai and Xiaofei He},
  doi          = {10.1109/TIP.2020.3046904},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1676-1686},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {ES-net: Erasing salient parts to learn more in re-identification},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Loss-based attention for interpreting image-level prediction
of convolutional neural networks. <em>TIP</em>, <em>30</em>, 1662–1675.
(<a href="https://doi.org/10.1109/TIP.2020.3046875">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although deep neural networks have achieved great success on numerous large-scale tasks, poor interpretability is still a notorious obstacle for practical applications. In this paper, we propose a novel and general attention mechanism, loss-based attention, upon which we modify deep neural networks to mine significant image patches for explaining which parts determine the image decision-making. This is inspired by the fact that some patches contain significant objects or their parts for image-level decision. Unlike previous attention mechanisms that adopt different layers and parameters to learn weights and image prediction, the proposed loss-based attention mechanism mines significant patches by utilizing the same parameters to learn patch weights and logits (class vectors), and image prediction simultaneously, so as to connect the attention mechanism with the loss function for boosting the patch precision and recall. Additionally, different from previous popular networks that utilize max-pooling or stride operations in convolutional layers without considering the spatial relationship of features, the modified deep architectures first remove them to preserve the spatial relationship of image patches and greatly reduce their dependencies, and then add two convolutional or capsule layers to extract their features. With the learned patch weights, the image-level decision of the modified deep architectures is the weighted sum on patches. Extensive experiments on large-scale benchmark databases demonstrate that the proposed architectures can obtain better or competitive performance to state-of-the-art baseline networks with better interpretability. The source codes are available on: https://github.com/xsshi2015/Loss-based-Attention-for-Interpreting-Image-level-Prediction-of-Convolutional-Neural-Networks.},
  archive      = {J_TIP},
  author       = {Xiaoshuang Shi and Fuyong Xing and Kaidi Xu and Pingjun Chen and Yun Liang and Zhiyong Lu and Zhenhua Guo},
  doi          = {10.1109/TIP.2020.3046875},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1662-1675},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Loss-based attention for interpreting image-level prediction of convolutional neural networks},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Few-shot human-object interaction recognition with
semantic-guided attentive prototypes network. <em>TIP</em>, <em>30</em>,
1648–1661. (<a href="https://doi.org/10.1109/TIP.2020.3046861">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extreme instance imbalance among categories and combinatorial explosion make the recognition of Human-Object Interaction (HOI) a challenging task. Few studies have addressed both challenges directly. Motivated by the success of few-shot learning that learns a robust model from a few instances, we formulate HOI as a few-shot task in a meta-learning framework to alleviate the above challenges. Due to the fact that the intrinsical characteristic of HOI is diverse and interactive, we propose a Semantic-guided Attentive Prototypes Network (SAPNet) framework to learn a semantic-guided metric space where HOI recognition can be performed by computing distances to attentive prototypes of each class. Specifically, the model generates attentive prototypes guided by the category names of actions and objects, which highlight the commonalities of images from the same class in HOI. In addition, we design two alternative prototypes calculation methods, i.e., Prototypes Shift (PS) approach and Hallucinatory Graph Prototypes (HGP) approach, which explore to learn a suitable category prototypes representations in HOI. Finally, in order to realize the task of few-shot HOI, we reorganize 2 HOI benchmark datasets with 2 split strategies, i.e., HICO-NN, TUHOI-NN, HICO-NF, and TUHOI-NF. Extensive experimental results on these datasets have demonstrated the effectiveness of our proposed SAPNet approach.},
  archive      = {J_TIP},
  author       = {Zhong Ji and Xiyao Liu and Yanwei Pang and Wangli Ouyang and Xuelong Li},
  doi          = {10.1109/TIP.2020.3046861},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1648-1661},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Few-shot human-object interaction recognition with semantic-guided attentive prototypes network},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). EnAET: A self-trained framework for semi-supervised and
supervised learning with ensemble transformations. <em>TIP</em>,
<em>30</em>, 1639–1647. (<a
href="https://doi.org/10.1109/TIP.2020.3044220">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks have been successfully applied to many real-world applications. However, such successes rely heavily on large amounts of labeled data that is expensive to obtain. Recently, many methods for semi-supervised learning have been proposed and achieved excellent performance. In this study, we propose a new EnAET framework to further improve existing semi-supervised methods with self-supervised information. To our best knowledge, all current semi-supervised methods improve performance with prediction consistency and confidence ideas. We are the first to explore the role of self-supervised representations in semi-supervised learning under a rich family of transformations. Consequently, our framework can integrate the self-supervised information as a regularization term to further improve all current semi-supervised methods. In the experiments, we use MixMatch, which is the current state-of-the-art method on semi-supervised learning, as a baseline to test the proposed EnAET framework. Across different datasets, we adopt the same hyper-parameters, which greatly improves the generalization ability of the EnAET framework. Experiment results on different datasets demonstrate that the proposed EnAET framework greatly improves the performance of current semi-supervised algorithms. Moreover, this framework can also improve supervised learning by a large margin, including the extremely challenging scenarios with only 10 images per class. The code and experiment records are available in https://github.com/maple-research-lab/EnAET.},
  archive      = {J_TIP},
  author       = {Xiao Wang and Daisuke Kihara and Jiebo Luo and Guo-Jun Qi},
  doi          = {10.1109/TIP.2020.3044220},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1639-1647},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {EnAET: A self-trained framework for semi-supervised and supervised learning with ensemble transformations},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust and efficient graph correspondence transfer for
person re-identification. <em>TIP</em>, <em>30</em>, 1623–1638. (<a
href="https://doi.org/10.1109/TIP.2019.2914575">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatial misalignment caused by variations in poses and viewpoints is one of the most critical issues that hinder the performance improvement in existing person re-identification (Re-ID) algorithms. Although it is straightforward to explore correspondence learning algorithms for alignment, online learning is intractable for negative pairs due to the intrinsic visual difference between negative pairs and efficiency concern. To address this problem, in this paper, we present a robust and efficient graph correspondence transfer (REGCT) approach for explicit spatial alignment in Re-ID. Specifically, we propose the off-line correspondence learning and on-line correspondence transfer framework. During training, patch-wise correspondences between positive training pairs are established via graph matching. By exploiting both spatial and visual contexts of human appearance in graph matching, meaningful semantic correspondences can be obtained. During testing, the off-line learned patch-wise correspondence templates are transferred to test pairs with similar pose-pair configurations for local feature distance calculation. To enhance the robustness of correspondence transfer, we design a novel pose context descriptor to accurately model human body configurations, and present an approach to measure the similarity between a pair of pose context descriptors. Meanwhile, to improve testing efficiency, we propose a correspondence template ensemble method using the voting mechanism, which significantly reduces the amount of patch-wise matchings involved in distance calculation. With the aforementioned strategies, the REGCT model can effectively and efficiently handle the spatial misalignment problem in Re-ID. Extensive experiments on five challenging benchmarks, including VIPeR, Road, PRID450S, 3DPES, and CUHK01, evidence the superior performance of REGCT over other state-of-the-art approaches.},
  archive      = {J_TIP},
  author       = {Qin Zhou and Heng Fan and Hua Yang and Hang Su and Shibao Zheng and Shuang Wu and Haibin Ling},
  doi          = {10.1109/TIP.2019.2914575},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1623-1638},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Robust and efficient graph correspondence transfer for person re-identification},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Efficient depth intra frame coding in 3D-HEVC by corner
points. <em>TIP</em>, <em>30</em>, 1608–1622. (<a
href="https://doi.org/10.1109/TIP.2020.3046866">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To improve the coding performance of depth maps, 3D-HEVC includes several new depth intra coding tools at the expense of increased complexity due to a flexible quadtree Coding Unit/Prediction Unit (CU/PU) partitioning structure and a huge number of intra mode candidates. Compared to natural images, depth maps contain large plain regions surrounded by sharp edges at the object boundaries. Our observation finds that the features proposed in the literature either speed up the CU/PU size decision or intra mode decision and they are also difficult to make proper predictions for CUs/PUs with the multi-directional edges in depth maps. In this work, we reveal that the CUs with multi-directional edges are highly correlated with the distribution of corner points (CPs) in the depth map. CP is proposed as a good feature that can guide to split the CUs with multi-directional edges into smaller units until only single directional edge remains. This smaller unit can then be well predicted by the conventional intra mode. Besides, a fast intra mode decision is also proposed for non-CP PUs, which prunes the conventional HEVC intra modes, skips the depth modeling mode decision, and early determines segment-wise depth coding. Furthermore, a two-step adaptive corner point selection technique is designed to make the proposed algorithm adaptive to frame content and quantization parameters, with the capability of providing the flexible tradeoff between the synthesized view quality and complexity. Simulation results show that the proposed algorithm can provide about 66\% time reduction of the 3D-HEVC intra encoder without incurring noticeable performance degradation for synthesized views and it also outperforms the previous state-of-the-art algorithms in term of time reduction and $\Delta $ BDBR.},
  archive      = {J_TIP},
  author       = {Chang-Hong Fu and Yui-Lam Chan and Hong-Bin Zhang and Sik Ho Tsang and Meng-Ting Xu},
  doi          = {10.1109/TIP.2020.3046866},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1608-1622},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Efficient depth intra frame coding in 3D-HEVC by corner points},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-domain adversarial feature generalization for person
re-identification. <em>TIP</em>, <em>30</em>, 1596–1607. (<a
href="https://doi.org/10.1109/TIP.2020.3046864">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the assistance of sophisticated training methods applied to single labeled datasets, the performance of fully-supervised person re-identification (Person Re-ID) has been improved significantly in recent years. However, these models trained on a single dataset usually suffer from considerable performance degradation when applied to videos of a different camera network. To make Person Re-ID systems more practical and scalable, several cross-dataset domain adaptation methods have been proposed, which achieve high performance without the labeled data from the target domain. However, these approaches still require the unlabeled data of the target domain during the training process, making them impractical. A practical Person Re-ID system pre-trained on other datasets should start running immediately after deployment on a new site without having to wait until sufficient images or videos are collected and the pre-trained model is tuned. To serve this purpose, in this paper, we reformulate person re-identification as a multi-dataset domain generalization problem. We propose a multi-dataset feature generalization network (MMFA-AAE), which is capable of learning a universal domain-invariant feature representation from multiple labeled datasets and generalizing it to `unseen&#39; camera systems. The network is based on an adversarial auto-encoder to learn a generalized domain-invariant latent feature representation with the Maximum Mean Discrepancy (MMD) measure to align the distributions across multiple domains. Extensive experiments demonstrate the effectiveness of the proposed method. Our MMFA-AAE approach not only outperforms most of the domain generalization Person Re-ID methods, but also surpasses many state-of-the-art supervised methods and unsupervised domain adaptation methods by a large margin.},
  archive      = {J_TIP},
  author       = {Shan Lin and Chang-Tsun Li and Alex C. Kot},
  doi          = {10.1109/TIP.2020.3046864},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1596-1607},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Multi-domain adversarial feature generalization for person re-identification},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bi-directional exponential angular triplet loss for
RGB-infrared person re-identification. <em>TIP</em>, <em>30</em>,
1583–1595. (<a href="https://doi.org/10.1109/TIP.2020.3045261">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {RGB-Infrared person re-identification (RGB-IR Re-ID) is a cross-modality matching problem, where the modality discrepancy is a big challenge. Most existing works use Euclidean metric based constraints to resolve the discrepancy between features of images from different modalities. However, these methods are incapable of learning angularly discriminative feature embedding because Euclidean distance cannot measure the included angle between embedding vectors effectively. As an angularly discriminative feature space is important for classifying the human images based on their embedding vectors, in this paper, we propose a novel ranking loss function, named Bi-directional Exponential Angular Triplet Loss, to help learn an angularly separable common feature space by explicitly constraining the included angles between embedding vectors. Moreover, to help stabilize and learn the magnitudes of embedding vectors, we adopt a common space batch normalization layer. The quantitative and qualitative experiments on the SYSU-MM01 and RegDB dataset support our analysis. On SYSU-MM01 dataset, the performance is improved from 7.40\% / 11.46\% to 38.57\% / 38.61\% for rank-1 accuracy / mAP compared with the baseline. The proposed method can be generalized to the task of single-modality Re-ID and improves the rank-1 accuracy / mAP from 92.0\% / 81.7\% to 94.7\% / 86.6\% on the Market-1501 dataset, from 82.6\% / 70.6\% to 87.6\% / 77.1\% on the DukeMTMC-reID dataset.},
  archive      = {J_TIP},
  author       = {Hanrong Ye and Hong Liu and Fanyang Meng and Xia Li},
  doi          = {10.1109/TIP.2020.3045261},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1583-1595},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Bi-directional exponential angular triplet loss for RGB-infrared person re-identification},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fabric retrieval based on multi-task learning. <em>TIP</em>,
<em>30</em>, 1570–1582. (<a
href="https://doi.org/10.1109/TIP.2020.3043877">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the potential values in many areas such as e-commerce and inventory management, fabric image retrieval, which is a special case in Content Based Image Retrieval (CBIR), has recently become a research hotspot. It is also a challenging issue with serval obstacles: variety and complexity of fabric appearance, high requirements for retrieval accuracy. To address this issue, this paper proposes a novel approach for fabric image retrieval based on multi-task learning and deep hashing. According to the cognitive system of fabric, a multi-classification-task learning model with uncertainty loss and constraint is presented to learn fabric image representation. Then we adopt an unsupervised deep network to encode the extracted features into 128-bits hashing codes. Further, the hashing codes are regarded as the index of fabrics image for image retrieval. To evaluate the proposed approach, we expanded and upgraded the dataset WFID, which was built in our previous research specifically for fabric image retrieval. The experimental results show that the proposed approach outperforms the state-of-the-art.},
  archive      = {J_TIP},
  author       = {Jun Xiang and Ning Zhang and Ruru Pan and Weidong Gao},
  doi          = {10.1109/TIP.2020.3043877},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1570-1582},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Fabric retrieval based on multi-task learning},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A global-local self-adaptive network for drone-view object
detection. <em>TIP</em>, <em>30</em>, 1556–1569. (<a
href="https://doi.org/10.1109/TIP.2020.3045636">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Directly benefiting from the deep learning methods, object detection has witnessed a great performance boost in recent years. However, drone-view object detection remains challenging for two main reasons: (1) Objects of tiny-scale with more blurs w.r.t. ground-view objects offer less valuable information towards accurate and robust detection; (2) The unevenly distributed objects make the detection inefficient, especially for regions occupied by crowded objects. Confronting such challenges, we propose an end-to-end global-local self-adaptive network (GLSAN) in this paper. The key components in our GLSAN include a global-local detection network (GLDN), a simple yet efficient self-adaptive region selecting algorithm (SARSA), and a local super-resolution network (LSRN). We integrate a global-local fusion strategy into a progressive scale-varying network to perform more precise detection, where the local fine detector can adaptively refine the target&#39;s bounding boxes detected by the global coarse detector via cropping the original images for higher-resolution detection. The SARSA can dynamically crop the crowded regions in the input images, which is unsupervised and can be easily plugged into the networks. Additionally, we train the LSRN to enlarge the cropped images, providing more detailed information for finer-scale feature extraction, helping the detector distinguish foreground and background more easily. The SARSA and LSRN also contribute to data augmentation towards network training, which makes the detector more robust. Extensive experiments and comprehensive evaluations on the VisDrone2019-DET benchmark dataset and UAVDT dataset demonstrate the effectiveness and adaptivity of our method. Towards an industrial application, our network is also applied to a DroneBolts dataset with proven advantages. Our source codes have been available at https://github.com/dengsutao/glsan.},
  archive      = {J_TIP},
  author       = {Sutao Deng and Shuai Li and Ke Xie and Wenfeng Song and Xiao Liao and Aimin Hao and Hong Qin},
  doi          = {10.1109/TIP.2020.3045636},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1556-1569},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A global-local self-adaptive network for drone-view object detection},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Structure-texture image decomposition using discriminative
patch recurrence. <em>TIP</em>, <em>30</em>, 1542–1555. (<a
href="https://doi.org/10.1109/TIP.2020.3043665">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Morphology component analysis provides an effective framework for structure-texture image decomposition, which characterizes the structure and texture components by sparsifying them with certain transforms respectively. Due to the complexity and randomness of texture, it is challenging to design effective sparsifying transforms for texture components. This paper aims at exploiting the recurrence of texture patterns, one important property of texture, to develop a nonlocal transform for texture component sparsification. Since the plain patch recurrence holds for both cartoon contours and texture regions, the nonlocal sparsifying transform constructed based on such patch recurrence sparsifies both the structure and texture components well. As a result, cartoon contours could be wrongly assigned to the texture component, yielding ambiguity in decomposition. To address this issue, we introduce a discriminative prior on patch recurrence, that the spatial arrangement of recurrent patches in texture regions exhibits isotropic structure which differs from that of cartoon contours. Based on the prior, a nonlocal transform is constructed which only sparsifies texture regions well. Incorporating the constructed transform into morphology component analysis, we propose an effective approach for structure-texture decomposition. Extensive experiments have demonstrated the superior performance of our approach over existing ones.},
  archive      = {J_TIP},
  author       = {Ruotao Xu and Yong Xu and Yuhui Quan},
  doi          = {10.1109/TIP.2020.3043665},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1542-1555},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Structure-texture image decomposition using discriminative patch recurrence},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hierarchical lossy bilevel image compression based on cutset
sampling. <em>TIP</em>, <em>30</em>, 1527–1541. (<a
href="https://doi.org/10.1109/TIP.2020.3043587">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider lossy compression of a broad class of bilevel images that satisfy the smoothness criterion, namely, images in which the black and white regions are separated by smooth or piecewise smooth boundaries, and especially lossy compression of complex bilevel images in this class. We propose a new hierarchical compression approach that extends the previously proposed fixed-grid lossy cutset coding (LCC) technique by adapting the grid size to local image detail. LCC was claimed to have the best rate-distortion performance of any lossy compression technique in the given image class, but cannot take advantage of detail variations across an image. The key advantages of the hierarchical LCC (HLCC) is that, by adapting to local detail, it provides constant quality controlled by a single parameter (distortion threshold), independent of image content, and better overall visual quality and rate-distortion performance, over a wider range of bitrates. We also introduce several other enhancements of LCC that improve reconstruction accuracy and perceptual quality. These include the use of multiple connection bits that provide structural information by specifying which black (or white) runs on the boundary of a block must be connected, a boundary presmoothing step, stricter connectivity constraints, and more elaborate probability estimation for arithmetic coding. We also propose a progressive variation that refines the image reconstruction as more bits are transmitted, with very small additional overhead. Experimental results with a wide variety of, and especially complex, bilevel images in the given class confirm that the proposed techniques provide substantially better visual quality and rate-distortion performance than existing lossy bilevel compression techniques, at bitrates lower than lossless compression with the JBIG or JBIG2 standards.},
  archive      = {J_TIP},
  author       = {Shengxin Zha and Thrasyvoulos N. Pappas and David L. Neuhoff},
  doi          = {10.1109/TIP.2020.3043587},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1527-1541},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Hierarchical lossy bilevel image compression based on cutset sampling},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A study of multi-task and region-wise deep learning for food
ingredient recognition. <em>TIP</em>, <em>30</em>, 1514–1526. (<a
href="https://doi.org/10.1109/TIP.2020.3045639">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Food recognition has captured numerous research attention for its importance for health-related applications. The existing approaches mostly focus on the categorization of food according to dish names, while ignoring the underlying ingredient composition. In reality, two dishes with the same name do not necessarily share the exact list of ingredients. Therefore, the dishes under the same food category are not mandatorily equal in nutrition content. Nevertheless, due to limited datasets available with ingredient labels, the problem of ingredient recognition is often overlooked. Furthermore, as the number of ingredients is expected to be much less than the number of food categories, ingredient recognition is more tractable in the real-world scenario. This paper provides an insightful analysis of three compelling issues in ingredient recognition. These issues involve recognition in either image-level or region level, pooling in either single or multiple image scales, learning in either single or multi-task manner. The analysis is conducted on a large food dataset, Vireo Food-251, contributed by this paper. The dataset is composed of 169,673 images with 251 popular Chinese food and 406 ingredients. The dataset includes adequate challenges in scale and complexity to reveal the limit of the current approaches in ingredient recognition.},
  archive      = {J_TIP},
  author       = {Jingjing Chen and Bin Zhu and Chong-Wah Ngo and Tat-Seng Chua and Yu-Gang Jiang},
  doi          = {10.1109/TIP.2020.3045639},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1514-1526},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A study of multi-task and region-wise deep learning for food ingredient recognition},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Harnessing multi-view perspective of light fields for
low-light imaging. <em>TIP</em>, <em>30</em>, 1501–1513. (<a
href="https://doi.org/10.1109/TIP.2020.3045617">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Light Field (LF) offers unique advantages such as post-capture refocusing and depth estimation, but low-light conditions severely limit these capabilities. To restore low-light LFs we should harness the geometric cues present in different LF views, which is not possible using single-frame low-light enhancement techniques. We propose a deep neural network L3Fnet for Low-Light Light Field (L3F) restoration, which not only performs visual enhancement of each LF view but also preserves the epipolar geometry across views. We achieve this by adopting a two-stage architecture for L3Fnet. Stage-I looks at all the LF views to encode the LF geometry. This encoded information is then used in Stage-II to reconstruct each LF view. To facilitate learning-based techniques for low-light LF imaging, we collected a comprehensive LF dataset of various scenes. For each scene, we captured four LFs, one with near-optimal exposure and ISO settings and the others at different levels of low-light conditions varying from low to extreme low-light settings. The effectiveness of the proposed L3Fnet is supported by both visual and numerical comparisons on this dataset. To further analyze the performance of low-light restoration methods, we also propose the L3F-wild dataset that contains LF captured late at night with almost zero lux values. No ground truth is available in this dataset. To perform well on the L3F-wild dataset, any method must adapt to the light level of the captured scene. To do this we use a pre-processing block that makes L3Fnet robust to various degrees of low-light conditions. Lastly, we show that L3Fnet can also be used for low-light enhancement of single-frame images, despite it being engineered for LF data. We do so by converting the single-frame DSLR image into a form suitable to L3Fnet, which we call as pseudo-LF. Our code and dataset is available for download at https://mohitlamba94.github.io/L3Fnet/.},
  archive      = {J_TIP},
  author       = {Mohit Lamba and Kranthi Kumar Rachavarapu and Kaushik Mitra},
  doi          = {10.1109/TIP.2020.3045617},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1501-1513},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Harnessing multi-view perspective of light fields for low-light imaging},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). AMP-net: Denoising-based deep unfolding for compressive
image sensing. <em>TIP</em>, <em>30</em>, 1487–1500. (<a
href="https://doi.org/10.1109/TIP.2020.3044472">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most compressive sensing (CS) reconstruction methods can be divided into two categories, i.e. model-based methods and classical deep network methods. By unfolding the iterative optimization algorithm for model-based methods onto networks, deep unfolding methods have the good interpretation of model-based methods and the high speed of classical deep network methods. In this article, to solve the visual image CS problem, we propose a deep unfolding model dubbed AMP-Net. Rather than learning regularization terms, it is established by unfolding the iterative denoising process of the well-known approximate message passing algorithm. Furthermore, AMP-Net integrates deblocking modules in order to eliminate the blocking artifacts that usually appear in CS of visual images. In addition, the sampling matrix is jointly trained with other network parameters to enhance the reconstruction performance. Experimental results show that the proposed AMP-Net has better reconstruction accuracy than other state-of-the-art methods with high reconstruction speed and a small number of network parameters.},
  archive      = {J_TIP},
  author       = {Zhonghao Zhang and Yipeng Liu and Jiani Liu and Fei Wen and Ce Zhu},
  doi          = {10.1109/TIP.2020.3044472},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1487-1500},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {AMP-net: Denoising-based deep unfolding for compressive image sensing},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning event representations for temporal segmentation of
image sequences by dynamic graph embedding. <em>TIP</em>, <em>30</em>,
1476–1486. (<a href="https://doi.org/10.1109/TIP.2020.3044448">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, self-supervised learning has proved to be effective to learn representations of events suitable for temporal segmentation in image sequences, where events are understood as sets of temporally adjacent images that are semantically perceived as a whole. However, although this approach does not require expensive manual annotations, it is data hungry and suffers from domain adaptation problems. As an alternative, in this work, we propose a novel approach for learning event representations named Dynamic Graph Embedding (DGE). The assumption underlying our model is that a sequence of images can be represented by a graph that encodes both semantic and temporal similarity. The key novelty of DGE is to learn jointly the graph and its graph embedding. At its core, DGE works by iterating over two steps: 1) updating the graph representing the semantic and temporal similarity of the data based on the current data representation, and 2) updating the data representation to take into account the current data graph structure. The main advantage of DGE over state-of-the-art self-supervised approaches is that it does not require any training set, but instead learns iteratively from the data itself a low-dimensional embedding that reflects their temporal and semantic similarity. Experimental results on two benchmark datasets of real image sequences captured at regular time intervals demonstrate that the proposed DGE leads to event representations effective for temporal segmentation. In particular, it achieves robust temporal segmentation on the EDUBSeg and EDUBSeg-Desc benchmark datasets, outperforming the state of the art. Additional experiments on two Human Motion Segmentation benchmark datasets demonstrate the generalization capabilities of the proposed DGE.},
  archive      = {J_TIP},
  author       = {Mariella Dimiccoli and Herwig Wendt},
  doi          = {10.1109/TIP.2020.3044448},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1476-1486},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Learning event representations for temporal segmentation of image sequences by dynamic graph embedding},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Personal fixations-based object segmentation with object
localization and boundary preservation. <em>TIP</em>, <em>30</em>,
1461–1475. (<a href="https://doi.org/10.1109/TIP.2020.3044440">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a natural way for human-computer interaction, fixation provides a promising solution for interactive image segmentation. In this paper, we focus on Personal Fixations-based Object Segmentation (PFOS) to address issues in previous studies, such as the lack of appropriate dataset and the ambiguity in fixations-based interaction. In particular, we first construct a new PFOS dataset by carefully collecting pixel-level binary annotation data over an existing fixation prediction dataset, such dataset is expected to greatly facilitate the study along the line. Then, considering characteristics of personal fixations, we propose a novel network based on Object Localization and Boundary Preservation (OLBP) to segment the gazed objects. Specifically, the OLBP network utilizes an Object Localization Module (OLM) to analyze personal fixations and locates the gazed objects based on the interpretation. Then, a Boundary Preservation Module (BPM) is designed to introduce additional boundary information to guard the completeness of the gazed objects. Moreover, OLBP is organized in the mixed bottom-up and top-down manner with multiple types of deep supervision. Extensive experiments on the constructed PFOS dataset show the superiority of the proposed OLBP network over 17 state-of-the-art methods, and demonstrate the effectiveness of the proposed OLM and BPM components. The constructed PFOS dataset and the proposed OLBP network are available at https://github.com/MathLee/OLBPNet4PFOS .},
  archive      = {J_TIP},
  author       = {Gongyang Li and Zhi Liu and Ran Shi and Zheng Hu and Weijie Wei and Yong Wu and Mengke Huang and Haibin Ling},
  doi          = {10.1109/TIP.2020.3044440},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1461-1475},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Personal fixations-based object segmentation with object localization and boundary preservation},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Surface regularity via the estimation of fractional brownian
motion index. <em>TIP</em>, <em>30</em>, 1453–1460. (<a
href="https://doi.org/10.1109/TIP.2020.3043892">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recent definition of fractional Brownian motions on surfaces has raised the statistical issue of estimating the Hurst index characterizing these models. To deal with this open issue, we propose a method which is based on a spectral representation of surfaces built upon their Laplace-Beltrami operator. This method includes a first step where the surface supporting the motion is recovered using a mean curvature flow, and a second one where the Hurst index is estimated by linear regression on the motion spectrum. The method is evaluated on synthetic surfaces. The interest of the method is further illustrated on some fetal cortical surfaces extracted from magnetic resonance images as a means to quantify the brain complexity during the gestational age.},
  archive      = {J_TIP},
  author       = {Hamed Rabiei and Olivier Coulon and Julien Lefèvre and Frédéric J. P. Richard},
  doi          = {10.1109/TIP.2020.3043892},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1453-1460},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Surface regularity via the estimation of fractional brownian motion index},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Tracking-by-counting: Using network flows on crowd density
maps for tracking multiple targets. <em>TIP</em>, <em>30</em>,
1439–1452. (<a href="https://doi.org/10.1109/TIP.2020.3044219">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {State-of-the-art multi-object tracking (MOT) methods follow the tracking-by-detection paradigm, where object trajectories are obtained by associating per-frame outputs of object detectors. In crowded scenes, however, detectors often fail to obtain accurate detections due to heavy occlusions and high crowd density. In this paper, we propose a new MOT paradigm, tracking-by-counting, tailored for crowded scenes. Using crowd density maps, we jointly model detection, counting, and tracking of multiple targets as a network flow program, which simultaneously finds the global optimal detections and trajectories of multiple targets over the whole video. This is in contrast to prior MOT methods that either ignore the crowd density and thus are prone to errors in crowded scenes, or rely on a suboptimal two-step process using heuristic density-aware point-tracks for matching targets. Our approach yields promising results on public benchmarks of various domains including people tracking, cell tracking, and fish tracking.},
  archive      = {J_TIP},
  author       = {Weihong Ren and Xinchao Wang and Jiandong Tian and Yandong Tang and Antoni B. Chan},
  doi          = {10.1109/TIP.2020.3044219},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1439-1452},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Tracking-by-counting: Using network flows on crowd density maps for tracking multiple targets},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hyperspectral image super-resolution via deep progressive
zero-centric residual learning. <em>TIP</em>, <em>30</em>, 1423–1438.
(<a href="https://doi.org/10.1109/TIP.2020.3044214">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper explores the problem of hyperspectral image (HSI) super-resolution that merges a low resolution HSI (LR-HSI) and a high resolution multispectral image (HR-MSI). The cross-modality distribution of the spatial and spectral information makes the problem challenging. Inspired by the classic wavelet decomposition-based image fusion, we propose a novel lightweight deep neural network-based framework, namely progressive zero-centric residual network (PZRes-Net), to address this problem efficiently and effectively. Specifically, PZRes-Net learns a high resolution and zero-centric residual image, which contains high-frequency spatial details of the scene across all spectral bands, from both inputs in a progressive fashion along the spectral dimension. And the resulting residual image is then superimposed onto the up-sampled LR-HSI in a meanvalue invariant manner, leading to a coarse HR-HSI, which is further refined by exploring the coherence across all spectral bands simultaneously. To learn the residual image efficiently and effectively, we employ spectral-spatial separable convolution with dense connections. In addition, we propose zero-mean normalization implemented on the feature maps of each layer to realize the zero-mean characteristic of the residual image. Extensive experiments over both real and synthetic benchmark datasets demonstrate that our PZRes-Net outperforms stateof-the-art methods to a significant extent in terms of both 4 quantitative metrics and visual quality, e.g., our PZRes-Net improves the PSNR more than 3dB, while saving 2.3× parameters and consuming 15× less FLOPs. The code is publicly available at https://github.com/zbzhzhy/PZRes-Net.},
  archive      = {J_TIP},
  author       = {Zhiyu Zhu and Junhui Hou and Jie Chen and Huanqiang Zeng and Jiantao Zhou},
  doi          = {10.1109/TIP.2020.3044214},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1423-1438},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Hyperspectral image super-resolution via deep progressive zero-centric residual learning},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Perceptual video quality prediction emphasizing chroma
distortions. <em>TIP</em>, <em>30</em>, 1408–1422. (<a
href="https://doi.org/10.1109/TIP.2020.3043127">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Measuring the quality of digital videos viewed by human observers has become a common practice in numerous multimedia applications, such as adaptive video streaming, quality monitoring, and other digital TV applications. Here we explore a significant, yet relatively unexplored problem: measuring perceptual quality on videos arising from both luma and chroma distortions from compression. Toward investigating this problem, it is important to understand the kinds of chroma distortions that arise, how they relate to luma compression distortions, and how they can affect perceived quality. We designed and carried out a subjective experiment to measure subjective video quality on both luma and chroma distortions, introduced both in isolation as well as together. Specifically, the new subjective dataset comprises a total of 210 videos afflicted by distortions caused by varying levels of luma quantization commingled with different amounts of chroma quantization. The subjective scores were evaluated by 34 subjects in a controlled environmental setting. Using the newly collected subjective data, we were able to demonstrate important shortcomings of existing video quality models, especially in regards to chroma distortions. Further, we designed an objective video quality model which builds on existing video quality algorithms, by considering the fidelity of chroma channels in a principled way. We also found that this quality analysis implies that there is room for reducing bitrate consumption in modern video codecs by creatively increasing the compression factor on chroma channels. We believe that this work will both encourage further research in this direction, as well as advance progress on the ultimate goal of jointly optimizing luma and chroma compression in modern video encoders.},
  archive      = {J_TIP},
  author       = {Li-Heng Chen and Christos G. Bampis and Zhi Li and Joel Sole and Alan C. Bovik},
  doi          = {10.1109/TIP.2020.3043127},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1408-1422},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Perceptual video quality prediction emphasizing chroma distortions},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021c). Embedding perspective analysis into multi-column
convolutional neural network for crowd counting. <em>TIP</em>,
<em>30</em>, 1395–1407. (<a
href="https://doi.org/10.1109/TIP.2020.3043122">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The crowd counting is challenging for deep networks due to several factors. For instance, the networks can not efficiently analyze the perspective information of arbitrary scenes, and they are naturally inefficient to handle the scale variations. In this work, we deliver a simple yet efficient multi-column network, which integrates the perspective analysis method with the counting network. The proposed method explicitly excavates the perspective information and drives the counting network to analyze the scenes. More concretely, we explore the perspective information from the estimated density maps and quantify the perspective space into several separate scenes. We then embed the perspective analysis into the multi-column framework with a recurrent connection. Therefore, the proposed network matches various scales with the different receptive fields efficiently. Secondly, we share the parameters of the branches with various receptive fields. This strategy drives the convolutional kernels to be sensitive to the instances with various scales. Furthermore, to improve the evaluation accuracy of the column with a large receptive field, we propose a transform dilated convolution. The transform dilated convolution breaks the fixed sampling structure of the deep network. Moreover, it needs no extra parameters and training, and the offsets are constrained in a local region, which is designed for the congested scenes. The proposed method achieves state-of-the-art performance on five datasets (ShanghaiTech, UCF CC 50, WorldEXPO&#39;10, UCSD, and TRANCOS).},
  archive      = {J_TIP},
  author       = {Yifan Yang and Guorong Li and Dawei Du and Qingming Huang and Nicu Sebe},
  doi          = {10.1109/TIP.2020.3043122},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1395-1407},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Embedding perspective analysis into multi-column convolutional neural network for crowd counting},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-temporal scene classification and scene change
detection with correlation based fusion. <em>TIP</em>, <em>30</em>,
1382–1394. (<a href="https://doi.org/10.1109/TIP.2020.3039328">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classifying multi-temporal scene land-use categories and detecting their semantic scene-level changes for remote sensing imagery covering urban regions could straightly reflect the land-use transitions. Existing methods for scene change detection rarely focus on the temporal correlation of bi-temporal features, and are mainly evaluated on small scale scene change detection datasets. In this work, we proposed a CorrFusion module that fuses the highly correlated components in bi-temporal feature embeddings. We first extract the deep representations of the bi-temporal inputs with deep convolutional networks. Then the extracted features will be projected into a lower-dimensional space to extract the most correlated components and compute the instance-level correlation. The cross-temporal fusion will be performed based on the computed correlation in CorrFusion module. The final scene classification results are obtained with softmax layers. In the objective function, we introduced a new formulation to calculate the temporal correlation more efficiently and stably. The detailed derivation of backpropagation gradients for the proposed module is also given. Besides, we presented a much larger scale scene change detection dataset with more semantic categories and conducted extensive experiments on this dataset. The experimental results demonstrated that our proposed CorrFusion module could remarkably improve the multi-temporal scene classification and scene change detection results.},
  archive      = {J_TIP},
  author       = {Lixiang Ru and Bo Du and Chen Wu},
  doi          = {10.1109/TIP.2020.3039328},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1382-1394},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Multi-temporal scene classification and scene change detection with correlation based fusion},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A novel eye center localization method for head poses with
large rotations. <em>TIP</em>, <em>30</em>, 1369–1381. (<a
href="https://doi.org/10.1109/TIP.2020.3044209">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Eye localization is undoubtedly crucial to acquiring large amounts of information. It not only helps people improve their understanding of others but is also a technology that enables machines to better understand humans. Although studies have reported satisfactory accuracy for frontal faces or head poses at limited angles, large head rotations generate numerous defects (e.g., disappearance of the eye), and existing methods are not effective enough to accurately localize eye centers. Therefore, this study makes three contributions to address these limitations. First, we propose a novel complete representation (CR) pipeline that can flexibly learn and generate two complete representations, namely the CR-center and CR-region, of the same identity. We also propose two novel eye center localization methods. This first method employs geometric transformation to estimate the rotational difference between two faces and an unknown-localization strategy for accurate transformation of the CR-center. The second method is based on image translation learning and uses the CR-region to train the generative adversarial network, which can then accurately generate and localize eye centers. Five image databases are employed to verify the proposed methods, and tests reveal that compared with existing methods, the proposed method can more accurately and robustly localize eye centers in challenging images, such as those showing considerable head rotation (both yaw rotation of -67.5° to +67.5° and roll rotation of +120° to -120°), complete occlusion of both eyes, poor illumination in addition to head rotation, head pose changes in the dark, and various gaze interaction.},
  archive      = {J_TIP},
  author       = {Wei-Yen Hsu and Chi-Jui Chung},
  doi          = {10.1109/TIP.2020.3044209},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1369-1381},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A novel eye center localization method for head poses with large rotations},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Deep dehazing network with latent ensembling architecture
and adversarial learning. <em>TIP</em>, <em>30</em>, 1354–1368. (<a
href="https://doi.org/10.1109/TIP.2020.3044208">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing dehazing algorithms recover haze-free image by solving the hazy imaging model using estimated transmission map and global atmospheric light. However, inaccurate estimation of these variables and the strong assumptions of imaging model result in unrealistic dehazing results. In this paper, we use the adversarial game between a pair of neural networks to accomplish end-to-end photo-realistic dehazing. To avoid uniform contrast enhancement, the generator learns to simultaneously restore haze-free image and capture the non-uniformity of haze. The modules for the two tasks are assembled in sequential and parallel manners to enable information sharing at different levels, and the architecture of the generator implicitly forms an ensemble of dehazing models that allows for feature selection. A multi-scale discriminator competes with the generator by learning to detect dehazing artifacts and the inconsistency between dehazed image and the spatial variation of haze. Unlike existing works that penalize dehazing artifacts via hand-crafted loss, the proposed algorithm uses the identity mapping in the space of clear-scene images to regularize data-driven dehazing. The proposed work also addresses the adaptability of data-driven dehazing to high-level computer vision task. We propose a task-driven training strategy that can optimize the object detection performance on dehazed images without updating the parameters of object detector. Performance of the proposed algorithm is assessed on the RESIDE, I-Haze, and O-Haze benchmarks. The comparison with ten state-of-the-art algorithms shows that the proposed work is the best performer in most competitions.},
  archive      = {J_TIP},
  author       = {Yuenan Li and Yuhang Liu and Qixin Yan and Kuangshi Zhang},
  doi          = {10.1109/TIP.2020.3044208},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1354-1368},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Deep dehazing network with latent ensembling architecture and adversarial learning},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DASGIL: Domain adaptation for semantic and geometric-aware
image-based localization. <em>TIP</em>, <em>30</em>, 1342–1353. (<a
href="https://doi.org/10.1109/TIP.2020.3043875">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Long-Term visual localization under changing environments is a challenging problem in autonomous driving and mobile robotics due to season, illumination variance, etc. Image retrieval for localization is an efficient and effective solution to the problem. In this paper, we propose a novel multi-task architecture to fuse the geometric and semantic information into the multi-scale latent embedding representation for visual place recognition. To use the high-quality ground truths without any human effort, the effective multi-scale feature discriminator is proposed for adversarial training to achieve the domain adaptation from synthetic virtual KITTI dataset to real-world KITTI dataset. The proposed approach is validated on the Extended CMU-Seasons dataset and Oxford RobotCar dataset through a series of crucial comparison experiments, where our performance outperforms state-of-the-art baselines for retrieval-based localization and large-scale place recognition under the challenging environment.},
  archive      = {J_TIP},
  author       = {Hanjiang Hu and Zhijian Qiao and Ming Cheng and Zhe Liu and Hesheng Wang},
  doi          = {10.1109/TIP.2020.3043875},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1342-1353},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {DASGIL: Domain adaptation for semantic and geometric-aware image-based localization},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deconvolved image restoration from auto-correlations.
<em>TIP</em>, <em>30</em>, 1332–1341. (<a
href="https://doi.org/10.1109/TIP.2020.3043387">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recovery of a real signal from its auto-correlation is a wide-spread problem in computational imaging, and it is equivalent to retrieve the phase linked to a given Fourier modulus. Image-deconvolution, on the other hand, is a funda- mental aspect to take into account when we aim at increasing the resolution of blurred signals. These problems are addressed separately in a large number of experimental situations, ranging from adaptive astronomy to optical microscopy. Here, instead, we tackle both at the same time, performing auto-correlation inversion while deconvolving the current object estimation. To this end, we propose a method based on I -divergence optimization, turning our formalism into an iterative scheme inspired by Bayesian-based approaches. We demonstrate the method by recovering sharp signals from blurred auto-correlations, regardless of whether the blurring acts in auto-correlation, object, or Fourier domain.},
  archive      = {J_TIP},
  author       = {Daniele Ancora and Andrea Bassi},
  doi          = {10.1109/TIP.2020.3043387},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1332-1341},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Deconvolved image restoration from auto-correlations},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). BSNet: Bi-similarity network for few-shot fine-grained image
classification. <em>TIP</em>, <em>30</em>, 1318–1331. (<a
href="https://doi.org/10.1109/TIP.2020.3043128">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot learning for fine-grained image classification has gained recent attention in computer vision. Among the approaches for few-shot learning, due to the simplicity and effectiveness, metric-based methods are favorably state-of-the-art on many tasks. Most of the metric-based methods assume a single similarity measure and thus obtain a single feature space. However, if samples can simultaneously be well classified via two distinct similarity measures, the samples within a class can distribute more compactly in a smaller feature space, producing more discriminative feature maps. Motivated by this, we propose a so-called Bi-Similarity Network (BSNet) that consists of a single embedding module and a bi-similarity module of two similarity measures. After the support images and the query images pass through the convolution-based embedding module, the bi-similarity module learns feature maps according to two similarity measures of diverse characteristics. In this way, the model is enabled to learn more discriminative and less similarity-biased features from few shots of fine-grained images, such that the model generalization ability can be significantly improved. Through extensive experiments by slightly modifying established metric/similarity based networks, we show that the proposed approach produces a substantial improvement on several fine-grained image benchmark datasets. Codes are available at: https://github.com/PRIS-CV/BSNet.},
  archive      = {J_TIP},
  author       = {Xiaoxu Li and Jijie Wu and Zhuo Sun and Zhanyu Ma and Jie Cao and Jing-Hao Xue},
  doi          = {10.1109/TIP.2020.3043128},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1318-1331},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {BSNet: Bi-similarity network for few-shot fine-grained image classification},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dense attention fluid network for salient object detection
in optical remote sensing images. <em>TIP</em>, <em>30</em>, 1305–1317.
(<a href="https://doi.org/10.1109/TIP.2020.3042084">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the remarkable advances in visual saliency analysis for natural scene images (NSIs), salient object detection (SOD) for optical remote sensing images (RSIs) still remains an open and challenging problem. In this paper, we propose an end-to-end Dense Attention Fluid Network (DAFNet) for SOD in optical RSIs. A Global Context-aware Attention (GCA) module is proposed to adaptively capture long-range semantic context relationships, and is further embedded in a Dense Attention Fluid (DAF) structure that enables shallow attention cues flow into deep layers to guide the generation of high-level feature attention maps. Specifically, the GCA module is composed of two key components, where the global feature aggregation module achieves mutual reinforcement of salient feature embeddings from any two spatial locations, and the cascaded pyramid attention module tackles the scale variation issue by building up a cascaded pyramid framework to progressively refine the attention map in a coarse-to-fine manner. In addition, we construct a new and challenging optical RSI dataset for SOD that contains 2,000 images with pixel-wise saliency annotations, which is currently the largest publicly available benchmark. Extensive experiments demonstrate that our proposed DAFNet significantly outperforms the existing state-of-the-art SOD competitors. https://github.com/rmcong/DAFNet_TIP20.},
  archive      = {J_TIP},
  author       = {Qijian Zhang and Runmin Cong and Chongyi Li and Ming-Ming Cheng and Yuming Fang and Xiaochun Cao and Yao Zhao and Sam Kwong},
  doi          = {10.1109/TIP.2020.3042084},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1305-1317},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Dense attention fluid network for salient object detection in optical remote sensing images},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Interpreting and improving adversarial robustness of deep
neural networks with neuron sensitivity. <em>TIP</em>, <em>30</em>,
1291–1304. (<a href="https://doi.org/10.1109/TIP.2020.3042083">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks (DNNs) are vulnerable to adversarial examples where inputs with imperceptible perturbations mislead DNNs to incorrect results. Despite the potential risk they bring, adversarial examples are also valuable for providing insights into the weakness and blind-spots of DNNs. Thus, the interpretability of a DNN in the adversarial setting aims to explain the rationale behind its decision-making process and makes deeper understanding which results in better practical applications. To address this issue, we try to explain adversarial robustness for deep models from a new perspective of neuron sensitivity which is measured by neuron behavior variation intensity against benign and adversarial examples. In this paper, we first draw the close connection between adversarial robustness and neuron sensitivities, as sensitive neurons make the most non-trivial contributions to model predictions in the adversarial setting. Based on that, we further propose to improve adversarial robustness by stabilizing the behaviors of sensitive neurons. Moreover, we demonstrate that state-of-the-art adversarial training methods improve model robustness by reducing neuron sensitivities, which in turn confirms the strong connections between adversarial robustness and neuron sensitivity. Extensive experiments on various datasets demonstrate that our algorithm effectively achieves excellent results. To the best of our knowledge, we are the first to study adversarial robustness using neuron sensitivities.},
  archive      = {J_TIP},
  author       = {Chongzhi Zhang and Aishan Liu and Xianglong Liu and Yitao Xu and Hang Yu and Yuqing Ma and Tianlin Li},
  doi          = {10.1109/TIP.2020.3042083},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1291-1304},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Interpreting and improving adversarial robustness of deep neural networks with neuron sensitivity},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). KT-GAN: Knowledge-transfer generative adversarial network
for text-to-image synthesis. <em>TIP</em>, <em>30</em>, 1275–1290. (<a
href="https://doi.org/10.1109/TIP.2020.3026728">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a new framework, Knowledge-Transfer Generative Adversarial Network (KT-GAN), for fine-grained text-to-image generation. We introduce two novel mechanisms: an Alternate Attention-Transfer Mechanism (AATM) and a Semantic Distillation Mechanism (SDM), to help generator better bridge the cross-domain gap between text and image. The AATM updates word attention weights and attention weights of image sub-regions alternately, to progressively highlight important word information and enrich details of synthesized images. The SDM uses the image encoder trained in the Image-to-Image task to guide training of the text encoder in the Text-to-Image task, for generating better text features and higher-quality images. With extensive experimental validation on two public datasets, our KT-GAN outperforms the baseline method significantly, and also achieves the competive results over different evaluation metrics.},
  archive      = {J_TIP},
  author       = {Hongchen Tan and Xiuping Liu and Meng Liu and Baocai Yin and Xin Li},
  doi          = {10.1109/TIP.2020.3026728},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1275-1290},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {KT-GAN: Knowledge-transfer generative adversarial network for text-to-image synthesis},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A bilevel integrated model with data-driven layer ensemble
for multi-modality image fusion. <em>TIP</em>, <em>30</em>, 1261–1274.
(<a href="https://doi.org/10.1109/TIP.2020.3043125">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image fusion plays a critical role in a variety of vision and learning applications. Current fusion approaches are designed to characterize source images, focusing on a certain type of fusion task while limited in a wide scenario. Moreover, other fusion strategies (i.e., weighted averaging, choose-max) cannot undertake the challenging fusion tasks, which furthermore leads to undesirable artifacts facilely emerged in their fused results. In this paper, we propose a generic image fusion method with a bilevel optimization paradigm, targeting on multi-modality image fusion tasks. Corresponding alternation optimization is conducted on certain components decoupled from source images. Via adaptive integration weight maps, we are able to get the flexible fusion strategy across multi-modality images. We successfully applied it to three types of image fusion tasks, including infrared and visible, computed tomography and magnetic resonance imaging, and magnetic resonance imaging and single-photon emission computed tomography image fusion. Results highlight the performance and versatility of our approach from both quantitative and qualitative aspects.},
  archive      = {J_TIP},
  author       = {Risheng Liu and Jinyuan Liu and Zhiying Jiang and Xin Fan and Zhongxuan Luo},
  doi          = {10.1109/TIP.2020.3043125},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1261-1274},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A bilevel integrated model with data-driven layer ensemble for multi-modality image fusion},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Low-complexity error resilient HEVC video coding: A deep
learning approach. <em>TIP</em>, <em>30</em>, 1245–1260. (<a
href="https://doi.org/10.1109/TIP.2020.3043124">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intra/inter switching-based error resilient video coding effectively enhances the robustness of video streaming when transmitting over error-prone networks. But it has a high computation complexity, due to the detailed end-to-end distortion prediction and brute-force search for rate-distortion optimization. In this article, a Low Complexity Mode Switching based Error Resilient Encoding (LC-MSERE) method is proposed to reduce the complexity of the encoder through a deep learning approach. By designing and training multi-scale information fusion-based convolutional neural networks (CNN), intra and inter mode coding unit (CU) partitions can be predicted by the networks rapidly and accurately, instead of using brute-force search and a large number of end-to-end distortion estimations. In the intra CU partition prediction, we propose a spatial multi-scale information fusion based CNN (SMIF-Intra). In this network a shortcut convolution architecture is designed to learn the multi-scale and multi-grained image information, which is correlated with the CU partition. In the inter CU partition, we propose a spatial-temporal multi-scale information fusion-based CNN (STMIF-Inter), in which a two-stream convolution architecture is designed to learn the spatial-temporal image texture and the distortion propagation among frames. With information from the image, and coding and transmission parameters, the networks are able to accurately predict CU partitions for both intra and inter coding tree units (CTUs). Experiments show that our approach significantly reduces computation time for error resilient video encoding with acceptable quality decrement.},
  archive      = {J_TIP},
  author       = {Taiyu Wang and Fan Li and Xiaoya Qiao and Pamela C. Cosman},
  doi          = {10.1109/TIP.2020.3043124},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1245-1260},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Low-complexity error resilient HEVC video coding: A deep learning approach},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bio-inspired video enhancement for small moving target
detection. <em>TIP</em>, <em>30</em>, 1232–1244. (<a
href="https://doi.org/10.1109/TIP.2020.3043113">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Moving targets at a very large distance from a camera appear small and of low contrast. The low signal-to-noise-ratio and the presence of clutter in the background degrade the detection performance of conventional moving object detection techniques. To address these challenges, we propose temporal pre-processing of video frames using a biologically-inspired vision model. The bio-inspired model consists of multiple layers of processing analogous to the photoreceptor cells in the visual system of small insects. The adaptive filtering mechanism in the photoreceptor cells suppresses clutter and expands the possible range of input signal changes which improves the target background contrast. We perform experiments on real world video sequences of small moving targets captured with a high bit depth, high resolution and high frame-rate camera. Experimental results show that the biological vision based pre-processing leads to improved detection performance when used in conjunction with a variety of computer vision based moving object detection algorithms. The temporal bio-processing alone has improved the area under the receiver operating characteristic (AUROC) curve of the best performing algorithm by 75.4\%. Our results suggest that the bio-inspired pre-processing has strong potential to become a key component of a practical small target detection system.},
  archive      = {J_TIP},
  author       = {Muhammad Uzair and Russell SA Brinkworth and Anthony Finn},
  doi          = {10.1109/TIP.2020.3043113},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1232-1244},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Bio-inspired video enhancement for small moving target detection},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning spatial attention for face super-resolution.
<em>TIP</em>, <em>30</em>, 1219–1231. (<a
href="https://doi.org/10.1109/TIP.2020.3043093">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {General image super-resolution techniques have difficulties in recovering detailed face structures when applying to low resolution face images. Recent deep learning based methods tailored for face images have achieved improved performance by jointly trained with additional task such as face parsing and landmark prediction. However, multi-task learning requires extra manually labeled data. Besides, most of the existing works can only generate relatively low resolution face images (e.g., 128 × 128), and their applications are therefore limited. In this paper, we introduce a novel SPatial Attention Residual Network (SPARNet) built on our newly proposed Face Attention Units (FAUs) for face super-resolution. Specifically, we introduce a spatial attention mechanism to the vanilla residual blocks. This enables the convolutional layers to adaptively bootstrap features related to the key face structures and pay less attention to those less feature-rich regions. This makes the training more effective and efficient as the key face structures only account for a very small portion of the face image. Visualization of the attention maps shows that our spatial attention network can capture the key face structures well even for very low resolution faces (e.g., 16×16). Quantitative comparisons on various kinds of metrics (including PSNR, SSIM, identity similarity, and landmark detection) demonstrate the superiority of our method over current state-of-the-arts. We further extend SPARNet with multi-scale discriminators, named as SPARNetHD, to produce high resolution results (i.e., 512×512). We show that SPARNetHD trained with synthetic data can not only produce high quality and high resolution outputs for synthetically degraded face images, but also show good generalization ability to real world low quality face images. Codes are available at https://github.com/chaofengc/Face-SPARNet.},
  archive      = {J_TIP},
  author       = {Chaofeng Chen and Dihong Gong and Hao Wang and Zhifeng Li and Kwan-Yee K. Wong},
  doi          = {10.1109/TIP.2020.3043093},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1219-1231},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Learning spatial attention for face super-resolution},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Insights into algorithms for separable nonlinear least
squares problems. <em>TIP</em>, <em>30</em>, 1207–1218. (<a
href="https://doi.org/10.1109/TIP.2020.3043087">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Separable nonlinear least squares (SNLLS) problems have attracted interest in a wide range of research fields such as machine learning, computer vision, and signal processing. During the past few decades, several algorithms, including the joint optimization algorithm, alternated least squares (ALS) algorithm, embedded point iterations (EPI) algorithm, and variable projection (VP) algorithms, have been employed for solving SNLLS problems in the literature. The VP approach has been proven to be quite valuable for SNLLS problems and the EPI method has been successful in solving many computer vision tasks. However, no clear explanations about the intrinsic relationships of these algorithms have been provided in the literature. In this paper, we give some insights into these algorithms for SNLLS problems. We derive the relationships among different forms of the VP algorithms, EPI algorithm and ALS algorithm. In addition, the convergence and robustness of some algorithms are investigated. Moreover, the analysis of the VP algorithm generates a negative answer to Kaufman&#39;s conjecture. Numerical experiments on the image restoration task, fitting the time series data using the radial basis function network based autoregressive (RBF-AR) model, and bundle adjustment are given to compare the performance of different algorithms.},
  archive      = {J_TIP},
  author       = {Guang-Yong Chen and Min Gan and Shuqiang Wang and C. L. Philip Chen},
  doi          = {10.1109/TIP.2020.3043087},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1207-1218},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Insights into algorithms for separable nonlinear least squares problems},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hypergraph spectral analysis and processing in 3D point
cloud. <em>TIP</em>, <em>30</em>, 1193–1206. (<a
href="https://doi.org/10.1109/TIP.2020.3042088">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Along with increasingly popular virtual reality applications, the three-dimensional (3D) point cloud has become a fundamental data structure to characterize 3D objects and surroundings. To process 3D point clouds efficiently, a suitable model for the underlying structure and outlier noises is always critical. In this work, we propose a hypergraph-based new point cloud model that is amenable to efficient analysis and processing. We introduce tensor-based methods to estimate hypergraph spectrum components and frequency coefficients of point clouds in both ideal and noisy settings. We establish an analytical connection between hypergraph frequencies and structural features. We further evaluate the efficacy of hypergraph spectrum estimation in two common applications of sampling and denoising of point clouds for which we provide specific hypergraph filter design and spectral properties. Experimental results demonstrate the strength of hypergraph signal processing as a tool in characterizing the underlying properties of 3D point clouds.},
  archive      = {J_TIP},
  author       = {Songyang Zhang and Shuguang Cui and Zhi Ding},
  doi          = {10.1109/TIP.2020.3042088},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1193-1206},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Hypergraph spectral analysis and processing in 3D point cloud},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Cross-domain image captioning via cross-modal retrieval and
model adaptation. <em>TIP</em>, <em>30</em>, 1180–1192. (<a
href="https://doi.org/10.1109/TIP.2020.3042086">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, large scale datasets of paired images and sentences have enabled the remarkable success in automatically generating descriptions for images, namely image captioning. However, it is labour-intensive and time-consuming to collect a sufficient number of paired images and sentences in each domain. It may be beneficial to transfer the image captioning model trained in an existing domain with pairs of images and sentences (i.e., source domain) to a new domain with only unpaired data (i.e., target domain). In this paper, we propose a cross-modal retrieval aided approach to cross-domain image captioning that leverages a cross-modal retrieval model to generate pseudo pairs of images and sentences in the target domain to facilitate the adaptation of the captioning model. To learn the correlation between images and sentences in the target domain, we propose an iterative cross-modal retrieval process where a cross-modal retrieval model is first pre-trained using the source domain data and then applied to the target domain data to acquire an initial set of pseudo image-sentence pairs. The pseudo image-sentence pairs are further refined by iteratively fine-tuning the retrieval model with the pseudo image-sentence pairs and updating the pseudo image-sentence pairs using the retrieval model. To make the linguistic patterns of the sentences learned in the source domain adapt well to the target domain, we propose an adaptive image captioning model with a self-attention mechanism fine-tuned using the refined pseudo image-sentence pairs. Experimental results on several settings where MSCOCO is used as the source domain and five different datasets (Flickr30k, TGIF, CUB-200, Oxford-102 and Conceptual) are used as the target domains demonstrate that our method achieves mostly better or comparable performance against the state-of-the-art methods. We also extend our method to cross-domain video captioning where MSR-VTT is used as the source domain and two other datasets (MSVD and Charades Captions) are used as the target domains to further demonstrate the effectiveness of our method.},
  archive      = {J_TIP},
  author       = {Wentian Zhao and Xinxiao Wu and Jiebo Luo},
  doi          = {10.1109/TIP.2020.3042086},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1180-1192},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Cross-domain image captioning via cross-modal retrieval and model adaptation},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). CGNet: A light-weight context guided network for semantic
segmentation. <em>TIP</em>, <em>30</em>, 1169–1179. (<a
href="https://doi.org/10.1109/TIP.2020.3042065">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The demand of applying semantic segmentation model on mobile devices has been increasing rapidly. Current state-of-the-art networks have enormous amount of parameters hence unsuitable for mobile devices, while other small memory footprint models follow the spirit of classification network and ignore the inherent characteristic of semantic segmentation. To tackle this problem, we propose a novel Context Guided Network (CGNet), which is a light-weight and efficient network for semantic segmentation. We first propose the Context Guided (CG) block, which learns the joint feature of both local feature and surrounding context effectively and efficiently, and further improves the joint feature with the global context. Based on the CG block, we develop CGNet which captures contextual information in all stages of the network. CGNet is specially tailored to exploit the inherent property of semantic segmentation and increase the segmentation accuracy. Moreover, CGNet is elaborately designed to reduce the number of parameters and save memory footprint. Under an equivalent number of parameters, the proposed CGNet significantly outperforms existing light-weight segmentation networks. Extensive experiments on Cityscapes and CamVid datasets verify the effectiveness of the proposed approach. Specifically, without any post-processing and multi-scale testing, the proposed CGNet achieves 64.8\% mean IoU on Cityscapes with less than 0.5 M parameters.},
  archive      = {J_TIP},
  author       = {Tianyi Wu and Sheng Tang and Rui Zhang and Juan Cao and Yongdong Zhang},
  doi          = {10.1109/TIP.2020.3042065},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1169-1179},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {CGNet: A light-weight context guided network for semantic segmentation},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). FFD: Fast feature detector. <em>TIP</em>, <em>30</em>,
1153–1168. (<a href="https://doi.org/10.1109/TIP.2020.3042057">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scale-invariance, good localization and robustness to noise and distortions are the main properties that a local feature detector should possess. Most existing local feature detectors find excessive unstable feature points that increase the number of keypoints to be matched and the computational time of the matching step. In this paper, we show that robust and accurate keypoints exist in the specific scale-space domain. To this end, we first formulate the superimposition problem into a mathematical model and then derive a closed-form solution for multiscale analysis. The model is formulated via difference-of-Gaussian (DoG) kernels in the continuous scale-space domain, and it is proved that setting the scale-space pyramid&#39;s blurring ratio and smoothness to 2 and 0.627, respectively, facilitates the detection of reliable keypoints. For the applicability of the proposed model to discrete images, we discretize it using the undecimated wavelet transform and the cubic spline function. Theoretically, the complexity of our method is less than 5\% of that of the popular baseline Scale Invariant Feature Transform (SIFT). Extensive experimental results show the superiority of the proposed feature detector over the existing representative hand-crafted and learning-based techniques in accuracy and computational time. The code and supplementary materials can be found at https://github.com/mogvision/FFD.},
  archive      = {J_TIP},
  author       = {Morteza Ghahremani and Yonghuai Liu and Bernard Tiddeman},
  doi          = {10.1109/TIP.2020.3042057},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1153-1168},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {FFD: Fast feature detector},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning to anticipate egocentric actions by imagination.
<em>TIP</em>, <em>30</em>, 1143–1152. (<a
href="https://doi.org/10.1109/TIP.2020.3040521">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anticipating actions before they are executed is crucial for a wide range of practical applications, including autonomous driving and robotics. In this paper, we study the egocentric action anticipation task, which predicts future action seconds before it is performed for egocentric videos. Previous approaches focus on summarizing the observed content and directly predicting future action based on past observations. We believe it would benefit the action anticipation if we could mine some cues to compensate for the missing information of the unobserved frames. We then propose to decompose the action anticipation into a series of future feature predictions. We imagine how the visual feature changes in the near future and then predicts future action labels based on these imagined representations. Differently, our ImagineRNN is optimized in a contrastive learning way instead of feature regression. We utilize a proxy task to train the ImagineRNN, i.e. , selecting the correct future states from distractors. We further improve ImagineRNN by residual anticipation, i.e. , changing its target to predicting the feature difference of adjacent frames instead of the frame content. This promotes the network to focus on our target, i.e. , the future action, as the difference between adjacent frame features is more important for forecasting the future. Extensive experiments on two large-scale egocentric action datasets validate the effectiveness of our method. Our method significantly outperforms previous methods on both the seen test set and the unseen test set of the EPIC Kitchens Action Anticipation Challenge.},
  archive      = {J_TIP},
  author       = {Yu Wu and Linchao Zhu and Xiaohan Wang and Yi Yang and Fei Wu},
  doi          = {10.1109/TIP.2020.3040521},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1143-1152},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Learning to anticipate egocentric actions by imagination},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A scalable optimization mechanism for pairwise based
discrete hashing. <em>TIP</em>, <em>30</em>, 1130–1142. (<a
href="https://doi.org/10.1109/TIP.2020.3040536">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Maintaining the pairwise relationship among originally high-dimensional data into a low-dimensional binary space is a popular strategy to learn binary codes. One simple and intuitive method is to utilize two identical code matrices produced by hash functions to approximate a pairwise real label matrix. However, the resulting quartic problem in term of hash functions is difficult to directly solve due to the non-convex and non-smooth nature of the objective. In this paper, unlike previous optimization methods using various relaxation strategies, we aim to directly solve the original quartic problem using a novel alternative optimization mechanism to linearize the quartic problem by introducing a linear regression model. Additionally, we find that gradually learning each batch of binary codes in a sequential mode, i.e. batch by batch, is greatly beneficial to the convergence of binary code learning. Based on this significant discovery and the proposed strategy, we introduce a scalable symmetric discrete hashing algorithm that gradually and smoothly updates each batch of binary codes. To further improve the smoothness, we also propose a greedy symmetric discrete hashing algorithm to update each bit of batch binary codes. Moreover, we extend the proposed optimization mechanism to solve the non-convex optimization problems for binary code learning in many other pairwise based hashing algorithms. Extensive experiments on benchmark single-label and multi-label databases demonstrate the superior performance of the proposed mechanism over recent state-of-the-art methods on two kinds of retrieval tasks: similarity and ranking order. The source codes are available on https://github.com/xsshi2015/Scalable-Pairwise-based-Discrete-Hashing.},
  archive      = {J_TIP},
  author       = {Xiaoshuang Shi and Fuyong Xing and Zizhao Zhang and Manish Sapkota and Zhenhua Guo and Lin Yang},
  doi          = {10.1109/TIP.2020.3040536},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1130-1142},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A scalable optimization mechanism for pairwise based discrete hashing},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning guided convolutional network for depth completion.
<em>TIP</em>, <em>30</em>, 1116–1129. (<a
href="https://doi.org/10.1109/TIP.2020.3040528">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dense depth perception is critical for autonomous driving and other robotics applications. However, modern LiDAR sensors only provide sparse depth measurement. It is thus necessary to complete the sparse LiDAR data, where a synchronized guidance RGB image is often used to facilitate this completion. Many neural networks have been designed for this task. However, they often naïvely fuse the LiDAR data and RGB image information by performing feature concatenation or element-wise addition. Inspired by the guided image filtering, we design a novel guided network to predict kernel weights from the guidance image. These predicted kernels are then applied to extract the depth image features. In this way, our network generates content-dependent and spatially-variant kernels for multi-modal feature fusion. Dynamically generated spatially-variant kernels could lead to prohibitive GPU memory consumption and computation overhead. We further design a convolution factorization to reduce computation and memory consumption. The GPU memory reduction makes it possible for feature fusion to work in multi-stage scheme. We conduct comprehensive experiments to verify our method on real-world outdoor, indoor and synthetic datasets. Our method produces strong results. It outperforms state-of-the-art methods on the NYUv2 dataset and ranks 1st on the KITTI depth completion benchmark at the time of submission. It also presents strong generalization capability under different 3D point densities, various lighting and weather conditions as well as cross-dataset evaluations. The code will be released for reproduction.},
  archive      = {J_TIP},
  author       = {Jie Tang and Fei-Peng Tian and Wei Feng and Jian Li and Ping Tan},
  doi          = {10.1109/TIP.2020.3040528},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1116-1129},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Learning guided convolutional network for depth completion},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep retinex network for single image dehazing.
<em>TIP</em>, <em>30</em>, 1100–1115. (<a
href="https://doi.org/10.1109/TIP.2020.3040075">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a retinex-based decomposition model for a hazy image and a novel end-to-end image dehazing network. In the model, the illumination of the hazy image is decomposed into natural illumination for the haze-free image and residual illumination caused by haze. Based on this model, we design a deep retinex dehazing network (RDN) to jointly estimate the residual illumination map and the haze-free image. Our RDN consists of a multiscale residual dense network for estimating the residual illumination map and a U-Net with channel and spatial attention mechanisms for image dehazing. The multiscale residual dense network can simultaneously capture global contextual information from small-scale receptive fields and local detailed information from large-scale receptive fields to precisely estimate the residual illumination map caused by haze. In the dehazing U-Net, we apply the channel and spatial attention mechanisms in the skip connection of the U-Net to achieve a trade-off between overdehazing and underdehazing by automatically adjusting the channel-wise and pixel-wise attention weights. Compared with scattering model-based networks, fully data-driven networks, and prior-based dehazing methods, our RDN can avoid the errors associated with the simplified scattering model and provide better generalization ability with no dependence on prior information. Extensive experiments show the superiority of the RDN to various state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Pengyue Li and Jiandong Tian and Yandong Tang and Guolin Wang and Chengdong Wu},
  doi          = {10.1109/TIP.2020.3040075},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1100-1115},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Deep retinex network for single image dehazing},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Rethinking shape from shading for spoofing detection.
<em>TIP</em>, <em>30</em>, 1086–1099. (<a
href="https://doi.org/10.1109/TIP.2020.3042082">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spoofing attacks are critical threats to modern face recognition systems, and most common countermeasures exploit 2D texture features as they are easy to extract and deploy. 3D shape-based methods can substantially improve spoofing prevention, but extracting the 3D shape of the face often requires complex hardware such as a 3D scanner and expensive computation. Motivated by the classical shape-from-shading model, we propose to obtain 3D facial features that can be used to recognize the presence of an actual 3D face, without explicit shape reconstruction. Such shading-based 3D features are extracted highly efficiently from a pair of images captured under different illumination, e.g., two images captured with and without flash. Thus the proposed method provides a rich 3D geometrical representation at negligible computational cost and minimal to none additional hardware. A theoretical analysis is provided to support why such simple 3D features can effectively describe the presence of an actual 3D shape while avoiding complicated calibration steps or hardware setup. Experimental validation shows that the proposed method can produce state-of-the-art spoofing prevention and enhance existing texture-based solutions.},
  archive      = {J_TIP},
  author       = {J. Matías Di Martino and Qiang Qiu and Guillermo Sapiro},
  doi          = {10.1109/TIP.2020.3042082},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1086-1099},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Rethinking shape from shading for spoofing detection},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Low-rank constrained super-resolution for mixed-resolution
multiview video. <em>TIP</em>, <em>30</em>, 1072–1085. (<a
href="https://doi.org/10.1109/TIP.2020.3042064">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiview video allows for simultaneously presenting dynamic imaging from multiple viewpoints, enabling a broad range of immersive applications. This paper proposes a novel super-resolution (SR) approach to mixed-resolution (MR) multiview video, whereby the low-resolution (LR) videos produced by MR camera setups are up-sampled based on the neighboring HR videos. Our solution analyzes the statistical correlation of different resolutions between multiple views, and introduces a low-rank prior based SR optimization framework using local linear embedding and weighted nuclear norm minimization. The target HR patch is reconstructed by learning texture details from the neighboring HR camera views using local linear embedding. A low-rank constrained patch optimization solution is introduced to effectively restrain visual artifacts and the ADMM framework is used to solve the resulting optimization problem. Comprehensive experiments including objective and subjective test metrics demonstrate that the proposed method outperforms the state-of-the-art SR methods for MR multiview video.},
  archive      = {J_TIP},
  author       = {Shao-Ping Lu and Sen-Mao Li and Rong Wang and Gauthier Lafruit and Ming-Ming Cheng and Adrian Munteanu},
  doi          = {10.1109/TIP.2020.3042064},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1072-1085},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Low-rank constrained super-resolution for mixed-resolution multiview video},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Light field image super-resolution using deformable
convolution. <em>TIP</em>, <em>30</em>, 1057–1071. (<a
href="https://doi.org/10.1109/TIP.2020.3042059">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Light field (LF) cameras can record scenes from multiple perspectives, and thus introduce beneficial angular information for image super-resolution (SR). However, it is challenging to incorporate angular information due to disparities among LF images. In this paper, we propose a deformable convolution network (i.e., LF-DFnet) to handle the disparity problem for LF image SR. Specifically, we design an angular deformable alignment module (ADAM) for feature-level alignment. Based on ADAM, we further propose a collect-and-distribute approach to perform bidirectional alignment between the center-view feature and each side-view feature. Using our approach, angular information can be well incorporated and encoded into features of each view, which benefits the SR reconstruction of all LF images. Moreover, we develop a baseline-adjustable LF dataset to evaluate SR performance under different disparity variations. Experiments on both public and our self-developed datasets have demonstrated the superiority of our method. Our LF-DFnet can generate high-resolution images with more faithful details and achieve state-of-the-art reconstruction accuracy. Besides, our LF-DFnet is more robust to disparity variations, which has not been well addressed in literature.},
  archive      = {J_TIP},
  author       = {Yingqian Wang and Jungang Yang and Longguang Wang and Xinyi Ying and Tianhao Wu and Wei An and Yulan Guo},
  doi          = {10.1109/TIP.2020.3042059},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1057-1071},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Light field image super-resolution using deformable convolution},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Novel convergence results of adaptive stochastic gradient
descents. <em>TIP</em>, <em>30</em>, 1044–1056. (<a
href="https://doi.org/10.1109/TIP.2020.3038535">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adaptive stochastic gradient descent, which uses unbiased samples of the gradient with stepsizes chosen from the historical information, has been widely used to train neural networks for computer vision and pattern recognition tasks. This paper revisits the theoretical aspects of two classes of adaptive stochastic gradient descent methods, which contain several existing state-of-the-art schemes. We focus on the presentation of novel findings: In the general smooth case, the nonergodic convergence results are given, that is, the expectation of the gradients&#39; norm rather than the minimum of past iterates is proved to converge; We also studied their performances under Polyak-Łojasiewicz property on the objective function. In this case, the nonergodic convergence rates are given for the expectation of the function values. Our findings show that more substantial restrictions on the steps are needed to guarantee the nonergodic function values&#39; convergence (rates).},
  archive      = {J_TIP},
  author       = {Tao Sun and Linbo Qiao and Qing Liao and Dongsheng Li},
  doi          = {10.1109/TIP.2020.3038535},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1044-1056},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Novel convergence results of adaptive stochastic gradient descents},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Blind deconvolution for poissonian blurred image with total
variation and l0-norm gradient regularizations. <em>TIP</em>,
<em>30</em>, 1030–1043. (<a
href="https://doi.org/10.1109/TIP.2020.3038518">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a regularized blind deconvolution method for restoring Poissonian blurred image. The problem is formulated by utilizing the L 0 -norm of image gradients and total variation (TV) to regularize the latent image and point spread function (PSF), respectively, and combining them with the negative logarithmic Poisson log-likelihood. To solve the problem, we propose an approach which combines the methods of variable splitting and Lagrange multiplier to convert the original problem into three sub-problems, and then design an alternating minimization algorithm which incorporates the estimation of PSF and latent image as well as the updation of Lagrange multiplier into account. We also design a non-blind deconvolution method based on TV regularization to further improve the quality of the restored image. Experimental results on both synthetic and real-world Poissonian blurred images show that the proposed method can achieve restored images of very high quality, which is competitive with or even better than some state of the art methods.},
  archive      = {J_TIP},
  author       = {Wende Dong and Shuyin Tao and Guili Xu and Yueting Chen},
  doi          = {10.1109/TIP.2020.3038518},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1030-1043},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Blind deconvolution for poissonian blurred image with total variation and l0-norm gradient regularizations},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Face video retrieval based on the deep CNN with RBF loss.
<em>TIP</em>, <em>30</em>, 1015–1029. (<a
href="https://doi.org/10.1109/TIP.2020.3040847">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a novel framework to extract highly compact and discriminative features for face video retrieval tasks using the deep convolutional neural network (CNN). The face video retrieval task is to find the videos containing the face of a specific person from a database with a face image or a face video of the same person as a query. A key challenge is to extract discriminative features with small storage space from face videos with large intra-class variations caused by different angle, illumination, and facial expression. In recent years, the CNN-based binary hashing and metric learning methods showed notable progress in image/video retrieval tasks. However, the existing CNN-based binary hashing and metric learning have limitations in terms of inevitable information loss and storage inefficiency, respectively. To cope with these problems, the proposed framework consists of two parts: first, a novel loss function using a radial basis function kernel (RBF Loss) is introduced to train a neural network to generate compact and discriminative high-level features, and secondly, an optimized quantization using a logistic function (Logistic Quantization) is suggested to convert a real-valued feature to a 1-byte integer with the minimum information loss. Through the face video retrieval experiments on a challenging TV series data set (ICT-TV), it is demonstrated that the proposed framework outperforms the existing state-of-the-art feature extraction methods. Furthermore, the effectiveness of RBF loss was also demonstrated through the image classification and retrieval experiments on the CIFAR-10 and Fashion-MNIST data sets with LeNet-5.},
  archive      = {J_TIP},
  author       = {Young Rok Choi and Rhee Man Kil},
  doi          = {10.1109/TIP.2020.3040847},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1015-1029},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Face video retrieval based on the deep CNN with RBF loss},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning complete and discriminative direction pattern for
robust palmprint recognition. <em>TIP</em>, <em>30</em>, 1001–1014. (<a
href="https://doi.org/10.1109/TIP.2020.3039895">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Palmprint direction patterns have been widely and successfully used in palmprint recognition methods. Most existing direction-based methods utilize the pre-defined filters to achieve the genuine line responses in the palmprint image, which requires rich prior knowledge and usually ignores the vital direction information. In addition, some line responses influenced by noise will degrade the recognition accuracy. Furthermore, how to extract the discriminative features to make the palmprint more separable is also a dilemma for improving the recognition performance. To solve these problems, we propose to learn complete and discriminative direction patterns in this study. We first extract the complete and salient local direction patterns, which contains a complete local direction feature (CLDF) and a salient convolution difference feature (SCDF) extracted from the palmprint image. Afterwards, two learning models are proposed to learn sparse and discriminative directions from CLDF and to achieve the underlying structure for the SCDFs in the training samples, respectively. Lastly, the projected CLDF and the projected SCDF are concatenated forming the complete and discriminative direction feature for palmprint recognition. Experimental results on seven palmprint databases, as well as three noisy datasets clearly demonstrates the effectiveness of the proposed method.},
  archive      = {J_TIP},
  author       = {Shuping Zhao and Bob Zhang},
  doi          = {10.1109/TIP.2020.3039895},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1001-1014},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Learning complete and discriminative direction pattern for robust palmprint recognition},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Asymmetric supervised consistent and specific hashing for
cross-modal retrieval. <em>TIP</em>, <em>30</em>, 986–1000. (<a
href="https://doi.org/10.1109/TIP.2020.3038365">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hashing-based techniques have provided attractive solutions to cross-modal similarity search when addressing vast quantities of multimedia data. However, existing cross-modal hashing (CMH) methods face two critical limitations: 1) there is no previous work that simultaneously exploits the consistent or modality-specific information of multi-modal data; 2) the discriminative capabilities of pairwise similarity is usually neglected due to the computational cost and storage overhead. Moreover, to tackle the discrete constraints, relaxation-based strategy is typically adopted to relax the discrete problem to the continuous one, which severely suffers from large quantization errors and leads to sub-optimal solutions. To overcome the above limitations, in this article, we present a novel supervised CMH method, namely Asymmetric Supervised Consistent and Specific Hashing (ASCSH). Specifically, we explicitly decompose the mapping matrices into the consistent and modality-specific ones to sufficiently exploit the intrinsic correlation between different modalities. Meanwhile, a novel discrete asymmetric framework is proposed to fully explore the supervised information, in which the pairwise similarity and semantic labels are jointly formulated to guide the hash code learning process. Unlike existing asymmetric methods, the discrete asymmetric structure developed is capable of solving the binary constraint problem discretely and efficiently without any relaxation. To validate the effectiveness of the proposed approach, extensive experiments on three widely used datasets are conducted and encouraging results demonstrate the superiority of ASCSH over other state-of-the-art CMH methods.},
  archive      = {J_TIP},
  author       = {Min Meng and Haitao Wang and Jun Yu and Hui Chen and Jigang Wu},
  doi          = {10.1109/TIP.2020.3038365},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {986-1000},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Asymmetric supervised consistent and specific hashing for cross-modal retrieval},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Self-supervised deep correlation tracking. <em>TIP</em>,
<em>30</em>, 976–985. (<a
href="https://doi.org/10.1109/TIP.2020.3037518">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The training of a feature extraction network typically requires abundant manually annotated training samples, making this a time-consuming and costly process. Accordingly, we propose an effective self-supervised learning-based tracker in a deep correlation framework (named: self-SDCT). Motivated by the forward-backward tracking consistency of a robust tracker, we propose a multi-cycle consistency loss as self-supervised information for learning feature extraction network from adjacent video frames. At the training stage, we generate pseudo-labels of consecutive video frames by forward-backward prediction under a Siamese correlation tracking framework and utilize the proposed multi-cycle consistency loss to learn a feature extraction network. Furthermore, we propose a similarity dropout strategy to enable some low-quality training sample pairs to be dropped and also adopt a cycle trajectory consistency loss in each sample pair to improve the training loss function. At the tracking stage, we employ the pre-trained feature extraction network to extract features and utilize a Siamese correlation tracking framework to locate the target using forward tracking alone. Extensive experimental results indicate that the proposed self-supervised deep correlation tracker (self-SDCT) achieves competitive tracking performance contrasted to state-of-the-art supervised and unsupervised tracking methods on standard evaluation benchmarks.},
  archive      = {J_TIP},
  author       = {Di Yuan and Xiaojun Chang and Po-Yao Huang and Qiao Liu and Zhenyu He},
  doi          = {10.1109/TIP.2020.3037518},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {976-985},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Self-supervised deep correlation tracking},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Ultra high fidelity deep image decompression with
l∞-constrained compression. <em>TIP</em>, <em>30</em>, 963–975. (<a
href="https://doi.org/10.1109/TIP.2020.3040074">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel asymmetric image compression system of light ℓ ∞ -constrained predictive encoding and heavy-duty CNN-based soft decoding. The system achieves superior rate-distortion performances over the best of existing image compression methods, including BPG, WebP, FLIF and recent CNN codecs, in both ℓ 2 and ℓ ∞ error metrics, for bit rates near or above the threshold of perceptually transparent reconstruction. These remarkable coding gains are made by deep learning for compression artifact removal. A restoration CNN is designed to map a lossy compressed image to its original. Its unique strength is to enforce a tight error bound on a per pixel basis. As such, no small distinctive structures of the original image can be dropped or distorted, even if they are statistical outliers that are otherwise sacrificed by mainstream CNN restoration methods.},
  archive      = {J_TIP},
  author       = {Xi Zhang and Xiaolin Wu},
  doi          = {10.1109/TIP.2020.3040074},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {963-975},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Ultra high fidelity deep image decompression with l∞-constrained compression},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DSNet: A flexible detect-to-summarize network for video
summarization. <em>TIP</em>, <em>30</em>, 948–962. (<a
href="https://doi.org/10.1109/TIP.2020.3039886">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a Detect-to-Summarize network (DSNet) framework for supervised video summarization. Our DSNet contains anchor-based and anchor-free counterparts. The anchor-based method generates temporal interest proposals to determine and localize the representative contents of video sequences, while the anchor-free method eliminates the pre-defined temporal proposals and directly predicts the importance scores and segment locations. Different from existing supervised video summarization methods which formulate video summarization as a regression problem without temporal consistency and integrity constraints, our interest detection framework is the first attempt to leverage temporal consistency via the temporal interest detection formulation. Specifically, in the anchor-based approach, we first provide a dense sampling of temporal interest proposals with multi-scale intervals that accommodate interest variations in length, and then extract their long-range temporal features for interest proposal location regression and importance prediction. Notably, positive and negative segments are both assigned for the correctness and completeness information of the generated summaries. In the anchor-free approach, we alleviate drawbacks of temporal proposals by directly predicting importance scores of video frames and segment locations. Particularly, the interest detection framework can be flexibly plugged into off-the-shelf supervised video summarization methods. We evaluate the anchor-based and anchor-free approaches on the SumMe and TVSum datasets. Experimental results clearly validate the effectiveness of the anchor-based and anchor-free approaches.},
  archive      = {J_TIP},
  author       = {Wencheng Zhu and Jiwen Lu and Jiahao Li and Jie Zhou},
  doi          = {10.1109/TIP.2020.3039886},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {948-962},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {DSNet: A flexible detect-to-summarize network for video summarization},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Ratio-and-scale-aware YOLO for pedestrian detection.
<em>TIP</em>, <em>30</em>, 934–947. (<a
href="https://doi.org/10.1109/TIP.2020.3039574">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current deep learning methods seldom consider the effects of small pedestrian ratios and considerable differences in the aspect ratio of input images, which results in low pedestrian detection performance. This study proposes the ratio-and-scale-aware YOLO (RSA-YOLO) method to solve the aforementioned problems. The following procedure is adopted in this method. First, ratio-aware mechanisms are introduced to dynamically adjust the input layer length and width hyperparameters of YOLOv3, thereby solving the problem of considerable differences in the aspect ratio. Second, intelligent splits are used to automatically and appropriately divide the original images into two local images. Ratio-aware YOLO (RA-YOLO) is iteratively performed on the two local images. Because the original and local images produce low- and high-resolution pedestrian detection information after RA-YOLO, respectively, this study proposes new scale-aware mechanisms in which multiresolution fusion is used to solve the problem of misdetection of remarkably small pedestrians in images. The experimental results indicate that the proposed method produces favorable results for images with extremely small objects and those with considerable differences in the aspect ratio. Compared with the original YOLOs (i.e., YOLOv2 and YOLOv3) and several state-of-the-art approaches, the proposed method demonstrated a superior performance for the VOC 2012 comp4, INRIA, and ETH databases in terms of the average precision, intersection over union, and lowest log-average miss rate.},
  archive      = {J_TIP},
  author       = {Wei-Yen Hsu and Wen-Yen Lin},
  doi          = {10.1109/TIP.2020.3039574},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {934-947},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Ratio-and-scale-aware YOLO for pedestrian detection},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). New approaches for monitoring image data. <em>TIP</em>,
<em>30</em>, 921–933. (<a
href="https://doi.org/10.1109/TIP.2020.3039389">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we develop new techniques for monitoring image processes under a fairly general setting with spatially correlated pixels in the image. Monitoring and handling the pixels directly is infeasible due to an extremely high image resolution. To overcome this problem, we suggest control charts that are based on regions of interest. The regions of interest cover the original image which leads to a dimension reduction. Nevertheless, the data are still high-dimensional. We consider residual charts based on the generalized likelihood ratio approach. Existing control statistics typically depend on the inverse of the covariance matrix of the process, involving high computing times and frequently generating instable results in a high-dimensional setting. As a solution of this issue, we suggest two further control charts that can be regarded as modifications of the generalized likelihood ratio statistic. Within an extensive simulation study, we compare the newly proposed control charts using the median run length as a performance criterion.},
  archive      = {J_TIP},
  author       = {Yarema Okhrin and Wolfgang Schmid and Ivan Semeniuk},
  doi          = {10.1109/TIP.2020.3039389},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {921-933},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {New approaches for monitoring image data},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Push for center learning via orthogonalization and subspace
masking for person re-identification. <em>TIP</em>, <em>30</em>,
907–920. (<a href="https://doi.org/10.1109/TIP.2020.3036720">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person re-identification aims to identify whether pairs of images belong to the same person or not. This problem is challenging due to large differences in camera views, lighting and background. One of the mainstream in learning CNN features is to design loss functions which reinforce both the class separation and intra-class compactness. In this paper, we propose a novel Orthogonal Center Learning method with Subspace Masking for person re-identification. We make the following contributions: 1) we develop a center learning module to learn the class centers by simultaneously reducing the intra-class differences and inter-class correlations by orthogonalization; 2) we introduce a subspace masking mechanism to enhance the generalization of the learned class centers; and 3) we propose to integrate the average pooling and max pooling in a regularizing manner that fully exploits their powers. Extensive experiments show that our proposed method consistently outperforms the state-of-the-art methods on large-scale ReID datasets including Market-1501, DukeMTMC-ReID, CUHK03 and MSMT17.},
  archive      = {J_TIP},
  author       = {Weinong Wang and Wenjie Pei and Qiong Cao and Shu Liu and Guangming Lu and Yu-Wing Tai},
  doi          = {10.1109/TIP.2020.3036720},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {907-920},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Push for center learning via orthogonalization and subspace masking for person re-identification},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Attention guided multiple source and target domain
adaptation. <em>TIP</em>, <em>30</em>, 892–906. (<a
href="https://doi.org/10.1109/TIP.2020.3031161">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain adaptation aims to alleviate the distribution discrepancy between source and target domains. Most conventional methods focus on one target domain setting adapted from one or multiple source domains while neglecting the multi-target domain setting. We argue that different target domains also have complementary information, which is very important for performance improvement. In this paper, we propose an Attention-guided Multiple source-and-target Domain Adaptation (AMDA) method to capture the context dependency information on transferable regions among multiple source and target domains. The innovation points of this paper are as follows: (1) We use numerous adversarial strategies to harvest sufficient information from multiple source and target domains, which extends the generalization and robustness of the feature pools. (2) We propose an intra-domain and inter-domain attention module to explore transferable context information. The proposed attention module can learn domain-invariant representations and reduce the negative transfer by focusing on transferable knowledge. Extensive experiments validate the effectiveness of our method with achieving state-of-the-art performance on several unsupervised domain adaptation datasets.},
  archive      = {J_TIP},
  author       = {Yuxi Wang and Zhaoxiang Zhang and Wangli Hao and Chunfeng Song},
  doi          = {10.1109/TIP.2020.3031161},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {892-906},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Attention guided multiple source and target domain adaptation},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sparse learning-based correlation filter for robust
tracking. <em>TIP</em>, <em>30</em>, 878–891. (<a
href="https://doi.org/10.1109/TIP.2020.3039392">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many objective tracking methods are based on the framework of correlation filtering (CF) due to its high efficiency. In this paper, we propose a $l_{2}$ -norm based sparse response regularization term to restrain unexpected crests in response for CF framework. CF trackers learn online to regress the region of interest into a Gaussian response. However, due to the uncertain transformations of tracked object, there are many unexpected crests in the response map. When the response of tracked object is corrupted by other crests, the tracker will lost the object. Therefore, the sparse response is used to increase the robustness to transformations of tracked object. Since the novel term is directly incorporated into the objective function of the CF framework, it can be used to improve the performance of many methods which are based on this framework. Moreover, from the solutions we derive, the new method will not increase the computational complexity. Through the experiments on benchmarks of OTB-100, TempleColor, VOT2016 and VOT2017, the proposed regularization term can improve the tracking performance of various CF trackers, including those based on standard discriminative CF framework and those based on context-aware CF framework. We also embed the sparse response regularization term in the state-of-the-art integrated tracker MCCT to test its generalization performance. Although MCCT is an expert integrated tracker and owns an exquisite algorithm for selecting experts, the experimental results show that our method can still improve its long-term tracking performance without increasing computational complexity.},
  archive      = {J_TIP},
  author       = {Wenhua Zhang and Licheng Jiao and Yuxuan Li and Jia Liu},
  doi          = {10.1109/TIP.2020.3039392},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {878-891},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Sparse learning-based correlation filter for robust tracking},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DRCNN: Dynamic routing convolutional neural network for
multi-view 3D object recognition. <em>TIP</em>, <em>30</em>, 868–877.
(<a href="https://doi.org/10.1109/TIP.2020.3039378">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D object recognition is one of the most important tasks in 3D data processing, and has been extensively studied recently. Researchers have proposed various 3D recognition methods based on deep learning, among which a class of view-based approaches is a typical one. However, in the view-based methods, the commonly used view pooling layer to fuse multi-view features causes a loss of visual information. To alleviate this problem, in this paper, we construct a novel layer called Dynamic Routing Layer (DRL) by modifying the dynamic routing algorithm of capsule network, to more effectively fuse the features of each view. Concretely, in DRL, we use rearrangement and affine transformation to convert features, then leverage the modified dynamic routing algorithm to adaptively choose the converted features, instead of ignoring all but the most active feature in view pooling layer. We also illustrate that the view pooling layer is a special case of our DRL. In addition, based on DRL, we further present a Dynamic Routing Convolutional Neural Network (DRCNN) for multi-view 3D object recognition. Our experiments on three 3D benchmark datasets show that our proposed DRCNN outperforms many state-of-the-arts, which demonstrates the efficacy of our method.},
  archive      = {J_TIP},
  author       = {Kai Sun and Jiangshe Zhang and Junmin Liu and Ruixuan Yu and Zengjie Song},
  doi          = {10.1109/TIP.2020.3039378},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {868-877},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {DRCNN: Dynamic routing convolutional neural network for multi-view 3D object recognition},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Designing color filters that make cameras more colorimetric.
<em>TIP</em>, <em>30</em>, 853–867. (<a
href="https://doi.org/10.1109/TIP.2020.3038523">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When we place a colored filter in front of a camera the effective camera response functions are equal to the given camera spectral sensitivities multiplied by the filter spectral transmittance. In this article, we solve for the filter which returns the modified sensitivities as close to being a linear transformation from the color matching functions of the human visual system as possible. When this linearity condition - sometimes called the Luther condition- is approximately met, the `camera+filter&#39; system can be used for accurate color measurement. Then, we reformulate our filter design optimisation for making the sensor responses as close to the CIEXYZ tristimulus values as possible given the knowledge of real measured surfaces and illuminants spectra data. This data-driven method in turn is extended to incorporate constraints on the filter (smoothness and bounded transmission). Also, because how the optimisation is initialised is shown to impact on the performance of the solved-for filters, a multi-initialisation optimisation is developed. Experiments demonstrate that, by taking pictures through our optimised color filters, we can make cameras significantly more colorimetric.},
  archive      = {J_TIP},
  author       = {Graham D. Finlayson and Yuteng Zhu},
  doi          = {10.1109/TIP.2020.3038523},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {853-867},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Designing color filters that make cameras more colorimetric},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Heavy-tailed self-similarity modeling for single image super
resolution. <em>TIP</em>, <em>30</em>, 838–852. (<a
href="https://doi.org/10.1109/TIP.2020.3038521">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-similarity is a prominent characteristic of natural images that can play a major role when it comes to their denoising, restoration or compression. In this paper, we propose a novel probabilistic model that is based on the concept of image patch similarity and applied to the problem of Single Image Super Resolution. Based on this model, we derive a Variational Bayes algorithm, which super resolves low-resolution images, where the assumed distribution for the quantified similarity between two image patches is heavy-tailed. Moreover, we prove mathematically that the proposed algorithm is both an extended and superior version of the probabilistic Non-Local Means (NLM). Its prime advantage remains though, which is that it requires no training. A comparison of the proposed approach with state-of-the-art methods, using various quantitative metrics shows that it is almost on par, for images depicting rural themes and in terms of the Structural Similarity Index (SSIM) with the best performing methods that rely on trained deep learning models. On the other hand, it is clearly inferior to them, for urban themed images and in terms of all metrics, especially for the Mean-Squared-Error (MSE). In addition, qualitative evaluation of the proposed approach is performed using the Perceptual Index metric, which has been introduced to better mimic the human perception of the image quality. This evaluation favors our approach when compared to the best performing method that requires no training, even if they perform equally in qualitative terms, reinforcing the argument that MSE is not always an accurate metric for image quality.},
  archive      = {J_TIP},
  author       = {Giannis Chantas and Spiros N. Nikolopoulos and Ioannis Kompatsiaris},
  doi          = {10.1109/TIP.2020.3038521},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {838-852},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Heavy-tailed self-similarity modeling for single image super resolution},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). FREE: A fast and robust end-to-end video text spotter.
<em>TIP</em>, <em>30</em>, 822–837. (<a
href="https://doi.org/10.1109/TIP.2020.3038520">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, video text spotting tasks usually fall into the four-staged pipeline: detecting text regions in individual images, recognizing localized text regions frame-wisely, tracking text streams and post-processing to generate final results. However, they may suffer from the huge computational cost as well as sub-optimal results due to the interferences of low-quality text and the none-trainable pipeline strategy. In this article, we propose a fast and robust end-to-end video text spotting framework named FREE by only recognizing the localized text stream one-time instead of frame-wise recognition. Specifically, FREE first employs a well-designed spatial-temporal detector that learns text locations among video frames. Then a novel text recommender is developed to select the highest-quality text from text streams for recognizing. Here, the recommender is implemented by assembling text tracking, quality scoring and recognition into a trainable module. It not only avoids the interferences from the low-quality text but also dramatically speeds up the video text spotting. FREE unites the detector and recommender into a whole framework, and helps achieve global optimization. Besides, we collect a large scale video text dataset for promoting the video text spotting community, containing 100 videos from 21 real-life scenarios. Extensive experiments on public benchmarks show our method greatly speeds up the text spotting process, and also achieves the remarkable state-of-the-art.},
  archive      = {J_TIP},
  author       = {Zhanzhan Cheng and Jing Lu and Baorui Zou and Liang Qiao and Yunlu Xu and Shiliang Pu and Yi Niu and Fei Wu and Shuigeng Zhou},
  doi          = {10.1109/TIP.2020.3038520},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {822-837},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {FREE: A fast and robust end-to-end video text spotter},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptive multiple subtraction based on an accelerating
iterative curvelet thresholding method. <em>TIP</em>, <em>30</em>,
806–821. (<a href="https://doi.org/10.1109/TIP.2020.3038519">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the seismic exploration, recorded data contain primaries and multiples, where primaries, as signals of interest, can be used to image the subsurface geology. Surface-related multiple elimination (SRME), one important class of multiple attenuation algorithms, operates in two stages, multiple prediction and subtraction. Due to the phase and amplitude errors in the predicted multiples, adaptive multiple subtraction (AMS) is the key step of SRME. The main challenge of this technique resides in removing multiples without distorting primaries. The curvelet-based AMS methods, which exploit the sparsity of primary and multiple in curvelet domain and the misfit between the original and estimated signals in data domain, have shown outstanding performances in real seismic data processing. These methods are realized by using the iterative curvelet thresholding (ICT), which has heavy computation burden since it includes two forward/inverse curvelet transform (CuT) pairs in each iteration. To ameliorate the computational cost, we propose an accelerating ICT method by exploiting the misfit between the original and estimated signals in curvelet domain directly. Since the proposed method only needs do one forward/inverse CuT pair, it is faster than the traditional ICT method. Considering that the error of the predicted multiple is frequency-dependent, we furthermore introduce the joint constraints within different frequency bands to stabilize and improve the multiple attenuation. Synthetic and field examples demonstrate that the proposed method outperforms the traditional ICT method. In addition, the proposed method has shown to be suitable for refining other AMS methods&#39; results, yielding a SNR improvement of 0.5-2.8 dB.},
  archive      = {J_TIP},
  author       = {Bowu Jiang and Wenkai Lu},
  doi          = {10.1109/TIP.2020.3038519},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {806-821},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Adaptive multiple subtraction based on an accelerating iterative curvelet thresholding method},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A game theory based CTU-level bit allocation scheme for HEVC
region of interest coding. <em>TIP</em>, <em>30</em>, 794–805. (<a
href="https://doi.org/10.1109/TIP.2020.3038515">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, a new CTU-level bit allocation scheme aimed at subjectively optimized video coding for video conferencing applications is presented, in which the non-cooperative Stackelberg game is used for formulating and solving the bit allocation problem during the encoding process. Videos are divided into the Region of interests (ROI) which attracts people more and the non-ROI. The two regions are defined as the players in the game, where the ROI is the leader who takes the priority in strategy making and the non-ROI follows the leader&#39;s strategy. Based on the formulated game, the bit allocation problem can be expressed as a utility optimization problem. By solving the corresponding utility optimization problem, the bit allocation strategy between the ROI and the non-ROI will be established. Then the bits will be allocated to each CTU by a Newton-method-based algorithm for encoding, in which a trade-off between the ROI&#39;s quality and the overall quality can be achieved. Both the objective and subjective experimental results show that our proposed bit allocation method can improve the quality of ROI significantly with an acceptable overall quality degradation, leading to a better visual experience.},
  archive      = {J_TIP},
  author       = {Zizheng Liu and Xiang Pan and Yiming Li and Zhenzhong Chen},
  doi          = {10.1109/TIP.2020.3038515},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {794-805},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A game theory based CTU-level bit allocation scheme for HEVC region of interest coding},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Calibration for camera–projector pairs using spheres.
<em>TIP</em>, <em>30</em>, 783–793. (<a
href="https://doi.org/10.1109/TIP.2020.3038514">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A newly developed calibration algorithm for camera-projector system using spheres is presented in this paper. Previous studies have exploited image conics of sphere to calibrate the camera, whereas this approach can be strengthened to apply in the projector and ultimately achieve the overall calibration for single or multiple pairs of camera-projector. Following the concept of taking the projector as an inverse camera, we retrieve the image conic of the sphere on the projector plane based on a pole-polar relationship we found. At least 3 image conics on the image plane of each device are required to calculate the intrinsic parameters of the device. The extrinsic parameters for all devices in the system are determined by the position of sphere centers in each coordinates frame of the device. Based on the isotropy of the calibration object (sphere), this work is mainly interested in accomplishing the entire calibration for multiple camera-projector systems in which sensors surround a central observation volume. Experiments are conducted on both synthetic and real datasets to evaluate its performance.},
  archive      = {J_TIP},
  author       = {Jian Yu and Feipeng Da and Wenjian Li},
  doi          = {10.1109/TIP.2020.3038514},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {783-793},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Calibration for Camera–Projector pairs using spheres},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A pairwise attentive adversarial spatiotemporal network for
cross-domain few-shot action recognition-r2. <em>TIP</em>, <em>30</em>,
767–782. (<a href="https://doi.org/10.1109/TIP.2020.3038372">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Action recognition is a popular research topic in the computer vision and machine learning domains. Although many action recognition methods have been proposed, only a few researchers have focused on cross-domain few-shot action recognition, which must often be performed in real security surveillance. Since the problems of action recognition, domain adaptation, and few-shot learning need to be simultaneously solved, the cross-domain few-shot action recognition task is a challenging problem. To solve these issues, in this work, we develop a novel end-to-end pairwise attentive adversarial spatiotemporal network (PASTN) to perform the cross-domain few-shot action recognition task, in which spatiotemporal information acquisition, few-shot learning, and video domain adaptation are realised in a unified framework. Specifically, the Resnet-50 network is selected as the backbone of the PASTN, and a 3D convolution block is embedded in the top layer of the 2D CNN (ResNet-50) to capture the spatiotemporal representations. Moreover, a novel attentive adversarial network architecture is designed to align the spatiotemporal dynamics actions with higher domain discrepancies. In addition, the pairwise margin discrimination loss is designed for the pairwise network architecture to improve the discrimination of the learned domain-invariant spatiotemporal feature. The results of extensive experiments performed on three public benchmarks of the cross-domain action recognition datasets, including SDAI Action I, SDAI Action II and UCF50-OlympicSport, demonstrate that the proposed PASTN can significantly outperform the state-of-the-art cross-domain action recognition methods in terms of both the accuracy and computational time. Even when only two labelled training samples per category are considered in the office1 scenario of the SDAI Action I dataset, the accuracy of the PASTN is improved by 6.1\%, 10.9\%, 16.8\%, and 14\% compared to that of the $TA^{3}N$ , TemporalPooling, I3D, and P3D methods, respectively.},
  archive      = {J_TIP},
  author       = {Zan Gao and Leming Guo and Weili Guan and An-An Liu and Tongwei Ren and Shengyong Chen},
  doi          = {10.1109/TIP.2020.3038372},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {767-782},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A pairwise attentive adversarial spatiotemporal network for cross-domain few-shot action recognition-r2},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Coupled network for robust pedestrian detection with gated
multi-layer feature extraction and deformable occlusion handling.
<em>TIP</em>, <em>30</em>, 754–766. (<a
href="https://doi.org/10.1109/TIP.2020.3038371">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pedestrian detection methods have been significantly improved with the development of deep convolutional neural networks. Nevertheless, detecting ismall-scaled pedestrians and occluded pedestrians remains a challenging problem. In this paper, we propose a pedestrian detection method with a couple-network to simultaneously address these two issues. One of the sub-networks, the gated multi-layer feature extraction sub-network, aims to adaptively generate discriminative features for pedestrian candidates in order to robustly detect pedestrians with large variations on scale. The second sub-network targets on handling the occlusion problem of pedestrian detection by using deformable regional region of interest (RoI)-pooling. We investigate two different gate units for the gated sub-network, namely, the channel-wise gate unit and the spatio-wise gate unit, which can enhance the representation ability of the regional convolutional features among the channel dimensions or across the spatial domain, repetitively. Ablation studies have validated the effectiveness of both the proposed gated multi-layer feature extraction sub-network and the deformable occlusion handling sub-network. With the coupled framework, our proposed pedestrian detector achieves promising results on both two pedestrian datasets, especially on detecting small or occluded pedestrians. On the CityPersons dataset, the proposed detector achieves the lowest missing rates (i.e. 40.78\% and 34.60\%) on detecting small and occluded pedestrians, surpassing the second best comparison method by 6.0\% and 5.87\%, respectively.},
  archive      = {J_TIP},
  author       = {Tianrui Liu and Wenhan Luo and Lin Ma and Jun-Jie Huang and Tania Stathaki and Tianhong Dai},
  doi          = {10.1109/TIP.2020.3038371},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {754-766},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Coupled network for robust pedestrian detection with gated multi-layer feature extraction and deformable occlusion handling},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). PWD-3DNet: A deep learning-based fully-automated
segmentation of multiple structures on temporal bone CT scans.
<em>TIP</em>, <em>30</em>, 739–753. (<a
href="https://doi.org/10.1109/TIP.2020.3038363">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The temporal bone is a part of the lateral skull surface that contains organs responsible for hearing and balance. Mastering surgery of the temporal bone is challenging because of this complex and microscopic three-dimensional anatomy. Segmentation of intra-temporal anatomy based on computed tomography (CT) images is necessary for applications such as surgical training and rehearsal, amongst others. However, temporal bone segmentation is challenging due to the similar intensities and complicated anatomical relationships among critical structures, undetectable small structures on standard clinical CT, and the amount of time required for manual segmentation. This paper describes a single multi-class deep learning-based pipeline as the first fully automated algorithm for segmenting multiple temporal bone structures from CT volumes, including the sigmoid sinus, facial nerve, inner ear, malleus, incus, stapes, internal carotid artery and internal auditory canal. The proposed fully convolutional network, PWD-3DNet, is a patch-wise densely connected (PWD) three-dimensional (3D) network. The accuracy and speed of the proposed algorithm was shown to surpass current manual and semi-automated segmentation techniques. The experimental results yielded significantly high Dice similarity scores and low Hausdorff distances for all temporal bone structures with an average of 86\% and 0.755 millimeter (mm), respectively. We illustrated that overlapping in the inference sub-volumes improves the segmentation performance. Moreover, we proposed augmentation layers by using samples with various transformations and image artefacts to increase the robustness of PWD-3DNet against image acquisition protocols, such as smoothing caused by soft tissue scanner settings and larger voxel sizes used for radiation reduction. The proposed algorithm was tested on low-resolution CTs acquired by another center with different scanner parameters than the ones used to create the algorithm and shows potential for application beyond the particular training data used in the study.},
  archive      = {J_TIP},
  author       = {Soodeh Nikan and Kylen Van Osch and Mandolin Bartling and Daniel G. Allen and S. Alireza Rohani and Ben Connors and Sumit K. Agrawal and Hanif M. Ladak},
  doi          = {10.1109/TIP.2020.3038363},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {739-753},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {PWD-3DNet: A deep learning-based fully-automated segmentation of multiple structures on temporal bone CT scans},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning recurrent memory activation networks for visual
tracking. <em>TIP</em>, <em>30</em>, 725–738. (<a
href="https://doi.org/10.1109/TIP.2020.3038356">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facilitated by deep neural networks, numerous tracking methods have made significant advances. Existing deep trackers mainly utilize independent frames to model the target appearance, while paying less attention to its temporal coherence. In this paper, we propose a recurrent memory activation network (RMAN) to exploit the untapped temporal coherence of the target appearance for visual tracking. We build the RMAN on top of the long short-term memory network (LSTM) with an additional memory activation layer. Specifically, we first use the LSTM to model the temporal changes of the target appearance. Then we selectively activate the memory blocks via the activation layer to produce a temporally coherent representation. The recurrent memory activation layer enriches the target representations from independent frames and reduces the background interference through temporal consistency. The proposed RMAN is fully differentiable and can be optimized end-to-end. To facilitate network training, we propose a temporal coherence loss together with the original binary classification loss. Extensive experimental results on standard benchmarks demonstrate that our method performs favorably against the state-of-the-art approaches.},
  archive      = {J_TIP},
  author       = {Shi Pu and Yibing Song and Chao Ma and Honggang Zhang and Ming-Hsuan Yang},
  doi          = {10.1109/TIP.2020.3038356},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {725-738},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Learning recurrent memory activation networks for visual tracking},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Improving performance and adaptivity of anchor-based
detector using differentiable anchoring with efficient target
generation. <em>TIP</em>, <em>30</em>, 712–724. (<a
href="https://doi.org/10.1109/TIP.2020.3038349">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most anchor-based object detection methods have adopted predefined anchor boxes as regression references. However, the proper setting of anchor boxes may vary significantly across different datasets, improperly designed anchors severely limit the performances and adaptabilities of detectors. Recently, some works have tackled this problem by learning anchor shapes from datasets. However, all of these works explicitly or implicitly rely on predefined anchors, limiting universalities of detectors. In this paper, we propose a simple learning anchoring scheme with an effective target generation method to cast off predefined anchor dependencies. The proposed anchoring scheme, named as differentiable anchoring, simplifies learning anchor shape process by adding only one branch in parallel with the existing classification and bounding box regression branches. The proposed target generation method, including the $L_{p}$ norm ball approximation and the optimization difficulty-based pyramid level assignment approach, generates positive samples for the new branch. Compared with existing learning anchoring-based approaches, the proposed method doesn’t require any predefined anchors, while tremendously improving performances and adaptiveness of detectors. The proposed method can be seamlessly integrated to Faster RCNN, RetinaNet, and SSD, improving the detection mAP by 2.8\%, 2.1\% and 2.3\% respectively on MS COCO 2017 test-dev set. Moreover, the differentiable anchoring-based detectors can be directly applied to specific scenarios without any modification of the hyperparameters or using a specialized optimization. Specifically, the differentiable anchoring-based RetinaNet achieves very competitive performances on tiny face detection and text detection tasks, which are not well handled by the conventional and guided anchoring based RetinaNets for the MS COCO dataset.},
  archive      = {J_TIP},
  author       = {Zeyang Dou and Kun Gao and Xiaodian Zhang and Hong Wang and Junwei Wang},
  doi          = {10.1109/TIP.2020.3038349},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {712-724},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Improving performance and adaptivity of anchor-based detector using differentiable anchoring with efficient target generation},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Iterative training of neural networks for intra prediction.
<em>TIP</em>, <em>30</em>, 697–711. (<a
href="https://doi.org/10.1109/TIP.2020.3038348">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents an iterative training of neural networks for intra prediction in a block-based image and video codec. First, the neural networks are trained on blocks arising from the codec partitioning of images, each paired with its context. Then, iteratively, blocks are collected from the partitioning of images via the codec including the neural networks trained at the previous iteration, each paired with its context, and the neural networks are retrained on the new pairs. Thanks to this training, the neural networks can learn intra prediction functions that both stand out from those already in the initial codec and boost the codec in terms of rate-distortion. Moreover, the iterative process allows the design of training data cleansings essential for the neural network training. When the iteratively trained neural networks are put into H.265 (HM-16.15), -4.2\% of mean BD-rate reduction is obtained, i.e. -1.8\% above the state-of-the-art. By moving them into H.266 (VTM-5.0), the mean BD-rate reduction reaches -1.9\%.},
  archive      = {J_TIP},
  author       = {Thierry Dumas and Franck Galpin and Philippe Bordes},
  doi          = {10.1109/TIP.2020.3038348},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {697-711},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Iterative training of neural networks for intra prediction},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Joint person objectness and repulsion for person search.
<em>TIP</em>, <em>30</em>, 685–696. (<a
href="https://doi.org/10.1109/TIP.2020.3038347">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person search targets to search the probe person from the unconstrainted scene images, which can be treated as the combination of person detection and person matching. However, the existing methods based on the Detection-Matching framework ignore the person objectness and repulsion (OR) which are both beneficial to reduce the effect of distractor images. In this paper, we propose an OR similarity by jointly considering the objectness and repulsion information. Besides the traditional visual similarity term, the OR similarity also contains an objectness term and a repulsion term. The objectness term can reduce the similarity of distractor images that not contain a person and boost the performance of person search by improving the ranking of positive samples. Because the probe person has a different person ID with its neighbors, the gallery images having a higher similarity with the neighbors of probe should have a lower similarity with the probe person. Based on this repulsion constraint, the repulsion term is proposed to reduce the similarity of distractor images that are not most similar to the probe person. Treating the Faster R-CNN as the person detector, the OR similarity is evaluated on PRW and CUHK-SYSU datasets by the Detection-Matching framework with six description models. The extensive experiments demonstrate that the proposed OR similarity can effectively reduce the similarity of distractor samples and further boost the performance of person search, e.g., improve the mAP from 92.32\% to 93.23\% for CUHK-SYSY dataset, and from 50.91\% to 52.30\% for PRW datasets.},
  archive      = {J_TIP},
  author       = {Hantao Yao and Changsheng Xu},
  doi          = {10.1109/TIP.2020.3038347},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {685-696},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Joint person objectness and repulsion for person search},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Multi-domain image-to-image translation via a unified
circular framework. <em>TIP</em>, <em>30</em>, 670–684. (<a
href="https://doi.org/10.1109/TIP.2020.3037528">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The image-to-image translation aims to learn the corresponding information between the source and target domains. Several state-of-the-art works have made significant progress based on generative adversarial networks (GANs). However, most existing one-to-one translation methods ignore the correlations among different domain pairs. We argue that there is common information among different domain pairs and it is vital to multiple domain pairs translation. In this paper, we propose a unified circular framework for multiple domain pairs translation, leveraging a shared knowledge module across numerous domains. One selected translation pair can benefit from the complementary information from other pairs, and the sharing knowledge is conducive to mutual learning between domains. Moreover, absolute consistency loss is proposed and applied in the corresponding feature maps to ensure intra-domain consistency. Furthermore, our model can be trained in an end-to-end manner. Extensive experiments demonstrate the effectiveness of our approach on several complex translation scenarios, such as Thermal IR switching, weather changing, and semantic transfer tasks.},
  archive      = {J_TIP},
  author       = {Yuxi Wang and Zhaoxiang Zhang and Wangli Hao and Chunfeng Song},
  doi          = {10.1109/TIP.2020.3037528},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {670-684},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Multi-domain image-to-image translation via a unified circular framework},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dual modulated QR codes for proximal privacy and security.
<em>TIP</em>, <em>30</em>, 657–669. (<a
href="https://doi.org/10.1109/TIP.2020.3037524">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ubiquitous presence of surveillance cameras severely compromises the security of private information (e.g. passwords) entered via a conventional keyboard interface in public places. We address this problem by proposing dual modulated QR (DMQR) codes, a novel QR code extension via which users can securely communicate private information in public places using their smartphones and a camera interface. Dual modulated QR codes use the same synchronization patterns and module geometry as conventional monochrome QR codes. Within each module, primary data is embedded using intensity modulation compatible with conventional QR code decoding. Specifically, depending on the bit to be embedded, a module is either left white or an elliptical black dot is placed within it. Additionally, for each module containing an elliptical dot, secondary data is embedded by orientation modulation; that is, by using different orientations for the elliptical dots. Because the orientation of the elliptical dots can only be reliably assessed when the barcodes are captured from a close distance, the secondary data provides “proximal privacy” and can be effectively used to communicate private information securely in public settings. Tests conducted using several alternative parameter settings demonstrate that the proposed DMQR codes are effective in meeting their objective- the secondary data can be accurately decoded for short capture distances (6 in.) but cannot be recovered from images captured over long distances (&gt;12 in.). Furthermore, the proximal privacy can be adapted to application needs by varying the eccentricity of the elliptical dots used.},
  archive      = {J_TIP},
  author       = {Irving Barron and Hsin Jui Yeh and Karthik Dinesh and Gaurav Sharma},
  doi          = {10.1109/TIP.2020.3037524},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {657-669},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Dual modulated QR codes for proximal privacy and security},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Online learning-based multi-stage complexity control for
live video coding. <em>TIP</em>, <em>30</em>, 641–656. (<a
href="https://doi.org/10.1109/TIP.2020.3036766">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High Efficiency Video Coding (HEVC) can significantly improve the compression efficiency in comparison with the preceding H.264/Advanced Video Coding (AVC) but at the cost of extremely high computational complexity. Hence, it is challenging to realize live video applications on low-delay and power-constrained devices, such as the smart mobile devices. In this article, we propose an online learning-based multi-stage complexity control method for live video coding. The proposed method consists of three stages: multi-accuracy Coding Unit (CU) decision, multi-stage complexity allocation, and Coding Tree Unit (CTU) level complexity control. Consequently, the encoding complexity can be accurately controlled to correspond with the computing capability of the video-capable device by replacing the traditional brute-force search with the proposed algorithm, which properly determines the optimal CU size. Specifically, the multi-accuracy CU decision model is obtained by an online learning approach to accommodate the different characteristics of input videos. In addition, multi-stage complexity allocation is implemented to reasonably allocate the complexity budgets to each coding level. In order to achieve a good trade-off between complexity control and rate distortion (RD) performance, the CTU-level complexity control is proposed to select the optimal accuracy of the CU decision model. The experimental results show that the proposed algorithm can accurately control the coding complexity from 100\% to 40\%. Furthermore, the proposed algorithm outperforms the state-of-the-art algorithms in terms of both accuracy of complexity control and RD performance.},
  archive      = {J_TIP},
  author       = {Chao Huang and Zongju Peng and Yong Xu and Fen Chen and Qiuping Jiang and Yun Zhang and Gangyi Jiang and Yo-Sung Ho},
  doi          = {10.1109/TIP.2020.3036766},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {641-656},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Online learning-based multi-stage complexity control for live video coding},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Siamese regression tracking with reinforced template
updating. <em>TIP</em>, <em>30</em>, 628–640. (<a
href="https://doi.org/10.1109/TIP.2020.3036723">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Siamese networks are prevalent in visual tracking because of the efficient localization. The networks take both a search patch and a target template as inputs where the target template is usually from the initial frame. Meanwhile, Siamese trackers do not update network parameters online for real-time efficiency. The fixed target template and CNN parameters make Siamese trackers not effective to capture target appearance variations. In this paper, we propose a template updating method via reinforcement learning for Siamese regression trackers. We collect a series of templates and learn to maintain them based on an actor-critic framework. Among this framework, the actor network that is trained by deep reinforcement learning effectively updates the templates based on the tracking result on each frame. Besides the target template, we update the Siamese regression tracker online to adapt to target appearance variations. The experimental results on the standard benchmarks show the effectiveness of both template and network updating. The proposed tracker SiamRTU performs favorably against state-of-the-art approaches.},
  archive      = {J_TIP},
  author       = {Fei Zhao and Ting Zhang and Yibing Song and Ming Tang and Xiaobo Wang and Jinqiao Wang},
  doi          = {10.1109/TIP.2020.3036723},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {628-640},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Siamese regression tracking with reinforced template updating},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep relation embedding for cross-modal retrieval.
<em>TIP</em>, <em>30</em>, 617–627. (<a
href="https://doi.org/10.1109/TIP.2020.3038354">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-modal retrieval aims to identify relevant data across different modalities. In this work, we are dedicated to cross-modal retrieval between images and text sentences, which is formulated into similarity measurement for each image-text pair. To this end, we propose a Cross-modal Relation Guided Network (CRGN) to embed image and text into a latent feature space. The CRGN model uses GRU to extract text feature and ResNet model to learn the globally guided image feature. Based on the global feature guiding and sentence generation learning, the relation between image regions can be modeled. The final image embedding is generated by a relation embedding module with an attention mechanism. With the image embeddings and text embeddings, we conduct cross-modal retrieval based on the cosine similarity. The learned embedding space well captures the inherent relevance between image and text. We evaluate our approach with extensive experiments on two public benchmark datasets, i.e., MS-COCO and Flickr30K. Experimental results demonstrate that our approach achieves better or comparable performance with the state-of-the-art methods with notable efficiency.},
  archive      = {J_TIP},
  author       = {Yifan Zhang and Wengang Zhou and Min Wang and Qi Tian and Houqiang Li},
  doi          = {10.1109/TIP.2020.3038354},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {617-627},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Deep relation embedding for cross-modal retrieval},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Person re-identification with reinforced attribute attention
selection. <em>TIP</em>, <em>30</em>, 603–616. (<a
href="https://doi.org/10.1109/TIP.2020.3036762">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person re-identification (Re-ID) aims to match pedestrian images across various scenes in video surveillance. There are a few works using attribute information to boost Re-ID performance. Specifically, those methods leverage attribute information to boost Re-ID performance by introducing auxiliary tasks like verifying the image level attribute information of two pedestrian images or recognizing identity level attributes. Identity level attribute annotations cost less manpower and are well-fitted for person re-identification task compared with image-level attribute annotations. However, the identity attribute information may be very noisy due to incorrect attribute annotation or lack of discriminativeness to distinguish different persons, which is probably unhelpful for the Re-ID task. In this paper, we propose a novel Attribute Attentional Block (AAB), which can be integrated into any backbone network or framework. Our AAB adopts reinforcement learning to drop noisy attributes based on our designed reward and then utilizes aggregated attribute attention of the remaining attributes to facilitate the Re-ID task. Experimental results demonstrate that our proposed method achieves state-of-the-art results on three benchmark datasets.},
  archive      = {J_TIP},
  author       = {Jianfu Zhang and Li Niu and Liqing Zhang},
  doi          = {10.1109/TIP.2020.3036762},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {603-616},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Person re-identification with reinforced attribute attention selection},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multidataset independent subspace analysis with application
to multimodal fusion. <em>TIP</em>, <em>30</em>, 588–602. (<a
href="https://doi.org/10.1109/TIP.2020.3028452">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised latent variable models-blind source separation (BSS) especially-enjoy a strong reputation for their interpretability. But they seldom combine the rich diversity of information available in multiple datasets, even though multidatasets yield insightful joint solutions otherwise unavailable in isolation. We present a direct, principled approach to multidataset combination that takes advantage of multidimensional subspace structures. In turn, we extend BSS models to capture the underlying modes of shared and unique variability across and within datasets. Our approach leverages joint information from heterogeneous datasets in a flexible and synergistic fashion. We call this method multidataset independent subspace analysis (MISA). Methodological innovations exploiting the Kotz distribution for subspace modeling, in conjunction with a novel combinatorial optimization for evasion of local minima, enable MISA to produce a robust generalization of independent component analysis (ICA), independent vector analysis (IVA), and independent subspace analysis (ISA) in a single unified model. We highlight the utility of MISA for multimodal information fusion, including sample-poor regimes (N = 600) and low signal-to-noise ratio, promoting novel applications in both unimodal and multimodal brain imaging data.},
  archive      = {J_TIP},
  author       = {Rogers F. Silva and Sergey M. Plis and Tülay Adalı and Marios S. Pattichis and Vince D. Calhoun},
  doi          = {10.1109/TIP.2020.3028452},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {588-602},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Multidataset independent subspace analysis with application to multimodal fusion},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). A spatial-temporal recurrent neural network for video
saliency prediction. <em>TIP</em>, <em>30</em>, 572–587. (<a
href="https://doi.org/10.1109/TIP.2020.3036749">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a recurrent neural network is designed for video saliency prediction considering spatial-temporal features. In our work, video frames are routed through the static network for spatial features and the dynamic network for temporal features. For the spatial-temporal feature integration, a novel select and re-weight fusion model is proposed which can learn and adjust the fusion weights based on the spatial and temporal features in different scenes automatically. Finally, an attention-aware convolutional long short term memory (ConvLSTM) network is developed to predict salient regions based on the features extracted from consecutive frames and generate the ultimate saliency map for each video frame. The proposed method is compared with state-of-the-art saliency models on five public video saliency benchmark datasets. The experimental results demonstrate that our model can achieve advanced performance on video saliency prediction.},
  archive      = {J_TIP},
  author       = {Kao Zhang and Zhenzhong Chen and Shan Liu},
  doi          = {10.1109/TIP.2020.3036749},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {572-587},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A spatial-temporal recurrent neural network for video saliency prediction},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). VR sickness versus VR presence: A statistical prediction
model. <em>TIP</em>, <em>30</em>, 559–571. (<a
href="https://doi.org/10.1109/TIP.2020.3036782">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although it is well-known that the negative effects of VR sickness, and the desirable sense of presence are important determinants of a user&#39;s immersive VR experience, there remains a lack of definitive research outcomes to enable the creation of methods to predict and/or optimize the trade-offs between them. Most VR sickness assessment (VRSA) and VR presence assessment (VRPA) studies reported to date have utilized simple image patterns as probes, hence their results are difficult to apply to the highly diverse contents encountered in general, real-world VR environments. To help fill this void, we have constructed a large, dedicated VR sickness/presence (VR-SP) database, which contains 100 VR videos with associated human subjective ratings. Using this new resource, we developed a statistical model of spatio-temporal and rotational frame difference maps to predict VR sickness. We also designed an exceptional motion feature, which is expressed as the correlation between an instantaneous change feature and averaged temporal features. By adding additional features (visual activity, content features) to capture the sense of presence, we use the new data resource to explore the relationship between VRSA and VRPA. We also show the aggregate VR-SP model is able to predict VR sickness with an accuracy of 90\% and VR presence with an accuracy of 75\% using the new VR-SP dataset.},
  archive      = {J_TIP},
  author       = {Woojae Kim and Sanghoon Lee and Alan Conrad Bovik},
  doi          = {10.1109/TIP.2020.3036782},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {559-571},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {VR sickness versus VR presence: A statistical prediction model},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). 3DCD: Scene independent end-to-end spatiotemporal feature
learning framework for change detection in unseen videos. <em>TIP</em>,
<em>30</em>, 546–558. (<a
href="https://doi.org/10.1109/TIP.2020.3037472">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Change detection is an elementary task in computer vision and video processing applications. Recently, a number of supervised methods based on convolutional neural networks have reported high performance over the benchmark dataset. However, their success depends upon the availability of certain proportions of annotated frames from test video during training. Thus, their performance on completely unseen videos or scene independent setup is undocumented in the literature. In this work, we present a scene independent evaluation (SIE) framework to test the supervised methods in completely unseen videos to obtain generalized models for change detection. In addition, a scene dependent evaluation (SDE) is also performed to document the comparative analysis with the existing approaches. We propose a fast (speed-25 fps) and lightweight (0.13 million parameters, model size-1.16 MB) end-to-end 3D-CNN based change detection network (3DCD) with multiple spatiotemporal learning blocks. The proposed 3DCD consists of a gradual reductionist block for background estimation from past temporal history. It also enables motion saliency estimation, multi-schematic feature encoding-decoding, and finally foreground segmentation through several modular blocks. The proposed 3DCD outperforms the existing state-of-the-art approaches evaluated in both SIE and SDE setup over the benchmark CDnet 2014, LASIESTA and SBMI2015 datasets. To the best of our knowledge, this is a first attempt to present results in clearly defined SDE and SIE setups in three change detection datasets.},
  archive      = {J_TIP},
  author       = {Murari Mandal and Vansh Dhar and Abhishek Mishra and Santosh Kumar Vipparthi and Mohamed Abdel-Mottaleb},
  doi          = {10.1109/TIP.2020.3037472},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {546-558},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {3DCD: Scene independent end-to-end spatiotemporal feature learning framework for change detection in unseen videos},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Weakly supervised learning for single depth-based hand shape
recovery. <em>TIP</em>, <em>30</em>, 532–545. (<a
href="https://doi.org/10.1109/TIP.2020.3037479">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent emerging technologies such AR/VR and HCI are drawing high demand on more comprehensive hand shape understanding, requiring not only 3D hand skeleton pose but also hand shape geometry. In this paper, we propose a deep learning framework to produce 3D hand shape from a single depth image. To address the challenge that capturing ground truth 3D hand shape in the training dataset is non-trivial, we leverage synthetic data to construct a statistical hand shape model and adopt weak supervision from widely accessible hand skeleton pose annotation. To bridge the gap due to the different hand skeleton definitions in the existing public datasets, we propose a joint regression network for hand pose adaptation. To reconstruct the hand shape, we use Chamfer loss between the predicted hand shape and the point cloud from the input depth to learn the shape reconstruction model in a weakly-supervised manner. Experiments demonstrate that our model adapts well to the real data and produces accurate hand shapes that outperform the state-of-the-art methods both qualitatively and quantitatively.},
  archive      = {J_TIP},
  author       = {Xiaoming Deng and Yuying Zhu and Yinda Zhang and Zhaopeng Cui and Ping Tan and Wentian Qu and Cuixia Ma and Hongan Wang},
  doi          = {10.1109/TIP.2020.3037479},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {532-545},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Weakly supervised learning for single depth-based hand shape recovery},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Quality assessment of free-viewpoint videos by quantifying
the elastic changes of multi-scale motion trajectories. <em>TIP</em>,
<em>30</em>, 517–531. (<a
href="https://doi.org/10.1109/TIP.2020.3037504">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual viewpoints synthesis is an essential process for many immersive applications including Free-viewpoint TV (FTV). A widely used technique for viewpoints synthesis is Depth-Image-Based-Rendering (DIBR) technique. However, such technique may introduce challenging non-uniform spatial-temporal structure-related distortions. Most of the existing state-of-the-art quality metrics fail to handle these distortions, especially the temporal structure inconsistencies observed during the switch of different viewpoints. To tackle this problem, an elastic metric and multi-scale trajectory based video quality metric (EM-VQM) is proposed in this paper. Dense motion trajectory is first used as a proxy for selecting temporal sensitive regions, where local geometric distortions might significantly diminish the perceived quality. Afterwards, the amount of temporal structure inconsistencies and unsmooth viewpoints transitions are quantified by calculating 1) the amount of motion trajectory deformations with elastic metric and, 2) the spatial-temporal structural dissimilarity. According to the comprehensive experimental results on two FTV video datasets, the proposed metric outperforms the state-of-the-art metrics designed for free-viewpoint videos significantly and achieves a gain of 12.86\% and 16.75\% in terms of median Pearson linear correlation coefficient values on the two datasets compared to the best one, respectively.},
  archive      = {J_TIP},
  author       = {Suiyi Ling and Jing Li and Zhaohui Che and Xiongkuo Min and Guangtao Zhai and Patrick Le Callet},
  doi          = {10.1109/TIP.2020.3037504},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {517-531},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Quality assessment of free-viewpoint videos by quantifying the elastic changes of multi-scale motion trajectories},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Zero-shot learning to index on semantic trees for scalable
image retrieval. <em>TIP</em>, <em>30</em>, 501–516. (<a
href="https://doi.org/10.1109/TIP.2020.3036779">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we develop a new approach, called zero-shot learning to index on semantic trees (LTI-ST), for efficient image indexing and scalable image retrieval. Our method learns to model the inherent correlation structure between visual representations using a binary semantic tree from training images which can be effectively transferred to new test images from unknown classes. Based on predicted correlation structure, we construct an efficient indexing scheme for the whole test image set. Unlike existing image index methods, our proposed LTI-ST method has the following two unique characteristics. First, it does not need to analyze the test images in the query database to construct the index structure. Instead, it is directly predicted by a network learnt from the training set. This zero-shot capability is critical for flexible, distributed, and scalable implementation and deployment of the image indexing and retrieval services at large scales. Second, unlike the existing distance-based index methods, our index structure is learnt using the LTI-ST deep neural network with binary encoding and decoding on a hierarchical semantic tree. Our extensive experimental results on benchmark datasets and ablation studies demonstrate that the proposed LTI-ST method outperforms existing index methods by a large margin while providing the above new capabilities which are highly desirable in practice.},
  archive      = {J_TIP},
  author       = {Shichao Kan and Yi Cen and Yigang Cen and Mladenovic Vladimir and Yang Li and Zhihai He},
  doi          = {10.1109/TIP.2020.3036779},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {501-516},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Zero-shot learning to index on semantic trees for scalable image retrieval},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hierarchical predictive coding-based JND estimation for
image compression. <em>TIP</em>, <em>30</em>, 487–500. (<a
href="https://doi.org/10.1109/TIP.2020.3037525">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The human visual system (HVS) is a hierarchical system, in which visual signals are processed hierarchically. In this paper, the HVS is modeled as a three-level communication system and visual perception is divided into three stages according to the hierarchical predictive coding theory. Then, a novel just noticeable distortion (JND) estimation scheme is proposed. In visual perception, the input signals are predicted constantly and spontaneously in each hierarchy, and neural response is evoked by the central residue and inhibited by surrounding residues. These two types&#39; residues are regarded as the positive and negative visual incentives which cause positive and negative perception effects, respectively. In neuroscience, the effect of incentive on observer is measured by the surprise of this incentive. Thus, we propose a surprise-based measurement method to measure both perception effects. Specifically, considering the biased competition of visual attention, we define the product of the residue self-information (i.e., surprise) and the competition biases as the perceptual surprise to measure the positive perception effect. As for the negative perception effect, it is measured by the average surprise (i.e., the local Shannon entropy). The JND threshold of each stage is estimated individually by considering both perception effects. The total JND threshold is finally obtained by non-linear superposition of three stage thresholds. Furthermore, the proposed JND estimation scheme is incorporated into the codec of Versatile Video Coding for image compression. Experimental results show that the proposed JND model outperforms the relevant existing ones, and over 16\% of bit rate can be reduced without jeopardizing the perceptual quality.},
  archive      = {J_TIP},
  author       = {Hongkui Wang and Li Yu and Junhui Liang and Haibing Yin and Tiansong Li and Shengwei Wang},
  doi          = {10.1109/TIP.2020.3037525},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {487-500},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Hierarchical predictive coding-based JND estimation for image compression},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A lightweight spatial and temporal multi-feature fusion
network for defect detection. <em>TIP</em>, <em>30</em>, 472–486. (<a
href="https://doi.org/10.1109/TIP.2020.3036770">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a hybrid multi-dimensional features fusion structure of spatial and temporal segmentation model for automated thermography defects detection. In addition, the newly designed attention block encourages local interaction among the neighboring pixels to recalibrate the feature maps adaptively. A Sequence-PCA layer is embedded in the network to provide enhanced semantic information. The final model results in a lightweight structure with smaller number of parameters and yet yields uncompromising performance after model compression. The proposed model allows better capture of the semantic information to improve the detection rate in an end-to-end procedure. Compared with current state-of-the-art deep semantic segmentation algorithms, the proposed model presents more accurate and robust results. In addition, the proposed attention module has led to improved performance on two classification tasks compared with other prevalent attention blocks. In order to verify the effectiveness and robustness of the proposed model, experimental studies have been carried out for defects detection on four different datasets. The demo code of the proposed method can be linked soon: http://faculty.uestc.edu.cn/gaobin/zh_CN/lwcg/153392/list/index.htm},
  archive      = {J_TIP},
  author       = {Bozhen Hu and Bin Gao and Wai Lok Woo and Lingfeng Ruan and Jikun Jin and Yang Yang and Yongjie Yu},
  doi          = {10.1109/TIP.2020.3036770},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {472-486},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A lightweight spatial and temporal multi-feature fusion network for defect detection},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Data-level recombination and lightweight fusion scheme for
RGB-d salient object detection. <em>TIP</em>, <em>30</em>, 458–471. (<a
href="https://doi.org/10.1109/TIP.2020.3037470">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing RGB-D salient object detection methods treat depth information as an independent component to complement RGB and widely follow the bistream parallel network architecture. To selectively fuse the CNN features extracted from both RGB and depth as a final result, the state-of-the-art (SOTA) bistream networks usually consist of two independent subbranches: one subbranch is used for RGB saliency, and the other aims for depth saliency. However, depth saliency is persistently inferior to the RGB saliency because the RGB component is intrinsically more informative than the depth component. The bistream architecture easily biases its subsequent fusion procedure to the RGB subbranch, leading to a performance bottleneck. In this paper, we propose a novel data-level recombination strategy to fuse RGB with D (depth) before deep feature extraction, where we cyclically convert the original 4-dimensional RGB-D into DGB, RDB and RGD. Then, a newly lightweight designed triple-stream network is applied over these novel formulated data to achieve an optimal channel-wise complementary fusion status between the RGB and D, achieving a new SOTA performance.},
  archive      = {J_TIP},
  author       = {Xuehao Wang and Shuai Li and Chenglizhao Chen and Yuming Fang and Aimin Hao and Hong Qin},
  doi          = {10.1109/TIP.2020.3037470},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {458-471},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Data-level recombination and lightweight fusion scheme for RGB-D salient object detection},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Facial expression recognition using frequency neural
network. <em>TIP</em>, <em>30</em>, 444–457. (<a
href="https://doi.org/10.1109/TIP.2020.3037467">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial expression recognition has become a newly-emerging topic in recent decades, which has important value in the field of human-computer interaction. In this paper, we present a deep learning based approach, named frequency neural network (FreNet), for facial expression recognition. Different from convolutional neural network in spatial domain, FreNet inherits the advantages of processing image in frequency domain, such as efficient computation and spatial redundancy elimination. First, we propose the learnable multiplication kernel and construct multiple multiplication layers to learn features in frequency domain. Second, a summarization layer is proposed following multiplication layers to further yield high-level features. Third, based on the property of discrete cosine transform (DCT), we utilize multiplication layers and summarization layer to construct the Basic-FreNet, which can yield high-level features on the widely used DCT feature. Finally, to further achieve better performance on Basic-FreNet, we propose the Block-FreNet in which the weight-shared multiplication kernel is designed for feature learning and the block sub-sampling is designed for dimension reduction. The experimental results show that the Block-FreNet not only achieves superior performance, but also greatly reduces the computational cost. To our best knowledge, the proposed approach is the first attempt to fill in the blank of frequency based deep learning model for facial expression recognition.},
  archive      = {J_TIP},
  author       = {Yan Tang and Xingming Zhang and Xiping Hu and Siqi Wang and Haoxiang Wang},
  doi          = {10.1109/TIP.2020.3037467},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {444-457},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Facial expression recognition using frequency neural network},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Contour-aware loss: Boundary-aware learning for salient
object segmentation. <em>TIP</em>, <em>30</em>, 431–443. (<a
href="https://doi.org/10.1109/TIP.2020.3037536">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a learning model that makes full use of boundary information for salient object segmentation. Specifically, we come up with a novel loss function, i.e., Contour Loss, which leverages object contours to guide models to perceive salient object boundaries. Such a boundary-aware network can learn boundary-wise distinctions between salient objects and background, hence effectively facilitating the salient object segmentation. Yet the Contour Loss emphasizes the boundaries to capture the contextual details in the local range. We further propose the hierarchical global attention module (HGAM), which forces the model hierarchically to attend to global contexts, thus captures the global visual saliency. Comprehensive experiments on six benchmark datasets show that our method achieves superior performance over state-of-the-art ones. Moreover, our model has a real-time speed of 26 fps on a TITAN X GPU.},
  archive      = {J_TIP},
  author       = {Zixuan Chen and Huajun Zhou and Jianhuang Lai and Lingxiao Yang and Xiaohua Xie},
  doi          = {10.1109/TIP.2020.3037536},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {431-443},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Contour-aware loss: Boundary-aware learning for salient object segmentation},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Enhancement and speedup of photometric compensation for
projectors by reducing inter-pixel coupling and calibration patterns.
<em>TIP</em>, <em>30</em>, 418–430. (<a
href="https://doi.org/10.1109/TIP.2020.3036768">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For a procam to preserve the color appearance of an image projected on a color surface, the photometric distortion introduced by the color surface has to be properly compensated. The performance of such photometric compensation relies on an accurate estimation of the projector nonlinearity. In this paper, we improve the accuracy of projector nonlinearity estimation by taking inter-pixel coupling into consideration. In addition, to respond quickly to the change of projection area due to projector movement, we reduce the number of calibration patterns from six to one and use the projected image as the calibration pattern. This greatly improves the computational efficiency of re-calibration that needs to be performed on the fly during a multimedia presentation without breaking its continuity. Both objective and subjective results are provided to illustrate the effectiveness of the proposed method for color compensation.},
  archive      = {J_TIP},
  author       = {Kuang-Tsu Shih and Jen-Shuo Liu and Frank Shyu and Homer H. Chen},
  doi          = {10.1109/TIP.2020.3036768},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {418-430},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Enhancement and speedup of photometric compensation for projectors by reducing inter-pixel coupling and calibration patterns},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Highly efficient multiview depth coding based on histogram
projection and allowable depth distortion. <em>TIP</em>, <em>30</em>,
402–417. (<a href="https://doi.org/10.1109/TIP.2020.3036760">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mismatches between the precisions of representing the disparity, depth value and rendering position in 3D video systems cause redundancies in depth map representations. In this paper, we propose a highly efficient multiview depth coding scheme based on Depth Histogram Projection (DHP) and Allowable Depth Distortion (ADD) in view synthesis. Firstly, DHP exploits the sparse representation of depth maps generated from stereo matching to reduce the residual error from INTER and INTRA predictions in depth coding. We provide a mathematical foundation for DHP-based lossless depth coding by theoretically analyzing its rate-distortion cost. Then, due to the mismatch between depth value and rendering position, there is a many-to-one mapping relationship between them in view synthesis, which induces the ADD model. Based on this ADD model and DHP, depth coding with lossless view synthesis quality is proposed to further improve the compression performance of depth coding while maintaining the same synthesized video quality. Experimental results reveal that the proposed DHP based depth coding can achieve an average bit rate saving of 20.66\% to 19.52\% for lossless coding on Multiview High Efficiency Video Coding (MV-HEVC) with different groups of pictures. In addition, our depth coding based on DHP and ADD achieves an average depth bit rate reduction of 46.69\%, 34.12\% and 28.68\% for lossless view synthesis quality when the rendering precision varies from integer, half to quarter pixels, respectively. We obtain similar gains for lossless depth coding on the 3D-HEVC, HEVC Intra coding and JPEG2000 platforms.},
  archive      = {J_TIP},
  author       = {Yun Zhang and Linwei Zhu and Raouf Hamzaoui and Sam Kwong and Yo-Sung Ho},
  doi          = {10.1109/TIP.2020.3036760},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {402-417},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Highly efficient multiview depth coding based on histogram projection and allowable depth distortion},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Evaluating the stability of spatial keypoints via cluster
core correspondence index. <em>TIP</em>, <em>30</em>, 386–401. (<a
href="https://doi.org/10.1109/TIP.2020.3036759">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detection and analysis of informative keypoints is a fundamental problem in image analysis and computer vision. Keypoint detectors are omnipresent in visual automation tasks, and recent years have witnessed a significant surge in the number of such techniques. Evaluating the quality of keypoint detectors remains a challenging task owing to the inherent ambiguity over what constitutes a good keypoint. In this context, we introduce a reference based keypoint quality index which is based on the theory of spatial pattern analysis. Unlike traditional correspondence-based quality evaluation which counts the number of feature matches within a specified neighborhood, we present a rigorous mathematical framework to compute the statistical correspondence of the detections inside a set of salient zones (cluster cores) defined by the spatial distribution of a reference set of keypoints. We leverage the versatility of the level sets to handle hypersurfaces of arbitrary geometry, and develop a mathematical framework to estimate the model parameters analytically to reflect the robustness of a feature detection algorithm. Extensive experimental studies involving several keypoint detectors tested under different imaging scenarios demonstrate efficacy of our method to evaluate keypoint quality for generic applications in computer vision and image analysis.},
  archive      = {J_TIP},
  author       = {Suvadip Mukherjee and Thibault Lagache and Jean-Christophe Olivo-Marin},
  doi          = {10.1109/TIP.2020.3036759},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {386-401},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Evaluating the stability of spatial keypoints via cluster core correspondence index},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SDP-GAN: Saliency detail preservation generative adversarial
networks for high perceptual quality style transfer. <em>TIP</em>,
<em>30</em>, 374–385. (<a
href="https://doi.org/10.1109/TIP.2020.3036754">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper proposes a solution to effectively handle salient regions for style transfer between unpaired datasets. Recently, Generative Adversarial Networks (GAN) have demonstrated their potentials of translating images from source domain ${X}$ to target domain ${Y}$ in the absence of paired examples. However, such a translation cannot guarantee to generate high perceptual quality results. Existing style transfer methods work well with relatively uniform content, they often fail to capture geometric or structural patterns that always belong to salient regions. Detail losses in structured regions and undesired artifacts in smooth regions are unavoidable even if each individual region is correctly transferred into the target style. In this paper, we propose SDP-GAN, a GAN-based network for solving such problems while generating enjoyable style transfer results. We introduce a saliency network, which is trained with the generator simultaneously. The saliency network has two functions: (1) providing constraints for content loss to increase punishment for salient regions, and (2) supplying saliency features to generator to produce coherent results. Moreover, two novel losses are proposed to optimize the generator and saliency networks. The proposed method preserves the details on important salient regions and improves the total image perceptual quality. Qualitative and quantitative comparisons against several leading prior methods demonstrates the superiority of our method.},
  archive      = {J_TIP},
  author       = {Ru Li and Chi-Hao Wu and Shuaicheng Liu and Jue Wang and Guangfu Wang and Guanghui Liu and Bing Zeng},
  doi          = {10.1109/TIP.2020.3036754},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {374-385},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {SDP-GAN: Saliency detail preservation generative adversarial networks for high perceptual quality style transfer},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). ProxIQA: A proxy approach to perceptual optimization of
learned image compression. <em>TIP</em>, <em>30</em>, 360–373. (<a
href="https://doi.org/10.1109/TIP.2020.3036752">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The use of ℓ p (p = 1,2) norms has largely dominated the measurement of loss in neural networks due to their simplicity and analytical properties. However, when used to assess the loss of visual information, these simple norms are not very consistent with human perception. Here, we describe a different “proximal” approach to optimize image analysis networks against quantitative perceptual models. Specifically, we construct a proxy network, broadly termed ProxIQA, which mimics the perceptual model while serving as a loss layer of the network. We experimentally demonstrate how this optimization framework can be applied to train an end-to-end optimized image compression network. By building on top of an existing deep image compression model, we are able to demonstrate a bitrate reduction of as much as 31\% over MSE optimization, given a specified perceptual quality (VMAF) level.},
  archive      = {J_TIP},
  author       = {Li-Heng Chen and Christos G. Bampis and Zhi Li and Andrey Norkin and Alan C. Bovik},
  doi          = {10.1109/TIP.2020.3036752},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {360-373},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {ProxIQA: A proxy approach to perceptual optimization of learned image compression},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Blur removal via blurred-noisy image pair. <em>TIP</em>,
<em>30</em>, 345–359. (<a
href="https://doi.org/10.1109/TIP.2020.3036745">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Complex blur such as the mixup of space-variant and space-invariant blur, which is hard to model mathematically, widely exists in real images. In this article, we propose a novel image deblurring method that does not need to estimate blur kernels. We utilize a pair of images that can be easily acquired in low-light situations: (1) a blurred image taken with low shutter speed and low ISO noise; and (2) a noisy image captured with high shutter speed and high ISO noise. Slicing the blurred image into patches, we extend the Gaussian mixture model (GMM) to model the underlying intensity distribution of each patch using the corresponding patches in the noisy image. We compute patch correspondences by analyzing the optical flow between the two images. The Expectation Maximization (EM) algorithm is utilized to estimate the parameters of GMM. To preserve sharp features, we add an additional bilateral term to the objective function in the M-step. We eventually add a detail layer to the deblurred image for refinement. Extensive experiments on both synthetic and real-world data demonstrate that our method outperforms state-of-the-art techniques, in terms of robustness, visual quality, and quantitative metrics.},
  archive      = {J_TIP},
  author       = {Chunzhi Gu and Xuequan Lu and Ying He and Chao Zhang},
  doi          = {10.1109/TIP.2020.3036745},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {345-359},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Blur removal via blurred-noisy image pair},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adversarial multi-label variational hashing. <em>TIP</em>,
<em>30</em>, 332–344. (<a
href="https://doi.org/10.1109/TIP.2020.3036735">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose an adversarial multi-label variational hashing (AMVH) method to learn compact binary codes for efficient image retrieval. Unlike most existing deep hashing methods which only learn binary codes from specific real samples, our AMVH learns hash functions from both synthetic and real data which make our model effective for unseen data. Specifically, we design an end-to-end deep hashing framework which consists of a generator network and a discriminator-hashing network by enforcing simultaneous adversarial learning and discriminative binary codes learning to learn compact binary codes. The discriminator-hashing network learns binary codes by optimizing a multi-label discriminative criterion and minimizing the quantization loss between binary codes and real-value codes. The generator network is learned so that latent representations can be sampled in a probabilistic manner and used to generate new synthetic training sample for the discriminator-hashing network. Experimental results on several benchmark datasets show the efficacy of the proposed approach.},
  archive      = {J_TIP},
  author       = {Jiwen Lu and Venice Erin Liong and Yap-Peng Tan},
  doi          = {10.1109/TIP.2020.3036735},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {332-344},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Adversarial multi-label variational hashing},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Cover-lossless robust image watermarking against geometric
deformations. <em>TIP</em>, <em>30</em>, 318–331. (<a
href="https://doi.org/10.1109/TIP.2020.3036727">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cover-lossless robust watermarking is a new research issue in the information hiding community, which can restore the cover image completely in case of no attacks. Most countermeasures proposed in the literature usually focus on additive noise-like manipulations such as JPEG compression, low-pass filtering and Gaussian additive noise, but few are resistant to challenging geometric deformations such as rotation and scaling. The main reason is that in the existing cover-lossless robust watermarking algorithms, those exploited robust features are related to the pixel position. In this article, we present a new cover-lossless robust image watermarking method by efficiently embedding a watermark into low-order Zernike moments and reversibly hiding the distortion due to the robust watermark as the compensation information for restoration of the cover image. The amplitude of the exploited low-order Zernike moments are: 1) mathematically invariant to scaling the size of an image and rotation with any angle; and 2) robust to interpolation errors during geometric transformations, and those common image processing operations. To reduce the compensation information, the robust watermarking process is elaborately and luminously designed by using the quantized error, the watermarked error and the rounded error to represent the difference between the original and the robust watermarked image. As a result, a cover-lossless robust watermarking system against geometric deformations is achieved with good performance. Experimental results show that the proposed robust watermarking method can effectively reduce the compensation information, and the new cover-lossless robust watermarking system provides strong robustness to those content-preserving manipulations including scaling, rotation, JPEG compression and other noise-like manipulations. In case of no attacks, the cover image can be recovered without any loss.},
  archive      = {J_TIP},
  author       = {Runwen Hu and Shijun Xiang},
  doi          = {10.1109/TIP.2020.3036727},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {318-331},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Cover-lossless robust image watermarking against geometric deformations},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). ICmSC: Incomplete cross-modal subspace clustering.
<em>TIP</em>, <em>30</em>, 305–317. (<a
href="https://doi.org/10.1109/TIP.2020.3036717">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-modal clustering aims to cluster the high-similar cross-modal data into one group while separating the dissimilar data. Despite the promising cross-modal methods have developed in recent years, existing state-of-the-arts cannot effectively capture the correlations between cross-modal data when encountering with incomplete cross-modal data, which can gravely degrade the clustering performance. To well tackle the above scenario, we propose a novel incomplete cross-modal clustering method that integrates canonical correlation analysis and exclusive representation, named incomplete Cross-modal Subspace Clustering (i.e., iCmSC). To learn a consistent subspace representation among incomplete cross-modal data, we maximize the intrinsic correlations among different modalities by deep canonical correlation analysis (DCCA), while an exclusive self-expression layer is proposed after the output layers of DCCA. We exploit a ℓ 1,2 -norm regularization in the learned subspace to make the learned representation more discriminative, which makes samples between different clusters mutually exclusive and samples among the same cluster attractive to each other. Meanwhile, the decoding networks are employed to reconstruct the feature representation, and further preserve the structural information among the original cross-modal data. To the end, we demonstrate the effectiveness of the proposed iCmSC via extensive experiments, which can justify that iCmSC achieves consistently large improvement compared with the state-of-the-arts.},
  archive      = {J_TIP},
  author       = {Qianqian Wang and Huanhuan Lian and Gan Sun and Quanxue Gao and Licheng Jiao},
  doi          = {10.1109/TIP.2020.3036717},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {305-317},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {ICmSC: Incomplete cross-modal subspace clustering},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Where to prune: Using LSTM to guide data-dependent soft
pruning. <em>TIP</em>, <em>30</em>, 293–304. (<a
href="https://doi.org/10.1109/TIP.2020.3035028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While convolutional neural network (CNN) has achieved overwhelming success in various vision tasks, its heavy computational cost and storage overhead limit the practical use on mobile or embedded devices. Recently, compressing CNN models has attracted considerable attention, where pruning CNN filters, also known as the channel pruning, has generated great research popularity due to its high compression rate. In this paper, a new channel pruning framework is proposed, which can significantly reduce the computational complexity while maintaining sufficient model accuracy. Unlike most existing approaches that seek to-be-pruned filters layer by layer, we argue that choosing appropriate layers for pruning is more crucial, which can result in more complexity reduction but less performance drop. To this end, we utilize a long short-term memory (LSTM) to learn the hierarchical characteristics of a network and generate a global network pruning scheme. On top of it, we propose a data-dependent soft pruning method, dubbed Squeeze-Excitation-Pruning (SEP), which does not physically prune any filters but selectively excludes some kernels involved in calculating forward and backward propagations depending on the pruning scheme. Compared with the hard pruning, our soft pruning can better retain the capacity and knowledge of the baseline model. Experimental results demonstrate that our approach still achieves comparable accuracy even when reducing 70.1\% Floating-point operation per second (FLOPs) for VGG and 47.5\% for Resnet-56.},
  archive      = {J_TIP},
  author       = {Guiguang Ding and Shuo Zhang and Zizhou Jia and Jing Zhong and Jungong Han},
  doi          = {10.1109/TIP.2020.3035028},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {293-304},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Where to prune: Using LSTM to guide data-dependent soft pruning},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Video frame interpolation and enhancement via pyramid
recurrent framework. <em>TIP</em>, <em>30</em>, 277–292. (<a
href="https://doi.org/10.1109/TIP.2020.3033617">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video frame interpolation aims to improve users&#39; watching experiences by generating high-frame-rate videos from low-frame-rate ones. Existing approaches typically focus on synthesizing intermediate frames using high-quality reference images. However, the captured reference frames may suffer from inevitable spatial degradations such as motion blur, sensor noise, etc. Few studies have approached the joint video enhancement problem, namely synthesizing high-frame-rate and high-quality results from low-frame-rate degraded inputs. In this paper, we propose a unified optimization framework for video frame interpolation with spatial degradations. Specifically, we develop a frame interpolation module with a pyramid structure to cyclically synthesize high-quality intermediate frames. The pyramid module features adjustable spatial receptive field and temporal scope, thus contributing to controllable computational complexity and restoration ability. Besides, we propose an inter-pyramid recurrent module to connect sequential models to exploit the temporal relationship. The pyramid module integrates the recurrent module, thus can iteratively synthesize temporally smooth results. And the pyramid modules share weights across iterations, thus it does not expand the model&#39;s parameter size. Our model can be generalized to several applications such as up-converting the frame rate of videos with motion blur, reducing compression artifacts, and jointly super-resolving low-resolution videos. Extensive experimental results demonstrate that our method performs favorably against state-of-the-art methods on various video frame interpolation and enhancement tasks.},
  archive      = {J_TIP},
  author       = {Wang Shen and Wenbo Bao and Guangtao Zhai and Li Chen and Xiongkuo Min and Zhiyong Gao},
  doi          = {10.1109/TIP.2020.3033617},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {277-292},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Video frame interpolation and enhancement via pyramid recurrent framework},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep face rectification for 360° dual-fisheye cameras.
<em>TIP</em>, <em>30</em>, 264–276. (<a
href="https://doi.org/10.1109/TIP.2020.3019661">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rectilinear face recognition models suffer from severe performance degradation when applied to fisheye images captured by 360° back-to-back dual fisheye cameras. We propose a novel face rectification method to combat the effect of fisheye image distortion on face recognition. The method consists of a classification network and a restoration network specifically designed to handle the non-linear property of fisheye projection. The classification network classifies an input fisheye image according to its distortion level. The restoration network takes a distorted image as input and restores the rectilinear geometric structure of the face. The performance of the proposed method is tested on an end-to-end face recognition system constructed by integrating the proposed rectification method with a conventional rectilinear face recognition system. The face verification accuracy of the integrated system is 99.18\% when tested on images in the synthetic Labeled Faces in the Wild (LFW) dataset and 95.70\% for images in a real image dataset, resulting in an average accuracy improvement of 6.57\% over the conventional face recognition system. For face identification, the average improvement over the conventional face recognition system is 4.51\%.},
  archive      = {J_TIP},
  author       = {Yi-Hsin Li and I-Chan Lo and Homer H. Chen},
  doi          = {10.1109/TIP.2020.3019661},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {264-276},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Deep face rectification for 360° dual-fisheye cameras},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Joint local and global information learning with single apex
frame detection for micro-expression recognition. <em>TIP</em>,
<em>30</em>, 249–263. (<a
href="https://doi.org/10.1109/TIP.2020.3035042">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Micro-expressions (MEs) are rapid and subtle facial movements that are difficult to detect and recognize. Most recent works have attempted to recognize MEs with spatial and temporal information from video clips. According to psychological studies, the apex frame conveys the most emotional information expressed in facial expressions. However, it is not clear how the single apex frame contributes to micro-expression recognition. To alleviate that problem, this paper firstly proposes a new method to detect the apex frame by estimating pixel-level change rates in the frequency domain. With frequency information, it performs more effectively on apex frame spotting than the currently existing apex frame spotting methods based on the spatio-temporal change information. Secondly, with the apex frame, this paper proposes a joint feature learning architecture coupling local and global information to recognize MEs, because not all regions make the same contribution to ME recognition and some regions do not even contain any emotional information. More specifically, the proposed model involves the local information learned from the facial regions contributing major emotion information, and the global information learned from the whole face. Leveraging the local and global information enables our model to learn discriminative ME representations and suppress the negative influence of unrelated regions to MEs. The proposed method is extensively evaluated using CASME, CASME II, SAMM, SMIC, and composite databases. Experimental results demonstrate that our method with the detected apex frame achieves considerably promising ME recognition performance, compared with the state-of-the-art methods employing the whole ME sequence. Moreover, the results indicate that the apex frame can significantly contribute to micro-expression recognition.},
  archive      = {J_TIP},
  author       = {Yante Li and Xiaohua Huang and Guoying Zhao},
  doi          = {10.1109/TIP.2020.3035042},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {249-263},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Joint local and global information learning with single apex frame detection for micro-expression recognition},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Dimensionality reduction for tensor data based on local
decision margin maximization. <em>TIP</em>, <em>30</em>, 234–248. (<a
href="https://doi.org/10.1109/TIP.2020.3034498">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In machine learning, the idea of maximizing the margin between two classes is widely used in classifier design. Enlighted by the idea, this paper proposes a novel supervised dimensionality reduction method for tensor data based on local decision margin maximization. The proposed method seeks to preserve and protect the local discriminant information of the original data in the low-dimensional data space. Firstly, we depart the original tensor dataset into overlapped localities with discriminant information. Then, we extract the similarity and anti-similarity coefficients of each high-dimensional locality and preserve these coefficients in the embedding data space via the multilinear projection scheme. Under the combined effect of these coefficients, each dimension-reduced locality tends to be a convex set where strongly correlated intraclass points gather. Simultaneously, the local decision margin, which is defined as the shortest distance from the boundary of each locality to the nearest point of each side, will be maximized. Therefore, the local discriminant structure of the original data could be well maintained in the low-dimensional data space. Moreover, a simple iterative scheme is proposed to solve the final optimization problem. Finally, the experiment results on 6 real-world datasets demonstrate the effectiveness of the proposed method.},
  archive      = {J_TIP},
  author       = {Shujie Zhang and Zhengming Ma and Weichao Gan},
  doi          = {10.1109/TIP.2020.3034498},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {234-248},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Dimensionality reduction for tensor data based on local decision margin maximization},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning dual encoding model for adaptive visual
understanding in visual dialogue. <em>TIP</em>, <em>30</em>, 220–233.
(<a href="https://doi.org/10.1109/TIP.2020.3034494">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Different from Visual Question Answering task that requires to answer only one question about an image, Visual Dialogue task involves multiple rounds of dialogues which cover a broad range of visual content that could be related to any objects, relationships or high-level semantics. Thus one of the key challenges in Visual Dialogue task is to learn a more comprehensive and semantic-rich image representation that can adaptively attend to the visual content referred by variant questions. In this paper, we first propose a novel scheme to depict an image from both visual and semantic views. Specifically, the visual view aims to capture the appearance-level information in an image, including objects and their visual relationships, while the semantic view enables the agent to understand high-level visual semantics from the whole image to the local regions. Furthermore, on top of such dual-view image representations, we propose a Dual Encoding Visual Dialogue (DualVD) module, which is able to adaptively select question-relevant information from the visual and semantic views in a hierarchical mode. To demonstrate the effectiveness of DualVD, we propose two novel visual dialogue models by applying it to the Late Fusion framework and Memory Network framework. The proposed models achieve state-of-the-art results on three benchmark datasets. A critical advantage of the DualVD module lies in its interpretability. We can analyze which modality (visual or semantic) has more contribution in answering the current question by explicitly visualizing the gate values. It gives us insights in understanding of information selection mode in the Visual Dialogue task. The code is available at https://github.com/JXZe/Learning_DualVD.},
  archive      = {J_TIP},
  author       = {Jing Yu and Xiaoze Jiang and Zengchang Qin and Weifeng Zhang and Yue Hu and Qi Wu},
  doi          = {10.1109/TIP.2020.3034494},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {220-233},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Learning dual encoding model for adaptive visual understanding in visual dialogue},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). TJU-DHD: A diverse high-resolution dataset for object
detection. <em>TIP</em>, <em>30</em>, 207–219. (<a
href="https://doi.org/10.1109/TIP.2020.3034487">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vehicles, pedestrians, and riders are the most important and interesting objects for the perception modules of self-driving vehicles and video surveillance. However, the state-of-the-art performance of detecting such important objects (esp. small objects) is far from satisfying the demand of practical systems. Large-scale, rich-diversity, and high-resolution datasets play an important role in developing better object detection methods to satisfy the demand. Existing public large-scale datasets such as MS COCO collected from websites do not focus on the specific scenarios. Moreover, the popular datasets ( e.g., KITTI and Citypersons) collected from the specific scenarios are limited in the number of images and instances, the resolution, and the diversity. To attempt to solve the problem, we build a diverse high-resolution dataset (called TJU-DHD). The dataset contains 115354 high-resolution images (52\% images have a resolution of $1624\times 1200$ pixels and 48\% images have a resolution of at least 2, $560\times 1.440$ pixels) and 709 330 labeled objects in total with a large variance in scale and appearance. Meanwhile, the dataset has a rich diversity in season variance, illumination variance, and weather variance. In addition, a new diverse pedestrian dataset is further built. With the four different detectors ( i.e., the one-stage RetinaNet, anchor-free FCOS, two-stage FPN, and Cascade R-CNN), experiments about object detection and pedestrian detection are conducted. We hope that the newly built dataset can help promote the research on object detection and pedestrian detection in these two scenes. The dataset is available at https://github.com/tjubiit/TJU-DHD .},
  archive      = {J_TIP},
  author       = {Yanwei Pang and Jiale Cao and Yazhao Li and Jin Xie and Hanqing Sun and Jinfeng Gong},
  doi          = {10.1109/TIP.2020.3034487},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {207-219},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {TJU-DHD: A diverse high-resolution dataset for object detection},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Patch-based dual-tree complex wavelet transform for kinship
recognition. <em>TIP</em>, <em>30</em>, 191–206. (<a
href="https://doi.org/10.1109/TIP.2020.3034027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Kinship recognition is a prominent research aiming to find if kinship relation exists between two different individuals. In general, child closely resembles his/her parents more than others based on facial similarities. These similarities are due to genetically inherited facial features that a child shares with his/her parents. Most existing researches in kinship recognition focus on full facial images to find these kinship similarities. This paper first presents kinship recognition for similar full facial images using proposed Global-based dual-tree complex wavelet transform (G-DTCWT). We then present novel patch-based kinship recognition methods based on dual-tree complex wavelet transform (DT-CWT): Local Patch-based DT-CWT (LP-DTCWT) and Selective Patch-Based DT-CWT (SP-DTCWT). LP-DTCWT extracts coefficients for smaller facial patches for kinship recognition. SP-DTCWT is an extension to LP-DTCWT and extracts coefficients only for representative patches with similarity scores above a normalized cumulative threshold. This threshold is computed by a novel patch selection process. These representative patches contribute more similarities in parent/child image pairs and improve kinship accuracy. Proposed methods are extensively evaluated on different publicly available kinship datasets to validate kinship accuracy. Experimental results showcase efficacy of proposed methods on all kinship datasets. SP-DTCWT achieves competitive accuracy to state-of-the-art methods. Mean kinship accuracy of SP-DTCWT is 95.85\% on baseline KinFaceW-I and 95.30\% on KinFaceW-II datasets. Further, SP-DTCWT achieves the state-of-the-art accuracy of 80.49\% on the largest kinship dataset, Families In the Wild (FIW).},
  archive      = {J_TIP},
  author       = {Aarti Goyal and Toshanlal Meenpal},
  doi          = {10.1109/TIP.2020.3034027},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {191-206},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Patch-based dual-tree complex wavelet transform for kinship recognition},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Image defogging quality assessment: Real-world database and
method. <em>TIP</em>, <em>30</em>, 176–190. (<a
href="https://doi.org/10.1109/TIP.2020.3033402">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fog removal from an image is an active research topic in computer vision. However, current literature is weak in the following two areas which in many ways are hindering progress for developing defogging algorithms. First, there is no true real-world and naturally occurring foggy image datasets suitable for developing defogging models. Second, there is no suitable mathematically simple and easy to use image quality assessment (IQA) methods for evaluating the visual quality of defogged images. We address these two aspects in this paper. We first introduce a new foggy image dataset called multiple real-world foggy image dataset (MRFID). MRFID contains foggy and clear images of 200 outdoor scenes. For each scene, one clear image and 4 foggy images of different densities defined as slightly foggy, moderately foggy, highly foggy, and extremely foggy, are manually selected from images taken from these scenes over the course of one calendar year. We then process the foggy images of MRFID using 16 defogging methods to obtain 12,800 defogged images (DFIs) and perform a comprehensive subjective evaluation of the visual quality of the DFIs. Through collecting the mean opinion score (MOS) of 120 subjects and evaluating a variety of fog-relevant image features, we have developed a new Fog-relevant Feature based SIMilarity index (FRFSIM) for assessing the visual quality of DFIs. We present extensive experimental results to show that our new visual quality assessment measure, the FRFSIM, is more consistent with the MOS than other IQA methods and is therefore more suitable for evaluating defogged images than other state-of-the-art IQA methods. Our dataset and relevant code are available at http://www.vistalab.ac.cn/MRFID-for-defogging/.},
  archive      = {J_TIP},
  author       = {Wei Liu and Fei Zhou and Tao Lu and Jiang Duan and Guoping Qiu},
  doi          = {10.1109/TIP.2020.3033402},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {176-190},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Image defogging quality assessment: Real-world database and method},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Global-feature encoding u-net (GEU-net) for multi-focus
image fusion. <em>TIP</em>, <em>30</em>, 163–175. (<a
href="https://doi.org/10.1109/TIP.2020.3033158">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The convolutional neural network (CNN)-based multi-focus image fusion methods which learn the focus map from the source images have greatly enhanced fusion performance compared with the traditional methods. However, these methods have not yet reached a satisfactory fusion result, since the convolution operation pays too much attention on the local region and generating the focus map as a local classification (classify each pixel into focus or de-focus classes) problem. In this article, a global-feature encoding U-Net (GEU-Net) is proposed for multi-focus image fusion. In the proposed GEU-Net, the U-Net network is employed for treating the generation of focus map as a global two-class segmentation task, which segments the focused and defocused regions from a global view. For improving the global feature encoding capabilities of U-Net, the global feature pyramid extraction module (GFPE) and global attention connection upsample module (GACU) are introduced to effectively extract and utilize the global semantic and edge information. The perceptual loss is added to the loss function, and a large-scale dataset is constructed for boosting the performance of GEU-Net. Experimental results show that the proposed GEU-Net can achieve superior fusion performance than some state-of-the-art methods in both human visual quality, objective assessment and network complexity.},
  archive      = {J_TIP},
  author       = {Bin Xiao and Bocheng Xu and Xiuli Bi and Weisheng Li},
  doi          = {10.1109/TIP.2020.3033158},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {163-175},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Global-feature encoding U-net (GEU-net) for multi-focus image fusion},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust tensor decomposition for image representation based
on generalized correntropy. <em>TIP</em>, <em>30</em>, 150–162. (<a
href="https://doi.org/10.1109/TIP.2020.3033151">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional tensor decomposition methods, e.g., two dimensional principal component analysis and two dimensional singular value decomposition, that minimize mean square errors, are sensitive to outliers. To overcome this problem, in this paper we propose a new robust tensor decomposition method using generalized correntropy criterion (Corr-Tensor). A Lagrange multiplier method is used to effectively optimize the generalized correntropy objective function in an iterative manner. The Corr-Tensor can effectively improve the robustness of tensor decomposition with the existence of outliers without introducing any extra computational cost. Experimental results demonstrated that the proposed method significantly reduces the reconstruction error on face reconstruction and improves the accuracies on handwritten digit recognition and facial image clustering.},
  archive      = {J_TIP},
  author       = {Miaohua Zhang and Yongsheng Gao and Changming Sun and Michael Blumenstein},
  doi          = {10.1109/TIP.2020.3033151},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {150-162},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Robust tensor decomposition for image representation based on generalized correntropy},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fast roughness minimizing image restoration under mixed
poisson–gaussian noise. <em>TIP</em>, <em>30</em>, 134–149. (<a
href="https://doi.org/10.1109/TIP.2020.3032036">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image acquisition in many biomedical imaging modalities is corrupted by Poisson noise followed by additive Gaussian noise. While total variation and related regularization methods for solving biomedical inverse problems are known to yield high quality reconstructions in most situations, such methods mostly use log-likelihood of either Gaussian or Poisson noise models, and rarely use mixed Poisson-Gaussian (PG) noise model. There is a recent work which deals with exact PG likelihood and total variation regularization. This method adapts the primal-dual approach involving gradients steps on the PG log-likelihood, with step size limited by the inverse of its Lipschitz constant. This leads to limitations in the convergence speed. Although the alternating direction method of multipliers (ADMM) algorithm does not have such step size restrictions, ADMM has never been applied for this problem, for the possible reason that PG log-likelihood is quite complex. In this paper, we develop an ADMM based optimization for roughness minimizing image restoration under PG log-likelihood. We achieve this by first developing a novel iterative method for computing the proximal solution of PG log-likelihood, deriving the termination conditions for this iterative method, and then integrating into a provably convergent ADMM scheme. We experimentally demonstrate that the proposed method outperform primal-dual method in most of the cases.},
  archive      = {J_TIP},
  author       = {Manu Ghulyani and Muthuvel Arigovindan},
  doi          = {10.1109/TIP.2020.3032036},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {134-149},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Fast roughness minimizing image restoration under mixed Poisson–Gaussian noise},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust face alignment by multi-order high-precision
hourglass network. <em>TIP</em>, <em>30</em>, 121–133. (<a
href="https://doi.org/10.1109/TIP.2020.3032029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heatmap regression (HR) has become one of the mainstream approaches for face alignment and has obtained promising results under constrained environments. However, when a face image suffers from large pose variations, heavy occlusions and complicated illuminations, the performances of HR methods degrade greatly due to the low resolutions of the generated landmark heatmaps and the exclusion of important high-order information that can be used to learn more discriminative features. To address the alignment problem for faces with extremely large poses and heavy occlusions, this paper proposes a heatmap subpixel regression (HSR) method and a multi-order cross geometry-aware (MCG) model, which are seamlessly integrated into a novel multi-order high-precision hourglass network (MHHN). The HSR method is proposed to achieve high-precision landmark detection by a well-designed subpixel detection loss (SDL) and subpixel detection technology (SDT). At the same time, the MCG model is able to use the proposed multi-order cross information to learn more discriminative representations for enhancing facial geometric constraints and context information. To the best of our knowledge, this is the first study to explore heatmap subpixel regression for robust and high-precision face alignment. The experimental results from challenging benchmark datasets demonstrate that our approach outperforms state-of-the-art methods in the literature.},
  archive      = {J_TIP},
  author       = {Jun Wan and Zhihui Lai and Jun Liu and Jie Zhou and Can Gao},
  doi          = {10.1109/TIP.2020.3032029},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {121-133},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Robust face alignment by multi-order high-precision hourglass network},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Low-rank preserving t-linear projection for robust image
feature extraction. <em>TIP</em>, <em>30</em>, 108–120. (<a
href="https://doi.org/10.1109/TIP.2020.3031813">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the cornerstone for joint dimension reduction and feature extraction, extensive linear projection algorithms were proposed to fit various requirements. When being applied to image data, however, existing methods suffer from representation deficiency since the multi-way structure of the data is (partially) neglected. To solve this problem, we propose a novel Low-Rank Preserving t-Linear Projection (LRP-tP) model that preserves the intrinsic structure of the image data using t-product-based operations. The proposed model advances in four aspects: 1) LRP-tP learns the t-linear projection directly from the tensorial dataset so as to exploit the correlation among the multi-way data structure simultaneously; 2) to cope with the widely spread data errors, e.g., noise and corruptions, the robustness of LRP-tP is enhanced via self-representation learning; 3) LRP-tP is endowed with good discriminative ability by integrating the empirical classification error into the learning procedure; 4) an adaptive graph considering the similarity and locality of the data is jointly learned to precisely portray the data affinity. We devise an efficient algorithm to solve the proposed LRP-tP model using the alternating direction method of multipliers. Extensive experiments on image feature extraction have demonstrated the superiority of LRP-tP compared to the state-of-the-arts.},
  archive      = {J_TIP},
  author       = {Xiaolin Xiao and Yongyong Chen and Yue-Jiao Gong and Yicong Zhou},
  doi          = {10.1109/TIP.2020.3031813},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {108-120},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Low-rank preserving t-linear projection for robust image feature extraction},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Relation graph network for 3D object detection in point
clouds. <em>TIP</em>, <em>30</em>, 92–107. (<a
href="https://doi.org/10.1109/TIP.2020.3031371">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional Neural Networks (CNNs) have emerged as a powerful tool for object detection in 2D images. However, their power has not been fully realised for detecting 3D objects directly in point clouds without conversion to regular grids. Moreover, existing state-of-the-art 3D object detection methods aim to recognize objects individually without exploiting their relationships during learning or inference. In this article, we first propose a strategy that associates the predictions of direction vectors with pseudo geometric centers, leading to a win-win solution for 3D bounding box candidates regression. Secondly, we propose point attention pooling to extract uniform appearance features for each 3D object proposal, benefiting from the learned direction features, semantic features and spatial coordinates of the object points. Finally, the appearance features are used together with the position features to build 3D object-object relationship graphs for all proposals to model their co-existence. We explore the effect of relation graphs on proposals&#39; appearance feature enhancement under supervised and unsupervised settings. The proposed relation graph network comprises a 3D object proposal generation module and a 3D relation module, making it an end-to-end trainable network for detecting 3D objects in point clouds. Experiments on challenging benchmark point cloud datasets (SunRGB-D, ScanNet and KITTI) show that our algorithm performs better than existing state-of-the-art.},
  archive      = {J_TIP},
  author       = {Mingtao Feng and Syed Zulqarnain Gilani and Yaonan Wang and Liang Zhang and Ajmal Mian},
  doi          = {10.1109/TIP.2020.3031371},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {92-107},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Relation graph network for 3D object detection in point clouds},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learnable descriptors for visual search. <em>TIP</em>,
<em>30</em>, 80–91. (<a
href="https://doi.org/10.1109/TIP.2020.3031216">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work proposes LDVS, a learnable binary local descriptor devised for matching natural images within the MPEG CDVS framework. LDVS descriptors are learned so that they can be sign-quantized and compared using the Hamming distance. The underlying convolutional architecture enjoys a moderate parameters count for operations on mobile devices. Our experiments show that LDVS descriptors perform favorably over comparable learned binary descriptors at patch matching on two different datasets. A complete pair-wise image matching pipeline is then designed around LDVS descriptors, integrating them in the reference CDVS evaluation framework. Experiments show that LDVS descriptors outperform the compressed CDVS SIFT-like descriptors at pair-wise image matching over the challenging CDVS image dataset.},
  archive      = {J_TIP},
  author       = {Andrea Migliorati and Attilio Fiandrotti and Gianluca Francini and Riccardo Leonardi},
  doi          = {10.1109/TIP.2020.3031216},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {80-91},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Learnable descriptors for visual search},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Improved multiple-image-based reflection removal algorithm
using deep neural networks. <em>TIP</em>, <em>30</em>, 68–79. (<a
href="https://doi.org/10.1109/TIP.2020.3031184">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When imaging through a semi-reflective medium such as glass, the reflection of another scene can often be found in the captured images. It degrades the quality of the images and affects their subsequent analyses. In this paper, a novel deep neural network approach for solving the reflection problem in imaging is presented. Traditional reflection removal methods not only require long computation time for solving different optimization functions, their performance is also not guaranteed. As array cameras are readily available in nowadays imaging devices, we first suggest in this paper a multiple-image based depth estimation method using a convolutional neural network (CNN). The proposed network avoids the depth ambiguity problem due to the reflection in the image, and directly estimates the depths along the image edges. They are then used to classify the edges as belonging to the background or reflection. Since edges having similar depth values are error prone in the classification, they are removed from the reflection removal process. We suggest a generative adversarial network (GAN) to regenerate the removed background edges. Finally, the estimated background edge map is fed to another auto-encoder network to assist the extraction of the background from the original image. Experimental results show that the proposed reflection removal algorithm achieves superior performance both quantitatively and qualitatively as compared to the state-of-the-art methods. The proposed algorithm also shows much faster speed compared to the existing approaches using the traditional optimization methods.},
  archive      = {J_TIP},
  author       = {Tingtian Li and Yuk-Hee Chan and Daniel P. K. Lun},
  doi          = {10.1109/TIP.2020.3031184},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {68-79},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Improved multiple-image-based reflection removal algorithm using deep neural networks},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hierarchical paired channel fusion network for street scene
change detection. <em>TIP</em>, <em>30</em>, 55–67. (<a
href="https://doi.org/10.1109/TIP.2020.3031173">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Street Scene Change Detection (SSCD) aims to locate the changed regions between a given street-view image pair captured at different times, which is an important yet challenging task in the computer vision community. The intuitive way to solve the SSCD task is to fuse the extracted image feature pairs, and then directly measure the dissimilarity parts for producing a change map. Therefore, the key for the SSCD task is to design an effective feature fusion method that can improve the accuracy of the corresponding change maps. To this end, we present a novel Hierarchical Paired Channel Fusion Network (HPCFNet), which utilizes the adaptive fusion of paired feature channels. Specifically, the features of a given image pair are jointly extracted by a Siamese Convolutional Neural Network (SCNN) and hierarchically combined by exploring the fusion of channel pairs at multiple feature levels. In addition, based on the observation that the distribution of scene changes is diverse, we further propose a Multi-Part Feature Learning (MPFL) strategy to detect diverse changes. Based on the MPFL strategy, our framework achieves a novel approach to adapt to the scale and location diversities of the scene change regions. Extensive experiments on three public datasets (i.e., PCD, VL-CMU-CD and CDnet2014) demonstrate that the proposed framework achieves superior performance which outperforms other state-of-the-art methods with a considerable margin.},
  archive      = {J_TIP},
  author       = {Yinjie Lei and Duo Peng and Pingping Zhang and Qiuhong Ke and Haifeng Li},
  doi          = {10.1109/TIP.2020.3031173},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {55-67},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Hierarchical paired channel fusion network for street scene change detection},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hier r-CNN: Instance-level human parts detection and a new
benchmark. <em>TIP</em>, <em>30</em>, 39–54. (<a
href="https://doi.org/10.1109/TIP.2020.3029901">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting human parts at instance-level is an essential prerequisite for the analysis of human keypoints, actions, and attributes. Nonetheless, there is a lack of a large-scale, rich-annotated dataset for human parts detection. We fill in the gap by proposing COCO Human Parts. The proposed dataset is based on the COCO 2017, which is the first instance-level human parts dataset, and contains images of complex scenes and high diversity. For reflecting the diversity of human body in natural scenes, we annotate human parts with (a) location in terms of a bounding-box, (b) various type including face, head, hand, and foot, (c) subordinate relationship between person and human parts, (d) fine-grained classification into right-hand/left-hand and left-foot/right-foot. A lot of higher-level applications and studies can be founded upon COCO Human Parts, such as gesture recognition, face/hand keypoint detection, visual actions, human-object interactions, and virtual reality. There are a total of 268,030 person instances from the 66,808 images, and 2.83 parts per person instance. We provide a statistical analysis of the accuracy of our annotations. In addition, we propose a strong baseline for detecting human parts at instance-level over this dataset in an end-to-end manner, call Hier(archy) R-CNN. It is a simple but effective extension of Mask R-CNN, which can detect human parts of each person instance and predict the subordinate relationship between them. Codes and dataset are publicly available (https://github.com/soeaver/Hier-R-CNN).},
  archive      = {J_TIP},
  author       = {Lu Yang and Qing Song and Zhihui Wang and Mengjie Hu and Chun Liu},
  doi          = {10.1109/TIP.2020.3029901},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {39-54},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Hier R-CNN: Instance-level human parts detection and a new benchmark},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Just noticeable distortion profile inference: A patch-level
structural visibility learning approach. <em>TIP</em>, <em>30</em>,
26–38. (<a href="https://doi.org/10.1109/TIP.2020.3029428">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose an effective approach to infer the just noticeable distortion (JND) profile based on patch-level structural visibility learning. Instead of pixel-level JND profile estimation, the image patch, which is regarded as the basic processing unit to better correlate with the human perception, can be further decomposed into three conceptually independent components for visibility estimation. In particular, to incorporate the structural degradation into the patch-level JND model, a deep learning-based structural degradation estimation model is trained to approximate the masking of structural visibility. In order to facilitate the learning process, a JND dataset is further established, including 202 pristine images and 7878 distorted images generated by advanced compression algorithms based on the upcoming Versatile Video Coding (VVC) standard. Extensive experimental results further show the superiority of the proposed approach over the state-of-the-art. Our dataset is available at: https://github.com/ShenXuelin-CityU/PWJNDInfer.},
  archive      = {J_TIP},
  author       = {Xuelin Shen and Zhangkai Ni and Wenhan Yang and Xinfeng Zhang and Shiqi Wang and Sam Kwong},
  doi          = {10.1109/TIP.2020.3029428},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {26-38},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Just noticeable distortion profile inference: A patch-level structural visibility learning approach},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Block proposal neural architecture search. <em>TIP</em>,
<em>30</em>, 15–25. (<a
href="https://doi.org/10.1109/TIP.2020.3028288">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The existing neural architecture search (NAS) methods usually restrict the search space to the pre-defined types of block for a fixed macro-architecture. However, this strategy will limit the search space and affect architecture flexibility if block proposal search (BPS) is not considered for NAS. As a result, block structure search is the bottleneck in many previous NAS works. In this work, we propose a new evolutionary algorithm referred to as latency EvoNAS (LEvoNAS) for block structure search, and also incorporate it to the NAS framework by developing a novel two-stage framework referred to as Block Proposal NAS (BP-NAS). Comprehensive experimental results on two computer vision tasks demonstrate the superiority of our newly proposed approach over the state-of-the-art lightweight methods. For the classification task on the ImageNet dataset, our BPN-A is better than 1.0-MobileNetV2 with similar latency, and our BPN-B saves 23.7\% latency when compared with 1.4-MobileNetV2 with higher top-1 accuracy. Furthermore, for the object detection task on the COCO dataset, our method achieves significant performance improvement than MobileNetV2, which demonstrates the generalization capability of our newly proposed framework.},
  archive      = {J_TIP},
  author       = {Jiaheng Liu and Shunfeng Zhou and Yichao Wu and Ken Chen and Wanli Ouyang and Dong Xu},
  doi          = {10.1109/TIP.2020.3028288},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {15-25},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Block proposal neural architecture search},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hierarchical and interactive refinement network for
edge-preserving salient object detection. <em>TIP</em>, <em>30</em>,
1–14. (<a href="https://doi.org/10.1109/TIP.2020.3027992">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Salient object detection has undergone a very rapid development with the blooming of Deep Neural Network (DNN), which is usually taken as an important preprocessing procedure in various computer vision tasks. However, the down-sampling operations, such as pooling and striding, always make the final predictions blurred at edges, which has seriously degenerated the performance of salient object detection. In this paper, we propose a simple yet effective approach, i.e. , Hierarchical and Interactive Refinement Network (HIRN), to preserve the edge structures in detecting salient objects. In particular, a novel multi-stage and dual-path network structure is designed to estimate the salient edges and regions from the low-level and high-level feature maps, respectively. As a result, the predicted regions will become more accurate by enhancing the weak responses at edges, while the predicted edges will become more semantic by suppressing the false positives in background. Once the salient maps of edges and regions are obtained at the output layers, a novel edge-guided inference algorithm is introduced to further filter the resulting regions along the predicted edges. Extensive experiments on several benchmark datasets have been conducted, in which the results show that our method significantly outperforms a variety of state-of-the-art approaches.},
  archive      = {J_TIP},
  author       = {Sanping Zhou and Jinjun Wang and Le Wang and Jimuyang Zhang and Fei Wang and Dong Huang and Nanning Zheng},
  doi          = {10.1109/TIP.2020.3027992},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Hierarchical and interactive refinement network for edge-preserving salient object detection},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
</ul>

</body>
</html>
