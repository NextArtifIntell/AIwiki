<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TAFFC_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="taffc---85">TAFFC - 85</h2>
<ul>
<li><details>
<summary>
(2021). Personality traits classification using deep visual
activity-based nonverbal features of key-dynamic images. <em>IEEE
Transactions on Affective Computing</em>, <em>12</em>(4), 1084–1099. (<a
href="https://doi.org/10.1109/TAFFC.2019.2944614">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This paper addresses nonverbal behavior analysis for the classification of perceived personality traits using novel deep visual activity (VA)-based features extracted only from key-dynamic images. Dynamic images represent short-term VA. Key-dynamic images carry more discriminative information i.e., nonverbal features (NFs) extracted from them contribute to the classification more than NFs extracted from other dynamic images. Dynamic image construction, learning long-term VA with CNN+LSTM, and detecting spatio-temporal saliency are applied to determine key-dynamic images. Once VA-based NFs are extracted, they are encoded using covariance, and resulting representation is used for classification. This method was evaluated on two datasets: small group meetings and vlogs. For the first dataset, proposed method outperforms not only the state-of-the-art VA-based methods but also multi-modal approaches for all personality traits. For extraversion classification, it performs better than i) the most popular key-frames selection algorithm, ii) random and uniform dynamic image selection, and iii) NFs extracted from all dynamic images. Furthermore, the ablation study proves the superiority of proposed method. For the further dataset, it performs as well as the state-of-the-art visual-NFs on average, while showing improved performance for agreeableness classification. Proposed method can be adapted to any application based on nonverbal behavior analysis, thanks to being data-driven.},
  archive  = {J},
  author   = {Cigdem Beyan and Andrea Zunino and Muhammad Shahid and Vittorio Murino},
  doi      = {10.1109/TAFFC.2019.2944614},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {1084-1099},
  title    = {Personality traits classification using deep visual activity-based nonverbal features of key-dynamic images},
  volume   = {12},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Jointly aligning and predicting continuous emotion
annotations. <em>IEEE Transactions on Affective Computing</em>,
<em>12</em>(4), 1069–1083. (<a
href="https://doi.org/10.1109/TAFFC.2019.2917047">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Time-continuous dimensional descriptions of emotions (e.g., arousal, valence) allow researchers to characterize short-time changes and to capture long-term trends in emotion expression. However, continuous emotion labels are generally not synchronized with the input speech signal due to delays caused by reaction-time, which is inherent in human evaluations. To deal with this challenge, we introduce a new convolutional neural network ( multi-delay sinc network ) that is able to simultaneously align and predict labels in an end-to-end manner. The proposed network is a stack of convolutional layers followed by an aligner network that aligns the speech signal and emotion labels. This network is implemented using a new convolutional layer that we introduce, the delayed sinc layer . It is a time-shifted low-pass (sinc) filter that uses a gradient-based algorithm to learn a single delay. Multiple delayed sinc layers can be used to compensate for a non-stationary delay that is a function of the acoustic space. We test the efficacy of this system on two common emotion datasets, RECOLA and SEWA, and show that this approach obtains state-of-the-art speech-only results by learning time-varying delays while predicting dimensional descriptors of emotions.},
  archive  = {J},
  author   = {Soheil Khorram and Melvin G McInnis and Emily Mower Provost},
  doi      = {10.1109/TAFFC.2019.2917047},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {1069-1083},
  title    = {Jointly aligning and predicting continuous emotion annotations},
  volume   = {12},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Improving cross-corpus speech emotion recognition with
adversarial discriminative domain generalization (ADDoG). <em>IEEE
Transactions on Affective Computing</em>, <em>12</em>(4), 1055–1068. (<a
href="https://doi.org/10.1109/TAFFC.2019.2916092">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Automatic speech emotion recognition provides computers with critical context to enable user understanding. While methods trained and tested within the same dataset have been shown successful, they often fail when applied to unseen datasets. To address this, recent work has focused on adversarial methods to find more generalized representations of emotional speech. However, many of these methods have issues converging, and only involve datasets collected in laboratory conditions. In this paper, we introduce Adversarial Discriminative Domain Generalization (ADDoG), which follows an easier to train “meet in the middle” approach. The model iteratively moves representations learned for each dataset closer to one another, improving cross-dataset generalization. We also introduce Multiclass ADDoG, or MADDoG, which is able to extend the proposed method to more than two datasets, simultaneously. Our results show consistent convergence for the introduced methods, with significantly improved results when not using labels from the target dataset. We also show how, in most cases, ADDoG and MADDoG can be used to improve upon baseline state-of-the-art methods when target dataset labels are added and in-the-wild data are considered. Even though our experiments focus on cross-corpus speech emotion, these methods could be used to remove unwanted factors of variation in other settings.},
  archive  = {J},
  author   = {John Gideon and Melvin G McInnis and Emily Mower Provost},
  doi      = {10.1109/TAFFC.2019.2916092},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {1055-1068},
  title    = {Improving cross-corpus speech emotion recognition with adversarial discriminative domain generalization (ADDoG)},
  volume   = {12},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Effects of computerized emotional training on children with
high functioning autism. <em>IEEE Transactions on Affective
Computing</em>, <em>12</em>(4), 1045–1054. (<a
href="https://doi.org/10.1109/TAFFC.2019.2916023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {An evaluation study of a serious game and a system for the automatic emotion recognition designed for helping autistic children to learn to recognize and express emotions by means of their full-body movement is presented. Three-dimensional motion data of full-body movements are obtained from RGB-D sensors and used to recognize emotions by means of linear SVMs. Ten children diagnosed with High Functioning Autism or Asperger Syndrome were involved in the evaluation phase, consisting of repeated sessions to play a specifically designed serious game. Results from the evaluation study show an increase of tasks accuracy from the beginning to the end of training sessions in the trained group. In particular, while the increase of recognition accuracy was concentrated in the first sessions of the game, the increase for expression accuracy is more gradual throughout all sessions. Moreover, the training seems to produce a transfer effect on facial expression recognition.},
  archive  = {J},
  author   = {Stefano Piana and Chiara Malagoli and Maria Carmen Usai and Antonio Camurri},
  doi      = {10.1109/TAFFC.2019.2916023},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {1045-1054},
  title    = {Effects of computerized emotional training on children with high functioning autism},
  volume   = {12},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Speech-driven expressive talking lips with conditional
sequential generative adversarial networks. <em>IEEE Transactions on
Affective Computing</em>, <em>12</em>(4), 1031–1044. (<a
href="https://doi.org/10.1109/TAFFC.2019.2916031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Articulation, emotion, and personality play strong roles in the orofacial movements. To improve the naturalness and expressiveness of virtual agents (VAs), it is important that we carefully model the complex interplay between these factors. This paper proposes a conditional generative adversarial network, called conditional sequential GAN (CSG), which learns the relationship between emotion, lexical content and lip movements in a principled manner. This model uses a set of spectral and emotional speech features directly extracted from the speech signal as conditioning inputs, generating realistic movements. A key feature of the approach is that it is a speech-driven framework that does not require transcripts. Our experiments show the superiority of this model over three state-of-the-art baselines in terms of objective and subjective evaluations. When the target emotion is known, we propose to create emotionally dependent models by either adapting the base model with the target emotional data (CSG-Emo-Adapted), or adding emotional conditions as the input of the model (CSG-Emo-Aware). Objective evaluations of these models show improvements for the CSG-Emo-Adapted compared with the CSG model, as the trajectory sequences are closer to the original sequences. Subjective evaluations show significantly better results for this model compared with the CSG model when the target emotion is happiness.},
  archive  = {J},
  author   = {Najmeh Sadoughi and Carlos Busso},
  doi      = {10.1109/TAFFC.2019.2916031},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {1031-1044},
  title    = {Speech-driven expressive talking lips with conditional sequential generative adversarial networks},
  volume   = {12},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep facial action unit recognition and intensity estimation
from partially labelled data. <em>IEEE Transactions on Affective
Computing</em>, <em>12</em>(4), 1018–1030. (<a
href="https://doi.org/10.1109/TAFFC.2019.2914654">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Research on facial action unit (AU) analysis typically require facial images that are labelled with those action units. While unlabelled facial images abound, labelling those images with action units or intensity is costly and time-consuming. Our approach makes it possible to analyze facial AUs when only some of the images have been labelled. We use many facial images to learn a deep framework that is able to take advantage of the facial representations. A restricted Boltzmann machine uses the available AU annotations to learn the AU label or intensity distribution. We train a support vector machine for AU recognition and a support vector regression for AU intensity estimation by maximizing the log likelihood of the AU mapping functions, taking into account the learned multiple AU distribution for all training data, while simultaneously diminishing errors between the predicted action units and ground-truth action unit occurrence or intensities for all labelled data. We perform experiments on two databases. The results demonstrate the superiority of a deep neural network for learning facial features, as well as the benefit of action unit label or intensity constraints for action unit occurrence recognition or intensity estimation in fully or semi-supervised scenarios.},
  archive  = {J},
  author   = {Shangfei Wang and Bowen Pan and Shan Wu and Qiang Ji},
  doi      = {10.1109/TAFFC.2019.2914654},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {1018-1030},
  title    = {Deep facial action unit recognition and intensity estimation from partially labelled data},
  volume   = {12},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Video affective content analysis by exploring domain
knowledge. <em>IEEE Transactions on Affective Computing</em>,
<em>12</em>(4), 1002–1017. (<a
href="https://doi.org/10.1109/TAFFC.2019.2912377">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Film grammar is often used to invoke certain emotional experiences from audiences through changing visual, speech, and musical elements of videos. Such film grammar, referred to as domain knowledge, is of great importance for video affective content analysis but has not been thoroughly examined in research. In this paper, we propose an improved method for emotion recognition and regression from videos through exploring domain knowledge. We first investigate the domain knowledge of visual, speech, and musical elements, and infer probabilistic dependencies between elements and emotions from the summarized film grammar. Then, we transfer the summarized dependencies between elements and emotions as constraints, and formulate video affective content analysis, including both emotion recognition and emotion regression from video content, as a constrained optimization problem. Experiments on the LIRIS-ACCEDE database, the FilmStim database, and the DEAP database demonstrate that the proposed video affective content analysis method can successfully leverage well-established film grammar to improve emotion recognition and regression from video content.},
  archive  = {J},
  author   = {Shangfei Wang and Can Wang and Tanfang Chen and Yaxin Wang and Yangyang Shu and Qiang Ji},
  doi      = {10.1109/TAFFC.2019.2912377},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {1002-1017},
  title    = {Video affective content analysis by exploring domain knowledge},
  volume   = {12},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Exploring macroscopic and microscopic fluctuations of
elicited facial expressions for mood disorder classification. <em>IEEE
Transactions on Affective Computing</em>, <em>12</em>(4), 989–1001. (<a
href="https://doi.org/10.1109/TAFFC.2019.2909873">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In the clinical diagnosis of mood disorder, a large proportion of patients with bipolar disorder (BD) are misdiagnosed as having unipolar depression (UD). Generally, long-term tracking is required for patients with BD to conduct an appropriate diagnosis by using traditional diagnosis tools. A one-time diagnosis system for facilitating diagnosis procedures is thus highly desirable. Accordingly, in this study, the facial expressions of patients with BD, patients with UD, and healthy controls elicited by emotional video clips were used for conducting mood disorder classification; the classification was performed by exploring the temporal fluctuation characteristics among the three groups. First, macroscopic facial expressions characterized by action units (AUs) were applied for describing the temporal transformation of muscles. Modulation spectrum analysis was applied to extract short-term intensity variations in the AUs. An interval-based multilayer perceptron (MLP) neural network was then used to classify mood disorder on the basis of the detected AU intensities. Moreover, motion vectors (MVs) were employed to describe subtle changes in facial expressions in the microscopic view. Eight basic orientations of MV change were considered for representing microfluctuation. Wavelet decomposition was then applied to extract entropy and energy features in different frequency bands. A long short-term memory model was finally used to model long-term variations for conducting mood disorder classification. A decision-level fusion approach was conducted on the combined results of macroscopic and microscopic facial expressions. For evaluating the described methods, the facial expressions elicited from the 36 subjects (12 from each of the BD, UD, and control groups) were used in 12-fold cross-validation experiments. Approaches for macroscopic and microscopic expressions achieved classification accuracies of 63.9 and 66.7 percent, respectively, and the accuracy of the fusion approach reached 72.2 percent. The results indicate that macroscopic and microscopic view descriptors are complementary to each other and helpful for conducting mood disorder classification.},
  archive  = {J},
  author   = {Qian-Bei Hong and Chung-Hsien Wu and Ming-Hsiang Su and Chia-Cheng Chang},
  doi      = {10.1109/TAFFC.2019.2909873},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {989-1001},
  title    = {Exploring macroscopic and microscopic fluctuations of elicited facial expressions for mood disorder classification},
  volume   = {12},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Automatic detection of mind wandering from video in the lab
and in the classroom. <em>IEEE Transactions on Affective Computing</em>,
<em>12</em>(4), 974–988. (<a
href="https://doi.org/10.1109/TAFFC.2019.2908837">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We report two studies that used facial features to automatically detect mind wandering, a ubiquitous phenomenon whereby attention drifts from the current task to unrelated thoughts. In a laboratory study, university students $(N = 152)$ read a scientific text, whereas in a classroom study high school students $(N = 135)$ learned biology from an intelligent tutoring system. Mind wandering was measured using validated self-report methods. In the lab, we recorded face videos and analyzed these at six levels of granularity: (1) upper-body movement; (2) head pose; (3) facial textures; (4) facial action units (AUs); (5) co-occurring AUs; and (6) temporal dynamics of AUs. Due to privacy constraints, videos were not recorded in the classroom. Instead, we extracted head pose, AUs, and AU co-occurrences in real-time. Machine learning models, consisting of support vector machines (SVM) and deep neural networks, achieved $F_{1}$ scores of .478 and .414 (25.4 and 20.9 percent above-chance improvements, both with SVMs) for detecting mind wandering in the lab and classroom, respectively. The lab-based detectors achieved 8.4 percent improvement over the previous state-of-the-art; no comparison is available for classroom detectors. We discuss how the detectors can integrate into intelligent interfaces to increase engagement and learning by responding to wandering minds.},
  archive  = {J},
  author   = {Nigel Bosch and Sidney K. D&#39;Mello},
  doi      = {10.1109/TAFFC.2019.2908837},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {974-988},
  title    = {Automatic detection of mind wandering from video in the lab and in the classroom},
  volume   = {12},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Towards disorder-independent automatic assessment of
emotional competence in neurological patients with a classical emotion
recognition system: Application in foreign accent syndrome. <em>IEEE
Transactions on Affective Computing</em>, <em>12</em>(4), 962–973. (<a
href="https://doi.org/10.1109/TAFFC.2019.2908365">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Emotive speech is a non-invasive and cost-effective biomarker in a wide spectrum of neurological disorders with computational systems built to automate the diagnosis. In order to explore the possibilities for the automation of a routine speech analysis in the presence of hard to learn pathology patterns, we propose a framework to assess the level of competence in paralinguistic communication. Initially, the assessment relies on a perceptual experiment completed by human listeners, and a model called the Aggregated Ear has been proposed that draws a conclusion about the level of competence demonstrated by the patient. Then, the automation of the Aggregated Ear has been undertaken and resulted in a computational model that summarizes the portfolio of speech evidence on the patient. The summarizing system has a classical emotion recognition system as its central component. The code and the data are available from the corresponding author on request.},
  archive  = {J},
  author   = {Julia Sidorova and Simon Karlsson and Oliver Rosander and Marcelo L. Berthier and Ignacio Moreno-Torres},
  doi      = {10.1109/TAFFC.2019.2908365},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {962-973},
  title    = {Towards disorder-independent automatic assessment of emotional competence in neurological patients with a classical emotion recognition system: Application in foreign accent syndrome},
  volume   = {12},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SchiNet: Automatic estimation of symptoms of schizophrenia
from facial behaviour analysis. <em>IEEE Transactions on Affective
Computing</em>, <em>12</em>(4), 949–961. (<a
href="https://doi.org/10.1109/TAFFC.2019.2907628">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Patients with schizophrenia often display impairments in the expression of emotion and speech and those are observed in their facial behaviour. Automatic analysis of patients’ facial expressions that is aimed at estimating symptoms of schizophrenia has received attention recently. However, the datasets that are typically used for training and evaluating the developed methods, contain only a small number of patients (4-34) and are recorded while the subjects were performing controlled tasks such as listening to life vignettes, or answering emotional questions. In this paper, we use videos of professional-patient interviews, in which symptoms were assessed in a standardised way as they should/may be assessed in practice, and which were recorded in realistic conditions (i.e., varying illumination levels and camera viewpoints) at the patients’ homes or at mental health services. We automatically analyse the facial behaviour of 91 out-patients – this is almost 3 times the number of patients in other studies – and propose SchiNet, a novel neural network architecture that estimates expression-related symptoms in two different assessment interviews. We evaluate the proposed SchiNet for patient-independent prediction of symptoms of schizophrenia. Experimental results show that some automatically detected facial expressions are significantly correlated to symptoms of schizophrenia, and that the proposed network for estimating symptom severity delivers promising results.},
  archive  = {J},
  author   = {Mina Bishay and Petar Palasek and Stefan Priebe and Ioannis Patras},
  doi      = {10.1109/TAFFC.2019.2907628},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {949-961},
  title    = {SchiNet: Automatic estimation of symptoms of schizophrenia from facial behaviour analysis},
  volume   = {12},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Survey on style in 3D human body motion: Taxonomy, data,
recognition and its applications. <em>IEEE Transactions on Affective
Computing</em>, <em>12</em>(4), 928–948. (<a
href="https://doi.org/10.1109/TAFFC.2019.2906167">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The meaning of the word style depends on its context. While actions have already been quite studied for a while, style in human body motion is a growing topic of interest. In the context of animation, style is crucial as it brings realism and expressiveness to the motion of a character. Even though it is undoubtedly a key element in motions, its definition and the use of the word style in itself, among research works, lack consensus. Achieving realistic motions is tedious. It requires either a large motion capture dataset or the considerable work of artist animators. The lack of consistent style data is thus a challenge. Stylistic motion generation is quite studied in order to overcome this issue. This paper focuses on the study of style in human body motion from 3D human body skeletal data. It establishes a taxonomy of definitions of style , describes the data that have been used up until now, introduces key notions about motion capture data as well as machine learning, and presents approaches about style recognition, person identification through their style and motion style generation.},
  archive  = {J},
  author   = {Sarah Ribet and Hazem Wannous and Jean-Philippe Vandeborre},
  doi      = {10.1109/TAFFC.2019.2906167},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {928-948},
  title    = {Survey on style in 3D human body motion: Taxonomy, data, recognition and its applications},
  volume   = {12},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Designing an experience sampling method for smartphone based
emotion detection. <em>IEEE Transactions on Affective Computing</em>,
<em>12</em>(4), 913–927. (<a
href="https://doi.org/10.1109/TAFFC.2019.2905561">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Smartphones provide the capability to perform in-situ sampling of human behavior using Experience Sampling Method (ESM). Designing an ESM schedule involves probing the user repeatedly at suitable moments to collect self-reports. Timely probe generation to collect high fidelity user responses while keeping probing rate low is challenging. In mobile-based ESM, timeliness of the probe is also impacted by user&#39;s availability to respond to self-report request. Thus, a good ESM design must consider - probing frequency , timely self-report collection , and notifying at opportune moment to ensure high response quality . We propose a two-phase ESM design, where the first phase (a) balances between probing frequency and self-report timeliness, and (b) in parallel, constructs a predictive model to identify opportune probing moments. The second phase uses this model to further improve response quality by eliminating inopportune probes. We use typing-based emotion detection in smartphone as a case study to validate proposed ESM design. Our results demonstrate that it reduces probing rate by 64 percent, samples self-reports timely by reducing elapsed time between self-report collection, and event trigger by 9 percent while detecting inopportune moments with an average accuracy of 89 percent. These design choices improve the response quality, as manifested by 96 percent valid response collection and a maximum improvement of 24 percent in emotion classification accuracy.},
  archive  = {J},
  author   = {Surjya Ghosh and Niloy Ganguly and Bivas Mitra and Pradipta De},
  doi      = {10.1109/TAFFC.2019.2905561},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {913-927},
  title    = {Designing an experience sampling method for smartphone based emotion detection},
  volume   = {12},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Improving attention model based on cognition grounded data
for sentiment analysis. <em>IEEE Transactions on Affective
Computing</em>, <em>12</em>(4), 900–912. (<a
href="https://doi.org/10.1109/TAFFC.2019.2903056">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Attention models are proposed in sentiment analysis and other classification tasks because some words are more important than others to train the attention models. However, most existing methods either use local context based information, affective lexicons, or user preference information. In this work, we propose a novel attention model trained by cognition grounded eye-tracking data. First,a reading prediction model is built using eye-tracking data as dependent data and other features in the context as independent data. The predicted reading time is then used to build a cognition grounded attention layer for neural sentiment analysis. Our model can capture attentions in context both in terms of words at sentence level as well as sentences at document level. Other attention mechanisms can also be incorporated together to capture other aspects of attentions, such as local attention, and affective lexicons. Results of our work include two parts. The first part compares our proposed cognition ground attention model with other state-of-the-art sentiment analysis models. The second part compares our model with an attention model based on other lexicon based sentiment resources. Evaluations show that sentiment analysis using cognition grounded attention model outperforms the state-of-the-art sentiment analysis methods significantly. Comparisons to affective lexicons also indicate that using cognition grounded eye-tracking data has advantages over other sentiment resources by considering both word information and context information. This work brings insight to how cognition grounded data can be integrated into natural language processing (NLP) tasks.},
  archive  = {J},
  author   = {Yunfei Long and Rong Xiang and Qin Lu and Chu-Ren Huang and Minglei Li},
  doi      = {10.1109/TAFFC.2019.2903056},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {900-912},
  title    = {Improving attention model based on cognition grounded data for sentiment analysis},
  volume   = {12},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adapting software with affective computing: A systematic
review. <em>IEEE Transactions on Affective Computing</em>,
<em>12</em>(4), 883–899. (<a
href="https://doi.org/10.1109/TAFFC.2019.2902379">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Strategies aimed at keeping the user&#39;s interest in using computer applications are being studied to provide greater user engagement, and can influence how people interact with computers. One of the approaches that can promote user engagement is Affective Computing (AC), based on the premise of recognizing the user&#39;s emotional state and adjusting the computer application to respond to such state in real-time. Although it is a relatively new area, over the past few years many research works have investigated the use of AC in various activities and objectives. To provide an overview on the use of AC in computer applications, this article presents a systematic literature review based on available articles on the main scientific databases of the Computer Science area. The main contribution of this review is the analysis of different types of applications. Based on the 58 articles analyzed, the main emotion recognition techniques and approaches to the adaptation of computer applications, as well as the limitations and challenges to be overcome were compiled. Our conclusions present the limitations and challenges still to be overcome in the area of automatic adaptation of computer applications by means of AC.},
  archive  = {J},
  author   = {Renan Vinicius Aranha and Cléber Gimenez Corrêa and Fátima L. S. Nunes},
  doi      = {10.1109/TAFFC.2019.2902379},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {883-899},
  title    = {Adapting software with affective computing: A systematic review},
  volume   = {12},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Over-sampling emotional speech data based on subjective
evaluations provided by multiple individuals. <em>IEEE Transactions on
Affective Computing</em>, <em>12</em>(4), 870–882. (<a
href="https://doi.org/10.1109/TAFFC.2019.2901465">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {A common step in the area of speech emotion recognition is to obtain ground-truth labels describing the emotional content of a sentence. The underlying emotion of a given recording is usually unknown, so perceptual evaluations are conducted to annotate its perceived emotion. Each sentence is often annotated by multiple raters, which are aggregated with methods such as majority vote rules. This paper argues that several labels provided by different individuals convey more information than the consensus labels. We demonstrate that leveraging the information provided by separate evaluations collected by multiple raters can help in building more robust classifiers which maximize the utilization of labeled data. Motivated by the synthetic minority over-sampling technique (SMOTE), we present a novel over-sampling approach during training, where the samples with categorical emotion labels are over-sampled according to the labels assigned by multiple individuals. This approach (1) increases the number of sentences from classes with underrepresented consensus labels, and (2) utilizes sentences with ambiguous emotional content even if they do not reach consensus agreement. The experimental evaluation shows the benefits of the approach over a baseline classifier trained with consensus labels, which increases the F1-score by 5.2 percent (absolute) for the USC-IEMOCAP corpus, and 5.4 percent (absolute) for the MSP-IMPROV corpus.},
  archive  = {J},
  author   = {Reza Lotfian and Carlos Busso},
  doi      = {10.1109/TAFFC.2019.2901465},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {870-882},
  title    = {Over-sampling emotional speech data based on subjective evaluations provided by multiple individuals},
  volume   = {12},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Feature extraction and selection for emotion recognition
from electrodermal activity. <em>IEEE Transactions on Affective
Computing</em>, <em>12</em>(4), 857–869. (<a
href="https://doi.org/10.1109/TAFFC.2019.2901673">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Electrodermal activity (EDA) is indicative of psychological processes related to human cognition and emotions. Previous research has studied many methods for extracting EDA features; however, their appropriateness for emotion recognition has been tested using a small number of distinct feature sets and on different, usually small, data sets. In the current research, we reviewed 25 studies and implemented 40 different EDA features across time, frequency and time-frequency domains on the publicly available AMIGOS dataset. We performed a systematic comparison of these EDA features using three feature selection methods, Joint Mutual Information (JMI), Conditional Mutual Information Maximization (CMIM) and Double Input Symmetrical Relevance (DISR) and machine learning techniques. We found that approximately the same numbers of features are required to obtain the optimal accuracy for the arousal recognition and the valence recognition. Also, the subject-dependent classification results were significantly higher than the subject-independent classification for both arousal and valence recognition. Statistical features related to the Mel-Frequency Cepstral Coefficients (MFCC) were explored for the first time for the emotion recognition from EDA signals and they outperformed all other feature groups, including the most commonly used Skin Conductance Response (SCR) related features.},
  archive  = {J},
  author   = {Jainendra Shukla and Miguel Barreda-Ángeles and Joan Oliver and G. C. Nandi and Domènec Puig},
  doi      = {10.1109/TAFFC.2019.2901673},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {857-869},
  title    = {Feature extraction and selection for emotion recognition from electrodermal activity},
  volume   = {12},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Prediction of car design perception using EEG and gaze
patterns. <em>IEEE Transactions on Affective Computing</em>,
<em>12</em>(4), 843–856. (<a
href="https://doi.org/10.1109/TAFFC.2019.2901733">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In this paper, we deal with the issue of implicit monitoring of perceptual responses to product design through electroencephalography (EEG) and eye tracking. Four evaluation factors, namely, preference, luxury, complexity, and harmony are considered to investigate how people perceive the car design. In particular, the quantified perceptual responses are predicted based on EEG and gaze data. Average root-mean-square errors of 0.210 and 1.215 are obtained from subject-dependent and subject-independent regressions on a 7-point score scale, respectively, which demonstrates that perception of car design can be predicted via implicit monitoring.},
  archive  = {J},
  author   = {Seong-Eun Moon and Jun-Hyuk Kim and Sun-Wook Kim and Jong-Seok Lee},
  doi      = {10.1109/TAFFC.2019.2901733},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {843-856},
  title    = {Prediction of car design perception using EEG and gaze patterns},
  volume   = {12},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An EEG-based brain computer interface for emotion
recognition and its application in patients with disorder of
consciousness. <em>IEEE Transactions on Affective Computing</em>,
<em>12</em>(4), 832–842. (<a
href="https://doi.org/10.1109/TAFFC.2019.2901456">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Recognizing human emotions based on electroencephalogram (EEG) signals has received a great deal of attentions. Most of the existing studies focused on offline analysis, and real-time emotion recognition using a brain computer interface (BCI) approach remains to be further investigated. In this paper, we proposed an EEG-based BCI system for emotion recognition. Specifically, two classes of video clips that represented positive and negative emotions were presented to the subjects one by one, while the EEG data were collected and processed simultaneously, and instant feedback was provided after each clip. Ten healthy subjects participated in the experiment and achieved a high average online accuracy of 91.5 $\pm$ 6.34 percent. The experimental results demonstrated that the subjects emotions had been sufficiently evoked and efficiently recognized by our system. Clinically, patients with disorder of consciousness (DOC), such as coma, vegetative state, minimally conscious state and emergence minimally conscious state, suffer from motor impairment and generally cannot provide adequate emotion expressions. Consequently, doctors have difficulty in detecting the emotional states of these patients. Therefore, we applied our emotion recognition BCI system to patients with DOC. Eight DOC patients participated in our experiment, and three of them achieved significant online accuracy. The experimental results show that the proposed BCI system could be a promising tool to detect the emotional states of patients with DOC.},
  archive  = {J},
  author   = {Haiyun Huang and Qiuyou Xie and Jiahui Pan and Yanbin He and Zhenfu Wen and Ronghao Yu and Yuanqing Li},
  doi      = {10.1109/TAFFC.2019.2901456},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {832-842},
  title    = {An EEG-based brain computer interface for emotion recognition and its application in patients with disorder of consciousness},
  volume   = {12},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Capturing emotion distribution for multimedia emotion
tagging. <em>IEEE Transactions on Affective Computing</em>,
<em>12</em>(4), 821–831. (<a
href="https://doi.org/10.1109/TAFFC.2019.2900240">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Multimedia collections usually induce multiple emotions in audiences. The data distribution of multiple emotions can be leveraged to facilitate the learning process of emotion tagging, yet has not been thoroughly explored. To address this, we propose adversarial learning to fully capture emotion distributions for emotion tagging of multimedia data. The proposed multimedia emotion tagging approach includes an emotion classifier and a discriminator. The emotion classifier predicts emotion labels of multimedia data from their content. The discriminator distinguishes the predicted emotion labels from the ground truth labels. The emotion classifier and the discriminator are trained simultaneously in competition with each other. By jointly minimizing the traditional supervised loss and maximizing the distribution similarity between the predicted emotion labels and the ground truth emotion labels, the proposed multimedia emotion tagging approach successfully captures both the mapping function between multimedia content and emotion labels as well as prior distribution in emotion labels, and thus achieves state-of-the-art performance for multiple emotion tagging, as demonstrated by the experimental results on four benchmark databases.},
  archive  = {J},
  author   = {Shangfei Wang and Guozhu Peng and Zhuangqiang Zheng and Zhiwei Xu},
  doi      = {10.1109/TAFFC.2019.2900240},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {10},
  number   = {4},
  pages    = {821-831},
  title    = {Capturing emotion distribution for multimedia emotion tagging},
  volume   = {12},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A review on nonlinear methods using electroencephalographic
recordings for emotion recognition. <em>IEEE Transactions on Affective
Computing</em>, <em>12</em>(3), 801–820. (<a
href="https://doi.org/10.1109/TAFFC.2018.2890636">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Electroencephalographic (EEG) recordings are receiving growing attention in the field of emotion recognition, since they monitor the brain&#39;s first response to an external stimulus. Traditionally, EEG signals have been studied from a linear viewpoint by means of statistical and frequency features. Nevertheless, given that the brain follows a completely nonlinear and nonstationary behavior, linear metrics present certain important limitations. In this sense, the use of nonlinear methods has recently revealed new information that may help to understand how the brain works under a series of emotional states. Hence, this paper summarizes the most recent works that have applied nonlinear methods in EEG signal analysis for emotion recognition. This paper also identifies some nonlinear indices that have not been employed yet in this research area.},
  archive  = {J},
  author   = {Beatriz García-Martínez and Arturo Martínez-Rodrigo and Raúl Alcaraz and Antonio Fernández-Caballero},
  doi      = {10.1109/TAFFC.2018.2890636},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {801-820},
  title    = {A review on nonlinear methods using electroencephalographic recordings for emotion recognition},
  volume   = {12},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). First impressions count! The role of the human’s emotional
state on rapport established with an empathic versus neutral virtual
therapist. <em>IEEE Transactions on Affective Computing</em>,
<em>12</em>(3), 788–800. (<a
href="https://doi.org/10.1109/TAFFC.2019.2899305">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Intelligent virtual agents are being endowed with empathic behaviours to perform roles such as virtual therapists. Studies often evaluate the level of rapport established, but do not measure the therapeutic benefit and the relative advantage of empathic versus neutral behaviours. We have created two virtual (empathic/neutral) therapists. Our experiment with 63 participants consisted of one within-subjects (empathic/neutral) and one between-subjects (order) factors. Regardless of the virtual therapist used, improvements in baseline emotion were reported after the first interaction (time one) and further improvement after the second interaction (time two). Our study reveals that if the human initially expresses strong emotional feeling for a problem they are facing, rapport will be higher for the empathic therapist and the level of rapport established at the first meeting will persist regardless of whether the second encounter used empathic or neutral dialogue. Conversely, participants experiencing low emotional feeling reported greater rapport with the neutral therapist, and that level of rapport persisted in the second encounter with the alternative therapist. This study shows that an empathic agent will not necessarily build more rapport or deliver better emotional outcomes than a neutral agent. Further studies are needed to determine when tailoring and complex behaviours are justified.},
  archive  = {J},
  author   = {Hedieh Ranjbartabar and Deborah Richards and Ayse Aysin Bilgin and Cat Kutay},
  doi      = {10.1109/TAFFC.2019.2899305},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {788-800},
  title    = {First impressions count! the role of the human&#39;s emotional state on rapport established with an empathic versus neutral virtual therapist},
  volume   = {12},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Early detection of user engagement breakdown in spontaneous
human-humanoid interaction. <em>IEEE Transactions on Affective
Computing</em>, <em>12</em>(3), 776–787. (<a
href="https://doi.org/10.1109/TAFFC.2019.2898399">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This paper presents a supervised classification system for forecasting a potential user engagement breakdown in human-robot interaction. We define engagement breakdown as a failure to successfully complete a predefined interaction scenario, where the user leaves before the expected end. The goal is thus to detect as early as possible such a potential engagement breakdown during the interaction between a human and a humanoid robot. To this end, we exploit a dataset that we have collected in real-world conditions where a set of participants were left to spontaneously engage in an interaction with the robot. The dataset is labeled according to the presence/absence of engagement breakdown. This study investigates the use of a multimodal approach to this problem, where a set of non-verbal features is considered to characterize the users&#39; behavior. The use of combined multimodal features is found to effectively improve the performance of the system. The optimal set of data streams useful for this task is the combination of the distance to the robot, gaze and head motion, as well as facial expressions and speech. We study the time extent over which a user&#39;s departure can be anticipated. We find that this ability to anticipate the departure depends on the window during which we observe the user behavior.},
  archive  = {J},
  author   = {Atef Ben-Youssef and Chloé Clavel and Slim Essid},
  doi      = {10.1109/TAFFC.2019.2898399},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {776-787},
  title    = {Early detection of user engagement breakdown in spontaneous human-humanoid interaction},
  volume   = {12},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Neural attentive network for cross-domain aspect-level
sentiment classification. <em>IEEE Transactions on Affective
Computing</em>, <em>12</em>(3), 761–775. (<a
href="https://doi.org/10.1109/TAFFC.2019.2897093">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This work takes the lead to study the aspect-level sentiment classification in the domain adaptation scenario . Given a document of any domains, the model needs to figure out the sentiments with respect to fine-grained aspects in the documents. Two main challenges exist in this problem. One is to build a robust document modeling across domains; the other is to mine the domain-specific aspects and make use of the sentiment lexicon. In this paper, we propose a novel approach Neural Attentive model for cross-domain Aspect-level sentiment CLassification (NAACL), which leverages the benefits of the supervised deep neural network as well as the unsupervised probabilistic generative model to strengthen the representation learning. NAACL jointly learns two tasks: (i) a domain classifier, working on documents in both the source and target domains to recognize the domain information of input texts and transfer knowledge from the source domain to the target domain. In particular, a weakly supervised Latent Dirichlet Allocation model (wsLDA) is proposed to learn the domain-specific aspect and sentiment lexicon representations that are then used to calculate the aspect/lexicon-aware document representations via a multi-view attention mechanism; (ii) an aspect-level sentiment classifier, sharing the document modeling with the domain classifier. It makes use of the domain classification results and the aspect/sentiment-aware document representations to classify the aspect-level sentiment of the document in domain adaptation scenario. NAACL is evaluated on both English and Chinese datasets with the out-of-domain as well as in-domain setups. Quantitatively, the experiments demonstrate that NAACL has robust superiority over the compared methods in terms of classification accuracy and F1 score. The qualitative evaluation also shows that the proposed model is capable of reasonably paying attention to those words that are important to judge the sentiment polarity of the input text given an aspect.},
  archive  = {J},
  author   = {Min Yang and Wenpeng Yin and Qiang Qu and Wenting Tu and Ying Shen and Xiaojun Chen},
  doi      = {10.1109/TAFFC.2019.2897093},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {761-775},
  title    = {Neural attentive network for cross-domain aspect-level sentiment classification},
  volume   = {12},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-modal pain intensity recognition based on the
SenseEmotion database. <em>IEEE Transactions on Affective
Computing</em>, <em>12</em>(3), 743–760. (<a
href="https://doi.org/10.1109/TAFFC.2019.2892090">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The subjective nature of pain makes it a very challenging phenomenon to assess. Most of the current pain assessment approaches rely on an individual’s ability to recognise and report an observed pain episode. However, pain perception and expression are affected by numerous factors ranging from personality traits to physical and psychological health state. Hence, several approaches have been proposed for the automatic recognition of pain intensity, based on measurable physiological and audiovisual parameters. In the current paper, an assessment of several fusion architectures for the development of a multi-modal pain intensity classification system is performed. The contribution of the presented work is two-fold: (1) 3 distinctive modalities consisting of audio, video and physiological channels are assessed and combined for the classification of several levels of pain elicitation. (2) An extensive assessment of several fusion strategies is carried out in order to design a classification architecture that improves the performance of the pain recognition system. The assessment is based on the SenseEmotion Database and experimental validation demonstrates the relevance of the multi-modal classification approach, which achieves classification rates of respectively $83.39\%$ , $59.53\%$ and $43.89\%$ in a 2-class, 3-class and 4-class pain intensity classification task.},
  archive  = {J},
  author   = {Patrick Thiam and Viktor Kessler and Mohammadreza Amirian and Peter Bellmann and Georg Layher and Yan Zhang and Maria Velana and Sascha Gruss and Steffen Walter and Harald C. Traue and Daniel Schork and Jonghwa Kim and Elisabeth André and Heiko Neumann and Friedhelm Schwenker},
  doi      = {10.1109/TAFFC.2019.2892090},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {743-760},
  title    = {Multi-modal pain intensity recognition based on the SenseEmotion database},
  volume   = {12},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-feature based network revealing the structural
abnormalities in autism spectrum disorder. <em>IEEE Transactions on
Affective Computing</em>, <em>12</em>(3), 732–742. (<a
href="https://doi.org/10.1109/TAFFC.2018.2890597">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Autism spectrum disorder (ASD) is accompanied with impaired social-emotional functioning, such as emotional regulation and recognition, communication, and related behavior. Study of the alternations of the brain networks in ASD may not only help us in understanding this disorder but also inform us the mechanisms of affective computing in the brain. Although morphological features have been used in the diagnosis of a variety of neurological and psychiatric disorders, these features did not show significant discriminative value in identifying patients with ASD, possibly due to the omission of the information related to the changes in structural similarities among cortical regions. In this study, structural images from 66 high-functioning adults with ASD and 66 matched typically-developing controls (TDC) were used to test the hypothesis of cortico-cortical relationships are abnormal in ASD. Seven morphological features of each of the 360 brain regions were extracted and elastic network was used to quantify the similarities between each target region and all other regions. The similarities were then used to construct multi-feature-based networks (MFN), which were then submitted to a support vector machine classifier to classify the individuals of the two groups. Results showed that the classifier with features of MFN significantly improved the accuracy of discriminating patients with ASD from TDCs (78.63 percent) compared to using morphological features only (&lt;; 65 percent). The combination of MFN features with morphological features and other high-level MFN properties did not further enhance the classification performance. Our findings demonstrate that the variations in cortico-cortical similarities are important in the etiology of ASD and can be used as biomarkers in the diagnostic process.},
  archive  = {J},
  author   = {Weihao Zheng and Tehila Eilam-Stock and Tingting Wu and Alfredo Spagna and Chao Chen and Bin Hu and Jin Fan},
  doi      = {10.1109/TAFFC.2018.2890597},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {732-742},
  title    = {Multi-feature based network revealing the structural abnormalities in autism spectrum disorder},
  volume   = {12},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A scalable off-the-shelf framework for measuring patterns of
attention in young children and its application in autism spectrum
disorder. <em>IEEE Transactions on Affective Computing</em>,
<em>12</em>(3), 722–731. (<a
href="https://doi.org/10.1109/TAFFC.2018.2890610">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Autism spectrum disorder (ASD) is associated with deficits in the processing of social information and difficulties in social interaction, and individuals with ASD exhibit atypical attention and gaze. Traditionally, gaze studies have relied upon precise and constrained means of monitoring attention using expensive equipment in laboratories. In this work we develop a low-cost off-the-shelf alternative for measuring attention that can be used in natural settings. The head and iris positions of 104 16-31 months children, an age range appropriate for ASD screening and diagnosis, 22 of them diagnosed with ASD, were recorded using the front facing camera in an iPad while they watched on the device screen a movie displaying dynamic stimuli, social stimuli on the left and non-social stimuli on the right. The head and iris position were then automatically analyzed via computer vision algorithms to detect the direction of attention. We validate the proposed framework and computational tool showing that children in the ASD group paid less attention to the movie, showed less attention to the social as compared to the non-social stimuli, and often fixated their attention to one side of the screen. These results are expected from the ASD literature, here obtained with significantly simpler and less expensive attention tracking methods. The proposed method provides a low-cost means of monitoring attention to properly designed stimuli, demonstrating that the integration of stimuli design and automatic response analysis results in the opportunity to use off-the-shelf cameras to assess behavioral biomarkers.},
  archive  = {J},
  author   = {Matthieu Bovery and Geraldine Dawson and Jordan Hashemi and Guillermo Sapiro},
  doi      = {10.1109/TAFFC.2018.2890610},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {722-731},
  title    = {A scalable off-the-shelf framework for measuring patterns of attention in young children and its application in autism spectrum disorder},
  volume   = {12},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Cross-cultural and cultural-specific production and
perception of facial expressions of emotion in the wild. <em>IEEE
Transactions on Affective Computing</em>, <em>12</em>(3), 707–721. (<a
href="https://doi.org/10.1109/TAFFC.2018.2887267">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Automatic recognition of emotion from facial expressions is an intense area of research, with a potentially long list of important application. Yet, the study of emotion requires knowing which facial expressions are used within and across cultures in the wild, not in controlled lab conditions; but such studies do not exist. Which and how many cross-cultural and cultural-specific facial expressions do people commonly use? And, what affect variables does each expression communicate to observers? If we are to design technology that understands the emotion of users, we need answers to these two fundamental questions. In this paper, we present the first large-scale study of the production and visual perception of facial expressions of emotion in the wild. We find that of the 16,384 possible facial configurations that people can theoretically produce, only 35 are successfully used to transmit emotive information across cultures, and only 8 within a smaller number of cultures. Crucially, we find that visual analysis of cross-cultural expressions yields consistent perception of emotion categories and valence, but not arousal. In contrast, visual analysis of cultural-specific expressions yields consistent perception of valence and arousal, but not of emotion categories. Additionally, we find that the number of expressions used to communicate each emotion is also different, e.g., 17 expressions transmit happiness, but only 1 is used to convey disgust.},
  archive  = {J},
  author   = {Ramprakash Srinivasan and Aleix M. Martinez},
  doi      = {10.1109/TAFFC.2018.2887267},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {707-721},
  title    = {Cross-cultural and cultural-specific production and perception of facial expressions of emotion in the wild},
  volume   = {12},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A mathematical description of emotional processes and its
potential applications to affective computing. <em>IEEE Transactions on
Affective Computing</em>, <em>12</em>(3), 692–706. (<a
href="https://doi.org/10.1109/TAFFC.2018.2887385">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In affective computing, machines should detect and recognize human emotions, and adapt their behavior to them. Nowadays these objectives are mainly achieved through the exploitation of machine learning techniques. These techniques are employed to process different emotional-related features and to produce classification labels or coordinates in a valence-arousal space. This approach, however, ignores the neurophysiological processes governing implicit emotional states and, consequently, suffers from substantial limitations. Moreover, machine learning methods employed today do not benefit from the knowledge of emotional dynamics for properly adapting themselves. In this manuscript, starting from recent neuroscience and computational theories, we show how a simple mathematical description of processes governing implicit emotional dynamics can be developed. Moreover, we discuss how our mathematical models can be exploited to improve tracking, estimation and active modulation of human emotions.},
  archive  = {J},
  author   = {Luca Puviani and Sidita Rama and Giorgio Matteo Vitetta},
  doi      = {10.1109/TAFFC.2018.2887385},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {692-706},
  title    = {A mathematical description of emotional processes and its potential applications to affective computing},
  volume   = {12},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Autoencoder for semisupervised multiple emotion detection of
conversation transcripts. <em>IEEE Transactions on Affective
Computing</em>, <em>12</em>(3), 682–691. (<a
href="https://doi.org/10.1109/TAFFC.2018.2885304">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Textual emotion detection is a challenge in computational linguistics and affective computing study as it involves the discovery of all associated emotions expressed within a given piece of text. It becomes an even more difficult problem when applied to conversation transcripts, as we need to model the spoken utterances between speakers, keeping in mind the context of the entire conversation. In this paper, we propose a semisupervised multilabel method of predicting emotions from conversation transcripts. The corpus contains conversational quotes extracted from movies. A small number of them are annotated, while the rest are used for unsupervised training. We use the word2vec word-embedding method to build an emotion lexicon from the corpus and to embed the utterances into vector representations. A deep-learning autoencoder is then used to discover the underlying structure of the unsupervised data. We fine-tune the learned model on labeled training data, and measure its performance on a test set. The experiment result suggests that the method is effective and is only slightly behind human annotators.},
  archive  = {J},
  author   = {Duc-Anh Phan and Yuji Matsumoto and Hiroyuki Shindo},
  doi      = {10.1109/TAFFC.2018.2885304},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {682-691},
  title    = {Autoencoder for semisupervised multiple emotion detection of conversation transcripts},
  volume   = {12},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Using circular models to improve music emotion recognition.
<em>IEEE Transactions on Affective Computing</em>, <em>12</em>(3),
666–681. (<a href="https://doi.org/10.1109/TAFFC.2018.2885744">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The two commonly accepted models of affect used in affective computing are categorical and two-dimensional. However, categorical models are limited to datasets that only contain music for which human annotators fully agree upon, while two-dimensional models use descriptors to which users may not relate to (e.g., Valence and Arousal). This paper explores the hypothesis that the music emotion problem is circular, and shows how circular models can be used for automatic music emotion recognition. This hypothesis is tested through experiments on the two commonly accepted models of affect, as well as on an original circular model proposed by the authors. First, an original dataset was assembled and annotated as a way to investigate agreement among annotators. Then, polygonal approximations of circular regression are proposed as a practical method to investigate whether the circularity of the annotations can be exploited. Experiments with different polygons demonstrate consistent improvements over the categorical model on a dataset containing musical extracts for which the human annotators did not fully agree upon. Finally, a proposed multi-tagging strategy based on the circular predictions is put forward as a pragmatic method to automatically annotate music based on the circular models.},
  archive  = {J},
  author   = {Isabelle Dufour and George Tzanetakis},
  doi      = {10.1109/TAFFC.2018.2885744},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {666-681},
  title    = {Using circular models to improve music emotion recognition},
  volume   = {12},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multilevel longitudinal analysis of shooting performance as
a function of stress and cardiovascular responses. <em>IEEE Transactions
on Affective Computing</em>, <em>12</em>(3), 648–665. (<a
href="https://doi.org/10.1109/TAFFC.2020.2995769">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Virtual reality (VR) systems are increasingly using physiology to improve human training. However, these systems do not account for the complex intra-individual variability in physiology and human performance across multiple timescales and psychophysiological demands. To fill this gap, we propose a theory of multilevel variability where tractable neurobiological mechanisms generate complex variability in performance over time and in response to heterogeneous sources. Based on this theory, we also present a study that examines changes in cardiovascular activity and performance during a stressful shooting task in VR. We examined physiology and performance at three important levels of analysis: task-to-task, block-to-block, session-to-session. Findings indicated joint patterns of physiology and performance that notably varied by the level of analysis. At the task level, higher task difficulty worsened performance but did not change cardiovascular activation. At the block level, there were nonlinear changes in performance and heart rate variability. At the session level, performance improved while blood pressure decreased and heart rate variability increased across days. Of all the physiological metrics, only heart rate variability was correlated with marksmanship performance. Findings are consistent with our multilevel theory and highlight the need for VR and other affective computing systems to assess physiology across multiple timescales.},
  archive  = {J},
  author   = {Derek P. Spangler and Sazedul Alam and Saad Rahman and Joshua Crone and Ryan Robucci and Nilanjan Banerjee and Scott E. Kerick and Justin R. Brooks},
  doi      = {10.1109/TAFFC.2020.2995769},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {648-665},
  title    = {Multilevel longitudinal analysis of shooting performance as a function of stress and cardiovascular responses},
  volume   = {12},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Longitudinal observational evidence of the impact of emotion
regulation strategies on affective expression. <em>IEEE Transactions on
Affective Computing</em>, <em>12</em>(3), 636–647. (<a
href="https://doi.org/10.1109/TAFFC.2019.2961912">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The ability to regulate our emotions plays an important role in our psychological and physical health. Regulating emotions influences how and when emotions are expressed. We performed a large scale, longitudinal observational study to investigate the effect of emotion regulation ability on expressed affect. We found that expression of negative affect increased throughout the day. For people who suppress emotion this increase is slower that for those who do not. For those with stronger cognitive reappraisal abilities, though not significant, there was a trend for higher positive affect and negative affect increased significantly less steeply, suggesting that they might experience more positive and less negative affect. These results reflect some of the first results based on large scale, continuous tracking of behavioral expression of emotion longitudinally. Our results demonstrate the need to carefully consider the time of day and emotion regulation ability, in addition to gender and age, when attempting to automatically infer affective states for facial behavior.},
  archive  = {J},
  author   = {Daniel McDuff and Eunice Jun and Kael Rowan and Mary Czerwinski},
  doi      = {10.1109/TAFFC.2019.2961912},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {636-647},
  title    = {Longitudinal observational evidence of the impact of emotion regulation strategies on affective expression},
  volume   = {12},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Task load estimation from multimodal head-worn sensors using
event sequence features. <em>IEEE Transactions on Affective
Computing</em>, <em>12</em>(3), 622–635. (<a
href="https://doi.org/10.1109/TAFFC.2019.2956135">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {For longitudinal behavior analysis, task type is an inevitable and important variable. In this article, we propose an event-based behavior modeling approach and employ non-invasive wearable sensing modalities (eye activity, speech and head movement) to recognize task load level under four different task load types. The novelty lies in converting physiological and behavioral signals into meaningful events and utilizing their sequence across multiple modalities to distinguish load levels and types. We evaluated this approach on head-worn sensor data from 24 participants completing four different tasks for recognizing (i) low and high load level for a given task load type, (ii) low and high load level regardless of load type, and (iii) both load level and load type. Findings show that the recognition rate is reasonable in (i), close to chance level in (ii), and well above chance level in (iii) for 8 classes using participant-dependent and -independent schemes. Further, a fusion of the proposed event-based features and conventional continuous features achieved the best or similar performance in most cases. These results suggest that task type needs to be considered when using continuous features and that the proposed event-based modeling paradigm is promising for longitudinal behavior analysis.},
  archive  = {J},
  author   = {Siyuan Chen and Julien Epps},
  doi      = {10.1109/TAFFC.2019.2956135},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {622-635},
  title    = {Task load estimation from multimodal head-worn sensors using event sequence features},
  volume   = {12},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On the effect of observed subject biases in apparent
personality analysis from audio-visual signals. <em>IEEE Transactions on
Affective Computing</em>, <em>12</em>(3), 607–621. (<a
href="https://doi.org/10.1109/TAFFC.2019.2956030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Personality perception is implicitly biased due to many subjective factors, such as cultural, social, contextual, gender, and appearance. Approaches developed for automatic personality perception are not expected to predict the real personality of the target but the personality external observers attributed to it. Hence, they have to deal with human bias, inherently transferred to the training data. However, bias analysis in personality computing is an almost unexplored area. In this article, we study different possible sources of bias affecting personality perception, including emotions from facial expressions, attractiveness, age, gender, and ethnicity, as well as their influence on prediction ability for apparent personality estimation. To this end, we propose a multimodal deep neural network that combines raw audio and visual information alongside predictions of attribute-specific models to regress apparent personality. We also analyze spatio-temporal aggregation schemes and the effect of different time intervals on first impressions. We base our study on the ChaLearn first impressions dataset, consisting of one-person conversational videos. Our model shows state-of-the-art results regressing apparent personality based on the Big-Five model. Furthermore, given the interpretability nature of our network design, we provide an incremental analysis on the impact of each possible source of bias on final network predictions.},
  archive  = {J},
  author   = {Ricardo Darío Pérez Principi and Cristina Palmero and Julio C. S. Jacques Junior and Sergio Escalera},
  doi      = {10.1109/TAFFC.2019.2956030},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {607-621},
  title    = {On the effect of observed subject biases in apparent personality analysis from audio-visual signals},
  volume   = {12},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Exploiting multi-CNN features in CNN-RNN based dimensional
emotion recognition on the OMG in-the-wild dataset. <em>IEEE
Transactions on Affective Computing</em>, <em>12</em>(3), 595–606. (<a
href="https://doi.org/10.1109/TAFFC.2020.3014171">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This article presents a novel CNN-RNN based approach, which exploits multiple CNN features for dimensional emotion recognition in-the-wild, utilizing the One-Minute Gradual-Emotion (OMG-Emotion) dataset. Our approach includes first pre-training with the relevant and large in size, Aff-Wild and Aff-Wild2 emotion databases. Low-, mid- and high-level features are extracted from the trained CNN component and are exploited by RNN subnets in a multi-task framework. Their outputs constitute an intermediate level prediction; final estimates are obtained as the mean or median values of these predictions. Fusion of the networks is also examined for boosting the obtained performance, at Decision-, or at Model-level; in the latter case a RNN was used for the fusion. Our approach, although using only the visual modality, outperformed state-of-the-art methods that utilized audio and visual modalities. Some of our developments have been submitted to the OMG-Emotion Challenge, ranking second among the technologies which used only visual information for valence estimation; ranking third overall. Through extensive experimentation, we further show that arousal estimation is greatly improved when low-level features are combined with high-level ones.},
  archive  = {J},
  author   = {Dimitrios Kollias and Stefanos Zafeiriou},
  doi      = {10.1109/TAFFC.2020.3014171},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {595-606},
  title    = {Exploiting multi-CNN features in CNN-RNN based dimensional emotion recognition on the OMG in-the-wild dataset},
  volume   = {12},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Modeling emotion in complex stories: The stanford emotional
narratives dataset. <em>IEEE Transactions on Affective Computing</em>,
<em>12</em>(3), 579–594. (<a
href="https://doi.org/10.1109/TAFFC.2019.2955949">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Human emotions unfold over time, and more affective computing research has to prioritize capturing this crucial component of real-world affect. Modeling dynamic emotional stimuli requires solving the twin challenges of time-series modeling and of collecting high-quality time-series datasets. We begin by assessing the state-of-the-art in time-series emotion recognition, and we review contemporary time-series approaches in affective computing, including discriminative and generative models. We then introduce the first version of the Stanford Emotional Narratives Dataset (SENDv1): a set of rich, multimodal videos of self-paced, unscripted emotional narratives, annotated for emotional valence over time. The complex narratives and naturalistic expressions in this dataset provide a challenging test for contemporary time-series emotion recognition models. We demonstrate several baseline and state-of-the-art modeling approaches on the SEND, including a Long Short-Term Memory model and a multimodal Variational Recurrent Neural Network, which perform comparably to the human-benchmark. We end by discussing the implications for future research in time-series affective computing.},
  archive  = {J},
  author   = {Desmond C. Ong and Zhengxuan Wu and Zhi-Xuan Tan and Marianne Reddan and Isabella Kahhale and Alison Mattek and Jamil Zaki},
  doi      = {10.1109/TAFFC.2019.2955949},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {579-594},
  title    = {Modeling emotion in complex stories: The stanford emotional narratives dataset},
  volume   = {12},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Spatio-temporal encoder-decoder fully convolutional network
for video-based dimensional emotion recognition. <em>IEEE Transactions
on Affective Computing</em>, <em>12</em>(3), 565–578. (<a
href="https://doi.org/10.1109/TAFFC.2019.2940224">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Video-based dimensional emotion recognition aims to map human affect into the dimensional emotion space based on visual signals, which is a fundamental challenge in affective computing and human-computer interaction. In this paper, we present a novel encoder-decoder framework to tackle this problem. It adopts a fully convolutional design with the cascaded 2D convolution based spatial encoder and 1D convolution based temporal encoder-decoder for joint spatio-temporal modeling. In particular, to address the key issue of capturing discriminative long-term dynamic dependency, our temporal model, referred to as Temporal Hourglass Convolutional Neural Network (TH-CNN), extracts contextual relationship through integrating both low-level encoded and high-level decoded clues. Temporal Intermediate Supervision (TIS) is then introduced to enhance affective representations generated by TH-CNN under a multi-resolution strategy, which guides TH-CNN to learn macroscopic long-term trend and refined short-term fluctuations progressively. Furthermore, thanks to TH-CNN and TIS, knowledge learnt from the intermediate layers also makes it possible to offer customized solutions to different applications by adjusting the decoder depth. Extensive experiments are conducted on three benchmark databases (RECOLA, SEWA and OMG) and superior results are shown compared to state-of-the-art methods, which indicates the effectiveness of the proposed approach.},
  archive  = {J},
  author   = {Zhengyin Du and Suowei Wu and Di Huang and Weixin Li and Yunhong Wang},
  doi      = {10.1109/TAFFC.2019.2940224},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {565-578},
  title    = {Spatio-temporal encoder-decoder fully convolutional network for video-based dimensional emotion recognition},
  volume   = {12},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). EmoBed: Strengthening monomodal emotion recognition via
training with crossmodal emotion embeddings. <em>IEEE Transactions on
Affective Computing</em>, <em>12</em>(3), 553–564. (<a
href="https://doi.org/10.1109/TAFFC.2019.2928297">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Despite remarkable advances in emotion recognition, they are severely restrained from either the essentially limited property of the employed single modality, or the synchronous presence of all involved multiple modalities. Motivated by this, we propose a novel crossmodal emotion embedding framework called EmoBed, which aims to leverage the knowledge from other auxiliary modalities to improve the performance of an emotion recognition system at hand. The framework generally includes two main learning components, i.e., joint multimodal training and crossmodal training. Both of them tend to explore the underlying semantic emotion information but with a shared recognition network or with a shared emotion embedding space, respectively. In doing this, the enhanced system trained with this approach can efficiently make use of the complementary information from other modalities. Nevertheless, the presence of these auxiliary modalities is not demanded during inference. To empirically investigate the effectiveness and robustness of the proposed framework, we perform extensive experiments on the two benchmark databases RECOLA and OMG-Emotion for the tasks of dimensional emotion regression and categorical emotion classification, respectively. The obtained results show that the proposed framework significantly outperforms related baselines in monomodal inference, and are also competitive or superior to the recently reported systems, which emphasises the importance of the proposed crossmodal learning for emotion recognition.},
  archive  = {J},
  author   = {Jing Han and Zixing Zhang and Zhao Ren and Björn Schuller},
  doi      = {10.1109/TAFFC.2019.2928297},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {553-564},
  title    = {EmoBed: Strengthening monomodal emotion recognition via training with crossmodal emotion embeddings},
  volume   = {12},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Special issue on automated perception of human affect from
longitudinal behavioral data. <em>IEEE Transactions on Affective
Computing</em>, <em>12</em>(3), 551–552. (<a
href="https://doi.org/10.1109/TAFFC.2021.3079535">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The papers in this special section are aimed at contributions from computational neuroscience and psychology, artificial intelligence, machine learning, and affective computing, challenging and expanding current research on interpretation and estimation of human affective behavior from longitudinal data, i.e., single or multiple modalities captured over extended periods of time allowing efficient representation of behavior and inference in terms of affect and other socio-cognitive dimensions.},
  archive  = {J},
  author   = {Pablo Barros and Stefan Wermter and Ognjen Rudovic and Hatice Gunes},
  doi      = {10.1109/TAFFC.2021.3079535},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {7},
  number   = {3},
  pages    = {551-552},
  title    = {Special issue on automated perception of human affect from longitudinal behavioral data},
  volume   = {12},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Facial expression recognition with identity and emotion
joint learning. <em>IEEE Transactions on Affective Computing</em>,
<em>12</em>(2), 544–550. (<a
href="https://doi.org/10.1109/TAFFC.2018.2880201">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Different subjects may express a specific expression in different ways due to inter-subject variabilities. In this work, besides training deep-learned facial expression feature (emotional feature), we also consider the influence of latent face identity feature such as the shape or appearance of face. We propose an identity and emotion joint learning approach with deep convolutional neural networks (CNNs) to enhance the performance of facial expression recognition (FER) tasks. First, we learn the emotion and identity features separately using two different CNNs with their corresponding training data. Second, we concatenate these two features together as a deep-learned Tandem Facial Expression (TFE) Feature and feed it to the subsequent fully connected layers to form a new model. Finally, we perform joint learning on the newly merged network using only the facial expression training data. Experimental results show that our proposed approach achieves 99.31 and 84.29 percent accuracy on the CK+ and the FER+ database, respectively, which outperforms the residual network baseline as well as many other state-of-the-art methods.},
  archive  = {J},
  author   = {Ming Li and Hao Xu and Xingchang Huang and Zhanmei Song and Xiaolin Liu and Xin Li},
  doi      = {10.1109/TAFFC.2018.2880201},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {544-550},
  title    = {Facial expression recognition with identity and emotion joint learning},
  volume   = {12},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep learning for human affect recognition: Insights and new
developments. <em>IEEE Transactions on Affective Computing</em>,
<em>12</em>(2), 524–543. (<a
href="https://doi.org/10.1109/TAFFC.2018.2890471">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Automatic human affect recognition is a key step towards more natural human-computer interaction. Recent trends include recognition in the wild using a fusion of audiovisual and physiological sensors, a challenging setting for conventional machine learning algorithms. Since 2010, novel deep learning algorithms have been applied increasingly in this field. In this paper, we review the literature on human affect recognition between 2010 and 2017, with a special focus on approaches using deep neural networks. By classifying a total of 950 studies according to their usage of shallow or deep architectures, we are able to show a trend towards deep learning. Reviewing a subset of 233 studies that employ deep neural networks, we comprehensively quantify their applications in this field. We find that deep learning is used for learning of (i) spatial feature representations, (ii) temporal feature representations, and (iii) joint feature representations for multimodal sensor data. Exemplary state-of-the-art architectures illustrate the progress. Our findings show the role deep architectures will play in human affect recognition, and can serve as a reference point for researchers working on related applications.},
  archive  = {J},
  author   = {Philipp V. Rouast and Marc T. P. Adam and Raymond Chiong},
  doi      = {10.1109/TAFFC.2018.2890471},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {524-543},
  title    = {Deep learning for human affect recognition: Insights and new developments},
  volume   = {12},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Survey on emotional body gesture recognition. <em>IEEE
Transactions on Affective Computing</em>, <em>12</em>(2), 505–523. (<a
href="https://doi.org/10.1109/TAFFC.2018.2874986">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Automatic emotion recognition has become a trending research topic in the past decade. While works based on facial expressions or speech abound, recognizing affect from body gestures remains a less explored topic. We present a new comprehensive survey hoping to boost research in the field. We first introduce emotional body gestures as a component of what is commonly known as ”body language” and comment general aspects as gender differences and culture dependence. We then define a complete framework for automatic emotional body gesture recognition. We introduce person detection and comment static and dynamic body pose estimation methods both in RGB and 3D. We then comment the recent literature related to representation learning and emotion recognition from images of emotionally expressive gestures. We also discuss multi-modal approaches that combine speech or face with body gestures for improved emotion recognition. While pre-processing methodologies (e.g., human detection and pose estimation) are nowadays mature technologies fully developed for robust large scale analysis, we show that for emotion recognition the quantity of labelled data is scarce. There is no agreement on clearly defined output spaces and the representations are shallow and largely based on naive geometrical representations.},
  archive  = {J},
  author   = {Fatemeh Noroozi and Ciprian Adrian Corneanu and Dorota Kamińska and Tomasz Sapiński and Sergio Escalera and Gholamreza Anbarjafari},
  doi      = {10.1109/TAFFC.2018.2874986},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {505-523},
  title    = {Survey on emotional body gesture recognition},
  volume   = {12},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A bi-hemisphere domain adversarial neural network model for
EEG emotion recognition. <em>IEEE Transactions on Affective
Computing</em>, <em>12</em>(2), 494–504. (<a
href="https://doi.org/10.1109/TAFFC.2018.2885474">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In this paper, we propose a novel neural network model, called bi-hemisphere domain adversarial neural network (BiDANN) model, for electroencephalograph (EEG) emotion recognition. The BiDANN model is inspired by the neuroscience findings that the left and right hemispheres of human&#39;s brain are asymmetric to the emotional response. It contains a global and two local domain discriminators that work adversarially with a classifier to learn discriminative emotional features for each hemisphere. At the same time, it tries to reduce the possible domain differences in each hemisphere between the source and target domains so as to improve the generality of the recognition model. In addition, we also propose an improved version of BiDANN, denoted by BiDANN-S, for subject-independent EEG emotion recognition problem by lowering the influences of the personal information of subjects to the EEG emotion recognition. Extensive experiments on the SEED database are conducted to evaluate the performance of both BiDANN and BiDANN-S. The experimental results have shown that the proposed BiDANN and BiDANN models achieve state-of-the-art performance in the EEG emotion recognition.},
  archive  = {J},
  author   = {Yang Li and Wenming Zheng and Yuan Zong and Zhen Cui and Tong Zhang and Xiaoyan Zhou},
  doi      = {10.1109/TAFFC.2018.2885474},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {494-504},
  title    = {A bi-hemisphere domain adversarial neural network model for EEG emotion recognition},
  volume   = {12},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). AMIGOS: A dataset for affect, personality and mood research
on individuals and groups. <em>IEEE Transactions on Affective
Computing</em>, <em>12</em>(2), 479–493. (<a
href="https://doi.org/10.1109/TAFFC.2018.2884461">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We present AMIGOS- A dataset for Multimodal research of affect, personality traits and mood on Individuals and GrOupS. Different to other databases, we elicited affect using both short and long videos in two social contexts, one with individual viewers and one with groups of viewers. The database allows the multimodal study of the affective responses, by means of neuro-physiological signals of individuals in relation to their personality and mood, and with respect to the social context and videos&#39; duration. The data is collected in two experimental settings. In the first one, 40 participants watched 16 short emotional videos. In the second one, the participants watched 4 long videos, some of them alone and the rest in groups. The participants&#39; signals, namely, Electroencephalogram (EEG), Electrocardiogram (ECG) and Galvanic Skin Response (GSR), were recorded using wearable sensors. Participants&#39; frontal HD video and both RGB and depth full body videos were also recorded. Participants emotions have been annotated with both self-assessment of affective levels (valence, arousal, control, familiarity, liking and basic emotions) felt during the videos as well as external-assessment of levels of valence and arousal. We present a detailed correlation analysis of the different dimensions as well as baseline methods and results for single-trial classification of valence and arousal, personality traits, mood and social context. The database is made publicly available.},
  archive  = {J},
  author   = {Juan Abdon Miranda-Correa and Mojtaba Khomami Abadi and Nicu Sebe and Ioannis Patras},
  doi      = {10.1109/TAFFC.2018.2884461},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {479-493},
  title    = {AMIGOS: A dataset for affect, personality and mood research on individuals and groups},
  volume   = {12},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sentiment polarity classification at EVALITA: Lessons
learned and open challenges. <em>IEEE Transactions on Affective
Computing</em>, <em>12</em>(2), 466–478. (<a
href="https://doi.org/10.1109/TAFFC.2018.2884015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Sentiment analysis in social media is a popular task attracting the interest of the research community, also in recent evaluation campaigns of natural language processing tasks in several languages. We report on our experience in the organization of SENTIment POLarity Classification Task (SENTIPOLC), a shared task on sentiment classification of Italian tweets, proposed for the first time in 2014 within the Evalita evaluation campaign. We present the datasets-which include an enriched annotation scheme for dealing with the impact of figurative language on polarity-the evaluation methodology, and discuss the approaches and results of participating systems. We also offer a reflection on the open challenges of state-of-the-art systems for sentiment analysis of microblogging in Italian, as they emerge from a qualitative analysis of misclassified tweets. Finally, we provide an evaluation of the resources we have created, and share the lessons learned by running this task for two consecutive editions.},
  archive  = {J},
  author   = {Valerio Basile and Nicole Novielli and Danilo Croce and Francesco Barbieri and Malvina Nissim and Viviana Patti},
  doi      = {10.1109/TAFFC.2018.2884015},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {466-478},
  title    = {Sentiment polarity classification at EVALITA: Lessons learned and open challenges},
  volume   = {12},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Biofeedback arrests sympathetic and behavioral effects in
distracted driving. <em>IEEE Transactions on Affective Computing</em>,
<em>12</em>(2), 453–465. (<a
href="https://doi.org/10.1109/TAFFC.2018.2883950">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Operating machinery while distracted is a dangerous behavior, often habitual, which is the source of accidents. Distracted driving in particular has assumed the form of an epidemic, fueled by the ubiquity of smartphone usage and the tendency to slip into absent-mindedness in tedious commutes. Here we show that a method capable of detecting and communicating overarousal trends associated with the onset of distractions, can pull the driver out of a downward psychophysiological spiral. The method is reliable, unobtrusive, and subtle in its intervention-all important characteristics for real-time corrections on human handling of critical machinery. Arousal estimation is performed by a conservative statistical filter acting upon the driver&#39;s perinasal perspiration signal, as this is continuously extracted from a thermal imaging feed. Overarousal notices are communicated via a visual indicator placed in the driver&#39;s peripheral vision. Using this method, we conducted a parallel group experiment, where a control CLCL (n=23n=23) and a biofeedback BFBF (n=24n=24) cohort were distracted mentally and physically while driving, with only the biofeedback group receiving the benefit of overarousal notification. Results show that heeding biofeedback notices, cuts dramatically the time BFBF subjects are engaged in distractions with respect to the control group, significantly reducing their arousal levels and improving their driving behaviors in the context of a typical commute.},
  archive  = {J},
  author   = {Ioannis Pavlidis and Ashik Khatri and Pradeep Buddharaju and Michael Manser and Robert Wunderlich and Ergun Akleman and Panagiotis Tsiamyrtzis},
  doi      = {10.1109/TAFFC.2018.2883950},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {453-465},
  title    = {Biofeedback arrests sympathetic and behavioral effects in distracted driving},
  volume   = {12},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Compensation techniques for speaker variability in
continuous emotion prediction. <em>IEEE Transactions on Affective
Computing</em>, <em>12</em>(2), 439–452. (<a
href="https://doi.org/10.1109/TAFFC.2018.2883044">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Continuous time-varying prediction of emotions based on speech in terms of attributes (i.e., arousal) has received considerable attention in the past few years. However, the variability introduced by factors not related to emotion, such as speaker and phonetic variability, which in turn may lead to less reliable models and less accurate emotion predictions, has not been fully explored yet. In particular, even though speaker variability has been shown to be a significant confounding factor in continuous emotion prediction systems, there remains a paucity of analyses about how speaker variability affects continuous emotion prediction systems and which methods can be applied to compensate for this variability. This paper first formulates speaker variability systematically in terms of probability distributions in both feature and model spaces, and quantifies the effect of speaker variability by comparing inter- and intra-speaker variability between speaker-dependent models. Second, two compensation techniques based on partial least squares dimensional reduction and feature mapping are proposed. Finally, the effectiveness of the proposed techniques is validated on three databases, across which they show consistent improvement in arousal, valence and dominance prediction. Additional quantitative analyse reveals that the two proposed techniques compensate for speaker variability in both the feature and model spaces simultaneously.},
  archive  = {J},
  author   = {Ting Dang and Vidhyasaharan Sethu and Eliathamby Ambikairajah},
  doi      = {10.1109/TAFFC.2018.2883044},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {439-452},
  title    = {Compensation techniques for speaker variability in continuous emotion prediction},
  volume   = {12},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Layered-modeling of affective and sensory experiences using
structural equation modeling: Touch experiences of plastic surfaces as
an example. <em>IEEE Transactions on Affective Computing</em>,
<em>12</em>(2), 429–438. (<a
href="https://doi.org/10.1109/TAFFC.2018.2879944">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Developing a multilayered structure of adjective words that explains semantic relationships between human perceptual and affective responses to stimuli is instrumental in the design of affective aspects of products. However, the determination of multilayered structure is demanding and, thus far, it has been conducted by experienced developers in a trial-and-error manner. This study developed a method to systematically establish such structures through common tasks for sensory evaluation where products are rated along adjective labels. This method gradually expands the model from a simple two-layered to complex multilayered structures until it is accepted by structural equation modeling. The lower and higher layers of the initial two-layered model are composed of sensory and affective adjectives, respectively. The parts with weak fit indices of the higher layer are then remodeled, resulting in a multilayered affective structure. To validate the method, we built adjective structures based on responses to touching plastic plates. The method resulted in three- and four-layered structures that were quantitatively and semantically valid.},
  archive  = {J},
  author   = {Shogo Okamoto and Haruyo Kojima and Atsushi Yamagishi and Kyoichi Kato and Atsuko Tamada},
  doi      = {10.1109/TAFFC.2018.2879944},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {429-438},
  title    = {Layered-modeling of affective and sensory experiences using structural equation modeling: Touch experiences of plastic surfaces as an example},
  volume   = {12},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Brain dynamics during arousal-dependent pleasant/unpleasant
visual elicitation: An electroencephalographic study on the circumplex
model of affect. <em>IEEE Transactions on Affective Computing</em>,
<em>12</em>(2), 417–428. (<a
href="https://doi.org/10.1109/TAFFC.2018.2879343">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Emotion regulation to pleasant and unpleasant stimuli involves several brain areas, such as the prefrontal cortex, amygdala, and insular cortex. However, how a specific arousal level affects such brain dynamics is not fully understood. To this effect, we propose an electroencephalography (EEG)-based study, where 22 healthy subjects were emotionally elicited through affective pictures gathered from the International Affective Picture System. Based on the circumplex model of affect, we used four arousing levels, each with two valence levels (i.e., pleasant and unpleasant). Considering these levels, we investigated the EEG power spectra and functional connectivity among channels. We then used this information to build an automatic valence classifier. The experimental results showed that the functional connectivity at the highest frequency bands (i.e., &gt; 30 Hz) was most sensitive to arousal modulation. Specifically, high connectivity over the right hemisphere occurred during pleasant elicitation, whereas that over the left hemisphere occurred during negative elicitation. In addition, short-range connections in the frontal regions became weaker with increased arousal level, whereas long-range connections were enhanced. Concerning the spectral analysis, the most significant valence-dependent changes were found at intermediate arousing elicitations over the prefrontal and occipital regions. The automatic valence classification showed a recognition accuracy of up to 86.37 percent.},
  archive  = {J},
  author   = {Alberto Greco and Gaetano Valenza and Enzo Pasquale Scilingo},
  doi      = {10.1109/TAFFC.2018.2879343},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {417-428},
  title    = {Brain dynamics during arousal-dependent Pleasant/Unpleasant visual elicitation: An electroencephalographic study on the circumplex model of affect},
  volume   = {12},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Predicting emotionally salient regions using qualitative
agreement of deep neural network regressors. <em>IEEE Transactions on
Affective Computing</em>, <em>12</em>(2), 402–416. (<a
href="https://doi.org/10.1109/TAFFC.2018.2878715">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Automatic emotion recognition plays a crucial role in various fields such as healthcare, human-computer interaction (HCI) and security and defense. While most of previous studies have focused on the recognition of emotion in isolated utterances, a more natural approach is to continuously track emotions during human interaction, identifying regions that are highly emotional. This study proposes a framework to define emotionally salient regions (hotspots), which we then attempt to dynamically detect. Our proposed approach defines hotspots relying on the qualitative agreement (QA) method, which searches for trends across continuous-time evaluations provided by different raters for arousal and valence. We illustrate the benefits of the QA method over averaging absolute values of the traces without considering trends across evaluators. After defining hotspot regions, we propose a deep learning framework to automatically detect these emotional hotspots. The proposed method relies on an ensemble of bidirectional long short term memory (BLSTM) regressors, trained on individual emotional traces provided by the evaluators, which are combined to automatically detect emotional hotspots. An appealing fusion approach to combine these regressors is to rely again on the QA method, which detects emotional salient regions with F1-scores as high as 60.9 percent for arousal and 50.4 percent for valence on the RECOLA dataset.},
  archive  = {J},
  author   = {Srinivas Parthasarathy and Carlos Busso},
  doi      = {10.1109/TAFFC.2018.2878715},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {402-416},
  title    = {Predicting emotionally salient regions using qualitative agreement of deep neural network regressors},
  volume   = {12},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On the influence of affect in EEG-based subject
identification. <em>IEEE Transactions on Affective Computing</em>,
<em>12</em>(2), 391–401. (<a
href="https://doi.org/10.1109/TAFFC.2018.2877986">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Biometric signals have been extensively used for user identification and authentication due to their inherent characteristics that are unique to each person. The variation exhibited between the brain signals (EEG) of different people makes such signals especially suitable for biometric user identification. However, the characteristics of these signals are also influenced by the user&#39;s current condition, including his/her affective state. In this paper, we analyze the significance of the affect-related component of brain signals within the subject identification context. Consistent results are obtained across three different public datasets, suggesting that the dominant component of the signal is subject-related, but the affective state also has a contribution that affects identification accuracy. Results show that identification accuracy increases when the system has been trained with EEG recordings that refer to similar affective states as the sample that is to be identified. This improvement holds independently of the features and classification algorithm used, and it is generally above 10 percent under a rigorous setting, when the training and validation datasets do not share data from the same recording days. This finding emphasizes the potential benefits of considering affective information in applications that require subject identification, such as user authentication.},
  archive  = {J},
  author   = {Pablo Arnau-González and Miguel Arevalillo-Herráez and Stamos Katsigiannis and Naeem Ramzan},
  doi      = {10.1109/TAFFC.2018.2877986},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {391-401},
  title    = {On the influence of affect in EEG-based subject identification},
  volume   = {12},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Automatic recognition of facial displays of unfelt emotions.
<em>IEEE Transactions on Affective Computing</em>, <em>12</em>(2),
377–390. (<a href="https://doi.org/10.1109/TAFFC.2018.2874996">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Humans modify their facial expressions in order to communicate their internal states and sometimes to mislead observers regarding their true emotional states. Evidence in experimental psychology shows that discriminative facial responses are short and subtle. This suggests that such behavior would be easier to distinguish when captured in high resolution at an increased frame rate. We are proposing SASE-FE, the first dataset of facial expressions that are either congruent or incongruent with underlying emotion states. We show that overall the problem of recognizing whether facial movements are expressions of authentic emotions or not can be successfully addressed by learning spatio-temporal representations of the data. For this purpose, we propose a method that aggregates features along fiducial trajectories in a deeply learnt space. Performance of the proposed model shows that on average, it is easier to distinguish among genuine facial expressions of emotion than among unfelt facial expressions of emotion and that certain emotion pairs such as contempt and disgust are more difficult to distinguish than the rest. Furthermore, the proposed methodology improves state of the art results on CK+ and OULU-CASIA datasets for video emotion recognition, and achieves competitive results when classifying facial action units on BP4D datase.},
  archive  = {J},
  author   = {Kaustubh Kulkarni and Ciprian Adrian Corneanu and Ikechukwu Ofodile and Sergio Escalera and Xavier Baró and Sylwia Hyniewska and Jüri Allik and Gholamreza Anbarjafari},
  doi      = {10.1109/TAFFC.2018.2874996},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {377-390},
  title    = {Automatic recognition of facial displays of unfelt emotions},
  volume   = {12},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep learning for spatio-temporal modeling of dynamic
spontaneous emotions. <em>IEEE Transactions on Affective Computing</em>,
<em>12</em>(2), 363–376. (<a
href="https://doi.org/10.1109/TAFFC.2018.2873600">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Facial expressions involve dynamic morphological changes in a face, conveying information about the expresser&#39;s feelings. Each emotion has a specific spatial deformation over the face and temporal profile with distinct time segments. We aim at modeling the human dynamic emotional behavior by taking into consideration the visual content of the face and its evolution. But emotions can both speed-up or slow-down, therefore it is important to incorporate information from the local neighborhood frames (short-term dependencies) and the global setting (long-term dependencies) to summarize the segment context despite of its time variations. A 3D-Convolutional Neural Networks (3D-CNN) is used to learn early local spatiotemporal features. The 3D-CNN is designed to capture subtle spatiotemporal changes that may occur on the face. Then, a Convolutional-Long-Short-Term-Memory (ConvLSTM) network is designed to learn semantic information by taking into account longer spatiotemporal dependencies. The ConvLSTM network helps considering the global visual saliency of the expression. That is locating and learning features in space and time that stand out from their local neighbors in order to signify distinctive facial expression features along the entire sequence. Non-variant representations based on aggregating global spatiotemporal features at increasingly fine resolutions are then done using a weighted Spatial Pyramid Pooling layer.},
  archive  = {J},
  author   = {Dawood Al Chanti and Alice Caplier},
  doi      = {10.1109/TAFFC.2018.2873600},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {363-376},
  title    = {Deep learning for spatio-temporal modeling of dynamic spontaneous emotions},
  volume   = {12},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Towards transparent robot learning through TDRL-based
emotional expressions. <em>IEEE Transactions on Affective
Computing</em>, <em>12</em>(2), 352–362. (<a
href="https://doi.org/10.1109/TAFFC.2019.2893348">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Robots and virtual agents need to adapt existing and learn novel behavior to function autonomously in our society. Robot learning is often in interaction with or in the vicinity of humans. As a result the learning process needs to be transparent to humans. Reinforcement Learning (RL) has been used successfully for robot task learning. However, this learning process is often not transparent to the users. This results in a lack of understanding of what the robot is trying to do and why. The lack of transparency will directly impact robot learning. The expression of emotion is used by humans and other animals to signal information about the internal state of the individual in a language-independent, and even species-independent way, also during learning and exploration. In this article we argue that simulation and subsequent expression of emotion should be used to make the learning process of robots more transparent. We propose that the TDRL Theory of Emotion gives sufficient structure on how to develop such an emotionally expressive learning robot. Finally, we argue that next to such a generic model of RL-based emotion simulation we need personalized emotion interpretation for robots to better cope with individual expressive differences of users.},
  archive  = {J},
  author   = {Joost Broekens and Mohamed Chetouani},
  doi      = {10.1109/TAFFC.2019.2893348},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {352-362},
  title    = {Towards transparent robot learning through TDRL-based emotional expressions},
  volume   = {12},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Embodied robot models for interdisciplinary emotion
research. <em>IEEE Transactions on Affective Computing</em>,
<em>12</em>(2), 340–351. (<a
href="https://doi.org/10.1109/TAFFC.2019.2908162">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Due to their complex nature, emotions cannot be properly understood from the perspective of a single discipline. In this paper, I discuss how the use of robots as models is beneficial for interdisciplinary emotion research. Addressing this issue through the lens of my own research, I focus on a critical analysis of embodied robots models of different aspects of emotion, relate them to theories in psychology and neuroscience, and provide representative examples. I discuss concrete ways in which embodied robot models can be used to carry out interdisciplinary emotion research, assessing their contributions: as hypothetical models, and as operational models of specific emotional phenomena, of general emotion principles, and of specific emotion “dimensions”. I conclude by discussing the advantages of using embodied robot models over other models.},
  archive  = {J},
  author   = {Lola Cañamero},
  doi      = {10.1109/TAFFC.2019.2908162},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {340-351},
  title    = {Embodied robot models for interdisciplinary emotion research},
  volume   = {12},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A computational model of focused attention meditation and
its transfer to a sustained attention task. <em>IEEE Transactions on
Affective Computing</em>, <em>12</em>(2), 329–339. (<a
href="https://doi.org/10.1109/TAFFC.2019.2908172">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Meditation has been shown to aid with the management of affective disorders through improving emotion regulation. Here we begin to develop a theory of meditation by creating a computational cognitive model of focused attention meditation. Our model was created within Prims, a derivative of the ACT-R cognitive architecture. We implemented a model based on an extensive literature review of how the meditation experience unfolds over time. We then tested the Prims model in a sustained attention task, intending to capture a faculty that may be trained with meditation practice. The model was significantly better able to maintain focus after the meditation practice than before. These results agree qualitatively with empirical findings of a longitudinal study on the effects of meditation conducted in 2010. The central mechanism for increasing task relevant focus in the model seems to be a feedback loop. The meditation and mind-wandering processes reinforce themselves and weaken the other. However, this reinforcement is more dispersed in the more elaborate mind-wandering process, which causes it to decrease over time. We speculate that observed improvements in emotion regulation observed after meditation arise from the ability to maintain focus, because it allows the practitioner to avoid emotions spiraling out of control.},
  archive  = {J},
  author   = {Amir J. Moye and Marieke K. van Vugt},
  doi      = {10.1109/TAFFC.2019.2908172},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {329-339},
  title    = {A computational model of focused attention meditation and its transfer to a sustained attention task},
  volume   = {12},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A hybrid cognitive architecture with primal affect and
physiology. <em>IEEE Transactions on Affective Computing</em>,
<em>12</em>(2), 318–328. (<a
href="https://doi.org/10.1109/TAFFC.2019.2906162">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Though computational cognitive architectures have been used to study several processes associated with human behavior, the study of integration of affect and emotion in these processes has been relatively sparse. Theory from affective science and affective neuroscience can be used to systematically integrate affect into cognitive architectures, particularly in areas where cognitive system behavior is known to be associated with physiological structure and behavior. I introduce a unified theory and model of human behavior that integrates physiology and primal affect with cognitive processes in a cognitive architecture. This new architecture gives a more tractable, mechanistic way to simulate affect-cognition interactions to provide specific, quantitative predictions. It considers affect as a lower-level, functional process that interacts with cognitive processes (e.g., declarative memory) to result in emotional behavior. This formulation makes it more straightforward to connect these affective representations with other related moderating processes that may not specifically be considered as emotional (e.g., thirst or stress). An improved understanding of the architecture that constrains our behavior gives us a better opportunity to comprehend why we behave the way we do and how we can use this knowledge to recognize and construct a more ideal internal and external environment.},
  archive  = {J},
  author   = {Christopher L. Dancy},
  doi      = {10.1109/TAFFC.2019.2906162},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {318-328},
  title    = {A hybrid cognitive architecture with primal affect and physiology},
  volume   = {12},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Applying probabilistic programming to affective computing.
<em>IEEE Transactions on Affective Computing</em>, <em>12</em>(2),
306–317. (<a href="https://doi.org/10.1109/TAFFC.2019.2905211">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Affective Computing is a rapidly growing field spurred by advancements in artificial intelligence, but often, held back by the inability to translate psychological theories of emotion into tractable computational models. To address this, we propose a probabilistic programming approach to affective computing, which models psychological-grounded theories as generative models of emotion, and implements them as stochastic, executable computer programs. We first review probabilistic approaches that integrate reasoning about emotions with reasoning about other latent mental states (e.g., beliefs, desires) in context. Recently-developed probabilistic programming languages offer several key desidarata over previous approaches, such as: (i) flexibility in representing emotions and emotional processes; (ii) modularity and compositionality; (iii) integration with deep learning libraries that facilitate efficient inference and learning from large, naturalistic data; and (iv) ease of adoption. Furthermore, using a probabilistic programming framework allows a standardized platform for theory-building and experimentation: Competing theories (e.g., of appraisal or other emotional processes) can be easily compared via modular substitution of code followed by model comparison. To jumpstart adoption, we illustrate our points with executable code that researchers can easily modify for their own models. We end with a discussion of applications and future directions of the probabilistic programming approach.},
  archive  = {J},
  author   = {Desmond C. Ong and Harold Soh and Jamil Zaki and Noah D. Goodman},
  doi      = {10.1109/TAFFC.2019.2905211},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {306-317},
  title    = {Applying probabilistic programming to affective computing},
  volume   = {12},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An architecture for emotional facial expressions as social
signals. <em>IEEE Transactions on Affective Computing</em>,
<em>12</em>(2), 293–305. (<a
href="https://doi.org/10.1109/TAFFC.2019.2906200">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We focus on affective architecture issues relating to the generation of expressive facial behaviour, critique approaches that treat expressive behaviour as only a mirror of internal state rather than as also a social signal and discuss the advantages of combining the two approaches. Using the FAtiMA architecture, we analyse the requirements for generating expressive behavior as social signals at both reactive and cognitive levels. We discuss how facial expressions can be generated in a dynamic fashion. We propose generic architectural mechanisms to meet these requirements based on an explicit mind-body loop and Theory of Mind (ToM) processing. A illustrative scenario is given.},
  archive  = {J},
  author   = {Ruth Aylett and Christopher Ritter and Mei Yii Lim and Frank Broz and Peter E McKenna and Ingo Keller and Gnanathusharan Rajendran},
  doi      = {10.1109/TAFFC.2019.2906200},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {293-305},
  title    = {An architecture for emotional facial expressions as social signals},
  volume   = {12},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Towards a prediction and data driven computational process
model of emotion. <em>IEEE Transactions on Affective Computing</em>,
<em>12</em>(2), 279–292. (<a
href="https://doi.org/10.1109/TAFFC.2019.2905209">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Starting from the assumption that computational models of emotion (CME) should produce tangible and useful results, I focus on their potential role in developing theoretical predictions and comparative testing of different theories. Concretely, I suggest a specific type of CME for this purpose, the prediction and data driven computational process (PDCP) model of emotion, as based on largely shared assumptions about the emotion process and the underlying components. After providing an overview of the wide variety of emotion theories and their suitability for computational modeling, the current version of the component process model (CPM) of emotion is described, including the specific predictions amenable to modeling. I then review the empirical data confirming many of these predictions and thus providing a solid basis for the development of CMEs. On this basis, I outline the skeleton of a realistic PDCP model that should allow testing competing theories and models. Specifically, I propose an incremental approach to concrete static and dynamic modeling efforts using recently developed advanced experimental procedures and assessment instruments as well as expert systems.},
  archive  = {J},
  author   = {Klaus R. Scherer},
  doi      = {10.1109/TAFFC.2019.2905209},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {279-292},
  title    = {Towards a prediction and data driven computational process model of emotion},
  volume   = {12},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Introduction to the special issue on computational modelling
of emotion. <em>IEEE Transactions on Affective Computing</em>,
<em>12</em>(2), 277–278. (<a
href="https://doi.org/10.1109/TAFFC.2021.3073214">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The papers in this special issue focus on computational modeling of emotion recognition. Emotions play a pervasive role in personal, social, and professional life. As artificially intelligent systems become pervasive in our lives, it is important that these systems are able to understand emotion in humans and simulate the function of emotion to be effective in their interactions with people. Computational models of emotion contribute towards this goal by, on the one hand, serving as a means to test emotion theories and help understand the function of emotion and, on the other, as the end in itself by simulating appropriate emotion and its downstream consequences – such as expressions of emotion – in computational agents. This special issue presents a critical overview of this cross-disciplinary field, with contributions from some of the leading scholars in cognitive psychology and affective computing, focusing both on theory and practice.},
  archive  = {J},
  author   = {Celso de Melo and Dean Petters and Joel Parthemore and David Moffatt and Christian Becker-Asano},
  doi      = {10.1109/TAFFC.2021.3073214},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {4},
  number   = {2},
  pages    = {277-278},
  title    = {Introduction to the special issue on computational modelling of emotion},
  volume   = {12},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Towards analyzing and predicting the experience of live
performances with wearable sensing. <em>IEEE Transactions on Affective
Computing</em>, <em>12</em>(1), 269–276. (<a
href="https://doi.org/10.1109/TAFFC.2018.2875987">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We present an approach to interpret the response of audiences to live performances by processing mobile sensor data. We apply our method on three different datasets obtained from three live performances, where each audience member wore a single tri-axial accelerometer and proximity sensor embedded inside a smart sensor pack. Using these sensor data, we developed a novel approach to predict audience members’ self-reported experience of the performances in terms of enjoyment, immersion, willingness to recommend the event to others, and change in mood. The proposed method uses an unsupervised method to identify informative intervals of the event, using the linkage of the audience members’ bodily movements, and uses data from these intervals only to estimate the audience members’ experience. We also analyze how the relative location of members of the audience can affect their experience and present an automatic way of recovering neighborhood information based on proximity sensors. We further show that the linkage of the audience members’ bodily movements is informative of memorable moments which were later reported by the audience.},
  archive  = {J},
  author   = {Ekin Gedik and Laura Cabrera-Quiros and Claudio Martella and Gwenn Englebienne and Hayley Hung},
  doi      = {10.1109/TAFFC.2018.2875987},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {269-276},
  title    = {Towards analyzing and predicting the experience of live performances with wearable sensing},
  volume   = {12},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Video-based depression level analysis by encoding deep
spatiotemporal features. <em>IEEE Transactions on Affective
Computing</em>, <em>12</em>(1), 262–268. (<a
href="https://doi.org/10.1109/TAFFC.2018.2870884">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {As a serious mood disorder problem, depression causes severe symptoms that affect how people feel, think, and handle daily activities, such as sleeping, eating, or working. In this paper, a novel framework is proposed to estimate the Beck Depression Inventory II (BDI-II) values from video data, which uses a 3D convolutional neural network to automatically learn the spatiotemporal features at two different scales of the face regions. Then, a Recurrent Neural Network (RNN) is used to learn further from the sequence of the spatiotemporal information. This formulation, called RNN-C3D, can model the local and global spatiotemporal information from consecutive face expressions, in order to predict the depression levels. Experiments on the AVEC2013 and AVEC2014 depression datasets show that our proposed approach is promising, when compared to the state-of-the-art visual-based depression analysis methods.},
  archive  = {J},
  author   = {Mohamad Al Jazaery and Guodong Guo},
  doi      = {10.1109/TAFFC.2018.2870884},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {262-268},
  title    = {Video-based depression level analysis by encoding deep spatiotemporal features},
  volume   = {12},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sparse MDMO: Learning a discriminative feature for
micro-expression recognition. <em>IEEE Transactions on Affective
Computing</em>, <em>12</em>(1), 254–261. (<a
href="https://doi.org/10.1109/TAFFC.2018.2854166">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Micro-expressions are the rapid movements of facial muscles that can be used to reveal concealed emotions. Recognizing them from video clips has a wide range of applications and receives increasing attention recently. Among existing methods, the main directional mean optical-flow (MDMO) feature achieves state-of-the-art performance for recognizing spontaneous micro-expressions. For a video clip, the MDMO feature is computed by averaging a set of atomic features frame-by-frame. Despite its simplicity, the average operation in MDMO can easily lose the underlying manifold structure inherent in the feature space. In this paper we propose a sparse MDMO feature that learns an effective dictionary from a micro-expression video dataset. In particular, a new distance metric is proposed based on the sparsity of sample points in the MDMO feature space, which can efficiently reveal the underlying manifold structure. The proposed sparse MDMO feature is obtained by incorporating this new metric into the classic graph regularized sparse coding (GraphSC) scheme. We evaluate sparse MDMO and four representative features (LBP-TOP, STCLQP, MDMO and FDM) on three spontaneous micro-expression datasets (SMIC, CASME and CASME II). The results show that sparse MDMO outperforms these representative features.},
  archive  = {J},
  author   = {Yong-Jin Liu and Bing-Jun Li and Yu-Kun Lai},
  doi      = {10.1109/TAFFC.2018.2854166},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {254-261},
  title    = {Sparse MDMO: Learning a discriminative feature for micro-expression recognition},
  volume   = {12},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Integrating deep and shallow models for multi-modal
depression analysis—hybrid architectures. <em>IEEE Transactions on
Affective Computing</em>, <em>12</em>(1), 239–253. (<a
href="https://doi.org/10.1109/TAFFC.2018.2870398">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {At present, although great progress has been made in automatic depression assessment, most of the recent works only concern the audio and video paralinguistic information, rather than the linguistic information from the spoken content. In this work, we argue that beside developing good audio and video features, to build reliable depression detection systems, text-based content features are also of importance to analyse depression-related textual indicators. Furthermore, to improve the performance of automatic depression assessment systems, powerful models, capable of modelling the characteristics of depression embedded in the audio, visual and text descriptors, are also required. This paper proposes new text and video features and hybridizes deep and shallow models for depression estimation and classification from audio, video and text descriptors. The proposed hybrid framework consists of three main parts: 1) A Deep Convolutional Neural Network (DCNN) and Deep Neural Network (DNN) based audio-visual multi-modal depression recognition model for estimating the Patient Health Questionnaire depression scale (PHQ-8); 2) A Paragraph Vector (PV) and Support Vector Machine (SVM) based model for inferring the physical and mental conditions of the individual from the transcripts of the interview; 3) A Random Forest (RF) model for depression classification from the estimated PHQ-8 score and the inferred conditions of the individual. In the PV-SVM model, PV embedding is used to obtain fixed-length feature vectors from transcripts of the answers to the questions associated with psychoanalytic aspects of depression, which are subsequently fed into the SVM classifiers for detecting the presence/absence of the considered psychoanalytic symptoms. To our best knowledge, this approach is the first attempt to apply PV for depression analysis. Besides, we propose a new visual descriptor - Histogram of Displacement Range (HDR) to characterize the displacement and velocity of the facial landmarks in the video segment. Experiments have been carried out on the Audio Visual Emotion Challenge (AVEC2016) depression dataset, they demonstrate that: 1) The proposed hybrid framework effectively improves the accuracies of both depression estimation and depression classification, with an average F1 measure up to 0.746, which is higher than the best result (0.724) of the depression sub-challenge of AVEC2016. 2) HDR obtains better depression recognition performance than Bag-of-Words (BoW) and Motion History Histogram (MHH) features.},
  archive  = {J},
  author   = {Le Yang and Dongmei Jiang and Hichem Sahli},
  doi      = {10.1109/TAFFC.2018.2870398},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {239-253},
  title    = {Integrating deep and shallow models for multi-modal depression Analysis—Hybrid architectures},
  volume   = {12},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bio-inspired deep attribute learning towards facial
aesthetic prediction. <em>IEEE Transactions on Affective Computing</em>,
<em>12</em>(1), 227–238. (<a
href="https://doi.org/10.1109/TAFFC.2018.2868651">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Computational prediction of facial aesthetics has attracted ever-increasing research focus, which has wide range of prospects in multimedia applications. The key challenge lies in extracting discriminative and perception-aware features to characterize the facial beautifulness. To this end, the existing schemes simply adopt a direct feature mapping, which relies on handcraft-designed low-level features that cannot reflect human-level aesthetic perception. In this paper, we present a systematic framework towards designing biology-inspired, discriminative representation for facial aesthetic prediction. First, we design a group of biological experiments that adopt eye tracker to identify spatial regions of interest during the facial aesthetic judgments of subjects, which forms a Bio-inspired Facial Aesthetic Ontology (Bio-FAO) and is made public available. Second, we adopt the cutting-edge convolutional neural network to train a set of Bio-inspired Attribute features, termed Bio-AttriBank, which forms a mid-level interpretable representation corresponding to the aforementioned Bio-FAO. For a given image, the facial aesthetic prediction is then formulated as a classification problem over the Bio-AttriBank descriptor responses, which well bridges the affective gap, and provides explainable evidences on why/how a face is beautiful or not. We have carried out extensive experiments on both JAFFE and FaceWarehouse datasets, with comparisons to a set of state-of-the-art and alternative approaches. Superior performance gains in the experiments have demonstrated the merits of the proposed scheme.},
  archive  = {J},
  author   = {Mingliang Xu and Fuhai Chen and Lu Li and Chen Shen and Pei Lv and Bing Zhou and Rongrong Ji},
  doi      = {10.1109/TAFFC.2018.2868651},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {227-238},
  title    = {Bio-inspired deep attribute learning towards facial aesthetic prediction},
  volume   = {12},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Computer vision analysis for quantification of autism risk
behaviors. <em>IEEE Transactions on Affective Computing</em>,
<em>12</em>(1), 215–226. (<a
href="https://doi.org/10.1109/TAFFC.2018.2868196">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Observational behavior analysis plays a key role for the discovery and evaluation of risk markers for many neurodevelopmental disorders. Research on autism spectrum disorder (ASD) suggests that behavioral risk markers can be observed at 12 months of age or earlier, with diagnosis possible at 18 months. To date, these studies and evaluations involving observational analysis tend to rely heavily on clinical practitioners and specialists who have undergone intensive training to be able to reliably administer carefully designed behavioral-eliciting tasks, code the resulting behaviors, and interpret such behaviors. These methods are therefore extremely expensive, time-intensive, and are not easily scalable for large population or longitudinal observational analysis. We developed a self-contained, closed-loop, mobile application with movie stimuli designed to engage the child&#39;s attention and elicit specific behavioral and social responses, which are recorded with a mobile device camera and then analyzed via computer vision algorithms. Here, in addition to presenting this paradigm, we validate the system to measure engagement, name-call responses, and emotional responses of toddlers with and without ASD who were presented with the application. Additionally, we show examples of how the proposed framework can further risk marker research with fine-grained quantification of behaviors. The results suggest these objective and automatic methods can be considered to aid behavioral analysis, and can be suited for objective automatic analysis for future studies.},
  archive  = {J},
  author   = {Jordan Hashemi and Geraldine Dawson and Kimberly L. H. Carpenter and Kathleen Campbell and Qiang Qiu and Steven Espinosa and Samuel Marsan and Jeffrey P. Baker and Helen L. Egger and Guillermo Sapiro},
  doi      = {10.1109/TAFFC.2018.2868196},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {215-226},
  title    = {Computer vision analysis for quantification of autism risk behaviors},
  volume   = {12},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multimodal classification of stressful environments in
visually impaired mobility using EEG and peripheral biosignals. <em>IEEE
Transactions on Affective Computing</em>, <em>12</em>(1), 203–214. (<a
href="https://doi.org/10.1109/TAFFC.2018.2866865">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In this study, we aim to better understand the cognitive-emotional experience of visually impaired people when navigating in unfamiliar urban environments, both outdoor and indoor. We propose a multimodal framework based on random forest classifiers, which predict the actual environment among predefined generic classes of urban settings, inferring on real-time, non-invasive, ambulatory monitoring of brain and peripheral biosignals. Model performance reached 93 for the outdoor and 87 percent for the indoor environments (expressed in weighted AUROC), demonstrating the potential of the approach. Estimating the density distributions of the most predictive biomarkers, we present a series of geographic and temporal visualizations depicting the environmental contexts in which the most intense affective and cognitive reactions take place. A linear mixed model analysis revealed significant differences between categories of vision impairment, but not between normal and impaired vision. Despite the limited size of our cohort, these findings pave the way to emotionally intelligent mobility-enhancing systems, capable of implicit adaptation not only to changing environments but also to shifts in the affective state of the user in relation to different environmental and situational factors.},
  archive  = {J},
  author   = {Charalampos Saitis and Kyriaki Kalimeri},
  doi      = {10.1109/TAFFC.2018.2866865},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {203-214},
  title    = {Multimodal classification of stressful environments in visually impaired mobility using EEG and peripheral biosignals},
  volume   = {12},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Induction and profiling of strong multi-componential
emotions in virtual reality. <em>IEEE Transactions on Affective
Computing</em>, <em>12</em>(1), 189–202. (<a
href="https://doi.org/10.1109/TAFFC.2018.2864730">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Psychological theories of emotion have often defined an emotion as simultaneous changes in several mental and bodily components. In addition, appraisal theories assume that an appraisal component elicits changes in the other emotion components (e.g., motivational, behavioural, experiential). Neither the componential definition of emotion nor appraisal theory have been systematically translated to paradigms for emotion induction, many of which rely on passive emotion induction without a clear theoretical framework. As a result, the observed emotions are often weak. This study explored the potential of virtual reality (VR) to evoke strong emotions in ecologically valid scenarios that fully engaged the mental and bodily components of the participant. Participants played several VR games and reported on their emotions. Multivariate analyses using hierarchical clustering and multilevel linear modelling showed that participants experienced intense, multi-componential emotions in VR. We identified joy and fear clusters of responses, each involving changes in appraisal, motivation, physiology, feeling, and regulation. Appraisal variables were found to be the most predictive for fear and joy intensities, compared to other emotion components, and were found to explain individual differences in VR scenarios, as predicted by appraisal theory. The results advocate for upgraded methodologies for the induction and analysis of emotion processes.},
  archive  = {J},
  author   = {Ben Meuleman and David Rudrauf},
  doi      = {10.1109/TAFFC.2018.2864730},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {189-202},
  title    = {Induction and profiling of strong multi-componential emotions in virtual reality},
  volume   = {12},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Feature pooling of modulation spectrum features for improved
speech emotion recognition in the wild. <em>IEEE Transactions on
Affective Computing</em>, <em>12</em>(1), 177–188. (<a
href="https://doi.org/10.1109/TAFFC.2018.2858255">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Interest in affective computing is burgeoning, in great part due to its role in emerging affective human-computer interfaces (HCI). To date, the majority of existing research on automated emotion analysis has relied on data collected in controlled environments. With the rise of HCI applications on mobile devices, however, so-called “in-the-wild” settings have posed a serious threat for emotion recognition systems, particularly those based on voice. In this case, environmental factors such as ambient noise and reverberation severely hamper system performance. In this paper, we quantify the detrimental effects that the environment has on emotion recognition and explore the benefits achievable with speech enhancement. Moreover, we propose a modulation spectral feature pooling scheme that is shown to outperform a state-of-the-art benchmark system for environment-robust prediction of spontaneous arousal and valence emotional primitives. Experiments on an environment-corrupted version of the RECOLA dataset of spontaneous interactions show the proposed feature pooling scheme, combined with speech enhancement, outperforming the benchmark across different noise-only, reverberation-only and noise-plus-reverberation conditions. Additional tests with the SEWA database show the benefits of the proposed method for in-the-wild applications.},
  archive  = {J},
  author   = {Anderson R. Avila and Zahid Akhtar and João F. Santos and Douglas O&#39;Shaughnessy and Tiago H. Falk},
  doi      = {10.1109/TAFFC.2018.2858255},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {177-188},
  title    = {Feature pooling of modulation spectrum features for improved speech emotion recognition in the wild},
  volume   = {12},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On social involvement in mingling scenarios: Detecting
associates of f-formations in still images. <em>IEEE Transactions on
Affective Computing</em>, <em>12</em>(1), 165–176. (<a
href="https://doi.org/10.1109/TAFFC.2018.2855750">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In this paper, we carry out an extensive study of social involvement in free standing conversing groups (the so-called F-formations) from static images. By introducing a novel feature representation, we show that the standard features which have been used to represent full membership in an F-formation cannot be applied to the detection of so-called associates of F-formations due to their sparser nature. We also enrich state-of-the-art F-formation modelling by learning a frustum of attention that accounts for the spatial context. That is, F-formation configurations vary with respect to the arrangement of furniture and the non-uniform crowdedness in the space during mingling scenarios. Moroever, the majority of prior works have considered the labelling of conversing groups as an objective task, requiring only a single annotator. However, we show that by embracing the subjectivity of social involvement, we not only generate a richer model of the social interactions in a scene but can use the detected associates to improve initial estimates of the full members of an F-formation. We carry out extensive experimental validation of our proposed approach by collecting a novel set of multi-annotator labels of involvement on two publicly available datasets; The Idiap Poster Data and SALSA data set. Moreover, we show that parameters learned from the Idiap Poster Data can be transferred to the SALSA data, showing the power of our proposed representation in generalising over new unseen data from a different environment.},
  archive  = {J},
  author   = {Lu Zhang and Hayley Hung},
  doi      = {10.1109/TAFFC.2018.2855750},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {165-176},
  title    = {On social involvement in mingling scenarios: Detecting associates of F-formations in still images},
  volume   = {12},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Empirical evidence relating EEG signal duration to emotion
classification performance. <em>IEEE Transactions on Affective
Computing</em>, <em>12</em>(1), 154–164. (<a
href="https://doi.org/10.1109/TAFFC.2018.2854168">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In emotion recognition using EEG, it is not generally agreed upon how much time an EEG signal sequence must have in order to maximize precision and recall rates. To the best of our knowledge, there is not a systematic evaluation of effects on classifier performance related to EEG signal durations. The human factors related to attention decreasing and tiredness increasing have imposed difficulties to create EEG datasets containing a rich variation of signal samples. This paper proposes an experimental evaluation of three different EEG datasets (DEAP, MAHNOB, and STEED) each one mainly characterized by short, intermediate and long signal (or stimulus) durations. Statistical evaluation pointed out that for an EEG dataset to be well-suited for emotion recognition it should have two main characteristics: emotion stimulus data should be publicly available and evaluated by world-wide volunteers, and media stimulus should have duration long enough to affect the subjects. Our statistical analysis revealed that, at least for the considered datasets, signals with duration longer than 60 seconds allow better classification results. This work did not analyse the impact to humans of longer stimulus media.},
  archive  = {J},
  author   = {Eanes Torres Pereira and Herman Martins Gomes and Luciana Ribeiro Veloso and Moisés Roberto A. Mota},
  doi      = {10.1109/TAFFC.2018.2854168},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {154-164},
  title    = {Empirical evidence relating EEG signal duration to emotion classification performance},
  volume   = {12},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Partial reinforcement in game biofeedback for relaxation
training. <em>IEEE Transactions on Affective Computing</em>,
<em>12</em>(1), 141–153. (<a
href="https://doi.org/10.1109/TAFFC.2018.2842727">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This paper investigates the effect of reinforcement schedules on biofeedback games for stress self-regulation. In particular, it examines whether partial reinforcement can improve resistance to extinction of relaxation behaviors, i.e., once biofeedback is removed. Namely, we compare two types of reinforcement schedules (partial and continuous) in a mobile biofeedback game that encourages players to slow their breathing during gameplay. The game uses a negative-reinforcement instrumental conditioning paradigm, removing an aversive stimulus (random actions in the game) if players slows down their breathing. We conducted an experimental trial with 24 participants to compare the two reinforcement schedules against a control condition. Our results indicate that partial reinforcement improves resistance to extinction, as measured by breathing rate and skin conductance post-treatment. In addition, based on linear regression and correlation analysis we found that participants in the partial reinforcement learned to slow their breathing at the same pace as those under continuous reinforcement. The article discusses the implications of these results and directions for future work.},
  archive  = {J},
  author   = {Avinash Parnandi and Ricardo Gutierrez-Osuna},
  doi      = {10.1109/TAFFC.2018.2842727},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {141-153},
  title    = {Partial reinforcement in game biofeedback for relaxation training},
  volume   = {12},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A novel classification strategy to distinguish five levels
of pain using the EEG signal features. <em>IEEE Transactions on
Affective Computing</em>, <em>12</em>(1), 131–140. (<a
href="https://doi.org/10.1109/TAFFC.2018.2851236">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Research studies have tried to extract pain-related features from electroencephalogram(EEG) signals for quantitative measuring of pain. In this study, we go one step further to measure three/five levels of pain by proposing efficient EEG processing steps in conjunction with a new classification strategy. 24 healthy subjects voluntarily performed the cold pressor test while their EEGs were recorded. First, the EEGs were decomposed by independent component analysis and the artifact sources were removed. Among the remained sources, pain-related sources, were chosen according to an adopted information criterion. Next, the EEGs were reconstructed by projecting back the selected sources. Then, grand average brain maps of train subjects were estimated for each pain level over the Alpha(8-12 Hz) and Delta(0.5-4 Hz) bands. By tracing the brain maps&#39; changes over different pain levels, the structure of the proposed decision tree was formed. To enrich the feature set, we also extracted other EEG features. For each decision node, a specific subset of features was selected by sequential forward selection method. Considering k-nearest neighbor(KNN) as the decision marker,the classification accuracies for the three and five pain levels was determined 80 ± 5 and 60 ± 5 percent, respectively while by choosing support vector machine(SVM), the results improved up to 83 ± 5 and 62 ± 6 percent,respectively.},
  archive  = {J},
  author   = {T. Nezam and R. Boostani and V. Abootalebi and K. Rastegar},
  doi      = {10.1109/TAFFC.2018.2851236},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {131-140},
  title    = {A novel classification strategy to distinguish five levels of pain using the EEG signal features},
  volume   = {12},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The MatchNMingle dataset: A novel multi-sensor resource for
the analysis of social interactions and group dynamics in-the-wild
during free-standing conversations and speed dates. <em>IEEE
Transactions on Affective Computing</em>, <em>12</em>(1), 113–130. (<a
href="https://doi.org/10.1109/TAFFC.2018.2848914">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We present MatchNMingle , a novel multimodal/multisensor dataset for the analysis of free-standing conversational groups and speed-dates in-the-wild. MatchNMingle leverages the use of wearable devices and overhead cameras to record social interactions of 92 people during real-life speed-dates, followed by a cocktail party. To our knowledge, MatchNMingle has the largest number of participants, longest recording time and largest set of manual annotations for social actions available in this context in a real-life scenario. It consists of 2 hours of data from wearable acceleration, binary proximity, video, audio, personality surveys, frontal pictures and speed-date responses. Participants’ positions and group formations were manually annotated; as were social actions (eg. speaking, hand gesture) for 30 minutes at 20 FPS making it the first dataset to incorporate the annotation of such cues in this context. We present an empirical analysis of the performance of crowdsourcing workers against trained annotators in simple and complex annotation tasks, founding that although efficient for simple tasks, using crowdsourcing workers for more complex tasks like social action annotation led to additional overhead and poor inter-annotator agreement compared to trained annotators (differences up to 0.4 in Fleiss’ Kappa coefficients). We also provide example experiments of how MatchNMingle can be used.},
  archive  = {J},
  author   = {Laura Cabrera-Quiros and Andrew Demetriou and Ekin Gedik and Leander van der Meij and Hayley Hung},
  doi      = {10.1109/TAFFC.2018.2848914},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {113-130},
  title    = {The MatchNMingle dataset: A novel multi-sensor resource for the analysis of social interactions and group dynamics in-the-wild during free-standing conversations and speed dates},
  volume   = {12},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Brain’s night symphony (BraiNSy): A methodology for EEG
sonification. <em>IEEE Transactions on Affective Computing</em>,
<em>12</em>(1), 103–112. (<a
href="https://doi.org/10.1109/TAFFC.2018.2850008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This paper describes a method for converting sleep Electroencephalogram (EEG) signals into music. For that purpose, a new segmentation procedure is used for extracting relevant information from the sleep EEG that is then translated into sequences of notes, chords, arpeggios and pauses, with a varying tempo that is defined by sleep stages. The final outcome is a direct time-domain conversion of the brain activity during sleep into sound. Since typical sleep EEGs vary with age and sleep disorders, different groups of subjects were used in the experiments: babies, sane adults and patients with air-flow limitation.},
  archive  = {J},
  author   = {Carlos M. Fernandes and Daria Migotina and Agostinho C. Rosa},
  doi      = {10.1109/TAFFC.2018.2850008},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {103-112},
  title    = {Brain&#39;s night symphony (BraiNSy): A methodology for EEG sonification},
  volume   = {12},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Inter-brain EEG feature extraction and analysis for
continuous implicit emotion tagging during video watching. <em>IEEE
Transactions on Affective Computing</em>, <em>12</em>(1), 92–102. (<a
href="https://doi.org/10.1109/TAFFC.2018.2849758">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {How to efficiently tag the emotional experience of multimedia contents is an important and challenging problem in the field of affective computing. This paper presents an EEG-based real-time emotion tagging approach, by extracting inter-brain features from a group of participants when they watch the same emotional video clips. First, the continuous subjective reports on both the arousal and valence dimensions of emotion were obtained by employing a three-round behavioral rating paradigm. Second, the inter-brain features were systematically explored in both spectral and temporal domain. Finally, regression analyses were performed to evaluate the effectiveness of inter-brain amplitude and phase features. The inter-brain amplitude feature showed significantly better prediction performance than the inter-brain phase feature, as well as another two conventional features (spectral power and inter-subject correlation). By combining the four types of features, regression values (R2) were obtained for the prediction of arousal (0.61 + 0.01) and valence (0.70 + 0.01), corresponding to prediction errors of 1.01 + 0.02 and 0.78 + 0.02 (unit on 9-point scales), respectively. The contributions of different electrodes and frequency bands were also analyzed. Our results show promising potentials of inter-brain EEG features in real-time emotion tagging applications.},
  archive  = {J},
  author   = {Yue Ding and Xin Hu and Zhenyi Xia and Yong-Jin Liu and Dan Zhang},
  doi      = {10.1109/TAFFC.2018.2849758},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {92-102},
  title    = {Inter-brain EEG feature extraction and analysis for continuous implicit emotion tagging during video watching},
  volume   = {12},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Leveraging affective hashtags for ranking music
recommendations. <em>IEEE Transactions on Affective Computing</em>,
<em>12</em>(1), 78–91. (<a
href="https://doi.org/10.1109/TAFFC.2018.2846596">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Mood and emotion play an important role when it comes to choosing musical tracks to listen to. In the field of music information retrieval and recommendation, emotion is considered contextual information that is hard to capture, albeit highly influential. In this study, we analyze the connection between users` emotional states and their musical choices. Particularly, we perform a large-scale study based on two data sets containing 560,000 and 90,000 #nowplaying tweets, respectively. We extract affective contextual information from hashtags contained in these tweets by applying an unsupervised sentiment dictionary approach. Subsequently, we utilize a state-of-the-art network embedding method to learn latent feature representations of users, tracks and hashtags. Based on both the affective information and the latent features, a set of eight ranking methods is proposed. We find that relying on a ranking approach that incorporates the latent representations of users and tracks allows for capturing a user&#39;s general musical preferences well (regardless of used hashtags or affective information). However, for capturing context-specific preferences (a more complex and personal ranking task), we find that ranking strategies that rely on affective information and that leverage hashtags as context information outperform the other ranking strategies.},
  archive  = {J},
  author   = {Eva Zangerle and Chih-Ming Chen and Ming-Feng Tsai and Yi-Hsuan Yang},
  doi      = {10.1109/TAFFC.2018.2846596},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {78-91},
  title    = {Leveraging affective hashtags for ranking music recommendations},
  volume   = {12},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Induction of emotional states in educational video games
through a fuzzy control system. <em>IEEE Transactions on Affective
Computing</em>, <em>12</em>(1), 66–77. (<a
href="https://doi.org/10.1109/TAFFC.2018.2840988">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {It has been shown that the emotional state of students plays an important role towards learning; for instance, engaged concentration is positively correlated with learning. This paper proposes Inductive Control (IC) for educational games. Unlike conventional approaches that only modify the game level, the proposed technique also induces emotions in the player for supporting the learning process. This paper explores a fuzzy system that analyzes the players&#39; performance and their emotional state for controlling the level and aesthetic content of an educational video game. The emotional state of the player is recognized through voice analysis. A total of 40 subjects played a video game designed to practice basic math skills; for each trial, a student plays twice in a row the same game but each time the game was controlled by one of the two approaches-Dynamic Difficulty Adjustment (DDA) and IC, the playing order was assigned randomly. Results show that when the proposed approach is used the participants changed faster from unpleasant-low to pleasant or high emotions. These experiments demonstrate that the inductive control technique improves the learning effectiveness through detection and stimulation of positive emotions.},
  archive  = {J},
  author   = {Carlos Lara-Álvarez and Hugo Mitre-Hernandez and Juan J. Flores and Humberto Pérez-Espinosa},
  doi      = {10.1109/TAFFC.2018.2840988},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {66-77},
  title    = {Induction of emotional states in educational video games through a fuzzy control system},
  volume   = {12},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Faded smiles? A largescale observational study of smiling
from adolescence to old age. <em>IEEE Transactions on Affective
Computing</em>, <em>12</em>(1), 53–65. (<a
href="https://doi.org/10.1109/TAFFC.2019.2922341">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {A relatively large body of work exists examining sex differences in expressiveness; however, there remains little research of differences in expressiveness associated with aging. Observational studies of facial expressivity across ages are limited in part due to the poor scalability of traditional research methods. We collected over 17,000 videos of natural facial behavior using the Internet and performed a large observational study of smiling responses of people ages 18 to 70 years. Using automated facial coding we quantified the presence of smiles as people watched a set of controlled mundane online content. The likelihood of smiles and the duration of smiles increased with age. We attribute this to greater expression of positive emotion in older people. Women smiled more than men over all and gender differences increased significantly with age. We question whether results may be influenced by the effect of age on the accuracy of the automated smile detection; however, validation on a large set of human coded videos shows that the observed effects were not due to smile detection performance.},
  archive  = {J},
  author   = {Daniel McDuff and Stephanie Glass},
  doi      = {10.1109/TAFFC.2019.2922341},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {53-65},
  title    = {Faded smiles? a largescale observational study of smiling from adolescence to old age},
  volume   = {12},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Recognizing induced emotions of movie audiences from
multimodal information. <em>IEEE Transactions on Affective
Computing</em>, <em>12</em>(1), 36–52. (<a
href="https://doi.org/10.1109/TAFFC.2019.2902091">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Recognizing emotional reactions of movie audiences to affective movie content is a challenging task in affective computing. Previous research on induced emotion recognition has mainly focused on using audio-visual movie content. Nevertheless, the relationship between the perceptions of the affective movie content (perceived emotions) and the emotions evoked in the audiences (induced emotions) is unexplored. In this work, we studied the relationship between perceived and induced emotions of movie audiences. Moreover, we investigated multimodal modelling approaches to predict movie induced emotions from movie content based features, as well as physiological and behavioral reactions of movie audiences. To carry out analysis of induced and perceived emotions, we first extended an existing database for movie affect analysis by annotating perceived emotions in a crowd-sourced manner. We find that perceived and induced emotions are not always consistent with each other. In addition, we show that perceived emotions, movie dialogues, and aesthetic highlights are discriminative for movie induced emotion recognition besides spectators’ physiological and behavioral reactions. Also, our experiments revealed that induced emotion recognition could benefit from including temporal information and performing multimodal fusion. Moreover, our work deeply investigated the gap between affective content analysis and induced emotion recognition by gaining insight into the relationships between aesthetic highlights, induced emotions, and perceived emotions.},
  archive  = {J},
  author   = {Michal Muszynski and Leimin Tian and Catherine Lai and Johanna D. Moore and Theodoros Kostoulas and Patrizia Lombardo and Thierry Pun and Guillaume Chanel},
  doi      = {10.1109/TAFFC.2019.2902091},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {36-52},
  title    = {Recognizing induced emotions of movie audiences from multimodal information},
  volume   = {12},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The ordinal nature of emotions: An emerging approach.
<em>IEEE Transactions on Affective Computing</em>, <em>12</em>(1),
16–35. (<a href="https://doi.org/10.1109/TAFFC.2018.2879512">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Computational representation of everyday emotional states is a challenging task and, arguably, one of the most fundamental for affective computing. Standard practice in emotion annotation is to ask people to assign a value of intensity or a class value to each emotional behavior they observe. Psychological theories and evidence from multiple disciplines including neuroscience, economics and artificial intelligence, however, suggest that the task of assigning reference-based values to subjective notions is better aligned with the underlying representations. This paper draws together the theoretical reasons to favor ordinal labels for representing and annotating emotion, reviewing the literature across several disciplines. We go on to discuss good and bad practices of treating ordinal and other forms of annotation data and make the case for preference learning methods as the appropriate approach for treating ordinal labels. We finally discuss the advantages of ordinal annotation with respect to both reliability and validity through a number of case studies in affective computing, and address common objections to the use of ordinal data. More broadly, the thesis that emotions are by nature ordinal is supported by both theoretical arguments and evidence, and opens new horizons for the way emotions are viewed, represented and analyzed computationally.},
  archive  = {J},
  author   = {Georgios N. Yannakakis and Roddy Cowie and Carlos Busso},
  doi      = {10.1109/TAFFC.2018.2879512},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {16-35},
  title    = {The ordinal nature of emotions: An emerging approach},
  volume   = {12},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Decoding the perception of sincerity in dialogues. <em>IEEE
Transactions on Affective Computing</em>, <em>12</em>(1), 2–15. (<a
href="https://doi.org/10.1109/TAFFC.2018.2854179">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Identifying deception in online conversations is an important problem. A core component in interpreting deception is understanding how people perceive sincerity. Given that the larger proportion of human communication is non-verbal, decoding perceived sincerity is difficult in media where the only signal is written text. We introduce psycholinguistic features of perceived sincerity for two domains, dating and online games. Our results further the understanding of how language, deception, and perception connect.},
  archive  = {J},
  author   = {Codruta Girlea and Roxana Girju},
  doi      = {10.1109/TAFFC.2018.2854179},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {2-15},
  title    = {Decoding the perception of sincerity in dialogues},
  volume   = {12},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Editorial: Transactions on affective computing – affective
computing in the times of pandemics. <em>IEEE Transactions on Affective
Computing</em>, <em>12</em>(1), 1. (<a
href="https://doi.org/10.1109/TAFFC.2021.3059491">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Presents the editorial for thiis issue of the publication.},
  archive  = {J},
  author   = {Elisabeth André},
  doi      = {10.1109/TAFFC.2021.3059491},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {1},
  number   = {1},
  pages    = {1},
  title    = {Editorial: Transactions on affective computing – affective computing in the times of pandemics},
  volume   = {12},
  year     = {2021},
}
</textarea>
</details></li>
</ul>

</body>
</html>
