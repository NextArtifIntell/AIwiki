<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TPAMI_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tpami---341">TPAMI - 341</h2>
<ul>
<li><details>
<summary>
(2021). Unique geometry and texture from corresponding image
patches. <em>TPAMI</em>, <em>43</em>(12), 4519–4522. (<a
href="https://doi.org/10.1109/TPAMI.2021.3081360">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a sufficient condition for recovering unique texture and viewpoints from unknown orthographic projections of a flat texture process. We show that four observations are sufficient in general, and we characterize the ambiguous cases. The results are applicable to shape from texture and texture-based structure from motion.},
  archive      = {J_TPAMI},
  author       = {Dor Verbin and Steven J. Gortler and Todd Zickler},
  doi          = {10.1109/TPAMI.2021.3081360},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {4519-4522},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Unique geometry and texture from corresponding image patches},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Topology-aware graph pooling networks. <em>TPAMI</em>,
<em>43</em>(12), 4512–4518. (<a
href="https://doi.org/10.1109/TPAMI.2021.3062794">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pooling operations have shown to be effective on computer vision and natural language processing tasks. One challenge of performing pooling operations on graph data is the lack of locality that is not well-defined on graphs. Previous studies used global ranking methods to sample some of the important nodes, but most of them are not able to incorporate graph topology. In this work, we propose the topology-aware pooling (TAP) layer that explicitly considers graph topology. Our TAP layer is a two-stage voting process that selects more important nodes in a graph. It first performs local voting to generate scores for each node by attending each node to its neighboring nodes. The scores are generated locally such that topology information is explicitly considered. In addition, graph topology is incorporated in global voting to compute the importance score of each node globally in the entire graph. Altogether, the final ranking score for each node is computed by combining its local and global voting scores. To encourage better graph connectivity in the sampled graph, we propose to add a graph connectivity term to the computation of ranking scores. Results on graph classification tasks demonstrate that our methods achieve consistently better performance than previous methods.},
  archive      = {J_TPAMI},
  author       = {Hongyang Gao and Yi Liu and Shuiwang Ji},
  doi          = {10.1109/TPAMI.2021.3062794},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {4512-4518},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Topology-aware graph pooling networks},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning rates for stochastic gradient descent with
nonconvex objectives. <em>TPAMI</em>, <em>43</em>(12), 4505–4511. (<a
href="https://doi.org/10.1109/TPAMI.2021.3068154">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stochastic gradient descent (SGD) has become the method of choice for training highly complex and nonconvex models since it can not only recover good solutions to minimize training errors but also generalize well. Computational and statistical properties are separately studied to understand the behavior of SGD in the literature. However, there is a lacking study to jointly consider the computational and statistical properties in a nonconvex learning setting. In this paper, we develop novel learning rates of SGD for nonconvex learning by presenting high-probability bounds for both computational and statistical errors. We show that the complexity of SGD iterates grows in a controllable manner with respect to the iteration number, which sheds insights on how an implicit regularization can be achieved by tuning the number of passes to balance the computational and statistical errors. As a byproduct, we also slightly refine the existing studies on the uniform convergence of gradients by showing its connection to Rademacher chaos complexities.},
  archive      = {J_TPAMI},
  author       = {Yunwen Lei and Ke Tang},
  doi          = {10.1109/TPAMI.2021.3068154},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {4505-4511},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Learning rates for stochastic gradient descent with nonconvex objectives},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Video snapshot: Single image motion expansion via invertible
motion embedding. <em>TPAMI</em>, <em>43</em>(12), 4491–4504. (<a
href="https://doi.org/10.1109/TPAMI.2020.3001644">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unlike images, finding the desired video content in a large pool of videos is not easy due to the time cost of loading and watching. Most video streaming and sharing services provide the video preview function for a better browsing experience. In this paper, we aim to generate a video preview from a single image. To this end, we propose two cascaded networks, the motion embedding network and the motion expansion network. The motion embedding network aims to embed the spatio-temporal information into an embedded image, called video snapshot . On the other end, the motion expansion network is proposed to invert the video back from the input video snapshot. To hold the invertibility of motion embedding and expansion during training, we design four tailor-made losses and a motion attention module to make the network focus on the temporal information. In order to enhance the viewing experience, our expansion network involves an interpolation module to produce a longer video preview with a smooth transition. Extensive experiments demonstrate that our method can successfully embed the spatio-temporal information of a video into one “ live ” image, which can be converted back to a video preview. Quantitative and qualitative evaluations are conducted on a large number of videos to prove the effectiveness of our proposed method. In particular, statistics of PSNR and SSIM on a large number of videos show the proposed method is general, and it can generate a high-quality video from a single image.},
  archive      = {J_TPAMI},
  author       = {Qianshu Zhu and Chu Han and Guoqiang Han and Tien-Tsin Wong and Shengfeng He},
  doi          = {10.1109/TPAMI.2020.3001644},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {4491-4504},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Video snapshot: Single image motion expansion via invertible motion embedding},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Progressive cross-stream cooperation in spatial and temporal
domain for action localization. <em>TPAMI</em>, <em>43</em>(12),
4477–4490. (<a
href="https://doi.org/10.1109/TPAMI.2020.2997860">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatio-temporal action localization consists of three levels of tasks: spatial localization, action classification, and temporal localization. In this work, we propose a new progressive cross-stream cooperation (PCSC) framework that improves all three tasks above. The basic idea is to utilize both spatial region ( resp. , temporal segment proposals) and features from one stream (i.e., the Flow/RGB stream) to help another stream (i.e., the RGB/Flow stream) to iteratively generate better bounding boxes in the spatial domain ( resp. , temporal segments in the temporal domain). In this way, not only the actions could be more accurately localized both spatially and temporally, but also the action classes could be predicted more precisely. Specifically, we first combine the latest region proposals (for spatial detection) or segment proposals (for temporal localization) from both streams to form a larger set of labelled training samples to help learn better action detection or segment detection models. Second, to learn better representations, we also propose a new message passing approach to pass information from one stream to another stream, which also leads to better action detection and segment detection models. By first using our newly proposed PCSC framework for spatial localization at the frame-level and then applying our temporal PCSC framework for temporal localization at the tube-level, the action localization results are progressively improved at both the frame level and the video level. Comprehensive experiments on two benchmark datasets UCF-101-24 and J-HMDB demonstrate the effectiveness of our newly proposed approaches for spatio-temporal action localization in realistic scenarios.},
  archive      = {J_TPAMI},
  author       = {Rui Su and Dong Xu and Luping Zhou and Wanli Ouyang},
  doi          = {10.1109/TPAMI.2020.2997860},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {4477-4490},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Progressive cross-stream cooperation in spatial and temporal domain for action localization},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On connections between regularizations for improving DNN
robustness. <em>TPAMI</em>, <em>43</em>(12), 4469–4476. (<a
href="https://doi.org/10.1109/TPAMI.2020.3006917">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper analyzes regularization terms proposed recently for improving the adversarial robustness of deep neural networks (DNNs), from a theoretical point of view. Specifically, we study possible connections between several effective methods, including input-gradient regularization, Jacobian regularization, curvature regularization, and a cross-Lipschitz functional. We investigate them on DNNs with general rectified linear activations, which constitute one of the most prevalent families of models for image classification and a host of other machine learning applications. We shed light on essential ingredients of these regularizations and re-interpret their functionality. Through the lens of our study, more principled and efficient regularizations can possibly be invented in the near future.},
  archive      = {J_TPAMI},
  author       = {Yiwen Guo and Long Chen and Yurong Chen and Changshui Zhang},
  doi          = {10.1109/TPAMI.2020.3006917},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {4469-4476},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {On connections between regularizations for improving DNN robustness},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-view representation learning with deep gaussian
processes. <em>TPAMI</em>, <em>43</em>(12), 4453–4468. (<a
href="https://doi.org/10.1109/TPAMI.2020.3001433">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view representation learning is a promising and challenging research topic, which aims to integrate multiple data information from different views to improve the learning performance. The recent deep Gaussian processes (DGPs) have the advantages of good uncertainty estimates, powerful non-linear mapping ability and great generalization capability, which can be used as an excellent data representation learning method. However, DGPs only focus on single view data and are rarely applied to the multi-view scenario. In this paper, we propose a multi-view representation learning algorithm with deep Gaussian processes (named MvDGPs), which inherits the advantages of deep Gaussian processes and multi-view representation learning, and can learn more effective representation of multi-view data. The MvDGPs consist of two stages. The first stage is multi-view data representation learning, which is mainly used to learn more comprehensive representations of multi-view data. The second stage is classifier design, which aims to select an appropriate classifier to better employ the representations obtained in the first stage. In contrast with DGPs, MvDGPs support asymmetrical modeling depths for different views of data, resulting in better characterizations of the discrepancies among different views. Experimental results on real-world multi-view data sets verify the effectiveness of the proposed algorithm, which indicates that MvDGPs can integrate the complementary information in multiple views to discover a good representation of the data.},
  archive      = {J_TPAMI},
  author       = {Shiliang Sun and Wenbo Dong and Qiuyang Liu},
  doi          = {10.1109/TPAMI.2020.3001433},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {4453-4468},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Multi-view representation learning with deep gaussian processes},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MultiDIAL: Domain alignment layers for (multisource)
unsupervised domain adaptation. <em>TPAMI</em>, <em>43</em>(12),
4441–4452. (<a
href="https://doi.org/10.1109/TPAMI.2020.3001338">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the main challenges for developing visual recognition systems working in the wild is to devise computational models immune from the domain shift problem, i.e., accurate when test data are drawn from a (slightly) different data distribution than training samples. In the last decade, several research efforts have been devoted to devise algorithmic solutions for this issue. Recent attempts to mitigate domain shift have resulted into deep learning models for domain adaptation which learn domain-invariant representations by introducing appropriate loss terms, by casting the problem within an adversarial learning framework or by embedding into deep network specific domain normalization layers. This paper describes a novel approach for unsupervised domain adaptation. Similarly to previous works we propose to align the learned representations by embedding them into appropriate network feature normalization layers. Opposite to previous works, our Domain Alignment Layers are designed not only to match the source and target feature distributions but also to automatically learn the degree of feature alignment required at different levels of the deep network. Differently from most previous deep domain adaptation methods, our approach is able to operate in a multi-source setting. Thorough experiments on four publicly available benchmarks confirm the effectiveness of our approach.},
  archive      = {J_TPAMI},
  author       = {Fabio Maria Carlucci and Lorenzo Porzi and Barbara Caputo and Elisa Ricci and Samuel Rota Buló},
  doi          = {10.1109/TPAMI.2020.3001338},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {4441-4452},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {MultiDIAL: Domain alignment layers for (Multisource) unsupervised domain adaptation},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Large graph clustering with simultaneous spectral embedding
and discretization. <em>TPAMI</em>, <em>43</em>(12), 4426–4440. (<a
href="https://doi.org/10.1109/TPAMI.2020.3002587">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spectral clustering methods are gaining more and more interests and successfully applied in many fields because of their superior performance. However, there still exist two main problems to be solved: 1) spectral clustering methods consist of two successive optimization stages—spectral embedding and spectral rotation, which may not lead to globally optimal solutions, 2) and it is known that spectral methods are time-consuming with very high computational complexity. There are methods proposed to reduce the complexity for data vectors but not for graphs that only have information about similarity matrices. In this paper, we propose a new method to solve these two challenging problems for graph clustering. In the new method, a new framework is established to perform spectral embedding and spectral rotation simultaneously. The newly designed objective function consists of both terms of embedding and rotation, and we use an improved spectral rotation method to make it mathematically rigorous for the optimization. To further accelerate the algorithm, we derive a low-dimensional representation matrix from a graph by using label propagation, with which, in return, we can reconstruct a double-stochastic and positive semidefinite similarity matrix. Experimental results demonstrate that our method has excellent performance in time cost and accuracy.},
  archive      = {J_TPAMI},
  author       = {Zhen Wang and Zhaoqing Li and Rong Wang and Feiping Nie and Xuelong Li},
  doi          = {10.1109/TPAMI.2020.3002587},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {4426-4440},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Large graph clustering with simultaneous spectral embedding and discretization},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Kernel k-groups via hartigan’s method. <em>TPAMI</em>,
<em>43</em>(12), 4411–4425. (<a
href="https://doi.org/10.1109/TPAMI.2020.2998120">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Energy statistics was proposed by Székely in the 80’s inspired by Newton’s gravitational potential in classical mechanics and it provides a model-free hypothesis test for equality of distributions. In its original form, energy statistics was formulated in euclidean spaces. More recently, it was generalized to metric spaces of negative type. In this paper, we consider a formulation for the clustering problem using a weighted version of energy statistics in spaces of negative type. We show that this approach leads to a quadratically constrained quadratic program in the associated kernel space, establishing connections with graph partitioning problems and kernel methods in machine learning. To find local solutions of such an optimization problem, we propose kernel k-groups, which is an extension of Hartigan’s method to kernel spaces. Kernel k-groups is cheaper than spectral clustering and has the same computational cost as kernel k-means (which is based on Lloyd’s heuristic) but our numerical results show an improved performance, especially in higher dimensions. Moreover, we verify the efficiency of kernel k-groups in community detection in sparse stochastic block models which has fascinating applications in several areas of science.},
  archive      = {J_TPAMI},
  author       = {Guilherme França and Maria L. Rizzo and Joshua T. Vogelstein},
  doi          = {10.1109/TPAMI.2020.2998120},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {4411-4425},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Kernel k-groups via hartigan’s method},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Infinite feature selection: A graph-based feature filtering
approach. <em>TPAMI</em>, <em>43</em>(12), 4396–4410. (<a
href="https://doi.org/10.1109/TPAMI.2020.3002843">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a filtering feature selection framework that considers subsets of features as paths in a graph, where a node is a feature and an edge indicates pairwise (customizable) relations among features, dealing with relevance and redundancy principles. By two different interpretations (exploiting properties of power series of matrices and relying on Markov chains fundamentals) we can evaluate the values of paths (i.e., feature subsets) of arbitrary lengths, eventually go to infinite, from which we dub our framework Infinite Feature Selection (Inf-FS). Going to infinite allows to constrain the computational complexity of the selection process, and to rank the features in an elegant way, that is, considering the value of any path (subset) containing a particular feature. We also propose a simple unsupervised strategy to cut the ranking, so providing the subset of features to keep. In the experiments, we analyze diverse settings with heterogeneous features, for a total of 11 benchmarks, comparing against 18 widely-known comparative approaches. The results show that Inf-FS behaves better in almost any situation, that is, when the number of features to keep are fixed a priori, or when the decision of the subset cardinality is part of the process.},
  archive      = {J_TPAMI},
  author       = {Giorgio Roffo and Simone Melzi and Umberto Castellani and Alessandro Vinciarelli and Marco Cristani},
  doi          = {10.1109/TPAMI.2020.3002843},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {4396-4410},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Infinite feature selection: A graph-based feature filtering approach},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Evaluation of saccadic scanpath prediction: Subjective
assessment database and recurrent neural network based metric.
<em>TPAMI</em>, <em>43</em>(12), 4378–4395. (<a
href="https://doi.org/10.1109/TPAMI.2020.3002168">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, predicting the saccadic scanpaths of humans has become a new trend in the field of visual attention modeling. Given various saccadic algorithms, determining how to evaluate their ability to model a dynamic saccade has become an important yet understudied issue. To our best knowledge, existing metrics for evaluating saccadic prediction models are often heuristically designed, which may produce results that are inconsistent with human subjective assessment. To this end, we first construct a subjective database by collecting the assessments on 5,000 pairs of scanpaths from ten subjects. Based on this database, we can compare different metrics according to their consistency with human visual perception. In addition, we also propose a data-driven metric to measure scanpath similarity based on the human subjective comparison. To achieve this goal, we employ a long short-term memory (LSTM) network to learn the inference from the relationship of encoded scanpaths to a binary measurement. Experimental results have demonstrated that the LSTM-based metric outperforms other existing metrics. Moreover, we believe the constructed database can be used as a benchmark to inspire more insights for future metric selection.},
  archive      = {J_TPAMI},
  author       = {Chen Xia and Junwei Han and Dingwen Zhang},
  doi          = {10.1109/TPAMI.2020.3002168},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {4378-4395},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Evaluation of saccadic scanpath prediction: Subjective assessment database and recurrent neural network based metric},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep non-rigid structure from motion with missing data.
<em>TPAMI</em>, <em>43</em>(12), 4365–4377. (<a
href="https://doi.org/10.1109/TPAMI.2020.2997026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Non-rigid structure from motion (NRSfM) refers to the problem of reconstructing cameras and the 3D point cloud of a non-rigid object from an ensemble of images with 2D correspondences. Current NRSfM algorithms are limited from two perspectives: (i) the number of images, and (ii) the type of shape variability they can handle. These difficulties stem from the inherent conflict between the condition of the system and the degrees of freedom needing to be modeled – which has hampered its practical utility for many applications within vision. In this paper we propose a novel hierarchical sparse coding model for NRSFM which can overcome (i) and (ii) to such an extent, that NRSFM can be applied to problems in vision previously thought too ill posed. Our approach is realized in practice as the training of an unsupervised deep neural network (DNN) auto-encoder with a unique architecture that is able to disentangle pose from 3D structure. Using modern deep learning computational platforms allows us to solve NRSfM problems at an unprecedented scale and shape complexity. Our approach has no 3D supervision, relying solely on 2D point correspondences. Further, our approach is also able to handle missing/occluded 2D points without the need for matrix completion. Extensive experiments demonstrate the impressive performance of our approach where we exhibit superior precision and robustness against all available state-of-the-art works in some instances by an order of magnitude. We further propose a new quality measure (based on the network weights) which circumvents the need for 3D ground-truth to ascertain the confidence we have in the reconstructability. We believe our work to be a significant advance over state-of-the-art in NRSFM.},
  archive      = {J_TPAMI},
  author       = {Chen Kong and Simon Lucey},
  doi          = {10.1109/TPAMI.2020.2997026},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {4365-4377},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Deep non-rigid structure from motion with missing data},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep learning for 3D point clouds: A survey. <em>TPAMI</em>,
<em>43</em>(12), 4338–4364. (<a
href="https://doi.org/10.1109/TPAMI.2020.3005434">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point cloud learning has lately attracted increasing attention due to its wide applications in many areas, such as computer vision, autonomous driving, and robotics. As a dominating technique in AI, deep learning has been successfully used to solve various 2D vision problems. However, deep learning on point clouds is still in its infancy due to the unique challenges faced by the processing of point clouds with deep neural networks. Recently, deep learning on point clouds has become even thriving, with numerous methods being proposed to address different problems in this area. To stimulate future research, this paper presents a comprehensive review of recent progress in deep learning methods for point clouds. It covers three major tasks, including 3D shape classification, 3D object detection and tracking, and 3D point cloud segmentation. It also presents comparative results on several publicly available datasets, together with insightful observations and inspiring future research directions.},
  archive      = {J_TPAMI},
  author       = {Yulan Guo and Hanyun Wang and Qingyong Hu and Hao Liu and Li Liu and Mohammed Bennamoun},
  doi          = {10.1109/TPAMI.2020.3005434},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {4338-4364},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Deep learning for 3D point clouds: A survey},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep back-ProjectiNetworks for single image
super-resolution. <em>TPAMI</em>, <em>43</em>(12), 4323–4337. (<a
href="https://doi.org/10.1109/TPAMI.2020.3002836">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Previous feed-forward architectures of recently proposed deep super-resolution networks learn the features of low-resolution inputs and the non-linear mapping from those to a high-resolution output. However, this approach does not fully address the mutual dependencies of low- and high-resolution images. We propose Deep Back-Projection Networks (DBPN), the winner of two image super-resolution challenges (NTIRE2018 and PIRM2018), that exploit iterative up- and down-sampling layers. These layers are formed as a unit providing an error feedback mechanism for projection errors. We construct mutually-connected up- and down-sampling units each of which represents different types of low- and high-resolution components. We also show that extending this idea to demonstrate a new insight towards more efficient network design substantially, such as parameter sharing on the projection module and transition layer on projection step. The experimental results yield superior results and in particular establishing new state-of-the-art results across multiple data sets, especially for large scaling factors such as $8\times$ .},
  archive      = {J_TPAMI},
  author       = {Muhammad Haris and Greg Shakhnarovich and Norimichi Ukita},
  doi          = {10.1109/TPAMI.2020.3002836},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {4323-4337},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Deep back-ProjectiNetworks for single image super-resolution},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep autoencoding topic model with scalable hybrid bayesian
inference. <em>TPAMI</em>, <em>43</em>(12), 4306–4322. (<a
href="https://doi.org/10.1109/TPAMI.2020.3003660">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To build a flexible and interpretable model for document analysis, we develop deep autoencoding topic model (DATM) that uses a hierarchy of gamma distributions to construct its multi-stochastic-layer generative network. In order to provide scalable posterior inference for the parameters of the generative network, we develop topic-layer-adaptive stochastic gradient Riemannian MCMC that jointly learns simplex-constrained global parameters across all layers and topics, with topic and layer specific learning rates. Given a posterior sample of the global parameters, in order to efficiently infer the local latent representations of a document under DATM across all stochastic layers, we propose a Weibull upward-downward variational encoder that deterministically propagates information upward via a deep neural network, followed by a Weibull distribution based stochastic downward generative model. To jointly model documents and their associated labels, we further propose supervised DATM that enhances the discriminative power of its latent representations. The efficacy and scalability of our models are demonstrated on both unsupervised and supervised learning tasks on big corpora.},
  archive      = {J_TPAMI},
  author       = {Hao Zhang and Bo Chen and Yulai Cong and Dandan Guo and Hongwei Liu and Mingyuan Zhou},
  doi          = {10.1109/TPAMI.2020.3003660},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {4306-4322},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Deep autoencoding topic model with scalable hybrid bayesian inference},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). CrossNet++: Cross-scale large-parallax warping for
reference-based super-resolution. <em>TPAMI</em>, <em>43</em>(12),
4291–4305. (<a
href="https://doi.org/10.1109/TPAMI.2020.2997007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ability of camera arrays to efficiently capture higher space-bandwidth product than single cameras has led to various multiscale and hybrid systems. These systems play vital roles in computational photography, including light field imaging, 360 VR camera, gigapixel videography, etc. One of the critical tasks in multiscale hybrid imaging is matching and fusing cross-resolution images from different cameras under perspective parallax. In this paper, we investigate the reference-based super-resolution (RefSR) problem associated with dual-camera or multi-camera systems. RefSR consists of super-resolving a low-resolution (LR) image given an external high-resolution (HR) reference image, where they suffer both a significant resolution gap ( $8\times$ ) and large parallax ( $\sim 10\%$ pixel displacement). We present CrossNet++, an end-to-end network containing novel two-stage cross-scale warping modules, image encoder and fusion decoder. The stage I learns to narrow down the parallax distinctively with the strong guidance of landmarks and intensity distribution consensus. Then the stage II operates more fine-grained alignment and aggregation in feature domain to synthesize the final super-resolved image. To further address the large parallax, new hybrid loss functions comprising warping loss, landmark loss and super-resolution loss are proposed to regularize training and enable better convergence. CrossNet++ significantly outperforms the state-of-art on light field datasets as well as real dual-camera data. We further demonstrate the generalization of our framework by transferring it to video super-resolution and video denoising.},
  archive      = {J_TPAMI},
  author       = {Yang Tan and Haitian Zheng and Yinheng Zhu and Xiaoyun Yuan and Xing Lin and David Brady and Lu Fang},
  doi          = {10.1109/TPAMI.2020.2997007},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {4291-4305},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {CrossNet++: Cross-scale large-parallax warping for reference-based super-resolution},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bridging the gap between computational photography and
visual recognition. <em>TPAMI</em>, <em>43</em>(12), 4272–4290. (<a
href="https://doi.org/10.1109/TPAMI.2020.2996538">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {What is the current state-of-the-art for image restoration and enhancement applied to degraded images acquired under less than ideal circumstances? Can the application of such algorithms as a pre-processing step improve image interpretability for manual analysis or automatic visual recognition to classify scene content? While there have been important advances in the area of computational photography to restore or enhance the visual quality of an image, the capabilities of such techniques have not always translated in a useful way to visual recognition tasks. Consequently, there is a pressing need for the development of algorithms that are designed for the joint problem of improving visual appearance and recognition, which will be an enabling factor for the deployment of visual recognition tools in many real-world scenarios. To address this, we introduce the UG $^2$ dataset as a large-scale benchmark composed of video imagery captured under challenging conditions, and two enhancement tasks designed to test algorithmic impact on visual quality and automatic object recognition. Furthermore, we propose a set of metrics to evaluate the joint improvement of such tasks as well as individual algorithmic advances, including a novel psychophysics-based evaluation regime for human assessment and a realistic set of quantitative measures for object recognition performance. We introduce six new algorithms for image restoration or enhancement, which were created as part of the IARPA sponsored UG $^2$ Challenge workshop held at CVPR 2018. Under the proposed evaluation regime, we present an in-depth analysis of these algorithms and a host of deep learning-based and classic baseline approaches. From the observed results, it is evident that we are in the early days of building a bridge between computational photography and visual recognition, leaving many opportunities for innovation in this area.},
  archive      = {J_TPAMI},
  author       = {Rosaura G. VidalMata and Sreya Banerjee and Brandon RichardWebster and Michael Albright and Pedro Davalos and Scott McCloskey and Ben Miller and Asong Tambo and Sushobhan Ghosh and Sudarshan Nagesh and Ye Yuan and Yueyu Hu and Junru Wu and Wenhan Yang and Xiaoshuai Zhang and Jiaying Liu and Zhangyang Wang and Hwann-Tzong Chen and Tzu-Wei Huang and Wen-Chi Chin and Yi-Chun Li and Mahmoud Lababidi and Charles Otto and Walter J. Scheirer},
  doi          = {10.1109/TPAMI.2020.2996538},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {4272-4290},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Bridging the gap between computational photography and visual recognition},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Attention-based dropout layer for weakly supervised single
object localization and semantic segmentation. <em>TPAMI</em>,
<em>43</em>(12), 4256–4271. (<a
href="https://doi.org/10.1109/TPAMI.2020.2999099">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Both weakly supervised single object localization and semantic segmentation techniques learn an object’s location using only image-level labels. However, these techniques are limited to cover only the most discriminative part of the object and not the entire object. To address this problem, we propose an attention-based dropout layer, which utilizes the attention mechanism to locate the entire object efficiently. To achieve this, we devise two key components, 1) hiding the most discriminative part from the model to capture the entire object, and 2) highlighting the informative region to improve the classification power of the model. These allow the classifier to be maintained with a reasonable accuracy while the entire object is covered. Through extensive experiments, we demonstrate that the proposed method effectively improves the weakly supervised single object localization accuracy, thereby achieving a new state-of-the-art localization accuracy on the CUB-200-2011 and a comparable accuracy existing state-of-the-arts on the ImageNet-1k. The proposed method is also effective in improving the weakly supervised semantic segmentation performance on the Pascal VOC and MS COCO. Furthermore, the proposed method is more efficient than existing techniques in terms of parameter and computation overheads. Additionally, the proposed method can be easily applied in various backbone networks.},
  archive      = {J_TPAMI},
  author       = {Junsuk Choe and Seungho Lee and Hyunjung Shim},
  doi          = {10.1109/TPAMI.2020.2999099},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {4256-4271},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Attention-based dropout layer for weakly supervised single object localization and semantic segmentation},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Accelerated variance reduction stochastic ADMM for
large-scale machine learning. <em>TPAMI</em>, <em>43</em>(12),
4242–4255. (<a
href="https://doi.org/10.1109/TPAMI.2020.3000512">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, many stochastic variance reduced alternating direction methods of multipliers (ADMMs) (e.g., SAG-ADMM and SVRG-ADMM) have made exciting progress such as linear convergence rate for strongly convex (SC) problems. However, their best-known convergence rate for non-strongly convex (non-SC) problems is $\mathcal {O}(1/T)$ as opposed to $\mathcal {O}(1/T^2)$ of accelerated deterministic algorithms, where $T$ is the number of iterations. Thus, there remains a gap in the convergence rates of existing stochastic ADMM and deterministic algorithms. To bridge this gap, we introduce a new momentum acceleration trick into stochastic variance reduced ADMM, and propose a novel accelerated SVRG-ADMM method (called ASVRG-ADMM) for the machine learning problems with the constraint $Ax + By = c$ . Then we design a linearized proximal update rule and a simple proximal one for the two classes of ADMM-style problems with $B = \tau I$ and $B\ne \tau I$ , respectively, where $I$ is an identity matrix and $\tau$ is an arbitrary bounded constant. Note that our linearized proximal update rule can avoid solving sub-problems iteratively. Moreover, we prove that ASVRG-ADMM converges linearly for SC problems. In particular, ASVRG-ADMM improves the convergence rate from $\mathcal {O}(1/T)$ to $\mathcal {O}(1/T^2)$ for non-SC problems. Finally, we apply ASVRG-ADMM to various machine learning problems, e.g., graph-guided fused Lasso, graph-guided logistic regression, graph-guided SVM, generalized graph-guided fused Lasso and multi-task learning, and show that ASVRG-ADMM consistently converges faster than the state-of-the-art methods.},
  archive      = {J_TPAMI},
  author       = {Yuanyuan Liu and Fanhua Shang and Hongying Liu and Lin Kong and Licheng Jiao and Zhouchen Lin},
  doi          = {10.1109/TPAMI.2020.3000512},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {4242-4255},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Accelerated variance reduction stochastic ADMM for large-scale machine learning},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MannequinChallenge: Learning the depths of moving people by
watching frozen people. <em>TPAMI</em>, <em>43</em>(12), 4229–4241. (<a
href="https://doi.org/10.1109/TPAMI.2020.2974454">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a method for predicting dense depth in scenarios where both a monocular camera and people in the scene are freely moving (right). Existing methods for recovering depth for dynamic, non-rigid objects from monocular video impose strong assumptions on the objects’ motion and may only recover sparse depth. In this paper, we take a data-driven approach and learn human depth priors from a new source of data: thousands of Internet videos of people imitating mannequins, i.e., freezing in diverse, natural poses, while a hand-held camera tours the scene (left). Because people are stationary, geometric constraints hold, thus training data can be generated using multi-view stereo reconstruction. At inference time, our method uses motion parallax cues from the static areas of the scenes to guide the depth prediction. We evaluate our method on real-world sequences of complex human actions captured by a moving hand-held camera, show improvement over state-of-the-art monocular depth prediction methods, and demonstrate various 3D effects produced using our predicted depth.},
  archive      = {J_TPAMI},
  author       = {Zhengqi Li and Tali Dekel and Forrester Cole and Richard Tucker and Noah Snavely and Ce Liu and William T. Freeman},
  doi          = {10.1109/TPAMI.2020.2974454},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {4229-4241},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {MannequinChallenge: Learning the depths of moving people by watching frozen people},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A style-based generator architecture for generative
adversarial networks. <em>TPAMI</em>, <em>43</em>(12), 4217–4228. (<a
href="https://doi.org/10.1109/TPAMI.2020.2970919">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose an alternative generator architecture for generative adversarial networks, borrowing from style transfer literature. The new architecture leads to an automatically learned, unsupervised separation of high-level attributes (e.g., pose and identity when trained on human faces) and stochastic variation in the generated images (e.g., freckles, hair), and it enables intuitive, scale-specific control of the synthesis. The new generator improves the state-of-the-art in terms of traditional distribution quality metrics, leads to demonstrably better interpolation properties, and also better disentangles the latent factors of variation. To quantify interpolation quality and disentanglement, we propose two new, automated methods that are applicable to any generator architecture. Finally, we introduce a new, highly varied and high-quality dataset of human faces.},
  archive      = {J_TPAMI},
  author       = {Tero Karras and Samuli Laine and Timo Aila},
  doi          = {10.1109/TPAMI.2020.2970919},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {4217-4228},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {A style-based generator architecture for generative adversarial networks},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Vision-language navigation policy learning and adaptation.
<em>TPAMI</em>, <em>43</em>(12), 4205–4216. (<a
href="https://doi.org/10.1109/TPAMI.2020.2972281">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vision-language navigation (VLN) is the task of navigating an embodied agent to carry out natural language instructions inside real 3D environments. In this paper, we study how to address three critical challenges for this task: the cross-modal grounding, the ill-posed feedback, and the generalization problems. First, we propose a novel Reinforced Cross-Modal Matching (RCM) approach that enforces cross-modal grounding both locally and globally via reinforcement learning (RL). Particularly, a matching critic is used to provide an intrinsic reward to encourage global matching between instructions and trajectories, and a reasoning navigator is employed to perform cross-modal grounding in the local visual scene. Evaluation on a VLN benchmark dataset shows that our RCM model significantly outperforms baseline methods by 10 percent on Success Rate weighted by Path Length (SPL) and achieves the state-of-the-art performance. To improve the generalizability of the learned policy, we further introduce a Self-Supervised Imitation Learning (SIL) method to explore and adapt to unseen environments by imitating its own past, good decisions. We demonstrate that SIL can approximate a better and more efficient policy, which tremendously minimizes the success rate performance gap between seen and unseen environments (from 30.7 to 11.7 percent).},
  archive      = {J_TPAMI},
  author       = {Xin Wang and Qiuyuan Huang and Asli Celikyilmaz and Jianfeng Gao and Dinghan Shen and Yuan-Fang Wang and William Yang Wang and Lei Zhang},
  doi          = {10.1109/TPAMI.2020.2972281},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {4205-4216},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Vision-language navigation policy learning and adaptation},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Editorial: Introduction to the special section on CVPR2019
best papers. <em>TPAMI</em>, <em>43</em>(12), 4203–4204. (<a
href="https://doi.org/10.1109/TPAMI.2021.3080715">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The three papers in this special section were presented at the 2019 CVPR conference that was held in Long Beach CA, in June of 2019.},
  archive      = {J_TPAMI},
  author       = {Gang Hua and Derek Hoiem and Abhinav Gupta and Zhuowen Tu},
  doi          = {10.1109/TPAMI.2021.3080715},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {12},
  pages        = {4203-4204},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Editorial: Introduction to the special section on CVPR2019 best papers},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Guided zoom: Zooming into network evidence to refine
fine-grained model decisions. <em>TPAMI</em>, <em>43</em>(11),
4196–4202. (<a
href="https://doi.org/10.1109/TPAMI.2021.3054303">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In state-of-the-art deep single-label classification models, the top- $k$ $(k=2,3,4, \dots)$ accuracy is usually significantly higher than the top-1 accuracy. This is more evident in fine-grained datasets, where differences between classes are quite subtle. Exploiting the information provided in the top $k$ predicted classes boosts the final prediction of a model. We propose Guided Zoom, a novel way in which explainability could be used to improve model performance. We do so by making sure the model has “the right reasons” for a prediction. The reason/evidence upon which a deep neural network makes a prediction is defined to be the grounding, in the pixel space, for a specific class conditional probability in the model output. Guided Zoom examines how reasonable the evidence used to make each of the top- $k$ predictions is. Test time evidence is deemed reasonable if it is coherent with evidence used to make similar correct decisions at training time. This leads to better informed predictions. We explore a variety of grounding techniques and study their complementarity for computing evidence. We show that Guided Zoom results in an improvement of a model&#39;s classification accuracy and achieves state-of-the-art classification performance on four fine-grained classification datasets. Our code is available at https://github.com/andreazuna89/Guided-Zoom .},
  archive      = {J_TPAMI},
  author       = {Sarah Adel Bargal and Andrea Zunino and Vitali Petsiuk and Jianming Zhang and Kate Saenko and Vittorio Murino and Stan Sclaroff},
  doi          = {10.1109/TPAMI.2021.3054303},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {4196-4202},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Guided zoom: Zooming into network evidence to refine fine-grained model decisions},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Discriminative triad matching and reconstruction for weakly
referring expression grounding. <em>TPAMI</em>, <em>43</em>(11),
4189–4195. (<a
href="https://doi.org/10.1109/TPAMI.2021.3058684">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we are tackling the weakly-supervised referring expression grounding task, for the localization of a referent object in an image according to a query sentence, where the mapping between image regions and queries are not available during the training stage. In traditional methods, an object region that best matches the referring expression is picked out, and then the query sentence is reconstructed from the selected region, where the reconstruction difference serves as the loss for back-propagation. The existing methods, however, conduct both the matching and the reconstruction approximately as they ignore the fact that the matching correctness is unknown. To overcome this limitation, a discriminative triad is designed here as the basis to the solution, through which a query can be converted into one or multiple discriminative triads in a very scalable way. Based on the discriminative triad, we further propose the triad-level matching and reconstruction modules which are lightweight yet effective for the weakly-supervised training, making it three times lighter and faster than the previous state-of-the-art methods. One important merit of our work is its superior performance despite the simple and neat design. Specifically, the proposed method achieves a new state-of-the-art accuracy when evaluated on RefCOCO (39.21 percent), RefCOCO+ (39.18 percent) and RefCOCOg (43.24 percent) datasets, that is 4.17, 4.08 and 7.8 percent higher than the previous one, respectively. The code is available at https://github.com/insomnia94/DTWREG .},
  archive      = {J_TPAMI},
  author       = {Mingjie Sun and Jimin Xiao and Eng Gee Lim and Si Liu and John Y. Goulermas},
  doi          = {10.1109/TPAMI.2021.3058684},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {4189-4195},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Discriminative triad matching and reconstruction for weakly referring expression grounding},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Wavefront marching methods: A unified algorithm to solve
eikonal and static hamilton-jacobi equations. <em>TPAMI</em>,
<em>43</em>(11), 4177–4188. (<a
href="https://doi.org/10.1109/TPAMI.2020.2993500">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a unified propagation method for dealing with both the classic Eikonal equation, where the motion direction does not affect the propagation, and the more general static Hamilton-Jacobi equations, where it does. While classic Fast Marching Method (FMM) techniques achieve the solution to the Eikonal equation with a O(M log M) (or O(M) assuming some modifications), solving the more general static Hamilton-Jacobi equation requires a higher complexity. The proposed framework maintains the O(M log M) complexity for both problems, while achieving higher accuracy than available state-of-the-art. The key idea behind the proposed method is the creation of ‘mini wave-fronts’, where the solution is interpolated to minimize the discretization error. Experimental results show how our algorithm can outperform the state-of-the-art both in precision and computational cost.},
  archive      = {J_TPAMI},
  author       = {Brais Cancela and Amparo Alonso-Betanzos},
  doi          = {10.1109/TPAMI.2020.2993500},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {4177-4188},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Wavefront marching methods: A unified algorithm to solve eikonal and static hamilton-jacobi equations},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Unpaired person image generation with semantic parsing
transformation. <em>TPAMI</em>, <em>43</em>(11), 4161–4176. (<a
href="https://doi.org/10.1109/TPAMI.2020.2992105">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we tackle the problem of pose-guided person image generation with unpaired data, which is a challenging problem due to non-rigid spatial deformation. Instead of learning a fixed mapping directly between human bodies as previous methods, we propose a new pathway to decompose a single fixed mapping into two subtasks, namely, semantic parsing transformation and appearance generation. First, to simplify the learning for non-rigid deformation, a semantic generative network is developed to transform semantic parsing maps between different poses. Second, guided by semantic parsing maps, we render the foreground and background image, respectively. A foreground generative network learns to synthesize semantic-aware textures, and another background generative network learns to predict missing background regions caused by pose changes. Third, we enable pseudo-label training with unpaired data, and demonstrate that end-to-end training of the overall network further refines the semantic map prediction and final results accordingly. Moreover, our method is generalizable to other person image generation tasks defined on semantic maps, e.g., clothing texture transfer, controlled image manipulation, and virtual try-on. Experimental results on DeepFashion and Market-1501 datasets demonstrate the superiority of our method, especially in keeping better body shapes and clothing attributes, as well as rendering structure-coherent backgrounds.},
  archive      = {J_TPAMI},
  author       = {Sijie Song and Wei Zhang and Jiaying Liu and Zongming Guo and Tao Mei},
  doi          = {10.1109/TPAMI.2020.2992105},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {4161-4176},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Unpaired person image generation with semantic parsing transformation},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Towards a complete 3D morphable model of the human head.
<em>TPAMI</em>, <em>43</em>(11), 4142–4160. (<a
href="https://doi.org/10.1109/TPAMI.2020.2991150">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Three-dimensional morphable models (3DMMs) are powerful statistical tools for representing the 3D shapes and textures of an object class. Here we present the most complete 3DMM of the human head to date that includes face, cranium, ears, eyes, teeth and tongue. To achieve this, we propose two methods for combining existing 3DMMs of different overlapping head parts: (i). use a regressor to complete missing parts of one model using the other, and (ii). use the Gaussian Process framework to blend covariance matrices from multiple models. Thus, we build a new combined face-and-head shape model that blends the variability and facial detail of an existing face model (the LSFM) with the full head modelling capability of an existing head model (the LYHM). Then we construct and fuse a highly-detailed ear model to extend the variation of the ear shape. Eye and eye region models are incorporated into the head model, along with basic models of the teeth, tongue and inner mouth cavity. The new model achieves state-of-the-art performance. We use our model to reconstruct full head representations from single, unconstrained images allowing us to parameterize craniofacial shape and texture, along with the ear shape, eye gaze and eye color.},
  archive      = {J_TPAMI},
  author       = {Stylianos Ploumpis and Evangelos Ververas and Eimear O&#39; Sullivan and Stylianos Moschoglou and Haoyang Wang and Nick Pears and William A. P. Smith and Baris Gecer and Stefanos Zafeiriou},
  doi          = {10.1109/TPAMI.2020.2991150},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {4142-4160},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Towards a complete 3D morphable model of the human head},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The EPIC-KITCHENS dataset: Collection, challenges and
baselines. <em>TPAMI</em>, <em>43</em>(11), 4125–4141. (<a
href="https://doi.org/10.1109/TPAMI.2020.2991965">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since its introduction in 2018, EPIC-KITCHENS has attracted attention as the largest egocentric video benchmark, offering a unique viewpoint on people’s interaction with objects, their attention, and even intention. In this paper, we detail how this large-scale dataset was captured by 32 participants in their native kitchen environments, and densely annotated with actions and object interactions. Our videos depict nonscripted daily activities, as recording is started every time a participant entered their kitchen. Recording took place in four countries by participants belonging to ten different nationalities, resulting in highly diverse kitchen habits and cooking styles. Our dataset features 55 hours of video consisting of 11.5M frames, which we densely labelled for a total of 39.6K action segments and 454.2K object bounding boxes. Our annotation is unique in that we had the participants narrate their own videos (after recording), thus reflecting true intention, and we crowd-sourced ground-truths based on these. We describe our object, action and anticipation challenges, and evaluate several baselines over two test splits, seen and unseen kitchens. We introduce new baselines that highlight the multimodal nature of the dataset and the importance of explicit temporal modelling to discriminate fine-grained actions (e.g., ‘closing a tap’ from ‘opening’ it up).},
  archive      = {J_TPAMI},
  author       = {Dima Damen and Hazel Doughty and Giovanni Maria Farinella and Sanja Fidler and Antonino Furnari and Evangelos Kazakos and Davide Moltisanti and Jonathan Munro and Toby Perrett and Will Price and Michael Wray},
  doi          = {10.1109/TPAMI.2020.2991965},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {4125-4141},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {The EPIC-KITCHENS dataset: Collection, challenges and baselines},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The bayesian cut. <em>TPAMI</em>, <em>43</em>(11),
4111–4124. (<a
href="https://doi.org/10.1109/TPAMI.2020.2994396">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An important task in the analysis of graphs is separating nodes into densely connected groups with little interaction between each other. Prominent methods here include flow based graph cutting procedures as well as statistical network modeling approaches. However, adequately accounting for this, the so-called community structure, in complex networks remains a major challenge. We present a novel generic Bayesian probabilistic model for graph cutting in which we derive an analytical solution to the marginalization of nuisance parameters under constraints enforcing community structure. As a part of the solution a large scale approximation for integrals involving multiple incomplete gamma functions is derived. Our multiple cluster solution presents a generic tool for Bayesian inference on Poisson weighted graphs across different domains. Applied on three real world social networks as well as three image segmentation problems our approach shows on par or better performance to existing spectral graph cutting and community detection methods, while learning the underlying parameter space. The developed procedure provides a principled statistical framework for graph cutting and the Bayesian Cut source code provided enables easy adoption of the procedure as an alternative to existing graph cutting methods.},
  archive      = {J_TPAMI},
  author       = {Petr Taborsky and Laurent Vermue and Maciej Korzepa and Morten Mørup},
  doi          = {10.1109/TPAMI.2020.2994396},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {4111-4124},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {The bayesian cut},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Task-feature collaborative learning with application to
personalized attribute prediction. <em>TPAMI</em>, <em>43</em>(11),
4094–4110. (<a
href="https://doi.org/10.1109/TPAMI.2020.2991344">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an effective learning paradigm against insufficient training samples, multi-task learning (MTL) encourages knowledge sharing across multiple related tasks so as to improve the overall performance. In MTL, a major challenge springs from the phenomenon that sharing the knowledge with dissimilar and hard tasks, known as negative transfer , often results in a worsened performance. Though a substantial amount of studies have been carried out against the negative transfer, most of the existing methods only model the transfer relationship as task correlations, with the transfer across features and tasks left unconsidered. Different from the existing methods, our goal is to alleviate negative transfer collaboratively across features and tasks. To this end, we propose a novel multi-task learning method called task-feature collaborative learning (TFCL). Specifically, we first propose a base model with a heterogeneous block-diagonal structure regularizer to leverage the collaborative grouping of features and tasks and suppressing inter-group knowledge sharing. We then propose an optimization method for the model. Extensive theoretical analysis shows that our proposed method has the following benefits: (a) it enjoys the global convergence property and (b) it provides a block-diagonal structure recovery guarantee. As a practical extension, we extend the base model by allowing overlapping features and differentiating the hard tasks. We further apply it to the personalized attribute prediction problem with fine-grained modeling of user behaviors. Finally, experimental results on both simulated dataset and real-world datasets demonstrate the effectiveness of our proposed method.},
  archive      = {J_TPAMI},
  author       = {Zhiyong Yang and Qianqian Xu and Xiaochun Cao and Qingming Huang},
  doi          = {10.1109/TPAMI.2020.2991344},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {4094-4110},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Task-feature collaborative learning with application to personalized attribute prediction},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SurfaceNet+: An end-to-end 3D neural network for very sparse
multi-view stereopsis. <em>TPAMI</em>, <em>43</em>(11), 4078–4093. (<a
href="https://doi.org/10.1109/TPAMI.2020.2996798">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view stereopsis (MVS) tries to recover the 3D model from 2D images. As the observations become sparser, the significant 3D information loss makes the MVS problem more challenging. Instead of only focusing on densely sampled conditions, we investigate sparse-MVS with large baseline angles since the sparser sensation is more practical and more cost-efficient. By investigating various observation sparsities, we show that the classical depth-fusion pipeline becomes powerless for the case with a larger baseline angle that worsens the photo-consistency check. As another line of the solution, we present SurfaceNet+, a volumetric method to handle the ‘incompleteness’ and the ‘inaccuracy’ problems induced by a very sparse MVS setup. Specifically, the former problem is handled by a novel volume-wise view selection approach. It owns superiority in selecting valid views while discarding invalid occluded views by considering the geometric prior. Furthermore, the latter problem is handled via a multi-scale strategy that consequently refines the recovered geometry around the region with the repeating pattern. The experiments demonstrate the tremendous performance gap between SurfaceNet+ and state-of-the-art methods in terms of precision and recall. Under the extreme sparse-MVS settings in two datasets, where existing methods can only return very few points, SurfaceNet+ still works as well as in the dense MVS setting.},
  archive      = {J_TPAMI},
  author       = {Mengqi Ji and Jinzhi Zhang and Qionghai Dai and Lu Fang},
  doi          = {10.1109/TPAMI.2020.2996798},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {4078-4093},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {SurfaceNet+: An end-to-end 3D neural network for very sparse multi-view stereopsis},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Single image deraining: From model-based to data-driven and
beyond. <em>TPAMI</em>, <em>43</em>(11), 4059–4077. (<a
href="https://doi.org/10.1109/TPAMI.2020.2995190">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of single-image deraining is to restore the rain-free background scenes of an image degraded by rain streaks and rain accumulation. The early single-image deraining methods employ a cost function, where various priors are developed to represent the properties of rain and background layers. Since 2017, single-image deraining methods step into a deep-learning era, and exploit various types of networks, i.e., convolutional neural networks, recurrent neural networks, generative adversarial networks, etc., demonstrating impressive performance. Given the current rapid development, in this paper, we provide a comprehensive survey of deraining methods over the last decade. We summarize the rain appearance models, and discuss two categories of deraining approaches: model-based and data-driven approaches. For the former, we organize the literature based on their basic models and priors. For the latter, we discuss the developed ideas related to architectures, constraints, loss functions, and training datasets. We present milestones of single-image deraining methods, review a broad selection of previous works in different categories, and provide insights on the historical development route from the model-based to data-driven methods. We also summarize performance comparisons quantitatively and qualitatively. Beyond discussing the technicality of deraining methods, we also discuss the future possible directions.},
  archive      = {J_TPAMI},
  author       = {Wenhan Yang and Robby T. Tan and Shiqi Wang and Yuming Fang and Jiaying Liu},
  doi          = {10.1109/TPAMI.2020.2995190},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {4059-4077},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Single image deraining: From model-based to data-driven and beyond},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Self-supervised visual feature learning with deep neural
networks: A survey. <em>TPAMI</em>, <em>43</em>(11), 4037–4058. (<a
href="https://doi.org/10.1109/TPAMI.2020.2992393">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large-scale labeled data are generally required to train deep neural networks in order to obtain better performance in visual feature learning from images or videos for computer vision applications. To avoid extensive cost of collecting and annotating large-scale datasets, as a subset of unsupervised learning methods, self-supervised learning methods are proposed to learn general image and video features from large-scale unlabeled data without using any human-annotated labels. This paper provides an extensive review of deep learning-based self-supervised general visual feature learning methods from images or videos. First, the motivation, general pipeline, and terminologies of this field are described. Then the common deep neural network architectures that used for self-supervised learning are summarized. Next, the schema and evaluation metrics of self-supervised learning methods are reviewed followed by the commonly used datasets for images, videos, audios, and 3D data, as well as the existing self-supervised visual feature learning methods. Finally, quantitative performance comparisons of the reviewed methods on benchmark datasets are summarized and discussed for both image and video feature learning. At last, this paper is concluded and lists a set of promising future directions for self-supervised visual feature learning.},
  archive      = {J_TPAMI},
  author       = {Longlong Jing and Yingli Tian},
  doi          = {10.1109/TPAMI.2020.2992393},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {4037-4058},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Self-supervised visual feature learning with deep neural networks: A survey},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Rolling-unrolling LSTMs for action anticipation from
first-person video. <em>TPAMI</em>, <em>43</em>(11), 4021–4036. (<a
href="https://doi.org/10.1109/TPAMI.2020.2992889">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we tackle the problem of egocentric action anticipation, i.e., predicting what actions the camera wearer will perform in the near future and which objects they will interact with. Specifically, we contribute Rolling-Unrolling LSTM, a learning architecture to anticipate actions from egocentric videos. The method is based on three components: 1) an architecture comprised of two LSTMs to model the sub-tasks of summarizing the past and inferring the future, 2) a Sequence Completion Pre-Training technique which encourages the LSTMs to focus on the different sub-tasks, and 3) a Modality ATTention (MATT) mechanism to efficiently fuse multi-modal predictions performed by processing RGB frames, optical flow fields and object-based features. The proposed approach is validated on EPIC-Kitchens, EGTEA Gaze+ and ActivityNet. The experiments show that the proposed architecture is state-of-the-art in the domain of egocentric videos, achieving top performances in the 2019 EPIC-Kitchens egocentric action anticipation challenge. The approach also achieves competitive performance on ActivityNet with respect to methods not based on unsupervised pre-training and generalizes to the tasks of early action recognition and action recognition. To encourage research on this challenging topic, we made our code, trained models, and pre-extracted features available at our web page: http://iplab.dmi.unict.it/rulstm .},
  archive      = {J_TPAMI},
  author       = {Antonino Furnari and Giovanni Maria Farinella},
  doi          = {10.1109/TPAMI.2020.2992889},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {4021-4036},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Rolling-unrolling LSTMs for action anticipation from first-person video},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). RefineFace: Refinement neural network for high performance
face detection. <em>TPAMI</em>, <em>43</em>(11), 4008–4020. (<a
href="https://doi.org/10.1109/TPAMI.2020.2997456">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face detection has achieved significant progress in recent years. However, high performance face detection still remains a very challenging problem, especially when there exists many tiny faces. In this paper, we present a single-shot refinement face detector namely RefineFace to achieve high performance. Specifically, it consists of five modules: selective two-step regression (STR), selective two-step classification (STC), scale-aware margin loss (SML), feature supervision module (FSM) and receptive field enhancement (RFE). To enhance the regression ability for high location accuracy, STR coarsely adjusts locations and sizes of anchors from high level detection layers to provide better initialization for subsequent regressor. To improve the classification ability for high recall efficiency, STC first filters out most simple negatives from low level detection layers to reduce search space for subsequent classifier, then SML is applied to better distinguish faces from background at various scales and FSM is introduced to let the backbone learn more discriminative features for classification. Besides, RFE is presented to provide more diverse receptive field to better capture faces in some extreme poses. Extensive experiments conducted on WIDER FACE, AFW, PASCAL Face, FDDB, MAFA demonstrate that our method achieves state-of-the-art results and runs at 37.3 FPS with ResNet-18 for VGA-resolution images.},
  archive      = {J_TPAMI},
  author       = {Shifeng Zhang and Cheng Chi and Zhen Lei and Stan Z. Li},
  doi          = {10.1109/TPAMI.2020.2997456},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {4008-4020},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {RefineFace: Refinement neural network for high performance face detection},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Plane segmentation based on the optimal-vector-field in
LiDAR point clouds. <em>TPAMI</em>, <em>43</em>(11), 3991–4007. (<a
href="https://doi.org/10.1109/TPAMI.2020.2994935">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One key challenge in the point cloud segmentation is the detection and split of overlapping regions between different planes. The existing methods depend on the similarity and the dissimilarity in neighbor regions without a global constraint, which brings the ‘over-’ and ‘under-’ segmentation in the results. Hence, this paper presents a pipeline of the accurate plane segmentation for point clouds to address the shortcoming in the local optimization. There are two phases included in the proposed segmentation process. One is a local phase to calculate connectivity scores between different planes based on local variations of surface normals. In this phase, a new optimal-vector-field is formulated to detect the plane intersections. The optimal-vector-field is large in magnitude at plane intersections and vanishing at other regions. The other one is a global phase to smooth local segmentation cues to mimic leading eigenvector computation in the graph-cut. Evaluation of two datasets shows that the achieved precision and recall is 94.50 percent and 90.81 percent on the collected mobile LiDAR data and obtains an average accuracy of 75.4 percent on an open benchmark, which outperforms the state-of-the-art methods in terms of completeness and correctness.},
  archive      = {J_TPAMI},
  author       = {Sheng Xu and Ruisheng Wang and Hao Wang and Ruigang Yang},
  doi          = {10.1109/TPAMI.2020.2994935},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {3991-4007},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Plane segmentation based on the optimal-vector-field in LiDAR point clouds},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Norm-preservation: Why residual networks can become
extremely deep? <em>TPAMI</em>, <em>43</em>(11), 3980–3990. (<a
href="https://doi.org/10.1109/TPAMI.2020.2990339">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Augmenting neural networks with skip connections, as introduced in the so-called ResNet architecture, surprised the community by enabling the training of networks of more than 1,000 layers with significant performance gains. This paper deciphers ResNet by analyzing the effect of skip connections, and puts forward new theoretical results on the advantages of identity skip connections in neural networks. We prove that the skip connections in the residual blocks facilitate preserving the norm of the gradient, and lead to stable back-propagation, which is desirable from optimization perspective. We also show that, perhaps surprisingly, as more residual blocks are stacked, the norm-preservation of the network is enhanced. Our theoretical arguments are supported by extensive empirical evidence. Can we push for extra norm-preservation? We answer this question by proposing an efficient method to regularize the singular values of the convolution operator and making the ResNet’s transition layers extra norm-preserving. Our numerical investigations demonstrate that the learning dynamics and the classification performance of ResNet can be improved by making it even more norm preserving. Our results and the introduced modification for ResNet, referred to as Procrustes ResNets, can be used as a guide for training deeper networks and can also inspire new deeper architectures.},
  archive      = {J_TPAMI},
  author       = {Alireza Zaeemzadeh and Nazanin Rahnavard and Mubarak Shah},
  doi          = {10.1109/TPAMI.2020.2990339},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {3980-3990},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Norm-preservation: Why residual networks can become extremely deep?},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Normalizing flows: An introduction and review of current
methods. <em>TPAMI</em>, <em>43</em>(11), 3964–3979. (<a
href="https://doi.org/10.1109/TPAMI.2020.2992934">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Normalizing Flows are generative models which produce tractable distributions where both sampling and density evaluation can be efficient and exact. The goal of this survey article is to give a coherent and comprehensive review of the literature around the construction and use of Normalizing Flows for distribution learning. We aim to provide context and explanation of the models, review current state-of-the-art literature, and identify open questions and promising future directions.},
  archive      = {J_TPAMI},
  author       = {Ivan Kobyzev and Simon J.D. Prince and Marcus A. Brubaker},
  doi          = {10.1109/TPAMI.2020.2992934},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {3964-3979},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Normalizing flows: An introduction and review of current methods},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Mining interpretable AOG representations from convolutional
networks via active question answering. <em>TPAMI</em>, <em>43</em>(11),
3949–3963. (<a
href="https://doi.org/10.1109/TPAMI.2020.2993147">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present a method to mine object-part patterns from conv-layers of a pre-trained convolutional neural network (CNN). The mined object-part patterns are organized by an And-Or graph (AOG). This interpretable AOG representation consists of a four-layer semantic hierarchy, i.e., semantic parts, part templates, latent patterns, and neural units. The AOG associates each object part with certain neural units in feature maps of conv-layers. The AOG is constructed with very few annotations (e.g., 3–20) of object parts. We develop a question-answering (QA) method that uses active human-computer communications to mine patterns from a pre-trained CNN, in order to explain features in conv-layers incrementally. During the learning process, our QA method uses the current AOG for part localization. The QA method actively identifies objects, whose feature maps cannot be explained by the AOG. Then, our method asks people to annotate parts on the unexplained objects, and uses answers to discover CNN patterns corresponding to newly labeled parts. In this way, our method gradually grows new branches and refines existing branches on the AOG to semanticize CNN representations. In experiments, our method exhibited a high learning efficiency. Our method used about $1/6$ – $1/3$ of the part annotations for training, but achieved similar or better part-localization performance than fast-RCNN methods.},
  archive      = {J_TPAMI},
  author       = {Quanshi Zhang and Jie Ren and Ge Huang and Ruiming Cao and Ying Nian Wu and Song-Chun Zhu},
  doi          = {10.1109/TPAMI.2020.2993147},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {3949-3963},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Mining interpretable AOG representations from convolutional networks via active question answering},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Minimal solvers for rectifying from radially-distorted
conjugate translations. <em>TPAMI</em>, <em>43</em>(11), 3931–3948. (<a
href="https://doi.org/10.1109/TPAMI.2020.2992261">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces minimal solvers that jointly solve for radial lens undistortion and affine-rectification using local features extracted from the image of coplanar translated and reflected scene texture, which is common in man-made environments. The proposed solvers accommodate different types of local features and sampling strategies, and three of the proposed variants require just one feature correspondence. State-of-the-art techniques from algebraic geometry are used to simplify the formulation of the solvers. The generated solvers are stable, small and fast. Synthetic and real-image experiments show that the proposed solvers have superior robustness to noise compared to the state of the art. The solvers are integrated with an automated system for rectifying imaged scene planes from coplanar repeated texture. Accurate rectifications on challenging imagery taken with narrow to wide field-of-view lenses demonstrate the applicability of the proposed solvers.},
  archive      = {J_TPAMI},
  author       = {James Pritts and Zuzana Kukelova and Viktor Larsson and Yaroslava Lochman and Ondřej Chum},
  doi          = {10.1109/TPAMI.2020.2992261},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {3931-3948},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Minimal solvers for rectifying from radially-distorted conjugate translations},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Maximum density divergence for domain adaptation.
<em>TPAMI</em>, <em>43</em>(11), 3918–3930. (<a
href="https://doi.org/10.1109/TPAMI.2020.2991050">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised domain adaptation addresses the problem of transferring knowledge from a well-labeled source domain to an unlabeled target domain where the two domains have distinctive data distributions. Thus, the essence of domain adaptation is to mitigate the distribution divergence between the two domains. The state-of-the-art methods practice this very idea by either conducting adversarial training or minimizing a metric which defines the distribution gaps. In this paper, we propose a new domain adaptation method named adversarial tight match (ATM) which enjoys the benefits of both adversarial training and metric learning. Specifically, at first, we propose a novel distance loss, named maximum density divergence (MDD), to quantify the distribution divergence. MDD minimizes the inter-domain divergence (“match” in ATM) and maximizes the intra-class density (“tight” in ATM). Then, to address the equilibrium challenge issue in adversarial domain adaptation, we consider leveraging the proposed MDD into adversarial domain adaptation framework. At last, we tailor the proposed MDD as a practical learning loss and report our ATM. Both empirical evaluation and theoretical analysis are reported to verify the effectiveness of the proposed method. The experimental results on four benchmarks, both classical and large-scale, show that our method is able to achieve new state-of-the-art performance on most evaluations.},
  archive      = {J_TPAMI},
  author       = {Jingjing Li and Erpeng Chen and Zhengming Ding and Lei Zhu and Ke Lu and Heng Tao Shen},
  doi          = {10.1109/TPAMI.2020.2991050},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {3918-3930},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Maximum density divergence for domain adaptation},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Intrinsic grassmann averages for online linear, robust and
nonlinear subspace learning. <em>TPAMI</em>, <em>43</em>(11), 3904–3917.
(<a href="https://doi.org/10.1109/TPAMI.2020.2992392">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Principal component analysis (PCA) and Kernel principal component analysis (KPCA) are fundamental methods in machine learning for dimensionality reduction. The former is a technique for finding this approximation in finite dimensions and the latter is often in an infinite dimensional reproducing Kernel Hilbert-space (RKHS). In this paper, we present a geometric framework for computing the principal linear subspaces in both (finite and infinite) situations as well as for the robust PCA case, that amounts to computing the intrinsic average on the space of all subspaces: the Grassmann manifold. Points on this manifold are defined as the subspaces spanned by $K$ -tuples of observations. The intrinsic Grassmann average of these subspaces are shown to coincide with the principal components of the observations when they are drawn from a Gaussian distribution. We show similar results in the RKHS case and provide an efficient algorithm for computing the projection onto the this average subspace. The result is a method akin to KPCA which is substantially faster. Further, we present a novel online version of the KPCA using our geometric framework. Competitive performance of all our algorithms are demonstrated on a variety of real and synthetic data sets.},
  archive      = {J_TPAMI},
  author       = {Rudrasis Chakraborty and Liu Yang and Søren Hauberg and Baba C. Vemuri},
  doi          = {10.1109/TPAMI.2020.2992392},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {3904-3917},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Intrinsic grassmann averages for online linear, robust and nonlinear subspace learning},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Integrating multiple receptive fields through grouped active
convolution. <em>TPAMI</em>, <em>43</em>(11), 3892–3903. (<a
href="https://doi.org/10.1109/TPAMI.2020.2995864">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional networks have achieved great success in various vision tasks. This is mainly due to a considerable amount of research on network structure. In this study, instead of focusing on architectures, we focused on the convolution unit itself. The existing convolution unit has a fixed shape and is limited to observing restricted receptive fields. In earlier work, we proposed the active convolution unit (ACU), which can freely define its shape and learn by itself. In this paper, we provide a detailed analysis of the previously proposed unit and show that it is an efficient representation of a sparse weight convolution. Furthermore, we extend an ACU to a grouped ACU, which can observe multiple receptive fields in one layer. We found that the performance of a naive grouped convolution is degraded by increasing the number of groups; however, the proposed unit retains the accuracy even though the number of parameters decreases. Based on this result, we suggest a depthwise ACU (DACU), and various experiments have shown that our unit is efficient and can replace the existing convolutions.},
  archive      = {J_TPAMI},
  author       = {Yunho Jeon and Junmo Kim},
  doi          = {10.1109/TPAMI.2020.2995864},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {3892-3903},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Integrating multiple receptive fields through grouped active convolution},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Heterogeneous few-shot model rectification with semantic
mapping. <em>TPAMI</em>, <em>43</em>(11), 3878–3891. (<a
href="https://doi.org/10.1109/TPAMI.2020.2994749">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There still involve lots of challenges when applying machine learning algorithms in unknown environments, especially those with limited training data. To handle the data insufficiency and make a further step towards robust learning, we adopt the learnware notion Z.-H. Zhou, “Learnware: On the future of machine learning,” Front. Comput. Sci. , vol. 10, no. 4 pp. 589–590, 2016 which equips a model with an essential reusable property—the model learned in a related task could be easily adapted to the current data-scarce environment without data sharing . To this end, we propose the REctiFy via heterOgeneous pRedictor Mapping ( ReForm ) framework enabling the current model to take advantage of a related model from two kinds of heterogeneous environment , i.e., either with different sets of features or labels. By Encoding Meta InformaTion ( Emit ) of features and labels as the model specification, we utilize an optimal transported semantic mapping to characterize and bridge the environment changes. After fine-tuning over a few labeled examples through a biased regularization objective, the transformed heterogeneous model adapts to the current task efficiently. We apply ReForm over both synthetic and real-world tasks such as few-shot image classification with either learned or pre-defined specifications. Experimental results validate the effectiveness and practical utility of the proposed ReForm framework.},
  archive      = {J_TPAMI},
  author       = {Han-Jia Ye and De-Chuan Zhan and Yuan Jiang and Zhi-Hua Zhou},
  doi          = {10.1109/TPAMI.2020.2994749},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {3878-3891},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Heterogeneous few-shot model rectification with semantic mapping},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Extraction of an explanatory graph to interpret a CNN.
<em>TPAMI</em>, <em>43</em>(11), 3863–3877. (<a
href="https://doi.org/10.1109/TPAMI.2020.2992207">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces an explanatory graph representation to reveal object parts encoded inside convolutional layers of a CNN. Given a pre-trained CNN, each filter 1 in a conv-layer usually represents a mixture of object parts. We develop a simple yet effective method to learn an explanatory graph, which automatically disentangles object parts from each filter without any part annotations. Specifically, given the feature map of a filter, we mine neural activations from the feature map, which correspond to different object parts. The explanatory graph is constructed to organize each mined part as a graph node. Each edge connects two nodes, whose corresponding object parts usually co-activate and keep a stable spatial relationship. Experiments show that each graph node consistently represented the same object part through different images, which boosted the transferability of CNN features. The explanatory graph transferred features of object parts to the task of part localization, and our method significantly outperformed other approaches.},
  archive      = {J_TPAMI},
  author       = {Quanshi Zhang and Xin Wang and Ruiming Cao and Ying Nian Wu and Feng Shi and Song-Chun Zhu},
  doi          = {10.1109/TPAMI.2020.2992207},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {3863-3877},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Extraction of an explanatory graph to interpret a CNN},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). End-to-end learning for omnidirectional stereo matching with
uncertainty prior. <em>TPAMI</em>, <em>43</em>(11), 3850–3862. (<a
href="https://doi.org/10.1109/TPAMI.2020.2992497">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel end-to-end deep neural network model for omnidirectional depth estimation from a wide-baseline multi-view stereo setup. The images captured with ultra-wide field-of-view cameras on an omnidirectional rig are processed by the feature extraction module, and then the deep feature maps are warped onto the concentric spheres swept through all candidate depths using the calibrated camera parameters. The 3D encoder-decoder block takes the aligned feature volume to produce an omnidirectional depth estimate with regularization on uncertain regions utilizing the global context information. For more accurate depth estimation we also propose an uncertainty prior guidance in two ways: depth map filtering and guiding regularization. In addition, we present large-scale synthetic datasets for training and testing omnidirectional multi-view stereo algorithms. Our datasets consist of 13K ground-truth depth maps and 53K fisheye images in four orthogonal directions with various objects and environments. Experimental results show that the proposed method generates excellent results in both synthetic and real-world environments, and it outperforms the prior art and the omnidirectional versions of the state-of-the-art conventional stereo algorithms.},
  archive      = {J_TPAMI},
  author       = {Changhee Won and Jongbin Ryu and Jongwoo Lim},
  doi          = {10.1109/TPAMI.2020.2992497},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {3850-3862},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {End-to-end learning for omnidirectional stereo matching with uncertainty prior},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Decoding brain representations by multimodal learning of
neural activity and visual features. <em>TPAMI</em>, <em>43</em>(11),
3833–3849. (<a
href="https://doi.org/10.1109/TPAMI.2020.2995909">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work presents a novel method of exploring human brain-visual representations, with a view towards replicating these processes in machines. The core idea is to learn plausible computational and biological representations by correlating human neural activity and natural images. Thus, we first propose a model, EEG-ChannelNet , to learn a brain manifold for EEG classification. After verifying that visual information can be extracted from EEG data, we introduce a multimodal approach that uses deep image and EEG encoders, trained in a siamese configuration, for learning a joint manifold that maximizes a compatibility measure between visual features and brain representations. We then carry out image classification and saliency detection on the learned manifold. Performance analyses show that our approach satisfactorily decodes visual information from neural signals. This, in turn, can be used to effectively supervise the training of deep learning models, as demonstrated by the high performance of image classification and saliency detection on out-of-training classes. The obtained results show that the learned brain-visual features lead to improved performance and simultaneously bring deep models more in line with cognitive neuroscience work related to visual perception and attention.},
  archive      = {J_TPAMI},
  author       = {Simone Palazzo and Concetto Spampinato and Isaak Kavasidis and Daniela Giordano and Joseph Schmidt and Mubarak Shah},
  doi          = {10.1109/TPAMI.2020.2995909},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {3833-3849},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Decoding brain representations by multimodal learning of neural activity and visual features},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Contextual translation embedding for visual relationship
detection and scene graph generation. <em>TPAMI</em>, <em>43</em>(11),
3820–3832. (<a
href="https://doi.org/10.1109/TPAMI.2020.2992222">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Relations amongst entities play a central role in image understanding. Due to the complexity of modeling ( subject , predicate , object ) relation triplets, it is crucial to develop a method that can not only recognize seen relations, but also generalize to unseen cases. Inspired by a previously proposed visual translation embedding model, or VTransE [1] , we propose a context-augmented translation embedding model that can capture both common and rare relations. The previous VTransE model maps entities and predicates into a low-dimensional embedding vector space where the predicate is interpreted as a translation vector between the embedded features of the bounding box regions of the subject and the object . Our model additionally incorporates the contextual information captured by the bounding box of the union of the subject and the object, and learns the embeddings guided by the constraint predicate $\approx$ union ( subject , object ) $-$ subject $-$ object . In a comprehensive evaluation on multiple challenging benchmarks, our approach outperforms previous translation-based models and comes close to or exceeds the state of the art across a range of settings, from small-scale to large-scale datasets, from common to previously unseen relations. It also achieves promising results for the recently introduced task of scene graph generation.},
  archive      = {J_TPAMI},
  author       = {Zih-Siou Hung and Arun Mallya and Svetlana Lazebnik},
  doi          = {10.1109/TPAMI.2020.2992222},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {3820-3832},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Contextual translation embedding for visual relationship detection and scene graph generation},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Chart mining: A survey of methods for automated chart
analysis. <em>TPAMI</em>, <em>43</em>(11), 3799–3819. (<a
href="https://doi.org/10.1109/TPAMI.2020.2992028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Charts are useful communication tools for the presentation of data in a visually appealing format that facilitates comprehension. There have been many studies dedicated to chart mining, which refers to the process of automatic detection, extraction and analysis of charts to reproduce the tabular data that was originally used to create them. By allowing access to data which might not be available in other formats, chart mining facilitates the creation of many downstream applications. This paper presents a comprehensive survey of approaches across all components of the automated chart mining pipeline, such as (i) automated extraction of charts from documents; (ii) processing of multi-panel charts; (iii) automatic image classifiers to collect chart images at scale; (iv) automated extraction of data from each chart image, for popular chart types as well as selected specialized classes; (v) applications of chart mining; and (vi) datasets for training and evaluation, and the methods that were used to build them. Finally, we summarize the main trends found in the literature and provide pointers to areas for further research in chart mining.},
  archive      = {J_TPAMI},
  author       = {Kenny Davila and Srirangaraj Setlur and David Doermann and Bhargava Urala Kota and Venu Govindaraju},
  doi          = {10.1109/TPAMI.2020.2992028},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {3799-3819},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Chart mining: A survey of methods for automated chart analysis},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). AP-loss for accurate one-stage object detection.
<em>TPAMI</em>, <em>43</em>(11), 3782–3798. (<a
href="https://doi.org/10.1109/TPAMI.2020.2991457">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One-stage object detectors are trained by optimizing classification-loss and localization-loss simultaneously, with the former suffering much from extreme foreground-background class imbalance issue due to the large number of anchors. This paper alleviates this issue by proposing a novel framework to replace the classification task in one-stage detectors with a ranking task, and adopting the average-precision loss (AP-loss) for the ranking problem. Due to its non-differentiability and non-convexity, the AP-loss cannot be optimized directly. For this purpose, we develop a novel optimization algorithm, which seamlessly combines the error-driven update scheme in perceptron learning and backpropagation algorithm in deep networks. We provide in-depth analyses on the good convergence property and computational complexity of the proposed algorithm, both theoretically and empirically. Experimental results demonstrate notable improvement in addressing the imbalance issue in object detection over existing AP-based optimization algorithms. An improved state-of-the-art performance is achieved in one-stage detectors based on AP-loss over detectors using classification-losses on various standard benchmarks. The proposed framework is also highly versatile in accommodating different network architectures. Code is available at https://github.com/cccorn/AP-loss .},
  archive      = {J_TPAMI},
  author       = {Kean Chen and Weiyao Lin and Jianguo Li and John See and Ji Wang and Junni Zou},
  doi          = {10.1109/TPAMI.2020.2991457},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {3782-3798},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {AP-loss for accurate one-stage object detection},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Active image synthesis for efficient labeling.
<em>TPAMI</em>, <em>43</em>(11), 3770–3781. (<a
href="https://doi.org/10.1109/TPAMI.2020.2993221">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The great success achieved by deep neural networks attracts increasing attention from the manufacturing and healthcare communities. However, the limited availability of data and high costs of data collection are the major challenges for the applications in those fields. We propose in this work AISEL, an active image synthesis method for efficient labeling, to improve the performance of the small-data learning tasks. Specifically, a complementary AISEL dataset is generated, with labels actively acquired via a physics-based method to incorporate underlining physical knowledge at hand. An important component of our AISEL method is the bidirectional generative invertible network (GIN), which can extract interpretable features from the training images and generate physically meaningful virtual images. Our AISEL method then efficiently samples virtual images not only further exploits the uncertain regions but also explores the entire image space. We then discuss the interpretability of GIN both theoretically and experimentally, demonstrating clear visual improvements over the benchmarks. Finally, we demonstrate the effectiveness of our AISEL framework on aortic stenosis application, in which our method lowers the labeling cost by 90 percent while achieving a 15 percent improvement in prediction accuracy.},
  archive      = {J_TPAMI},
  author       = {Jialei Chen and Yujia Xie and Kan Wang and Chuck Zhang and Mani A. Vannan and Ben Wang and Zhen Qian},
  doi          = {10.1109/TPAMI.2020.2993221},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {3770-3781},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Active image synthesis for efficient labeling},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A new approach to robust estimation of parametric
structures. <em>TPAMI</em>, <em>43</em>(11), 3754–3769. (<a
href="https://doi.org/10.1109/TPAMI.2020.2994190">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most robust estimators require tuning the parameters of the algorithm for the particular application, a bottleneck for practical applications. The paper presents the multiple input structures with robust estimator (MISRE), where each structure, inlier or outlier, is processed independently. The same two constants are used to find the scale estimates over expansions for each structure. The inlier/outlier classification is straightforward since the data is processed and ordered with the relevant inlier structures listed first. If the inlier noises are similar, MISRE’s performance is equivalent to RANSAC-type algorithms. MISRE still returns the correct inlier estimates when inlier noises are very different, while RANSAC-type algorithms do not perform as well. MISRE’s failures are gradual when too many outliers are present, beginning with the least significant inlier structure. Examples from 2D images and 3D point clouds illustrate the estimation.},
  archive      = {J_TPAMI},
  author       = {Xiang Yang and Peter Meer and Jonathan Meer},
  doi          = {10.1109/TPAMI.2020.2994190},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {3754-3769},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {A new approach to robust estimation of parametric structures},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). 3D hand pose estimation using synthetic data and weakly
labeled RGB images. <em>TPAMI</em>, <em>43</em>(11), 3739–3753. (<a
href="https://doi.org/10.1109/TPAMI.2020.2993627">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compared with depth-based 3D hand pose estimation, it is more challenging to infer 3D hand pose from monocular RGB images, due to the substantial depth ambiguity and the difficulty of obtaining fully-annotated training data. Different from the existing learning-based monocular RGB-input approaches that require accurate 3D annotations for training, we propose to leverage the depth images that can be easily obtained from commodity RGB-D cameras during training, while during testing we take only RGB inputs for 3D joint predictions. In this way, we alleviate the burden of the costly 3D annotations in real-world dataset. Particularly, we propose a weakly-supervised method, adaptating from fully-annotated synthetic dataset to weakly-labeled real-world single RGB dataset with the aid of a depth regularizer, which serves as weak supervision for 3D pose prediction. To further exploit the physical structure of 3D hand pose, we present a novel CVAE-based statistical framework to embed the pose-specific subspace from RGB images, which can then be used to infer the 3D hand joint locations. Extensive experiments on benchmark datasets validate that our proposed approach outperforms baselines and state-of-the-art methods, which proves the effectiveness of the proposed depth regularizer and the CVAE-based framework.},
  archive      = {J_TPAMI},
  author       = {Yujun Cai and Liuhao Ge and Jianfei Cai and Nadia Magnenat Thalmann and Junsong Yuan},
  doi          = {10.1109/TPAMI.2020.2993627},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {11},
  pages        = {3739-3753},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {3D hand pose estimation using synthetic data and weakly labeled RGB images},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). [Front inside cover]. <em>TPAMI</em>, <em>43</em>(10), C4.
(<a href="https://doi.org/10.1109/TPAMI.2021.3105756">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presents a listing of the editorial board, board of governors, current staff, committee members, and/or society editors for this issue of the publication.},
  archive      = {J_TPAMI},
  doi          = {10.1109/TPAMI.2021.3105756},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {C4},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {[Front inside cover]},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The mutex watershed and its objective: Efficient,
parameter-free graph partitioning. <em>TPAMI</em>, <em>43</em>(10),
3724–3738. (<a
href="https://doi.org/10.1109/TPAMI.2020.2980827">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image partitioning, or segmentation without semantics, is the task of decomposing an image into distinct segments, or equivalently to detect closed contours. Most prior work either requires seeds, one per segment; or a threshold; or formulates the task as multicut / correlation clustering, an NP-hard problem. Here, we propose an efficient algorithm for graph partitioning, the “Mutex Watershed”. Unlike seeded watershed, the algorithm can accommodate not only attractive but also repulsive cues, allowing it to find a previously unspecified number of segments without the need for explicit seeds or a tunable threshold. We also prove that this simple algorithm solves to global optimality an objective function that is intimately related to the multicut / correlation clustering integer linear programming formulation. The algorithm is deterministic, very simple to implement, and has empirically linearithmic complexity. When presented with short-range attractive and long-range repulsive cues from a deep neural network, the Mutex Watershed gives the best results currently known for the competitive ISBI 2012 EM segmentation benchmark.},
  archive      = {J_TPAMI},
  author       = {Steffen Wolf and Alberto Bailoni and Constantin Pape and Nasim Rahaman and Anna Kreshuk and Ullrich Köthe and Fred A. Hamprecht},
  doi          = {10.1109/TPAMI.2020.2980827},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {3724-3738},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {The mutex watershed and its objective: Efficient, parameter-free graph partitioning},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). TE141K: Artistic text benchmark for text effect transfer.
<em>TPAMI</em>, <em>43</em>(10), 3709–3723. (<a
href="https://doi.org/10.1109/TPAMI.2020.2983697">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text effects are combinations of visual elements such as outlines, colors and textures of text, which can dramatically improve its artistry. Although text effects are extensively utilized in the design industry, they are usually created by human experts due to their extreme complexity; this is laborious and not practical for normal users. In recent years, some efforts have been made toward automatic text effect transfer; however, the lack of data limits the capabilities of transfer models. To address this problem, we introduce a new text effects dataset, TE141K, 1 1.Project page: https://daooshee.github.io/TE141K/. with 141,081 text effect/glyph pairs in total. Our dataset consists of 152 professionally designed text effects rendered on glyphs, including English letters, Chinese characters, and Arabic numerals. To the best of our knowledge, this is the largest dataset for text effect transfer to date. Based on this dataset, we propose a baseline approach called text effect transfer GAN (TET-GAN), which supports the transfer of all 152 styles in one model and can efficiently extend to new styles. Finally, we conduct a comprehensive comparison in which 14 style transfer models are benchmarked. Experimental results demonstrate the superiority of TET-GAN both qualitatively and quantitatively and indicate that our dataset is effective and challenging.},
  archive      = {J_TPAMI},
  author       = {Shuai Yang and Wenjing Wang and Jiaying Liu},
  doi          = {10.1109/TPAMI.2020.2983697},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {3709-3723},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {TE141K: Artistic text benchmark for text effect transfer},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Visibility-aware point-based multi-view stereo network.
<em>TPAMI</em>, <em>43</em>(10), 3695–3708. (<a
href="https://doi.org/10.1109/TPAMI.2020.2988729">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce VA-Point-MVSNet, a novel visibility-aware point-based deep framework for multi-view stereo (MVS). Distinct from existing cost volume approaches, our method directly processes the target scene as point clouds. More specifically, our method predicts the depth in a coarse-to-fine manner. We first generate a coarse depth map, convert it into a point cloud and refine the point cloud iteratively by estimating the residual between the depth of the current iteration and that of the ground truth. Our network leverages 3D geometry priors and 2D texture information jointly and effectively by fusing them into a feature-augmented point cloud, and processes the point cloud to estimate the 3D flow for each point. This point-based architecture allows higher accuracy, more computational efficiency and more flexibility than cost-volume-based counterparts. Furthermore, our visibility-aware multi-view feature aggregation allows the network to aggregate multi-view appearance cues while taking into account visibility. Experimental results show that our approach achieves a significant improvement in reconstruction quality compared with state-of-the-art methods on the DTU and the Tanks and Temples dataset. The code of VA-Point-MVSNet proposed in this work will be released at https://github.com/callmeray/PointMVSNet .},
  archive      = {J_TPAMI},
  author       = {Rui Chen and Songfang Han and Jing Xu and Hao Su},
  doi          = {10.1109/TPAMI.2020.2988729},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {3695-3708},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Visibility-aware point-based multi-view stereo network},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Supervision by registration and triangulation for landmark
detection. <em>TPAMI</em>, <em>43</em>(10), 3681–3694. (<a
href="https://doi.org/10.1109/TPAMI.2020.2983935">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present supervision by registration and triangulation (SRT), an unsupervised approach that utilizes unlabeled multi-view video to improve the accuracy and precision of landmark detectors. Being able to utilize unlabeled data enables our detectors to learn from massive amounts of unlabeled data freely available and not be limited by the quality and quantity of manual human annotations. To utilize unlabeled data, there are two key observations: (I) The detections of the same landmark in adjacent frames should be coherent with registration, i.e., optical flow. (II) The detections of the same landmark in multiple synchronized and geometrically calibrated views should correspond to a single 3D point, i.e., multi-view consistency. Registration and multi-view consistency are sources of supervision that do not require manual labeling, thus it can be leveraged to augment existing training data during detector training. End-to-end training is made possible by differentiable registration and 3D triangulation modules. Experiments with 11 datasets and a newly proposed metric to measure precision demonstrate accuracy and precision improvements in landmark detection on both images and video.},
  archive      = {J_TPAMI},
  author       = {Xuanyi Dong and Yi Yang and Shih-En Wei and Xinshuo Weng and Yaser Sheikh and Shoou-I Yu},
  doi          = {10.1109/TPAMI.2020.2983935},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {3681-3694},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Supervision by registration and triangulation for landmark detection},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Spherical kernel for efficient graph convolution on 3D point
clouds. <em>TPAMI</em>, <em>43</em>(10), 3664–3680. (<a
href="https://doi.org/10.1109/TPAMI.2020.2983410">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a spherical kernel for efficient graph convolution of 3D point clouds. Our metric-based kernels systematically quantize the local 3D space to identify distinctive geometric relationships in the data. Similar to the regular grid CNN kernels, the spherical kernel maintains translation-invariance and asymmetry properties, where the former guarantees weight sharing among similar local structures in the data and the latter facilitates fine geometric learning. The proposed kernel is applied to graph neural networks without edge-dependent filter generation, making it computationally attractive for large point clouds. In our graph networks, each vertex is associated with a single point location and edges connect the neighborhood points within a defined range. The graph gets coarsened in the network with farthest point sampling. Analogous to the standard CNNs, we define pooling and unpooling operations for our network. We demonstrate the effectiveness of the proposed spherical kernel with graph neural networks for point cloud classification and semantic segmentation using ModelNet, ShapeNet, RueMonge2014, ScanNet and S3DIS datasets. The source code and the trained models can be downloaded from https://github.com/hlei-ziyan/SPH3D-GCN.},
  archive      = {J_TPAMI},
  author       = {Huan Lei and Naveed Akhtar and Ajmal Mian},
  doi          = {10.1109/TPAMI.2020.2983410},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {3664-3680},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Spherical kernel for efficient graph convolution on 3D point clouds},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Unifying offline and online multi-graph matching via finding
shortest paths on supergraph. <em>TPAMI</em>, <em>43</em>(10),
3648–3663. (<a
href="https://doi.org/10.1109/TPAMI.2020.2989928">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the problem of multiple graph matching (MGM) by considering both offline batch mode and online setting. We explore the concept of cycle-consistency over pairwise matchings and formulate the problem as finding optimal composition path on the supergraph, whose vertices refer to graphs and edge weights denote score function regarding consistency and affinity. By our theoretical study we show that the offline and online MGM on supergraph can be converted to finding all pairwise shortest paths and single-source shortest paths respectively. We adopt the Floyd algorithm [1] and shortest path faster algorithm (SPFA) [2] , [3] to effectively find the optimal path. Extensive experimental results show our methods surpass state-of-the-art MGM methods, including CAO [4] , MISM [5] , IMGM [6] , and many other recent methods in offline and online settings. Source code will be made publicly available.},
  archive      = {J_TPAMI},
  author       = {Zetian Jiang and Tianzhe Wang and Junchi Yan},
  doi          = {10.1109/TPAMI.2020.2989928},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {3648-3663},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Unifying offline and online multi-graph matching via finding shortest paths on supergraph},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Show, match and segment: Joint weakly supervised learning of
semantic matching and object co-segmentation. <em>TPAMI</em>,
<em>43</em>(10), 3632–3647. (<a
href="https://doi.org/10.1109/TPAMI.2020.2985395">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present an approach for jointly matching and segmenting object instances of the same category within a collection of images. In contrast to existing algorithms that tackle the tasks of semantic matching and object co-segmentation in isolation, our method exploits the complementary nature of the two tasks. The key insights of our method are two-fold. First, the estimated dense correspondence fields from semantic matching provide supervision for object co-segmentation by enforcing consistency between the predicted masks from a pair of images. Second, the predicted object masks from object co-segmentation in turn allow us to reduce the adverse effects due to background clutters for improving semantic matching. Our model is end-to-end trainable and does not require supervision from manually annotated correspondences and object masks. We validate the efficacy of our approach on five benchmark datasets: TSS, Internet, PF-PASCAL, PF-WILLOW, and SPair-71k, and show that our algorithm performs favorably against the state-of-the-art methods on both semantic matching and object co-segmentation tasks.},
  archive      = {J_TPAMI},
  author       = {Yun-Chun Chen and Yen-Yu Lin and Ming-Hsuan Yang and Jia-Bin Huang},
  doi          = {10.1109/TPAMI.2020.2985395},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {3632-3647},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Show, match and segment: Joint weakly supervised learning of semantic matching and object co-segmentation},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Recent advances in open set recognition: A survey.
<em>TPAMI</em>, <em>43</em>(10), 3614–3631. (<a
href="https://doi.org/10.1109/TPAMI.2020.2981604">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In real-world recognition/classification tasks, limited by various objective factors, it is usually difficult to collect training samples to exhaust all classes when training a recognizer or classifier. A more realistic scenario is open set recognition (OSR), where incomplete knowledge of the world exists at training time, and unknown classes can be submitted to an algorithm during testing, requiring the classifiers to not only accurately classify the seen classes, but also effectively deal with unseen ones. This paper provides a comprehensive survey of existing open set recognition techniques covering various aspects ranging from related definitions, representations of models, datasets, evaluation criteria, and algorithm comparisons. Furthermore, we briefly analyze the relationships between OSR and its related tasks including zero-shot, one-shot (few-shot) recognition/learning techniques, classification with reject option, and so forth. Additionally, we also review the open world recognition which can be seen as a natural extension of OSR. Importantly, we highlight the limitations of existing approaches and point out some promising subsequent research directions in this field.},
  archive      = {J_TPAMI},
  author       = {Chuanxing Geng and Sheng-Jun Huang and Songcan Chen},
  doi          = {10.1109/TPAMI.2020.2981604},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {3614-3631},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Recent advances in open set recognition: A survey},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Pixel2Mesh: 3D mesh model generation via image guided
deformation. <em>TPAMI</em>, <em>43</em>(10), 3600–3613. (<a
href="https://doi.org/10.1109/TPAMI.2020.2984232">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose an end-to-end deep learning architecture that generates 3D triangular meshes from single color images. Restricted by the nature of prevalent deep learning techniques, the majority of previous works represent 3D shapes in volumes or point clouds. However, it is non-trivial to convert these representations to compact and ready-to-use mesh models. Unlike the existing methods, our network represents 3D shapes in meshes, which are essentially graphs and well suited for graph-based convolutional neural networks. Leveraging perceptual features extracted from an input image, our network produces the correct geometry by progressively deforming an ellipsoid. To make the whole deformation procedure stable, we adopt a coarse-to-fine strategy, and define various mesh/surface related losses to capture properties of various aspects, which benefits producing the visually appealing and physically accurate 3D geometry. In addition, our model by nature can be adapted to objects in specific domains, e.g., human faces, and be easily extended to learn per-vertex properties, e.g., color. Extensive experiments show that our method not only qualitatively produces the mesh model with better details, but also achieves the higher 3D shape estimation accuracy compared against the state-of-the-arts.},
  archive      = {J_TPAMI},
  author       = {Nanyang Wang and Yinda Zhang and Zhuwen Li and Yanwei Fu and Hang Yu and Wei Liu and Xiangyang Xue and Yu-Gang Jiang},
  doi          = {10.1109/TPAMI.2020.2984232},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {3600-3613},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Pixel2Mesh: 3D mesh model generation via image guided deformation},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Partial multi-label learning via credible label elicitation.
<em>TPAMI</em>, <em>43</em>(10), 3587–3599. (<a
href="https://doi.org/10.1109/TPAMI.2020.2985210">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Partial multi-label learning (PML) deals with the problem where each training example is associated with an overcomplete set of candidate labels, among which only some candidate labels are valid. The task of PML naturally arises in learning scenarios with inaccurate supervision, and the goal is to induce a multi-label predictor which can assign a set of proper labels for unseen instance. The PML training procedure is prone to be misled by false positive labels concealed in the candidate label set, which serves as the major modeling difficulty for partial multi-label learning. In this paper, a novel two-stage PML approach is proposed which works by eliciting credible labels from the candidate label set for model induction. In the first stage, the labeling confidence of candidate label for each PML training example is estimated via iterative label propagation. In the second stage, by utilizing credible labels with high labeling confidence, multi-label predictor is induced via pairwise label ranking coupled with virtual label splitting or maximum a posteriori (MAP) reasoning. Experimental studies show that the proposed approach can achieve highly competitive generalization performance by excluding most false positive labels from the training procedure via credible label elicitation.},
  archive      = {J_TPAMI},
  author       = {Min-Ling Zhang and Jun-Peng Fang},
  doi          = {10.1109/TPAMI.2020.2985210},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {3587-3599},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Partial multi-label learning via credible label elicitation},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multiview feature selection for single-view classification.
<em>TPAMI</em>, <em>43</em>(10), 3573–3586. (<a
href="https://doi.org/10.1109/TPAMI.2020.2987013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many real-world scenarios, data from multiple modalities (sources) are collected during a development phase. Such data are referred to as multiview data. While additional information from multiple views often improves the performance, collecting data from such additional views during the testing phase may not be desired due to the high costs associated with measuring such views or, unavailability of such additional views. Therefore, in many applications, despite having a multiview training data set, it is desired to do performance testing using data from only one view. In this paper, we present a multiview feature selection method that leverages the knowledge of all views and use it to guide the feature selection process in an individual view. We realize this via a multiview feature weighting scheme such that the local margins of samples in each view are maximized and similarities of samples to some reference points in different views are preserved. Also, the proposed formulation can be used for cross-view matching when the view-specific feature weights are pre-computed on an auxiliary data set. Promising results have been achieved on nine real-world data sets as well as three biometric recognition applications. On average, the proposed feature selection method has improved the classification error rate by 31 percent of the error rate of the state-of-the-art.},
  archive      = {J_TPAMI},
  author       = {Majid Komeili and Narges Armanfard and Dimitrios Hatzinakos},
  doi          = {10.1109/TPAMI.2020.2987013},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {3573-3586},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Multiview feature selection for single-view classification},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Optimizing regularized cholesky score for order-based
learning of bayesian networks. <em>TPAMI</em>, <em>43</em>(10),
3555–3572. (<a
href="https://doi.org/10.1109/TPAMI.2020.2990820">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian networks are a class of popular graphical models that encode causal and conditional independence relations among variables by directed acyclic graphs (DAGs). We propose a novel structure learning method, annealing on regularized Cholesky score (ARCS), to search over topological sorts, or permutations of nodes, for a high-scoring Bayesian network. Our scoring function is derived from regularizing Gaussian DAG likelihood, and its optimization gives an alternative formulation of the sparse Cholesky factorization problem from a statistical viewpoint. We combine simulated annealing over permutation space with a fast proximal gradient algorithm, operating on triangular matrices of edge coefficients, to compute the score of any permutation. Combined, the two approaches allow us to quickly and effectively search over the space of DAGs without the need to verify the acyclicity constraint or to enumerate possible parent sets given a candidate topological sort. The annealing aspect of the optimization is able to consistently improve the accuracy of DAGs learned by greedy and deterministic search algorithms. In addition, we develop several techniques to facilitate the structure learning, including pre-annealing data-driven tuning parameter selection and post-annealing constraint-based structure refinement. Through extensive numerical comparisons, we show that ARCS outperformed existing methods by a substantial margin, demonstrating its great advantage in structure learning of Bayesian networks from both observational and experimental data. We also establish the consistency of our scoring function in estimating topological sorts and DAG structures in the large-sample limit. Source code of ARCS is available at https://github.com/yeqiaoling/arcs_bn .},
  archive      = {J_TPAMI},
  author       = {Qiaoling Ye and Arash A. Amini and Qing Zhou},
  doi          = {10.1109/TPAMI.2020.2990820},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {3555-3572},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Optimizing regularized cholesky score for order-based learning of bayesian networks},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multilinear modelling of faces and expressions.
<em>TPAMI</em>, <em>43</em>(10), 3540–3554. (<a
href="https://doi.org/10.1109/TPAMI.2020.2986496">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we present a new versatile 3D multilinear statistical face model, based on a tensor factorisation of 3D face scans, that decomposes the shapes into person and expression subspaces. Investigation of the expression subspace reveals an inherent low-dimensional substructure, and further, a star-shaped structure. This is due to two novel findings. (1) Increasing the strength of one emotion approximately forms a linear trajectory in the subspace. (2) All these trajectories intersect at a single point – not at the neutral expression as assumed by almost all prior works—but at an apathetic expression. We utilise these structural findings by reparameterising the expression subspace by the fourth-order moment tensor centred at the point of apathy. We propose a 3D face reconstruction method from single or multiple 2D projections by assuming an uncalibrated projective camera model. The non-linearity caused by the perspective projection can be neatly included into the model. The proposed algorithm separates person and expression subspaces convincingly, and enables flexible, natural modelling of expressions for a wide variety of human faces. Applying the method on independent faces showed that morphing between different persons and expressions can be performed without strong deformations.},
  archive      = {J_TPAMI},
  author       = {Stella Grasshof and Hanno Ackermann and Sami Sebastian Brandt and Jörn Ostermann},
  doi          = {10.1109/TPAMI.2020.2986496},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {3540-3554},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Multilinear modelling of faces and expressions},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Model study of transient imaging with multi-frequency
time-of-flight sensors. <em>TPAMI</em>, <em>43</em>(10), 3523–3539. (<a
href="https://doi.org/10.1109/TPAMI.2020.2981574">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an emerging imaging modality, transient imaging that records the transient information of light transport has significantly shaped our understanding of scenes. In spite of the great progress made in computer vision and optical imaging fields, commonly used multi-frequency time-of-flight (ToF) sensors are still afflicted with the band-limited modulation frequency and long acquisition process. To overcome such barriers, more effective image-formation schemes and reconstruction algorithms are highly desired. In this paper, we propose a compressive transient imaging model, without any priori knowledge, by constructing a near-tight-frame based representation of the ToF imaging principle. We prove that the compressibility of sensor measurements can be presented in the Fourier domain and held in the frame, and the ToF measurements possess multi-scale characteristics. Solving the inverse problems in transient imaging with our proposed model consists of two major steps, including a compressed-sensing-based approach for full measurement recovery, which essentially reduces the capture time, and a wavelet-based transient image reconstruction framework, which realizes adaptive transient image reconstruction and achieves highly accurate reconstruction results. The compressive transient imaging model is suitable for various existing multi-frequency ToF sensors and requires no hardware modifications. Experimental results using synthetic and real online datasets demonstrate its promising performance.},
  archive      = {J_TPAMI},
  author       = {Hongman Wang and Hui Qiao and Jingyu Lin and Rihui Wu and Yebin Liu and Qionghai Dai},
  doi          = {10.1109/TPAMI.2020.2981574},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {3523-3539},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Model study of transient imaging with multi-frequency time-of-flight sensors},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Minimizing negative transfer of knowledge in multivariate
gaussian processes: A scalable and regularized approach. <em>TPAMI</em>,
<em>43</em>(10), 3508–3522. (<a
href="https://doi.org/10.1109/TPAMI.2020.2987482">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently there has been an increasing interest in the multivariate Gaussian process (MGP) which extends the Gaussian process (GP) to deal with multiple outputs. One approach to construct the MGP and account for non-trivial commonalities amongst outputs employs a convolution process (CP). The CP is based on the idea of sharing latent functions across several convolutions. Despite the elegance of the CP construction, it provides new challenges that need yet to be tackled. First, even with a moderate number of outputs, model building is extremely prohibitive due to the huge increase in computational demands and number of parameters to be estimated. Second, the negative transfer of knowledge may occur when some outputs do not share commonalities. In this paper we address these issues. We propose a regularized pairwise modeling approach for the MGP established using CP. The key feature of our approach is to distribute the estimation of the full multivariate model into a group of bivariate GPs which are individually built. Interestingly pairwise modeling turns out to possess unique characteristics, which allows us to tackle the challenge of negative transfer through penalizing the latent function that facilitates information sharing in each bivariate model. Predictions are then made through combining predictions from the bivariate models within a Bayesian framework. The proposed method has excellent scalability when the number of outputs is large and minimizes the negative transfer of knowledge between uncorrelated outputs. Statistical guarantees for the proposed method are studied and its advantageous features are demonstrated through numerical studies.},
  archive      = {J_TPAMI},
  author       = {Raed Kontar and Garvesh Raskutti and Shiyu Zhou},
  doi          = {10.1109/TPAMI.2020.2987482},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {3508-3522},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Minimizing negative transfer of knowledge in multivariate gaussian processes: A scalable and regularized approach},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Low-tubal-rank plus sparse tensor recovery with prior
subspace information. <em>TPAMI</em>, <em>43</em>(10), 3492–3507. (<a
href="https://doi.org/10.1109/TPAMI.2020.2986773">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tensor principal component pursuit (TPCP) is a powerful approach in the tensor robust principal component analysis (TRPCA), where the goal is to decompose a data tensor to a low-tubal-rank part plus a sparse residual. TPCP is shown to be effective under certain tensor incoherence conditions, which can be restrictive in practice. In this paper, we propose a Modified-TPCP, which incorporates the prior subspace information in the analysis. With the aid of prior info, the proposed method is able to recover the low-tubal-rank and the sparse components under a significantly weaker incoherence assumption. We further design an efficient algorithm to implement Modified-TPCP based upon the alternating direction method of multipliers (ADMM). The promising performance of the proposed method is supported by simulations and real data applications.},
  archive      = {J_TPAMI},
  author       = {Feng Zhang and Jianjun Wang and Wendong Wang and Chen Xu},
  doi          = {10.1109/TPAMI.2020.2986773},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {3492-3507},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Low-tubal-rank plus sparse tensor recovery with prior subspace information},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning to model relationships for zero-shot video
classification. <em>TPAMI</em>, <em>43</em>(10), 3476–3491. (<a
href="https://doi.org/10.1109/TPAMI.2020.2985708">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the explosive growth of video categories, zero-shot learning (ZSL) in video classification has become a promising research direction in pattern analysis and machine learning. Based on some auxiliary information such as word embeddings and attributes, the key to a robust ZSL method is to transfer the learned knowledge from seen classes to unseen classes, which requires relationship modeling between these concepts (e.g., categories and attributes). However, most existing approaches ignore to model the explicit relationships in an end-to-end manner, resulting in low effectiveness of knowledge transfer. To tackle this problem, we reconsider the video ZSL task as a task-driven message passing process to jointly enjoy several merits including alleviated heterogeneity gap, low domain shift, and robust temporal modeling. Specifically, we propose a prototype-sample GNN (PS-GNN) consisting of a prototype branch and a sample branch to directly and adaptively model all the relationships between category-attribute, category-category, and attribute-attribute. The prototype branch aims to learn robust representations of video categories, which takes as input a set of word-embedding vectors corresponding to the concepts. The sample branch is designed to generate features of a video sample by leveraging its object semantics. With the co-adaption and cooperation between both branches, a unified and robust ZSL framework is achieved. Extensive experiments strongly evidence that PS-GNN obtains favorable performance on five popular video benchmarks consistently.},
  archive      = {J_TPAMI},
  author       = {Junyu Gao and Tianzhu Zhang and Changsheng Xu},
  doi          = {10.1109/TPAMI.2020.2985708},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {3476-3491},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Learning to model relationships for zero-shot video classification},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Line drawings for face portraits from photos using global
and local structure based GANs. <em>TPAMI</em>, <em>43</em>(10),
3462–3475. (<a
href="https://doi.org/10.1109/TPAMI.2020.2987931">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite significant effort and notable success of neural style transfer, it remains challenging for highly abstract styles, in particular line drawings. In this paper, we propose APDrawingGAN++, a generative adversarial network (GAN) for transforming face photos to artistic portrait drawings (APDrawings), which addresses substantial challenges including highly abstract style, different drawing techniques for different facial features, and high perceptual sensitivity to artifacts. To address these, we propose a composite GAN architecture that consists of local networks (to learn effective representations for specific facial features) and a global network (to capture the overall content). We provide a theoretical explanation for the necessity of this composite GAN structure by proving that any GAN with a single generator cannot generate artistic styles like APDrawings. We further introduce a classification-and-synthesis approach for lips and hair where different drawing styles are used by artists, which applies suitable styles for a given input. To capture the highly abstract art form inherent in APDrawings, we address two challenging operations-(1) coping with lines with small misalignments while penalizing large discrepancy and (2) generating more continuous lines-by introducing two novel loss terms: one is a novel distance transform loss with nonlinear mapping and the other is a novel line continuity loss, both of which improve the line quality. We also develop dedicated data augmentation and pre-training to further improve results. Extensive experiments, including a user study, show that our method outperforms state-of-the-art methods, both qualitatively and quantitatively.},
  archive      = {J_TPAMI},
  author       = {Ran Yi and Mengfei Xia and Yong-Jin Liu and Yu-Kun Lai and Paul L. Rosin},
  doi          = {10.1109/TPAMI.2020.2987931},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {3462-3475},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Line drawings for face portraits from photos using global and local structure based GANs},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning content-weighted deep image compression.
<em>TPAMI</em>, <em>43</em>(10), 3446–3461. (<a
href="https://doi.org/10.1109/TPAMI.2020.2983926">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning-based lossy image compression usually involves the joint optimization of rate-distortion performance, and requires to cope with the spatial variation of image content and contextual dependence among learned codes. Traditional entropy models can spatially adapt the local bit rate based on the image content, but usually are limited in exploiting context in code space. On the other hand, most deep context models are computationally very expensive and cannot efficiently perform decoding over the symbols in parallel. In this paper, we present a content-weighted encoder-decoder model, where the channel-wise multi-valued quantization is deployed for the discretization of the encoder features, and an importance map subnet is introduced to generate the importance masks for spatially varying code pruning. Consequently, the summation of importance masks can serve as an upper bound of the length of bitstream. Furthermore, the quantized representations of the learned code and importance map are still spatially dependent, which can be losslessly compressed using arithmetic coding. To compress the codes effectively and efficiently, we propose an upper-triangular masked convolutional network (triuMCN) for large context modeling. Experiments show that the proposed method can produce visually much better results, and performs favorably against deep and traditional lossy image compression approaches.},
  archive      = {J_TPAMI},
  author       = {Mu Li and Wangmeng Zuo and Shuhang Gu and Jane You and David Zhang},
  doi          = {10.1109/TPAMI.2020.2983926},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {3446-3461},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Learning content-weighted deep image compression},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning channel-wise interactions for binary convolutional
neural networks. <em>TPAMI</em>, <em>43</em>(10), 3432–3445. (<a
href="https://doi.org/10.1109/TPAMI.2020.2988262">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a channel-wise interaction based binary convolutional neural networks (CI-BCNN) approach for efficient inference. Conventional binary convolutional neural networks usually apply the xnor and bitcount operations in the binary convolution with notable quantization errors, which obtain opposite signs of pixels in binary feature maps compared to their full-precision counterparts and lead to significant information loss. In our proposed CI-BCNN method, we exploit the channel-wise interactions with the prior knowledge which aims to alleviate inconsistency of signs in binary feature maps and preserves the information of input samples during inference. Specifically, we mine the channel-wise interactions by using a reinforcement learning model, and impose channel-wise priors on the intermediate feature maps to correct inconsistent signs through the interacted bitcount. Since CI-BCNN mines the channel-wise interactions in a large search space where each channel may correlate with others, the search deficiency caused by sparse interactions obstacles the agent to obtain the optimal policy. To address this, we further present a hierarchical channel-wise interaction based binary convolutional neural networks (HCI-BCNN) method to shrink the search space via hierarchical reinforcement learning. Moreover, we propose a denoising interacted bitcount operation in binary convolution by smoothing the channel-wise interactions, so that noise in channel-wise priors can be alleviated. Extensive experimental results on the CIFAR-10 and ImageNet datasets demonstrate the effectiveness of the proposed CI-BCNN and HCI-BCNN.},
  archive      = {J_TPAMI},
  author       = {Ziwei Wang and Jiwen Lu and Jie Zhou},
  doi          = {10.1109/TPAMI.2020.2988262},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {3432-3445},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Learning channel-wise interactions for binary convolutional neural networks},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Interpretable CNNs for object classification.
<em>TPAMI</em>, <em>43</em>(10), 3416–3431. (<a
href="https://doi.org/10.1109/TPAMI.2020.2982882">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a generic method to learn interpretable convolutional filters in a deep convolutional neural network (CNN) for object classification, where each interpretable filter encodes features of a specific object part. Our method does not require additional annotations of object parts or textures for supervision. Instead, we use the same training data as traditional CNNs. Our method automatically assigns each interpretable filter in a high conv-layer with an object part of a certain category during the learning process. Such explicit knowledge representations in conv-layers of the CNN help people clarify the logic encoded in the CNN, i.e., answering what patterns the CNN extracts from an input image and uses for prediction. We have tested our method using different benchmark CNNs with various architectures to demonstrate the broad applicability of our method. Experiments have shown that our interpretable filters are much more semantically meaningful than traditional filters.},
  archive      = {J_TPAMI},
  author       = {Quanshi Zhang and Xin Wang and Ying Nian Wu and Huilin Zhou and Song-Chun Zhu},
  doi          = {10.1109/TPAMI.2020.2982882},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {3416-3431},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Interpretable CNNs for object classification},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Imbalance problems in object detection: A review.
<em>TPAMI</em>, <em>43</em>(10), 3388–3415. (<a
href="https://doi.org/10.1109/TPAMI.2020.2981890">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present a comprehensive review of the imbalance problems in object detection. To analyze the problems in a systematic manner, we introduce a problem-based taxonomy. Following this taxonomy, we discuss each problem in depth and present a unifying yet critical perspective on the solutions in the literature. In addition, we identify major open issues regarding the existing imbalance problems as well as imbalance problems that have not been discussed before. Moreover, in order to keep our review up to date, we provide an accompanying webpage which catalogs papers addressing imbalance problems, according to our problem-based taxonomy. Researchers can track newer studies on this webpage available at: https://github.com/kemaloksuz/ObjectDetectionImbalance.},
  archive      = {J_TPAMI},
  author       = {Kemal Oksuz and Baris Can Cam and Sinan Kalkan and Emre Akbas},
  doi          = {10.1109/TPAMI.2020.2981890},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {3388-3415},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Imbalance problems in object detection: A review},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep learning for image super-resolution: A survey.
<em>TPAMI</em>, <em>43</em>(10), 3365–3387. (<a
href="https://doi.org/10.1109/TPAMI.2020.2982166">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image Super-Resolution (SR) is an important class of image processing techniqueso enhance the resolution of images and videos in computer vision. Recent years have witnessed remarkable progress of image super-resolution using deep learning techniques. This article aims to provide a comprehensive survey on recent advances of image super-resolution using deep learning approaches. In general, we can roughly group the existing studies of SR techniques into three major categories: supervised SR, unsupervised SR, and domain-specific SR. In addition, we also cover some other important issues, such as publicly available benchmark datasets and performance evaluation metrics. Finally, we conclude this survey by highlighting several future directions and open issues which should be further addressed by the community in the future.},
  archive      = {J_TPAMI},
  author       = {Zhihao Wang and Jian Chen and Steven C. H. Hoi},
  doi          = {10.1109/TPAMI.2020.2982166},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {3365-3387},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Deep learning for image super-resolution: A survey},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep high-resolution representation learning for visual
recognition. <em>TPAMI</em>, <em>43</em>(10), 3349–3364. (<a
href="https://doi.org/10.1109/TPAMI.2020.2983686">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-resolution representations are essential for position-sensitive vision problems, such as human pose estimation, semantic segmentation, and object detection. Existing state-of-the-art frameworks first encode the input image as a low-resolution representation through a subnetwork that is formed by connecting high-to-low resolution convolutions in series (e.g., ResNet, VGGNet), and then recover the high-resolution representation from the encoded low-resolution representation. Instead, our proposed network, named as High-Resolution Network (HRNet), maintains high-resolution representations through the whole process. There are two key characteristics: (i) Connect the high-to-low resolution convolution streams in parallel and (ii) repeatedly exchange the information across resolutions. The benefit is that the resulting representation is semantically richer and spatially more precise. We show the superiority of the proposed HRNet in a wide range of applications, including human pose estimation, semantic segmentation, and object detection, suggesting that the HRNet is a stronger backbone for computer vision problems. All the codes are available at https://github.com/HRNet .},
  archive      = {J_TPAMI},
  author       = {Jingdong Wang and Ke Sun and Tianheng Cheng and Borui Jiang and Chaorui Deng and Yang Zhao and Dong Liu and Yadong Mu and Mingkui Tan and Xinggang Wang and Wenyu Liu and Bin Xiao},
  doi          = {10.1109/TPAMI.2020.2983686},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {3349-3364},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Deep high-resolution representation learning for visual recognition},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep convolutional neural network for multi-modal image
restoration and fusion. <em>TPAMI</em>, <em>43</em>(10), 3333–3348. (<a
href="https://doi.org/10.1109/TPAMI.2020.2984244">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel deep convolutional neural network to solve the general multi-modal image restoration (MIR) and multi-modal image fusion (MIF) problems. Different from other methods based on deep learning, our network architecture is designed by drawing inspirations from a new proposed multi-modal convolutional sparse coding (MCSC) model. The key feature of the proposed network is that it can automatically split the common information shared among different modalities, from the unique information that belongs to each single modality, and is therefore denoted with CU-Net, i.e., common and unique information splitting network. Specifically, the CU-Net is composed of three modules, i.e., the unique feature extraction module (UFEM), common feature preservation module (CFPM), and image reconstruction module (IRM). The architecture of each module is derived from the corresponding part in the MCSC model, which consists of several learned convolutional sparse coding (LCSC) blocks. Extensive numerical results verify the effectiveness of our method on a variety of MIR and MIF tasks, including RGB guided depth image super-resolution, flash guided non-flash image denoising, multi-focus and multi-exposure image fusion.},
  archive      = {J_TPAMI},
  author       = {Xin Deng and Pier Luigi Dragotti},
  doi          = {10.1109/TPAMI.2020.2984244},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {3333-3348},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Deep convolutional neural network for multi-modal image restoration and fusion},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Camera pose estimation using first-order curve differential
geometry. <em>TPAMI</em>, <em>43</em>(10), 3321–3332. (<a
href="https://doi.org/10.1109/TPAMI.2020.2985310">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper considers and solves the problem of estimating camera pose given a pair of point-tangent correspondences between a 3D scene and a projected image. The problem arises when considering curve geometry as the basis of forming correspondences, computation of structure and calibration, which in its simplest form is a point augmented with the curve tangent. We show that while the resectioning problem is solved with a minimum of three points given the intrinsic parameters, when points are augmented with tangent information only two points are required, leading to substantial robustness and computational savings, e.g., as a minimal engine within ransac. In addition, algorithms are developed to find a practical solution shown to effectively recover camera pose using synthetic and real datasets. This technology is intended as a building block of curve-based structure from motion systems, allowing new views to be incrementally registered to a core set of views for which relative pose has been computed.},
  archive      = {J_TPAMI},
  author       = {Ricardo Fabbri and Peter Giblin and Benjamin Kimia},
  doi          = {10.1109/TPAMI.2020.2985310},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {3321-3332},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Camera pose estimation using first-order curve differential geometry},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adversarial attacks on time series. <em>TPAMI</em>,
<em>43</em>(10), 3309–3320. (<a
href="https://doi.org/10.1109/TPAMI.2020.2986319">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time series classification models have been garnering significant importance in the research community. However, not much research has been done on generating adversarial samples for these models. These adversarial samples can become a security concern. In this paper, we propose utilizing an adversarial transformation network (ATN) on a distilled model to attack various time series classification models. The proposed attack on the classification model utilizes a distilled model as a surrogate that mimics the behavior of the attacked classical time series classification models. Our proposed methodology is applied onto 1-nearest neighbor dynamic time warping (1-NN DTW) and a fully convolutional network (FCN), all of which are trained on 42 University of California Riverside (UCR) datasets. In this paper, we show both models were susceptible to attacks on all 42 datasets. When compared to Fast Gradient Sign Method, the proposed attack generates a larger faction of successful adversarial black-box attacks. A simple defense mechanism is successfully devised to reduce the fraction of successful adversarial samples. Finally, we recommend future researchers that develop time series classification models to incorporating adversarial data samples into their training data sets to improve resilience on adversarial samples.},
  archive      = {J_TPAMI},
  author       = {Fazle Karim and Somshubra Majumdar and Houshang Darabi},
  doi          = {10.1109/TPAMI.2020.2986319},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {3309-3320},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Adversarial attacks on time series},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An end-to-end learning framework for video compression.
<em>TPAMI</em>, <em>43</em>(10), 3292–3308. (<a
href="https://doi.org/10.1109/TPAMI.2020.2988453">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional video compression approaches build upon the hybrid coding framework with motion-compensated prediction and residual transform coding. In this paper, we propose the first end-to-end deep video compression framework to take advantage of both the classical compression architecture and the powerful non-linear representation ability of neural networks. Our framework employs pixel-wise motion information, which is learned from an optical flow network and further compressed by an auto-encoder network to save bits. The other compression components are also implemented by the well-designed networks for high efficiency. All the modules are jointly optimized by using the rate-distortion trade-off and can collaborate with each other. More importantly, the proposed deep video compression framework is very flexible and can be easily extended by using lightweight or advanced networks for higher speed or better efficiency. We also propose to introduce the adaptive quantization layer to reduce the number of parameters for variable bitrate coding. Comprehensive experimental results demonstrate the effectiveness of the proposed framework on the benchmark datasets.},
  archive      = {J_TPAMI},
  author       = {Guo Lu and Xiaoyun Zhang and Wanli Ouyang and Li Chen and Zhiyong Gao and Dong Xu},
  doi          = {10.1109/TPAMI.2020.2988453},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {3292-3308},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {An end-to-end learning framework for video compression},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A dual camera system for high spatiotemporal resolution
video acquisition. <em>TPAMI</em>, <em>43</em>(10), 3275–3291. (<a
href="https://doi.org/10.1109/TPAMI.2020.2983371">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a dual camera system for high spatiotemporal resolution (HSTR) video acquisition, where one camera shoots a video with high spatial resolution and low frame rate (HSR-LFR) and another one captures a low spatial resolution and high frame rate (LSR-HFR) video. Our main goal is to combine videos from LSR-HFR and HSR-LFR cameras to create an HSTR video. We propose an end-to-end learning framework, AWnet, mainly consisting of a FlowNet and a FusionNet that learn an adaptive weighting function in pixel domain to combine inputs in a frame recurrent fashion. To improve the reconstruction quality for cameras used in reality, we also introduce noise regularization under the same framework. Our method has demonstrated noticeable performance gains in terms of both objective PSNR measurement in simulation with different publicly available video and light-field datasets and subjective evaluation with real data captured by dual iPhone 7 and Grasshopper3 cameras. Ablation studies are further conducted to investigate and explore various aspects, such as reference structure, camera parallax, exposure time, etc) of our system to fully understand its capability for potential applications.},
  archive      = {J_TPAMI},
  author       = {Ming Cheng and Zhan Ma and M. Salman Asif and Yiling Xu and Haojie Liu and Wenbo Bao and Jun Sun},
  doi          = {10.1109/TPAMI.2020.2983371},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {10},
  pages        = {3275-3291},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {A dual camera system for high spatiotemporal resolution video acquisition},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). [Front inside cover]. <em>TPAMI</em>, <em>43</em>(9), C4.
(<a href="https://doi.org/10.1109/TPAMI.2021.3099276">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presents a listing of the editorial board, board of governors, current staff, committee members, and/or society editors for this issue of the publication.},
  archive      = {J_TPAMI},
  doi          = {10.1109/TPAMI.2021.3099276},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {9},
  pages        = {C4},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {[Front inside cover]},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021c). [Front inside cover]. <em>TPAMI</em>, <em>43</em>(9), C3.
(<a href="https://doi.org/10.1109/TPAMI.2021.3099278">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presents a listing of the editorial board, board of governors, current staff, committee members, and/or society editors for this issue of the publication.},
  archive      = {J_TPAMI},
  doi          = {10.1109/TPAMI.2021.3099278},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {9},
  pages        = {C3},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {[Front inside cover]},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021c). Erratum to “a bayesian formulation of coherent point
drift.” <em>TPAMI</em>, <em>43</em>(9), 3273. (<a
href="https://doi.org/10.1109/TPAMI.2021.3092384">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presents corrections to the above named paper.},
  archive      = {J_TPAMI},
  author       = {Osamu Hirose},
  doi          = {10.1109/TPAMI.2021.3092384},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {9},
  pages        = {3273},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Erratum to “A bayesian formulation of coherent point drift”},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SibNet: Sibling convolutional encoder for video captioning.
<em>TPAMI</em>, <em>43</em>(9), 3259–3272. (<a
href="https://doi.org/10.1109/TPAMI.2019.2940007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual captioning, the task of describing an image or a video using one or few sentences, is a challenging task owing to the complexity of understanding the copious visual information and describing it using natural language. Motivated by the success of applying neural networks for machine translation, previous work applies sequence to sequence learning to translate videos into sentences. In this work, different from previous work that encodes visual information using a single flow, we introduce a novel Sibling Convolutional Encoder (SibNet) for visual captioning, which employs a dual-branch architecture to collaboratively encode videos. The first content branch encodes visual content information of the video with an autoencoder, capturing the visual appearance information of the video as other networks often do. While the second semantic branch encodes semantic information of the video via visual-semantic joint embedding, which brings complementary representation by considering the semantics when extracting features from videos. Then both branches are effectively combined with soft-attention mechanism and finally fed into a RNN decoder to generate captions. With our SibNet explicitly capturing both content and semantic information, the proposed model can better represent the rich information in videos. To validate the advantages of the proposed model, we conduct experiments on two benchmarks for video captioning, YouTube2Text and MSR-VTT. Our results demonstrate that the proposed SibNet consistently outperforms existing methods across different evaluation metrics.},
  archive      = {J_TPAMI},
  author       = {Sheng Liu and Zhou Ren and Junsong Yuan},
  doi          = {10.1109/TPAMI.2019.2940007},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {9},
  pages        = {3259-3272},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {SibNet: Sibling convolutional encoder for video captioning},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Semi-supervised clustering with constraints of different
types from multiple information sources. <em>TPAMI</em>, <em>43</em>(9),
3247–3258. (<a
href="https://doi.org/10.1109/TPAMI.2020.2979699">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semi-supervised clustering is one of important research topics in cluster analysis, which uses pre-given knowledge as constraints to improve the clustering performance. While clustering a data set, people often get prior constraints from different information sources, which may have different representations and contents, to guide clustering process. However, most of existing semi-supervised clustering algorithms are based on single-source constraints and rarely consider to integrate multi-source constraints to enhance the clustering quality. To solve the problem, we analyze the relations among different types of constraints and propose an uniform representation for them. Based it, we propose a new semi-supervised clustering algorithm to find out a clustering that has good cluster structure and high consensus of all the sources of constraints. In the algorithm, we construct an optimization objective model and its solution method to achieve the aim. This algorithm can integrate multi-source constraints well to reduce the effect of incorrect constraints from single sources and find out a high-quality clustering. By the experimental studies on several benchmark data sets, we illustrate the effectiveness of the proposed algorithm, compared to other semi-supervised clustering algorithms.},
  archive      = {J_TPAMI},
  author       = {Liang Bai and JiYe Liang and Fuyuan Cao},
  doi          = {10.1109/TPAMI.2020.2979699},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {9},
  pages        = {3247-3258},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Semi-supervised clustering with constraints of different types from multiple information sources},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Point set registration for 3D range scans using fuzzy
cluster-based metric and efficient global optimization. <em>TPAMI</em>,
<em>43</em>(9), 3229–3246. (<a
href="https://doi.org/10.1109/TPAMI.2020.2978477">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study presents a new point set registration method to align 3D range scans. In our method, fuzzy clusters are utilized to represent a scan, and the registration of two given scans is realized by minimizing a fuzzy weighted sum of the distances between their fuzzy cluster centers. This fuzzy cluster-based metric has a broad basin of convergence and is robust to noise. Moreover, this metric provides analytic gradients, allowing standard gradient-based algorithms to be applied for optimization. Based on this metric, the outlier issues are addressed. In addition, for the first time in rigid point set registration, a registration quality assessment in the absence of ground truth is provided. Furthermore, given specified rotation and translation spaces, we derive the upper and lower bounds of the fuzzy cluster-based metric and develop a branch-and-bound (BnB)-based optimization scheme, which can globally minimize the metric regardless of the initialization. This optimization scheme is performed in an efficient coarse-to-fine fashion: First, fuzzy clustering is applied to describe each of the two given scans by a small number of fuzzy clusters. Then, a global search, which integrates BnB and gradient-based algorithms, is implemented to achieve a coarse alignment for the two scans. During the global search, the registration quality assessment offers a beneficial stop criterion to detect whether a good result is obtained. Afterwards, a relatively large number of points of the two scans are directly taken as the fuzzy cluster centers, and then, the coarse solution is refined to be an exact alignment using the gradient-based local convergence. Compared to existing counterparts, this optimization scheme makes a large improvement in terms of robustness and efficiency by virtue of the fuzzy cluster-based metric and the registration quality assessment. In the experiments, the registration results of several 3D range scan pairs demonstrate the accuracy and effectiveness of the proposed method, as well as its superiority to state-of-the-art registration approaches.},
  archive      = {J_TPAMI},
  author       = {Qianfang Liao and Da Sun and Henrik Andreasson},
  doi          = {10.1109/TPAMI.2020.2978477},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {9},
  pages        = {3229-3246},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Point set registration for 3D range scans using fuzzy cluster-based metric and efficient global optimization},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hardness-aware deep metric learning. <em>TPAMI</em>,
<em>43</em>(9), 3214–3228. (<a
href="https://doi.org/10.1109/TPAMI.2020.2980231">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a hardness-aware deep metric learning (HDML) framework for image clustering and retrieval. Most existing deep metric learning methods employ the hard negative mining strategy to alleviate the lack of informative samples for training. However, this mining strategy only utilizes a subset of training data, which may not be enough to characterize the global geometry of the embedding space comprehensively. To address this problem, we perform linear interpolation on embeddings to adaptively manipulate their hardness levels and generate corresponding label-preserving synthetics for recycled training so that information buried in all samples can be fully exploited and the metric is always challenged with proper difficulty. As a single synthetic for each sample may still not be enough to describe the unobserved distributions of the training data which is crucial for the generalization performance, we further extend HDML to generate multiple synthetics for each sample. We propose a randomly hardness-aware deep metric learning (HDML-R) method and an adaptively hardness-aware deep metric learning (HDML-A) method to sample multiple random and adaptive directions, respectively, for hardness-aware synthesis. Since the generated multiple synthetics might not all be useful and adaptive, we propose a synthetic selection method with three criteria for the selection of qualified synthetics that are beneficial to the training of the metric. Extensive experimental results on the widely used CUB-200-2011, Cars196, Stanford Online Products, In-Shop Clothes Retrieval, and VehicleID datasets demonstrate the effectiveness of the proposed framework.},
  archive      = {J_TPAMI},
  author       = {Wenzhao Zheng and Jiwen Lu and Jie Zhou},
  doi          = {10.1109/TPAMI.2020.2980231},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {9},
  pages        = {3214-3228},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Hardness-aware deep metric learning},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Gaussian graphical model exploration and selection in high
dimension low sample size setting. <em>TPAMI</em>, <em>43</em>(9),
3196–3213. (<a
href="https://doi.org/10.1109/TPAMI.2020.2980542">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gaussian graphical models (GGM) are often used to describe the conditional correlations between the components of a random vector. In this article, we compare two families of GGM inference methods: the nodewise approach and the penalised likelihood maximisation. We demonstrate on synthetic data that, when the sample size is small, the two methods produce graphs with either too few or too many edges when compared to the real one. As a result, we propose a composite procedure that explores a family of graphs with a nodewise numerical scheme and selects a candidate among them with an overall likelihood criterion. We demonstrate that, when the number of observations is small, this selection method yields graphs closer to the truth and corresponding to distributions with better KL divergence with regards to the real distribution than the other two. Finally, we show the interest of our algorithm on two concrete cases: first on brain imaging data, then on biological nephrology data. In both cases our results are more in line with current knowledge in each field.},
  archive      = {J_TPAMI},
  author       = {Thomas Lartigue and Simona Bottani and Stéphanie Baron and Olivier Colliot and Stanley Durrleman and Stéphanie Allassonnière},
  doi          = {10.1109/TPAMI.2020.2980542},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {9},
  pages        = {3196-3213},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Gaussian graphical model exploration and selection in high dimension low sample size setting},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Feature-aware uniform tessellations on video manifold for
content-sensitive supervoxels. <em>TPAMI</em>, <em>43</em>(9),
3183–3195. (<a
href="https://doi.org/10.1109/TPAMI.2020.2979714">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over-segmenting a video into supervoxels has strong potential to reduce the complexity of downstream computer vision applications. Content-sensitive supervoxels (CSSs) are typically smaller in content-dense regions (i.e., with high variation of appearance and/or motion) and larger in content-sparse regions. In this paper, we propose to compute feature-aware CSSs (FCSSs) that are regularly shaped 3D primitive volumes well aligned with local object/region/motion boundaries in video. To compute FCSSs, we map a video to a 3D manifold embedded in a combined color and spatiotemporal space, in which the volume elements of video manifold give a good measure of the video content density. Then any uniform tessellation on video manifold can induce CSS in the video. Our idea is that among all possible uniform tessellations on the video manifold, FCSS finds one whose cell boundaries well align with local video boundaries. To achieve this goal, we propose a novel restricted centroidal Voronoi tessellation method that simultaneously minimizes the tessellation energy (leading to uniform cells in the tessellation) and maximizes the average boundary distance (leading to good local feature alignment). Theoretically our method has an optimal competitive ratio O(1)O(1), and its time and space complexities are O(NK)O(NK) and O(N+K)O(N+K) for computing KK supervoxels in an NN-voxel video. We also present a simple extension of FCSS to streaming FCSS for processing long videos that cannot be loaded into main memory at once. We evaluate FCSS, streaming FCSS and ten representative supervoxel methods on four video datasets and two novel video applications. The results show that our method simultaneously achieves state-of-the-art performance with respect to various evaluation criteria.},
  archive      = {J_TPAMI},
  author       = {Ran Yi and Zipeng Ye and Wang Zhao and Minjing Yu and Yu-Kun Lai and Yong-Jin Liu},
  doi          = {10.1109/TPAMI.2020.2979714},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {9},
  pages        = {3183-3195},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Feature-aware uniform tessellations on video manifold for content-sensitive supervoxels},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Eigendecomposition-free training of deep networks for linear
least-square problems. <em>TPAMI</em>, <em>43</em>(9), 3167–3182. (<a
href="https://doi.org/10.1109/TPAMI.2020.2978812">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many classical Computer Vision problems, such as essential matrix computation and pose estimation from 3D to 2D correspondences, can be tackled by solving a linear least-square problem, which can be done by finding the eigenvector corresponding to the smallest, or zero, eigenvalue of a matrix representing a linear system. Incorporating this in deep learning frameworks would allow us to explicitly encode known notions of geometry, instead of having the network implicitly learn them from data. However, performing eigendecomposition within a network requires the ability to differentiate this operation. While theoretically doable, this introduces numerical instability in the optimization process in practice. In this paper, we introduce an eigendecomposition-free approach to training a deep network whose loss depends on the eigenvector corresponding to a zero eigenvalue of a matrix predicted by the network. We demonstrate that our approach is much more robust than explicit differentiation of the eigendecomposition using two general tasks, outlier rejection and denoising, with several practical examples including wide-baseline stereo, the perspective-n-point problem, and ellipse fitting. Empirically, our method has better convergence properties and yields state-of-the-art results.},
  archive      = {J_TPAMI},
  author       = {Zheng Dang and Kwang Moo Yi and Yinlin Hu and Fei Wang and Pascal Fua and Mathieu Salzmann},
  doi          = {10.1109/TPAMI.2020.2978812},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {9},
  pages        = {3167-3182},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Eigendecomposition-free training of deep networks for linear least-square problems},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deeply supervised discriminative learning for adversarial
defense. <em>TPAMI</em>, <em>43</em>(9), 3154–3166. (<a
href="https://doi.org/10.1109/TPAMI.2020.2978474">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks can easily be fooled by an adversary with minuscule perturbations added to an input image. The existing defense techniques suffer greatly under white-box attack settings, where an adversary has full knowledge of the network and can iterate several times to find strong perturbations. We observe that the main reason for the existence of such vulnerabilities is the close proximity of different class samples in the learned feature space of deep models. This allows the model decisions to be completely changed by adding an imperceptible perturbation to the inputs. To counter this, we propose to class-wise disentangle the intermediate feature representations of deep networks, specifically forcing the features for each class to lie inside a convex polytope that is maximally separated from the polytopes of other classes. In this manner, the network is forced to learn distinct and distant decision regions for each class. We observe that this simple constraint on the features greatly enhances the robustness of learned models, even against the strongest white-box attacks, without degrading the classification performance on clean images. We report extensive evaluations in both black-box and white-box attack scenarios and show significant gains in comparison to state-of-the-art defenses.},
  archive      = {J_TPAMI},
  author       = {Aamir Mustafa and Salman H. Khan and Munawar Hayat and Roland Goecke and Jianbing Shen and Ling Shao},
  doi          = {10.1109/TPAMI.2020.2978474},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {9},
  pages        = {3154-3166},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Deeply supervised discriminative learning for adversarial defense},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Comprehensive instructional video analysis: The COIN dataset
and performance evaluation. <em>TPAMI</em>, <em>43</em>(9), 3138–3153.
(<a href="https://doi.org/10.1109/TPAMI.2020.2980824">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Thanks to the substantial and explosively increased instructional videos on the Internet, novices are able to acquire knowledge for completing various tasks. Over the past decade, growing efforts have been devoted to investigating the problem on instructional video analysis. However, most existing instructional video datasets have limitations in diversity and scale, which makes them far from many real-world applications where more diverse activities occur. To address this, in this article, we propose a large-scale dataset called “COIN” for COmprehensive INstructional video analysis. Organized with a hierarchical structure, the COIN dataset contains 11,827 videos of 180 tasks in 12 domains (e.g., vehicles, gadgets, etc.) related to our daily life. With a new developed toolbox, all the videos are annotated efficiently with a series of step labels and the corresponding temporal boundaries. In order to provide a benchmark for instructional video analysis, we evaluate plenty of approaches on our COIN dataset under five different settings. Furthermore, we exploit two important characteristics (i.e., task-consistency and ordering-dependency) for localizing important steps in instructional videos. Accordingly, we propose two simple yet effective methods, which can be easily plugged into conventional proposal-based action detection models. We believe the introduction of the COIN dataset will promote the future in-depth research on instructional video analysis for the community. Our dataset, annotation toolbox and source codes are available at http://coin-dataset.github.io.},
  archive      = {J_TPAMI},
  author       = {Yansong Tang and Jiwen Lu and Jie Zhou},
  doi          = {10.1109/TPAMI.2020.2980824},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {9},
  pages        = {3138-3153},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Comprehensive instructional video analysis: The COIN dataset and performance evaluation},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bias in cross-entropy-based training of deep survival
networks. <em>TPAMI</em>, <em>43</em>(9), 3126–3137. (<a
href="https://doi.org/10.1109/TPAMI.2020.2979450">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the last years, utilizing deep learning for the analysis of survival data has become attractive to many researchers. This has led to the advent of numerous network architectures for the prediction of possibly censored time-to-event variables. Unlike networks for cross-sectional data (used e.g., in classification), deep survival networks require the specification of a suitably defined loss function that incorporates typical characteristics of survival data such as censoring and time-dependent features. Here, we provide an in-depth analysis of the cross-entropy loss function, which is a popular loss function for training deep survival networks. For each time point t, the cross-entropy loss is defined in terms of a binary outcome with levels “event at or before t” and “event after t”. Using both theoretical and empirical approaches, we show that this definition may result in a high prediction error and a heavy bias in the predicted survival probabilities. To overcome this problem, we analyze an alternative loss function that is derived from the negative log-likelihood function of a discrete time-to-event model. We show that replacing the cross-entropy loss by the negative log-likelihood loss results in much better calibrated prediction rules and also in an improved discriminatory power, as measured by the concordance index.},
  archive      = {J_TPAMI},
  author       = {Shekoufeh Gorgi Zadeh and Matthias Schmid},
  doi          = {10.1109/TPAMI.2020.2979450},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {9},
  pages        = {3126-3137},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Bias in cross-entropy-based training of deep survival networks},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Winning solutions and post-challenge analyses of the
ChaLearn AutoDL challenge 2019. <em>TPAMI</em>, <em>43</em>(9),
3108–3125. (<a
href="https://doi.org/10.1109/TPAMI.2021.3075372">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper reports the results and post-challenge analyses of ChaLearn’s AutoDL challenge series, which helped sorting out a profusion of AutoML solutions for Deep Learning (DL) that had been introduced in a variety of settings, but lacked fair comparisons. All input data modalities (time series, images, videos, text, tabular) were formatted as tensors and all tasks were multi-label classification problems. Code submissions were executed on hidden tasks, with limited time and computational resources, pushing solutions that get results quickly. In this setting, DL methods dominated, though popular Neural Architecture Search (NAS) was impractical. Solutions relied on fine-tuned pre-trained networks, with architectures matching data modality. Post-challenge tests did not reveal improvements beyond the imposed time limit. While no component is particularly original or novel, a high level modular organization emerged featuring a “meta-learner”, “data ingestor”, “model selector”, “model/learner”, and “evaluator”. This modularity enabled ablation studies, which revealed the importance of (off-platform) meta-learning, ensembling, and efficient data management. Experiments on heterogeneous module combinations further confirm the (local) optimality of the winning solutions. Our challenge legacy includes an ever-lasting benchmark ( http://autodl.chalearn.org ), the open-sourced code of the winners, and a free “AutoDL self-service.”},
  archive      = {J_TPAMI},
  author       = {Zhengying Liu and Adrien Pavao and Zhen Xu and Sergio Escalera and Fabio Ferreira and Isabelle Guyon and Sirui Hong and Frank Hutter and Rongrong Ji and Julio C. S. Jacques Junior and Ge Li and Marius Lindauer and Zhipeng Luo and Meysam Madadi and Thomas Nierhoff and Kangning Niu and Chunguang Pan and Danny Stoll and Sebastien Treguer and Jin Wang and Peng Wang and Chenglin Wu and Youcheng Xiong and Arbër Zela and Yang Zhang},
  doi          = {10.1109/TPAMI.2021.3075372},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {9},
  pages        = {3108-3125},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Winning solutions and post-challenge analyses of the ChaLearn AutoDL challenge 2019},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Evolving fully automated machine learning via life-long
knowledge anchors. <em>TPAMI</em>, <em>43</em>(9), 3091–3107. (<a
href="https://doi.org/10.1109/TPAMI.2021.3069250">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automated machine learning (AutoML) has achieved remarkable progress on various tasks, which is attributed to its minimal involvement of manual feature and model designs. However, most of existing AutoML pipelines only touch parts of the full machine learning pipeline, e.g., neural architecture search or optimizer selection. This leaves potentially important components such as data cleaning and model ensemble out of the optimization, and still results in considerable human involvement and suboptimal performance. The main challenges lie in the huge search space assembling all possibilities over all components, as well as the generalization ability over different tasks like image, text, and tabular etc . In this paper, we present a first-of-its-kind fully AutoML pipeline, to comprehensively automate data preprocessing, feature engineering, model generation/selection/training and ensemble for an arbitrary dataset and evaluation metric. Our innovation lies in the comprehensive scope of a learning pipeline, with a novel “life-long” knowledge anchor design to fundamentally accelerate the search over the full search space. Such knowledge anchors record detailed information of pipelines and integrates them with an evolutionary algorithm for joint optimization across components. Experiments demonstrate that the result pipeline achieves state-of-the-art performance on multiple datasets and modalities. Specifically, the proposed framework was extensively evaluated in the NeurIPS 2019 AutoDL challenge, and won the only champion with a significant gap against other approaches, on all the image, video, speech, text and tabular tracks.},
  archive      = {J_TPAMI},
  author       = {Xiawu Zheng and Yang Zhang and Sirui Hong and Huixia Li and Lang Tang and Youcheng Xiong and Jin Zhou and Yan Wang and Xiaoshuai Sun and Pengfei Zhu and Chenglin Wu and Rongrong Ji},
  doi          = {10.1109/TPAMI.2021.3069250},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {9},
  pages        = {3091-3107},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Evolving fully automated machine learning via life-long knowledge anchors},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Auto-pytorch: Multi-fidelity MetaLearning for efficient and
robust AutoDL. <em>TPAMI</em>, <em>43</em>(9), 3079–3090. (<a
href="https://doi.org/10.1109/TPAMI.2021.3067763">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While early AutoML frameworks focused on optimizing traditional ML pipelines and their hyperparameters, a recent trend in AutoML is to focus on neural architecture search. In this paper, we introduce Auto-PyTorch, which brings together the best of these two worlds by jointly and robustly optimizing the network architecture and the training hyperparameters to enable fully automated deep learning (AutoDL). Auto-PyTorch achieves state-of-the-art performance on several tabular benchmarks by combining multi-fidelity optimization with portfolio construction for warmstarting and ensembling of deep neural networks (DNNs) and common baselines for tabular data. To thoroughly study our assumptions on how to design such an AutoDL system, we additionally introduce a new benchmark on learning curves for DNNs, dubbed LCBench, and run extensive ablation studies of the full Auto-PyTorch on typical AutoML benchmarks, eventually showing that Auto-PyTorch performs better than several state-of-the-art competitors.},
  archive      = {J_TPAMI},
  author       = {Lucas Zimmer and Marius Lindauer and Frank Hutter},
  doi          = {10.1109/TPAMI.2021.3067763},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {9},
  pages        = {3079-3090},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Auto-pytorch: Multi-fidelity MetaLearning for efficient and robust AutoDL},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptation strategies for automated machine learning on
evolving data. <em>TPAMI</em>, <em>43</em>(9), 3067–3078. (<a
href="https://doi.org/10.1109/TPAMI.2021.3062900">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automated Machine Learning (AutoML) systems have been shown to efficiently build good models for new datasets. However, it is often not clear how well they can adapt when the data evolves over time. The main goal of this study is to understand the effect of concept drift on the performance of AutoML methods, and which adaptation strategies can be employed to make them more robust to changes in the underlying data. To that end, we propose 6 concept drift adaptation strategies and evaluate their effectiveness on a variety of AutoML approaches for building machine learning pipelines, including Bayesian optimization, genetic programming, and random search with automated stacking. These are evaluated empirically on real-world and synthetic data streams with different types of concept drift. Based on this analysis, we propose ways to develop more sophisticated and robust AutoML techniques.},
  archive      = {J_TPAMI},
  author       = {Bilge Celik and Joaquin Vanschoren},
  doi          = {10.1109/TPAMI.2021.3062900},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {9},
  pages        = {3067-3078},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Adaptation strategies for automated machine learning on evolving data},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Predicting machine learning pipeline runtimes in the context
of automated machine learning. <em>TPAMI</em>, <em>43</em>(9),
3055–3066. (<a
href="https://doi.org/10.1109/TPAMI.2021.3056950">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automated machine learning (AutoML) seeks to automatically find so-called machine learning pipelines that maximize the prediction performance when being used to train a model on a given dataset. One of the main and yet open challenges in AutoMLis an effective use of computational resources: An AutoML process involves the evaluation of many candidate pipelines, which are costly but often ineffective because they are canceled due to a timeout. In this paper, we present an approach to predict the runtime of two-step machine learning pipelines with up to one pre-processor, which can be used to anticipate whether or not a pipeline will time out. Separate runtime models are trained offline for each algorithm that may be used in a pipeline, and an overall prediction is derived from these models. We empirically show that the approach increases successful evaluations made by an AutoML tool while preserving or even improving on the previously best solutions.},
  archive      = {J_TPAMI},
  author       = {Felix Mohr and Marcel Wever and Alexander Tornede and Eyke Hüllermeier},
  doi          = {10.1109/TPAMI.2021.3056950},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {9},
  pages        = {3055-3066},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Predicting machine learning pipeline runtimes in the context of automated machine learning},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). AutoML for multi-label classification: Overview and
empirical evaluation. <em>TPAMI</em>, <em>43</em>(9), 3037–3054. (<a
href="https://doi.org/10.1109/TPAMI.2021.3051276">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automated machine learning (AutoML) supports the algorithmic construction and data-specific customization of machine learning pipelines, including the selection, combination, and parametrization of machine learning algorithms as main constituents. Generally speaking, AutoML approaches comprise two major components: a search space model and an optimizer for traversing the space. Recent approaches have shown impressive results in the realm of supervised learning, most notably (single-label) classification (SLC). Moreover, first attempts at extending these approaches towards multi-label classification (MLC) have been made. While the space of candidate pipelines is already huge in SLC, the complexity of the search space is raised to an even higher power in MLC. One may wonder, therefore, whether and to what extent optimizers established for SLC can scale to this increased complexity, and how they compare to each other. This paper makes the following contributions: First, we survey existing approaches to AutoML for MLC. Second, we augment these approaches with optimizers not previously tried for MLC. Third, we propose a benchmarking framework that supports a fair and systematic comparison. Fourth, we conduct an extensive experimental study, evaluating the methods on a suite of MLC problems. We find a grammar-based best-first search to compare favorably to other optimizers.},
  archive      = {J_TPAMI},
  author       = {Marcel Wever and Alexander Tornede and Felix Mohr and Eyke Hüllermeier},
  doi          = {10.1109/TPAMI.2021.3051276},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {9},
  pages        = {3037-3054},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {AutoML for multi-label classification: Overview and empirical evaluation},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Additive tree-structured conditional parameter spaces in
bayesian optimization: A novel covariance function and a fast
implementation. <em>TPAMI</em>, <em>43</em>(9), 3024–3036. (<a
href="https://doi.org/10.1109/TPAMI.2020.3026019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian optimization (BO) is a sample-efficient global optimization algorithm for black-box functions which are expensive to evaluate. Existing literature on model based optimization in conditional parameter spaces are usually built on trees. In this work, we generalize the additive assumption to tree-structured functions and propose an additive tree-structured covariance function, showing improved sample-efficiency, wider applicability and greater flexibility. Furthermore, by incorporating the structure information of parameter spaces and the additive assumption in the BO loop, we develop a parallel algorithm to optimize the acquisition function and this optimization can be performed in a low dimensional space. We demonstrate our method on an optimization benchmark function, on a neural network compression problem and on pruning pre-trained VGG16 and ResNet50 models. Experimental results show our approach significantly outperforms the current state of the art for conditional parameter optimization including SMAC, TPE and Jenatton et al. (2017).},
  archive      = {J_TPAMI},
  author       = {Xingchen Ma and Matthew B. Blaschko},
  doi          = {10.1109/TPAMI.2020.3026019},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {9},
  pages        = {3024-3036},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Additive tree-structured conditional parameter spaces in bayesian optimization: A novel covariance function and a fast implementation},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). NAS-FAS: Static-dynamic central difference network search
for face anti-spoofing. <em>TPAMI</em>, <em>43</em>(9), 3005–3023. (<a
href="https://doi.org/10.1109/TPAMI.2020.3036338">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face anti-spoofing (FAS) plays a vital role in securing face recognition systems. Existing methods heavily rely on the expert-designed networks, which may lead to a sub-optimal solution for FAS task. Here we propose the first FAS method based on neural architecture search (NAS), called NAS-FAS, to discover the well-suited task-aware networks. Unlike previous NAS works mainly focus on developing efficient search strategies in generic object classification, we pay more attention to study the search spaces for FAS task. The challenges of utilizing NAS for FAS are in two folds: the networks searched on 1) a specific acquisition condition might perform poorly in unseen conditions, and 2) particular spoofing attacks might generalize badly for unseen attacks. To overcome these two issues, we develop a novel search space consisting of central difference convolution and pooling operators. Moreover, an efficient static-dynamic representation is exploited for fully mining the FAS-aware spatio-temporal discrepancy. Besides, we propose Domain/Type-aware Meta-NAS, which leverages cross-domain/type knowledge for robust searching. Finally, in order to evaluate the NAS transferability for cross datasets and unknown attack types, we release a large-scale 3D mask dataset, namely CASIA-SURF 3DMask, for supporting the new `cross-dataset cross-type&#39; testing protocol. Experiments demonstrate that the proposed NAS-FAS achieves state-of-the-art performance on nine FAS benchmark datasets with four testing protocols.},
  archive      = {J_TPAMI},
  author       = {Zitong Yu and Jun Wan and Yunxiao Qin and Xiaobai Li and Stan Z. Li and Guoying Zhao},
  doi          = {10.1109/TPAMI.2020.3036338},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {9},
  pages        = {3005-3023},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {NAS-FAS: Static-dynamic central difference network search for face anti-spoofing},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). FNA++: Fast network adaptation via parameter remapping and
architecture search. <em>TPAMI</em>, <em>43</em>(9), 2990–3004. (<a
href="https://doi.org/10.1109/TPAMI.2020.3044416">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks achieve remarkable performance in many computer vision tasks. Most state-of-the-art (SOTA) semantic segmentation and object detection approaches reuse neural network architectures designed for image classification as the backbone, commonly pre-trained on ImageNet. However, performance gains can be achieved by designing network architectures specifically for detection and segmentation, as shown by recent neural architecture search (NAS) research for detection and segmentation. One major challenge though is that ImageNet pre-training of the search space representation (a.k.a. super network) or the searched networks incurs huge computational cost. In this paper, we propose a Fast Network Adaptation (FNA++) method, which can adapt both the architecture and parameters of a seed network (e.g., an ImageNet pre-trained network) to become a network with different depths, widths, or kernel sizes via a parameter remapping technique, making it possible to use NAS for segmentation and detection tasks a lot more efficiently. In our experiments, we apply FNA++ on MobileNetV2 to obtain new networks for semantic segmentation, object detection, and human pose estimation that clearly outperform existing networks designed both manually and by NAS. We also implement FNA++ on ResNets and NAS networks, which demonstrates a great generalization ability. The total computation cost of FNA++ is significantly less than SOTA segmentation and detection NAS approaches: 1737× less than DPC, 6.8× less than Auto-DeepLab, and 8.0× less than DetNAS. A series of ablation studies are performed to demonstrate the effectiveness, and detailed analysis is provided for more insights into the working mechanism. Codes are available at https://github.com/JaminFong/FNA.},
  archive      = {J_TPAMI},
  author       = {Jiemin Fang and Yuzhu Sun and Qian Zhang and Kangjian Peng and Yuan Li and Wenyu Liu and Xinggang Wang},
  doi          = {10.1109/TPAMI.2020.3044416},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {9},
  pages        = {2990-3004},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {FNA++: Fast network adaptation via parameter remapping and architecture search},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Neural architecture transfer. <em>TPAMI</em>,
<em>43</em>(9), 2971–2989. (<a
href="https://doi.org/10.1109/TPAMI.2021.3052758">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural architecture search (NAS) has emerged as a promising avenue for automatically designing task-specific neural networks. Existing NAS approaches require one complete search for each deployment specification of hardware or objective. This is a computationally impractical endeavor given the potentially large number of application scenarios. In this paper, we propose Neural Architecture Transfer (NAT) to overcome this limitation. NAT is designed to efficiently generate task-specific custom models that are competitive under multiple conflicting objectives. To realize this goal we learn task-specific supernets from which specialized subnets can be sampled without any additional training. The key to our approach is an integrated online transfer learning and many-objective evolutionary search procedure. A pre-trained supernet is iteratively adapted while simultaneously searching for task-specific subnets. We demonstrate the efficacy of NAT on 11 benchmark image classification tasks ranging from large-scale multi-class to small-scale fine-grained datasets. In all cases, including ImageNet, NATNets improve upon the state-of-the-art under mobile settings (≤≤ 600M Multiply-Adds). Surprisingly, small-scale fine-grained datasets benefit the most from NAT. At the same time, the architecture search and transfer is orders of magnitude more efficient than existing NAS methods. Overall, experimental evaluation indicates that, across diverse image classification tasks and computational objectives, NAT is an appreciably more effective alternative to conventional transfer learning of fine-tuning weights of an existing network architecture learned on standard datasets. Code is available at https://github.com/human-analysis/neural-architecture-transfer.},
  archive      = {J_TPAMI},
  author       = {Zhichao Lu and Gautam Sreekumar and Erik Goodman and Wolfgang Banzhaf and Kalyanmoy Deb and Vishnu Naresh Boddeti},
  doi          = {10.1109/TPAMI.2021.3052758},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {9},
  pages        = {2971-2989},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Neural architecture transfer},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Partially-connected neural architecture search for reduced
computational redundancy. <em>TPAMI</em>, <em>43</em>(9), 2953–2970. (<a
href="https://doi.org/10.1109/TPAMI.2021.3059510">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Differentiable architecture search (DARTS) enables effective neural architecture search (NAS) using gradient descent, but suffers from high memory and computational costs. In this paper, we propose a novel approach, namely Partially-Connected DARTS (PC-DARTS), to achieve efficient and stable neural architecture search by reducing the channel and spatial redundancies of the super-network. In the channel level, partial channel connection is presented to randomly sample a small subset of channels for operation selection to accelerate the search process and suppress the over-fitting of the super-network. Side operation is introduced for bypassing (non-sampled) channels to guarantee the performance of searched architectures under extremely low sampling rates. In the spatial level, input features are down-sampled to eliminate spatial redundancy and enhance the efficiency of the mixed computation for operation selection. Furthermore, edge normalization is developed to maintain the consistency of edge selection based on channel sampling with the architectural parameters for edges. Theoretical analysis shows that partial channel connection and parameterized side operation are equivalent to regularizing the super-network on the weights and architectural parameters during bilevel optimization. Experimental results demonstrate that the proposed approach achieves higher search speed and training stability than DARTS. PC-DARTS obtains a top-1 error rate of 2.55 percent on CIFAR-10 with 0.07 GPU-days for architecture search, and a state-of-the-art top-1 error rate of 24.1 percent on ImageNet (under the mobile setting) within 2.8 GPU-days.},
  archive      = {J_TPAMI},
  author       = {Yuhui Xu and Lingxi Xie and Wenrui Dai and Xiaopeng Zhang and Xin Chen and Guo-Jun Qi and Hongkai Xiong and Qi Tian},
  doi          = {10.1109/TPAMI.2021.3059510},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {9},
  pages        = {2953-2970},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Partially-connected neural architecture search for reduced computational redundancy},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MIGO-NAS: Towards fast and generalizable neural architecture
search. <em>TPAMI</em>, <em>43</em>(9), 2936–2952. (<a
href="https://doi.org/10.1109/TPAMI.2021.3065138">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural architecture search (NAS) has achieved unprecedented performance in various computer vision tasks. However, most existing NAS methods are defected in search efficiency and model generalizability. In this paper, we propose a novel NAS framework, termed MIGO-NAS, with the aim to guarantee the efficiency and generalizability in arbitrary search spaces. On the one hand, we formulate the search space as a multivariate probabilistic distribution, which is then optimized by a novel multivariate information-geometric optimization (MIGO). By approximating the distribution with a sampling, training, and testing pipeline, MIGO guarantees the memory efficiency, training efficiency, and search flexibility. Besides, MIGO is the first time to decrease the estimation error of natural gradient in multivariate distribution. On the other hand, for a set of specific constraints, the neural architectures are generated by a novel dynamic programming network generation (DPNG), which significantly reduces the training cost under various hardware environments. Experiments validate the advantages of our approach over existing methods by establishing a superior accuracy and efficiency i.e., 2.39 test error on CIFAR-10 benchmark and 21.7 on ImageNet benchmark, with only 1.5 GPU hours and 96 GPU hours for searching, respectively. Besides, the searched architectures can be well generalize to computer vision tasks including object detection and semantic segmentation, i.e., 25×25× FLOPs compression, with 6.4 mAP gain over Pascal VOC dataset, and 29.9×29.9× FLOPs compression, with only 1.41 percent performance drop over Cityscapes dataset. The code is publicly available.},
  archive      = {J_TPAMI},
  author       = {Xiawu Zheng and Rongrong Ji and Yuhang Chen and Qiang Wang and Baochang Zhang and Jie Chen and Qixiang Ye and Feiyue Huang and Yonghong Tian},
  doi          = {10.1109/TPAMI.2021.3065138},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {9},
  pages        = {2936-2952},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {MIGO-NAS: Towards fast and generalizable neural architecture search},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). One-shot neural architecture search: Maximising diversity to
overcome catastrophic forgetting. <em>TPAMI</em>, <em>43</em>(9),
2921–2935. (<a
href="https://doi.org/10.1109/TPAMI.2020.3035351">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One-shot neural architecture search (NAS) has recently become mainstream in the NAS community because it significantly improves computational efficiency through weight sharing. However, the supernet training paradigm in one-shot NAS introduces catastrophic forgetting, where each step of the training can deteriorate the performance of other architectures that contain partially-shared weights with current architecture. To overcome this problem of catastrophic forgetting, we formulate supernet training for one-shot NAS as a constrained continual learning optimization problem such that learning the current architecture does not degrade the validation accuracy of previous architectures. The key to solving this constrained optimization problem is a novelty search based architecture selection (NSAS) loss function that regularizes the supernet training by using a greedy novelty search method to find the most representative subset. We applied the NSAS loss function to two one-shot NAS baselines and extensively tested them on both a common search space and a NAS benchmark dataset. We further derive three variants based on the NSAS loss function, the NSAS with depth constrain (NSAS-C) to improve the transferability, and NSAS-G and NSAS-LG to handle the situation with a limited number of constraints. The experiments on the common NAS search space demonstrate that NSAS and it variants improve the predictive ability of supernet training in one-shot NAS with remarkable and efficient performance on the CIFAR-10, CIFAR-100, and ImageNet datasets. The results with the NAS benchmark dataset also confirm the significant improvements these one-shot NAS baselines can make.},
  archive      = {J_TPAMI},
  author       = {Miao Zhang and Huiqi Li and Shirui Pan and Xiaojun Chang and Chuan Zhou and Zongyuan Ge and Steven Su},
  doi          = {10.1109/TPAMI.2020.3035351},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {9},
  pages        = {2921-2935},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {One-shot neural architecture search: Maximising diversity to overcome catastrophic forgetting},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DATA: Differentiable ArchiTecture approximation with
distribution guided sampling. <em>TPAMI</em>, <em>43</em>(9), 2905–2920.
(<a href="https://doi.org/10.1109/TPAMI.2020.3020315">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural architecture search (NAS) is inherently subject to the gap of architectures during searching and validating. To bridge this gap effectively, we develop Differentiable ArchiTecture Approximation (DATA) with Ensemble Gumbel-Softmax (EGS) estimator and Architecture Distribution Constraint (ADC) to automatically approximate architectures during searching and validating in a differentiable manner. Technically, the EGS estimator consists of a group of Gumbel-Softmax estimators, which is capable of converting probability vectors to binary codes and passing gradients reversely, reducing the estimation bias in a differentiable way. To narrow the distribution gap between sampled architectures and supernet, further, the ADC is introduced to reduce the variance of sampling during searching. Benefiting from such modeling, architecture probabilities and network weights in the NAS model can be jointly optimized with the standard back-propagation, yielding an end-to-end learning mechanism for searching deep neural architectures in an extended search space. Conclusively, in the validating process, a high-performance architecture that approaches to the learned one during searching is readily built. Extensive experiments on various tasks including image classification, few-shot learning, unsupervised clustering, semantic segmentation and language modeling strongly demonstrate that DATA is capable of discovering high-performance architectures while guaranteeing the required efficiency. Code is available at https://github.com/XinbangZhang/DATA-NAS.},
  archive      = {J_TPAMI},
  author       = {Xinbang Zhang and Jianlong Chang and Yiwen Guo and Gaofeng Meng and Shiming Xiang and Zhouchen Lin and Chunhong Pan},
  doi          = {10.1109/TPAMI.2020.3020315},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {9},
  pages        = {2905-2920},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {DATA: Differentiable ArchiTecture approximation with distribution guided sampling},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). You only search once: Single shot neural architecture search
via direct sparse optimization. <em>TPAMI</em>, <em>43</em>(9),
2891–2904. (<a
href="https://doi.org/10.1109/TPAMI.2020.3020300">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently neural architecture search (NAS) has raised great interest in both academia and industry. However, it remains challenging because of its huge and non-continuous search space. Instead of applying evolutionary algorithm or reinforcement learning as previous works, this paper proposes a direct sparse optimization NAS (DSO-NAS) method. The motivation behind DSO-NAS is to address the task in the view of model pruning. To achieve this goal, we start from a completely connected block, and then introduce scaling factors to scale the information flow between operations. Next, sparse regularizations are imposed to prune useless connections in the architecture. Lastly, an efficient and theoretically sound optimization method is derived to solve it. Our method enjoys both advantages of differentiability and efficiency, therefore it can be directly applied to large datasets like ImageNet and tasks beyond classification. Particularly, on the CIFAR-10 dataset, DSO-NAS achieves an average test error 2.74 percent, while on the ImageNet dataset DSO-NAS achieves 25.4 percent test error under 600M FLOPs with 8 GPUs in 18 hours. As for semantic segmentation task, DSO-NAS also achieve competitive result compared with manually designed architectures on the PASCAL VOC dataset. Code is available at https://github.com/XinbangZhang/DSO-NAS.},
  archive      = {J_TPAMI},
  author       = {Xinbang Zhang and Zehao Huang and Naiyan Wang and Shiming Xiang and Chunhong Pan},
  doi          = {10.1109/TPAMI.2020.3020300},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {9},
  pages        = {2891-2904},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {You only search once: Single shot neural architecture search via direct sparse optimization},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Guest editorial: Automated machine learning. <em>TPAMI</em>,
<em>43</em>(9), 2887–2890. (<a
href="https://doi.org/10.1109/TPAMI.2021.3077106">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This special section is formed by 15 articles of outstanding quality that together comprise a snapshot of cutting edge automated machine learning (AutoML) research.},
  archive      = {J_TPAMI},
  author       = {Hugo Jair Escalante and Quanming Yao and Wei-Wei Tu and Nelishia Pillay and Rong Qu and Yang Yu and Neil Houlsby},
  doi          = {10.1109/TPAMI.2021.3077106},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {9},
  pages        = {2887-2890},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Guest editorial: Automated machine learning},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). [Back cover]. <em>TPAMI</em>, <em>43</em>(8), C4. (<a
href="https://doi.org/10.1109/TPAMI.2021.3089308">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Provides a listing of current committee members and society officers.},
  archive      = {J_TPAMI},
  doi          = {10.1109/TPAMI.2021.3089308},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {C4},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {[Back cover]},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Editorial board. <em>TPAMI</em>, <em>43</em>(8), C2. (<a
href="https://doi.org/10.1109/TPAMI.2021.3089316">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Provides a listing of current committee members and society officers.},
  archive      = {J_TPAMI},
  doi          = {10.1109/TPAMI.2021.3089316},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {C2},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Editorial board},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). IEEE computer society jobs board. <em>TPAMI</em>,
<em>43</em>(8), 2886. (<a
href="https://doi.org/10.1109/TPAMI.2021.3089312">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advertisement, IEEE Computer Society.},
  archive      = {J_TPAMI},
  doi          = {10.1109/TPAMI.2021.3089312},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {2886},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {IEEE computer society jobs board},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). ComputingEdge. <em>TPAMI</em>, <em>43</em>(8), 2885. (<a
href="https://doi.org/10.1109/TPAMI.2021.3089314">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advertisement, IEEE. ComputingEdge: Your one-stop resource for industry hot topics, technical overviews, and in-depth articles.},
  archive      = {J_TPAMI},
  doi          = {10.1109/TPAMI.2021.3089314},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {2885},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {ComputingEdge},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Blur-invariant similarity measurement of images.
<em>TPAMI</em>, <em>43</em>(8), 2882–2884. (<a
href="https://doi.org/10.1109/TPAMI.2020.3036630">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article is a comment on the recent TPAMI paper (Gopalan et al., 2012) that introduced a blur-invariant distance measure between two images. We point out two mistakes of the theory presented in (Gopalan et al., 2012) and propose a correction. We also compare the original and corrected methods experimentally.},
  archive      = {J_TPAMI},
  author       = {Matěj Lébl and Filip Šroubek and Jan Flusser},
  doi          = {10.1109/TPAMI.2020.3036630},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {2882-2884},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Blur-invariant similarity measurement of images},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-task head pose estimation in-the-wild. <em>TPAMI</em>,
<em>43</em>(8), 2874–2881. (<a
href="https://doi.org/10.1109/TPAMI.2020.3046323">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a deep learning-based multi-task approach for head pose estimation in images. We contribute with a network architecture and training strategy that harness the strong dependencies among face pose, alignment and visibility, to produce a top performing model for all three tasks. Our architecture is an encoder-decoder CNN with residual blocks and lateral skip connections. We show that the combination of head pose estimation and landmark-based face alignment significantly improve the performance of the former task. Further, the location of the pose task at the bottleneck layer, at the end of the encoder, and that of tasks depending on spatial information, such as visibility and alignment, in the final decoder layer, also contribute to increase the final performance. In the experiments conducted the proposed model outperforms the state-of-the-art in the face pose and visibility tasks. By including a final landmark regression step it also produces face alignment results on par with the state-of-the-art.},
  archive      = {J_TPAMI},
  author       = {Roberto Valle and José M. Buenaposada and Luis Baumela},
  doi          = {10.1109/TPAMI.2020.3046323},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {2874-2881},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Multi-task head pose estimation in-the-wild},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning saliency from single noisy labelling: A robust
model fitting perspective. <em>TPAMI</em>, <em>43</em>(8), 2866–2873.
(<a href="https://doi.org/10.1109/TPAMI.2020.3046486">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The advances made in predicting visual saliency using deep neural networks come at the expense of collecting large-scale annotated data. However, pixel-wise annotation is labor-intensive and overwhelming. In this paper, we propose to learn saliency prediction from a single noisy labelling , which is easy to obtain (e.g., from imperfect human annotation or from unsupervised saliency prediction methods). With this goal, we address a natural question: Can we learn saliency prediction while identifying clean labels in a unified framework? To answer this question, we call on the theory of robust model fitting and formulate deep saliency prediction from a single noisy labelling as robust network learning and exploit model consistency across iterations to identify inliers and outliers (i.e., noisy labels). Extensive experiments on different benchmark datasets demonstrate the superiority of our proposed framework, which can learn comparable saliency prediction with state-of-the-art fully supervised saliency methods. Furthermore, we show that simply by treating ground truth annotations as noisy labelling, our framework achieves tangible improvements over state-of-the-art methods.},
  archive      = {J_TPAMI},
  author       = {Jing Zhang and Yuchao Dai and Tong Zhang and Mehrtash Harandi and Nick Barnes and Richard Hartley},
  doi          = {10.1109/TPAMI.2020.3046486},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {2866-2873},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Learning saliency from single noisy labelling: A robust model fitting perspective},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Acceleration of non-rigid point set registration with
downsampling and gaussian process regression. <em>TPAMI</em>,
<em>43</em>(8), 2858–2865. (<a
href="https://doi.org/10.1109/TPAMI.2020.3043769">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Non-rigid point set registration is the process of transforming a shape represented as a point set into a shape matching another shape. In this paper, we propose an acceleration method for solving non-rigid point set registration problems. We accelerate non-rigid registration by dividing it into three steps: i) downsampling of point sets; ii) non-rigid registration of downsampled point sets; and iii) interpolation of shape deformation vectors corresponding to points removed during downsampling. To register downsampled point sets, we use a registration algorithm based on a prior distribution, called motion coherence prior. Using the same prior, we derive an interpolation method interpreted as Gaussian process regression. Through numerical experiments, we demonstrate that our algorithm registers point sets containing over ten million points. We also show that our algorithm reduces computing time more radically than a state-of-the-art acceleration algorithm.},
  archive      = {J_TPAMI},
  author       = {Osamu Hirose},
  doi          = {10.1109/TPAMI.2020.3043769},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {2858-2865},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Acceleration of non-rigid point set registration with downsampling and gaussian process regression},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A unified approach to kinship verification. <em>TPAMI</em>,
<em>43</em>(8), 2851–2857. (<a
href="https://doi.org/10.1109/TPAMI.2020.3036993">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we propose a deep learning-based approach for kin verification using a unified multi-task learning scheme where all kinship classes are jointly learned. This allows us to better utilize small training sets that are typical of kin verification. We introduce a novel approach for fusing the embeddings of kin images, to avoid overfitting, which is a common issue in training such networks. An adaptive sampling scheme is derived for the training set images, to resolve the inherent imbalance in kin verification datasets. A thorough ablation study exemplifies the effectivity of our approach, which is experimentally shown to outperform contemporary state-of-the-art kin verification results when applied to the Families In the Wild, FG2018, and FG2020 datasets.},
  archive      = {J_TPAMI},
  author       = {Eran Dahan and Yosi Keller},
  doi          = {10.1109/TPAMI.2020.3036993},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {2851-2857},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {A unified approach to kinship verification},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Variational level set evolution for non-rigid 3D
reconstruction from a single depth camera. <em>TPAMI</em>,
<em>43</em>(8), 2838–2850. (<a
href="https://doi.org/10.1109/TPAMI.2020.2976065">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a framework for real-time 3D reconstruction of non-rigidly moving surfaces captured with a single RGB-D camera. Based on the variational level set method, it warps a given truncated signed distance field (TSDF) to a target TSDF via gradient flow without explicit correspondence search. We optimize an energy that contains a data term which steers towards voxel-wise alignment. To ensure geometrically consistent reconstructions, we develop and compare different strategies, namely an approximately Killing vector field regularizer, gradient flow in Sobolev space and newly devised accelerated optimization. The underlying TSDF evolution makes our approach capable of capturing rapid motions, topological changes and interacting agents, but entails loss of data association. To recover correspondences, we propose to utilize the lowest-frequency Laplacian eigenfunctions of the TSDFs, which encode inherent deformation patterns. For moderate motions we are able to obtain implicit associations via a term that imposes voxel-wise eigenfunction alignment. This is not sufficient for larger motions, so we explicitly estimate voxel correspondences via signature matching of lower-dimensional eigenfunction embeddings. We carry out qualitative and quantitative evaluation of our geometric reconstruction fidelity and voxel correspondence accuracy, demonstrating advantages over related techniques in handling topological changes and fast motions.},
  archive      = {J_TPAMI},
  author       = {Miroslava Slavcheva and Maximilian Baust and Slobodan Ilic},
  doi          = {10.1109/TPAMI.2020.2976065},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {2838-2850},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Variational level set evolution for non-rigid 3D reconstruction from a single depth camera},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Underwater single image color restoration using haze-lines
and a new quantitative dataset. <em>TPAMI</em>, <em>43</em>(8),
2822–2837. (<a
href="https://doi.org/10.1109/TPAMI.2020.2977624">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Underwater images suffer from color distortion and low contrast, because light is attenuated while it propagates through water. Attenuation under water varies with wavelength, unlike terrestrial images where attenuation is assumed to be spectrally uniform. The attenuation depends both on the water body and the 3D structure of the scene, making color restoration difficult. Unlike existing single underwater image enhancement techniques, our method takes into account multiple spectral profiles of different water types. By estimating just two additional global parameters: the attenuation ratios of the blue-red and blue-green color channels, the problem is reduced to single image dehazing, where all color channels have the same attenuation coefficients. Since the water type is unknown, we evaluate different parameters out of an existing library of water types. Each type leads to a different restored image and the best result is automatically chosen based on color distribution. We also contribute a dataset of 57 images taken in different locations. To obtain ground truth, we placed multiple color charts in the scenes and calculated its 3D structure using stereo imaging. This dataset enables a rigorous quantitative evaluation of restoration algorithms on natural images for the first time.},
  archive      = {J_TPAMI},
  author       = {Dana Berman and Deborah Levy and Shai Avidan and Tali Treibitz},
  doi          = {10.1109/TPAMI.2020.2977624},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {2822-2837},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Underwater single image color restoration using haze-lines and a new quantitative dataset},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SMRT: Multi-resident tracking in smart homes with sensor
vectorization. <em>TPAMI</em>, <em>43</em>(8), 2809–2821. (<a
href="https://doi.org/10.1109/TPAMI.2020.2973571">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Smart homes equipped with anonymous binary sensors offer a low-cost, unobtrusive solution that powers activity-aware applications, such as building automation, health monitoring, behavioral intervention, and home security. However, when multiple residents are living in a smart home, associating sensor events with the corresponding residents can pose a major challenge. Previous approaches to multi-resident tracking in smart homes rely on extra information, such as sensor layouts, floor plans, and annotated data, which may not be available or inconvenient to obtain in practice. To address those challenges in real-life deployment, we introduce the sMRT algorithm that simultaneously tracks the location of each resident and estimates the number of residents in the smart home, without relying on ground-truth annotated sensor data or other additional information. We evaluate the performance of our approach using two smart home datasets recorded in real-life settings and compare sMRT with two other methods that rely on sensor layout and ground truth-labeled sensor data.},
  archive      = {J_TPAMI},
  author       = {Tinghui Wang and Diane J. Cook},
  doi          = {10.1109/TPAMI.2020.2973571},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {2809-2821},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {SMRT: Multi-resident tracking in smart homes with sensor vectorization},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Self-supervised multi-view person association and its
applications. <em>TPAMI</em>, <em>43</em>(8), 2794–2808. (<a
href="https://doi.org/10.1109/TPAMI.2020.2974726">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reliable markerless motion tracking of people participating in a complex group activity from multiple moving cameras is challenging due to frequent occlusions, strong viewpoint and appearance variations, and asynchronous video streams. To solve this problem, reliable association of the same person across distant viewpoints and temporal instances is essential. We present a self-supervised framework to adapt a generic person appearance descriptor to the unlabeled videos by exploiting motion tracking, mutual exclusion constraints, and multi-view geometry. The adapted discriminative descriptor is used in a tracking-by-clustering formulation. We validate the effectiveness of our descriptor learning on WILDTRACK T. Chavdarova et al. , “WILDTRACK: A multi-camera HD dataset for dense unscripted pedestrian detection,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2018, pp. 5030–5039. and three new complex social scenes captured by multiple cameras with up to 60 people “in the wild”. We report significant improvement in association accuracy (up to 18 percent) and stable and coherent 3D human skeleton tracking (5 to 10 times) over the baseline. Using the reconstructed 3D skeletons, we cut the input videos into a multi-angle video where the image of a specified person is shown from the best visible front-facing camera. Our algorithm detects inter-human occlusion to determine the camera switching moment while still maintaining the flow of the action well. Website : http://www.cs.cmu.edu/~ILIM/projects/IM/Association4Tracking .},
  archive      = {J_TPAMI},
  author       = {Minh Vo and Ersin Yumer and Kalyan Sunkavalli and Sunil Hadap and Yaser Sheikh and Srinivasa G. Narasimhan},
  doi          = {10.1109/TPAMI.2020.2974726},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {2794-2808},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Self-supervised multi-view person association and its applications},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Rolling shutter homography and its applications.
<em>TPAMI</em>, <em>43</em>(8), 2780–2793. (<a
href="https://doi.org/10.1109/TPAMI.2020.2977644">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article we study the adaptation of the concept of homography to Rolling Shutter (RS) images. This extension has never been clearly adressed despite the many roles played by the homography matrix in multi-view geometry. We first show that a direct point-to-point relationship on a RS pair can be expressed as a set of 3 to 8 atomic 3x3 matrices depending on the kinematic model used for the instantaneous-motion during image acquisition. We call this group of matrices the RS Homography. We then propose linear solvers for the computation of these matrices using point correspondences. Finally, we derive linear and closed form solutions for two famous problems in computer vision in the case of RS images: image stitching and plane-based relative pose computation. Extensive experiments with both synthetic and real data from public benchmarks show that the proposed methods outperform state-of-art techniques.},
  archive      = {J_TPAMI},
  author       = {Yizhen Lao and Omar Ait-Aider},
  doi          = {10.1109/TPAMI.2020.2977644},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {2780-2793},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Rolling shutter homography and its applications},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Relationship-embedded representation learning for grounding
referring expressions. <em>TPAMI</em>, <em>43</em>(8), 2765–2779. (<a
href="https://doi.org/10.1109/TPAMI.2020.2973983">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Grounding referring expressions in images aims to locate the object instance in an image described by a referring expression. It involves a joint understanding of natural language and image content, and is essential for a range of visual tasks related to human-computer interaction. As a language-to-vision matching task, the core of this problem is to not only extract all the necessary information (i.e., objects and the relationships among them) in both the image and referring expression, but also make full use of context information to align cross-modal semantic concepts in the extracted information. Unfortunately, existing work on grounding referring expressions fails to accurately extract multi-order relationships from the referring expression and associate them with the objects and their related contexts in the image. In this paper, we propose a cross-modal relationship extractor (CMRE) to adaptively highlight objects and relationships (spatial and semantic relations) related to the given expression with a cross-modal attention mechanism, and represent the extracted information as a language-guided visual relation graph. In addition, we propose a Gated Graph Convolutional Network (GGCN) to compute multimodal semantic contexts by fusing information from different modes and propagating multimodal information in the structured relation graph. Experimental results on three common benchmark datasets show that our Cross-Modal Relationship Inference Network, which consists of CMRE and GGCN, significantly surpasses all existing state-of-the-art methods.},
  archive      = {J_TPAMI},
  author       = {Sibei Yang and Guanbin Li and Yizhou Yu},
  doi          = {10.1109/TPAMI.2020.2973983},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {2765-2779},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Relationship-embedded representation learning for grounding referring expressions},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-task deep learning for real-time 3D human pose
estimation and action recognition. <em>TPAMI</em>, <em>43</em>(8),
2752–2764. (<a
href="https://doi.org/10.1109/TPAMI.2020.2976014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human pose estimation and action recognition are related tasks since both problems are strongly dependent on the human body representation and analysis. Nonetheless, most recent methods in the literature handle the two problems separately. In this article, we propose a multi-task framework for jointly estimating 2D or 3D human poses from monocular color images and classifying human actions from video sequences. We show that a single architecture can be used to solve both problems in an efficient way and still achieves state-of-the-art or comparable results at each task while running with a throughput of more than 100 frames per second. The proposed method benefits from high parameters sharing between the two tasks by unifying still images and video clips processing in a single pipeline, allowing the model to be trained with data from different categories simultaneously and in a seamlessly way. Additionally, we provide important insights for end-to-end training the proposed multi-task model by decoupling key prediction parts, which consistently leads to better accuracy on both tasks. The reported results on four datasets (MPII, Human3.6M, Penn Action and NTU RGB+D) demonstrate the effectiveness of our method on the targeted tasks. Our source code and trained weights are publicly available at https://github.com/dluvizon/deephar .},
  archive      = {J_TPAMI},
  author       = {Diogo C. Luvizon and David Picard and Hedi Tabia},
  doi          = {10.1109/TPAMI.2020.2976014},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {2752-2764},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Multi-task deep learning for real-time 3D human pose estimation and action recognition},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Locate, size, and count: Accurately resolving people in
dense crowds via detection. <em>TPAMI</em>, <em>43</em>(8), 2739–2751.
(<a href="https://doi.org/10.1109/TPAMI.2020.2974830">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a detection framework for dense crowd counting and eliminate the need for the prevalent density regression paradigm. Typical counting models predict crowd density for an image as opposed to detecting every person. These regression methods, in general, fail to localize persons accurate enough for most applications other than counting. Hence, we adopt an architecture that locate s every person in the crowd, size s the spotted heads with bounding box and then count s them. Compared to normal object or face detectors, there exist certain unique challenges in designing such a detection system. Some of them are direct consequences of the huge diversity in dense crowds along with the need to predict boxes contiguously. We solve these issues and develop our LSC-CNN model, which can reliably detect heads of people across sparse to dense crowds. LSC-CNN employs a multi-column architecture with top-down feature modulation to better resolve persons and produce refined predictions at multiple resolutions. Interestingly, the proposed training regime requires only point head annotation, but can estimate approximate size information of heads. We show that LSC-CNN not only has superior localization than existing density regressors, but outperforms in counting as well. The code for our approach is available at https://github.com/val-iisc/lsc-cnn .},
  archive      = {J_TPAMI},
  author       = {Deepak Babu Sam and Skand Vishwanath Peri and Mukuntha Narayanan Sundararaman and Amogh Kamath and R. Venkatesh Babu},
  doi          = {10.1109/TPAMI.2020.2974830},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {2739-2751},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Locate, size, and count: Accurately resolving people in dense crowds via detection},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning to adapt invariance in memory for person
re-identification. <em>TPAMI</em>, <em>43</em>(8), 2723–2738. (<a
href="https://doi.org/10.1109/TPAMI.2020.2976933">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work considers the problem of unsupervised domain adaptation in person re-identification (re-ID), which aims to transfer knowledge from the source domain to the target domain. Existing methods are primary to reduce the inter-domain shift between the domains, which however usually overlook the relations among target samples. This paper investigates into the intra-domain variations of the target domain and proposes a novel adaptation framework w.r.t three types of underlying invariance, i.e., Exemplar-Invariance, Camera-Invariance, and Neighborhood-Invariance. Specifically, an exemplar memory is introduced to store features of samples, which can effectively and efficiently enforce the invariance constraints over the global dataset. We further present the Graph-based Positive Prediction (GPP) method to explore reliable neighbors for the target domain, which is built upon the memory and is trained on the source samples. Experiments demonstrate that 1) the three invariance properties are complementary and indispensable for effective domain adaptation, 2) the memory plays a key role in implementing invariance learning and improves the performance with limited extra computation cost, 3) GPP can facilitate the invariance learning and thus significantly improves the results, and 4) our approach produces new state-of-the-art adaptation accuracy on three re-ID large-scale benchmarks.},
  archive      = {J_TPAMI},
  author       = {Zhun Zhong and Liang Zheng and Zhiming Luo and Shaozi Li and Yi Yang},
  doi          = {10.1109/TPAMI.2020.2976933},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {2723-2738},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Learning to adapt invariance in memory for person re-identification},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning on hypergraphs with sparsity. <em>TPAMI</em>,
<em>43</em>(8), 2710–2722. (<a
href="https://doi.org/10.1109/TPAMI.2020.2974746">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hypergraph is a general way of representing high-order relations on a set of objects. It is a generalization of graph, in which only pairwise relations can be represented. It finds applications in various domains where relationships of more than two objects are observed. On a hypergraph, as a generalization of graph, one wishes to learn a smooth function with respect to its topology. A fundamental issue is to find suitable smoothness measures of functions on the nodes of a graph/hypergraph. We show a general framework that generalizes previously proposed smoothness measures and also generates new ones. To address the problem of irrelevant or noisy data, we wish to incorporate sparse learning framework into learning on hypergraphs. We propose sparsely smooth formulations that learn smooth functions and induce sparsity on hypergraphs at both hyperedge and node levels. We show their properties and sparse support recovery results. We conduct experiments to show that our sparsely smooth models are beneficial to learning irrelevant and noisy data, and usually give similar or improved performances compared to dense models.},
  archive      = {J_TPAMI},
  author       = {Canh Hao Nguyen and Hiroshi Mamitsuka},
  doi          = {10.1109/TPAMI.2020.2974746},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {2710-2722},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Learning on hypergraphs with sparsity},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning compressible 360<span
class="math inline"><sup>∘</sup></span>∘ video isomers. <em>TPAMI</em>,
<em>43</em>(8), 2697–2709. (<a
href="https://doi.org/10.1109/TPAMI.2020.2974472">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Standard video encoders developed for conventional narrow field-of-view video are widely applied to 360 ° video as well, with reasonable results. However, while this approach commits arbitrarily to a projection of the spherical frames, we observe that some orientations of a 360 ° video, once projected, are more compressible than others. We introduce an approach to predict the sphere rotation that will yield the maximal compression rate. Given video clips in their original encoding, a convolutional neural network learns the association between a clip&#39;s visual content and its compressibility at different rotations of a cubemap projection. Given a novel video, our learning-based approach efficiently infers the most compressible direction in one shot, without repeated rendering and compression of the source video. We validate our idea on thousands of video clips and multiple popular video codecs. The results show that this untapped dimension of 360 ° compression has substantial potential-“good” rotations are typically 8-18 percent more compressible than bad ones, and our learning approach can predict them reliably 78 percent of the time.},
  archive      = {J_TPAMI},
  author       = {Yu-Chuan Su and Kristen Grauman},
  doi          = {10.1109/TPAMI.2020.2974472},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {2697-2709},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Learning compressible 360$^{\circ }$∘ video isomers},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). LCBM: A multi-view probabilistic model for multi-label
classification. <em>TPAMI</em>, <em>43</em>(8), 2682–2696. (<a
href="https://doi.org/10.1109/TPAMI.2020.2974203">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-label classification is an important research topic in machine learning, for which exploiting label dependencies is an effective modeling principle. Recently, probabilistic models have shown great potential in discovering dependencies among labels. In this paper, motivated by the recent success of multi-view learning to improve the generalization performance, we propose a novel multi-view probabilistic model named latent conditional Bernoulli mixture (LCBM) for multi-label classification. LCBM is a generative model taking features from different views as inputs, and conditional on the latent subspace shared by the views a Bernoulli mixture model is adopted to build label dependencies. Inside each component of the mixture, the labels have a weak correlation which facilitates computational convenience. The mean field variational inference framework is used to carry out approximate posterior inference in the probabilistic model, where we propose a Gaussian mixture variational autoencoder (GMVAE) for effective posterior approximation. We further develop a scalable stochastic training algorithm for efficiently optimizing the model parameters and variational parameters, and derive an efficient prediction procedure based on greedy search. Experimental results on multiple benchmark datasets show that our approach outperforms other state-of-the-art methods under various metrics.},
  archive      = {J_TPAMI},
  author       = {Shiliang Sun and Daoming Zong},
  doi          = {10.1109/TPAMI.2020.2974203},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {2682-2696},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {LCBM: A multi-view probabilistic model for multi-label classification},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Laplacian coordinates: Theory and methods for seeded image
segmentation. <em>TPAMI</em>, <em>43</em>(8), 2665–2681. (<a
href="https://doi.org/10.1109/TPAMI.2020.2974475">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Seeded segmentation methods have gained a lot of attention due to their good performance in fragmenting complex images, easy usability and synergism with graph-based representations. These methods usually rely on sophisticated computational tools whose performance strongly depends on how good the training data reflect a sought image pattern. Moreover, poor adherence to the image contours, lack of unique solution, and high computational cost are other common issues present in most seeded segmentation methods. In this work we introduce Laplacian Coordinates, a quadratic energy minimization framework that tackles the issues above in an effective and mathematically sound manner. The proposed formulation builds upon graph Laplacian operators, quadratic energy functions, and fast minimization schemes to produce highly accurate segmentations. Moreover, the presented energy functions are not prone to local minima, i.e., the solution is guaranteed to be globally optimal, a trait not present in most image segmentation methods. Another key property is that the minimization procedure leads to a constrained sparse linear system of equations, enabling the segmentation of high-resolution images at interactive rates. The effectiveness of Laplacian Coordinates is attested by a comprehensive set of comparisons involving nine state-of-the-art methods and several benchmarks extensively used in the image segmentation literature.},
  archive      = {J_TPAMI},
  author       = {Wallace Casaca and João Paulo Gois and Harlen Costa Batagelo and Gabriel Taubin and Luis Gustavo Nonato},
  doi          = {10.1109/TPAMI.2020.2974475},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {2665-2681},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Laplacian coordinates: Theory and methods for seeded image segmentation},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). From points to parts: 3D object detection from point cloud
with part-aware and part-aggregation network. <em>TPAMI</em>,
<em>43</em>(8), 2647–2664. (<a
href="https://doi.org/10.1109/TPAMI.2020.2977026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D object detection from LiDAR point cloud is a challenging problem in 3D scene understanding and has many practical applications. In this paper, we extend our preliminary work PointRCNN to a novel and strong point-cloud-based 3D object detection framework, the part-aware and aggregation neural network (Part-A 2 net). The whole framework consists of the part-aware stage and the part-aggregation stage. First, the part-aware stage for the first time fully utilizes free-of-charge part supervisions derived from 3D ground-truth boxes to simultaneously predict high quality 3D proposals and accurate intra-object part locations. The predicted intra-object part locations within the same proposal are grouped by our new-designed RoI-aware point cloud pooling module, which results in an effective representation to encode the geometry-specific features of each 3D proposal. Then the part-aggregation stage learns to re-score the box and refine the box location by exploring the spatial relationship of the pooled intra-object part locations. Extensive experiments are conducted to demonstrate the performance improvements from each component of our proposed framework. Our Part-A 2 net outperforms all existing 3D detection methods and achieves new state-of-the-art on KITTI 3D object detection dataset by utilizing only the LiDAR point cloud data.},
  archive      = {J_TPAMI},
  author       = {Shaoshuai Shi and Zhe Wang and Jianping Shi and Xiaogang Wang and Hongsheng Li},
  doi          = {10.1109/TPAMI.2020.2977026},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {2647-2664},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {From points to parts: 3D object detection from point cloud with part-aware and part-aggregation network},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Efficient and effective regularized incomplete multi-view
clustering. <em>TPAMI</em>, <em>43</em>(8), 2634–2646. (<a
href="https://doi.org/10.1109/TPAMI.2020.2974828">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Incomplete multi-view clustering (IMVC) optimally combines multiple pre-specified incomplete views to improve clustering performance. Among various excellent solutions, the recently proposed multiple kernel k-means with incomplete kernels (MKKM-IK) forms a benchmark, which redefines IMVC as a joint optimization problem where the clustering and kernel matrix imputation tasks are alternately performed until convergence. Though demonstrating promising performance in various applications, we observe that the manner of kernel matrix imputation in MKKM-IK would incur intensive computational and storage complexities, over-complicated optimization and limitedly improved clustering performance. In this paper, we first propose an Efficient and Effective Incomplete Multi-view Clustering (EE-IMVC) algorithm to address these issues. Instead of completing the incomplete kernel matrices, EE-IMVC proposes to impute each incomplete base matrix generated by incomplete views with a learned consensus clustering matrix. Moreover, we further improve this algorithm by incorporating prior knowledge to regularize the learned consensus clustering matrix. Two three-step iterative algorithms are carefully developed to solve the resultant optimization problems with linear computational complexity, and their convergence is theoretically proven. After that, we theoretically study the generalization bound of the proposed algorithms. Furthermore, we conduct comprehensive experiments to study the proposed algorithms in terms of clustering accuracy, evolution of the learned consensus clustering matrix and the convergence. As indicated, our algorithms deliver their effectiveness by significantly and consistently outperforming some state-of-the-art ones.},
  archive      = {J_TPAMI},
  author       = {Xinwang Liu and Miaomiao Li and Chang Tang and Jingyuan Xia and Jian Xiong and Li Liu and Marius Kloft and En Zhu},
  doi          = {10.1109/TPAMI.2020.2974828},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {2634-2646},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Efficient and effective regularized incomplete multi-view clustering},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DSNet: Joint semantic learning for object detection in
inclement weather conditions. <em>TPAMI</em>, <em>43</em>(8), 2623–2633.
(<a href="https://doi.org/10.1109/TPAMI.2020.2977911">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the past half of the decade, object detection approaches based on the convolutional neural network have been widely studied and successfully applied in many computer vision applications. However, detecting objects in inclement weather conditions remains a major challenge because of poor visibility. In this article, we address the object detection problem in the presence of fog by introducing a novel dual-subnet network (DSNet) that can be trained end-to-end and jointly learn three tasks: visibility enhancement, object classification, and object localization. DSNet attains complete performance improvement by including two subnetworks: detection subnet and restoration subnet. We employ RetinaNet as a backbone network (also called detection subnet), which is responsible for learning to classify and locate objects. The restoration subnet is designed by sharing feature extraction layers with the detection subnet and adopting a feature recovery (FR) module for visibility enhancement. Experimental results show that our DSNet achieved 50.84 percent mean average precision (mAP) on a synthetic foggy dataset that we composed and 41.91 percent mAP on a public natural foggy dataset (Foggy Driving dataset), outperforming many state-of-the-art object detectors and combination models between dehazing and detection methods while maintaining a high speed.},
  archive      = {J_TPAMI},
  author       = {Shih-Chia Huang and Trung-Hieu Le and Da-Wei Jaw},
  doi          = {10.1109/TPAMI.2020.2977911},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {2623-2633},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {DSNet: Joint semantic learning for object detection in inclement weather conditions},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Depth sensing by near-infrared light absorption in water.
<em>TPAMI</em>, <em>43</em>(8), 2611–2622. (<a
href="https://doi.org/10.1109/TPAMI.2020.2973986">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a novel depth recovery method based on light absorption in water. Water absorbs light at almost all wavelengths whose absorption coefficient is related to the wavelength. Based on the Beer-Lambert model, we introduce a bispectral depth recovery method that leverages the light absorption difference between two near-infrared wavelengths captured with a distant point source and orthographic cameras. Through extensive analysis, we show that accurate depth can be recovered irrespective of the surface texture and reflectance, and introduce algorithms to correct for nonidealities of a practical implementation including tilted light source and camera placement, nonideal bandpass filters and the perspective effect of the camera with a diverging point light source. We construct a coaxial bispectral depth imaging system using low-cost off-the-shelf hardware and demonstrate its use for recovering the shapes of complex and dynamic objects in water. We also present a trispectral variant to further improve robustness to extremely challenging surface reflectance. Experimental results validate the theory and practical implementation of this novel depth recovery paradigm, which we refer to as shape from water.},
  archive      = {J_TPAMI},
  author       = {Yuta Asano and Yinqiang Zheng and Ko Nishino and Imari Sato},
  doi          = {10.1109/TPAMI.2020.2973986},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {2611-2622},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Depth sensing by near-infrared light absorption in water},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DENAO: Monocular depth estimation network with auxiliary
optical flow. <em>TPAMI</em>, <em>43</em>(8), 2598–2610. (<a
href="https://doi.org/10.1109/TPAMI.2020.2977021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimating depth from multi-view images captured by a localized monocular camera is an essential task in computer vision and robotics. In this study, we demonstrate that learning a convolutional neural network (CNN) for depth estimation with an auxiliary optical flow network and the epipolar geometry constraint can greatly benefit the depth estimation task and in turn yield large improvements in both accuracy and speed. Our architecture is composed of two tightly-coupled encoder-decoder networks, i.e., an optical flow net and a depth net, the core part being a list of exchange blocks between the two nets and an epipolar feature layer in the optical flow net to improve predictions of both depth and optical flow. Our architecture allows to input arbitrary number of multiview images with a linearly growing time cost for optical flow and depth estimation. Experimental result on five public datasets demonstrates that our method, named DENAO, runs at 38.46fps on a single Nvidia TITAN Xp GPU which is 5.15X ~ 142X faster than the state-of-the-art depth estimation methods Meanwhile, our DENAO can concurrently output predictions of both depth and optical flow, and performs on par with or outperforms the state-of-the-art depth estimation methods and optical flow methods.},
  archive      = {J_TPAMI},
  author       = {Jingyu Chen and Xin Yang and Qizeng Jia and Chunyuan Liao},
  doi          = {10.1109/TPAMI.2020.2977021},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {2598-2610},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {DENAO: Monocular depth estimation network with auxiliary optical flow},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep CNNs meet global covariance pooling: Better
representation and generalization. <em>TPAMI</em>, <em>43</em>(8),
2582–2597. (<a
href="https://doi.org/10.1109/TPAMI.2020.2974833">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compared with global average pooling in existing deep convolutional neural networks (CNNs), global covariance pooling can capture richer statistics of deep features, having potential for improving representation and generalization abilities of deep CNNs. However, integration of global covariance pooling into deep CNNs brings two challenges: (1) robust covariance estimation given deep features of high dimension and small sample size; (2) appropriate usage of geometry of covariances. To address these challenges, we propose a global Matrix Power Normalized COVariance (MPN-COV) Pooling. Our MPN-COV conforms to a robust covariance estimator, very suitable for scenario of high dimension and small sample size. It can also be regarded as Power-Euclidean metric between covariances, effectively exploiting their geometry. Furthermore, a global Gaussian embedding network is proposed to incorporate first-order statistics into MPN-COV. For fast training of MPN-COV networks, we implement an iterative matrix square root normalization, avoiding GPU unfriendly eigen-decomposition inherent in MPN-COV. Additionally, progressive 1×1 convolutions and group convolution are introduced to compress covariance representations. The proposed methods are highly modular, readily plugged into existing deep CNNs. Extensive experiments are conducted on large-scale object classification, scene categorization, fine-grained visual recognition and texture classification, showing our methods outperform the counterparts and obtain state-of-the-art performance.},
  archive      = {J_TPAMI},
  author       = {Qilong Wang and Jiangtao Xie and Wangmeng Zuo and Lei Zhang and Peihua Li},
  doi          = {10.1109/TPAMI.2020.2974833},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {2582-2597},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Deep CNNs meet global covariance pooling: Better representation and generalization},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). ChannelNets: Compact and efficient convolutional neural
networks via channel-wise convolutions. <em>TPAMI</em>, <em>43</em>(8),
2570–2581. (<a
href="https://doi.org/10.1109/TPAMI.2020.2975796">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks (CNNs) have shown great capability of solving various artificial intelligence tasks. However, the increasing model size has raised challenges in employing them in resource-limited applications. In this work, we propose to compress deep models by using channel-wise convolutions, which replace dense connections among feature maps with sparse ones in CNNs. Based on this novel operation, we build light-weight CNNs known as ChannelNets. ChannelNets use three instances of channel-wise convolutions; namely group channel-wise convolutions, depth-wise separable channel-wise convolutions, and the convolutional classification layer. Compared to prior CNNs designed for mobile devices, ChannelNets achieve a significant reduction in terms of the number of parameters and computational cost without loss in accuracy. Notably, our work represents an attempt to compress the fully-connected classification layer, which usually accounts for about 25 percent of total parameters in compact CNNs. Along this new direction, we investigate the behavior of our proposed convolutional classification layer and conduct detailed analysis. Based on our in-depth analysis, we further propose convolutional classification layers without weight-sharing. This new classification layer achieves a good trade-off between fully-connected classification layers and the convolutional classification layer. Experimental results on the ImageNet dataset demonstrate that ChannelNets achieve consistently better performance compared to prior methods.},
  archive      = {J_TPAMI},
  author       = {Hongyang Gao and Zhengyang Wang and Lei Cai and Shuiwang Ji},
  doi          = {10.1109/TPAMI.2020.2975796},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {2570-2581},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {ChannelNets: Compact and efficient convolutional neural networks via channel-wise convolutions},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A lightweight optical flow CNN —revisiting data fidelity and
regularization. <em>TPAMI</em>, <em>43</em>(8), 2555–2569. (<a
href="https://doi.org/10.1109/TPAMI.2020.2976928">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over four decades, the majority addresses the problem of optical flow estimation using variational methods. With the advance of machine learning, some recent works have attempted to address the problem using convolutional neural network (CNN) and have showed promising results. FlowNet2 [1] , the state-of-the-art CNN, requires over 160M parameters to achieve accurate flow estimation. Our LiteFlowNet2 outperforms FlowNet2 on Sintel and KITTI benchmarks, while being 25.3 times smaller in the model size and 3.1 times faster in the running speed. LiteFlowNet2 is built on the foundation laid by conventional methods and resembles the corresponding roles as data fidelity and regularization in variational methods. We compute optical flow in a spatial-pyramid formulation as SPyNet [2] but through a novel lightweight cascaded flow inference. It provides high flow estimation accuracy through early correction with seamless incorporation of descriptor matching. Flow regularization is used to ameliorate the issue of outliers and vague flow boundaries through feature-driven local convolutions. Our network also owns an effective structure for pyramidal feature extraction and embraces feature warping rather than image warping as practiced in FlowNet2 and SPyNet. Comparing to LiteFlowNet [3] , LiteFlowNet2 improves the optical flow accuracy on Sintel Clean by 23.3 percent, Sintel Final by 12.8 percent, KITTI 2012 by 19.6 percent, and KITTI 2015 by 18.8 percent, while being 2.2 times faster. Our network protocol and trained models are made publicly available on https://github.com/twhui/LiteFlowNet2.},
  archive      = {J_TPAMI},
  author       = {Tak-Wai Hui and Xiaoou Tang and Chen Change Loy},
  doi          = {10.1109/TPAMI.2020.2976928},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {2555-2569},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {A lightweight optical flow CNN —Revisiting data fidelity and regularization},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A generalized earley parser for human activity parsing and
prediction. <em>TPAMI</em>, <em>43</em>(8), 2538–2554. (<a
href="https://doi.org/10.1109/TPAMI.2020.2976971">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detection, parsing, and future predictions on sequence data (e.g., videos) require the algorithms to capture non-Markovian and compositional properties of high-level semantics. Context-free grammars are natural choices to capture such properties, but traditional grammar parsers (e.g., Earley parser) only take symbolic sentences as inputs. In this paper, we generalize the Earley parser to parse sequence data which is neither segmented nor labeled. Given the output of an arbitrary probabilistic classifier, this generalized Earley parser finds the optimal segmentation and labels in the language defined by the input grammar. Based on the parsing results, it makes top-down future predictions. The proposed method is generic, principled, and widely applicable. Experiment results clearly show the benefit of our method for both human activity parsing and prediction on three video datasets.},
  archive      = {J_TPAMI},
  author       = {Siyuan Qi and Baoxiong Jia and Siyuan Huang and Ping Wei and Song-Chun Zhu},
  doi          = {10.1109/TPAMI.2020.2976971},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {2538-2554},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {A generalized earley parser for human activity parsing and prediction},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A bayesian approach to recurrence in neural networks.
<em>TPAMI</em>, <em>43</em>(8), 2527–2537. (<a
href="https://doi.org/10.1109/TPAMI.2020.2976978">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We begin by reiterating that common neural network activation functions have simple Bayesian origins. In this spirit, we go on to show that Bayes&#39;s theorem also implies a simple recurrence relation; this leads to a Bayesian recurrent unit with a prescribed feedback formulation. We show that introduction of a context indicator leads to a variable feedback that is similar to the forget mechanism in conventional recurrent units. A similar approach leads to a probabilistic input gate. The Bayesian formulation leads naturally to the two pass algorithm of the Kalman smoother or forward-backward algorithm, meaning that inference naturally depends upon future inputs as well as past ones. Experiments on speech recognition confirm that the resulting architecture can perform as well as a bidirectional recurrent network with the same number of parameters as a unidirectional one. Further, when configured explicitly bidirectionally, the architecture can exceed the performance of a conventional bidirectional recurrence.},
  archive      = {J_TPAMI},
  author       = {Philip N. Garner and Sibo Tong},
  doi          = {10.1109/TPAMI.2020.2976978},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {8},
  pages        = {2527-2537},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {A bayesian approach to recurrence in neural networks},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021d). [Front inside cover]. <em>TPAMI</em>, <em>43</em>(7), C4.
(<a href="https://doi.org/10.1109/TPAMI.2021.3080156">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Provides a listing of current committee members and society officers.},
  archive      = {J_TPAMI},
  doi          = {10.1109/TPAMI.2021.3080156},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {C4},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {[Front inside cover]},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021e). [Front inside cover]. <em>TPAMI</em>, <em>43</em>(7), C3.
(<a href="https://doi.org/10.1109/TPAMI.2021.3080158">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Provides a listing of current committee members and society officers.},
  archive      = {J_TPAMI},
  doi          = {10.1109/TPAMI.2021.3080158},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {C3},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {[Front inside cover]},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). ComputingEdge. <em>TPAMI</em>, <em>43</em>(7), 2526. (<a
href="https://doi.org/10.1109/TPAMI.2021.3082379">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advertisement, IEEE. ComputingEdge: Your one-stop resource for industry hot topics, technical overviews, and in-depth articles.},
  archive      = {J_TPAMI},
  doi          = {10.1109/TPAMI.2021.3082379},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {2526},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {ComputingEdge},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Computing in science &amp; engineering. <em>TPAMI</em>,
<em>43</em>(7), 2525. (<a
href="https://doi.org/10.1109/TPAMI.2021.3082378">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advertisement, IEEE Computer Society.},
  archive      = {J_TPAMI},
  doi          = {10.1109/TPAMI.2021.3082378},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {2525},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Computing in science &amp; engineering},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). IEEE computer society jobs board. <em>TPAMI</em>,
<em>43</em>(7), 2524. (<a
href="https://doi.org/10.1109/TPAMI.2021.3082376">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advertisement, IEEE Computer Society.},
  archive      = {J_TPAMI},
  doi          = {10.1109/TPAMI.2021.3082376},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {2524},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {IEEE computer society jobs board},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Zero and few shot learning with semantic feature synthesis
and competitive learning. <em>TPAMI</em>, <em>43</em>(7), 2510–2523. (<a
href="https://doi.org/10.1109/TPAMI.2020.2965534">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zero-shot learning (ZSL) is made possible by learning a projection function between a feature space and a semantic space (e.g., an attribute space). Key to ZSL is thus to learn a projection that is robust against the often large domain gap between the seen and unseen class domains. In this work, this is achieved by unseen class data synthesis and robust projection function learning. Specifically, a novel semantic data synthesis strategy is proposed, by which semantic class prototypes (e.g., attribute vectors) are used to simply perturb seen class data for generating unseen class ones. As in any data synthesis/hallucination approach, there are ambiguities and uncertainties on how well the synthesised data can capture the targeted unseen class data distribution. To cope with this, the second contribution of this work is a novel projection learning model termed competitive bidirectional projection learning (BPL) designed to best utilise the ambiguous synthesised data. Specifically, we assume that each synthesised data point can belong to any unseen class; and the most likely two class candidates are exploited to learn a robust projection function in a competitive fashion. As a third contribution, we show that the proposed ZSL model can be easily extended to few-shot learning (FSL) by again exploiting semantic (class prototype guided) feature synthesis and competitive BPL. Extensive experiments show that our model achieves the state-of-the-art results on both problems.},
  archive      = {J_TPAMI},
  author       = {Jiechao Guan and Zhiwu Lu and Tao Xiang and Aoxue Li and An Zhao and Ji-Rong Wen},
  doi          = {10.1109/TPAMI.2020.2965534},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {2510-2523},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Zero and few shot learning with semantic feature synthesis and competitive learning},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Semi-supervised multi-view deep discriminant representation
learning. <em>TPAMI</em>, <em>43</em>(7), 2496–2509. (<a
href="https://doi.org/10.1109/TPAMI.2020.2973634">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning an expressive representation from multi-view data is a key step in various real-world applications. In this paper, we propose a semi-supervised multi-view deep discriminant representation learning (SMDDRL) approach. Unlike existing joint or alignment multi-view representation learning methods that cannot simultaneously utilize the consensus and complementary properties of multi-view data to learn inter-view shared and intra-view specific representations, SMDDRL comprehensively exploits the consensus and complementary properties as well as learns both shared and specific representations by employing the shared and specific representation learning network. Unlike existing shared and specific multi-view representation learning methods that ignore the redundancy problem in representation learning, SMDDRL incorporates the orthogonality and adversarial similarity constraints to reduce the redundancy of learned representations. Moreover, to exploit the information contained in unlabeled data, we design a semi-supervised learning framework by combining deep metric learning and density clustering. Experimental results on three typical multi-view learning tasks, i.e., webpage classification, image classification, and document classification demonstrate the effectiveness of the proposed approach.},
  archive      = {J_TPAMI},
  author       = {Xiaodong Jia and Xiao-Yuan Jing and Xiaoke Zhu and Songcan Chen and Bo Du and Ziyun Cai and Zhenyu He and Dong Yue},
  doi          = {10.1109/TPAMI.2020.2973634},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {2496-2509},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Semi-supervised multi-view deep discriminant representation learning},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Residual dense network for image restoration.
<em>TPAMI</em>, <em>43</em>(7), 2480–2495. (<a
href="https://doi.org/10.1109/TPAMI.2020.2968521">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, deep convolutional neural network (CNN) has achieved great success for image restoration (IR) and provided hierarchical features at the same time. However, most deep CNN based IR models do not make full use of the hierarchical features from the original low-quality images; thereby, resulting in relatively-low performance. In this work, we propose a novel and efficient residual dense network (RDN) to address this problem in IR, by making a better tradeoff between efficiency and effectiveness in exploiting the hierarchical features from all the convolutional layers. Specifically, we propose residual dense block (RDB) to extract abundant local features via densely connected convolutional layers. RDB further allows direct connections from the state of preceding RDB to all the layers of current RDB, leading to a contiguous memory mechanism. To adaptively learn more effective features from preceding and current local features and stabilize the training of wider network, we proposed local feature fusion in RDB. After fully obtaining dense local features, we use global feature fusion to jointly and adaptively learn global hierarchical features in a holistic way. We demonstrate the effectiveness of RDN with several representative IR applications, single image super-resolution, Gaussian image denoising, image compression artifact reduction, and image deblurring. Experiments on benchmark and real-world datasets show that our RDN achieves favorable performance against state-of-the-art methods for each IR task quantitatively and visually.},
  archive      = {J_TPAMI},
  author       = {Yulun Zhang and Yapeng Tian and Yu Kong and Bineng Zhong and Yun Fu},
  doi          = {10.1109/TPAMI.2020.2968521},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {2480-2495},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Residual dense network for image restoration},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Real-time nonparametric anomaly detection in
high-dimensional settings. <em>TPAMI</em>, <em>43</em>(7), 2463–2479.
(<a href="https://doi.org/10.1109/TPAMI.2020.2970410">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Timely detection of abrupt anomalies is crucial for real-time monitoring and security of modern systems producing high-dimensional data. With this goal, we propose effective and scalable algorithms. Proposed algorithms are nonparametric as both the nominal and anomalous multivariate data distributions are assumed unknown. We extract useful univariate summary statistics and perform anomaly detection in a single-dimensional space. We model anomalies as persistent outliers and propose to detect them via a cumulative sum-like algorithm. In case the observed data have a low intrinsic dimensionality, we find a submanifold in which the nominal data are embedded and evaluate whether the sequentially acquired data persistently deviate from the nominal submanifold. Further, in the general case, we determine an acceptance region for nominal data via Geometric Entropy Minimization and evaluate whether the sequentially observed data persistently fall outside the acceptance region. We provide an asymptotic lower bound and an asymptotic approximation for the average false alarm period of the proposed algorithm. Moreover, we provide a sufficient condition to asymptotically guarantee that the decision statistic of the proposed algorithm does not diverge in the absence of anomalies. Experiments illustrate the effectiveness of the proposed schemes in quick and accurate anomaly detection in high-dimensional settings.},
  archive      = {J_TPAMI},
  author       = {Mehmet Necip Kurt and Yasin Yılmaz and Xiaodong Wang},
  doi          = {10.1109/TPAMI.2020.2970410},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {2463-2479},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Real-time nonparametric anomaly detection in high-dimensional settings},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Physics-based generative adversarial models for image
restoration and beyond. <em>TPAMI</em>, <em>43</em>(7), 2449–2462. (<a
href="https://doi.org/10.1109/TPAMI.2020.2969348">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present an algorithm to directly solve numerous image restoration problems (e.g., image deblurring, image dehazing, and image deraining). These problems are ill-posed, and the common assumptions for existing methods are usually based on heuristic image priors. In this paper, we show that these problems can be solved by generative models with adversarial learning. However, a straightforward formulation based on a straightforward generative adversarial network (GAN) does not perform well in these tasks, and some structures of the estimated images are usually not preserved well. Motivated by an interesting observation that the estimated results should be consistent with the observed inputs under the physics models, we propose an algorithm that guides the estimation process of a specific task within the GAN framework. The proposed model is trained in an end-to-end fashion and can be applied to a variety of image restoration and low-level vision problems. Extensive experiments demonstrate that the proposed method performs favorably against state-of-the-art algorithms.},
  archive      = {J_TPAMI},
  author       = {Jinshan Pan and Jiangxin Dong and Yang Liu and Jiawei Zhang and Jimmy Ren and Jinhui Tang and Yu-Wing Tai and Ming-Hsuan Yang},
  doi          = {10.1109/TPAMI.2020.2969348},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {2449-2462},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Physics-based generative adversarial models for image restoration and beyond},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Perceptual texture similarity estimation: An evaluation of
computational features. <em>TPAMI</em>, <em>43</em>(7), 2429–2448. (<a
href="https://doi.org/10.1109/TPAMI.2020.2964533">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimation of texture similarity is fundamental to many material recognition tasks. This study uses fine-grained human perceptual similarity ground-truth to provide a comprehensive evaluation of 51 texture feature sets. We conduct two types of evaluation and both show that these features do not estimate similarity well when compared against human agreement rates, but that performances are improved when the features are combined using a Random Forest. Using a simple two-stage statistical model we show that few of the features capture long-range aperiodic relationships. We perform two psychophysical experiments which indicate that long-range interactions do provide humans with important cues for estimating texture similarity. This motivates an extension of the study to include Convolutional Neural Networks (CNNs) as they enable arbitrary features of large spatial extent to be learnt. Our conclusions derived from the use of two pre-trained CNNs are: that the large spatial extent exploited by the networks&#39; top convolutional and first fully-connected layers, together with the use of large numbers of filters, confers significant advantage for estimation of perceptual texture similarity.},
  archive      = {J_TPAMI},
  author       = {Xinghui Dong and Junyu Dong and Mike J. Chantler},
  doi          = {10.1109/TPAMI.2020.2964533},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {2429-2448},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Perceptual texture similarity estimation: An evaluation of computational features},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Paying attention to video object pattern understanding.
<em>TPAMI</em>, <em>43</em>(7), 2413–2428. (<a
href="https://doi.org/10.1109/TPAMI.2020.2966453">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper conducts a systematic study on the role of visual attention in video object pattern understanding. By elaborately annotating three popular video segmentation datasets (DAVIS 16 16, Youtube-Objects, and SegTrack V 2 ) with dynamic eye-tracking data in the unsupervised video object segmentation (UVOS) setting. For the first time, we quantitatively verified the high consistency of visual attention behavior among human observers, and found strong correlation between human attention and explicit primary object judgments during dynamic, task-driven viewing. Such novel observations provide an in-depth insight of the underlying rationale behind video object pattens. Inspired by these findings, we decouple UVOS into two sub-tasks: UVOS-driven Dynamic Visual Attention Prediction (DVAP) in spatiotemporal domain, and Attention-Guided Object Segmentation (AGOS) in spatial domain. Our UVOS solution enjoys three major advantages: 1) modular training without using expensive video segmentation annotations, instead, using more affordable dynamic fixation data to train the initial video attention module and using existing fixation-segmentation paired static/image data to train the subsequent segmentation module; 2) comprehensive foreground understanding through multi-source learning; and 3) additional interpretability from the biologically-inspired and assessable attention. Experiments on four popular benchmarks show that, even without using expensive video object mask annotations, our model achieves compelling performance compared with state-of-the-arts and enjoys fast processing speed (10 fps on a single GPU). Our collected eye-tracking data and algorithm implementations have been made publicly available at https://github.com/wenguanwang/AGS.},
  archive      = {J_TPAMI},
  author       = {Wenguan Wang and Jianbing Shen and Xiankai Lu and Steven C. H. Hoi and Haibin Ling},
  doi          = {10.1109/TPAMI.2020.2966453},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {2413-2428},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Paying attention to video object pattern understanding},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Novelty detection and online learning for chunk data
streams. <em>TPAMI</em>, <em>43</em>(7), 2400–2412. (<a
href="https://doi.org/10.1109/TPAMI.2020.2965531">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Datastream analysis aims at extracting discriminative information for classification from continuously incoming samples. It is extremely challenging to detect novel data while incrementally updating the model efficiently and stably, especially for high-dimensional and/or large-scale data streams. This paper proposes an efficient framework for novelty detection and incremental learning for unlabeled chunk data streams. First, an accurate factorization-free kernel discriminative analysis (FKDA-X) is put forward through solving a linear system in the kernel space. FKDA-X produces a Reproducing Kernel Hilbert Space (RKHS), in which unlabeled chunk data can be detected and classified by multiple known-classes in a single decision model with a deterministic classification boundary. Moreover, based on FKDA-X, two optimal methods FKDA-CX and FKDA-C are proposed. FKDA-CX uses the micro-cluster centers of original data as the input to achieve excellent performance in novelty detection. FKDA-C and incremental FKDA-C (IFKDA-C) using the class centers of original data as their input have extremely fast speed in online learning. Theoretical analysis and experimental validation on under-sampled and large-scale real-world datasets demonstrate that the proposed algorithms make it possible to learn unlabeled chunk data streams with significantly lower computational costs and comparable accuracies than the state-of-the-art approaches.},
  archive      = {J_TPAMI},
  author       = {Yi Wang and Yi Ding and Xiangjian He and Xin Fan and Chi Lin and Fengqi Li and Tianzhu Wang and Zhongxuan Luo and Jiebo Luo},
  doi          = {10.1109/TPAMI.2020.2965531},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {2400-2412},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Novelty detection and online learning for chunk data streams},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). LayoutGAN: Synthesizing graphic layouts with
vector-wireframe adversarial networks. <em>TPAMI</em>, <em>43</em>(7),
2388–2399. (<a
href="https://doi.org/10.1109/TPAMI.2019.2963663">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Layout is important for graphic design and scene generation. We propose a novel Generative Adversarial Network, called LayoutGAN, that synthesizes layouts by modeling geometric relations of different types of 2D elements. The generator of LayoutGAN takes as input a set of randomly-placed 2D graphic elements, represented by vectors and uses self-attention modules to refine their labels and geometric parameters jointly to produce a realistic layout. Accurate alignment is critical for good layouts. We, thus, propose a novel differentiable wireframe rendering layer that maps the generated layout to a wireframe image, upon which a CNN-based discriminator is used to optimize the layouts in image space. We validate the effectiveness of LayoutGAN in various experiments including MNIST digit generation, document layout generation, clipart abstract scene generation, tangram graphic design, mobile app layout design, and webpage layout optimization from hand-drawn sketches.},
  archive      = {J_TPAMI},
  author       = {Jianan Li and Jimei Yang and Aaron Hertzmann and Jianming Zhang and Tingfa Xu},
  doi          = {10.1109/TPAMI.2019.2963663},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {2388-2399},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {LayoutGAN: Synthesizing graphic layouts with vector-wireframe adversarial networks},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Explicit filterbank learning for neural image style transfer
and image processing. <em>TPAMI</em>, <em>43</em>(7), 2373–2387. (<a
href="https://doi.org/10.1109/TPAMI.2020.2964205">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image style transfer is to re-render the content of one image with the style of another. Most existing methods couple content and style information in their network structures and hyper-parameters, and learn it as a black-box. For better understanding, this paper aims to provide a new explicit decoupled perspective. Specifically, we propose StyleBank, which is composed of multiple convolution filter banks and each filter bank explicitly represents one style. To transfer an image to a specific style, the corresponding filter bank is operated on the intermediate feature produced by a single auto-encoder. The StyleBank and the auto-encoder are jointly learnt in such a way that the auto-encoder does not encode any style information. This explicit representation also enables us to conduct incremental learning to add a new style and fuse styles at not only the image level, but also the region level. Our method is the first style transfer network that links back to traditional texton mapping methods, and provides new understanding on neural style transfer. We further apply this general filterbank learning idea to two different multi-parameter image processing tasks: edge-aware image smoothing and denoising. Experiments demonstrate that it can achieve comparable results to its single parameter setting counterparts.},
  archive      = {J_TPAMI},
  author       = {Dongdong Chen and Lu Yuan and Jing Liao and Nenghai Yu and Gang Hua},
  doi          = {10.1109/TPAMI.2020.2964205},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {2373-2387},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Explicit filterbank learning for neural image style transfer and image processing},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Domain stylization: A fast covariance matching framework
towards domain adaptation. <em>TPAMI</em>, <em>43</em>(7), 2360–2372.
(<a href="https://doi.org/10.1109/TPAMI.2020.2969421">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generating computer graphics (CG) rendered synthetic images has been widely used to create simulation environments for robotics/autonomous driving and generate labeled data. Yet, the problem of training models purely with synthetic data remains challenging due to the considerable domain gaps caused by current limitations on rendering. In this paper, we propose a simple yet effective domain adaptation framework towards closing such gap at image level. Unlike many GAN-based approaches, our method aims to match the covariance of the universal feature embeddings across domains, making the adaptation a fast, convenient step and avoiding the need for potentially difficult GAN training. To align domains more precisely, we further propose a conditional covariance matching framework which iteratively estimates semantic segmentation regions and conditionally matches the class-wise feature covariance given the segmentation regions. We demonstrate that both tasks can mutually refine and considerably improve each other, leading to state-of-the-art domain adaptation results. Extensive experiments under multiple synthetic-to-real settings show that our approach exceeds the performance of latest domain adaptation approaches. In addition, we offer a quantitative analysis where our framework shows considerable reduction in Frechet Inception distance between source and target domains, demonstrating the effectiveness of this work in bridging the synthetic-to-real domain gap.},
  archive      = {J_TPAMI},
  author       = {Aysegul Dundar and Ming-Yu Liu and Zhiding Yu and Ting-Chun Wang and John Zedlewski and Jan Kautz},
  doi          = {10.1109/TPAMI.2020.2969421},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {2360-2372},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Domain stylization: A fast covariance matching framework towards domain adaptation},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dense cross-modal correspondence estimation with the deep
self-correlation descriptor. <em>TPAMI</em>, <em>43</em>(7), 2345–2359.
(<a href="https://doi.org/10.1109/TPAMI.2020.2965528">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present the deep self-correlation (DSC) descriptor for establishing dense correspondences between images taken under different imaging modalities, such as different spectral ranges or lighting conditions. We encode local self-similar structure in a pyramidal manner that yields both more precise localization ability and greater robustness to non-rigid image deformations. Specifically, DSC first computes multiple self-correlation surfaces with randomly sampled patches over a local support window, and then builds pyramidal self-correlation surfaces through average pooling on the surfaces. The feature responses on the self-correlation surfaces are then encoded through spatial pyramid pooling in a log-polar configuration. To better handle geometric variations such as scale and rotation, we additionally propose the geometry-invariant DSC (GI-DSC) that leverages multi-scale self-correlation computation and canonical orientation estimation. In contrast to descriptors based on deep convolutional neural networks (CNNs), DSC and GI-DSC are training-free (i.e., handcrafted descriptors), are robust to cross-modality, and generalize well to various modality variations. Extensive experiments demonstrate the state-of-the-art performance of DSC and GI-DSC on challenging cases of cross-modal image pairs having photometric and/or geometric variations.},
  archive      = {J_TPAMI},
  author       = {Seungryong Kim and Dongbo Min and Stephen Lin and Kwanghoon Sohn},
  doi          = {10.1109/TPAMI.2020.2965528},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {2345-2359},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Dense cross-modal correspondence estimation with the deep self-correlation descriptor},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep residual correction network for partial domain
adaptation. <em>TPAMI</em>, <em>43</em>(7), 2329–2344. (<a
href="https://doi.org/10.1109/TPAMI.2020.2964173">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep domain adaptation methods have achieved appealing performance by learning transferable representations from a well-labeled source domain to a different but related unlabeled target domain. Most existing works assume source and target data share the identical label space, which is often difficult to be satisfied in many real-world applications. With the emergence of big data, there is a more practical scenario called partial domain adaptation, where we are always accessible to a more large-scale source domain while working on a relative small-scale target domain. In this case, the conventional domain adaptation assumption should be relaxed, and the target label space tends to be a subset of the source label space. Intuitively, reinforcing the positive effects of the most relevant source subclasses and reducing the negative impacts of irrelevant source subclasses are of vital importance to address partial domain adaptation challenge. This paper proposes an efficiently-implemented Deep Residual Correction Network (DRCN) by plugging one residual block into the source network along with the task-specific feature layer, which effectively enhances the adaptation from source to target and explicitly weakens the influence from the irrelevant source classes. Specifically, the plugged residual block, which consists of several fully-connected layers, could deepen basic network and boost its feature representation capability correspondingly. Moreover, we design a weighted class-wise domain alignment loss to couple two domains by matching the feature distributions of shared classes between source and target. Comprehensive experiments on partial, traditional and fine-grained cross-domain visual recognition demonstrate that DRCN is superior to the competitive deep domain adaptation approaches.},
  archive      = {J_TPAMI},
  author       = {Shuang Li and Chi Harold Liu and Qiuxia Lin and Qi Wen and Limin Su and Gao Huang and Zhengming Ding},
  doi          = {10.1109/TPAMI.2020.2964173},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {2329-2344},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Deep residual correction network for partial domain adaptation},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). BlockQNN: Efficient block-wise neural network architecture
generation. <em>TPAMI</em>, <em>43</em>(7), 2314–2328. (<a
href="https://doi.org/10.1109/TPAMI.2020.2969193">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks have gained a remarkable success in computer vision. However, most popular network architectures are hand-crafted and usually require expertise and elaborate design. In this paper, we provide a block-wise network generation pipeline called BlockQNN which automatically builds high-performance networks using the Q-Learning paradigm with epsilon-greedy exploration strategy. The optimal network block is constructed by the learning agent which is trained to choose component layers sequentially. We stack the block to construct the whole auto-generated network. To accelerate the generation process, we also propose a distributed asynchronous framework and an early stop strategy. The block-wise generation brings unique advantages: (1) it yields state-of-the-art results in comparison to the hand-crafted networks on image classification, particularly, the best network generated by BlockQNN achieves 2.35 percent top-1 error rate on CIFAR-10. (2) it offers tremendous reduction of the search space in designing networks, spending only 3 days with 32 GPUs. A faster version can yield a comparable result with only 1 GPU in 20 hours. (3) it has strong generalizability in that the network built on CIFAR also performs well on the larger-scale dataset. The best network achieves very competitive accuracy of 82.0 percent top-1 and 96.0 percent top-5 on ImageNet.},
  archive      = {J_TPAMI},
  author       = {Zhao Zhong and Zichen Yang and Boyang Deng and Junjie Yan and Wei Wu and Jing Shao and Cheng-Lin Liu},
  doi          = {10.1109/TPAMI.2020.2969193},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {2314-2328},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {BlockQNN: Efficient block-wise neural network architecture generation},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Automated extraction of mutual independence patterns using
bayesian comparison of partition models. <em>TPAMI</em>, <em>43</em>(7),
2299–2313. (<a
href="https://doi.org/10.1109/TPAMI.2020.2968065">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mutual independence is a key concept in statistics that characterizes the structural relationships between variables. Existing methods to investigate mutual independence rely on the definition of two competing models, one being nested into the other and used to generate a null distribution for a statistic of interest, usually under the asymptotic assumption of large sample size. As such, these methods have a very restricted scope of application. In this article, we propose to change the investigation of mutual independence from a hypothesis-driven task that can only be applied in very specific cases to a blind and automated search within patterns of mutual independence. To this end, we treat the issue as one of model comparison that we solve in a Bayesian framework. We show the relationship between such an approach and existing methods in the case of multivariate normal distributions as well as cross-classified multinomial distributions. We propose a general Markov chain Monte Carlo (MCMC) algorithm to numerically approximate the posterior distribution on the space of all patterns of mutual independence. The relevance of the method is demonstrated on synthetic data as well as two real datasets, showing the unique insight provided by this approach.},
  archive      = {J_TPAMI},
  author       = {Guillaume Marrelec and Alain Giron},
  doi          = {10.1109/TPAMI.2020.2968065},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {2299-2313},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Automated extraction of mutual independence patterns using bayesian comparison of partition models},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An accurate and efficient voting scheme for a maximally
all-inlier 3D correspondence set. <em>TPAMI</em>, <em>43</em>(7),
2287–2298. (<a
href="https://doi.org/10.1109/TPAMI.2020.2963980">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a highly accurate and efficient, yet simple, two-stage voting scheme for distinguishing inlier 3D correspondences by densely assessing and ranking their local and global geometric consistencies. The strength of the proposed method stems from both the novel idea of post-validated voting set, as well as single-point superimposition transforms, which are computationally cheap and avoid rotational ambiguities. Using a well-known dataset consisting of various 3D models and numerous scenes that include different occlusion rates, the proposed scheme is evaluated against state-of-the-art 3D voting schemes, in terms of both the correspondence PR (precision-recall) AUC (area under curve), and the execution time. A total of 374 experiments were conducted for each method, which involved a combination of four models, 50 scenes, and two down-samplings. The proposed scheme outperforms the state-of-the-art 3D voting schemes in terms of both accuracy and speed. Quantitatively, the proposed scheme scores 97.0\% ±12.9\%97.0\%±12.9\% on the PR AUC metric, averaged over all of the experiments, while the two state-of-the-art schemes score 74.2\% ±22.2\%74.2\%±22.2\% and 78.3\% ±26.4\%78.3\%±26.4\%. Furthermore, the proposed scheme requires only 24.1\% ±6.0\%24.1\%±6.0\% of the time consumed by the fastest state-of-the-art scheme. The proposed voting scheme also demonstrates high robustness against occlusions and scarce inliers.},
  archive      = {J_TPAMI},
  author       = {Hamdi Sahloul and Shouhei Shirafuji and Jun Ota},
  doi          = {10.1109/TPAMI.2020.2963980},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {2287-2298},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {An accurate and efficient voting scheme for a maximally all-inlier 3D correspondence set},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). A bayesian formulation of coherent point drift.
<em>TPAMI</em>, <em>43</em>(7), 2269–2286. (<a
href="https://doi.org/10.1109/TPAMI.2020.2971687">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Coherent point drift is a well-known algorithm for solving point set registration problems, i.e., finding corresponding points between shapes represented as point sets. Despite its advantages over other state-of-the-art algorithms, theoretical and practical issues remain. Among theoretical issues, (1) it is unknown whether the algorithm always converges, and (2) the meaning of the parameters concerning motion coherence is unclear. Among practical issues, (3) the algorithm is relatively sensitive to target shape rotation, and (4) acceleration of the algorithm is restricted to the use of the Gaussian kernel. To overcome these issues and provide a different and more general perspective to the algorithm, we formulate coherent point drift in a Bayesian setting. The formulation brings the following consequences and advances to the field: convergence of the algorithm is guaranteed by variational Bayesian inference; the definition of motion coherence as a prior distribution provides a basis for interpretation of the parameters; rigid and non-rigid registration can be performed in a single algorithm, enhancing robustness against target rotation. We also propose an acceleration scheme for the algorithm that can be applied to non-Gaussian kernels and that provides greater efficiency than coherent point drift.},
  archive      = {J_TPAMI},
  author       = {Osamu Hirose},
  doi          = {10.1109/TPAMI.2020.2971687},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {2269-2286},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {A bayesian formulation of coherent point drift},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Non-line-of-sight imaging via neural transient fields.
<em>TPAMI</em>, <em>43</em>(7), 2257–2268. (<a
href="https://doi.org/10.1109/TPAMI.2021.3076062">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a neural modeling framework for non-line-of-sight (NLOS) imaging. Previous solutions have sought to explicitly recover the 3D geometry (e.g., as point clouds) or voxel density (e.g., within a pre-defined volume) of the hidden scene. In contrast, inspired by the recent Neural Radiance Field (NeRF) approach, we use a multi-layer perceptron (MLP) to represent the neural transient field or NeTF. However, NeTF measures the transient over spherical wavefronts rather than the radiance along lines. We therefore formulate a spherical volume NeTF reconstruction pipeline, applicable to both confocal and non-confocal setups. Compared with NeRF, NeTF samples a much sparser set of viewpoints (scanning spots) and the sampling is highly uneven. We thus introduce a Monte Carlo technique to improve the robustness in the reconstruction. Experiments on synthetic and real datasets demonstrate NeTF achieves state-of-the-art performance and can provide reliable reconstructions even under semi-occlusions and on non-Lambertian materials.},
  archive      = {J_TPAMI},
  author       = {Siyuan Shen and Zi Wang and Ping Liu and Zhengqing Pan and Ruiqian Li and Tian Gao and Shiying Li and Jingyi Yu},
  doi          = {10.1109/TPAMI.2021.3076062},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {2257-2268},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Non-line-of-sight imaging via neural transient fields},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Designing display pixel layouts for under-panel cameras.
<em>TPAMI</em>, <em>43</em>(7), 2245–2256. (<a
href="https://doi.org/10.1109/TPAMI.2021.3075978">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Under-panel cameras provide an intriguing way to maximize the display area for a mobile device. An under-panel camera images a scene via the openings in the display panel; hence, a captured photograph is noisy as well as endowed with a large diffractive blur as the display acts as an aperture on the lens. Unfortunately, the pattern of openings commonly found in current LED displays are not conducive to high-quality deblurring. This paper redesigns the layout of openings in the display to engineer a blur kernel that is robustly invertible in the presence of noise. We first provide a basic analysis using Fourier optics that indicates that the nature of the blur is critically affected by the periodicity of the display openings as well as the shape of the opening at each individual display pixel. Armed with this insight, we provide a suite of modifications to the pixel layout that promote the invertibility of the blur kernels. We evaluate the proposed layouts with photomasks placed in front of a cellphone camera, thereby emulating an under-panel camera. A key takeaway is that optimizing the display layout does indeed produce significant improvements.},
  archive      = {J_TPAMI},
  author       = {Anqi Yang and Aswin C. Sankaranarayanan},
  doi          = {10.1109/TPAMI.2021.3075978},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {2245-2256},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Designing display pixel layouts for under-panel cameras},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SASSI — super-pixelated adaptive spatio-spectral imaging.
<em>TPAMI</em>, <em>43</em>(7), 2233–2244. (<a
href="https://doi.org/10.1109/TPAMI.2021.3075228">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a novel video-rate hyperspectral imager with high spatial, temporal and spectral resolutions. Our key hypothesis is that spectral profiles of pixels within each super-pixel tend to be similar. Hence, a scene-adaptive spatial sampling of a hyperspectral scene, guided by its super-pixel segmented image, is capable of obtaining high-quality reconstructions. To achieve this, we acquire an RGB image of the scene, compute its super-pixels, from which we generate a spatial mask of locations where we measure high-resolution spectrum. The hyperspectral image is subsequently estimated by fusing the RGB image and the spectral measurements using a learnable guided filtering approach. Due to low computational complexity of the superpixel estimation step, our setup can capture hyperspectral images of the scenes with little overhead over traditional snapshot hyperspectral cameras, but with significantly higher spatial and spectral resolutions. We validate the proposed technique with extensive simulations as well as a lab prototype that measures hyperspectral video at a spatial resolution of 600 × 900 pixels, at a spectral resolution of 10 nm over visible wavebands, and achieving a frame rate at 18fps.},
  archive      = {J_TPAMI},
  author       = {Vishwanath Saragadam and Michael DeZeeuw and Richard G. Baraniuk and Ashok Veeraraghavan and Aswin C. Sankaranarayanan},
  doi          = {10.1109/TPAMI.2021.3075228},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {2233-2244},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {SASSI — super-pixelated adaptive spatio-spectral imaging},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Non-rigid shape from water. <em>TPAMI</em>, <em>43</em>(7),
2220–2232. (<a
href="https://doi.org/10.1109/TPAMI.2021.3075450">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a novel 3D sensing method for recovering a consistent, dense 3D shape of a dynamic, non-rigid object in water. The method reconstructs a complete (or fuller) 3D surface of the target object in a canonical frame (e.g., rest shape) as it freely deforms and moves between frames by estimating underwater 3D scene flow and using it to integrate per-frame depth estimates recovered from two near-infrared observations. The reconstructed shape is refined in the course of this global non-rigid shape recovery by leveraging both geometric and radiometric constraints. We implement our method with a single camera and a light source without the orthographic assumption on either by deriving a practical calibration method that estimates the point source position with respect to the camera. Our reconstruction method also accounts for scattering by water. We prototype a video-rate imaging system and show 3D shape reconstruction results on a number of real-world static, deformable, and dynamic objects and creatures in real-world water. The results demonstrate the effectiveness of the method in recovering complete shapes of complex, non-rigid objects in water, which opens new avenues of application for underwater 3D sensing in the sub-meter range.},
  archive      = {J_TPAMI},
  author       = {Meng-Yu Jennifer Kuo and Ryo Kawahara and Shohei Nobuhara and Ko Nishino},
  doi          = {10.1109/TPAMI.2021.3075450},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {2220-2232},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Non-rigid shape from water},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). High resolution, deep imaging using confocal time-of-flight
diffuse optical tomography. <em>TPAMI</em>, <em>43</em>(7), 2206–2219.
(<a href="https://doi.org/10.1109/TPAMI.2021.3075366">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Light scattering by tissue severely limits how deep beneath the surface one can image, and the spatial resolution one can obtain from these images. Diffuse optical tomography (DOT) is one of the most powerful techniques for imaging deep within tissue – well beyond the conventional $\sim$ 10-15 mean scattering lengths tolerated by ballistic imaging techniques such as confocal and two-photon microscopy. Unfortunately, existing DOT systems are limited, achieving only centimeter-scale resolution. Furthermore, they suffer from slow acquisition times and slow reconstruction speeds making real-time imaging infeasible. We show that time-of-flight diffuse optical tomography (ToF-DOT) and its confocal variant (CToF-DOT), by exploiting the photon travel time information, allow us to achieve millimeter spatial resolution in the highly scattered diffusion regime ( $&amp;gt; \!\!50$ mean free paths). In addition, we demonstrate two additional innovations: focusing on confocal measurements, and multiplexing the illumination sources allow us to significantly reduce the measurement acquisition time. Finally, we rely on a novel convolutional approximation that allows us to develop a fast reconstruction algorithm, achieving a 100× speedup in reconstruction time compared to traditional DOT reconstruction techniques. Together, we believe that these technical advances serve as the first step towards real-time, millimeter resolution, deep tissue imaging using DOT.},
  archive      = {J_TPAMI},
  author       = {Yongyi Zhao and Ankit Raghuram and Hyun K. Kim and Andreas H. Hielscher and Jacob T. Robinson and Ashok Veeraraghavan},
  doi          = {10.1109/TPAMI.2021.3075366},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {2206-2219},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {High resolution, deep imaging using confocal time-of-flight diffuse optical tomography},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Exploiting wavelength diversity for high resolution
time-of-flight 3D imaging. <em>TPAMI</em>, <em>43</em>(7), 2193–2205.
(<a href="https://doi.org/10.1109/TPAMI.2021.3075156">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The poor lateral and depth resolution of state-of-the-art 3D sensors based on the time-of-flight (ToF) principle has limited widespread adoption to a few niche applications. In this work, we introduce a novel sensor concept that provides ToF-based 3D measurements of real world objects and surfaces with depth precision up to 35 $\mu m$ and point cloud densities commensurate with the native sensor resolution of standard CMOS/CCD detectors (up to several megapixels). Such capabilities are realized by combining the best attributes of continuous wave ToF sensing, multi-wavelength interferometry, and heterodyne interferometry into a single approach. We describe multiple embodiments of the approach, each featuring a different sensing modality and associated tradeoffs.},
  archive      = {J_TPAMI},
  author       = {Fengqiang Li and Florian Willomitzer and Muralidhar Madabhushi Balaji and Prasanna Rangarajan and Oliver Cossairt},
  doi          = {10.1109/TPAMI.2021.3075156},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {2193-2205},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Exploiting wavelength diversity for high resolution time-of-flight 3D imaging},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning optimal wavefront shaping for multi-channel
imaging. <em>TPAMI</em>, <em>43</em>(7), 2179–2192. (<a
href="https://doi.org/10.1109/TPAMI.2021.3076873">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fast acquisition of depth information is crucial for accurate 3D tracking of moving objects. Snapshot depth sensing can be achieved by wavefront coding, in which the point-spread function (PSF) is engineered to vary distinctively with scene depth by altering the detection optics. In low-light applications, such as 3D localization microscopy, the prevailing approach is to condense signal photons into a single imaging channel with phase-only wavefront modulation to achieve a high pixel-wise signal to noise ratio. Here we show that this paradigm is generally suboptimal and can be significantly improved upon by employing multi-channel wavefront coding, even in low-light applications. We demonstrate our multi-channel optimization scheme on 3D localization microscopy in densely labelled live cells where detectability is limited by overlap of modulated PSFs. At extreme densities, we show that a split-signal system, with end-to-end learned phase masks, doubles the detection rate and reaches improved precision compared to the current state-of-the-art, single-channel design. We implement our method using a bifurcated optical system, experimentally validating our approach by snapshot volumetric imaging and 3D tracking of fluorescently labelled subcellular elements in dense environments.},
  archive      = {J_TPAMI},
  author       = {Elias Nehme and Boris Ferdman and Lucien E. Weiss and Tal Naor and Daniel Freedman and Tomer Michaeli and Yoav Shechtman},
  doi          = {10.1109/TPAMI.2021.3076873},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {2179-2192},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Learning optimal wavefront shaping for multi-channel imaging},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Guest editorial: Introduction to the special section on
computational photography. <em>TPAMI</em>, <em>43</em>(7), 2175–2178.
(<a href="https://doi.org/10.1109/TPAMI.2021.3078707">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The papers in this special section focus on computational photography. The past year has been significant in many ways. For the scientific community of computational photography, we have a hybrid in-person conference, one of the first in the vision/optics/graphics community post-pandemic. Further, we expanded our community to better include the physical optics community. This move was made consciously to strengthen and expand the span of computational photography and make these different, yet closely related communities, have a common venue to share ideas. This expansion we believe has significantly enriched the papers submitted to this special issue, through the IEEE International Conference on Computational Photography (ICCP’2021).},
  archive      = {J_TPAMI},
  author       = {Yoav Y. Schechner and Kavita Bala and Ori Katz and Kalyan Sunkavalli and Ko Nishino},
  doi          = {10.1109/TPAMI.2021.3078707},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {7},
  pages        = {2175-2178},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Guest editorial: Introduction to the special section on computational photography},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). [Back cover]. <em>TPAMI</em>, <em>43</em>(6), C4. (<a
href="https://doi.org/10.1109/TPAMI.2021.3074347">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Provides a listing of current committee members and society officers.},
  archive      = {J_TPAMI},
  doi          = {10.1109/TPAMI.2021.3074347},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {C4},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {[Back cover]},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). [Back inside cover]. <em>TPAMI</em>, <em>43</em>(6), C3.
(<a href="https://doi.org/10.1109/TPAMI.2021.3074345">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Provides a listing of current committee members and society officers.},
  archive      = {J_TPAMI},
  doi          = {10.1109/TPAMI.2021.3074345},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {C3},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {[Back inside cover]},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Computing in science &amp; engineering. <em>TPAMI</em>,
<em>43</em>(6), 2174. (<a
href="https://doi.org/10.1109/TPAMI.2021.3074332">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advertisement, IEEE Computer Society.},
  archive      = {J_TPAMI},
  doi          = {10.1109/TPAMI.2021.3074332},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {2174},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Computing in science &amp; engineering},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021c). IEEE computer society jobs board. <em>TPAMI</em>,
<em>43</em>(6), 2173. (<a
href="https://doi.org/10.1109/TPAMI.2021.3074331">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advertisement, IEEE Computer Society.},
  archive      = {J_TPAMI},
  doi          = {10.1109/TPAMI.2021.3074331},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {2173},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {IEEE computer society jobs board},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Correction to “nonlinear regression via deep negative
correlation learning.” <em>TPAMI</em>, <em>43</em>(6), 2172. (<a
href="https://doi.org/10.1109/TPAMI.2021.3071929">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reports on changes to the author information presented in the above named paper.},
  archive      = {J_TPAMI},
  author       = {Le Zhang and Zenglin Shi and Ming-Ming Cheng and Yun Liu and Jia-Wang Bian and Joey Tianyi Zhou and Guoyan Zheng and Zeng Zeng},
  doi          = {10.1109/TPAMI.2021.3071929},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {2172},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Correction to “Nonlinear regression via deep negative correlation learning”},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Spherical principal curves. <em>TPAMI</em>, <em>43</em>(6),
2165–2171. (<a
href="https://doi.org/10.1109/TPAMI.2020.3025327">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a new approach for dimension reduction of data observed on spherical surfaces. Several dimension reduction techniques have been developed in recent years for non-euclidean data analysis. As a pioneer work, (Hauberg 2016) attempted to implement principal curves on Riemannian manifolds. However, this approach uses approximations to process data on Riemannian manifolds, resulting in distorted results. This study proposes a new approach to project data onto a continuous curve to construct principal curves on spherical surfaces. Our approach lies in the same line of (Hastie and Stuetzle et al. 1989) that proposed principal curves for data on euclidean space. We further investigate the stationarity of the proposed principal curves that satisfy the self-consistency on spherical surfaces. The results on the real data analysis and simulation examples show promising empirical characteristics of the proposed approach.},
  archive      = {J_TPAMI},
  author       = {Jongmin Lee and Jang-Hyun Kim and Hee-Seok Oh},
  doi          = {10.1109/TPAMI.2020.3025327},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {2165-2171},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Spherical principal curves},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SensitiveNets: Learning agnostic representations with
application to face images. <em>TPAMI</em>, <em>43</em>(6), 2158–2164.
(<a href="https://doi.org/10.1109/TPAMI.2020.3015420">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work proposes a novel privacy-preserving neural network feature representation to suppress the sensitive information of a learned space while maintaining the utility of the data. The new international regulation for personal data protection forces data controllers to guarantee privacy and avoid discriminative hazards while managing sensitive data of users. In our approach, privacy and discrimination are related to each other. Instead of existing approaches aimed directly at fairness improvement, the proposed feature representation enforces the privacy of selected attributes. This way fairness is not the objective, but the result of a privacy-preserving learning method. This approach guarantees that sensitive information cannot be exploited by any agent who process the output of the model, ensuring both privacy and equality of opportunity. Our method is based on an adversarial regularizer that introduces a sensitive information removal function in the learning objective. The method is evaluated on three different primary tasks (identity, attractiveness, and smiling) and three publicly available benchmarks. In addition, we present a new face annotation dataset with balanced distribution between genders and ethnic origins. The experiments demonstrate that it is possible to improve the privacy and equality of opportunity while retaining competitive performance independently of the task.},
  archive      = {J_TPAMI},
  author       = {Aythami Morales and Julian Fierrez and Ruben Vera-Rodriguez and Ruben Tolosana},
  doi          = {10.1109/TPAMI.2020.3015420},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {2158-2164},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {SensitiveNets: Learning agnostic representations with application to face images},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust multi-task learning with flexible manifold
constraint. <em>TPAMI</em>, <em>43</em>(6), 2150–2157. (<a
href="https://doi.org/10.1109/TPAMI.2020.3007637">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-Task Learning attempts to explore and mine the sufficient information within multiple related tasks for the better solutions. However, the performance of the existing multi-task approaches would largely degenerate when dealing with the polluted data, i.e., outliers. In this paper, we propose a novel robust multi-task model by incorporating a flexible manifold constraint (FMC-MTL) and a robust loss. Specifically speaking, multi-task subspace is embedded with a relaxed and generalized Stiefel Manifold for considering point-wise correlation and preserving the data structure simultaneously. In addition, a robust loss function is developed to ensure the robustness to outliers by smoothly interpolating between l 2,1 ℓ2,1-norm and squared Frobenius norm. Equipped with an efficient algorithm, FMC-MTL serves as a robust solution to tackling the severely polluted data. Moreover, extensive experiments are conducted to verify the superiority of our model. Compared to the state-of-the-art multi-task models, the proposed FMC-MTL model demonstrates remarkable robustness to the contaminated data.},
  archive      = {J_TPAMI},
  author       = {Rui Zhang and Hongyuan Zhang and Xuelong Li},
  doi          = {10.1109/TPAMI.2020.3007637},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {2150-2157},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Robust multi-task learning with flexible manifold constraint},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). NWPU-crowd: A large-scale benchmark for crowd counting and
localization. <em>TPAMI</em>, <em>43</em>(6), 2141–2149. (<a
href="https://doi.org/10.1109/TPAMI.2020.3013269">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the last decade, crowd counting and localization attract much attention of researchers due to its wide-spread applications, including crowd monitoring, public safety, space design, etc. Many convolutional neural networks (CNN) are designed for tackling this task. However, currently released datasets are so small-scale that they can not meet the needs of the supervised CNN-based algorithms. To remedy this problem, we construct a large-scale congested crowd counting and localization dataset, NWPU-Crowd, consisting of 5,109 images, in a total of 2,133,375 annotated heads with points and boxes. Compared with other real-world datasets, it contains various illumination scenes and has the largest density range (0 ~ 20,0330~20,033). Besides, a benchmark website is developed for impartially evaluating the different methods, which allows researchers to submit the results of the test set. Based on the proposed dataset, we further describe the data characteristics, evaluate the performance of some mainstream state-of-the-art (SOTA) methods, and analyze the new problems that arise on the new data. What&#39;s more, the benchmark is deployed at https://www.crowdbenchmark.com/, and the dataset/code/models/results are available at https://gjy3035.github.io/NWPU-Crowd-Sample-Code/.},
  archive      = {J_TPAMI},
  author       = {Qi Wang and Junyu Gao and Wei Lin and Xuelong Li},
  doi          = {10.1109/TPAMI.2020.3013269},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {2141-2149},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {NWPU-crowd: A large-scale benchmark for crowd counting and localization},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Enhanced tensor RPCA and its application. <em>TPAMI</em>,
<em>43</em>(6), 2133–2140. (<a
href="https://doi.org/10.1109/TPAMI.2020.3017672">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the promising results, tensor robust principal component analysis (TRPCA), which aims to recover underlying low-rank structure of clean tensor data corrupted with noise/outliers by shrinking all singular values equally, cannot well preserve the salient content of image. The major reason is that, in real applications, there is a salient difference information between all singular values of a tensor image, and the larger singular values are generally associated with some salient parts in the image. Thus, the singular values should be treated differently. Inspired by this observation, we investigate whether there is a better alternative solution when using tensor rank minimization. In this paper, we develop an enhanced TRPCA (ETRPCA) which explicitly considers the salient difference information between singular values of tensor data by the weighted tensor Schatten p-norm minimization, and then propose an efficient algorithm, which has a good convergence, to solve ETRPCA. Extensive experimental results reveal that the proposed method ETRPCA is superior to several state-of-the-art variant RPCA methods in terms of performance.},
  archive      = {J_TPAMI},
  author       = {Quanxue Gao and Pu Zhang and Wei Xia and Deyan Xie and Xinbo Gao and Dacheng Tao},
  doi          = {10.1109/TPAMI.2020.3017672},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {2133-2140},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Enhanced tensor RPCA and its application},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Comparing graph clusterings: Set partition measures
vs. Graph-aware measures. <em>TPAMI</em>, <em>43</em>(6), 2127–2132. (<a
href="https://doi.org/10.1109/TPAMI.2020.3009862">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a family of graph partition similarity measures that take the topology of the graph into account. These graph-aware measures are alternatives to using set partition similarity measures that are not specifically designed for graphs. The two types of measures, graph-aware and set partition measures, are shown to have opposite behaviors with respect to resolution issues and provide complementary information necessary to compare graph partitions.},
  archive      = {J_TPAMI},
  author       = {Valérie Poulin and François Théberge},
  doi          = {10.1109/TPAMI.2020.3009862},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {2127-2132},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Comparing graph clusterings: Set partition measures vs. graph-aware measures},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adversarial metric attack and defense for person
re-identification. <em>TPAMI</em>, <em>43</em>(6), 2119–2126. (<a
href="https://doi.org/10.1109/TPAMI.2020.3031625">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person re-identification (re-ID) has attracted much attention recently due to its great importance in video surveillance. In general, distance metrics used to identify two person images are expected to be robust under various appearance changes. However, our work observes the extreme vulnerability of existing distance metrics to adversarial examples, generated by simply adding human-imperceptible perturbations to person images. Hence, the security danger is dramatically increased when deploying commercial re-ID systems in video surveillance. Although adversarial examples have been extensively applied for classification analysis, it is rarely studied in metric analysis like person re-identification. The most likely reason is the natural gap between the training and testing of re-ID networks, that is, the predictions of a re-ID network cannot be directly used during testing without an effective metric. In this work, we bridge the gap by proposing Adversarial Metric Attack, a parallel methodology to adversarial classification attacks. Comprehensive experiments clearly reveal the adversarial effects in re-ID systems. Meanwhile, we also present an early attempt of training a metric-preserving network, thereby defending the metric against adversarial attacks. At last, by benchmarking various adversarial settings, we expect that our work can facilitate the development of adversarial attack and defense in metric-based applications.},
  archive      = {J_TPAMI},
  author       = {Song Bai and Yingwei Li and Yuyin Zhou and Qizhu Li and Philip H.S. Torr},
  doi          = {10.1109/TPAMI.2020.3031625},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {2119-2126},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Adversarial metric attack and defense for person re-identification},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Visual scanpath prediction using IOR-ROI recurrent mixture
density network. <em>TPAMI</em>, <em>43</em>(6), 2101–2118. (<a
href="https://doi.org/10.1109/TPAMI.2019.2956930">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A visual scanpath represents the human eye movements when scanning the visual field for acquiring and receiving visual information. Predicting visual scanpaths when a certain stimulus is presented plays an important role in modeling overt human visual attention and search behavior. In this paper, we presented an &#39;Inhibition of Return - Region of Interest&#39; (IOR-ROI) recurrent mixture density network based framework learning to produce human-like visual scanpaths under task-free viewing conditions. The proposed model simultaneously predicts a sequence of ordered fixation positions and their corresponding fixation durations. Our model integrates bottom-up features and semantic features extracted by convolutional neural networks. Then the integrated feature maps are fed into the IOR-ROI Long Short-Term Memory (LSTM) which is the core component of the proposed model. The IOR-ROI LSTM is a dual LSTM unit, i.e., the IOR-LSTM and the ROI-LSTM, capturing IOR dynamics and gaze shift behavior simultaneously. IOR-LSTM simulates the visual working memory to adaptively maintain and update visual information regarding previously fixated regions. ROI-LSTM is responsible for predicting the next possible ROIs given the spatially inhibited image feature maps on the feature-wise basis. Fixation duration is predicted by a regression neural network given the viewing history and image feature maps corresponding to currently fixated ROI. Considering the eye movement pattern variations among subjects, a mixture density network is adopted to model the next fixation distribution as Gaussian mixtures and the fixation duration is also modeled using Gaussian distribution. Our model is evaluated on the OSIE and MIT low resolution eye-tracking datasets and experimental results indicate that the proposed method can achieve superior performance in predicting visual scanpaths. The code will be publicly available on URL: https://github.com/sunwj/scanpath.},
  archive      = {J_TPAMI},
  author       = {Wanjie Sun and Zhenzhong Chen and Feng Wu},
  doi          = {10.1109/TPAMI.2019.2956930},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {2101-2118},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Visual scanpath prediction using IOR-ROI recurrent mixture density network},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Towards robust discriminative projections learning via
non-greedy <span
class="math inline"><em>ℓ</em><sub>2, 1</sub></span>ℓ2,1-norm MinMax.
<em>TPAMI</em>, <em>43</em>(6), 2086–2100. (<a
href="https://doi.org/10.1109/TPAMI.2019.2961877">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Linear Discriminant Analysis (LDA) is one of the most successful supervised dimensionality reduction methods and has been widely used in many real-world applications. However, l 2 ℓ2-norm is employed as the distance metric in the objective of LDA, which is sensitive to outliers. Many previous works improve the robustness of LDA by using l 1 ℓ1-norm distance. However, the robustness against outliers is limited and the solver of l 1 ℓ1-norm is mostly based on the greedy search strategy, which is time-consuming and easy to get stuck in a local optimum. In this paper, we propose a novel robust LDA measured by l 2,1 ℓ2,1-norm to learn robust discriminative projections. The proposed model is challenging to solve since it needs to minimize and maximize (minmax) l 2,1 ℓ2,1-norm terms simultaneously. As a result, we first systematically derive an efficient iterative optimization algorithm to solve a general ratio minimization problem, and then rigorously prove its convergence. More importantly, an alternately non-greedy iterative re-weighted optimization algorithm is developed based on the preceding approach for solving proposed l 2,1 ℓ2,1-norm minmax problem. Besides, an optimal weighted mean mechanism is driven according to the designed objective and solver, which can be applied to other approaches for robustness improvement. Experimental results on several real-world datasets show the effectiveness of proposed method.},
  archive      = {J_TPAMI},
  author       = {Feiping Nie and Zheng Wang and Rong Wang and Zhen Wang and Xuelong Li},
  doi          = {10.1109/TPAMI.2019.2961877},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {2086-2100},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Towards robust discriminative projections learning via non-greedy $\ell _{2,1}$ℓ2,1-norm MinMax},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Time-resolved far infrared light transport decomposition for
thermal photometric stereo. <em>TPAMI</em>, <em>43</em>(6), 2075–2085.
(<a href="https://doi.org/10.1109/TPAMI.2019.2959304">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel time-resolved light transport decomposition method using thermal imaging. Because the speed of heat propagation is much slower than the speed of light propagation, the transient transport of far infrared light can be observed at a video frame rate. A key observation is that the thermal image looks similar to the visible light image in an appropriately controlled environment. This implies that conventional computer vision techniques can be straightforwardly applied to the thermal image. We show that the diffuse component in the thermal image can be separated, and therefore, the surface normals of objects can be estimated by the Lambertian photometric stereo. The effectiveness of our method is evaluated by conducting real-world experiments, and its applicability to black body, transparent, and translucent objects is shown.},
  archive      = {J_TPAMI},
  author       = {Kenichiro Tanaka and Nobuhiro Ikeya and Tsuyoshi Takatani and Hiroyuki Kubo and Takuya Funatomi and Vijay Ravi and Achuta Kadambi and Yasuhiro Mukaigawa},
  doi          = {10.1109/TPAMI.2019.2959304},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {2075-2085},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Time-resolved far infrared light transport decomposition for thermal photometric stereo},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Single day outdoor photometric stereo. <em>TPAMI</em>,
<em>43</em>(6), 2062–2074. (<a
href="https://doi.org/10.1109/TPAMI.2019.2962693">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Photometric Stereo (PS) under outdoor illumination remains a challenging, ill-posed problem due to insufficient variability in illumination. Months-long capture sessions are typically used in this setup, with little success on shorter, single-day time intervals. In this paper, we investigate the solution of outdoor PS over a single day, under different weather conditions. First, we investigate the relationship between weather and surface reconstructability in order to understand when natural lighting allows existing PS algorithms to work. Our analysis reveals that partially cloudy days improve the conditioning of the outdoor PS problem while sunny days do not allow the unambiguous recovery of surface normals from photometric cues alone. We demonstrate that calibrated PS algorithms can thus be employed to reconstruct Lambertian surfaces accurately under partially cloudy days. Second, we solve the ambiguity arising in clear days by combining photometric cues with prior knowledge on material properties, local surface geometry and the natural variations in outdoor lighting through a CNN-based, weakly-calibrated PS technique. Given a sequence of outdoor images captured during a single sunny day, our method robustly estimates the scene surface normals with unprecedented quality for the considered scenario. Our approach does not require precise geolocation and significantly outperforms several state-of-the-art methods on images with real lighting, showing that our CNN can combine efficiently learned priors and photometric cues available during a single sunny day.},
  archive      = {J_TPAMI},
  author       = {Yannick Hold-Geoffroy and Paulo Gotardo and Jean-François Lalonde},
  doi          = {10.1109/TPAMI.2019.2962693},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {2062-2074},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Single day outdoor photometric stereo},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Self-paced collaborative and adversarial network for
unsupervised domain adaptation. <em>TPAMI</em>, <em>43</em>(6),
2047–2061. (<a
href="https://doi.org/10.1109/TPAMI.2019.2962476">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a new unsupervised domain adaptation approach called Collaborative and Adversarial Network (CAN), which uses the domain-collaborative and domain-adversarial learning strategies for training the neural network. The domain-collaborative learning strategy aims to learn domain specific feature representation to preserve the discriminability for the target domain, while the domain adversarial learning strategy aims to learn domain invariant feature representation to reduce the domain distribution mismatch between the source and target domains. We show that these two learning strategies can be uniformly formulated as domain classifier learning with positive or negative weights on the losses. We then design a collaborative and adversarial training scheme, which automatically learns domain specific representations from lower blocks in CNNs through collaborative learning and domain invariant representations from higher blocks through adversarial learning. Moreover, to further enhance the discriminability in the target domain, we propose Self-Paced CAN (SPCAN), which progressively selects pseudo-labeled target samples for re-training the classifiers. We employ a self-paced learning strategy such that we can select pseudo-labeled target samples in an easy-to-hard fashion. Additionally, we build upon the popular two-stream approach to extend our domain adaptation approach for more challenging video action recognition task, which additionally considers the cooperation between the RGB stream and the optical flow stream. We propose the Two-stream SPCAN (TS-SPCAN) method to select and reweight the pseudo labeled target samples of one stream (RGB/Flow) based on the information from the other stream (Flow/RGB) in a cooperative way. As a result, our TS-SPCAN model is able to exchange the information between the two streams. Comprehensive experiments on different benchmark datasets, Office-31, ImageCLEF-DA and VISDA-2017 for the object recognition task, and UCF101-10 and HMDB51-10 for the video action recognition task, show our newly proposed approaches achieve the state-of-the-art performance, which clearly demonstrates the effectiveness of our proposed approaches for unsupervised domain adaptation.},
  archive      = {J_TPAMI},
  author       = {Weichen Zhang and Dong Xu and Wanli Ouyang and Wen Li},
  doi          = {10.1109/TPAMI.2019.2962476},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {2047-2061},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Self-paced collaborative and adversarial network for unsupervised domain adaptation},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Person re-identification by contour sketch under moderate
clothing change. <em>TPAMI</em>, <em>43</em>(6), 2029–2046. (<a
href="https://doi.org/10.1109/TPAMI.2019.2960509">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person re-identification (re-id), the process of matching pedestrian images across different camera views, is an important task in visual surveillance. Substantial development of re-id has recently been observed, and the majority of existing models are largely dependent on color appearance and assume that pedestrians do not change their clothes across camera views. This limitation, however, can be an issue for re-id when tracking a person at different places and at different time if that person (e.g., a criminal suspect) changes his/her clothes, causing most existing methods to fail, since they are heavily relying on color appearance, and thus, they are inclined to match a person to another person wearing similar clothes. In this work, we call the person re-id under clothing change the “cross-clothes person re-id.” In particular, we consider the case when a person only changes his clothes moderately as a first attempt at solving this problem based on visible light images; that is, we assume that a person wears clothes of a similar thickness, and thus the shape of a person would not change significantly when the weather does not change substantially within a short period of time. We perform cross-clothes person re-id based on a contour sketch of person image to take advantage of the shape of the human body instead of color information for extracting features that are robust to moderate clothing change. To select/sample more reliable and discriminative curve patterns on a body contour sketch, we introduce a learning-based spatial polar transformation (SPT) layer in the deep neural network to transform contour sketch images for extracting reliable and discriminant convolutional neural network (CNN) features in a polar coordinate space. An angle-specific extractor (ASE) is applied in the following layers to extract more fine-grained discriminant angle-specific features. By varying the sampling range of the SPT, we develop a multistream network for aggregating multi-granularity features to better identify a person. Due to the lack of a large-scale dataset for cross-clothes person re-id, we contribute a new dataset that consists of 33,698 images from 221 identities. Our experiments illustrate the challenges of cross-clothes person re-id and demonstrate the effectiveness of our proposed method.},
  archive      = {J_TPAMI},
  author       = {Qize Yang and Ancong Wu and Wei-Shi Zheng},
  doi          = {10.1109/TPAMI.2019.2960509},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {2029-2046},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Person re-identification by contour sketch under moderate clothing change},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On the importance of visual context for data augmentation in
scene understanding. <em>TPAMI</em>, <em>43</em>(6), 2014–2028. (<a
href="https://doi.org/10.1109/TPAMI.2019.2961896">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Performing data augmentation for learning deep neural networks is known to be important for training visual recognition systems. By artificially increasing the number of training examples, it helps reducing overfitting and improves generalization. While simple image transformations can already improve predictive performance in most vision tasks, larger gains can be obtained by leveraging task-specific prior knowledge. In this work, we consider object detection, semantic and instance segmentation and augment the training images by blending objects in existing scenes, using instance segmentation annotations. We observe that randomly pasting objects on images hurts the performance, unless the object is placed in the right context. To resolve this issue, we propose an explicit context model by using a convolutional neural network, which predicts whether an image region is suitable for placing a given object or not. In our experiments, we show that our approach is able to improve object detection, semantic and instance segmentation on the PASCAL VOC12 and COCO datasets, with significant gains in a limited annotation scenario, i.e., when only one category is annotated. We also show that the method is not limited to datasets that come with expensive pixel-wise instance annotations and can be used when only bounding boxes are available, by employing weakly-supervised learning for instance masks approximation.},
  archive      = {J_TPAMI},
  author       = {Nikita Dvornik and Julien Mairal and Cordelia Schmid},
  doi          = {10.1109/TPAMI.2019.2961896},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {2014-2028},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {On the importance of visual context for data augmentation in scene understanding},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning regional attraction for line segment detection.
<em>TPAMI</em>, <em>43</em>(6), 1998–2013. (<a
href="https://doi.org/10.1109/TPAMI.2019.2958642">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents regional attraction of line segment maps, and hereby poses the problem of line segment detection (LSD) as a problem of region coloring. Given a line segment map, the proposed regional attraction first establishes the relationship between line segments and regions in the image lattice. Based on this, the line segment map is equivalently transformed to an attraction field map (AFM), which can be remapped to a set of line segments without loss of information. Accordingly, we develop an end-to-end framework to learn attraction field maps for raw input images, followed by a squeeze module to detect line segments. Apart from existing works, the proposed detector properly handles the local ambiguity and does not rely on the accurate identification of edge pixels. Comprehensive experiments on the Wireframe dataset and the YorkUrban dataset demonstrate the superiority of our method. In particular, we achieve an F-measure of 0.831 on the Wireframe dataset, advancing the state-of-the-art performance by 10.3 percent.},
  archive      = {J_TPAMI},
  author       = {Nan Xue and Song Bai and Fu-Dong Wang and Gui-Song Xia and Tianfu Wu and Liangpei Zhang and Philip H.S. Torr},
  doi          = {10.1109/TPAMI.2019.2958642},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {1998-2013},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Learning regional attraction for line segment detection},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning a fixed-length fingerprint representation.
<em>TPAMI</em>, <em>43</em>(6), 1981–1997. (<a
href="https://doi.org/10.1109/TPAMI.2019.2961349">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present DeepPrint, a deep network, which learns to extract fixed-length fingerprint representations of only 200 bytes. DeepPrint incorporates fingerprint domain knowledge, including alignment and minutiae detection, into the deep network architecture to maximize the discriminative power of its representation. The compact, DeepPrint representation has several advantages over the prevailing variable length minutiae representation which (i) requires computationally expensive graph matching techniques, (ii) is difficult to secure using strong encryption schemes (e.g., homomorphic encryption), and (iii) has low discriminative power in poor quality fingerprints where minutiae extraction is unreliable. We benchmark DeepPrint against two top performing COTS SDKs (Verifinger and Innovatrics) from the NIST and FVC evaluations. Coupled with a re-ranking scheme, the DeepPrint rank-1 search accuracy on the NIST SD4 dataset against a gallery of 1.1 million fingerprints is comparable to the top COTS matcher, but it is significantly faster (DeepPrint: 98.80\% in 0.3 seconds vs. COTS A: 98.85\% in 27 seconds). To the best of our knowledge, the DeepPrint representation is the most compact and discriminative fixed-length fingerprint representation reported in the academic literature.},
  archive      = {J_TPAMI},
  author       = {Joshua J. Engelsma and Kai Cao and Anil K. Jain},
  doi          = {10.1109/TPAMI.2019.2961349},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {1981-1997},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Learning a fixed-length fingerprint representation},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). High speed and high dynamic range video with an event
camera. <em>TPAMI</em>, <em>43</em>(6), 1964–1980. (<a
href="https://doi.org/10.1109/TPAMI.2019.2963386">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Event cameras are novel sensors that report brightness changes in the form of a stream of asynchronous “events” instead of intensity frames. They offer significant advantages with respect to conventional cameras: high temporal resolution, high dynamic range, and no motion blur. While the stream of events encodes in principle the complete visual signal, the reconstruction of an intensity image from a stream of events is an ill-posed problem in practice. Existing reconstruction approaches are based on hand-crafted priors and strong assumptions about the imaging process as well as the statistics of natural images. In this work we propose to learn to reconstruct intensity images from event streams directly from data instead of relying on any hand-crafted priors. We propose a novel recurrent network to reconstruct videos from a stream of events, and train it on a large amount of simulated event data. During training we propose to use a perceptual loss to encourage reconstructions to follow natural image statistics. We further extend our approach to synthesize color images from color event streams. Our quantitative experiments show that our network surpasses state-of-the-art reconstruction methods by a large margin in terms of image quality (&gt; 20\%), while comfortably running in real-time. We show that the network is able to synthesize high framerate videos (&gt; 5,000 frames per second) of high-speed phenomena (e.g., a bullet hitting an object) and is able to provide high dynamic range reconstructions in challenging lighting conditions. As an additional contribution, we demonstrate the effectiveness of our reconstructions as an intermediate representation for event data. We show that off-the-shelf computer vision algorithms can be applied to our reconstructions for tasks such as object classification and visual-inertial odometry and that this strategy consistently outperforms algorithms that were specifically designed for event data. We release the reconstruction code, a pre-trained model and the datasets to enable further research.},
  archive      = {J_TPAMI},
  author       = {Henri Rebecq and René Ranftl and Vladlen Koltun and Davide Scaramuzza},
  doi          = {10.1109/TPAMI.2019.2963386},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {1964-1980},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {High speed and high dynamic range video with an event camera},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Estimating feature-label dependence using gini distance
statistics. <em>TPAMI</em>, <em>43</em>(6), 1947–1963. (<a
href="https://doi.org/10.1109/TPAMI.2019.2960358">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identifying statistical dependence between the features and the label is a fundamental problem in supervised learning. This paper presents a framework for estimating dependence between numerical features and a categorical label using generalized Gini distance, an energy distance in reproducing kernel Hilbert spaces (RKHS). Two Gini distance based dependence measures are explored: Gini distance covariance and Gini distance correlation. Unlike Pearson covariance and correlation, which do not characterize independence, the above Gini distance based measures define dependence as well as independence of random variables. The test statistics are simple to calculate and do not require probability density estimation. Uniform convergence bounds and asymptotic bounds are derived for the test statistics. Comparisons with distance covariance statistics are provided. It is shown that Gini distance statistics converge faster than distance covariance statistics in the uniform convergence bounds, hence tighter upper bounds on both Type I and Type II errors. Moreover, the probability of Gini distance covariance statistic under-performing the distance covariance statistic in Type II error decreases to 0 exponentially with the increase of the sample size. Extensive experimental results are presented to demonstrate the performance of the proposed method.},
  archive      = {J_TPAMI},
  author       = {Silu Zhang and Xin Dang and Dao Nguyen and Dawn Wilkins and Yixin Chen},
  doi          = {10.1109/TPAMI.2019.2960358},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {1947-1963},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Estimating feature-label dependence using gini distance statistics},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Direction concentration learning: Enhancing congruency in
machine learning. <em>TPAMI</em>, <em>43</em>(6), 1928–1946. (<a
href="https://doi.org/10.1109/TPAMI.2019.2963387">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the well-known challenges in computer vision tasks is the visual diversity of images, which could result in an agreement or disagreement between the learned knowledge and the visual content exhibited by the current observation. In this work, we first define such an agreement in a concepts learning process as congruency. Formally, given a particular task and sufficiently large dataset, the congruency issue occurs in the learning process whereby the task-specific semantics in the training data are highly varying. We propose a Direction Concentration Learning (DCL) method to improve congruency in the learning process, where enhancing congruency influences the convergence path to be less circuitous. The experimental results show that the proposed DCL method generalizes to state-of-the-art models and optimizers, as well as improves the performances of saliency prediction task, continual learning task, and classification task. Moreover, it helps mitigate the catastrophic forgetting problem in the continual learning task. The code is publicly available at https://github.com/luoyan407/congruency.},
  archive      = {J_TPAMI},
  author       = {Yan Luo and Yongkang Wong and Mohan Kankanhalli and Qi Zhao},
  doi          = {10.1109/TPAMI.2019.2963387},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {1928-1946},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Direction concentration learning: Enhancing congruency in machine learning},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep regionlets: Blended representation and deep learning
for generic object detection. <em>TPAMI</em>, <em>43</em>(6), 1914–1927.
(<a href="https://doi.org/10.1109/TPAMI.2019.2957780">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose a novel object detection algorithm named ”Deep Regionlets” by integrating deep neural networks and a conventional detection schema for accurate generic object detection. Motivated by the effectiveness of regionlets for modeling object deformations and multiple aspect ratios, we incorporate regionlets into an end-to-end trainable deep learning framework. The deep regionlets framework consists of a region selection network and a deep regionlet learning module. Specifically, given a detection bounding box proposal, the region selection network provides guidance on where to select sub-regions from which features can be learned from. An object proposal typically contains three – 16 sub-regions. The regionlet learning module focuses on local feature selection and transformations to alleviate the effects of appearance variations. To this end, we first realize non-rectangular region selection within the detection framework to accommodate variations in object appearance. Moreover, we design a “gating network” within the regionlet leaning module to enable instance dependent soft feature selection and pooling. The Deep Regionlets framework is trained end-to-end without additional efforts. We present ablation studies and extensive experiments on the PASCAL VOC dataset and the Microsoft COCO dataset. The proposed method yields competitive performance over state-of-the-art algorithms, such as RetinaNet and Mask R-CNN, even without additional segmentation labels.},
  archive      = {J_TPAMI},
  author       = {Hongyu Xu and Xutao Lv and Xiaoyu Wang and Zhou Ren and Navaneeth Bodla and Rama Chellappa},
  doi          = {10.1109/TPAMI.2019.2957780},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {1914-1927},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Deep regionlets: Blended representation and deep learning for generic object detection},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep non-negative matrix factorization architecture based on
underlying basis images learning. <em>TPAMI</em>, <em>43</em>(6),
1897–1913. (<a
href="https://doi.org/10.1109/TPAMI.2019.2962679">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The non-negative matrix factorization (NMF) algorithm represents the original image as a linear combination of a set of basis images. This image representation method is in line with the idea of “parts constitute a whole” in human thinking. The existing deep NMF performs deep factorization on the coefficient matrix. In these methods, the basis images used to represent the original image is essentially obtained by factorizing the original images once. To extract features reflecting the deep localization characteristics of images, a novel deep NMF architecture based on underlying basis images learning is proposed for the first time. The architecture learns the underlying basis images by deep factorization on the basis images matrix. The deep factorization architecture proposed in this paper has strong interpretability. To implement this architecture, this paper proposes a deep non-negative basis matrix factorization algorithm to obtain the underlying basis images. Then, the objective function is established with an added regularization term, which directly constrains the basis images matrix to obtain the basis images with good local characteristics, and a regularized deep non-negative basis matrix factorization algorithm is proposed. The regularized deep nonlinear non-negative basis matrix factorization algorithm is also proposed to handle pattern recognition tasks with complex data. This paper also theoretically proves the convergence of the algorithm. Finally, the experimental results show that the deep NMF architecture based on the underlying basis images learning proposed in this paper can obtain better recognition performance than the other state-of-the-art methods.},
  archive      = {J_TPAMI},
  author       = {Yang Zhao and Huiyang Wang and Jihong Pei},
  doi          = {10.1109/TPAMI.2019.2962679},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {1897-1913},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Deep non-negative matrix factorization architecture based on underlying basis images learning},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep clustering: On the link between discriminative models
and k-means. <em>TPAMI</em>, <em>43</em>(6), 1887–1896. (<a
href="https://doi.org/10.1109/TPAMI.2019.2962683">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the context of recent deep clustering studies, discriminative models dominate the literature and report the most competitive performances. These models learn a deep discriminative neural network classifier in which the labels are latent. Typically, they use multinomial logistic regression posteriors and parameter regularization, as is very common in supervised learning. It is generally acknowledged that discriminative objective functions (e.g., those based on the mutual information or the KL divergence) are more flexible than generative approaches (e.g., K-means) in the sense that they make fewer assumptions about the data distributions and, typically, yield much better unsupervised deep learning results. On the surface, several recent discriminative models may seem unrelated to K-means. This study shows that these models are, in fact, equivalent to K-means under mild conditions and common posterior models and parameter regularization. We prove that, for the commonly used logistic regression posteriors, maximizing the L 2 L2 regularized mutual information via an approximate alternating direction method (ADM) is equivalent to minimizing a soft and regularized K-means loss. Our theoretical analysis not only connects directly several recent state-of-the-art discriminative models to K-means, but also leads to a new soft and regularized deep K-means algorithm, which yields competitive performance on several image clustering benchmarks.},
  archive      = {J_TPAMI},
  author       = {Mohammed Jabi and Marco Pedersoli and Amar Mitiche and Ismail Ben Ayed},
  doi          = {10.1109/TPAMI.2019.2962683},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {1887-1896},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Deep clustering: On the link between discriminative models and K-means},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Anytime recognition with routing convolutional networks.
<em>TPAMI</em>, <em>43</em>(6), 1875–1886. (<a
href="https://doi.org/10.1109/TPAMI.2019.2959322">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Achieving an automatic trade-off between accuracy and efficiency for a single deep neural network is highly desired in time-sensitive computer vision applications. To achieve anytime prediction, existing methods only embed fixed exits to neural networks and make the predictions with the fixed exits for all the samples (refer to the “latest-all” strategy). However, it is observed that the latest exit within a time budget does not always provide a more accurate prediction than the earlier exits for testing samples of various difficulties, making the “latest-all” strategy a sub-optimal solution. Motivated by this, we propose to improve the anytime prediction accuracy by allowing each sample to adaptively select its own optimal exit within a specific time budget. Specifically, we propose a new Routing Convolutional Network (RCN). For any given time budget, it adaptively selects the optimal layer as exit for a specific testing sample. To learn an optimal policy for sample routing, a Q-network is embedded into the RCN at each exit, considering both potential information gain and time-cost. To further boost the anytime prediction accuracy, the exits and the Q-networks are optimized alternately to mutually boost each other under the cost-sensitive environment. Apart from applying to whole image classification, RCN can also be adapted to dense prediction tasks, e.g., scene parsing, to achieve the pixel-level anytime prediction. Extensive experimental results on CIFAR-10, CIFAR-100, and ImageNet classification benchmarks, and Cityscapes scene parsing benchmark demonstrate the efficacy of the proposed RCN for anytime recognition.},
  archive      = {J_TPAMI},
  author       = {Zequn Jie and Peng Sun and Xin Li and Jiashi Feng and Wei Liu},
  doi          = {10.1109/TPAMI.2019.2959322},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {1875-1886},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Anytime recognition with routing convolutional networks},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A performance evaluation of correspondence grouping methods
for 3D rigid data matching. <em>TPAMI</em>, <em>43</em>(6), 1859–1874.
(<a href="https://doi.org/10.1109/TPAMI.2019.2960234">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Seeking consistent point-to-point correspondences between 3D rigid data (point clouds, meshes, or depth maps) is a fundamental problem in 3D computer vision. While a number of correspondence selection methods have been proposed in recent years, their advantages and shortcomings remain unclear regarding different applications and perturbations. To fill this gap, this paper gives a comprehensive evaluation of nine state-of-the-art 3D correspondence grouping methods. A good correspondence grouping algorithm is expected to retrieve as many as inliers from initial feature matches, giving a rise in both precision and recall as well as facilitating accurate transformation estimation. Toward this rule, we deploy experiments on three benchmarks with different application contexts, including shape retrieval, 3D object recognition, and point cloud registration. We also investigate various perturbations such as noise, point density variation, clutter, occlusion, partial overlap, different scales of initial correspondences, and different combinations of keypoint detectors and descriptors. The rich variety of application scenarios and nuisances result in different spatial distributions and inlier ratios of initial feature correspondences, thus enabling a thorough evaluation. Based on the outcomes, we give a summary of the traits, merits, and demerits of evaluated approaches and indicate some potential future research directions.},
  archive      = {J_TPAMI},
  author       = {Jiaqi Yang and Ke Xian and Peng Wang and Yanning Zhang},
  doi          = {10.1109/TPAMI.2019.2960234},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {1859-1874},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {A performance evaluation of correspondence grouping methods for 3D rigid data matching},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A memory- and accuracy-aware gaussian parameter-based stereo
matching using confidence measure. <em>TPAMI</em>, <em>43</em>(6),
1845–1858. (<a
href="https://doi.org/10.1109/TPAMI.2019.2959613">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate stereo matching requires a large amount of memory at a high bandwidth, which restricts its use in resource-limited systems such as mobile devices. This problem is compounded by the recent trend of applications requiring significantly high pixel resolution and disparity levels. To alleviate this, we present a memory-efficient and robust stereo matching algorithm. For cost aggregation, we employ the semiglobal parametric approach, which significantly reduces the memory bandwidth by representing the costs of all disparities as a Gaussian mixture model. All costs on multiple paths in an image are aggregated by updating the Gaussian parameters. The aggregation is performed during the scanning in the forward and backward directions. To reduce the amount of memory for the intermediate results during the forward scan, we suggest to store only the Gaussian parameters which contribute significantly to the final disparity selection. We also propose a method to enhance the overall procedure through a learning-based confidence measure. The random forest framework is used to train various features which are extracted from the cost and intensity profile. The experimental results on KITTI dataset show that the proposed method reduces the memory requirement to less than 3 percent of that of semiglobal matching (SGM) while providing a robust depth map compared to those of state-of-the-art SGM-based algorithms.},
  archive      = {J_TPAMI},
  author       = {Yeongmin Lee and Chong-Min Kyung},
  doi          = {10.1109/TPAMI.2019.2959613},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {1845-1858},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {A memory- and accuracy-aware gaussian parameter-based stereo matching using confidence measure},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A lightweight neural network for monocular view generation
with occlusion handling. <em>TPAMI</em>, <em>43</em>(6), 1832–1844. (<a
href="https://doi.org/10.1109/TPAMI.2019.2960689">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we present a very lightweight neural network architecture, trained on stereo data pairs, which performs view synthesis from one single image. With the growing success of multi-view formats, this problem is indeed increasingly relevant. The network returns a prediction built from disparity estimation, which fills in wrongly predicted regions using a occlusion handling technique. To do so, during training, the network learns to estimate the left-right consistency structural constraint on the pair of stereo input images, to be able to replicate it at test time from one single image. The method is built upon the idea of blending two predictions: a prediction based on disparity estimation and a prediction based on direct minimization in occluded regions. The network is also able to identify these occluded areas at training and at test time by checking the pixelwise left-right consistency of the produced disparity maps. At test time, the approach can thus generate a left-side and a right-side view from one input image, as well as a depth map and a pixelwise confidence measure in the prediction. The work outperforms visually and metric-wise state-of-the-art approaches on the challenging KITTI dataset, all while reducing by a very significant order of magnitude (5 or 10 times) the required number of parameters (6.5 M).},
  archive      = {J_TPAMI},
  author       = {Simon Evain and Christine Guillemot},
  doi          = {10.1109/TPAMI.2019.2960689},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {1832-1844},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {A lightweight neural network for monocular view generation with occlusion handling},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Automatic detection of pain from facial expressions: A
survey. <em>TPAMI</em>, <em>43</em>(6), 1815–1831. (<a
href="https://doi.org/10.1109/TPAMI.2019.2958341">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pain sensation is essential for survival, since it draws attention to physical threat to the body. Pain assessment is usually done through self-reports. However, self-assessment of pain is not available in the case of noncommunicative patients, and therefore, observer reports should be relied upon. Observer reports of pain could be prone to errors due to subjective biases of observers. Moreover, continuous monitoring by humans is impractical. Therefore, automatic pain detection technology could be deployed to assist human caregivers and complement their service, thereby improving the quality of pain management, especially for noncommunicative patients. Facial expressions are a reliable indicator of pain, and are used in all observer-based pain assessment tools. Following the advancements in automatic facial expression analysis, computer vision researchers have tried to use this technology for developing approaches for automatically detecting pain from facial expressions. This paper surveys the literature published in this field over the past decade, categorizes it, and identifies future research directions. The survey covers the pain datasets used in the reviewed literature, the learning tasks targeted by the approaches, the features extracted from images and image sequences to represent pain-related information, and finally, the machine learning methods used.},
  archive      = {J_TPAMI},
  author       = {Teena Hassan and Dominik Seuß and Johannes Wollenberg and Katharina Weitz and Miriam Kunz and Stefan Lautenbacher and Jens-Uwe Garbas and Ute Schmid},
  doi          = {10.1109/TPAMI.2019.2958341},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {6},
  pages        = {1815-1831},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Automatic detection of pain from facial expressions: A survey},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021c). [Back cover]. <em>TPAMI</em>, <em>43</em>(5), C4. (<a
href="https://doi.org/10.1109/TPAMI.2021.3066364">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Provides a listing of current committee members and society officers.},
  archive      = {J_TPAMI},
  doi          = {10.1109/TPAMI.2021.3066364},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {5},
  pages        = {C4},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {[Back cover]},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). [Back inside cover]. <em>TPAMI</em>, <em>43</em>(5), C3.
(<a href="https://doi.org/10.1109/TPAMI.2021.3066362">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Provides a listing of current committee members and society officers.},
  archive      = {J_TPAMI},
  doi          = {10.1109/TPAMI.2021.3066362},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {5},
  pages        = {C3},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {[Back inside cover]},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning from large-scale noisy web data with ubiquitous
reweighting for image classification. <em>TPAMI</em>, <em>43</em>(5),
1808–1814. (<a
href="https://doi.org/10.1109/TPAMI.2019.2961910">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many important advances of deep learning techniques have originated from the efforts of addressing the image classification task on large-scale datasets. However, the construction of clean datasets is costly and time-consuming since the Internet is overwhelmed by noisy images with inadequate and inaccurate tags. In this paper, we propose a Ubiquitous Reweighting Network (URNet) that can learn an image classification model from noisy web data. By observing the web data, we find that there are five key challenges, i.e., imbalanced class sizes, high intra-classes diversity and inter-class similarity, imprecise instances, insufficient representative instances, and ambiguous class labels. With these challenges in mind, we assume every training instance has the potential to contribute positively by alleviating the data bias and noise via reweighting the influence of each instance according to different class sizes, large instance clusters, its confidence, small instance bags, and the labels. In this manner, the influence of bias and noise in the data can be gradually alleviated, leading to the steadily improving performance of URNet. Experimental results in the WebVision 2018 challenge with 16 million noisy training images from 5000 classes show that our approach outperforms state-of-the-art models and ranks first place in the image classification task.},
  archive      = {J_TPAMI},
  author       = {Jia Li and Yafei Song and Jianfeng Zhu and Lele Cheng and Ying Su and Lin Ye and Pengcheng Yuan and Shumin Han},
  doi          = {10.1109/TPAMI.2019.2961910},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {5},
  pages        = {1808-1814},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Learning from large-scale noisy web data with ubiquitous reweighting for image classification},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). What is a tabby? Interpretable model decisions by learning
attribute-based classification criteria. <em>TPAMI</em>, <em>43</em>(5),
1791–1807. (<a
href="https://doi.org/10.1109/TPAMI.2019.2954501">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {State-of-the-art classification models are usually considered as black boxes since their decision processes are implicit to humans. On the contrary, human experts classify objects according to a set of explicit hierarchical criteria. For example, “tabby is a domestic cat with stripes, dots, or lines”, where tabby is defined by combining its superordinate category (domestic cat) and some certain attributes (e.g., has stripes). Inspired by this mechanism, we propose an interpretable Hierarchical Criteria Network (HCN) by additionally learning such criteria. To achieve this goal, images and semantic entities (e.g., taxonomies and attributes) are embedded into a common space, where each category can be represented by the linear combination of its superordinate category and a set of learned discriminative attributes. Specifically, a two-stream convolutional neural network (CNN) is elaborately devised, which embeds images and taxonomies with the two streams respectively. The model is trained by minimizing the prediction error of hierarchy labels on both streams. Extensive experiments on two widely studied datasets (CIFAR-100 and ILSVRC) demonstrate that HCN can learn meaningful attributes as well as reasonable and interpretable classification criteria. Therefore, the proposed method enables further human feedback for model correction as an additional benefit.},
  archive      = {J_TPAMI},
  author       = {Haomiao Liu and Ruiping Wang and Shiguang Shan and Xilin Chen},
  doi          = {10.1109/TPAMI.2019.2954501},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {5},
  pages        = {1791-1807},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {What is a tabby? interpretable model decisions by learning attribute-based classification criteria},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Vision models for wide color gamut imaging in cinema.
<em>TPAMI</em>, <em>43</em>(5), 1777–1790. (<a
href="https://doi.org/10.1109/TPAMI.2019.2938499">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gamut mapping is the problem of transforming the colors of image or video content so as to fully exploit the color palette of the display device where the content will be shown, while preserving the artistic intent of the original content&#39;s creator. In particular, in the cinema industry, the rapid advancement in display technologies has created a pressing need to develop automatic and fast gamut mapping algorithms. In this article, we propose a novel framework that is based on vision science models, performs both gamut reduction and gamut extension, is of low computational complexity, produces results that are free from artifacts and outperforms state-of-the-art methods according to psychophysical tests. Our experiments also highlight the limitations of existing objective metrics for the gamut mapping problem.},
  archive      = {J_TPAMI},
  author       = {Syed Waqas Zamir and Javier Vazquez-Corral and Marcelo Bertalmío},
  doi          = {10.1109/TPAMI.2019.2938499},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {5},
  pages        = {1777-1790},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Vision models for wide color gamut imaging in cinema},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Variational bayesian inference for audio-visual tracking of
multiple speakers. <em>TPAMI</em>, <em>43</em>(5), 1761–1776. (<a
href="https://doi.org/10.1109/TPAMI.2019.2953020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we address the problem of tracking multiple speakers via the fusion of visual and auditory information. We propose to exploit the complementary nature and roles of these two modalities in order to accurately estimate smooth trajectories of the tracked persons, to deal with the partial or total absence of one of the modalities over short periods of time, and to estimate the acoustic status-either speaking or silent-of each tracked person over time. We propose to cast the problem at hand into a generative audio-visual fusion (or association) model formulated as a latent-variable temporal graphical model. This may well be viewed as the problem of maximizing the posterior joint distribution of a set of continuous and discrete latent variables given the past and current observations, which is intractable. We propose a variational inference model which amounts to approximate the joint distribution with a factorized distribution. The solution takes the form of a closed-form expectation maximization procedure. We describe in detail the inference algorithm, we evaluate its performance and we compare it with several baseline methods. These experiments show that the proposed audio-visual tracker performs well in informal meetings involving a time-varying number of people.},
  archive      = {J_TPAMI},
  author       = {Yutong Ban and Xavier Alameda-Pineda and Laurent Girin and Radu Horaud},
  doi          = {10.1109/TPAMI.2019.2953020},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {5},
  pages        = {1761-1776},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Variational bayesian inference for audio-visual tracking of multiple speakers},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Using statistical measures and machine learning for graph
reduction to solve maximum weight clique problems. <em>TPAMI</em>,
<em>43</em>(5), 1746–1760. (<a
href="https://doi.org/10.1109/TPAMI.2019.2954827">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we investigate problem reduction techniques using stochastic sampling and machine learning to tackle large-scale optimization problems. These techniques heuristically remove decision variables from the problem instance, that are not expected to be part of an optimal solution. First we investigate the use of statistical measures computed from stochastic sampling of feasible solutions compared with features computed directly from the instance data. Two measures are particularly useful for this: 1) a ranking-based measure, favoring decision variables that frequently appear in high-quality solutions; and 2) a correlation-based measure, favoring decision variables that are highly correlated with the objective values. To take this further we develop a machine learning approach, called Machine Learning for Problem Reduction (MLPR), that trains a supervised learning model on easy problem instances for which the optimal solution is known. This gives us a combination of features enabling us to better predict the decision variables that belong to the optimal solution for a given hard problem. We evaluate our approaches using a typical optimization problem on graphs—the maximum weight clique problem. The experimental results show our problem reduction techniques are very effective and can be used to boost the performance of existing solution methods.},
  archive      = {J_TPAMI},
  author       = {Yuan Sun and Xiaodong Li and Andreas Ernst},
  doi          = {10.1109/TPAMI.2019.2954827},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {5},
  pages        = {1746-1760},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Using statistical measures and machine learning for graph reduction to solve maximum weight clique problems},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Text-guided neural network training for image recognition in
natural scenes and medicine. <em>TPAMI</em>, <em>43</em>(5), 1733–1745.
(<a href="https://doi.org/10.1109/TPAMI.2019.2955476">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks (CNNs) are widely recognized as the foundation for machine vision systems. The conventional rule of teaching CNNs to understand images requires training images with human annotated labels, without any additional instructions. In this article, we look into a new scope and explore the guidance from text for neural network training. We present two versions of attention mechanisms to facilitate interactions between visual and semantic information and encourage CNNs to effectively distill visual features by leveraging semantic features. In contrast to dedicated text-image joint embedding methods, our method realizes asynchronous training and inference behavior: a trained model can classify images, irrespective of the text availability. This characteristic substantially improves the model scalability to multiple (multimodal) vision tasks. We also apply the proposed method onto medical imaging, which learns from richer clinical knowledge and achieves attention-based interpretable decision-making. With comprehensive validation on two natural and two medical datasets, we demonstrate that our method can effectively make use of semantic knowledge to improve CNN performance. Our method performs substantial improvement on medical image datasets. Meanwhile, it achieves promising performance for multi-label image classification and caption-image retrieval as well as excellent performance for phrase-based and multi-object localization on public benchmarks.},
  archive      = {J_TPAMI},
  author       = {Zizhao Zhang and Pingjun Chen and Xiaoshuang Shi and Lin Yang},
  doi          = {10.1109/TPAMI.2019.2955476},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {5},
  pages        = {1733-1745},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Text-guided neural network training for image recognition in natural scenes and medicine},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Tensor low-rank representation for data recovery and
clustering. <em>TPAMI</em>, <em>43</em>(5), 1718–1732. (<a
href="https://doi.org/10.1109/TPAMI.2019.2954874">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-way or tensor data analysis has attracted increasing attention recently, with many important applications in practice. This article develops a tensor low-rank representation (TLRR) method, which is the first approach that can exactly recover the clean data of intrinsic low-rank structure and accurately cluster them as well, with provable performance guarantees. In particular, for tensor data with arbitrary sparse corruptions, TLRR can exactly recover the clean data under mild conditions; meanwhile TLRR can exactly verify their true origin tensor subspaces and hence cluster them accurately. TLRR objective function can be optimized via efficient convex programing with convergence guarantees. Besides, we provide two simple yet effective dictionary construction methods, the simple TLRR (S-TLRR) and robust TLRR (R-TLRR), to handle slightly and severely corrupted data respectively. Experimental results on two computer vision data analysis tasks, image/video recovery and face clustering, clearly demonstrate the superior performance, efficiency and robustness of our developed method over state-of-the-arts including the popular LRR and SSC methods.},
  archive      = {J_TPAMI},
  author       = {Pan Zhou and Canyi Lu and Jiashi Feng and Zhouchen Lin and Shuicheng Yan},
  doi          = {10.1109/TPAMI.2019.2954874},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {5},
  pages        = {1718-1732},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Tensor low-rank representation for data recovery and clustering},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Superpixel soup: Monocular dense 3D reconstruction of a
complex dynamic scene. <em>TPAMI</em>, <em>43</em>(5), 1705–1717. (<a
href="https://doi.org/10.1109/TPAMI.2019.2955131">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work addresses the task of dense 3D reconstruction of a complex dynamic scene from images. The prevailing idea to solve this task is composed of a sequence of steps and is dependent on the success of several pipelines in its execution. To overcome such limitations with the existing algorithm, we propose a unified approach to solve this problem. We assume that a dynamic scene can be approximated by numerous piecewise planar surfaces, where each planar surface enjoys its own rigid motion, and the global change in the scene between two frames is as-rigid-as-possible (ARAP). Consequently, our model of a dynamic scene reduces to a soup of planar structures and rigid motion of these local planar structures. Using planar over-segmentation of the scene, we reduce this task to solving a “3D jigsaw puzzle” problem. Hence, the task boils down to correctly assemble each rigid piece to construct a 3D shape that complies with the geometry of the scene under the ARAP assumption. Further, we show that our approach provides an effective solution to the inherent scale-ambiguity in structure-from-motion under perspective projection. We provide extensive experimental results and evaluation on several benchmark datasets. Quantitative comparison with competing approaches shows state-of-the-art performance.},
  archive      = {J_TPAMI},
  author       = {Suryansh Kumar and Yuchao Dai and Hongdong Li},
  doi          = {10.1109/TPAMI.2019.2955131},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {5},
  pages        = {1705-1717},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Superpixel soup: Monocular dense 3D reconstruction of a complex dynamic scene},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Structure from motion on XSlit cameras. <em>TPAMI</em>,
<em>43</em>(5), 1691–1704. (<a
href="https://doi.org/10.1109/TPAMI.2019.2957119">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a structure-from-motion (SfM) framework based on a special type of multi-perspective camera called the cross-slit or XSlit camera. Traditional perspective camera based SfM suffers from the scale ambiguity which is inherent to the pinhole camera geometry. In contrast, an XSlit camera captures rays passing through two oblique lines in 3D space and we show such ray geometry directly resolves the scale ambiguity when employed for SfM. To accommodate the XSlit cameras, we develop tailored feature matching, camera pose estimation, triangulation, and bundle adjustment techniques. Specifically, we devise a SIFT feature variant using non-uniform Gaussian kernels to handle the distortions in XSlit images for reliable feature matching. Moreover, we demonstrate that the XSlit camera exhibits ambiguities in pose estimation process which can not be handled by existing work. Consequently, we propose a 14 point algorithm to properly handle the XSlit degeneracy and estimate the relative pose between XSlit cameras from feature correspondences. We further exploit the unique depth-dependent aspect ratio (DDAR) property to improve the bundle adjustment for the XSlit camera. Synthetic and real experiments demonstrate that the proposed XSlit SfM can conduct reliable and high fidelity 3D reconstruction at an absolute scale.},
  archive      = {J_TPAMI},
  author       = {Wei Yang and Yingliang Zhang and Jinwei Ye and Yu Ji and Zhong Li and Mingyuan Zhou and Jingyi Yu},
  doi          = {10.1109/TPAMI.2019.2957119},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {5},
  pages        = {1691-1704},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Structure from motion on XSlit cameras},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Scalar quantization as sparse least square optimization.
<em>TPAMI</em>, <em>43</em>(5), 1678–1690. (<a
href="https://doi.org/10.1109/TPAMI.2019.2952096">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantization aims to form new vectors or matrices with shared values close to the original. In recent years, the popularity of scalar quantization has been soaring as it is found huge utilities in reducing the resource cost of neural networks. Popular clustering-based techniques suffers substantially from the problems of dependency on the seed, empty or out-of-the-range clusters, and high time complexity. To overcome the problems, in this paper, scalar quantization is examined from a new perspective, namely sparse least square optimization. Specifically, several quantization algorithms based on l 1 l1 least square are proposed and implemented. In addition, similar schemes with l 1 + l 2 l1+l2 and l 0 l0 regularization are proposed. Furthermore, to compute quantization results with given amount of values/clusters, this paper proposes an iterative method and a clustering-based method, and both of them are built on sparse least square optimization. The algorithms proposed are tested under three data scenarios and their computational performance, including information loss, time consumption, and distribution of values of sparse vectors are compared. The paper offers a new perspective to probe the area of quantization, and the algorithms proposed are superior especially under bit-width reduction scenarios, where the required post-quantization resolution (the number of values) is not significantly lower than the original scalar.},
  archive      = {J_TPAMI},
  author       = {Chen Wang and Xiaomei Yang and Shaomin Fei and Kai Zhou and Xiaofeng Gong and Miao Du and Ruisen Luo},
  doi          = {10.1109/TPAMI.2019.2952096},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {5},
  pages        = {1678-1690},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Scalar quantization as sparse least square optimization},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Quicker ADC: Unlocking the hidden potential of product
quantization with SIMD. <em>TPAMI</em>, <em>43</em>(5), 1666–1677. (<a
href="https://doi.org/10.1109/TPAMI.2019.2952606">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efficient Nearest Neighbor (NN) search in high-dimensional spaces is a foundation of many multimedia retrieval systems. A common approach is to rely on Product Quantization, which allows the storage of large vector databases in memory and efficient distance computations. Yet, implementations of nearest neighbor search with Product Quantization have their performance limited by the many memory accesses they perform. Following this observation, André et al. proposed Quick ADC with up to 6×6× faster implementations of PQ m×4m×4 product quantizers (PQ) leveraging specific SIMD instructions. Quicker ADC is a generalization of Quick ADC not limited to PQ m×4m×4 codes and supporting AVX-512, the latest revision of SIMD instruction set. In doing so, Quicker ADC faces the challenge of using efficiently 5,6 and 7-bit shuffles that do not align to computer bytes or words. To this end, we introduce (i) irregular product quantizers combining sub-quantizers of different granularity and (ii) split tables allowing lookup tables larger than registers. We evaluate Quicker ADC with multiple indexes including Inverted Multi-Indexes and IVF HNSW and show that it outperforms the reference optimized implementations (i.e., FAISS and polysemous codes) for numerous configurations. Finally, we release an open-source fork of FAISS enhanced with Quicker ADC.},
  archive      = {J_TPAMI},
  author       = {Fabien André and Anne-Marie Kermarrec and Nicolas Le Scouarnec},
  doi          = {10.1109/TPAMI.2019.2952606},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {5},
  pages        = {1666-1677},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Quicker ADC: Unlocking the hidden potential of product quantization with SIMD},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Person re-identification with deep kronecker-product
matching and group-shuffling random walk. <em>TPAMI</em>,
<em>43</em>(5), 1649–1665. (<a
href="https://doi.org/10.1109/TPAMI.2019.2954313">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person re-identification (re-ID) aims to robustly measure visual affinities between person images. It has wide applications in intelligent surveillance by associating same persons&#39; images across multiple cameras. It is generally treated as an image retrieval problem: given a probe person image, the affinities between the probe image and gallery images (P2G affinities) are used to rank the retrieved gallery images. There exist two main challenges for effectively solving this problem. 1) Person images usually show significant variations because of different person poses and viewing angles. The spatial layouts and correspondences between person images are therefore vital information for tackling this problem. State-of-the-art methods either ignore such spatial variation or utilize extra pose information for handling the challenge. 2) Most existing person re-ID methods rank gallery images considering only P2G affinities but ignore the affinities between the gallery images (G2G affinity). Such affinities could provide important clues for accurate gallery image ranking but were only utilized in post-processing stages by current methods. In this article, we propose a unified end-to-end deep learning framework to tackle the two challenges. For handling viewpoint and pose variations between compared person images, we propose a novel Kronecker Product Matching operation to match and warp feature maps of different persons. Comparing warped feature maps results in more accurate P2G affinities. To fully utilize all available P2G and G2G affinities for accurately ranking gallery person images, a novel group-shuffling random walk operation is proposed. Both Kronecker Product Matching and Group-shuffling Random Walk operations are end-to-end trainable and are shown to improve the learned visual features if integrated in the deep learning framework. The proposed approach outperforms state-of-the-art methods on Market-1501, CUHK03 and DukeMTMC datasets, which demonstrates the effectiveness and generalization ability of our proposed approach. Code is available at https://github.com/YantaoShen/kpm_rw_person_reid.},
  archive      = {J_TPAMI},
  author       = {Yantao Shen and Tong Xiao and Shuai Yi and Dapeng Chen and Xiaogang Wang and Hongsheng Li},
  doi          = {10.1109/TPAMI.2019.2954313},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {5},
  pages        = {1649-1665},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Person re-identification with deep kronecker-product matching and group-shuffling random walk},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Ordinal multi-task part segmentation with recurrent prior
generation. <em>TPAMI</em>, <em>43</em>(5), 1636–1648. (<a
href="https://doi.org/10.1109/TPAMI.2019.2953854">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic object part segmentation is a fundamental task in object understanding and geometric analysis. The clear understanding of part relationships can be of great use to the segmentation process. In this work, we propose a novel Ordinal Multi-task Part Segmentation (OMPS) approach which explicitly models the part ordinal relationship to guide the segmentation process in a recurrent manner. Quantitative and qualitative experiments are conducted first to explore the mutual impacts among object parts and then an ordinal part inference algorithm is formulated via experimental observations. Specifically, our framework is mainly composed of two modules, the forward module to segment multiple parts as individual subtasks with prior knowledge, and the recurrent module to generate appropriate part priors with the ordinal inference algorithm. These two modules work iteratively to optimize the segmentation performance and the network parameters. Experimental results show that our approach outperforms the state-of-the-art models on human and vehicle part parsing benchmarks. Comprehensive evaluations are conducted to demonstrate the effectiveness of our approach in object part segmentation.},
  archive      = {J_TPAMI},
  author       = {Yifan Zhao and Jia Li and Yu Zhang and Yafei Song and Yonghong Tian},
  doi          = {10.1109/TPAMI.2019.2953854},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {5},
  pages        = {1636-1648},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Ordinal multi-task part segmentation with recurrent prior generation},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On symbiosis of attribute prediction and semantic
segmentation. <em>TPAMI</em>, <em>43</em>(5), 1620–1635. (<a
href="https://doi.org/10.1109/TPAMI.2019.2956039">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attributes are semantically meaningful characteristics whose applicability widely crosses category boundaries. They are particularly important in describing and recognizing concepts for which no explicit training example is given, e.g., zero-shot learning. Additionally, since attributes are human describable, they can be used for efficient human-computer interaction. In this article, we propose to employ semantic segmentation to improve person-related attribute prediction. The core idea lies in the fact that many attributes describe local properties. In other words, the probability of an attribute to appear in an image is far from being uniform in the spatial domain. We build our attribute prediction model jointly with a deep semantic segmentation network. This harnesses the localization cues learned by the semantic segmentation to guide the attention of the attribute prediction to the regions where different attributes naturally show up. As a result of this approach, in addition to prediction, we are able to localize the attributes despite merely having access to image-level labels (weak supervision) during training. We first propose semantic segmentation-based pooling and gating, respectively denoted as SSP and SSG. In the former, the estimated segmentation masks are used to pool the final activations of the attribute prediction network, from multiple semantically homogeneous regions. This is in contrast to global average pooling which is agnostic with respect to where in the spatial domain activations occur. In SSG, the same idea is applied to the intermediate layers of the network. Specifically, we create multiple copies of the internal activations. In each copy, only values that fall within a certain semantic region are preserved while outside of that, activations are suppressed. This mechanism allows us to prevent pooling operation from blending activations that are associated with semantically different regions. SSP and SSG, while effective, impose heavy memory utilization since each channel of the activations is pooled/gated with all the semantic segmentation masks. To circumvent this, we propose Symbiotic Augmentation (SA), where we learn only one mask per activation channel. SA allows the model to either pick one, or combine (weighted superposition) multiple semantic maps, in order to generate the proper mask for each channel. SA simultaneously applies the same mechanism to the reverse problem by leveraging output logits of attribute prediction to guide the semantic segmentation task. We evaluate our proposed methods for facial attributes on CelebA and LFWA datasets, while benchmarking WIDER Attribute and Berkeley Attributes of People for whole body attributes. Our proposed methods achieve superior results compared to the previous works. Furthermore, we show that in the reverse problem, semantic face parsing significantly improves when its associated task is jointly learned, through our proposed Symbiotic Augmentation (SA), with facial attribute prediction. We confirm that when few training instances are available, indeed image-level facial attribute labels can serve as an effective source of weak supervision to improve semantic face parsing. That reaffirms the need to jointly model these two interconnected tasks.},
  archive      = {J_TPAMI},
  author       = {Mahdi M. Kalayeh and Mubarak Shah},
  doi          = {10.1109/TPAMI.2019.2956039},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {5},
  pages        = {1620-1635},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {On symbiosis of attribute prediction and semantic segmentation},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning to localize sound sources in visual scenes:
Analysis and applications. <em>TPAMI</em>, <em>43</em>(5), 1605–1619.
(<a href="https://doi.org/10.1109/TPAMI.2019.2952095">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual events are usually accompanied by sounds in our daily lives. However, can the machines learn to correlate the visual scene and sound, as well as localize the sound source only by observing them like humans? To investigate its empirical learnability, in this work we first present a novel unsupervised algorithm to address the problem of localizing sound sources in visual scenes. In order to achieve this goal, a two-stream network structure which handles each modality with attention mechanism is developed for sound source localization. The network naturally reveals the localized response in the scene without human annotation. In addition, a new sound source dataset is developed for performance evaluation. Nevertheless, our empirical evaluation shows that the unsupervised method generates false conclusions in some cases. Thereby, we show that this false conclusion cannot be fixed without human prior knowledge due to the well-known correlation and causality mismatch misconception. To fix this issue, we extend our network to the supervised and semi-supervised network settings via a simple modification due to the general architecture of our two-stream network. We show that the false conclusions can be effectively corrected even with a small amount of supervision, i.e., semi-supervised setup. Furthermore, we present the versatility of the learned audio and visual embeddings on the cross-modal content alignment and we extend this proposed algorithm to a new application, sound saliency based automatic camera view panning in 360 degree videos.},
  archive      = {J_TPAMI},
  author       = {Arda Senocak and Tae-Hyun Oh and Junsik Kim and Ming-Hsuan Yang and In So Kweon},
  doi          = {10.1109/TPAMI.2019.2952095},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {5},
  pages        = {1605-1619},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Learning to localize sound sources in visual scenes: Analysis and applications},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Image-based 3D object reconstruction: State-of-the-art and
trends in the deep learning era. <em>TPAMI</em>, <em>43</em>(5),
1578–1604. (<a
href="https://doi.org/10.1109/TPAMI.2019.2954885">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D reconstruction is a longstanding ill-posed problem, which has been explored for decades by the computer vision, computer graphics, and machine learning communities. Since 2015, image-based 3D reconstruction using convolutional neural networks (CNN) has attracted increasing interest and demonstrated an impressive performance. Given this new era of rapid evolution, this article provides a comprehensive survey of the recent developments in this field. We focus on the works which use deep learning techniques to estimate the 3D shape of generic objects either from a single or multiple RGB images. We organize the literature based on the shape representations, the network architectures, and the training mechanisms they use. While this survey is intended for methods which reconstruct generic objects, we also review some of the recent works which focus on specific object classes such as human body shapes and faces. We provide an analysis and comparison of the performance of some key papers, summarize some of the open problems in this field, and discuss promising directions for future research.},
  archive      = {J_TPAMI},
  author       = {Xian-Feng Han and Hamid Laga and Mohammed Bennamoun},
  doi          = {10.1109/TPAMI.2019.2954885},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {5},
  pages        = {1578-1604},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Image-based 3D object reconstruction: State-of-the-art and trends in the deep learning era},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). GOT-10k: A large high-diversity benchmark for generic object
tracking in the wild. <em>TPAMI</em>, <em>43</em>(5), 1562–1577. (<a
href="https://doi.org/10.1109/TPAMI.2019.2957464">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce here a large tracking database that offers an unprecedentedly wide coverage of common moving objects in the wild, called GOT-10k. Specifically, GOT-10k is built upon the backbone of WordNet structure [1] and it populates the majority of over 560 classes of moving objects and 87 motion patterns, magnitudes wider than the most recent similar-scale counterparts [19], [20], [23], [26]. By releasing the large high-diversity database, we aim to provide a unified training and evaluation platform for the development of class-agnostic, generic purposed short-term trackers. The features of GOT-10k and the contributions of this article are summarized in the following. (1) GOT-10k offers over 10,000 video segments with more than 1.5 million manually labeled bounding boxes, enabling unified training and stable evaluation of deep trackers. (2) GOT-10k is by far the first video trajectory dataset that uses the semantic hierarchy of WordNet to guide class population, which ensures a comprehensive and relatively unbiased coverage of diverse moving objects. (3) For the first time, GOT-10k introduces the one-shot protocol for tracker evaluation, where the training and test classes are zero-overlapped. The protocol avoids biased evaluation results towards familiar objects and it promotes generalization in tracker development. (4) GOT-10k offers additional labels such as motion classes and object visible ratios, facilitating the development of motion-aware and occlusion-aware trackers. (5) We conduct extensive tracking experiments with 39 typical tracking algorithms and their variants on GOT-10k and analyze their results in this paper. (6) Finally, we develop a comprehensive platform for the tracking community that offers full-featured evaluation toolkits, an online evaluation server, and a responsive leaderboard. The annotations of GOT-10k&#39;s test data are kept private to avoid tuning parameters on it.},
  archive      = {J_TPAMI},
  author       = {Lianghua Huang and Xin Zhao and Kaiqi Huang},
  doi          = {10.1109/TPAMI.2019.2957464},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {5},
  pages        = {1562-1577},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {GOT-10k: A large high-diversity benchmark for generic object tracking in the wild},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Generalized separable nonnegative matrix factorization.
<em>TPAMI</em>, <em>43</em>(5), 1546–1561. (<a
href="https://doi.org/10.1109/TPAMI.2019.2956046">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nonnegative matrix factorization (NMF) is a linear dimensionality technique for nonnegative data with applications such as image analysis, text mining, audio source separation, and hyperspectral unmixing. Given a data matrix MM and a factorization rank rr, NMF looks for a nonnegative matrix WW with rr columns and a nonnegative matrix HH with rr rows such that M ≈ WHM≈WH. NMF is NP-hard to solve in general. However, it can be computed efficiently under the separability assumption which requires that the basis vectors appear as data points, that is, that there exists an index set K K such that W = M(:, K )W=M(:,K). In this article, we generalize the separability assumption. We only require that for each rank-one factor W(:,k)H(k,:)W(:,k)H(k,:) for k=1,2,...,rk=1,2,...,r, either W(:,k) = M(:,j)W(:,k)=M(:,j) for some jj or H(k,:) = M(i,:)H(k,:)=M(i,:) for some ii. We refer to the corresponding problem as generalized separable NMF (GS-NMF). We discuss some properties of GS-NMF and propose a convex optimization model which we solve using a fast gradient method. We also propose a heuristic algorithm inspired by the successive projection algorithm. To verify the effectiveness of our methods, we compare them with several state-of-the-art separable NMF and standard NMF algorithms on synthetic, document and image data sets.},
  archive      = {J_TPAMI},
  author       = {Junjun Pan and Nicolas Gillis},
  doi          = {10.1109/TPAMI.2019.2956046},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {5},
  pages        = {1546-1561},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Generalized separable nonnegative matrix factorization},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Exploring discriminative word-level domain contexts for
multi-domain neural machine translation. <em>TPAMI</em>, <em>43</em>(5),
1530–1545. (<a
href="https://doi.org/10.1109/TPAMI.2019.2954406">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Owing to its practical significance, multi-domain Neural Machine Translation (NMT) has attracted much attention recently. Recent studies mainly focus on constructing a unified NMT model with mixed-domain training corpora to switch translation between different domains. In these models, the words in the same sentence are not well distinguished, while intuitively, they are related to the sentence domain to varying degrees and thus should exert different effects on the multi-domain NMT model. In this article, we are committed to distinguishing and exploiting different word-level domain contexts for multi-domain NMT. For this purpose, we adopt multi-task learning to jointly model NMT and monolingual attention-based domain classification tasks, improving the NMT model in two ways: 1) One domain classifier and one adversarial domain classifier are introduced to conduct domain classifications of input sentences. During this process, two generated gating vectors are used to produce domain-specific and domain-shared annotations for decoder; 2) We equip decoder with an attentional domain classifier. Then, the derived attentional weights are utilized to refine the model training via word-level cost weighting, so that the impacts of target words can be discriminated by their relevance to sentence domain. Experimental results on several multi-domain translations demonstrate the effectiveness of our model.},
  archive      = {J_TPAMI},
  author       = {Jinsong Su and Jiali Zeng and Jun Xie and Huating Wen and Yongjing Yin and Yang Liu},
  doi          = {10.1109/TPAMI.2019.2954406},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {5},
  pages        = {1530-1545},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Exploring discriminative word-level domain contexts for multi-domain neural machine translation},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dynamical hyperparameter optimization via deep reinforcement
learning in tracking. <em>TPAMI</em>, <em>43</em>(5), 1515–1529. (<a
href="https://doi.org/10.1109/TPAMI.2019.2956703">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperparameters are numerical pre-sets whose values are assigned prior to the commencement of a learning process. Selecting appropriate hyperparameters is often critical for achieving satisfactory performance in many vision problems, such as deep learning-based visual object tracking. However, it is often difficult to determine their optimal values, especially if they are specific to each video input. Most hyperparameter optimization algorithms tend to search a generic range and are imposed blindly on all sequences. In this paper, we propose a novel dynamical hyperparameter optimization method that adaptively optimizes hyperparameters for a given sequence using an action-prediction network leveraged on continuous deep Q-learning. Since the observation space for object tracking is significantly more complex than those in traditional control problems, existing continuous deep Q-learning algorithms cannot be directly applied. To overcome this challenge, we introduce an efficient heuristic strategy to handle high dimensional state space, while also accelerating the convergence behavior. The proposed algorithm is applied to improve two representative trackers, a Siamese-based one and a correlation-filter-based one, to evaluate its generalizability. Their superior performances on several popular benchmarks are clearly demonstrated. Our source code is available at https://github.com/shenjianbing/dqltracking.},
  archive      = {J_TPAMI},
  author       = {Xingping Dong and Jianbing Shen and Wenguan Wang and Ling Shao and Haibin Ling and Fatih Porikli},
  doi          = {10.1109/TPAMI.2019.2956703},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {5},
  pages        = {1515-1529},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Dynamical hyperparameter optimization via deep reinforcement learning in tracking},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DBF: Dynamic belief fusion for combining multiple object
detectors. <em>TPAMI</em>, <em>43</em>(5), 1499–1514. (<a
href="https://doi.org/10.1109/TPAMI.2019.2952847">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose a novel and highly practical score-level fusion approach called dynamic belief fusion (DBF) that directly integrates inference scores of individual detections from multiple object detection methods. To effectively integrate the individual outputs of multiple detectors, the level of ambiguity in each detection score is estimated using a confidence model built on a precision-recall relationship of the corresponding detector. For each detector output, DBF then calculates the probabilities of three hypotheses (target, non-target, and intermediate state (target or non-target)) based on the confidence level of the detection score conditioned on the prior confidence model of individual detectors, which is referred to as basic probability assignment. The probability distributions over three hypotheses of all the detectors are optimally fused via the Dempster&#39;s combination rule. Experiments on the ARL, PASCAL VOC 07, and 12 datasets show that the detection accuracy of the DBF is significantly higher than any of the baseline fusion approaches as well as individual detectors used for the fusion.},
  archive      = {J_TPAMI},
  author       = {Hyungtae Lee and Heesung Kwon},
  doi          = {10.1109/TPAMI.2019.2952847},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {5},
  pages        = {1499-1514},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {DBF: Dynamic belief fusion for combining multiple object detectors},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Cascade r-CNN: High quality object detection and instance
segmentation. <em>TPAMI</em>, <em>43</em>(5), 1483–1498. (<a
href="https://doi.org/10.1109/TPAMI.2019.2956516">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In object detection, the intersection over union (IoU) threshold is frequently used to define positives/negatives. The threshold used to train a detector defines its quality. While the commonly used threshold of 0.5 leads to noisy (low-quality) detections, detection performance frequently degrades for larger thresholds. This paradox of high-quality detection has two causes: 1) overfitting, due to vanishing positive samples for large thresholds, and 2) inference-time quality mismatch between detector and test hypotheses. A multi-stage object detection architecture, the Cascade R-CNN, composed of a sequence of detectors trained with increasing IoU thresholds, is proposed to address these problems. The detectors are trained sequentially, using the output of a detector as training set for the next. This resampling progressively improves hypotheses quality, guaranteeing a positive training set of equivalent size for all detectors and minimizing overfitting. The same cascade is applied at inference, to eliminate quality mismatches between hypotheses and detectors. An implementation of the Cascade R-CNN without bells or whistles achieves state-of-the-art performance on the COCO dataset, and significantly improves high-quality detection on generic and specific object datasets, including VOC, KITTI, CityPerson, and WiderFace. Finally, the Cascade R-CNN is generalized to instance segmentation, with nontrivial improvements over the Mask R-CNN.},
  archive      = {J_TPAMI},
  author       = {Zhaowei Cai and Nuno Vasconcelos},
  doi          = {10.1109/TPAMI.2019.2956516},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {5},
  pages        = {1483-1498},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Cascade R-CNN: High quality object detection and instance segmentation},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). AD-VAT+: An asymmetric dueling mechanism for learning and
understanding visual active tracking. <em>TPAMI</em>, <em>43</em>(5),
1467–1482. (<a
href="https://doi.org/10.1109/TPAMI.2019.2952590">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual Active Tracking (VAT) aims at following a target object by autonomously controlling the motion system of a tracker given visual observations. To learn a robust tracker for VAT, in this article, we propose a novel adversarial reinforcement learning (RL) method which adopts an Asymmetric Dueling mechanism, referred to as AD-VAT. In the mechanism, the tracker and target, viewed as two learnable agents, are opponents and can mutually enhance each other during the dueling/competition: i.e., the tracker intends to lockup the target, while the target tries to escape from the tracker. The dueling is asymmetric in that the target is additionally fed with the tracker&#39;s observation and action, and learns to predict the tracker&#39;s reward as an auxiliary task. Such an asymmetric dueling mechanism produces a stronger target, which in turn induces a more robust tracker. To improve the performance of the tracker in the case of challenging scenarios such as obstacles, we employ more advanced environment augmentation technique and two-stage training strategies, termed as AD-VAT+. For a better understanding of the asymmetric dueling mechanism, we also analyze the target&#39;s behaviors as the training proceeds and visualize the latent space of the tracker. The experimental results, in both 2D and 3D environments, demonstrate that the proposed method leads to a faster convergence in training and yields more robust tracking behaviors in different testing scenarios. The potential of the active tracker is also shown in real-world videos.},
  archive      = {J_TPAMI},
  author       = {Fangwei Zhong and Peng Sun and Wenhan Luo and Tingyun Yan and Yizhou Wang},
  doi          = {10.1109/TPAMI.2019.2952590},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {5},
  pages        = {1467-1482},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {AD-VAT+: An asymmetric dueling mechanism for learning and understanding visual active tracking},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Ordered or orderless: A revisit for video based person
re-identification. <em>TPAMI</em>, <em>43</em>(4), 1460–1466. (<a
href="https://doi.org/10.1109/TPAMI.2020.2976969">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Is recurrent network really necessary for learning a good visual representation for video based person re-identification (VPRe-id)? In this paper, we first show that the common practice of employing recurrent neural networks (RNNs) to aggregate temporal-spatial features may not be optimal. Specifically, with a diagnostic analysis, we show that the recurrent structure may not be effective learn temporal dependencies than what we expected and implicitly yields an orderless representation. Based on this observation, we then present a simple yet surprisingly powerful approach for VPRe-id, where we treat VPRe-id as an efficient orderless ensemble of image based person re-identification problem. More specifically, we divide videos into individual images and re-identify person with ensemble of image based rankers. Under the i.i.d. assumption, we provide an error bound that sheds light upon how could we improve VPRe-id. Our work also presents a promising way to bridge the gap between video and image based person re-identification. Comprehensive experimental evaluations demonstrate that the proposed solution achieves state-of-the-art performances on multiple widely used datasets (iLIDS-VID, PRID 2011, and MARS).},
  archive      = {J_TPAMI},
  author       = {Le Zhang and Zenglin Shi and Joey Tianyi Zhou and Ming-Ming Cheng and Yun Liu and Jia-Wang Bian and Zeng Zeng and Chunhua Shen},
  doi          = {10.1109/TPAMI.2020.2976969},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {1460-1466},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Ordered or orderless: A revisit for video based person re-identification},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Gliding vertex on the horizontal bounding box for
multi-oriented object detection. <em>TPAMI</em>, <em>43</em>(4),
1452–1459. (<a
href="https://doi.org/10.1109/TPAMI.2020.2974745">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object detection has recently experienced substantial progress. Yet, the widely adopted horizontal bounding box representation is not appropriate for ubiquitous oriented objects such as objects in aerial images and scene texts. In this paper, we propose a simple yet effective framework to detect multi-oriented objects. Instead of directly regressing the four vertices, we glide the vertex of the horizontal bounding box on each corresponding side to accurately describe a multi-oriented object. Specifically, We regress four length ratios characterizing the relative gliding offset on each corresponding side. This may facilitate the offset learning and avoid the confusion issue of sequential label points for oriented objects. To further remedy the confusion issue for nearly horizontal objects, we also introduce an obliquity factor based on area ratio between the object and its horizontal bounding box, guiding the selection of horizontal or oriented detection for each object. We add these five extra target variables to the regression head of faster R-CNN, which requires ignorable extra computation time. Extensive experimental results demonstrate that without bells and whistles, the proposed method achieves superior performances on multiple multi-oriented object detection benchmarks including object detection in aerial images, scene text detection, pedestrian detection in fisheye images.},
  archive      = {J_TPAMI},
  author       = {Yongchao Xu and Mingtao Fu and Qimeng Wang and Yukang Wang and Kai Chen and Gui-Song Xia and Xiang Bai},
  doi          = {10.1109/TPAMI.2020.2974745},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {1452-1459},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Gliding vertex on the horizontal bounding box for multi-oriented object detection},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep multi-view enhancement hashing for image retrieval.
<em>TPAMI</em>, <em>43</em>(4), 1445–1451. (<a
href="https://doi.org/10.1109/TPAMI.2020.2975798">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hashing is an efficient method for nearest neighbor search in large-scale data space by embedding high-dimensional feature descriptors into a similarity preserving Hamming space with a low dimension. However, large-scale high-speed retrieval through binary code has a certain degree of reduction in retrieval accuracy compared to traditional retrieval methods. We have noticed that multi-view methods can well preserve the diverse characteristics of data. Therefore, we try to introduce the multi-view deep neural network into the hash learning field, and design an efficient and innovative retrieval model, which has achieved a significant improvement in retrieval performance. In this paper, we propose a supervised multi-view hash model which can enhance the multi-view information through neural networks. This is a completely new hash learning method that combines multi-view and deep learning methods. The proposed method utilizes an effective view stability evaluation method to actively explore the relationship among views, which will affect the optimization direction of the entire network. We have also designed a variety of multi-data fusion methods in the Hamming space to preserve the advantages of both convolution and multi-view. In order to avoid excessive computing resources on the enhancement procedure during retrieval, we set up a separate structure called memory network which participates in training together. The proposed method is systematically evaluated on the CIFAR-10, NUS-WIDE and MS-COCO datasets, and the results show that our method significantly outperforms the state-of-the-art single-view and multi-view hashing methods.},
  archive      = {J_TPAMI},
  author       = {Chenggang Yan and Biao Gong and Yuxuan Wei and Yue Gao},
  doi          = {10.1109/TPAMI.2020.2975798},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {1445-1451},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Deep multi-view enhancement hashing for image retrieval},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A sparse sampling-based framework for semantic fast-forward
of first-person videos. <em>TPAMI</em>, <em>43</em>(4), 1438–1444. (<a
href="https://doi.org/10.1109/TPAMI.2020.2983929">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Technological advances in sensors have paved the way for digital cameras to become increasingly ubiquitous, which, in turn, led to the popularity of the self-recording culture. As a result, the amount of visual data on the Internet is moving in the opposite direction of the available time and patience of the users. Thus, most of the uploaded videos are doomed to be forgotten and unwatched stashed away in some computer folder or website. In this paper, we address the problem of creating smooth fast-forward videos without losing the relevant content. We present a new adaptive frame selection formulated as a weighted minimum reconstruction problem. Using a smoothing frame transition and filling visual gaps between segments, our approach accelerates first-person videos emphasizing the relevant segments and avoids visual discontinuities. Experiments conducted on controlled videos and also on an unconstrained dataset of First-Person Videos (FPVs) show that, when creating fast-forward videos, our method is able to retain as much relevant information and smoothness as the state-of-the-art techniques, but in less processing time.},
  archive      = {J_TPAMI},
  author       = {Michel Silva and Washington Ramos and Mario Campos and Erickson R. Nascimento},
  doi          = {10.1109/TPAMI.2020.2983929},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {1438-1444},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {A sparse sampling-based framework for semantic fast-forward of first-person videos},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Weakly-supervised learning of category-specific 3D object
shapes. <em>TPAMI</em>, <em>43</em>(4), 1423–1437. (<a
href="https://doi.org/10.1109/TPAMI.2019.2949562">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Category-specific 3D object shape models have greatly boosted the recent advances in object detection, recognition and segmentation. However, even the most advanced approach for learning 3D object shapes still requires heavy manual annotations on large-scale 2D images. Such annotations include object categories, object keypoints, and figure-ground segmentation for the instances in each image. In particular, annotating figure-ground segmentation is unbearably labor-intensive and time-consuming. To address this problem, this paper devotes to learn category-specific 3D shape models under weak supervision, where only object categories and keypoints are required to be manually annotated on the training 2D images. By exploring the underlying relationship between two tasks: object segmentation and category-specific 3D shape reconstruction, we propose a novel weakly-supervised learning framework to jointly address these two tasks and combine them to boost the final performance of the learned 3D shape models. Moreover, learning without using figure-ground segmentation leads to ambiguous solutions. To this end, we develop the confidence weighting schemes in the viewpoint estimation and 3D shape learning procedure. These schemes effectively reduce the confusion caused by the noisy data and thus increase the chances for recovering more reliable 3D object shapes. Comprehensive experiments on the challenging PASCAL VOC benchmark show that our framework achieves comparable performance with the state-of-the-art methods that use expensive manual segmentation-level annotations. In addition, our experiments also demonstrate that our 3D shape models improve object segmentation performance.},
  archive      = {J_TPAMI},
  author       = {Junwei Han and Yang Yang and Dingwen Zhang and Dong Huang and Dong Xu and Fernando De La Torre},
  doi          = {10.1109/TPAMI.2019.2949562},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {1423-1437},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Weakly-supervised learning of category-specific 3D object shapes},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Visual semantic information pursuit: A survey.
<em>TPAMI</em>, <em>43</em>(4), 1404–1422. (<a
href="https://doi.org/10.1109/TPAMI.2019.2950025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual semantic information comprises two important parts: the meaning of each visual semantic unit and the coherent visual semantic relation conveyed by these visual semantic units. Essentially, the former one is a visual perception task while the latter corresponds to visual context reasoning. Remarkable advances in visual perception have been achieved due to the success of deep learning. In contrast, visual semantic information pursuit, a visual scene semantic interpretation task combining visual perception and visual context reasoning, is still in its early stage. It is the core task of many different computer vision applications, such as object detection, visual semantic segmentation, visual relationship detection, or scene graph generation. Since it helps to enhance the accuracy and the consistency of the resulting interpretation, visual context reasoning is often incorporated with visual perception in current deep end-to-end visual semantic information pursuit methods. Surprisingly, a comprehensive review for this exciting area is still lacking. In this survey, we present a unified theoretical paradigm for all these methods, followed by an overview of the major developments and the future trends in each potential direction. The common benchmark datasets, the evaluation metrics and the comparisons of the corresponding methods are also introduced.},
  archive      = {J_TPAMI},
  author       = {Daqi Liu and Miroslaw Bober and Josef Kittler},
  doi          = {10.1109/TPAMI.2019.2950025},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {1404-1422},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Visual semantic information pursuit: A survey},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SpaRTA tracking across occlusions via partitioning of 3D
clouds of points. <em>TPAMI</em>, <em>43</em>(4), 1394–1403. (<a
href="https://doi.org/10.1109/TPAMI.2019.2946796">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Any 3D tracking algorithm has to deal with occlusions: multiple targets get so close to each other that the loss of their identities becomes likely; hence, potentially affecting the very quality of the data with interrupted trajectories and identity switches. Here, we present a novel tracking method that addresses the problem of occlusions within large groups of featureless objects by means of three steps: i) it represents each target as a cloud of points in 3D; ii) once a 3D cluster corresponding to an occlusion occurs, it defines a partitioning problem by introducing a cost function that uses both attractive and repulsive spatio-temporal proximity links; and iii) it minimizes the cost function through a semi-definite optimization technique specifically designed to cope with the presence of multiminima landscapes. The algorithm is designed to work on 3D data regardless of the experimental method used: multicamera systems, lidars, radars, and RGB-D systems. By performing tests on public data-sets, we show that the new algorithm produces a significant improvement over the state-of-the-art tracking methods, both by reducing the number of identity switches and by increasing the accuracy of the estimated positions of the targets in real space.},
  archive      = {J_TPAMI},
  author       = {Andrea Cavagna and Stefania Melillo and Leonardo Parisi and Federico Ricci-Tersenghi},
  doi          = {10.1109/TPAMI.2019.2946796},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {1394-1403},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {SpaRTA tracking across occlusions via partitioning of 3D clouds of points},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sinusoidal sampling enhanced compressive camera for high
speed imaging. <em>TPAMI</em>, <em>43</em>(4), 1380–1393. (<a
href="https://doi.org/10.1109/TPAMI.2019.2946567">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compressive sensing technique allows capturing fast phenomena at a much higher frame rate than the camera sensor, by recovering a frame sequence from their encoded combination. However, most conventional compressive video sensing methods limit the achieved frame rate improvement to tenfold and only support low resolution recovery. Making use of the camera&#39;s redundant spatial resolution for further frame rate improve, here we report a novel compressive video acquisition technique termed Sinusoidal Sampling Enhanced Compressive Camera (S2EC2) to encode denser frames within a snapshot. Specifically, we decompose the dense frames into groups and apply combinational coding: random codes within each group for compressive acquisition; group specific sinusoidal codes to multiplex different groups onto the high resolution sensor. The sinusoidal codes designed for these groups would shift their frequency components by different offsets in the Fourier domain and staggered the dominant frequencies of the coded measurements of these groups. Correspondingly, the reconstruction successfully separate coded measurements of different groups and recovers frames within each group. Besides, we also solve the implementation problem of insufficient gray scale spatial light modulation speed, and build a prototype achieving 2000 fps reconstruction with a 15.6 fps camera (the actual compression ratio is 0.009). The extensive experiments validate the proposed approach.},
  archive      = {J_TPAMI},
  author       = {Chao Deng and Yuanlong Zhang and Yifeng Mao and Jingtao Fan and Jinli Suo and Zhili Zhang and Qionghai Dai},
  doi          = {10.1109/TPAMI.2019.2946567},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {1380-1393},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Sinusoidal sampling enhanced compressive camera for high speed imaging},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Semi-supervised semantic segmentation with high- and
low-level consistency. <em>TPAMI</em>, <em>43</em>(4), 1369–1379. (<a
href="https://doi.org/10.1109/TPAMI.2019.2960224">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ability to understand visual information from limited labeled data is an important aspect of machine learning. While image-level classification has been extensively studied in a semi-supervised setting, dense pixel-level classification with limited data has only drawn attention recently. In this work, we propose an approach for semi-supervised semantic segmentation that learns from limited pixel-wise annotated samples while exploiting additional annotation-free images. The proposed approach relies on adversarial training with a feature matching loss to learn from unlabeled images. It uses two network branches that link semi-supervised classification with semi-supervised segmentation including self-training. The dual-branch approach reduces both the low-level and the high-level artifacts typical when training with few labels. The approach attains significant improvement over existing methods, especially when trained with very few labeled samples. On several standard benchmarks-PASCAL VOC 2012, PASCAL-Context, and Cityscapes-the approach achieves new state-of-the-art in semi-supervised learning.},
  archive      = {J_TPAMI},
  author       = {Sudhanshu Mittal and Maxim Tatarchenko and Thomas Brox},
  doi          = {10.1109/TPAMI.2019.2960224},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {1369-1379},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Semi-supervised semantic segmentation with high- and low-level consistency},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Orthogonal deep neural networks. <em>TPAMI</em>,
<em>43</em>(4), 1352–1368. (<a
href="https://doi.org/10.1109/TPAMI.2019.2948352">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we introduce the algorithms of Orthogonal Deep Neural Networks (OrthDNNs) to connect with recent interest of spectrally regularized deep learning methods. OrthDNNs are theoretically motivated by generalization analysis of modern DNNs, with the aim to find solution properties of network weights that guarantee better generalization. To this end, we first prove that DNNs are of local isometry on data distributions of practical interest; by using a new covering of the sample space and introducing the local isometry property of DNNs into generalization analysis, we establish a new generalization error bound that is both scale- and range-sensitive to singular value spectrum of each of networks’ weight matrices. We prove that the optimal bound w.r.t. the degree of isometry is attained when each weight matrix has a spectrum of equal singular values, among which orthogonal weight matrix or a non-square one with orthonormal rows or columns is the most straightforward choice, suggesting the algorithms of OrthDNNs. We present both algorithms of strict and approximate OrthDNNs, and for the later ones we propose a simple yet effective algorithm called Singular Value Bounding (SVB), which performs as well as strict OrthDNNs, but at a much lower computational cost. We also propose Bounded Batch Normalization (BBN) to make compatible use of batch normalization with OrthDNNs. We conduct extensive comparative studies by using modern architectures on benchmark image classification. Experiments show the efficacy of OrthDNNs.},
  archive      = {J_TPAMI},
  author       = {Shuai Li and Kui Jia and Yuxin Wen and Tongliang Liu and Dacheng Tao},
  doi          = {10.1109/TPAMI.2019.2948352},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {1352-1368},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Orthogonal deep neural networks},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Large scale shadow annotation and detection using lazy
annotation and stacked CNNs. <em>TPAMI</em>, <em>43</em>(4), 1337–1351.
(<a href="https://doi.org/10.1109/TPAMI.2019.2948011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent shadow detection algorithms have shown initial success on small datasets of images from specific domains. However, shadow detection on broader image domains is still challenging due to the lack of annotated training data, caused by the intense manual labor required for annotating shadow data. In this paper we propose “lazy annotation”, an efficient annotation method where an annotator only needs to mark the important shadow areas and some non-shadow areas. This yields data with noisy labels that are not yet useful for training a shadow detector. We address the problem of label noise by jointly learning a shadow region classifier and recovering the labels in the training set. We consider the training labels as unknowns and formulate label recovery as the minimization of the sum of squared leave-one-out errors of a Least Squares SVM, which can be efficiently optimized. Experimental results show that a classifier trained with recovered labels achieves comparable performance to a classifier trained on the properly annotated data. These results motivated us to collect a new dataset that is 20 times larger than existing datasets and contains a large variety of scenes and image types. Naturally, such a large dataset is appropriate for training deep learning methods. Thus, we propose a stacked Convolutional Neural Network architecture that efficiently trains on patch level shadow examples while incorporating image level semantic information. This means that the detected shadow patches are refined based on image semantics. Our proposed pipeline, trained on recovered labels, performs at state-of-the art level. Furthermore, the proposed model performs exceptionally well on a cross dataset task, proving the generalization power of the proposed architecture and dataset.},
  archive      = {J_TPAMI},
  author       = {Le Hou and Tomás F. Yago Vicente and Minh Hoai and Dimitris Samaras},
  doi          = {10.1109/TPAMI.2019.2948011},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {1337-1351},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Large scale shadow annotation and detection using lazy annotation and stacked CNNs},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Joint embedding of graphs. <em>TPAMI</em>, <em>43</em>(4),
1324–1336. (<a
href="https://doi.org/10.1109/TPAMI.2019.2948619">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature extraction and dimension reduction for networks is critical in a wide variety of domains. Efficiently and accurately learning features for multiple graphs has important applications in statistical inference on graphs. We propose a method to jointly embed multiple undirected graphs. Given a set of graphs, the joint embedding method identifies a linear subspace spanned by rank one symmetric matrices and projects adjacency matrices of graphs into this subspace. The projection coefficients can be treated as features of the graphs, while the embedding components can represent vertex features. We also propose a random graph model for multiple graphs that generalizes other classical models for graphs. We show through theory and numerical experiments that under the model, the joint embedding method produces estimates of parameters with small errors. Via simulation experiments, we demonstrate that the joint embedding method produces features which lead to state of the art performance in classifying graphs. Applying the joint embedding method to human brain graphs, we find it extracts interpretable features with good prediction accuracy in different tasks.},
  archive      = {J_TPAMI},
  author       = {Shangsi Wang and Jesús Arroyo and Joshua T. Vogelstein and Carey E. Priebe},
  doi          = {10.1109/TPAMI.2019.2948619},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {1324-1336},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Joint embedding of graphs},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Interpreting the rhetoric of visual advertisements.
<em>TPAMI</em>, <em>43</em>(4), 1308–1323. (<a
href="https://doi.org/10.1109/TPAMI.2019.2947440">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual media have important persuasive power, but prior computer vision approaches have predominantly ignored the persuasive aspects of images. In this work, we propose a suite of data and techniques that enable progress on understanding the messages that visual advertisements convey. We make available a dataset of 64,832 image ads and 3,477 video ads, annotated with ten types of information: the topic and sentiment of the ad; whether it is funny, exciting, or effective; what action it prompts the viewer to do, and what arguments it provides for why this action should be taken; symbolic associations that the ad relies on; the metaphorical object transformations on which especially creative ads rely; and the climax in video ads. We develop methods that use multimodal cues, i.e., both visuals and slogans, for both the image and video domains. Our methods rely on finding poignant content spatially and temporally. We also examine the creative story construction in ads: for videos, we learn to predict when the climax occurs (if any), and how effective the story is; for images, we analyze how object transformations in ads metaphorically depict product properties.},
  archive      = {J_TPAMI},
  author       = {Keren Ye and Narges Honarvar Nazari and James Hahn and Zaeem Hussain and Mingda Zhang and Adriana Kovashka},
  doi          = {10.1109/TPAMI.2019.2947440},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {1308-1323},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Interpreting the rhetoric of visual advertisements},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). InLoc: Indoor visual localization with dense matching and
view synthesis. <em>TPAMI</em>, <em>43</em>(4), 1293–1307. (<a
href="https://doi.org/10.1109/TPAMI.2019.2952114">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We seek to predict the 6 degree-of-freedom (6DoF) pose of a query photograph with respect to a large indoor 3D map. The contributions of this work are three-fold. First, we develop a new large-scale visual localization method targeted for indoor spaces. The method proceeds along three steps: (i) efficient retrieval of candidate poses that scales to large-scale environments, (ii) pose estimation using dense matching rather than sparse local features to deal with weakly textured indoor scenes, and (iii) pose verification by virtual view synthesis that is robust to significant changes in viewpoint, scene layout, and occlusion. Second, we release a new dataset with reference 6DoF poses for large-scale indoor localization. Query photographs are captured by mobile phones at a different time than the reference 3D map, thus presenting a realistic indoor localization scenario. Third, we demonstrate that our method significantly outperforms current state-of-the-art indoor localization approaches on this new challenging data. Code and data are publicly available.},
  archive      = {J_TPAMI},
  author       = {Hajime Taira and Masatoshi Okutomi and Torsten Sattler and Mircea Cimpoi and Marc Pollefeys and Josef Sivic and Tomas Pajdla and Akihiko Torii},
  doi          = {10.1109/TPAMI.2019.2952114},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {1293-1307},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {InLoc: Indoor visual localization with dense matching and view synthesis},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Ideals of the multiview variety. <em>TPAMI</em>,
<em>43</em>(4), 1279–1292. (<a
href="https://doi.org/10.1109/TPAMI.2019.2950631">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The multiview variety of an arrangement of cameras is the Zariski closure of the images of world points in the cameras. The prime vanishing ideal of this complex projective variety is called the multiview ideal. We show that the bifocal and trifocal polynomials from the cameras generate the multiview ideal when the foci are distinct. In the computer vision literature, many sets of (determinantal) polynomials have been proposed to describe the multiview variety. We establish precise algebraic relationships between the multiview ideal and these various ideals. When the camera foci are noncoplanar, we prove that the ideal of bifocal polynomials saturate to give the multiview ideal. Finally, we prove that all the ideals we consider coincide when dehomogenized, to cut out the space of finite images.},
  archive      = {J_TPAMI},
  author       = {Sameer Agarwal and Andrew Pryhuber and Rekha R. Thomas},
  doi          = {10.1109/TPAMI.2019.2950631},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {1279-1292},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Ideals of the multiview variety},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Forecasting people trajectories and head poses by jointly
reasoning on tracklets and vislets. <em>TPAMI</em>, <em>43</em>(4),
1267–1278. (<a
href="https://doi.org/10.1109/TPAMI.2019.2949414">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we explore the correlation between people trajectories and their head orientations. We argue that people trajectory and head pose forecasting can be modelled as a joint problem. Recent approaches on trajectory forecasting leverage short-term trajectories (aka tracklets) of pedestrians to predict their future paths. In addition, sociological cues, such as expected destination or pedestrian interaction, are often combined with tracklets. In this article, we propose MiXing-LSTM (MX-LSTM) to capture the interplay between positions and head orientations (vislets) thanks to a joint unconstrained optimization of full covariance matrices during the LSTM backpropagation. We additionally exploit the head orientations as a proxy for the visual attention, when modeling social interactions. MX-LSTM predicts future pedestrians location and head pose, increasing the standard capabilities of the current approaches on long-term trajectory forecasting. Compared to the state-of-the-art, our approach shows better performances on an extensive set of public benchmarks. MX-LSTM is particularly effective when people move slowly, i.e., the most challenging scenario for all other models. The proposed approach also allows for accurate predictions on a longer time horizon.},
  archive      = {J_TPAMI},
  author       = {Irtiza Hasan and Francesco Setti and Theodore Tsesmelis and Vasileios Belagiannis and Sikandar Amin and Alessio Del Bue and Marco Cristani and Fabio Galasso},
  doi          = {10.1109/TPAMI.2019.2949414},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {1267-1278},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Forecasting people trajectories and head poses by jointly reasoning on tracklets and vislets},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Exploring explicit domain supervision for latent space
disentanglement in unpaired image-to-image translation. <em>TPAMI</em>,
<em>43</em>(4), 1254–1266. (<a
href="https://doi.org/10.1109/TPAMI.2019.2950198">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image-to-image translation tasks have been widely investigated with Generative Adversarial Networks (GANs). However, existing approaches are mostly designed in an unsupervised manner, while little attention has been paid to domain information within unpaired data. In this article, we treat domain information as explicit supervision and design an unpaired image-to-image translation framework, Domain-supervised GAN (DosGAN), which takes the first step towards the exploration of explicit domain supervision. In contrast to representing domain characteristics using different generators or domain codes, we pre-train a classification network to explicitly classify the domain of an image. After pre-training, this network is used to extract the domain-specific features of each image. Such features, together with the domain-independent features extracted by another encoder (shared across different domains), are used to generate image in target domain. Extensive experiments on multiple facial attribute translation, multiple identity translation, multiple season translation and conditional edges-to-shoes/handbags demonstrate the effectiveness of our method. In addition, we can transfer the domain-specific feature extractor obtained on the Facescrub dataset with domain supervision information to unseen domains, such as faces in the CelebA dataset. We also succeed in achieving conditional translation with any two images in CelebA, while previous models like StarGAN cannot handle this task.},
  archive      = {J_TPAMI},
  author       = {Jianxin Lin and Zhibo Chen and Yingce Xia and Sen Liu and Tao Qin and Jiebo Luo},
  doi          = {10.1109/TPAMI.2019.2950198},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {1254-1266},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Exploring explicit domain supervision for latent space disentanglement in unpaired image-to-image translation},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Effects of image degradation and degradation removal to
CNN-based image classification. <em>TPAMI</em>, <em>43</em>(4),
1239–1253. (<a
href="https://doi.org/10.1109/TPAMI.2019.2950923">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Just like many other topics in computer vision, image classification has achieved significant progress recently by using deep learning neural networks, especially the Convolutional Neural Networks (CNNs). Most of the existing works focused on classifying very clear natural images, evidenced by the widely used image databases, such as Caltech-256, PASCAL VOCs, and ImageNet. However, in many real applications, the acquired images may contain certain degradations that lead to various kinds of blurring, noise, and distortions. One important and interesting problem is the effect of such degradations to the performance of CNN-based image classification and whether degradation removal helps CNN-based image classification. More specifically, we wonder whether image classification performance drops with each kind of degradation, whether this drop can be avoided by including degraded images into training, and whether existing computer vision algorithms that attempt to remove such degradations can help improve the image classification performance. In this article, we empirically study those problems for nine kinds of degraded images—hazy images, motion-blurred images, fish-eye images, underwater images, low resolution images, salt-and-peppered images, images with white Gaussian noise, Gaussian-blurred images, and out-of-focus images. We expect this article can draw more interests from the community to study the classification of degraded images.},
  archive      = {J_TPAMI},
  author       = {Yanting Pei and Yaping Huang and Qi Zou and Xingyuan Zhang and Song Wang},
  doi          = {10.1109/TPAMI.2019.2950923},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {1239-1253},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Effects of image degradation and degradation removal to CNN-based image classification},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep depth from uncalibrated small motion clip.
<em>TPAMI</em>, <em>43</em>(4), 1225–1238. (<a
href="https://doi.org/10.1109/TPAMI.2019.2946806">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel approach to infer a high-quality depth map from a set of images with small viewpoint variations. In general, techniques for depth estimation from small motion consist of camera pose estimation and dense reconstruction. In contrast to prior approaches that recover scene geometry and camera motions using pre-calibrated cameras, we introduce in this paper a self-calibrating bundle adjustment method tailored for small motion which enables computation of camera poses without the need for camera calibration. For dense depth reconstruction, we present a convolutional neural network called DPSNet (Deep Plane Sweep Network) whose design is inspired by best practices of traditional geometry-based approaches. Rather than directly estimating depth or optical flow correspondence from image pairs as done in many previous deep learning methods, DPSNet takes a plane sweep approach that involves building a cost volume from deep features using the plane sweep algorithm, regularizing the cost volume, and regressing the depth map from the cost volume. The cost volume is constructed using a differentiable warping process that allows for end-to-end training of the network. Through the effective incorporation of conventional multiview stereo concepts within a deep learning framework, the proposed method achieves state-of-the-art results on a variety of challenging datasets.},
  archive      = {J_TPAMI},
  author       = {Sunghoon Im and Hyowon Ha and Hae-Gon Jeon and Stephen Lin and In So Kweon},
  doi          = {10.1109/TPAMI.2019.2946806},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {1225-1238},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Deep depth from uncalibrated small motion clip},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Corner detection using second-order generalized gaussian
directional derivative representations. <em>TPAMI</em>, <em>43</em>(4),
1213–1224. (<a
href="https://doi.org/10.1109/TPAMI.2019.2949302">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Corner detection is a critical component of many image analysis and image understanding tasks, such as object recognition and image matching. Our research indicates that existing corner detection algorithms cannot properly depict the difference between edges and corners and this results in wrong corner detections. In this paper, the capability of second-order generalized (isotropic and anisotropic) Gaussian directional derivative filters to suppress Gaussian noise is evaluated. The second-order generalized Gaussian directional derivative representations of step edge, L-type corner, Y- or T-type corner, X-type corner, and star-type corner are investigated and obtained. A number of properties for edges and corners are discovered which enable us to propose a new image corner detection method. Finally, the criteria on detection accuracy and average repeatability under affine image transformation, JPEG compression, and noise degradation, and the criteria on region repeatability are used to evaluate the proposed detector against nine state-of-the-art methods. The experimental results show that our proposed detector outperforms all the other tested detectors.},
  archive      = {J_TPAMI},
  author       = {Weichuan Zhang and Changming Sun},
  doi          = {10.1109/TPAMI.2019.2949302},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {1213-1224},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Corner detection using second-order generalized gaussian directional derivative representations},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bilinear image translation for temporal analysis of photo
collections. <em>TPAMI</em>, <em>43</em>(4), 1197–1212. (<a
href="https://doi.org/10.1109/TPAMI.2019.2950317">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose an approach for analyzing unpaired visual data annotated with time stamps by generating how images would have looked like if they were from different times. To isolate and transfer time dependent appearance variations, we introduce a new trainable bilinear factor separation module. We analyze its relation to classical factored representations [1] and concatenation-based auto-encoders [2] . We demonstrate this new module has clear advantages compared to standard concatenation when used in a bottleneck encoder-decoder convolutional neural network architecture. We also show that it can be inserted in a recent adversarial image translation architecture [3] , enabling the image transformation to multiple different target time periods using a single network. We apply our model to a challenging collection of more than 13,000 cars manufactured between 1920 and 2000 [4] and a dataset of high school yearbook portraits from 1930 to 2009 [5] . This allows us, for a given new input image, to generate a “history-lapse video” revealing changes over time by simply varying the target year. We show that by analyzing the generated history-lapse videos we can identify object deformations across time, extracting interesting changes in visual style over decades.},
  archive      = {J_TPAMI},
  author       = {Théophile Dalens and Mathieu Aubry and Josef Sivic},
  doi          = {10.1109/TPAMI.2019.2950317},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {1197-1212},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Bilinear image translation for temporal analysis of photo collections},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bayesian joint matrix decomposition for data integration
with heterogeneous noise. <em>TPAMI</em>, <em>43</em>(4), 1184–1196. (<a
href="https://doi.org/10.1109/TPAMI.2019.2946370">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Matrix decomposition is a popular and fundamental approach in machine learning and data mining. It has been successfully applied into various fields. Most matrix decomposition methods focus on decomposing a data matrix from one single source. However, it is common that data are from different sources with heterogeneous noise. A few of the matrix decomposition methods have been extended for such multi-view data integration and pattern discovery while only a few methods were designed to consider the heterogeneity of noise in such multi-view data for data integration explicitly. To this end, in this article, we propose a joint matrix decomposition framework (BJMD), which models the heterogeneity of noise by the Gaussian distribution in a Bayesian framework. We develop two algorithms to solve this model: one is a variational Bayesian inference algorithm, which makes full use of the posterior distribution; and another is a maximum a posterior algorithm, which is more scalable and can be easily paralleled. Extensive experiments on synthetic and real-world datasets demonstrate that BJMD is superior or competitive to the state-of-the-art methods.},
  archive      = {J_TPAMI},
  author       = {Chihao Zhang and Shihua Zhang},
  doi          = {10.1109/TPAMI.2019.2946370},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {1184-1196},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Bayesian joint matrix decomposition for data integration with heterogeneous noise},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Assessing transferability from simulation to reality for
reinforcement learning. <em>TPAMI</em>, <em>43</em>(4), 1172–1183. (<a
href="https://doi.org/10.1109/TPAMI.2019.2952353">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning robot control policies from physics simulations is of great interest to the robotics community as it may render the learning process faster, cheaper, and safer by alleviating the need for expensive real-world experiments. However, the direct transfer of learned behavior from simulation to reality is a major challenge. Optimizing a policy on a slightly faulty simulator can easily lead to the maximization of the `Simulation Optimization Bias&#39; (SOB). In this case, the optimizer exploits modeling errors of the simulator such that the resulting behavior can potentially damage the robot. We tackle this challenge by applying domain randomization, i.e., randomizing the parameters of the physics simulations during learning. We propose an algorithm called Simulation-based Policy Optimization with Transferability Assessment (SPOTA) which uses an estimator of the SOB to formulate a stopping criterion for training. The introduced estimator quantifies the over-fitting to the set of domains experienced while training. Our experimental results on two different second order nonlinear systems show that the new simulation-based policy search algorithm is able to learn a control policy exclusively from a randomized simulator, which can be applied directly to real systems without any additional training.},
  archive      = {J_TPAMI},
  author       = {Fabio Muratore and Michael Gienger and Jan Peters},
  doi          = {10.1109/TPAMI.2019.2952353},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {1172-1183},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Assessing transferability from simulation to reality for reinforcement learning},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Appearance and pose-conditioned human image generation using
deformable GANs. <em>TPAMI</em>, <em>43</em>(4), 1156–1171. (<a
href="https://doi.org/10.1109/TPAMI.2019.2947427">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we address the problem of generating person images conditioned on both pose and appearance information. Specifically, given an image $x_a$ of a person and a target pose $P(x_b)$ , extracted from an image $x_b$ , we synthesize a new image of that person in pose $P(x_b)$ , while preserving the visual details in $x_a$ . In order to deal with pixel-to-pixel misalignments caused by the pose differences between $P(x_a)$ and $P(x_b)$ , we introduce deformable skip connections in the generator of our Generative Adversarial Network. Moreover, a nearest-neighbour loss is proposed instead of the common $L_1$ and $L_2$ losses in order to match the details of the generated image with the target image. Quantitative and qualitative results, using common datasets and protocols recently proposed for this task, show that our approach is competitive with respect to the state of the art. Moreover, we conduct an extensive evaluation using off-the-shell person re-identification (Re-ID) systems trained with person-generation based augmented data, which is one of the main important applications for this task. Our experiments show that our Deformable GANs can significantly boost the Re-ID accuracy and are even better than data-augmentation methods specifically trained using Re-ID losses.},
  archive      = {J_TPAMI},
  author       = {Aliaksandr Siarohin and Stéphane Lathuilière and Enver Sangineto and Nicu Sebe},
  doi          = {10.1109/TPAMI.2019.2947427},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {1156-1171},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Appearance and pose-conditioned human image generation using deformable GANs},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Affine invariants of vector fields. <em>TPAMI</em>,
<em>43</em>(4), 1140–1155. (<a
href="https://doi.org/10.1109/TPAMI.2019.2951664">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vector fields are a special kind of multidimensional data, which are in a certain sense similar to digital color images, but are distinct from them in several aspects. In each pixel, the field is assigned to a vector that shows the direction and the magnitude of the quantity, which has been measured. To detect the patterns of interest in the field, special matching methods must be developed. In this paper, we propose a method for the description and matching of vector field patterns under an unknown affine transformation of the field. Unlike digital images, transformations of vector fields act not only on the spatial coordinates but also on the field values, which makes the detection different from the image case. To measure the similarity between the template and the field patch, we propose original invariants with respect to total affine transformation. They are designed from the vector field moments. It is demonstrated by experiments on real data from fluid mechanics that they perform significantly better than potential competitors.},
  archive      = {J_TPAMI},
  author       = {Jitka Kostková and Tomáš Suk and Jan Flusser},
  doi          = {10.1109/TPAMI.2019.2951664},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {1140-1155},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Affine invariants of vector fields},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adversarial margin maximization networks. <em>TPAMI</em>,
<em>43</em>(4), 1129–1139. (<a
href="https://doi.org/10.1109/TPAMI.2019.2948348">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The tremendous recent success of deep neural networks (DNNs) has sparked a surge of interest in understanding their predictive ability. Unlike the human visual system which is able to generalize robustly and learn with little supervision, DNNs normally require a massive amount of data to learn new concepts. In addition, research works also show that DNNs are vulnerable to adversarial examples-maliciously generated images which seem perceptually similar to the natural ones but are actually formed to fool learning models, which means the models have problem generalizing to unseen data with certain type of distortions. In this paper, we analyze the generalization ability of DNNs comprehensively and attempt to improve it from a geometric point of view. We propose adversarial margin maximization (AMM), a learning-based regularization which exploits an adversarial perturbation as a proxy. It encourages a large margin in the input space, just like the support vector machines. With a differentiable formulation of the perturbation, we train the regularized DNNs simply through back-propagation in an end-to-end manner. Experimental results on various datasets (including MNIST, CIFAR-10/100, SVHN and ImageNet) and different DNN architectures demonstrate the superiority of our method over previous state-of-the-arts. Code and models for reproducing our results will be made publicly available.},
  archive      = {J_TPAMI},
  author       = {Ziang Yan and Yiwen Guo and Changshui Zhang},
  doi          = {10.1109/TPAMI.2019.2948348},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {1129-1139},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Adversarial margin maximization networks},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). State of the journal editorial. <em>TPAMI</em>,
<em>43</em>(4), 1119–1128. (<a
href="https://doi.org/10.1109/TPAMI.2020.3047719">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presents the state of the journal review for this issue of the publication.},
  archive      = {J_TPAMI},
  author       = {Sven Dickinson},
  doi          = {10.1109/TPAMI.2020.3047719},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {4},
  pages        = {1119-1128},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {State of the journal editorial},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021f). [Front inside cover]. <em>TPAMI</em>, <em>43</em>(3), C3.
(<a href="https://doi.org/10.1109/TPAMI.2021.3052329">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presents a listing of the editorial board, board of governors, current staff, committee members, and/or society editors for this issue of the publication.},
  archive      = {J_TPAMI},
  doi          = {10.1109/TPAMI.2021.3052329},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {C3},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {[Front inside cover]},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hierarchical long short-term concurrent memory for human
interaction recognition. <em>TPAMI</em>, <em>43</em>(3), 1110–1118. (<a
href="https://doi.org/10.1109/TPAMI.2019.2942030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we aim to address the problem of human interaction recognition in videos by exploring the long-term inter-related dynamics among multiple persons. Recently, Long Short-Term Memory (LSTM) has become a popular choice to model individual dynamic for single-person action recognition due to its ability to capture the temporal motion information in a range. However, most existing LSTM-based methods focus only on capturing the dynamics of human interaction by simply combining all dynamics of individuals or modeling them as a whole. Such methods neglect the inter-related dynamics of how human interactions change over time. To this end, we propose a novel Hierarchical Long Short-Term Concurrent Memory (H-LSTCM) to model the long-term inter-related dynamics among a group of persons for recognizing human interactions. Specifically, we first feed each person&#39;s static features into a Single-Person LSTM to model the single-person dynamic. Subsequently, at one time step, the outputs of all Single-Person LSTM units are fed into a novel Concurrent LSTM (Co-LSTM) unit, which mainly consists of multiple sub-memory units, a new cell gate, and a new co-memory cell. In the Co-LSTM unit, each sub-memory unit stores individual motion information, while this Co-LSTM unit selectively integrates and stores inter-related motion information between multiple interacting persons from multiple sub-memory units via the cell gate and co-memory cell, respectively. Extensive experiments on several public datasets validate the effectiveness of the proposed H-LSTCM by comparing against baseline and state-of-the-art methods.},
  archive      = {J_TPAMI},
  author       = {Xiangbo Shu and Jinhui Tang and Guo-Jun Qi and Wei Liu and Jian Yang},
  doi          = {10.1109/TPAMI.2019.2942030},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {1110-1118},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Hierarchical long short-term concurrent memory for human interaction recognition},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adversarial attack type i: Cheat classifiers by significant
changes. <em>TPAMI</em>, <em>43</em>(3), 1100–1109. (<a
href="https://doi.org/10.1109/TPAMI.2019.2936378">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the great success of deep neural networks, the adversarial attack can cheat some well-trained classifiers by small permutations. In this paper, we propose another type of adversarial attack that can cheat classifiers by significant changes. For example, we can significantly change a face but well-trained neural networks still recognize the adversarial and the original example as the same person. Statistically, the existing adversarial attack increases Type II error and the proposed one aims at Type I error, which are hence named as Type II and Type I adversarial attack, respectively. The two types of attack are equally important but are essentially different, which are intuitively explained and numerically evaluated. To implement the proposed attack, a supervised variation autoencoder is designed and then the classifier is attacked by updating the latent variables using gradient information. Besides, with pre-trained generative models, Type I attack on latent spaces is investigated as well. Experimental results show that our method is practical and effective to generate Type I adversarial examples on large-scale image datasets. Most of these generated examples can pass detectors designed for defending Type II attack and the strengthening strategy is only efficient with a specific type attack, both implying that the underlying reasons for Type I and Type II attack are different.},
  archive      = {J_TPAMI},
  author       = {Sanli Tang and Xiaolin Huang and Mingjian Chen and Chengjin Sun and Jie Yang},
  doi          = {10.1109/TPAMI.2019.2936378},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {1100-1109},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Adversarial attack type i: Cheat classifiers by significant changes},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A differential approach for gaze estimation. <em>TPAMI</em>,
<em>43</em>(3), 1092–1099. (<a
href="https://doi.org/10.1109/TPAMI.2019.2957373">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most non-invasive gaze estimation methods regress gaze directions directly from a single face or eye image. However, due to important variabilities in eye shapes and inner eye structures amongst individuals, universal models obtain limited accuracies and their output usually exhibit high variance as well as subject dependent biases. Thus, increasing accuracy is usually done through calibration, allowing gaze predictions for a subject to be mapped to her actual gaze. In this article, we introduce a novel approach, which works by directly training a differential convolutional neural network to predict gaze differences between two eye input images of the same subject. Then, given a set of subject specific calibration images, we can use the inferred differences to predict the gaze direction of a novel eye sample. The assumption is that by comparing eye images of the same user, annoyance factors (alignment, eyelid closing, illumination perturbations) which usually plague single image prediction methods can be much reduced, allowing better prediction altogether. Furthermore, the differential network itself can be adapted via finetuning to make predictions consistent with the available user reference pairs. Experiments on 3 public datasets validate our approach which constantly outperforms state-of-the-art methods even when using only one calibration sample or those relying on subject specific gaze adaptation.},
  archive      = {J_TPAMI},
  author       = {Gang Liu and Yu Yu and Kenneth A. Funes Mora and Jean-Marc Odobez},
  doi          = {10.1109/TPAMI.2019.2957373},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {1092-1099},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {A differential approach for gaze estimation},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). 3D fingerprint recognition based on ridge-valley-guided 3D
reconstruction and 3D topology polymer feature extraction.
<em>TPAMI</em>, <em>43</em>(3), 1085–1091. (<a
href="https://doi.org/10.1109/TPAMI.2019.2949299">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An automated fingerprint recognition system (AFRS) for 3D fingerprints is essential and highly promising for biometric security. Despite the progress in developing 3D AFRSs, achieving high-quality real-time reconstruction and high-accuracy recognition of 3D fingerprints remain two challenging issues. To address them, we propose a robust 3D AFRS based on ridge-valley (RV)-guided 3D fingerprint reconstruction and 3D topology polymer (TTP) feature extraction. The former considers the unique fingerprint characteristics of the RV and achieves real-time reconstruction. Unlike traditional triangulation-based methods that establish correspondences between points by cross-correlation-based searching, we propose to establish RV correspondences (RVCs) between ridges/valleys by defining and calculating a RVC matrix based on the topology of RV curves. To enhance depth reconstruction, curve-based smoothing is proposed to refine our novel RV disparity map. The TTP feature codes the 3D topology by projecting the 3D minutiae onto multiple planes and extracting their corresponding 2D topologies and has proven to be effective and efficient for 3D fingerprint recognition. Comprehensive experimental results demonstrate that our method outperforms the state-of-the-art methods in terms of both reconstruction and recognition accuracy. Also, due to its very short running time, it is appropriate for practical applications.},
  archive      = {J_TPAMI},
  author       = {Xuefei Yin and Yanming Zhu and Jiankun Hu},
  doi          = {10.1109/TPAMI.2019.2949299},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {1085-1091},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {3D fingerprint recognition based on ridge-valley-guided 3D reconstruction and 3D topology polymer feature extraction},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Video anomaly detection with sparse coding inspired deep
neural networks. <em>TPAMI</em>, <em>43</em>(3), 1070–1084. (<a
href="https://doi.org/10.1109/TPAMI.2019.2944377">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents an anomaly detection method that is based on a sparse coding inspired Deep Neural Networks (DNN). Specifically, in light of the success of sparse coding based anomaly detection, we propose a Temporally-coherent Sparse Coding (TSC), where a temporally-coherent term is used to preserve the similarity between two similar frames. The optimization of sparse coefficients in TSC with the Sequential Iterative Soft-Thresholding Algorithm (SIATA) is equivalent to a special stacked Recurrent Neural Networks (sRNN) architecture. Further, to reduce the computational cost in alternatively updating the dictionary and sparse coefficients in TSC optimization and to alleviate hyperparameters selection in TSC, we stack one more layer on top of the TSC-inspired sRNN to reconstruct the inputs, and arrive at an sRNN-AE. We further improve sRNN-AE in the following aspects: i) rather than using a predefined similarity measurement between two frames, we propose to learn a data-dependent similarity measurement between neighboring frames in sRNN-AE to make it more suitable for anomaly detection; ii) to reduce computational costs in the inference stage, we reduce the depth of the sRNN in sRNN-AE and, consequently, our framework achieves real-time anomaly detection; iii) to improve computational efficiency, we conduct temporal pooling over the appearance features of several consecutive frames for summarizing information temporally, then we feed appearance features and temporally summarized features into a separate sRNN-AE for more robust anomaly detection. To facilitate anomaly detection evaluation, we also build a large-scale anomaly detection dataset which is even larger than the summation of all existing datasets for anomaly detection in terms of both the volume of data and the diversity of scenes. Extensive experiments on both a toy dataset under controlled settings and real datasets demonstrate that our method significantly outperforms existing methods, which validates the effectiveness of our sRNN-AE method for anomaly detection. Codes and data have been released at https://github.com/StevenLiuWen/sRNN_TSC_Anomaly_Detection .},
  archive      = {J_TPAMI},
  author       = {Weixin Luo and Wen Liu and Dongze Lian and Jinhui Tang and Lixin Duan and Xi Peng and Shenghua Gao},
  doi          = {10.1109/TPAMI.2019.2944377},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {1070-1084},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Video anomaly detection with sparse coding inspired deep neural networks},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Topology-aware non-rigid point cloud registration.
<em>TPAMI</em>, <em>43</em>(3), 1056–1069. (<a
href="https://doi.org/10.1109/TPAMI.2019.2940655">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we introduce a non-rigid registration pipeline for pairs of unorganized point clouds that may be topologically different. Standard warp field estimation algorithms, even under robust, discontinuity-preserving regularization, tend to produce erratic motion estimates on boundaries associated with `close-to-open&#39; topology changes. We overcome this limitation by exploiting backward motion: in the opposite motion direction, a `close-to-open&#39; event becomes `open-to-close&#39;, which is by default handled correctly. At the core of our approach lies a general, topology-agnostic warp field estimation algorithm, similar to those employed in recently introduced dynamic reconstruction systems from RGB-D input. We improve motion estimation on boundaries associated with topology changes in an efficient post-processing phase. Based on both forward and (inverted) backward warp hypotheses, we explicitly detect regions of the deformed geometry that undergo topological changes by means of local deformation criteria and broadly classify them as `contacts&#39; or `separations&#39;. Subsequently, the two motion hypotheses are seamlessly blended on a local basis, according to the type and proximity of detected events. Our method achieves state-of-the-art motion estimation accuracy on the MPI Sintel dataset. Experiments on a custom dataset with topological event annotations demonstrate the effectiveness of our pipeline in estimating motion on event boundaries, as well as promising performance in explicit topological event detection.},
  archive      = {J_TPAMI},
  author       = {Konstantinos Zampogiannis and Cornelia Fermüller and Yiannis Aloimonos},
  doi          = {10.1109/TPAMI.2019.2940655},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {1056-1069},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Topology-aware non-rigid point cloud registration},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Surface-aware blind image deblurring. <em>TPAMI</em>,
<em>43</em>(3), 1041–1055. (<a
href="https://doi.org/10.1109/TPAMI.2019.2941472">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blind image deblurring is a conundrum because there are infinitely many pairs of latent image and blur kernel. To get a stable and reasonable deblurred image, proper prior knowledge of the latent image and the blur kernel is urgently required. Different from the recent works on the statistical observations of the difference between the blurred image and the clean one, our method is built on the surface-aware strategy arising from the intrinsic geometrical consideration. This approach facilitates the blur kernel estimation due to the preserved sharp edges in the intermediate latent image. Extensive experiments demonstrate that our method outperforms the state-of-the-art methods on deblurring the text and natural images. Moreover, our method can achieve attractive results in some challenging cases, such as low-illumination images with large saturated regions and impulse noise. A direct extension of our method to the non-uniform deblurring problem also validates the effectiveness of the surface-aware prior.},
  archive      = {J_TPAMI},
  author       = {Jun Liu and Ming Yan and Tieyong Zeng},
  doi          = {10.1109/TPAMI.2019.2941472},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {1041-1055},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Surface-aware blind image deblurring},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SEWA DB: A rich database for audio-visual emotion and
sentiment research in the wild. <em>TPAMI</em>, <em>43</em>(3),
1022–1040. (<a
href="https://doi.org/10.1109/TPAMI.2019.2944808">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Natural human-computer interaction and audio-visual human behaviour sensing systems, which would achieve robust performance in-the-wild are more needed than ever as digital devices are increasingly becoming an indispensable part of our life. Accurately annotated real-world data are the crux in devising such systems. However, existing databases usually consider controlled settings, low demographic variability, and a single task. In this paper, we introduce the SEWA database of more than 2,000 minutes of audio-visual data of 398 people coming from six cultures, 50 percent female, and uniformly spanning the age range of 18 to 65 years old. Subjects were recorded in two different contexts: while watching adverts and while discussing adverts in a video chat. The database includes rich annotations of the recordings in terms of facial landmarks, facial action units (FAU), various vocalisations, mirroring, and continuously valued valence, arousal, liking, agreement, and prototypic examples of (dis)liking. This database aims to be an extremely valuable resource for researchers in affective computing and automatic human sensing and is expected to push forward the research in human behaviour analysis, including cultural studies. Along with the database, we provide extensive baseline experiments for automatic FAU detection and automatic valence, arousal, and (dis)liking intensity estimation.},
  archive      = {J_TPAMI},
  author       = {Jean Kossaifi and Robert Walecki and Yannis Panagakis and Jie Shen and Maximilian Schmitt and Fabien Ringeval and Jing Han and Vedhas Pandit and Antoine Toisoul and Björn Schuller and Kam Star and Elnar Hajiyev and Maja Pantic},
  doi          = {10.1109/TPAMI.2019.2944808},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {1022-1040},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {SEWA DB: A rich database for audio-visual emotion and sentiment research in the wild},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sequence-to-segments networks for detecting segments in
videos. <em>TPAMI</em>, <em>43</em>(3), 1009–1021. (<a
href="https://doi.org/10.1109/TPAMI.2019.2940225">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting segments of interest from videos is a common problem for many applications. And yet it is a challenging problem as it often requires not only knowledge of individual target segments, but also contextual understanding of the entire video and the relationships between the target segments. To address this problem, we propose the Sequence-to-Segments Network (S 2 N), a novel and general end-to-end sequential encoder-decoder architecture. S 2 N first encodes the input video into a sequence of hidden states that capture information progressively, as it appears in the video. It then employs the Segment Detection Unit (SDU), a novel decoding architecture, that sequentially detects segments. At each decoding step, the SDU integrates the decoder state and encoder hidden states to detect a target segment. During training, we address the problem of finding the best assignment of predicted segments to ground truth using the Hungarian Matching Algorithm with Lexicographic Cost. Additionally we propose to use the squared Earth Mover&#39;s Distance to optimize the localization errors of the segments. We show the state-of-the-art performance of S 2 N across numerous tasks, including video highlighting, video summarization, and human action proposal generation.},
  archive      = {J_TPAMI},
  author       = {Zijun Wei and Boyu Wang and Minh Hoai and Jianming Zhang and Xiaohui Shen and Zhe Lin and Radomír Měch and Dimitris Samaras},
  doi          = {10.1109/TPAMI.2019.2940225},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {1009-1021},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Sequence-to-segments networks for detecting segments in videos},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On the global geometry of sphere-constrained sparse blind
deconvolution. <em>TPAMI</em>, <em>43</em>(3), 999–1008. (<a
href="https://doi.org/10.1109/TPAMI.2019.2939237">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blind deconvolution is the problem of recovering a convolutional kernel a 0 and an activation signal x 0 from their convolution y = a 0 ⊗ x 0 . This problem is ill-posed without further constraints or priors. This paper studies the situation where the nonzero entries in the activation signal are sparsely and randomly populated. We normalize the convolution kernel to have unit Frobenius norm and cast the sparse blind deconvolution problem as a nonconvex optimization problem over the sphere. With this spherical constraint, every spurious local minimum turns out to be close to some signed shift truncation of the ground truth, under certain hypotheses. This benign property motivates an effective two stage algorithm that recovers the ground truth from the partial information offered by a suboptimal local minimum. This geometry-inspired algorithm recovers the ground truth for certain microscopy problems, also exhibits promising performance in the more challenging image deblurring problem. Our insights into the global geometry and the two stage algorithm extend to the convolutional dictionary learning problem, where a superposition of multiple convolution signals is observed.},
  archive      = {J_TPAMI},
  author       = {Yuqian Zhang and Yenson Lau and Han-Wen Kuo and Sky Cheung and Abhay Pasupathy and John Wright},
  doi          = {10.1109/TPAMI.2019.2939237},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {999-1008},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {On the global geometry of sphere-constrained sparse blind deconvolution},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Nonlinear regression via deep negative correlation
learning. <em>TPAMI</em>, <em>43</em>(3), 982–998. (<a
href="https://doi.org/10.1109/TPAMI.2019.2943860">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nonlinear regression has been extensively employed in many computer vision problems (e.g., crowd counting, age estimation, affective computing). Under the umbrella of deep learning, two common solutions exist i) transforming nonlinear regression to a robust loss function which is jointly optimizable with the deep convolutional network, and ii) utilizing ensemble of deep networks. Although some improved performance is achieved, the former may be lacking due to the intrinsic limitation of choosing a single hypothesis and the latter may suffer from much larger computational complexity. To cope with those issues, we propose to regress via an efficient “divide and conquer” manner. The core of our approach is the generalization of negative correlation learning that has been shown, both theoretically and empirically, to work well for non-deep regression problems. Without extra parameters, the proposed method controls the bias-variance-covariance trade-off systematically and usually yields a deep regression ensemble where each base model is both “accurate” and “diversified.” Moreover, we show that each sub-problem in the proposed method has less Rademacher Complexity and thus is easier to optimize. Extensive experiments on several diverse and challenging tasks including crowd counting, personality analysis, age estimation, and image super-resolution demonstrate the superiority over challenging baselines as well as the versatility of the proposed method. The source code and trained models are available on our project page: https://mmcheng.net/dncl/.},
  archive      = {J_TPAMI},
  author       = {Le Zhang and Zenglin Shi and Ming-Ming Cheng and Yun Liu and Jia-Wang Bian and Joey Tianyi Zhou and Guoyan Zheng and Zeng Zeng},
  doi          = {10.1109/TPAMI.2019.2943860},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {982-998},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Nonlinear regression via deep negative correlation learning},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MTFH: A matrix tri-factorization hashing framework for
efficient cross-modal retrieval. <em>TPAMI</em>, <em>43</em>(3),
964–981. (<a href="https://doi.org/10.1109/TPAMI.2019.2940446">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hashing has recently sparked a great revolution in cross-modal retrieval because of its low storage cost and high query speed. Recent cross-modal hashing methods often learn unified or equal-length hash codes to represent the multi-modal data and make them intuitively comparable. However, such unified or equal-length hash representations could inherently sacrifice their representation scalability because the data from different modalities may not have one-to-one correspondence and could be encoded more efficiently by different hash codes of unequal lengths. To mitigate these problems, this paper exploits a related and relatively unexplored problem: encode the heterogeneous data with varying hash lengths and generalize the cross-modal retrieval in various challenging scenarios. To this end, a generalized and flexible cross-modal hashing framework, termed Matrix Tri-Factorization Hashing (MTFH), is proposed to work seamlessly in various settings including paired or unpaired multi-modal data, and equal or varying hash length encoding scenarios. More specifically, MTFH exploits an efficient objective function to flexibly learn the modality-specific hash codes with different length settings, while synchronously learning two semantic correlation matrices to semantically correlate the different hash representations for heterogeneous data comparable. As a result, the derived hash codes are more semantically meaningful for various challenging cross-modal retrieval tasks. Extensive experiments evaluated on public benchmark datasets highlight the superiority of MTFH under various retrieval scenarios and show its competitive performance with the state-of-the-arts.},
  archive      = {J_TPAMI},
  author       = {Xin Liu and Zhikai Hu and Haibin Ling and Yiu-Ming Cheung},
  doi          = {10.1109/TPAMI.2019.2940446},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {964-981},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {MTFH: A matrix tri-factorization hashing framework for efficient cross-modal retrieval},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MFQE 2.0: A new approach for multi-frame quality enhancement
on compressed video. <em>TPAMI</em>, <em>43</em>(3), 949–963. (<a
href="https://doi.org/10.1109/TPAMI.2019.2944806">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The past few years have witnessed great success in applying deep learning to enhance the quality of compressed image/video. The existing approaches mainly focus on enhancing the quality of a single frame, not considering the similarity between consecutive frames. Since heavy fluctuation exists across compressed video frames as investigated in this paper, frame similarity can be utilized for quality enhancement of low-quality frames given their neighboring high-quality frames. This task is Multi-Frame Quality Enhancement (MFQE). Accordingly, this paper proposes an MFQE approach for compressed video, as the first attempt in this direction. In our approach, we first develop a Bidirectional Long Short-Term Memory (BiLSTM) based detector to locate Peak Quality Frames (PQFs) in compressed video. Then, a novel Multi-Frame Convolutional Neural Network (MF-CNN) is designed to enhance the quality of compressed video, in which the non-PQF and its nearest two PQFs are the input. In MF-CNN, motion between the non-PQF and PQFs is compensated by a motion compensation subnet. Subsequently, a quality enhancement subnet fuses the non-PQF and compensated PQFs, and then reduces the compression artifacts of the non-PQF. Also, PQF quality is enhanced in the same way. Finally, experiments validate the effectiveness and generalization ability of our MFQE approach in advancing the state-of-the-art quality enhancement of compressed video.},
  archive      = {J_TPAMI},
  author       = {Zhenyu Guan and Qunliang Xing and Mai Xu and Ren Yang and Tie Liu and Zulin Wang},
  doi          = {10.1109/TPAMI.2019.2944806},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {949-963},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {MFQE 2.0: A new approach for multi-frame quality enhancement on compressed video},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MEMC-net: Motion estimation and motion compensation driven
neural network for video interpolation and enhancement. <em>TPAMI</em>,
<em>43</em>(3), 933–948. (<a
href="https://doi.org/10.1109/TPAMI.2019.2941941">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motion estimation (ME) and motion compensation (MC) have been widely used for classical video frame interpolation systems over the past decades. Recently, a number of data-driven frame interpolation methods based on convolutional neural networks have been proposed. However, existing learning based methods typically estimate either flow or compensation kernels, thereby limiting performance on both computational efficiency and interpolation accuracy. In this work, we propose a motion estimation and compensation driven neural network for video frame interpolation. A novel adaptive warping layer is developed to integrate both optical flow and interpolation kernels to synthesize target frame pixels. This layer is fully differentiable such that both the flow and kernel estimation networks can be optimized jointly. The proposed model benefits from the advantages of motion estimation and compensation methods without using hand-crafted features. Compared to existing methods, our approach is computationally efficient and able to generate more visually appealing results. Furthermore, the proposed MEMC-Net architecture can be seamlessly adapted to several video enhancement tasks, e.g., super-resolution, denoising, and deblocking. Extensive quantitative and qualitative evaluations demonstrate that the proposed method performs favorably against the state-of-the-art video frame interpolation and enhancement algorithms on a wide range of datasets.},
  archive      = {J_TPAMI},
  author       = {Wenbo Bao and Wei-Sheng Lai and Xiaoyun Zhang and Zhiyong Gao and Ming-Hsuan Yang},
  doi          = {10.1109/TPAMI.2019.2941941},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {933-948},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {MEMC-net: Motion estimation and motion compensation driven neural network for video interpolation and enhancement},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Loss decomposition and centroid estimation for positive and
unlabeled learning. <em>TPAMI</em>, <em>43</em>(3), 918–932. (<a
href="https://doi.org/10.1109/TPAMI.2019.2941684">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies Positive and Unlabeled learning (PU learning), of which the target is to build a binary classifier where only positive data and unlabeled data are available for classifier training. To deal with the absence of negative training data, we first regard all unlabeled data as negative examples with false negative labels, and then convert PU learning into the risk minimization problem in the presence of such one-side label noise. Specifically, we propose a novel PU learning algorithm dubbed “Loss Decomposition and Centroid Estimation” (LDCE). By decomposing the loss function of corrupted negative examples into two parts, we show that only the second part is affected by the noisy labels. Thereby, we may estimate the centroid of corrupted negative set via an unbiased way to reduce the adverse impact of such label noise. Furthermore, we propose the “Kernelized LDCE” (KLDCE) by introducing the kernel trick, and show that KLDCE can be easily solved by combining Alternative Convex Search (ACS) and Sequential Minimal Optimization (SMO). Theoretically, we derive the generalization error bound which suggests that the generalization risk of our model converges to the empirical risk with the order of O(1= k + 1= n k+ 1=\%/n) (n and k are the amounts of training data and positive data correspondingly). Experimentally, we conduct intensive experiments on synthetic dataset, UCI benchmark datasets and real-world datasets, and the results demonstrate that our approaches (LDCE and KLDCE) achieve the top-level performance when compared with both classic and state-of-the-art PU learning methods.},
  archive      = {J_TPAMI},
  author       = {Chen Gong and Hong Shi and Tongliang Liu and Chuang Zhang and Jian Yang and Dacheng Tao},
  doi          = {10.1109/TPAMI.2019.2941684},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {918-932},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Loss decomposition and centroid estimation for positive and unlabeled learning},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning part-based convolutional features for person
re-identification. <em>TPAMI</em>, <em>43</em>(3), 902–917. (<a
href="https://doi.org/10.1109/TPAMI.2019.2938523">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Part-level features offer fine granularity for pedestrian image description. In this article, we generally aim to learn discriminative part-informed feature for person re-identification. Our contribution is two-fold. First, we introduce a general part-level feature learning method, named Part-based Convolutional Baseline (PCB). Given an image input, it outputs a convolutional descriptor consisting of several part-level features. PCB is general in that it is able to accommodate several part partitioning strategies, including pose estimation, human parsing and uniform part partitioning. In experiment, we show that the learned descriptor has a significantly higher discriminative ability than the global descriptor. Second, based on PCB, we propose refined part pooling (RPP), which allows the parts to be more precisely located. Our idea is that pixels within a well-located part should be similar to each other while being dissimilar with pixels from other parts. We call it within-part consistency. When a pixel-wise feature vector in a part is more similar to some other part, it is then an outlier, indicating inappropriate partitioning. RPP re-assigns these outliers to the parts they are closest to, resulting in refined parts with enhanced within-part consistency. RPP requires no part labels and is trained in a weakly supervised manner. Experiment confirms that RPP allows PCB to gain another round of performance boost. For instance, on the Market-1501 dataset, we achieve (77.4+4.2) percent mAP and (92.3+1.5) percent rank-1 accuracy, a competitive performance with the state of the art.},
  archive      = {J_TPAMI},
  author       = {Yifan Sun and Liang Zheng and Yali Li and Yi Yang and Qi Tian and Shengjin Wang},
  doi          = {10.1109/TPAMI.2019.2938523},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {902-917},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Learning part-based convolutional features for person re-identification},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Interpretable visual question answering by reasoning on
dependency trees. <em>TPAMI</em>, <em>43</em>(3), 887–901. (<a
href="https://doi.org/10.1109/TPAMI.2019.2943456">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Collaborative reasoning for understanding image-question pairs is a very critical but underexplored topic in interpretable visual question answering systems. Although very recent studies have attempted to use explicit compositional processes to assemble multiple subtasks embedded in questions, their models heavily rely on annotations or handcrafted rules to obtain valid reasoning processes, which leads to either heavy workloads or poor performance on compositional reasoning. In this paper, to better align image and language domains in diverse and unrestricted cases, we propose a novel neural network model that performs global reasoning on a dependency tree parsed from the question; thus, our model is called a parse-tree-guided reasoning network (PTGRN). This network consists of three collaborative modules: i) an attention module that exploits the local visual evidence of each word parsed from the question, ii) a gated residual composition module that composes the previously mined evidence, and iii) a parse-tree-guided propagation module that passes the mined evidence along the parse tree. Thus, PTGRN is capable of building an interpretable visual question answering (VQA) system that gradually derives image cues following question-driven parse-tree reasoning. Experiments on relational datasets demonstrate the superiority of PTGRN over current state-of-the-art VQA methods, and the visualization results highlight the explainable capability of our reasoning system.},
  archive      = {J_TPAMI},
  author       = {Qingxing Cao and Xiaodan Liang and Bailin Li and Liang Lin},
  doi          = {10.1109/TPAMI.2019.2943456},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {887-901},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Interpretable visual question answering by reasoning on dependency trees},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). High-dimensional dense residual convolutional neural network
for light field reconstruction. <em>TPAMI</em>, <em>43</em>(3), 873–886.
(<a href="https://doi.org/10.1109/TPAMI.2019.2945027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of high-dimensional light field reconstruction and develop a learning-based framework for spatial and angular super-resolution. Many current approaches either require disparity clues or restore the spatial and angular details separately. Such methods have difficulties with non-Lambertian surfaces or occlusions. In contrast, we formulate light field super-resolution (LFSR) as tensor restoration and develop a learning framework based on a two-stage restoration with 4-dimensional (4D) convolution. This allows our model to learn the features capturing the geometry information encoded in multiple adjacent views. Such geometric features vary near the occlusion regions and indicate the foreground object border. To train a feasible network, we propose a novel normalization operation based on a group of views in the feature maps, design a stage-wise loss function, and develop the multi-range training strategy to further improve the performance. Evaluations are conducted on a number of light field datasets including real-world scenes, synthetic data, and microscope light fields. The proposed method achieves superior performance and less execution time comparing with other state-of-the-art schemes.},
  archive      = {J_TPAMI},
  author       = {Nan Meng and Hayden K.-H. So and Xing Sun and Edmund Y. Lam},
  doi          = {10.1109/TPAMI.2019.2945027},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {873-886},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {High-dimensional dense residual convolutional neural network for light field reconstruction},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Harmonized multimodal learning with gaussian process latent
variable models. <em>TPAMI</em>, <em>43</em>(3), 858–872. (<a
href="https://doi.org/10.1109/TPAMI.2019.2942028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal learning aims to discover the relationship between multiple modalities. It has become an important research topic due to extensive multimodal applications such as cross-modal retrieval. This paper attempts to address the modality heterogeneity problem based on Gaussian process latent variable models (GPLVMs) to represent multimodal data in a common space. Previous multimodal GPLVM extensions generally adopt individual learning schemes on latent representations and kernel hyperparameters, which ignore their intrinsic relationship. To exploit strong complementarity among different modalities and GPLVM components, we develop a novel learning scheme called Harmonization, where latent representations and kernel hyperparameters are jointly learned from each other. Beyond the correlation fitting or intra-modal structure preservation paradigms widely used in existing studies, the harmonization is derived in a model-driven manner to encourage the agreement between modality-specific GP kernels and the similarity of latent representations. We present a range of multimodal learning models by incorporating the harmonization mechanism into several representative GPLVM-based approaches. Experimental results on four benchmark datasets show that the proposed models outperform the strong baselines for cross-modal retrieval tasks, and that the harmonized multimodal learning method is superior in discovering semantically consistent latent representation.},
  archive      = {J_TPAMI},
  author       = {Guoli Song and Shuhui Wang and Qingming Huang and Qi Tian},
  doi          = {10.1109/TPAMI.2019.2942028},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {858-872},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Harmonized multimodal learning with gaussian process latent variable models},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deterministic approximate methods for maximum consensus
robust fitting. <em>TPAMI</em>, <em>43</em>(3), 842–857. (<a
href="https://doi.org/10.1109/TPAMI.2019.2939307">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Maximum consensus estimation plays a critically important role in several robust fitting problems in computer vision. Currently, the most prevalent algorithms for consensus maximization draw from the class of randomized hypothesize-and-verify algorithms, which are cheap but can usually deliver only rough approximate solutions. On the other extreme, there are exact algorithms which are exhaustive search in nature and can be costly for practical-sized inputs. This paper fills the gap between the two extremes by proposing deterministic algorithms to approximately optimize the maximum consensus criterion. Our work begins by reformulating consensus maximization with linear complementarity constraints. Then, we develop two novel algorithms: one based on non-smooth penalty method with a Frank-Wolfe style optimization scheme, the other based on the Alternating Direction Method of Multipliers (ADMM). Both algorithms solve convex subproblems to efficiently perform the optimization. We demonstrate the capability of our algorithms to greatly improve a rough initial estimate, such as those obtained using least squares or a randomized algorithm. Compared to the exact algorithms, our approach is much more practical on realistic input sizes. Further, our approach is naturally applicable to estimation problems with geometric residuals. Matlab code and demo program for our methods can be downloaded from https://goo.gl/FQcxpi.},
  archive      = {J_TPAMI},
  author       = {Huu Le and Tat-Jun Chin and Anders Eriksson and Thanh-Toan Do and David Suter},
  doi          = {10.1109/TPAMI.2019.2939307},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {842-857},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Deterministic approximate methods for maximum consensus robust fitting},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Complex-valued disparity: Unified depth model of depth from
stereo, depth from focus, and depth from defocus based on the light
field gradient. <em>TPAMI</em>, <em>43</em>(3), 830–841. (<a
href="https://doi.org/10.1109/TPAMI.2019.2946159">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a unified depth model based on the light field gradient, in which estimated disparity is represented by the complex number. The complex-valued disparity by the proposed depth model can be represented in both the Cartesian and polar coordinates. In the Cartesian representation, the proposed depth model is represented by real and imaginary parts of the disparity. The real part can be used for disparity estimation with respect to the in-focus plane, whereas the imaginary part represents the non-Lambertian-ness. In the polar representation, the proposed depth model is expressed by the disparity magnitude and disparity angle. The disparity magnitude shows the relationship among depth from stereo, depth from focus, and depth from defocus, whereas the disparity angle shows whether or not the bundles of rays are flipped with respect to the in-focus plane. For disparity analysis, we present the real response, imaginary response, magnitude response, and angle response, which are represented by the three-dimensional volume. Experimental results on synthetic and real light field images show that the real and magnitude responses of the proposed depth model are valid for local disparity estimation.},
  archive      = {J_TPAMI},
  author       = {Jae Young Lee and Rae-Hong Park},
  doi          = {10.1109/TPAMI.2019.2946159},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {830-841},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Complex-valued disparity: Unified depth model of depth from stereo, depth from focus, and depth from defocus based on the light field gradient},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Are large-scale 3D models really necessary for accurate
visual localization? <em>TPAMI</em>, <em>43</em>(3), 814–829. (<a
href="https://doi.org/10.1109/TPAMI.2019.2941876">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate visual localization is a key technology for autonomous navigation. 3D structure-based methods employ 3D models of the scene to estimate the full 6 degree-of-freedom (DOF) pose of a camera very accurately. However, constructing (and extending) large-scale 3D models is still a significant challenge. In contrast, 2D image retrieval-based methods only require a database of geo-tagged images, which is trivial to construct and to maintain. They are often considered inaccurate since they only approximate the positions of the cameras. Yet, the exact camera pose can theoretically be recovered when enough relevant database images are retrieved. In this paper, we demonstrate experimentally that large-scale 3D models are not strictly necessary for accurate visual localization. We create reference poses for a large and challenging urban dataset. Using these poses, we show that combining image-based methods with local reconstructions results in a higher pose accuracy compared to state-of-the-art structure-based methods, albeight at higher run-time costs. We show that some of these run-time costs can be alleviated by exploiting known database image poses. Our results suggest that we might want to reconsider the need for large-scale 3D models in favor of more local models, but also that further research is necessary to accelerate the local reconstruction process.},
  archive      = {J_TPAMI},
  author       = {Akihiko Torii and Hajime Taira and Josef Sivic and Marc Pollefeys and Masatoshi Okutomi and Tomas Pajdla and Torsten Sattler},
  doi          = {10.1109/TPAMI.2019.2941876},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {814-829},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Are large-scale 3D models really necessary for accurate visual localization?},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Approximate graph laplacians for multimodal data clustering.
<em>TPAMI</em>, <em>43</em>(3), 798–813. (<a
href="https://doi.org/10.1109/TPAMI.2019.2945574">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the important approaches of handling data heterogeneity in multimodal data clustering is modeling each modality using a separate similarity graph. Information from the multiple graphs is integrated by combining them into a unified graph. A major challenge here is how to preserve cluster information while removing noise from individual graphs. In this regard, a novel algorithm, termed as CoALa, is proposed that integrates noise-free approximations of multiple similarity graphs. The proposed method first approximates a graph using the most informative eigenpairs of its Laplacian which contain cluster information. The approximate Laplacians are then integrated for the construction of a low-rank subspace that best preserves overall cluster information of multiple graphs. However, this approximate subspace differs from the full-rank subspace which integrates information from all the eigenpairs of each Laplacian. Matrix perturbation theory is used to theoretically evaluate how far approximate subspace deviates from the full-rank one for a given value of approximation rank. Finally, spectral clustering is performed on the approximate subspace to identify the clusters. Experimental results on several real-life cancer and benchmark data sets demonstrate that the proposed algorithm significantly and consistently outperforms state-of-the-art integrative clustering approaches.},
  archive      = {J_TPAMI},
  author       = {Aparajita Khan and Pradipta Maji},
  doi          = {10.1109/TPAMI.2019.2945574},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {798-813},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Approximate graph laplacians for multimodal data clustering},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adversarial distillation for learning with privileged
provisions. <em>TPAMI</em>, <em>43</em>(3), 786–797. (<a
href="https://doi.org/10.1109/TPAMI.2019.2942592">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge distillation aims to train a student (model) for accurate inference in a resource-constrained environment. Traditionally, the student is trained by a high-capacity teacher (model) whose training is resource-intensive. The student trained this way is suboptimal because it is difficult to learn the real data distribution from the teacher. To address this issue, we propose to train the student against a discriminator in a minimax game. Such a minimax game has an issue that it can take an excessively long time for the training to converge. To address this issue, we propose adversarial distillation consisting of a student, a teacher, and a discriminator. The discriminator is now a multi-class classifier that distinguishes among the real data, the student, and the teacher. The student and the teacher aim to fool the discriminator via adversarial losses, while they learn from each other via distillation losses. By optimizing the adversarial and the distillation losses simultaneously, the student and the teacher can learn the real data distribution. To accelerate the training, we propose to obtain low-variance gradient updates from the discriminator using a Gumbel-Softmax trick. We conduct extensive experiments to demonstrate the superiority of the proposed adversarial distillation under both accuracy and training speed.},
  archive      = {J_TPAMI},
  author       = {Xiaojie Wang and Rui Zhang and Yu Sun and Jianzhong Qi},
  doi          = {10.1109/TPAMI.2019.2942592},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {786-797},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Adversarial distillation for learning with privileged provisions},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A review of domain adaptation without target labels.
<em>TPAMI</em>, <em>43</em>(3), 766–785. (<a
href="https://doi.org/10.1109/TPAMI.2019.2945942">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain adaptation has become a prominent problem setting in machine learning and related fields. This review asks the question: How can a classifier learn from a source domain and generalize to a target domain? We present a categorization of approaches, divided into, what we refer to as, sample-based, feature-based, and inference-based methods. Sample-based methods focus on weighting individual observations during training based on their importance to the target domain. Feature-based methods revolve around on mapping, projecting, and representing features such that a source classifier performs well on the target domain and inference-based methods incorporate adaptation into the parameter estimation procedure, for instance through constraints on the optimization procedure. Additionally, we review a number of conditions that allow for formulating bounds on the cross-domain generalization error. Our categorization highlights recurring ideas and raises questions important to further research.},
  archive      = {J_TPAMI},
  author       = {Wouter M. Kouw and Marco Loog},
  doi          = {10.1109/TPAMI.2019.2945942},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {766-785},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {A review of domain adaptation without target labels},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A graph-based approach for making consensus-based decisions
in image search and person re-identification. <em>TPAMI</em>,
<em>43</em>(3), 753–765. (<a
href="https://doi.org/10.1109/TPAMI.2019.2944597">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image matching and retrieval is the underlying problem in various directions of computer vision research, such as image search, biometrics, and person re-identification. The problem involves searching for the closest match to a query image in a database of images. This work presents a method for generating a consensus amongst multiple algorithms for image matching and retrieval. The proposed algorithm, Shortest Hamiltonian Path Estimation (SHaPE), maps the process of ranking candidates based on a set of scores to a graph-theoretic problem. This mapping is extended to incorporate results from multiple sets of scores obtained from different matching algorithms. The problem of consensus-based decision-making is solved by searching for a suitable path in the graph under specified constraints using a two-step process. First, a greedy algorithm is employed to generate an approximate solution. In the second step, the graph is extended and the problem is solved by applying Ant Colony Optimization. Experiments are performed for image search and person re-identification to illustrate the efficiency of SHaPE in image matching and retrieval. Although SHaPE is presented in the context of image retrieval, it can be applied, in general, to any problem involving the ranking of candidates based on multiple sets of scores.},
  archive      = {J_TPAMI},
  author       = {Arko Barman and Shishir K. Shah},
  doi          = {10.1109/TPAMI.2019.2944597},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {3},
  pages        = {753-765},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {A graph-based approach for making consensus-based decisions in image search and person re-identification},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). [Masthead]. <em>TPAMI</em>, <em>43</em>(2), C4. (<a
href="https://doi.org/10.1109/TPAMI.2020.3043700">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Provides a listing of current staff, committee members and society officers.},
  archive      = {J_TPAMI},
  doi          = {10.1109/TPAMI.2020.3043700},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {C4},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {[Masthead]},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021g). [Front inside cover]. <em>TPAMI</em>, <em>43</em>(2), C3.
(<a href="https://doi.org/10.1109/TPAMI.2020.3043696">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presents a listing of the editorial board, board of governors, current staff, committee members, and/or society editors for this issue of the publication.},
  archive      = {J_TPAMI},
  doi          = {10.1109/TPAMI.2020.3043696},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {C3},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {[Front inside cover]},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Matching seqlets: An unsupervised approach for locality
preserving sequence matching. <em>TPAMI</em>, <em>43</em>(2), 745–752.
(<a href="https://doi.org/10.1109/TPAMI.2019.2934052">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel unsupervised approach for sequence matching by explicitly accounting for the locality properties in the sequences. In contrast to conventional approaches that rely on frame-to-frame matching, we conduct matching using sequencelet or seqlet, a sub-sequence wherein the frames share strong similarities and are thus grouped together. The optimal seqlets and matching between them are learned jointly, without any supervision from users. The learned seqlets preserve the locality information at the scale of interest and resolve the ambiguities during matching, which are omitted by frame-based matching methods. We show that our proposed approach outperforms the state-of-the-art ones on datasets of different domains including human actions, facial expressions, speech, and character strokes.},
  archive      = {J_TPAMI},
  author       = {Jiayan Qiu and Xinchao Wang and Pascal Fua and Dacheng Tao},
  doi          = {10.1109/TPAMI.2019.2934052},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {745-752},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Matching seqlets: An unsupervised approach for locality preserving sequence matching},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Virtual point removal for large-scale 3D point clouds with
multiple glass planes. <em>TPAMI</em>, <em>43</em>(2), 729–744. (<a
href="https://doi.org/10.1109/TPAMI.2019.2933818">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large-scale 3D point clouds (LS3DPCs) captured by terrestrial LiDAR scanners often include virtual points which are generated by glass reflection. The virtual points may degrade the performance of various computer vision techniques when applied to LS3DPCs. In this paper, we propose a virtual point removal algorithm for LS3DPCs with multiple glass planes. We first estimate multiple glass regions by modeling the reliability with respect to each glass plane, respectively, such that the regions are assigned high reliability when they have multiple echo pulses for each emitted laser pulse. Then we detect each point whether it is a virtual point or not. For a given point, we recursively traverse all the possible trajectories of reflection, and select the optimal trajectory which provides a point with a similar geometric feature to a given point at the symmetric location. We evaluate the performance of the proposed algorithm on various LS3DPC models with diverse numbers of glass planes. Experimental results show that the proposed algorithm estimates multiple glass regions faithfully and detects the virtual points successfully. Moreover, we also show that the proposed algorithm yields a much better performance of reflection artifact removal compared with the existing method qualitatively and quantitatively.},
  archive      = {J_TPAMI},
  author       = {Jae-Seong Yun and Jae-Young Sim},
  doi          = {10.1109/TPAMI.2019.2933818},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {729-744},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Virtual point removal for large-scale 3D point clouds with multiple glass planes},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Switchable normalization for learning-to-normalize deep
representation. <em>TPAMI</em>, <em>43</em>(2), 712–728. (<a
href="https://doi.org/10.1109/TPAMI.2019.2932062">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We address a learning-to-normalize problem by proposing Switchable Normalization (SN), which learns to select different normalizers for different normalization layers of a deep neural network. SN employs three distinct scopes to compute statistics (means and variances) including a channel, a layer, and a minibatch. SN switches between them by learning their importance weights in an end-to-end manner. It has several good properties. First, it adapts to various network architectures and tasks (see Fig. 1 ). Second, it is robust to a wide range of batch sizes, maintaining high performance even when small minibatch is presented (e.g., 2 images/GPU). Third, SN does not have sensitive hyper-parameter, unlike group normalization that searches the number of groups as a hyper-parameter. Without bells and whistles, SN outperforms its counterparts on various challenging benchmarks, such as ImageNet, COCO, CityScapes, ADE20K, MegaFace and Kinetics. Analyses of SN are also presented to answer the following three questions: (a) Is it useful to allow each normalization layer to select its own normalizer? (b) What impacts the choices of normalizers? (c) Do different tasks and datasets prefer different normalizers? We hope SN will help ease the usage and understand the normalization techniques in deep learning. The code of SN has been released at https://github.com/switchablenorms .},
  archive      = {J_TPAMI},
  author       = {Ping Luo and Ruimao Zhang and Jiamin Ren and Zhanglin Peng and Jingyu Li},
  doi          = {10.1109/TPAMI.2019.2932062},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {712-728},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Switchable normalization for learning-to-normalize deep representation},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Selfie video stabilization. <em>TPAMI</em>, <em>43</em>(2),
701–711. (<a href="https://doi.org/10.1109/TPAMI.2019.2931897">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel algorithm for stabilizing selfie videos. Our goal is to automatically generate stabilized video that has optimal smooth motion in the sense of both foreground and background. The key insight is that non-rigid foreground motion in selfie videos can be analyzed using a 3D face model, and background motion can be analyzed using optical flow. We use second derivative of temporal trajectory of selected pixels as the measure of smoothness. Our algorithm stabilizes selfie videos by minimizing the smoothness measure of the background, regularized by the motion of the foreground. Experiments show that our method outperforms state-of-the-art general video stabilization techniques in selfie videos.},
  archive      = {J_TPAMI},
  author       = {Jiyang Yu and Ravi Ramamoorthi},
  doi          = {10.1109/TPAMI.2019.2931897},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {701-711},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Selfie video stabilization},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Saliency prediction in the deep learning era: Successes and
limitations. <em>TPAMI</em>, <em>43</em>(2), 679–700. (<a
href="https://doi.org/10.1109/TPAMI.2019.2935715">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual saliency models have enjoyed a big leap in performance in recent years, thanks to advances in deep learning and large scale annotated data. Despite enormous effort and huge breakthroughs, however, models still fall short in reaching human-level accuracy. In this work, I explore the landscape of the field emphasizing on new deep saliency models, benchmarks, and datasets. A large number of image and video saliency models are reviewed and compared over two image benchmarks and two large scale video datasets. Further, I identify factors that contribute to the gap between models and humans and discuss the remaining issues that need to be addressed to build the next generation of more powerful saliency models. Some specific questions that are addressed include: in what ways current models fail, how to remedy them, what can be learned from cognitive studies of attention, how explicit saliency judgments relate to fixations, how to conduct fair model comparison, and what are the emerging applications of saliency models.},
  archive      = {J_TPAMI},
  author       = {Ali Borji},
  doi          = {10.1109/TPAMI.2019.2935715},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {679-700},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Saliency prediction in the deep learning era: Successes and limitations},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SafePredict: A meta-algorithm for machine learning that uses
refusals to guarantee correctness. <em>TPAMI</em>, <em>43</em>(2),
663–678. (<a href="https://doi.org/10.1109/TPAMI.2019.2932415">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {SafePredict is a novel meta-algorithm that works with any base prediction algorithm for online data to guarantee an arbitrarily chosen correctness rate, 1 - ε, by allowing refusals. Allowing refusals means that the meta-algorithm may refuse to emit a prediction produced by the base algorithm so that the error rate on non-refused predictions does not exceed ε. The SafePredict error bound does not rely on any assumptions on the data distribution or the base predictor. When the base predictor happens not to exceed the target error rate ε, SafePredict refuses only a finite number of times. When the error rate of the base predictor changes through time SafePredict makes use of a weight-shifting heuristic that adapts to these changes without knowing when the changes occur yet still maintains the correctness guarantee. Empirical results show that (i) SafePredict compares favorably with state-of-the-art confidence-based refusal mechanisms which fail to offer robust error guarantees; and (ii) combining SafePredict with such refusal mechanisms can in many cases further reduce the number of refusals. Our software is included in the supplementary material, which can be found on the Computer Society Digital Library at http://doi.ieeecomputersociety.org/10.1109/TPAMI.2019.2932415.},
  archive      = {J_TPAMI},
  author       = {Mustafa A. Kocak and David Ramirez and Elza Erkip and Dennis E. Shasha},
  doi          = {10.1109/TPAMI.2019.2932415},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {663-678},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {SafePredict: A meta-algorithm for machine learning that uses refusals to guarantee correctness},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Res2Net: A new multi-scale backbone architecture.
<em>TPAMI</em>, <em>43</em>(2), 652–662. (<a
href="https://doi.org/10.1109/TPAMI.2019.2938758">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Representing features at multiple scales is of great importance for numerous vision tasks. Recent advances in backbone convolutional neural networks (CNNs) continually demonstrate stronger multi-scale representation ability, leading to consistent performance gains on a wide range of applications. However, most existing methods represent the multi-scale features in a layer-wise manner. In this paper, we propose a novel building block for CNNs, namely Res2Net, by constructing hierarchical residual-like connections within one single residual block. The Res2Net represents multi-scale features at a granular level and increases the range of receptive fields for each network layer. The proposed Res2Net block can be plugged into the state-of-the-art backbone CNN models, e.g., ResNet, ResNeXt, and DLA. We evaluate the Res2Net block on all these models and demonstrate consistent performance gains over baseline models on widely-used datasets, e.g., CIFAR-100 and ImageNet. Further ablation studies and experimental results on representative computer vision tasks, i.e., object detection, class activation mapping, and salient object detection, further verify the superiority of the Res2Net over the state-of-the-art baseline methods. The source code and trained models are available on https://mmcheng.net/res2net/ .},
  archive      = {J_TPAMI},
  author       = {Shang-Hua Gao and Ming-Ming Cheng and Kai Zhao and Xin-Yu Zhang and Ming-Hsuan Yang and Philip Torr},
  doi          = {10.1109/TPAMI.2019.2938758},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {652-662},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Res2Net: A new multi-scale backbone architecture},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Reconstruction of geometric and optical parameters of
non-planar objects with thin film. <em>TPAMI</em>, <em>43</em>(2),
638–651. (<a href="https://doi.org/10.1109/TPAMI.2019.2937515">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Here, we propose a novel method to estimate the parameters of non-planar objects with thin film surfaces. Being able to estimate the optical parameters of objects with thin film surfaces has a wide range of applications from industrial inspections to biological and archaeology research. However, there are many challenging issues that need to be overcome to model such parameters. The appearance of thin film objects is highly dependent on the surface orientation and optical parameters such as the refractive index and film thickness. First, we therefore analyzed the optical parameters of non-planar objects with thin film surfaces. Next, we proposed and implemented an analysis procedure and demonstrated its effectiveness for studying planar objects with thin film surfaces. Finally, we developed a device to acquire the shapes and optical parameters of objects with thin film surfaces using a camera and demonstrated the effectiveness of our method experimentally. Then, we surveyed the errors caused by the light source. We discussed the difference between the theoretically obtained parameters and experimental data obtained using a hyper spectral camera.},
  archive      = {J_TPAMI},
  author       = {Yoshie Kobayashi and Tetsuro Morimoto and Imari Sato and Yasuhiro Mukaigawa and Takao Tomono and Katsushi Ikeuchi},
  doi          = {10.1109/TPAMI.2019.2937515},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {638-651},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Reconstruction of geometric and optical parameters of non-planar objects with thin film},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Reconstruct as far as you can: Consensus of non-rigid
reconstruction from feasible regions. <em>TPAMI</em>, <em>43</em>(2),
623–637. (<a href="https://doi.org/10.1109/TPAMI.2019.2931317">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Much progress has been made for non-rigid structure from motion (NRSfM) during the last two decades, which made it possible to provide reasonable solutions for synthetically-created benchmark data. In order to utilize these NRSfM techniques in more realistic situations, however, we are now facing two important problems that must be solved: First, general scenes contain complex deformations as well as multiple objects, which violates the usual assumptions of previous NRSfM proposals. Second, there are many unreconstructable regions in the video, either because of the discontinued tracks of 2D trajectories or those regions static towards the camera, which require careful manipulations. In this paper, we show that a consensus-based reconstruction framework can handle these issues effectively. Even though the entire scene is complex, its parts usually have simpler deformations, and even though there are some unreconstructable parts, they can be weeded out to reduce their harmful effect on the entire reconstruction. The main difficulty of this approach lies in identifying appropriate parts, however, it can be effectively avoided by sampling parts stochastically and then aggregate their reconstructions afterwards. Experimental results show that the proposed method renews the state-of-the-art for popular benchmark data under much harsher environments, i.e., narrow camera view ranges, and it can reconstruct video-based real-world data effectively for as many areas as it can without an elaborated user input.},
  archive      = {J_TPAMI},
  author       = {Geonho Cha and Minsik Lee and Jungchan Cho and Songhwai Oh},
  doi          = {10.1109/TPAMI.2019.2931317},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {623-637},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Reconstruct as far as you can: Consensus of non-rigid reconstruction from feasible regions},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Polyhedral conic classifiers for computer vision
applications and open set recognition. <em>TPAMI</em>, <em>43</em>(2),
608–622. (<a href="https://doi.org/10.1109/TPAMI.2019.2934455">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a family of quasi-linear discriminants that outperform current large-margin methods in sliding window visual object detection and open set recognition tasks. In these applications, the classification problems are both numerically imbalanced - positive (object class) training and test windows are much rarer than negative (non-class) ones - and geometrically asymmetric - the positive samples typically form compact, visually-coherent groups while negatives are much more diverse, including anything at all that is not a well-centered sample from the target class. For such tasks, there is a need for discriminants whose decision regions focus on tightly circumscribing the positive class, while still taking account of negatives in zones where the two classes overlap. To this end, we propose a family of quasi-linear “polyhedral conic” discriminants whose positive regions are distorted L 1 or L 2 balls. In addition, we also integrated the proposed classification loss into deep neural networks so that both the features and classifier can be learned simultaneously end-to-end fashion to improve the classification accuracies. The methods have properties and run-time complexities comparable to linear Support Vector Machines (SVMs), and they can be trained from either binary or positive-only samples using constrained quadratic programs related to SVMs. Our experiments show that they significantly outperform linear SVMs, deep neural networks using softmax loss function and existing one-class discriminants on a wide range of object detection, face verification, open set recognition and conventional closed-set classification tasks.},
  archive      = {J_TPAMI},
  author       = {Hakan Cevikalp and Halil Saglamlar},
  doi          = {10.1109/TPAMI.2019.2934455},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {608-622},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Polyhedral conic classifiers for computer vision applications and open set recognition},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Pattern of local gravitational force (PLGF): A novel local
image descriptor. <em>TPAMI</em>, <em>43</em>(2), 595–607. (<a
href="https://doi.org/10.1109/TPAMI.2019.2930192">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a novel local image descriptor called Pattern of Local Gravitational Force (PLGF). It is inspired by Law of Universal Gravitation. PLGF is a hybrid descriptor, which is a combination of two feature components: one is the Pattern of Local Gravitational Force Magnitude (PLGFM), and another is Pattern of Local Gravitational Force Angle (PLGFA). PLGFM encodes the local gravitational force magnitude, and PLGFA encodes the local gravitational force angle that the center pixel exerts on all other pixels within a local neighborhood. We propose a novel noise resistance and the edge-preserving binary pattern called neighbors to center difference binary pattern (NCDBP) for gravitational force magnitude encoding. Finally, the histograms of the two components are concatenated to construct the PLGF descriptor. Experimental results on the existing face recognition databases, texture database, and biomedical image database show that PLGF is an effective image descriptor, and it outperforms other widely used existing descriptors. Even if in complicated variations like noise, and illumination with smaller databases, a combination of PLGF and convolutional neural network (CNN) performs consistently better than other state-of-the-art techniques.},
  archive      = {J_TPAMI},
  author       = {Debotosh Bhattacharjee and Hiranmoy Roy},
  doi          = {10.1109/TPAMI.2019.2930192},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {595-607},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Pattern of local gravitational force (PLGF): A novel local image descriptor},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Parallel and scalable heat methods for geodesic distance
computation. <em>TPAMI</em>, <em>43</em>(2), 579–594. (<a
href="https://doi.org/10.1109/TPAMI.2019.2933209">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a parallel and scalable approach for geodesic distance computation on triangle meshes. Our key observation is that the recovery of geodesic distance with the heat method [1] can be reformulated as optimization of its gradients subject to integrability, which can be solved using an efficient first-order method that requires no linear system solving and converges quickly. Afterward, the geodesic distance is efficiently recovered by parallel integration of the optimized gradients in breadth-first order. Moreover, we employ a similar breadth-first strategy to derive a parallel Gauss-Seidel solver for the diffusion step in the heat method. To further lower the memory consumption from gradient optimization on faces, we also propose a formulation that optimizes the projected gradients on edges, which reduces the memory footprint by about 50 percent. Our approach is trivially parallelizable, with a low memory footprint that grows linearly with respect to the model size. This makes it particularly suitable for handling large models. Experimental results show that it can efficiently compute geodesic distance on meshes with more than 200 million vertices on a desktop PC with 128 GB RAM, outperforming the original heat method and other state-of-the-art geodesic distance solvers.},
  archive      = {J_TPAMI},
  author       = {Jiong Tao and Juyong Zhang and Bailin Deng and Zheng Fang and Yue Peng and Ying He},
  doi          = {10.1109/TPAMI.2019.2933209},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {579-594},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Parallel and scalable heat methods for geodesic distance computation},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Neural image compression for gigapixel histopathology image
analysis. <em>TPAMI</em>, <em>43</em>(2), 567–578. (<a
href="https://doi.org/10.1109/TPAMI.2019.2936841">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose Neural Image Compression (NIC), a two-step method to build convolutional neural networks for gigapixel image analysis solely using weak image-level labels. First, gigapixel images are compressed using a neural network trained in an unsupervised fashion, retaining high-level information while suppressing pixel-level noise. Second, a convolutional neural network (CNN) is trained on these compressed image representations to predict image-level labels, avoiding the need for fine-grained manual annotations. We compared several encoding strategies, namely reconstruction error minimization, contrastive training and adversarial feature learning, and evaluated NIC on a synthetic task and two public histopathology datasets. We found that NIC can exploit visual cues associated with image-level labels successfully, integrating both global and local visual information. Furthermore, we visualized the regions of the input gigapixel images where the CNN attended to, and confirmed that they overlapped with annotations from human experts.},
  archive      = {J_TPAMI},
  author       = {David Tellez and Geert Litjens and Jeroen van der Laak and Francesco Ciompi},
  doi          = {10.1109/TPAMI.2019.2936841},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {567-578},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Neural image compression for gigapixel histopathology image analysis},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Matrix completion with deterministic sampling: Theories and
methods. <em>TPAMI</em>, <em>43</em>(2), 549–566. (<a
href="https://doi.org/10.1109/TPAMI.2019.2937869">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In some significant applications such as data forecasting, the locations of missing entries cannot obey any non-degenerate distributions, questioning the validity of the prevalent assumption that the missing data is randomly chosen according to some probabilistic model. To break through the limits of random sampling, we explore in this paper the problem of real-valued matrix completion under the setup of deterministic sampling. We propose two conditions, isomeric condition and relative well-conditionedness, for guaranteeing an arbitrary matrix to be recoverable from a sampling of the matrix entries. It is provable that the proposed conditions are weaker than the assumption of uniform sampling and, most importantly, it is also provable that the isomeric condition is necessary for the completions of any partial matrices to be identifiable. Equipped with these new tools, we prove a collection of theorems for missing data recovery as well as convex/nonconvex matrix completion. Among other things, we study in detail a Schatten quasi-norm induced method termed isomeric dictionary pursuit (IsoDP), and we show that IsoDP exhibits some distinct behaviors absent in the traditional bilinear programs.},
  archive      = {J_TPAMI},
  author       = {Guangcan Liu and Qingshan Liu and Xiao-Tong Yuan and Meng Wang},
  doi          = {10.1109/TPAMI.2019.2937869},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {549-566},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Matrix completion with deterministic sampling: Theories and methods},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Mask TextSpotter: An end-to-end trainable neural network for
spotting text with arbitrary shapes. <em>TPAMI</em>, <em>43</em>(2),
532–548. (<a href="https://doi.org/10.1109/TPAMI.2019.2937086">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unifying text detection and text recognition in an end-to-end training fashion has become a new trend for reading text in the wild, as these two tasks are highly relevant and complementary. In this paper, we investigate the problem of scene text spotting, which aims at simultaneous text detection and recognition in natural images. An end-to-end trainable neural network named as Mask TextSpotter is presented. Different from the previous text spotters that follow the pipeline consisting of a proposal generation network and a sequence-to-sequence recognition network, Mask TextSpotter enjoys a simple and smooth end-to-end learning procedure, in which both detection and recognition can be achieved directly from two-dimensional space via semantic segmentation. Further, a spatial attention module is proposed to enhance the performance and universality. Benefiting from the proposed two-dimensional representation on both detection and recognition, it easily handles text instances of irregular shapes, for instance, curved text. We evaluate it on four English datasets and one multi-language dataset, achieving consistently superior performance over state-of-the-art methods in both detection and end-to-end text recognition tasks. Moreover, we further investigate the recognition module of our method separately, which significantly outperforms state-of-the-art methods on both regular and irregular text datasets for scene text recognition.},
  archive      = {J_TPAMI},
  author       = {Minghui Liao and Pengyuan Lyu and Minghang He and Cong Yao and Wenhao Wu and Xiang Bai},
  doi          = {10.1109/TPAMI.2019.2937086},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {532-548},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Mask TextSpotter: An end-to-end trainable neural network for spotting text with arbitrary shapes},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning energy-based spatial-temporal generative ConvNets
for dynamic patterns. <em>TPAMI</em>, <em>43</em>(2), 516–531. (<a
href="https://doi.org/10.1109/TPAMI.2019.2934852">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video sequences contain rich dynamic patterns, such as dynamic texture patterns that exhibit stationarity in the temporal domain, and action patterns that are non-stationary in either spatial or temporal domain. We show that an energy-based spatial-temporal generative ConvNet can be used to model and synthesize dynamic patterns. The model defines a probability distribution on the video sequence, and the log probability is defined by a spatial-temporal ConvNet that consists of multiple layers of spatial-temporal filters to capture spatial-temporal patterns of different scales. The model can be learned from the training video sequences by an “analysis by synthesis” learning algorithm that iterates the following two steps. Step 1 synthesizes video sequences from the currently learned model. Step 2 then updates the model parameters based on the difference between the synthesized video sequences and the observed training sequences. We show that the learning algorithm can synthesize realistic dynamic patterns. We also show that it is possible to learn the model from incomplete training sequences with either occluded pixels or missing frames, so that model learning and pattern completion can be accomplished simultaneously.},
  archive      = {J_TPAMI},
  author       = {Jianwen Xie and Song-Chun Zhu and Ying Nian Wu},
  doi          = {10.1109/TPAMI.2019.2934852},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {516-531},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Learning energy-based spatial-temporal generative ConvNets for dynamic patterns},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning continuous face age progression: A pyramid of GANs.
<em>TPAMI</em>, <em>43</em>(2), 499–515. (<a
href="https://doi.org/10.1109/TPAMI.2019.2930985">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The two underlying requirements of face age progression, i.e., aging accuracy and identity permanence, are not well studied in the literature. This paper presents a novel generative adversarial network based approach to address the issues in a coupled manner. It separately models the constraints for the intrinsic subject-specific characteristics and the age-specific facial changes with respect to the elapsed time, ensuring that the generated faces present desired aging effects while keeping personalized properties stable. To render photo-realistic facial details, high-level age-specific features conveyed by the synthesized face are estimated by a pyramidal adversarial discriminator at multiple scales, which simulates the aging effects in a finer way. Further, an adversarial learning scheme is introduced to simultaneously train a single generator and multiple parallel discriminators, resulting in smooth continuous face aging sequences. The proposed method is applicable even in the presence of variations in pose, expression, makeup, etc., achieving remarkably vivid aging effects. Quantitative evaluations by a COTS face recognition system demonstrate that the target age distributions are accurately recovered, and 99.88 and 99.98 percent age progressed faces can be correctly verified at 0.001 percent FAR after age transformations of approximately 28 and 23 years elapsed time on the MORPH and CACD databases, respectively. Both visual and quantitative assessments show that the approach advances the state-of-the-art.},
  archive      = {J_TPAMI},
  author       = {Hongyu Yang and Di Huang and Yunhong Wang and Anil K. Jain},
  doi          = {10.1109/TPAMI.2019.2930985},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {499-515},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Learning continuous face age progression: A pyramid of GANs},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Inferring latent domains for unsupervised deep domain
adaptation. <em>TPAMI</em>, <em>43</em>(2), 485–498. (<a
href="https://doi.org/10.1109/TPAMI.2019.2933829">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised Domain Adaptation (UDA) refers to the problem of learning a model in a target domain where labeled data are not available by leveraging information from annotated data in a source domain. Most deep UDA approaches operate in a single-source, single-target scenario, i.e., they assume that the source and the target samples arise from a single distribution. However, in practice most datasets can be regarded as mixtures of multiple domains. In these cases, exploiting traditional single-source, single-target methods for learning classification models may lead to poor results. Furthermore, it is often difficult to provide the domain labels for all data points, i.e. latent domains should be automatically discovered. This paper introduces a novel deep architecture which addresses the problem of UDA by automatically discovering latent domains in visual datasets and exploiting this information to learn robust target classifiers. Specifically, our architecture is based on two main components, i.e. a side branch that automatically computes the assignment of each sample to its latent domain and novel layers that exploit domain membership information to appropriately align the distribution of the CNN internal feature representations to a reference distribution. We evaluate our approach on publicly available benchmarks, showing that it outperforms state-of-the-art domain adaptation methods.},
  archive      = {J_TPAMI},
  author       = {Massimiliano Mancini and Lorenzo Porzi and Samuel Rota Buló and Barbara Caputo and Elisa Ricci},
  doi          = {10.1109/TPAMI.2019.2933829},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {485-498},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Inferring latent domains for unsupervised deep domain adaptation},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Graph embedding using frequency filtering. <em>TPAMI</em>,
<em>43</em>(2), 473–484. (<a
href="https://doi.org/10.1109/TPAMI.2019.2929519">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The target of graph embedding is to embed graphs in vector space such that the embedded feature vectors follow the differences and similarities of the source graphs. In this paper, a novel method named Frequency Filtering Embedding (FFE) is proposed which uses graph Fourier transform and Frequency filtering as a graph Fourier domain operator for graph feature extraction. Frequency filtering amplifies or attenuates selected frequencies using appropriate filter functions. Here, heat, anti-heat, part-sine and identity filter sets are proposed as the filter functions. A generalized version of FFE named GeFFE is also proposed by defining pseudo-Fourier operators. This method can be considered as a general framework for formulating some previously defined invariants in other works by choosing a suitable filter bank and defining suitable pseudo-Fourier operators. This flexibility empowers GeFFE to adapt itself to the properties of each graph dataset unlike the previous spectral embedding methods and leads to superior classification accuracy relative to the others. Utilizing the proposed part-sine filter set, which its members filter different parts of the spectrum in turn, improves the classification accuracy of GeFFE method. Additionally, GeFFE resolves the cospectrality problem entirely in tested datasets.},
  archive      = {J_TPAMI},
  author       = {Hoda Bahonar and Abdolreza Mirzaei and Saeed Sadri and Richard C. Wilson},
  doi          = {10.1109/TPAMI.2019.2929519},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {473-484},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Graph embedding using frequency filtering},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Faster first-order methods for stochastic non-convex
optimization on riemannian manifolds. <em>TPAMI</em>, <em>43</em>(2),
459–472. (<a href="https://doi.org/10.1109/TPAMI.2019.2933841">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {First-order non-convex Riemannian optimization algorithms have gained recent popularity in structured machine learning problems including principal component analysis and low-rank matrix completion. The current paper presents an efficient Riemannian Stochastic Path Integrated Differential EstimatoR (R-SPIDER) algorithm to solve the finite-sum and online Riemannian non-convex minimization problems. At the core of R-SPIDER is a recursive semi-stochastic gradient estimator that can accurately estimate Riemannian gradient under not only exponential mapping and parallel transport, but also general retraction and vector transport operations. Compared with prior Riemannian algorithms, such a recursive gradient estimation mechanism endows R-SPIDER with lower computational cost in first-order oracle complexity. Specifically, for finite-sum problems with $n$ components, R-SPIDER is proved to converge to an $\epsilon$ -approximate stationary point within $\mathcal {O}\big (\min \big (n+\frac{\sqrt{n}}{\epsilon ^2},\frac{1}{\epsilon ^3}\big)\big)$ stochastic gradient evaluations, beating the best-known complexity $\mathcal {O}\big (n+\frac{1}{\epsilon ^4}\big)$ ; for online optimization, R-SPIDER is shown to converge with $\mathcal {O}\big (\frac{1}{\epsilon ^3}\big)$ complexity which is, to the best of our knowledge, the first non-asymptotic result for online Riemannian optimization. For the special case of gradient dominated functions, we further develop a variant of R-SPIDER with improved linear rate of convergence. Extensive experimental results demonstrate the advantage of the proposed algorithms over the state-of-the-art Riemannian non-convex optimization methods.},
  archive      = {J_TPAMI},
  author       = {Pan Zhou and Xiao-Tong Yuan and Shuicheng Yan and Jiashi Feng},
  doi          = {10.1109/TPAMI.2019.2933841},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {459-472},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Faster first-order methods for stochastic non-convex optimization on riemannian manifolds},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fast exact evaluation of univariate kernel sums.
<em>TPAMI</em>, <em>43</em>(2), 447–458. (<a
href="https://doi.org/10.1109/TPAMI.2019.2930501">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents new methodology for computationally efficient evaluation of univariate kernel sums. It is shown that a rich class of kernels allows for exact evaluation of functions expressed as a sum of kernels using simple recursions. Given an ordered sample the computational complexity is linear in the sample size. Direct applications to the estimation of denisties and their derivatives shows that the proposed approach is competitive with the state-of-the-art. Extensions to multivariate problems including independent component analysis and spatial smoothing illustrate the versatility of univariate kernel estimators, and highlight the efficiency and accuracy of the proposed approach. Multiple applications in image processing, including image deconvolution; denoising; and reconstruction are considered, showing that the proposed approach offers very promising potential in these fields.},
  archive      = {J_TPAMI},
  author       = {David P. Hofmeyr},
  doi          = {10.1109/TPAMI.2019.2930501},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {447-458},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Fast exact evaluation of univariate kernel sums},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dual adversarial transfer for sequence labeling.
<em>TPAMI</em>, <em>43</em>(2), 434–446. (<a
href="https://doi.org/10.1109/TPAMI.2019.2931569">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new architecture for addressing sequence labeling, termed Dual Adversarial Transfer Network (DATNet). Specifically, the proposed DATNet includes two variants, i.e., DATNet-F and DATNet-P, which are proposed to explore effective feature fusion between high and low resource. To address the noisy and imbalanced training data, we propose a novel Generalized Resource-Adversarial Discriminator (GRAD) and adopt adversarial training to boost model generalization. We investigate the effects of different components of DATNet across different domains and languages, and show that significant improvement can be obtained especially for low-resource data. Without augmenting any additional hand-crafted features, we achieve state-of-the-art performances on CoNLL, Twitter, PTB-WSJ, OntoNotes and Universal Dependencies with three popular sequence labeling tasks, i.e., Named entity recognition (NER), Part-of-Speech (POS) Tagging and Chunking.},
  archive      = {J_TPAMI},
  author       = {Joey Tianyi Zhou and Hao Zhang and Di Jin and Xi Peng},
  doi          = {10.1109/TPAMI.2019.2931569},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {434-446},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Dual adversarial transfer for sequence labeling},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Discriminative video representation learning using support
vector classifiers. <em>TPAMI</em>, <em>43</em>(2), 420–433. (<a
href="https://doi.org/10.1109/TPAMI.2019.2937292">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most popular deep models for action recognition in videos generate independent predictions for short clips, which are then pooled heuristically to assign an action label to the full video segment. As not all frames may characterize the underlying action-indeed, many are common across multiple actions-pooling schemes that impose equal importance on all frames might be unfavorable. In an attempt to tackle this problem, we propose discriminative pooling, based on the notion that among the deep features generated on all short clips, there is at least one that characterizes the action. To identify these useful features, we resort to a negative bag consisting of features that are known to be irrelevant, for example, they are sampled either from datasets that are unrelated to our actions of interest or are CNN features produced via random noise as input. With the features from the video as a positive bag and the irrelevant features as the negative bag, we cast an objective to learn a (nonlinear) hyperplane that separates the unknown useful features from the rest in a multiple instance learning formulation within a support vector machine setup. We use the parameters of this separating hyperplane as a descriptor for the full video segment. Since these parameters are directly related to the support vectors in a max-margin framework, they can be treated as a weighted average pooling of the features from the bags, with zero weights given to non-support vectors. Our pooling scheme is end-to-end trainable within a deep learning framework. We report results from experiments on eight computer vision benchmark datasets spanning a variety of video-related tasks and demonstrate state-of-the-art performance across these tasks.},
  archive      = {J_TPAMI},
  author       = {Jue Wang and Anoop Cherian},
  doi          = {10.1109/TPAMI.2019.2937292},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {420-433},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Discriminative video representation learning using support vector classifiers},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep differentiable random forests for age estimation.
<em>TPAMI</em>, <em>43</em>(2), 404–419. (<a
href="https://doi.org/10.1109/TPAMI.2019.2937294">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Age estimation from facial images is typically cast as a label distribution learning or regression problem, since aging is a gradual progress. Its main challenge is the facial feature space w.r.t. ages is inhomogeneous, due to the large variation in facial appearance across different persons of the same age and the non-stationary property of aging. In this paper, we propose two Deep Differentiable Random Forests methods, Deep Label Distribution Learning Forest (DLDLF) and Deep Regression Forest (DRF), for age estimation. Both of them connect split nodes to the top layer of convolutional neural networks (CNNs) and deal with inhomogeneous data by jointly learning input-dependent data partitions at the split nodes and age distributions at the leaf nodes. This joint learning follows an alternating strategy: (1) Fixing the leaf nodes and optimizing the split nodes and the CNN parameters by Back-propagation; (2) Fixing the split nodes and optimizing the leaf nodes by Variational Bounding. Two Deterministic Annealing processes are introduced into the learning of the split and leaf nodes, respectively, to avoid poor local optima and obtain better estimates of tree parameters free of initial values. Experimental results show that DLDLF and DRF achieve state-of-the-art performance on three age estimation datasets.},
  archive      = {J_TPAMI},
  author       = {Wei Shen and Yilu Guo and Yan Wang and Kai Zhao and Bo Wang and Alan Yuille},
  doi          = {10.1109/TPAMI.2019.2937294},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {404-419},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Deep differentiable random forests for age estimation},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DAC-SDC low power object detection challenge for UAV
applications. <em>TPAMI</em>, <em>43</em>(2), 392–403. (<a
href="https://doi.org/10.1109/TPAMI.2019.2932429">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The 55th Design Automation Conference (DAC) held its first System Design Contest (SDC) in 2018. SDC&#39;18 features a lower power object detection challenge (LPODC) on designing and implementing novel algorithms based object detection in images taken from unmanned aerial vehicles (UAV). The dataset includes 95 categories and 150k images, and the hardware platforms include Nvidia&#39;s TX2 and Xilinx&#39;s PYNQ Z1. DAC-SDC&#39;18 attracted more than 110 entries from 12 countries. This paper presents in detail the dataset and evaluation procedure. It further discusses the methods developed by some of the entries as well as representative results. The paper concludes with directions for future improvements.},
  archive      = {J_TPAMI},
  author       = {Xiaowei Xu and Xinyi Zhang and Bei Yu and Xiaobo Sharon Hu and Christopher Rowen and Jingtong Hu and Yiyu Shi},
  doi          = {10.1109/TPAMI.2019.2932429},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {392-403},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {DAC-SDC low power object detection challenge for UAV applications},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Absolute pose estimation of central cameras using planar
regions. <em>TPAMI</em>, <em>43</em>(2), 377–391. (<a
href="https://doi.org/10.1109/TPAMI.2019.2931577">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel method is proposed for the absolute pose estimation of a central 2D camera with respect to 3D depth data without the use of any dedicated calibration pattern or explicit point correspondences. The proposed method has no specific assumption about the data source: plain depth information is expected from the 3D sensing device and a central camera is used to capture the 2D images. Both the perspective and omnidirectional central cameras are handled within a single generic camera model. Pose estimation is formulated as a 2D-3D nonlinear shape registration task which is solved without point correspondences or complex similarity metrics. It relies on a set of corresponding planar regions, and the pose parameters are obtained by solving an overdetermined system of nonlinear equations. The efficiency and robustness of the proposed method were confirmed on both large scale synthetic data and on real data acquired from various types of sensors.},
  archive      = {J_TPAMI},
  author       = {Robert Frohlich and Levente Tamas and Zoltan Kato},
  doi          = {10.1109/TPAMI.2019.2931577},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {2},
  pages        = {377-391},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Absolute pose estimation of central cameras using planar regions},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Cover. <em>TPAMI</em>, <em>43</em>(1), C3. (<a
href="https://doi.org/10.1109/TPAMI.2020.3036359">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {These instructions give guidelines for preparing papers for this publication. Presents information for authors publishing in this journal.},
  archive      = {J_TPAMI},
  doi          = {10.1109/TPAMI.2020.3036359},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {C3},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Cover},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). IEEE computer society has you covered! <em>TPAMI</em>,
<em>43</em>(1), 375. (<a
href="https://doi.org/10.1109/TPAMI.2020.3036355">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advertisement, IEEE.},
  archive      = {J_TPAMI},
  doi          = {10.1109/TPAMI.2020.3036355},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {375},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {IEEE computer society has you covered!},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Visual tracking via dynamic memory networks. <em>TPAMI</em>,
<em>43</em>(1), 360–374. (<a
href="https://doi.org/10.1109/TPAMI.2019.2929034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Template-matching methods for visual tracking have gained popularity recently due to their good performance and fast speed. However, they lack effective ways to adapt to changes in the target object&#39;s appearance, making their tracking accuracy still far from state-of-the-art. In this paper, we propose a dynamic memory network to adapt the template to the target&#39;s appearance variations during tracking. The reading and writing process of the external memory is controlled by an LSTM network with the search feature map as input. A spatial attention mechanism is applied to concentrate the LSTM input on the potential target as the location of the target is at first unknown. To prevent aggressive model adaptivity, we apply gated residual template learning to control the amount of retrieved memory that is used to combine with the initial template. In order to alleviate the drift problem, we also design a “negative” memory unit that stores templates for distractors, which are used to cancel out wrong responses from the object template. To further boost the tracking performance, an auxiliary classification loss is added after the feature extractor part. Unlike tracking-by-detection methods where the object&#39;s information is maintained by the weight parameters of neural networks, which requires expensive online fine-tuning to be adaptable, our tracker runs completely feed-forward and adapts to the target&#39;s appearance changes by updating the external memory. Moreover, the capacity of our model is not determined by the network size as with other trackers – the capacity can be easily enlarged as the memory requirements of a task increase, which is favorable for memorizing long-term object information. Extensive experiments on the OTB and VOT datasets demonstrate that our trackers perform favorably against state-of-the-art tracking methods while retaining real-time speed.},
  archive      = {J_TPAMI},
  author       = {Tianyu Yang and Antoni B. Chan},
  doi          = {10.1109/TPAMI.2019.2929034},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {360-374},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Visual tracking via dynamic memory networks},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Variational context: Exploiting visual and textual context
for grounding referring expressions. <em>TPAMI</em>, <em>43</em>(1),
347–359. (<a href="https://doi.org/10.1109/TPAMI.2019.2926266">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We focus on grounding (i.e., localizing or linking) referring expressions in images, e.g., “largest elephant standing behind baby elephant”. This is a general yet challenging vision-language task since it does not only require the localization of objects, but also the multimodal comprehension of context - visual attributes (e.g., “largest”, “baby”) and relationships (e.g., “behind”) that help to distinguish the referent from other objects, especially those of the same category. Due to the exponential complexity involved in modeling the context associated with multiple image regions, existing work oversimplifies this task to pairwise region modeling by multiple instance learning. In this paper, we propose a variational Bayesian method, called Variational Context, to solve the problem of complex context modeling in referring expression grounding. Specifically, our framework exploits the reciprocal relation between the referent and context, i.e., either of them influences estimation of the posterior distribution of the other, and thereby the search space of context can be greatly reduced. In addition to reciprocity, our framework considers the semantic information of context, i.e., the referring expression can be reproduced based on the estimated context. We also extend the model to unsupervised setting where no annotation for the referent is available. Extensive experiments on various benchmarks show consistent improvement over state-of-the-art methods in both supervised and unsupervised settings.},
  archive      = {J_TPAMI},
  author       = {Yulei Niu and Hanwang Zhang and Zhiwu Lu and Shih-Fu Chang},
  doi          = {10.1109/TPAMI.2019.2926266},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {347-359},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Variational context: Exploiting visual and textual context for grounding referring expressions},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Towards safe weakly supervised learning. <em>TPAMI</em>,
<em>43</em>(1), 334–346. (<a
href="https://doi.org/10.1109/TPAMI.2019.2922396">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study weakly supervised learning where a large amount of data supervision is not accessible. This includes i) incomplete supervision, where only a small subset of labels is given, such as semi-supervised learning and domain adaptation; ii) inexact supervision, where only coarse-grained labels are given, such as multi-instance learning and iii) inaccurate supervision, where the given labels are not always ground-truth, such as label noise learning. Unlike supervised learning which typically achieves performance improvement with more labeled examples, weakly supervised learning may sometimes even degenerate performance with more weakly supervised data. Such deficiency seriously hinders the deployment of weakly supervised learning to real tasks. It is thus highly desired to study safe weakly supervised learning, which never seriously hurts performance. To this end, we present a generic ensemble learning scheme to derive a safe prediction by integrating multiple weakly supervised learners. We optimize the worst-case performance gain and lead to a maximin optimization. This brings multiple advantages to safe weakly supervised learning. First, for many commonly used convex loss functions in classification and regression, it is guaranteed to derive a safe prediction under a mild condition. Second, prior knowledge related to the weight of the base weakly supervised learners can be flexibly embedded. Third, it can be globally and efficiently addressed by simple convex quadratic or linear program. Finally, it is in an intuitive geometric interpretation with the least square loss. Extensive experiments on various weakly supervised learning tasks, including semi-supervised learning, domain adaptation, multi-instance learning and label noise learning demonstrate our effectiveness.},
  archive      = {J_TPAMI},
  author       = {Yu-Feng Li and Lan-Zhe Guo and Zhi-Hua Zhou},
  doi          = {10.1109/TPAMI.2019.2922396},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {334-346},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Towards safe weakly supervised learning},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The perils and pitfalls of block design for EEG
classification experiments. <em>TPAMI</em>, <em>43</em>(1), 316–333. (<a
href="https://doi.org/10.1109/TPAMI.2020.2973153">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A recent paper [1] claims to classify brain processing evoked in subjects watching ImageNet stimuli as measured with EEG and to employ a representation derived from this processing to construct a novel object classifier. That paper, together with a series of subsequent papers [2] , [3] , [4] , [5] , [6] , [7] , [8] , claims to achieve successful results on a wide variety of computer-vision tasks, including object classification, transfer learning, and generation of images depicting human perception and thought using brain-derived representations measured through EEG. Our novel experiments and analyses demonstrate that their results crucially depend on the block design that they employ, where all stimuli of a given class are presented together, and fail with a rapid-event design, where stimuli of different classes are randomly intermixed. The block design leads to classification of arbitrary brain states based on block-level temporal correlations that are known to exist in all EEG data, rather than stimulus-related activity. Because every trial in their test sets comes from the same block as many trials in the corresponding training sets, their block design thus leads to classifying arbitrary temporal artifacts of the data instead of stimulus-related activity. This invalidates all subsequent analyses performed on this data in multiple published papers and calls into question all of the reported results. We further show that a novel object classifier constructed with a random codebook performs as well as or better than a novel object classifier constructed with the representation extracted from EEG data, suggesting that the performance of their classifier constructed with a representation extracted from EEG data does not benefit from the brain-derived representation. Together, our results illustrate the far-reaching implications of the temporal autocorrelations that exist in all neuroimaging data for classification experiments. Further, our results calibrate the underlying difficulty of the tasks involved and caution against overly optimistic, but incorrect, claims to the contrary.},
  archive      = {J_TPAMI},
  author       = {Ren Li and Jared S. Johansen and Hamad Ahmed and Thomas V. Ilyevsky and Ronnie B. Wilbur and Hari M. Bharadwaj and Jeffrey Mark Siskind},
  doi          = {10.1109/TPAMI.2020.2973153},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {316-333},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {The perils and pitfalls of block design for EEG classification experiments},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Stereo matching using multi-level cost volume and
multi-scale feature constancy. <em>TPAMI</em>, <em>43</em>(1), 300–315.
(<a href="https://doi.org/10.1109/TPAMI.2019.2928550">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For CNNs based stereo matching methods, cost volumes play an important role in achieving good matching accuracy. In this paper, we present an end-to-end trainable convolution neural network to fully use cost volumes for stereo matching. Our network consists of three sub-modules, i.e., shared feature extraction, initial disparity estimation, and disparity refinement. Cost volumes are calculated at multiple levels using the shared features, and are used in both initial disparity estimation and disparity refinement sub-modules. To improve the efficiency of disparity refinement, multi-scale feature constancy is introduced to measure the correctness of the initial disparity in feature space. These sub-modules of our network are tightly-coupled, making it compact and easy to train. Moreover, we investigate the problem of developing a robust model to perform well across multiple datasets with different characteristics. We achieve this by introducing a two-stage finetuning scheme to gently transfer the model to target datasets. Specifically, in the first stage, the model is finetuned using both a large synthetic dataset and the target datasets with a relatively large learning rate, while in the second stage the model is trained using only the target datasets with a small learning rate. The proposed method is tested on several benchmarks including the Middlebury 2014, KITTI 2015, ETH3D 2017, and SceneFlow datasets. Experimental results show that our method achieves the state-of-the-art performance on all the datasets. The proposed method also won the 1st prize on the Stereo task of Robust Vision Challenge 2018.},
  archive      = {J_TPAMI},
  author       = {Zhengfa Liang and Yulan Guo and Yiliu Feng and Wei Chen and Linbo Qiao and Li Zhou and Jianfeng Zhang and Hengzhu Liu},
  doi          = {10.1109/TPAMI.2019.2928550},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {300-315},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Stereo matching using multi-level cost volume and multi-scale feature constancy},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Simultaneous fidelity and regularization learning for image
restoration. <em>TPAMI</em>, <em>43</em>(1), 284–299. (<a
href="https://doi.org/10.1109/TPAMI.2019.2926357">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing non-blind restoration methods are based on the assumption that a precise degradation model is known. As the degradation process can only be partially known or inaccurately modeled, images may not be well restored. Rain streak removal and image deconvolution with inaccurate blur kernels are two representative examples of such tasks. For rain streak removal, although an input image can be decomposed into a scene layer and a rain streak layer, there exists no explicit formulation for modeling rain streaks and the composition with scene layer. For blind deconvolution, as estimation error of blur kernel is usually introduced, the subsequent non-blind deconvolution process does not restore the latent image well. In this paper, we propose a principled algorithm within the maximum a posterior framework to tackle image restoration with a partially known or inaccurate degradation model. Specifically, the residual caused by a partially known or inaccurate degradation model is spatially dependent and complexly distributed. With a training set of degraded and ground-truth image pairs, we parameterize and learn the fidelity term for a degradation model in a task-driven manner. Furthermore, the regularization term can also be learned along with the fidelity term, thereby forming a simultaneous fidelity and regularization learning model. Extensive experimental results demonstrate the effectiveness of the proposed model for image deconvolution with inaccurate blur kernels, deconvolution with multiple degradations and rain streak removal.},
  archive      = {J_TPAMI},
  author       = {Dongwei Ren and Wangmeng Zuo and David Zhang and Lei Zhang and Ming-Hsuan Yang},
  doi          = {10.1109/TPAMI.2019.2926357},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {284-299},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Simultaneous fidelity and regularization learning for image restoration},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). RotationNet for joint object categorization and unsupervised
pose estimation from multi-view images. <em>TPAMI</em>, <em>43</em>(1),
269–283. (<a href="https://doi.org/10.1109/TPAMI.2019.2922640">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a Convolutional Neural Network (CNN)-based model “RotationNet,” which takes multi-view images of an object as input and jointly estimates its pose and object category. Unlike previous approaches that use known viewpoint labels for training, our method treats the viewpoint labels as latent variables, which are learned in an unsupervised manner during the training using an unaligned object dataset. RotationNet uses only a partial set of multi-view images for inference, and this property makes it useful in practical scenarios where only partial views are available. Moreover, our pose alignment strategy enables one to obtain view-specific feature representations shared across classes, which is important to maintain high accuracy in both object categorization and pose estimation. Effectiveness of RotationNet is demonstrated by its superior performance to the state-of-the-art methods of 3D object classification on 10- and 40-class ModelNet datasets. We also show that RotationNet, even trained without known poses, achieves comparable performance to the state-of-the-art methods on an object pose estimation dataset. Furthermore, our object ranking method based on classification by RotationNet achieved the first prize in two tracks of the 3D Shape Retrieval Contest (SHREC) 2017. Finally, we demonstrate the performance of real-world applications of RotationNet trained with our newly created multi-view image dataset using a moving USB camera.},
  archive      = {J_TPAMI},
  author       = {Asako Kanezaki and Yasuyuki Matsushita and Yoshifumi Nishida},
  doi          = {10.1109/TPAMI.2019.2922640},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {269-283},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {RotationNet for joint object categorization and unsupervised pose estimation from multi-view images},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Rotation averaging with the chordal distance: Global
minimizers and strong duality. <em>TPAMI</em>, <em>43</em>(1), 256–268.
(<a href="https://doi.org/10.1109/TPAMI.2019.2930051">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we explore the role of duality principles within the problem of rotation averaging, a fundamental task in a wide range of applications. In its conventional form, rotation averaging is stated as a minimization over multiple rotation constraints. As these constraints are non-convex, this problem is generally considered challenging to solve globally. We show how to circumvent this difficulty through the use of Lagrangian duality. While such an approach is well-known it is normally not guaranteed to provide a tight relaxation. Based on spectral graph theory, we analytically prove that in many cases there is no duality gap unless the noise levels are severe. This allows us to obtain certifiably global solutions to a class of important non-convex problems in polynomial time. We also propose an efficient, scalable algorithm that outperforms general purpose numerical solvers by a large margin and compares favourably to current state-of-the-art. Further, our approach is able to handle the large problem instances commonly occurring in structure from motion settings and it is trivially parallelizable. Experiments are presented for a number of different instances of both synthetic and real-world data.},
  archive      = {J_TPAMI},
  author       = {Anders Eriksson and Carl Olsson and Fredrik Kahl and Tat-Jun Chin},
  doi          = {10.1109/TPAMI.2019.2930051},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {256-268},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Rotation averaging with the chordal distance: Global minimizers and strong duality},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust low-rank tensor recovery with rectification and
alignment. <em>TPAMI</em>, <em>43</em>(1), 238–255. (<a
href="https://doi.org/10.1109/TPAMI.2019.2929043">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-rank tensor recovery in the presence of sparse but arbitrary errors is an important problem with many practical applications. In this work, we propose a general framework that recovers low-rank tensors, in which the data can be deformed by some unknown transformations and corrupted by arbitrary sparse errors. We give a unified presentation of the surrogate-based formulations that incorporate the features of rectification and alignment simultaneously, and establish worst-case error bounds of the recovered tensor. In this context, the state-of-the-art methods `RASL&#39; and `TILT&#39; can be viewed as two special cases of our work, and yet each only performs part of the function of our method. Subsequently, we study the optimization aspects of the problem in detail by deriving two algorithms, one based on the alternating direction method of multipliers (ADMM) and the other based on proximal gradient. We provide convergence guarantees for the latter algorithm, and demonstrate the performance of the former through in-depth simulations. Finally, we present extensive experimental results on public datasets to demonstrate the effectiveness and efficiency of the proposed framework and algorithms.},
  archive      = {J_TPAMI},
  author       = {Xiaoqin Zhang and Di Wang and Zhengyuan Zhou and Yi Ma},
  doi          = {10.1109/TPAMI.2019.2929043},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {238-255},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Robust low-rank tensor recovery with rectification and alignment},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Revisiting video saliency prediction in the deep learning
era. <em>TPAMI</em>, <em>43</em>(1), 220–237. (<a
href="https://doi.org/10.1109/TPAMI.2019.2924417">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting where people look in static scenes, a.k.a visual saliency, has received significant research interest recently. However, relatively less effort has been spent in understanding and modeling visual attention over dynamic scenes. This work makes three contributions to video saliency research. First, we introduce a new benchmark, called DHF1K (Dynamic Human Fixation 1K), for predicting fixations during dynamic scene free-viewing, which is a long-time need in this field. DHF1K consists of 1K high-quality elaborately-selected video sequences annotated by 17 observers using an eye tracker device. The videos span a wide range of scenes, motions, object types and backgrounds. Second, we propose a novel video saliency model, called ACLNet (Attentive CNN-LSTM Network), that augments the CNN-LSTM architecture with a supervised attention mechanism to enable fast end-to-end saliency learning. The attention mechanism explicitly encodes static saliency information, thus allowing LSTM to focus on learning a more flexible temporal saliency representation across successive frames. Such a design fully leverages existing large-scale static fixation datasets, avoids overfitting, and significantly improves training efficiency and testing performance. Third, we perform an extensive evaluation of the state-of-the-art saliency models on three datasets : DHF1K, Hollywood-2, and UCF sports. An attribute-based analysis of previous saliency models and cross-dataset generalization are also presented. Experimental results over more than 1.2K testing videos containing 400K frames demonstrate that ACLNet outperforms other contenders and has a fast processing speed (40 fps using a single GPU). Our code and all the results are available at https://github.com/wenguanwang/DHF1K .},
  archive      = {J_TPAMI},
  author       = {Wenguan Wang and Jianbing Shen and Jianwen Xie and Ming-Ming Cheng and Haibin Ling and Ali Borji},
  doi          = {10.1109/TPAMI.2019.2924417},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {220-237},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Revisiting video saliency prediction in the deep learning era},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Relative saliency and ranking: Models, metrics, data and
benchmarks. <em>TPAMI</em>, <em>43</em>(1), 204–219. (<a
href="https://doi.org/10.1109/TPAMI.2019.2927203">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Salient object detection is a problem that has been considered in detail and many solutions have been proposed. In this paper, we argue that work to date has addressed a problem that is relatively ill-posed. Specifically, there is not universal agreement about what constitutes a salient object when multiple observers are queried. This implies that some objects are more likely to be judged salient than others, and implies a relative rank exists on salient objects. Initially, we present a novel deep learning solution based on a hierarchical representation of relative saliency and stage-wise refinement. Further to this, we present data, analysis and baseline benchmark results towards addressing the problem of salient object ranking. Methods for deriving suitable ranked salient object instances are presented, along with metrics suitable to measuring algorithm performance. In addition, we show how a derived dataset can be successively refined to provide cleaned results that correlate well with pristine ground truth in its characteristics and value for training and testing models. Finally, we provide a comparison among prevailing algorithms that address salient object ranking or detection to establish initial baselines providing a basis for comparison with future efforts addressing this problem. The source code and data are publicly available via our project page: ryersonvisionlab.github.io/cocosalrank.},
  archive      = {J_TPAMI},
  author       = {Mahmoud Kalash and Md Amirul Islam and Neil D. B. Bruce},
  doi          = {10.1109/TPAMI.2019.2927203},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {204-219},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Relative saliency and ranking: Models, metrics, data and benchmarks},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Recipe1M+: A dataset for learning cross-modal embeddings for
cooking recipes and food images. <em>TPAMI</em>, <em>43</em>(1),
187–203. (<a href="https://doi.org/10.1109/TPAMI.2019.2927476">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we introduce Recipe1M+, a new large-scale, structured corpus of over one million cooking recipes and 13 million food images. As the largest publicly available collection of recipe data, Recipe1M+ affords the ability to train high-capacity models on aligned, multimodal data. Using these data, we train a neural network to learn a joint embedding of recipes and images that yields impressive results on an image-recipe retrieval task. Moreover, we demonstrate that regularization via the addition of a high-level classification objective both improves retrieval performance to rival that of humans and enables semantic vector arithmetic. We postulate that these embeddings will provide a basis for further exploration of the Recipe1M+ dataset and food and cooking in general. Code, data and models are publicly available. 1 1.http://im2recipe.csail.mit.edu.},
  archive      = {J_TPAMI},
  author       = {Javier Marín and Aritro Biswas and Ferda Ofli and Nicholas Hynes and Amaia Salvador and Yusuf Aytar and Ingmar Weber and Antonio Torralba},
  doi          = {10.1109/TPAMI.2019.2927476},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {187-203},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Recipe1M+: A dataset for learning cross-modal embeddings for cooking recipes and food images},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). OpenPose: Realtime multi-person 2D pose estimation using
part affinity fields. <em>TPAMI</em>, <em>43</em>(1), 172–186. (<a
href="https://doi.org/10.1109/TPAMI.2019.2929257">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Realtime multi-person 2D pose estimation is a key component in enabling machines to have an understanding of people in images and videos. In this work, we present a realtime approach to detect the 2D pose of multiple people in an image. The proposed method uses a nonparametric representation, which we refer to as Part Affinity Fields (PAFs), to learn to associate body parts with individuals in the image. This bottom-up system achieves high accuracy and realtime performance, regardless of the number of people in the image. In previous work, PAFs and body part location estimation were refined simultaneously across training stages. We demonstrate that a PAF-only refinement rather than both PAF and body part location refinement results in a substantial increase in both runtime performance and accuracy. We also present the first combined body and foot keypoint detector, based on an internal annotated foot dataset that we have publicly released. We show that the combined detector not only reduces the inference time compared to running them sequentially, but also maintains the accuracy of each component individually. This work has culminated in the release of OpenPose, the first open-source realtime system for multi-person 2D pose detection, including body, foot, hand, and facial keypoints.},
  archive      = {J_TPAMI},
  author       = {Zhe Cao and Gines Hidalgo and Tomas Simon and Shih-En Wei and Yaser Sheikh},
  doi          = {10.1109/TPAMI.2019.2929257},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {172-186},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {OpenPose: Realtime multi-person 2D pose estimation using part affinity fields},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On learning 3D face morphable model from in-the-wild images.
<em>TPAMI</em>, <em>43</em>(1), 157–171. (<a
href="https://doi.org/10.1109/TPAMI.2019.2927975">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a classic statistical model of 3D facial shape and albedo, 3D Morphable Model (3DMM) is widely used in facial analysis, e.g., model fitting, image synthesis. Conventional 3DMM is learned from a set of 3D face scans with associated well-controlled 2D face images, and represented by two sets of PCA basis functions. Due to the type and amount of training data, as well as, the linear bases, the representation power of 3DMM can be limited. To address these problems, this paper proposes an innovative framework to learn a nonlinear 3DMM model from a large set of in-the-wild face images, without collecting 3D face scans. Specifically, given a face image as input, a network encoder estimates the projection, lighting, shape and albedo parameters. Two decoders serve as the nonlinear 3DMM to map from the shape and albedo parameters to the 3D shape and albedo, respectively. With the projection parameter, lighting, 3D shape, and albedo, a novel analytically-differentiable rendering layer is designed to reconstruct the original input face. The entire network is end-to-end trainable with only weak supervision. We demonstrate the superior representation power of our nonlinear 3DMM over its linear counterpart, and its contribution to face alignment, 3D reconstruction, and face editing. Source code and additional results can be found at our project page: http://cvlab.cse.msu.edu/project-nonlinear-3dmm.html.},
  archive      = {J_TPAMI},
  author       = {Luan Tran and Xiaoming Liu},
  doi          = {10.1109/TPAMI.2019.2927975},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {157-171},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {On learning 3D face morphable model from in-the-wild images},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multiset feature learning for highly imbalanced data
classification. <em>TPAMI</em>, <em>43</em>(1), 139–156. (<a
href="https://doi.org/10.1109/TPAMI.2019.2929166">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the expansion of data, increasing imbalanced data has emerged. When the imbalance ratio (IR) of data is high, most existing imbalanced learning methods decline seriously in classification performance. In this paper, we systematically investigate the highly imbalanced data classification problem, and propose an uncorrelated cost-sensitive multiset learning (UCML) approach for it. Specifically, UCML first constructs multiple balanced subsets through random partition, and then employs the multiset feature learning (MFL) to learn discriminant features from the constructed multiset. To enhance the usability of each subset and deal with the non-linearity issue existed in each subset, we further propose a deep metric based UCML (DM-UCML) approach. DM-UCML introduces the generative adversarial network technique into the multiset constructing process, such that each subset can own similar distribution with the original dataset. To cope with the non-linearity issue, DM-UCML integrates deep metric learning with MFL, such that more favorable performance can be achieved. In addition, DM-UCML designs a new discriminant term to enhance the discriminability of learned metrics. Experiments on eight traditional highly class-imbalanced datasets and two large-scale datasets indicate that: the proposed approaches outperform state-of-the-art highly imbalanced learning methods and are more robust to high IR.},
  archive      = {J_TPAMI},
  author       = {Xiao-Yuan Jing and Xinyu Zhang and Xiaoke Zhu and Fei Wu and Xinge You and Yang Gao and Shiguang Shan and Jing-Yu Yang},
  doi          = {10.1109/TPAMI.2019.2929166},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {139-156},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Multiset feature learning for highly imbalanced data classification},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Distributed variational representation learning.
<em>TPAMI</em>, <em>43</em>(1), 120–138. (<a
href="https://doi.org/10.1109/TPAMI.2019.2928806">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of distributed representation learning is one in which multiple sources of information X 1 ,..., X K are processed separately so as to learn as much information as possible about some ground truth Y. We investigate this problem from information-theoretic grounds, through a generalization of Tishby&#39;s centralized Information Bottleneck (IB) method to the distributed setting. Specifically, K encoders, K ≥ 2, compress their observations X 1 ,..., X K separately in a manner such that, collectively, the produced representations preserve as much information as possible about Y. We study both discrete memoryless (DM) and memoryless vector Gaussian data models. For the discrete model, we establish a single-letter characterization of the optimal tradeoff between complexity (or rate) and relevance (or information) for a class of memoryless sources (the observations X 1 ,..., X K being conditionally independent given Y). For the vector Gaussian model, we provide an explicit characterization of the optimal complexity-relevance tradeoff. Furthermore, we develop a variational bound on the complexity-relevance tradeoff which generalizes the evidence lower bound (ELBO) to the distributed setting. We also provide two algorithms that allow to compute this bound: i) a Blahut-Arimoto type iterative algorithm which enables to compute optimal complexity-relevance encoding mappings by iterating over a set of self-consistent equations, and ii) a variational inference type algorithm in which the encoding mappings are parametrized by neural networks and the bound approximated by Markov sampling and optimized with stochastic gradient descent. Numerical results on synthetic and real datasets are provided to support the efficiency of the approaches and algorithms developed in this paper.},
  archive      = {J_TPAMI},
  author       = {Iñaki Estella Aguerri and Abdellatif Zaidi},
  doi          = {10.1109/TPAMI.2019.2928806},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {120-138},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Distributed variational representation learning},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep affinity network for multiple object tracking.
<em>TPAMI</em>, <em>43</em>(1), 104–119. (<a
href="https://doi.org/10.1109/TPAMI.2019.2929520">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple Object Tracking (MOT) plays an important role in solving many fundamental problems in video analysis and computer vision. Most MOT methods employ two steps: Object Detection and Data Association. The first step detects objects of interest in every frame of a video, and the second establishes correspondence between the detected objects in different frames to obtain their tracks. Object detection has made tremendous progress in the last few years due to deep learning. However, data association for tracking still relies on hand crafted constraints such as appearance, motion, spatial proximity, grouping etc. to compute affinities between the objects in different frames. In this paper, we harness the power of deep learning for data association in tracking by jointly modeling object appearances and their affinities between different frames in an end-to-end fashion. The proposed Deep Affinity Network (DAN) learns compact, yet comprehensive features of pre-detected objects at several levels of abstraction, and performs exhaustive pairing permutations of those features in any two frames to infer object affinities. DAN also accounts for multiple objects appearing and disappearing between video frames. We exploit the resulting efficient affinity computations to associate objects in the current frame deep into the previous frames for reliable on-line tracking. Our technique is evaluated on popular multiple object tracking challenges MOT15, MOT17 and UA-DETRAC. Comprehensive benchmarking under twelve evaluation metrics demonstrates that our approach is among the best performing techniques on the leader board for these challenges. The open source implementation of our work is available at https://github.com/shijieS/SST.git.},
  archive      = {J_TPAMI},
  author       = {ShiJie Sun and Naveed Akhtar and HuanSheng Song and Ajmal Mian and Mubarak Shah},
  doi          = {10.1109/TPAMI.2019.2929520},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {104-119},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Deep affinity network for multiple object tracking},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Community detection using restrained random-walk similarity.
<em>TPAMI</em>, <em>43</em>(1), 89–103. (<a
href="https://doi.org/10.1109/TPAMI.2019.2926033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a restrained random-walk similarity method for detecting the community structures of graphs. The basic premise of our method is that the starting vertices of finite-length random walks are judged to be in the same community if the walkers pass similar sets of vertices. This idea is based on our consideration that a random walker tends to move in the community including the walker&#39;s starting vertex for some time after starting the walk. Therefore, the sets of vertices passed by random walkers starting from vertices in the same community must be similar. The idea is reinforced with two conditions. First, we exclude abnormal random walks. Random walks that depart from each vertex are executed many times, and vertices that are rarely passed by the walkers are excluded from the set of vertices that the walkers may pass. Second, we forcibly restrain random walks to an appropriate length. In our method, a random walk is terminated when the walker repeatedly visits vertices that they have already passed. Experiments on real-world networks demonstrate that our method outperforms previous techniques in terms of accuracy.},
  archive      = {J_TPAMI},
  author       = {Makoto Okuda and Shin&#39;ichi Satoh and Yoichi Sato and Yutaka Kidawara},
  doi          = {10.1109/TPAMI.2019.2926033},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {89-103},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Community detection using restrained random-walk similarity},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Blind deblurring of barcodes via kullback-leibler
divergence. <em>TPAMI</em>, <em>43</em>(1), 77–88. (<a
href="https://doi.org/10.1109/TPAMI.2019.2927311">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Barcode encoding schemes impose symbolic constraints which fix certain segments of the image. We present, implement, and assess a method for blind deblurring and denoising based entirely on Kullback-Leibler divergence. The method is designed to incorporate and exploit the full strength of barcode symbologies. Via both standard barcode reading software and smartphone apps, we demonstrate the remarkable ability of our method to blindly recover simulated images of highly blurred and noisy barcodes. As proof of concept, we present one application on a real-life out of focus camera image.},
  archive      = {J_TPAMI},
  author       = {Gabriel Rioux and Christopher Scarvelis and Rustum Choksi and Tim Hoheisel and Pierre Maréchal},
  doi          = {10.1109/TPAMI.2019.2927311},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {77-88},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Blind deblurring of barcodes via kullback-leibler divergence},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bayesian low-tubal-rank robust tensor factorization with
multi-rank determination. <em>TPAMI</em>, <em>43</em>(1), 62–76. (<a
href="https://doi.org/10.1109/TPAMI.2019.2923240">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robust tensor factorization is a fundamental problem in machine learning and computer vision, which aims at decomposing tensors into low-rank and sparse components. However, existing methods either suffer from limited modeling power in preserving low-rank structures, or have difficulties in determining the target tensor rank and the trade-off between the low-rank and sparse components. To address these problems, we propose a fully Bayesian treatment of robust tensor factorization along with a generalized sparsity-inducing prior. By adapting the recently proposed low-tubal-rank model in a generative manner, our method is effective in preserving low-rank structures. Moreover, benefiting from the proposed prior and the Bayesian framework, the proposed method can automatically determine the tensor rank while inferring the trade-off between the low-rank and sparse components. For model estimation, we develop a variational inference algorithm, and further improve its efficiency by reformulating the variational updates in the frequency domain. Experimental results on both synthetic and real-world datasets demonstrate the effectiveness of the proposed method in multi-rank determination as well as its superiority in image denoising and background modeling over state-of-the-art approaches.},
  archive      = {J_TPAMI},
  author       = {Yang Zhou and Yiu-Ming Cheung},
  doi          = {10.1109/TPAMI.2019.2923240},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {62-76},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Bayesian low-tubal-rank robust tensor factorization with multi-rank determination},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A microfacet-based model for photometric stereo with general
isotropic reflectance. <em>TPAMI</em>, <em>43</em>(1), 48–61. (<a
href="https://doi.org/10.1109/TPAMI.2019.2927909">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a precise, stable, and invertible reflectance model for photometric stereo. This microfacet-based model is applicable to all types of isotropic surface reflectance, covering cases from diffusion to specular reflections. We introduce a single variable to physically quantify the surface smoothness, and by monotonically sliding this variable between 0 and 1, our model enables a versatile representation that can smoothly transform between an ellipsoid of revolution and the equation for Lambertian reflectance. In the inverse domain, this model offers a compact and physically interpretable formulation, for which we introduce a fast and lightweight solver that allows accurate estimations for both surface smoothness and surface shape. Finally, extensive experiments on the appearances of synthesized and real objects evidence that this model is state-of-the-art in our off-the-shelf solution.},
  archive      = {J_TPAMI},
  author       = {Lixiong Chen and Yinqiang Zheng and Boxin Shi and Art Subpa-asa and Imari Sato},
  doi          = {10.1109/TPAMI.2019.2927909},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {48-61},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {A microfacet-based model for photometric stereo with general isotropic reflectance},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A general decoupled learning framework for parameterized
image operators. <em>TPAMI</em>, <em>43</em>(1), 33–47. (<a
href="https://doi.org/10.1109/TPAMI.2019.2925793">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many different deep networks have been used to approximate, accelerate or improve traditional image operators. Among these traditional operators, many contain parameters which need to be tweaked to obtain the satisfactory results, which we refer to as “parameterized image operators”. However, most existing deep networks trained for these operators are only designed for one specific parameter configuration, which does not meet the needs of real scenarios that usually require flexible parameters settings. To overcome this limitation, we propose a new decoupled learning algorithm to learn from the operator parameters to dynamically adjust the weights of a deep network for image operators, denoted as the base network. The learned algorithm is formed as another network, namely the weight learning network, which can be end-to-end jointly trained with the base network. Experiments demonstrate that the proposed framework can be successfully applied to many traditional parameterized image operators. To accelerate the parameter tuning for practical scenarios, the proposed framework can be further extended to dynamically change the weights of only one single layer of the base network while sharing most computation cost. We demonstrate that this cheap parameter-tuning extension of the proposed decoupled learning framework even outperforms the state-of-the-art alternative approaches.},
  archive      = {J_TPAMI},
  author       = {Qingnan Fan and Dongdong Chen and Lu Yuan and Gang Hua and Nenghai Yu and Baoquan Chen},
  doi          = {10.1109/TPAMI.2019.2925793},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {33-47},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {A general decoupled learning framework for parameterized image operators},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A framework of composite functional gradient methods for
generative adversarial models. <em>TPAMI</em>, <em>43</em>(1), 17–32.
(<a href="https://doi.org/10.1109/TPAMI.2019.2924428">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative adversarial networks (GAN) are trained through a minimax game between a generator and a discriminator to generate data that mimics observations. While being widely used, GAN training is known to be empirically unstable. This paper presents a new theory for generative adversarial methods that does not rely on the traditional minimax formulation. Our theory shows that with a strong discriminator, a good generator can be obtained by composite functional gradient learning, so that several distance measures (including the KL divergence and the JS divergence) between the probability distributions of real data and generated data are simultaneously improved after each functional gradient step until converging to zero. This new point of view leads to stable procedures for training generative models. It also gives a new theoretical insight into the original GAN. Empirical results on image generation show the effectiveness of our new method.},
  archive      = {J_TPAMI},
  author       = {Rie Johnson and Tong Zhang},
  doi          = {10.1109/TPAMI.2019.2924428},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {17-32},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {A framework of composite functional gradient methods for generative adversarial models},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). 3D rigid motion segmentation with mixed and unknown number
of models. <em>TPAMI</em>, <em>43</em>(1), 1–16. (<a
href="https://doi.org/10.1109/TPAMI.2019.2929146">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many real-world video sequences cannot be conveniently categorized as general or degenerate; in such cases, imposing a false dichotomy in using the fundamental matrix or homography model for motion segmentation on video sequences would lead to difficulty. Even when we are confronted with a general scene-motion, the fundamental matrix approach as a model for motion segmentation still suffers from several defects, which we discuss in this paper. The full potential of the fundamental matrix approach could only be realized if we judiciously harness information from the simpler homography model. From these considerations, we propose a multi-model spectral clustering framework that synergistically combines multiple models (homography and fundamental matrix) together. We show that the performance can be substantially improved in this way. For general motion segmentation tasks, the number of independently moving objects is often unknown a priori and needs to be estimated from the observations. This is referred to as model selection and it is essentially still an open research problem. In this work, we propose a set of model selection criteria balancing data fidelity and model complexity. We perform extensive testing on existing motion segmentation datasets with both segmentation and model selection tasks, achieving state-of-the-art performance on all of them; we also put forth a more realistic and challenging dataset adapted from the KITTI benchmark, containing real-world effects such as strong perspectives and strong forward translations not seen in the traditional datasets.},
  archive      = {J_TPAMI},
  author       = {Xun Xu and Loong-Fah Cheong and Zhuwen Li},
  doi          = {10.1109/TPAMI.2019.2929146},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number       = {1},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {3D rigid motion segmentation with mixed and unknown number of models},
  volume       = {43},
  year         = {2021},
}
</textarea>
</details></li>
</ul>

</body>
</html>
