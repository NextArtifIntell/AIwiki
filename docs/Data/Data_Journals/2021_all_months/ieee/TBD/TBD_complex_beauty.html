<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TBD_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tbd---80">TBD - 80</h2>
<ul>
<li><details>
<summary>
(2021). On security of an identity-based dynamic data auditing
protocol for big data storage. <em>IEEE Transactions on Big Data</em>,
<em>7</em>(6), 975–977. (<a
href="https://doi.org/10.1109/TBDATA.2020.3026318">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In this article, we point out the security weakness of Shang et al. ’s identity-based dynamic data auditing protocol for big data storage. Specifically, we identify that their protocol is vulnerable to a secret key reveal attack, i.e., the service provider (SP) can reveal the secret key of the data owner (DO) from the stored data. Further, SP can also generate a proof to pass the challenge of TPA (third party auditor) even if all block and tag pairs have been deleted. We hope that by identifying these design flaws, similar weaknesses can be avoided in future designs.},
  archive  = {J},
  author   = {Xiong Li and Shanpeng Liu and Rongxing Lu and Xiaosong Zhang},
  doi      = {10.1109/TBDATA.2020.3026318},
  journal  = {IEEE Transactions on Big Data},
  month    = {12},
  number   = {6},
  pages    = {975-977},
  title    = {On security of an identity-based dynamic data auditing protocol for big data storage},
  volume   = {7},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Comment on “efficient secure outsourcing of large-scale
sparse linear systems of equations.” <em>IEEE Transactions on Big
Data</em>, <em>7</em>(6), 973–974. (<a
href="https://doi.org/10.1109/TBDATA.2020.2995200">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The scheme [IEEE Trans. Big Data, 4 (1), 2018, 26-39] is flawed because the sparsity of coefficient matrix is neglected. In the discussed scenario, we find it is unnecessary to outsource such a problem because the client can solve it locally. Even if the matrix is not sparse, the proposed paradigm which requires a great number of interactions, is rarely adopted because of the possibly incurred massive communication cost.},
  archive  = {J},
  author   = {Zhengjun Cao and Olivier Markowitch},
  doi      = {10.1109/TBDATA.2020.2995200},
  journal  = {IEEE Transactions on Big Data},
  month    = {12},
  number   = {6},
  pages    = {973-974},
  title    = {Comment on “Efficient secure outsourcing of large-scale sparse linear systems of equations”},
  volume   = {7},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Regularized spectral clustering with entropy perturbation.
<em>IEEE Transactions on Big Data</em>, <em>7</em>(6), 967–972. (<a
href="https://doi.org/10.1109/TBDATA.2020.3039036">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Spectral clustering is a popular clustering method because it gives a natural way to reduce the dimensionality of data using eigenvectors. It is well known that the performance of spectral clustering could be improved via regularization. Nevertheless, it is hard to cope with the different cases by only one constant regularization parameter. To solve such a problem, a novel regularized spectral clustering method is proposed. Specifically, two modules are integrated in the proposed method. First, under matrix perturbation analysis, we prove that the entropy can be used as a rank score function to reveal the informative eigenvector, and the eigenvector corresponding to the minimal entropy will be the regularization to regularize the data matrix instead of a constant regularization parameter. Second, in order to ensure the perturbation on eigenspace is within the effective range, a perturbation boundary on eigenvectors is given. Numerical results showed that our proposal has superior performance than spectral clustering and k-means algorithm.},
  archive  = {J},
  author   = {Xiao Hu and Hui Zhang and ChunMing Yang and XuJian Zhao and Bo Li},
  doi      = {10.1109/TBDATA.2020.3039036},
  journal  = {IEEE Transactions on Big Data},
  month    = {12},
  number   = {6},
  pages    = {967-972},
  title    = {Regularized spectral clustering with entropy perturbation},
  volume   = {7},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Linear time community detection by a novel modularity gain
acceleration in label propagation. <em>IEEE Transactions on Big
Data</em>, <em>7</em>(6), 961–966. (<a
href="https://doi.org/10.1109/TBDATA.2020.2995621">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Community detection is an important problem in complex network analysis. Among numerous approaches for community detection, label propagation (LP) has attracted a lot of attention. LP selects the optimum community (i.e., label) of a network vertex by optimizing an objective function (e.g., Newman’s modularity) subject to the available labels in the vicinity of the vertex. In this article, a novel analysis of Newman’s modularity gain with respect to label transitions in graphs is presented. Here, we propose a new form of Newman’s modularity gain calculation that quantifies available label transitions for any LP based community detection. The proposed approach is called Modularity Gain Acceleration (MGA) and is simplified and divided into two components, the local and global sum-weights. The Local Sum-Weight (LSW) is the component with lower complexity and is calculated for each candidate label transition. The General Sum-Weight (GSW) is more computationally complex, and is calculated only once per each label. GSW is updated by leveraging a simple process for each node-label transition, instead of for all available labels. The proposed technique is applied to selected state-of-the-art LP-based community detection methods and the resulting network modularity and execution time are compared with traditional methods over small to big real world data sets. By applying MGA to LP-based methods, the run-time is significantly reduced–sometimes finishing before the traditional approach even finishes one iteration–achieving the same modularity performance and number of communities, i.e., community detection result. The MGA approach leads to significant efficiency improvements by reducing time consumption up to 85 percent relative to the original algorithms with the exact same quality in terms of modularity value which is highly valuable in analyses of big data sets.},
  archive  = {J},
  author   = {Sakineh Yazdanparast and Mohsen Jamalabdollahi and Timothy C. Havens},
  doi      = {10.1109/TBDATA.2020.2995621},
  journal  = {IEEE Transactions on Big Data},
  month    = {12},
  number   = {6},
  pages    = {961-966},
  title    = {Linear time community detection by a novel modularity gain acceleration in label propagation},
  volume   = {7},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sense and sensibility: Characterizing social media users
regarding the use of controversial terms for COVID-19. <em>IEEE
Transactions on Big Data</em>, <em>7</em>(6), 952–960. (<a
href="https://doi.org/10.1109/TBDATA.2020.2996401">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {With the world-wide development of 2019 novel coronavirus, although WHO has officially announced the disease as COVID-19, one controversial term - “Chinese Virus” is still being used by a great number of people. In the meantime, global online media coverage about COVID-19-related racial attacks increases steadily, most of which are anti-Chinese or anti-Asian. As this pandemic becomes increasingly severe, more people start to talk about it on social media platforms such as Twitter. When they refer to COVID-19, there are mainly two ways: using controversial terms like “Chinese Virus” or “Wuhan Virus”, or using non-controversial terms like “Coronavirus”. In this article, we attempt to characterize the Twitter users who use controversial terms and those who use non-controversial terms. We use the Tweepy API to retrieve 17 million related tweets and the information of their authors. We find the significant differences between these two groups of Twitter users across their demographics, user-level features like the number of followers, political following status, as well as their geo-locations. Moreover, we apply classification models to predict Twitter users who are more likely to use controversial terms. To our best knowledge, this is the first large-scale social media-based study to characterize users with respect to their usage of controversial terms during a major crisis.},
  archive  = {J},
  author   = {Hanjia Lyu and Long Chen and Yu Wang and Jiebo Luo},
  doi      = {10.1109/TBDATA.2020.2996401},
  journal  = {IEEE Transactions on Big Data},
  month    = {12},
  number   = {6},
  pages    = {952-960},
  title    = {Sense and sensibility: Characterizing social media users regarding the use of controversial terms for COVID-19},
  volume   = {7},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DIGDUG: Scalable separable dense graph pruning and join
operations in MapReduce. <em>IEEE Transactions on Big Data</em>,
<em>7</em>(6), 930–951. (<a
href="https://doi.org/10.1109/TBDATA.2020.2983650">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Linking topics to specific experts in technical documents and finding connections between experts are crucial for detecting the evolution of emerging topics and the relationships between their influencers in state-of-the-art research. Current techniques that make such connections are limited to similarity measures. Methods based on weights such as TF-IDF and frequency to identify important topics and self joins between topics and experts are generally utilized to identify connections between experts. However, such approaches are inadequate for identifying emerging keywords and experts since the most useful terms in technical documents tend to be infrequent and concentrated in just a few documents. This makes connecting experts through joins on large dense graphs challenging. In this article, we present DIGDUG, a framework that identifies emerging topics by applying graph operations to technical terms. The framework identifies connections between authors of patents and journal papers by performing joins on connected topics and topics associated with the authors at scale. The problem of scaling the graph operations for topics and experts is solved through dense graph pruning and graph joins categorized under their own scalable separable dense graph class. Experiments were performed on technical domains to validate the utility of the connections between interests and experts. Comparing our graph join and pruning technique against multiple graph and join methods in MapReduce revealed a significant improvement in performance using our approach.},
  archive  = {J},
  author   = {Manu Shukla and Dinesh Dharme and Pallavi Ramnarain and Ray Dos Santos and Chang-Tien Lu},
  doi      = {10.1109/TBDATA.2020.2983650},
  journal  = {IEEE Transactions on Big Data},
  month    = {12},
  number   = {6},
  pages    = {930-951},
  title    = {DIGDUG: Scalable separable dense graph pruning and join operations in MapReduce},
  volume   = {7},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Heterogeneous daily living activity learning through domain
invariant feature subspace. <em>IEEE Transactions on Big Data</em>,
<em>7</em>(6), 922–929. (<a
href="https://doi.org/10.1109/TBDATA.2020.2977626">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In the practical applications of supervised learning methods, the high cost of obtaining labeled data for learning tasks is a critical problem. One promising research area for solving the problem is transfer learning, which aims to learn a task in target domain by utilizing the training data in a different but related source domain. In this article, we propose a novel heterogeneous transfer learning algorithm called Heterogeneous Daily Living Activity Learning (HDLAL) which derives domain invariant feature representation space from cross-domain data distributions by projecting both domain data into the derived space in the close proximity of each other using Maximum Mean Discrepancy. Within the new feature space, we utilize ensemble classification algorithm to train multi-label classifier using the projected data to predict the labels in the target domain. We show the effectiveness of our approach by experimenting on real-world smart home datasets. The results shows that our HDLAL algorithm outperforms the common direct learning approaches in the context of predicting labels of activities of daily living (ADL).},
  archive  = {J},
  author   = {Batsergelen Myagmar and Jie Li and Shigetomo Kimura},
  doi      = {10.1109/TBDATA.2020.2977626},
  journal  = {IEEE Transactions on Big Data},
  month    = {12},
  number   = {6},
  pages    = {922-929},
  title    = {Heterogeneous daily living activity learning through domain invariant feature subspace},
  volume   = {7},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Identity-based dynamic data auditing for big data storage.
<em>IEEE Transactions on Big Data</em>, <em>7</em>(6), 913–921. (<a
href="https://doi.org/10.1109/TBDATA.2019.2941882">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Identity-based remote data auditing schemes can verify data integrity and provide a simple identity authentication and management for multiple users. However, prior works on identity-based remote data auditing lack the support of dynamic operations. In these schemes, tag generation is linked to the index of data block, which is related to update operations such as modification, insertion and deletion. If users perform dynamic operations on a data block, the tags of all subsequent blocks need to be modified. It means that if users want to update data on a big data platform, they have to download the whole file, update the file and send the updated file to the big data platform. Such pattern will bring huge communication overhead. In this paper, we propose an identity-based dynamic data auditing scheme which supports dynamic data operations, including modification, insertion and deletion. As far as we know, there is still no other identity-based data auditing scheme that supports dynamic operations. In particular, to achieve efficient dynamic operations, we use the data structure of Merkle hash tree for block tag authentication, which helps update data with integrity assurance. Analyses of security and performance show that the proposed scheme is efficient and secure.},
  archive  = {J},
  author   = {Tao Shang and Feng Zhang and Xingyue Chen and Jianwei Liu and Xinxi Lu},
  doi      = {10.1109/TBDATA.2019.2941882},
  journal  = {IEEE Transactions on Big Data},
  month    = {12},
  number   = {6},
  pages    = {913-921},
  title    = {Identity-based dynamic data auditing for big data storage},
  volume   = {7},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Citywide traffic volume inference with surveillance camera
records. <em>IEEE Transactions on Big Data</em>, <em>7</em>(6), 900–912.
(<a href="https://doi.org/10.1109/TBDATA.2019.2935057">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Real-time traffic monitoring becomes an essential part of an intelligent city. In recent years, the adoption of surveillance cameras is rapidly growing because they are helpful to manage and control the traffic. However, it is impossible to install cameras on every road in a city due to the high costs of deployment and maintenance. Given the information from limited surveillance cameras, can we infer the citywide traffic volume accurately? This is a challenging question because we have no historical data on the roads without cameras. It requires us to design a method that goes beyond the inference using nearby traffic data. Moreover, a nice property of surveillance camera data is that these AI-equipped cameras can recognize individual vehicles. So we can recover incomplete trajectories for vehicles using plate numbers in surveillance camera records. However, for road segments without cameras, we do not know whether those vehicles pass through them or not. How can such incomplete trajectories be effectively used to help citywide traffic inference? In this paper, we propose a framework named CityVolInf to infer citywide traffic volume based on surveillance camera records. Our framework combines a semi-supervised learning-based similarity module with a novel simulation module to address the above challenges. While the similarity module focuses on spatiotemporal correlations of traffic volume between road segments, the simulation module utilizes incomplete trajectories to model transitions of traffic volume between adjacent road segments. Our framework bridges the conventional data-driven approach and transportation domain knowledge from the simulator. We conduct extensive experiments on a real-world dataset, containing 405,370,631 camera records collected from 1,704 surveillance cameras over a period of 31 days in Jinan, China. The experimental results demonstrate the effectiveness of CityVolInf compared with existing methods.},
  archive  = {J},
  author   = {Yanwei Yu and Xianfeng Tang and Huaxiu Yao and Xiuwen Yi and Zhenhui Li},
  doi      = {10.1109/TBDATA.2019.2935057},
  journal  = {IEEE Transactions on Big Data},
  month    = {12},
  number   = {6},
  pages    = {900-912},
  title    = {Citywide traffic volume inference with surveillance camera records},
  volume   = {7},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Privacy-preserving public release of datasets for support
vector machine classification. <em>IEEE Transactions on Big Data</em>,
<em>7</em>(5), 893–899. (<a
href="https://doi.org/10.1109/TBDATA.2019.2963391">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We consider the problem of publicly releasing a dataset for support vector machine classification while not infringing on the privacy of data subjects (i.e., individuals whose private information is stored in the dataset). The dataset is systematically obfuscated using an additive noise for privacy protection. Motivated by the Cramér-Rao bound, inverse of the trace of the Fisher information matrix is used as a measure of the privacy. Conditions are established for ensuring that the classifier extracted from the original dataset and the obfuscated one are close to each other (capturing the utility). The optimal noise distribution is determined by maximizing a weighted sum of the measures of privacy and utility. The optimal privacy-preserving noise is proved to achieve local differential privacy. The results are generalized to a broader class of optimization-based supervised machine learning algorithms. Applicability of the methodology is demonstrated on multiple datasets.},
  archive  = {J},
  author   = {Farhad Farokhi},
  doi      = {10.1109/TBDATA.2019.2963391},
  journal  = {IEEE Transactions on Big Data},
  month    = {11},
  number   = {5},
  pages    = {893-899},
  title    = {Privacy-preserving public release of datasets for support vector machine classification},
  volume   = {7},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Grasping inter-attribute and temporal variability in
multivariate time series. <em>IEEE Transactions on Big Data</em>,
<em>7</em>(5), 885–892. (<a
href="https://doi.org/10.1109/TBDATA.2019.2918807">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The rising capabilities of storing and registering data has increased the number of temporal datasets, boosting the attention on time series classification and forecasting. In case of multivariate time series, symbolic methods that try to predict phenomena transform the data into a more compact format to produce a representation of the time series easy to be handled in a machine learning framework. However, up to now these representations do not grasp information on both inter-attribute variability and temporal variability. In this work we present an approach that, taking into account the relationships between attributes and their periodicity, reduces the multivariate time series to a collection of symbols, whose distribution is represented by histograms. The approach has been successfully tested on a publicly available dataset, the Telecom Italia Big Data Challenge 2014 dataset, reporting also the results attained by other methods available in the literature.},
  archive  = {J},
  author   = {Paolo Soda and Rosa Sicilia and Ludovica Acciai and Giulio Iannello},
  doi      = {10.1109/TBDATA.2019.2918807},
  journal  = {IEEE Transactions on Big Data},
  month    = {11},
  number   = {5},
  pages    = {885-892},
  title    = {Grasping inter-attribute and temporal variability in multivariate time series},
  volume   = {7},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Discovering and understanding geographical video viewing
patterns in urban neighborhoods. <em>IEEE Transactions on Big Data</em>,
<em>7</em>(5), 873–884. (<a
href="https://doi.org/10.1109/TBDATA.2021.3055860">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Video accounts for a large proportion of traffic on the Internet. Understanding its geographical viewing patterns is extremely valuable for the design of Internet ecosystems for content delivery, recommendation and ads. While previous works have addressed this problem at coarse-grain scales (e.g., national), the urban-scale geographical patterns of video access have never been revealed. To this end, this article aims to investigate the problem that whether there exists distinct viewing patterns among the neighborhoods of a large-scale city. To achieve this, we need to address several challenges including unknown of patterns profiles, complicate urban neighborhoods, and comprehensive viewing features. The contributions of this article include two aspects. First, we design a framework to automatically identify geographical video viewing patterns in urban neighborhoods. Second, by using a dataset of two months real video requests in Shanghai collected from one major ISP of China, we make a rigorous analysis of video viewing patterns in Shanghai. Our study reveals the following important observations. First, there exists four prevalent and distinct patterns of video access behavior in urban neighborhoods, which are corresponding to four different geographical contexts: downtown residential, office, suburb residential and hybrid regions. Second, there exists significant features that distinguish different patterns, e.g., the probabilities of viewing TV plays at midnight, and viewing cartoons at weekends can distinguish the two viewing patterns corresponding to downtown and suburb regions.},
  archive  = {J},
  author   = {Jiaqiang Liu and Huan Yan and Yong Li and Dmytro Karamshuk and Nishanth Sastry and Di Wu and Depeng Jin},
  doi      = {10.1109/TBDATA.2021.3055860},
  journal  = {IEEE Transactions on Big Data},
  month    = {11},
  number   = {5},
  pages    = {873-884},
  title    = {Discovering and understanding geographical video viewing patterns in urban neighborhoods},
  volume   = {7},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sparse user check-in venue prediction by exploring latent
decision contexts from location-based social networks. <em>IEEE
Transactions on Big Data</em>, <em>7</em>(5), 859–872. (<a
href="https://doi.org/10.1109/TBDATA.2019.2957118">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The proliferation of online Location-Based Social Networks (LBSN) has offered unprecedented opportunities for understanding fine-grained spatio-temporal behaviors of users and developing new location-aware applications. In this article, we focus on the problem of “Sparse User Check-in Venue Prediction,” where the goal is to predict the next venue LBSN users will visit by exploiting their sparse online check-in traces and the latent decision contexts. While efforts have been made to predict users’ check-in traces on a LBSN, several important challenges still exist. First, check-in traces contributed by LBSN users are often too sparse to provide sufficient evidence for a reliable prediction, especially when the prediction space is huge (e.g., hundreds of thousands of venues in large cities). Second, the user&#39;s decision context on which venue to visit next is often latent and has not been incorporated by current venue prediction models. Third, the dynamic and non-deterministic dependency between check-ins is either ignored or replaced by a simplified “consecutiveness” assumption in existing solutions, leading to sub-optimal prediction results. In this article, we develop a Context-aware Sparse Check-in Venue Prediction (CSCVP) scheme inspired by natural language processing techniques to address the above challenges. In particular, CSCVP predicts the venue category information and explores the similarity between users to address data sparsity challenge by significantly reducing the prediction space. It also leverages the Probabilistic Latent Semantic Analysis (PLSA) model to incorporate the user decision context into the prediction model. Finally, we develop a novel Temporal Adaptive Ngram (TA-Ngram) model in CSCVP to capture the dynamic and non-deterministic dependency between check-ins. We evaluate CSCVP using three real-world LBSN datasets. The results show that our scheme significantly improves accuracy (30.9 percent improvement) of the state-of-the-art user check-in venue prediction solutions.},
  archive  = {J},
  author   = {Daniel Zhang and Yang Zhang and Qi Li and Dong Wang},
  doi      = {10.1109/TBDATA.2019.2957118},
  journal  = {IEEE Transactions on Big Data},
  month    = {11},
  number   = {5},
  pages    = {859-872},
  title    = {Sparse user check-in venue prediction by exploring latent decision contexts from location-based social networks},
  volume   = {7},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Leakage resilient leveled &lt;inline-formula&gt;&lt;tex-math
notation=“LaTeX”&gt;<span
class="math inline">FHE</span>&lt;/tex-math&gt;&lt;mml:math
xmlns:mml=“http://www.w3.org/1998/math/MathML”&gt;&lt;mml:mi
mathvariant=“sans-serif”&gt;FHE&lt;/mml:mi&gt;&lt;/mml:math&gt;&lt;inline-graphic
xlink:href=“wang-ieq1-2726554.gif”
xmlns:xlink=“http://www.w3.org/1999/xlink”/&gt;&lt;/inline-formula&gt;
on multiple bits message. <em>IEEE Transactions on Big Data</em>,
<em>7</em>(5), 845–858. (<a
href="https://doi.org/10.1109/TBDATA.2017.2726554">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Fully Homomorphic Encryption ( &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$\mathsf {FHE}$&lt;/tex-math&gt;&lt;/inline-formula&gt; ) allows computing over encrypted data without decrypting the corresponding ciphertexts, and it constitutes a promising cryptographic primitive to preserve data privacy in the big data computing environments. In general, &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$\mathsf {FHE}$&lt;/tex-math&gt;&lt;/inline-formula&gt; schemes can be constructed by using the standard Learning with Errors ( &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$\mathsf {LWE}$&lt;/tex-math&gt;&lt;/inline-formula&gt; ) assumption, and the current crux lies in how to achieve efficient multi-bit &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$\mathsf {FHE}$&lt;/tex-math&gt;&lt;/inline-formula&gt; encryption while being leakage-resistent against attackers who may capture the information of cryptographic secret keys via side channel attacks. Based on Berkoff-Liu’s work at TCC’14, we aim to address this issue by giving a new structure of public key matrix with any number of &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$\mathsf {LWE}$&lt;/tex-math&gt;&lt;/inline-formula&gt; instances, thereby avoiding the use of a straightforward composition to achieve multi-bit &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$\mathsf {FHE}$&lt;/tex-math&gt;&lt;/inline-formula&gt; encryption under standard &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$\mathsf {LWE}$&lt;/tex-math&gt;&lt;/inline-formula&gt; . Particularly, our scheme attains provable security.},
  archive  = {J},
  author   = {Zengpeng Li and Chunguang Ma and Ding Wang},
  doi      = {10.1109/TBDATA.2017.2726554},
  journal  = {IEEE Transactions on Big Data},
  month    = {11},
  number   = {5},
  pages    = {845-858},
  title    = {Leakage resilient leveled &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$\mathsf {FHE}$&lt;/tex-math&gt;&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mml:mi mathvariant=&quot;sans-serif&quot;&gt;FHE&lt;/mml:mi&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=&quot;wang-ieq1-2726554.gif&quot; xmlns:xlink=&quot;http://www.w3.org/1999/xlink&quot;/&gt;&lt;/inline-formula&gt; on multiple bits message},
  volume   = {7},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Distributed sparse class-imbalance learning and its
applications. <em>IEEE Transactions on Big Data</em>, <em>7</em>(5),
832–844. (<a href="https://doi.org/10.1109/TBDATA.2017.2688372">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In the present work, the study on class imbalance problems in a distributed setting exploiting sparsity structure in the data has been carried out. We formulate the class-imbalance learning problem as a cost-sensitive learning problem with &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$L_1$&lt;/tex-math&gt;&lt;/inline-formula&gt; regularization. The cost-sensitive loss function is a cost-weighted smooth hinge loss. The resultant optimization problem is minimized within the Distributed Alternating Direction Method of Multiplier (DADMM) framework. We partition the data matrix across samples. This operation splits the original problem into a distributed &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$L_2$&lt;/tex-math&gt;&lt;/inline-formula&gt; regularized smooth loss minimization and a &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$L_1$&lt;/tex-math&gt;&lt;/inline-formula&gt; regularized squared loss minimization. &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$L_2$&lt;/tex-math&gt;&lt;/inline-formula&gt; regularized subproblem is solved via Limited-memory Broyden-Fletcher-Goldfarb-Shanno (L-BFGS) and random coordinate descent method in parallel at multiple processing nodes using MPI whereas &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$L_1$&lt;/tex-math&gt;&lt;/inline-formula&gt; regularized problem is just a simple soft-thresholding operation. We show, empirically, that the distributed solution approximates the centralized solution on many benchmark data sets. The centralized solution is obtained via Cost-Sensitive Stochastic Coordinate Descent (CSSCD). Empirical results on small and large-scale benchmark datasets show some promising avenues to further investigate the real-world applications of the proposed algorithms such as anomaly detection, class-imbalance learning, etc. To the best of our knowledge, ours is the first work to study class-imbalance in a distributed environment on large-scale sparse data.},
  archive  = {J},
  author   = {Chandresh Kumar Maurya and Durga Toshniwal and Gopalan Vijendran Venkoparao},
  doi      = {10.1109/TBDATA.2017.2688372},
  journal  = {IEEE Transactions on Big Data},
  month    = {11},
  number   = {5},
  pages    = {832-844},
  title    = {Distributed sparse class-imbalance learning and its applications},
  volume   = {7},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An adaptive pattern learning framework to personalize online
seizure prediction. <em>IEEE Transactions on Big Data</em>,
<em>7</em>(5), 819–831. (<a
href="https://doi.org/10.1109/TBDATA.2017.2675982">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The sudden and spontaneous occurrence of epileptic seizures can impose a significant burden on patients with epilepsy. If seizure onset can be prospectively predicted, it could greatly improve the life of patients with epilepsy and also open new therapeutic avenues for epilepsy treatment. However, discovering effective predictive patterns from massive brainwave signals is still a challenging problem. The prediction of epileptic seizures is still in its early stage. Most existing studies actually investigated the predictability of seizures offline instead of a truly prospective online prediction, and also the high inter-individual variability was not fully considered in prediction. In this study, we propose a novel adaptive pattern learning framework with a new online feature extraction approach to achieve personalized online prospective seizure prediction. In particular, a two-level online feature extraction approach is applied to monitor intracranial electroencephalogram (EEG) signals and construct a pattern library incrementally. Three prediction rules were developed and evaluated based on the continuously-updated patient-specific pattern library for each patient, including the adaptive probabilistic prediction (APP), adaptive linear-discriminant-analysis-based prediction (ALP), and adaptive Naive Bayes-based prediction (ANBP). The proposed online pattern learning and prediction system achieved impressive prediction results for 10 patients with epilepsy using long-term EEG recordings. The best testing prediction accuracy averaged over the 10 patients were 79, 78, and 82 percent for the APP, ALP, and ANBP prediction scheme, respectively. The experimental results confirmed that the proposed adaptive prediction framework offers a promising practical tool to solve the challenging seizure prediction problem.},
  archive  = {J},
  author   = {Cao Xiao and Shouyi Wang and Leon Iasemidis and Stephen Wong and Wanpracha Art Chaovalitwongse},
  doi      = {10.1109/TBDATA.2017.2675982},
  journal  = {IEEE Transactions on Big Data},
  month    = {11},
  number   = {5},
  pages    = {819-831},
  title    = {An adaptive pattern learning framework to personalize online seizure prediction},
  volume   = {7},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An improved secure high-order-lanczos based orthogonal
tensor SVD for outsourced cyber-physical-social big data reduction.
<em>IEEE Transactions on Big Data</em>, <em>7</em>(4), 808–818. (<a
href="https://doi.org/10.1109/TBDATA.2018.2881441">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Cyber-physical-social big data concern heterogeneous, multiaspect, large-volume data generated in cyber-physical-social systems (CPSS). Orthogonal tensor SVD (OTSVD) has emerged as a powerful tool to reduce cyber-physical-social big data. In this work, we propose an improved secure high-order-Lanczos based OTSVD for cyber-physical-social big data reduction in clouds. Specifically, to take advantage of the parallel processing capability of cloud computing, the improved secure high-order Lanczos algorithm is derived by restructuring the original high-order Lanczos algorithm such that only one synchronization point per iteration is required. To protect data privacy, the improved secure high-order-Lanczos based OTSVD employs homomorphic encryption integrated with batching technique, and garbled circuits, and makes all computations of the OTSVD algorithm in clouds come true. To our knowledge, this is the first study to efficiently tackle big data reduction in clouds in a privacy-preserving manner. Finally, we prove that our improved approach is secure in semi-trusted model. And we evaluate the proposed improved secure OTSVD on real datasets. The results show that our proposed improved secure approach is efficient and scalable for cyber-physical-social big data reduction.},
  archive  = {J},
  author   = {Jun Feng and Laurence T. Yang and Guohui Dai and Jinjun Chen and Zheng Yan},
  doi      = {10.1109/TBDATA.2018.2881441},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  number   = {4},
  pages    = {808-818},
  title    = {An improved secure high-order-lanczos based orthogonal tensor SVD for outsourced cyber-physical-social big data reduction},
  volume   = {7},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SecFact: Secure large-scale QR and LU factorizations.
<em>IEEE Transactions on Big Data</em>, <em>7</em>(4), 796–807. (<a
href="https://doi.org/10.1109/TBDATA.2017.2782809">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We are now in the big data era. Due to the emerging various systems and applications, such as the Internet of Things, cyber-physical systems, smart cities, smart healthcare, we are able to collect more data than ever before. On the other hand, it makes it very difficult to analyze such massive data in order to advance our science and engineering fields. We note that QR and LU factorizations are two of the most fundamental mathematical tools for data analysis. However, conducting QR or LU factorization of an m×n matrix requires computational complexity of O (m 2 n). This incurs a formidable challenge in efficiently analyzing large-scale data sets by normal users or small companies on traditional resource-limited computers. To overcome this limitation, industry and academia propose to employ cloud computing that can offer abundant computing resources. This, however, obviously raises security concerns and hence a lot of users are reluctant to reveal their data to the cloud. To this end, we propose two secure outsourcing algorithms for efficiently performing large-scale QR and LU factorizations, respectively. We implement the proposed algorithms on the Amazon Elastic Compute Cloud (EC2) platform and a laptop. The experiment results show significant time saving for the user.},
  archive  = {J},
  author   = {Changqing Luo and Kaijin Zhang and Sergio Salinas and Pan Li},
  doi      = {10.1109/TBDATA.2017.2782809},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  number   = {4},
  pages    = {796-807},
  title    = {SecFact: Secure large-scale QR and LU factorizations},
  volume   = {7},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Correlated differential privacy protection for mobile
crowdsensing. <em>IEEE Transactions on Big Data</em>, <em>7</em>(4),
784–795. (<a href="https://doi.org/10.1109/TBDATA.2017.2777862">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Mobile CrowdSensing (MCS) is a new paradigm that leverages pervasive mobile devices to efficiently collect the big sensory data, enabling various large-scale applications. However, people&#39;s concerns about the loss of individual privacy seriously hinder the prevalence of MCS applications. Differential privacy is widely focused owing to its rigorous definition and strong privacy guarantee, but the state-of-the-art studies still demonstrate its weakness on correlated data, resulting in compromising individual privacy. In this paper, we investigate the influence of sensing data correlation on differential privacy protection for MCS systems, and explore the perturbation mechanisms from two different perspectives. From a protector&#39;s perspective, based on the Bayesian Network to model the probabilistic relationship among sensing data, we use the classical definition of differential privacy to deduce the scale parameter, and present one perturbation mechanism. From an adversary&#39;s perspective, based on the Gaussian correlation model to describe the data correlation, we analyze the importance of the maximum correlated group to compute the Bayesian differential privacy leakage, and then provide another perturbation mechanism. Compared with the existing solutions, our mechanisms are applicable to arbitrary aggregate query function, and can avoid introducing too much noise. Moreover, we demonstrate the effectiveness of our mechanisms through extensive simulations.},
  archive  = {J},
  author   = {Jianwei Chen and Huadong Ma and Dong Zhao and Liang Liu},
  doi      = {10.1109/TBDATA.2017.2777862},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  number   = {4},
  pages    = {784-795},
  title    = {Correlated differential privacy protection for mobile crowdsensing},
  volume   = {7},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Privacy-accuracy trade-off in differentially-private
distributed classification: A game theoretical approach. <em>IEEE
Transactions on Big Data</em>, <em>7</em>(4), 770–783. (<a
href="https://doi.org/10.1109/TBDATA.2017.2777968">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Nowadays the privacy issue arising in data mining applications has attracted much attention. In the context of distributed data mining, a major concern of the participant is that its privacy may be disclosed to other participants or a third party. To protect privacy, one can apply a differential privacy approach to perturb the data before sharing them with others, which generally causes a negative effect on the mining result. Thus there is a trade-off between privacy and the mining result. In this paper, we study a distributed classification scenario where a mediator builds a classifier based on the perturbed query results returned by a number of users. We propose a game theoretical approach to analyze how users choose their privacy budgets. Specifically, interactions among users are modeled as a game in satisfaction form. And an algorithm is proposed for users to learn the satisfaction equilibrium (SE) of the game. Experimental results demonstrate that, when the differences among users&#39; expectations are not significant, the proposed learning algorithm can converge to an SE, at which every user achieves a balance between the accuracy of the classifier and the preserved privacy.},
  archive  = {J},
  author   = {Lei Xu and Chunxiao Jiang and Yi Qian and Jianhua Li and Youjian Zhao and Yong Ren},
  doi      = {10.1109/TBDATA.2017.2777968},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  number   = {4},
  pages    = {770-783},
  title    = {Privacy-accuracy trade-off in differentially-private distributed classification: A game theoretical approach},
  volume   = {7},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Enhanced locality-sensitive hashing for fingerprint
forensics over large multi-sensor databases. <em>IEEE Transactions on
Big Data</em>, <em>7</em>(4), 759–769. (<a
href="https://doi.org/10.1109/TBDATA.2017.2736547">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Searching the identity of an unknown fingerprint over large databases is very challenging. Minutia Cylinder-Code (MCC) has been proved to be very effective in mapping a minutiae-based representation (positions and directions only) into a set of fixed-length transformation-invariant binary vectors. Based on MCC, a Locality-Sensitive Hashing (LSH) scheme has been designed to index fingerprint in large databases, which uses a numerical approximation for the similarity between MCC vectors. However, the LSH scheme is not robust enough when there is certain distortion between template and searched samples, such as fingerprints captured by multi-sensors. In this paper, we propose a finer hash bit selection method based on LSH. Besides, we take into consideration another feature - the single maximum collision for indexing and fuse the candidate lists produced by both indexing methods to produce the final candidate list. Experimentations carried out on our collected multi-sensor database (2D and 3D databases) show that the proposed indexing approach greatly improves the performance of fingerprint indexing. Extensive evaluation was also conducted on some public benchmark databases for fingerprint indexing, and the results demonstrated that the new approach outperforms existing ones in almost all the cases.},
  archive  = {J},
  author   = {Wei Zhou and Jiankun Hu and Song Wang},
  doi      = {10.1109/TBDATA.2017.2736547},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  number   = {4},
  pages    = {759-769},
  title    = {Enhanced locality-sensitive hashing for fingerprint forensics over large multi-sensor databases},
  volume   = {7},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep feature learning for medical image analysis with
convolutional autoencoder neural network. <em>IEEE Transactions on Big
Data</em>, <em>7</em>(4), 750–758. (<a
href="https://doi.org/10.1109/TBDATA.2017.2717439">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {At present, computed tomography (CT) is widely used to assist disease diagnosis. Especially, computer aided diagnosis (CAD) based on artificial intelligence (AI) recently exhibits its importance in intelligent healthcare. However, it is a great challenge to establish an adequate labeled dataset for CT analysis assistance, due to the privacy and security issues. Therefore, this paper proposes a convolutional autoencoder deep learning framework to support unsupervised image features learning for lung nodule through unlabeled data, which only needs a small amount of labeled data for efficient feature learning. Through comprehensive experiments, it shows that the proposed scheme is superior to other approaches, which effectively solves the intrinsic labor-intensive problem during artificial image labeling. Moreover, it verifies that the proposed convolutional autoencoder approach can be extended for similarity measurement of lung nodules images. Especially, the features extracted through unsupervised learning are also applicable in other related scenarios.},
  archive  = {J},
  author   = {Min Chen and Xiaobo Shi and Yin Zhang and Di Wu and Mohsen Guizani},
  doi      = {10.1109/TBDATA.2017.2717439},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  number   = {4},
  pages    = {750-758},
  title    = {Deep feature learning for medical image analysis with convolutional autoencoder neural network},
  volume   = {7},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Survey on improving data utility in differentially private
sequential data publishing. <em>IEEE Transactions on Big Data</em>,
<em>7</em>(4), 729–749. (<a
href="https://doi.org/10.1109/TBDATA.2017.2715334">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The massive generation, extensive sharing, and deep exploitation of data in the big data era have raised unprecedented privacy threats. To address privacy concerns, various privacy paradigms have been proposed to achieve a good tradeoff between privacy and data utility. Particularly, differential privacy has been well accepted as one of the de facto standard for privacy preservation, and numerous schemes guaranteeing differential privacy have been proposed. Nonetheless, most of the existing works claiming a superior utility-privacy tradeoff only present specific methods, with distinct perspectives, and a complete comparative analysis and evaluation study has not been fully investigated. To this end, in this paper we review and investigate existing schemes on providing differential privacy from a broad and encompassing perspective to provide a comprehensive survey with respect to both the privacy guarantee and the effectiveness and efficiency in utility improvement. We categorize the existing schemes into distribution optimization, sensitivity calibration, transformation, decomposition, and correlations exploitation, based on their mechanisms in improving data utility. We also conduct some analysis and comparison of their various concepts and principles, focusing on improvements to data utility. Finally, we outline some challenges and provide future research directions.},
  archive  = {J},
  author   = {Xinyu Yang and Teng Wang and Xuebin Ren and Wei Yu},
  doi      = {10.1109/TBDATA.2017.2715334},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  number   = {4},
  pages    = {729-749},
  title    = {Survey on improving data utility in differentially private sequential data publishing},
  volume   = {7},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Computation outsourcing meets lossy channel: Secure sparse
robustness decoding service in multi-clouds. <em>IEEE Transactions on
Big Data</em>, <em>7</em>(4), 717–728. (<a
href="https://doi.org/10.1109/TBDATA.2017.2711040">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This paper addresses the problem of lossy outsourcing, i.e., clients outsource computation needs to the cloud side through lossy channels, which is very common in practice. We focus on the case that the clients transmit 2D sparse signals to the semi-trusted clouds over packet-loss networks, and the clouds provide sparse robustness decoding service (SRDS) for the users. In order to achieve high level of efficiency and security, we propose to jointly exploit parallel compressive sensing for robust signal encoding and employ multiple cloud servers for SRDS. Specifically, prior to encoding, a signal is encrypted by only altering the indices and amplitudes of its non-zero entries. The encrypted signal is sensed using a Gaussian measurement matrix and the generated compressive measurements are then sent to multi-clouds for SRDS, along with the occurrence of packet loss. Each column in compressive measurements can be regarded as a packet and each description consists of a certain number of packets. Each description together with a small portion of support set is distributed to a cloud. When receiving the request from a user, each cloud performs SRDS using the acquired description, where the reconstructed signal is still in encrypted form so that the signal privacy is well preserved. After receiving the reconstructed signal, the user accomplishes the decryption operation. Experimental results show that the encryption algorithm improves compressibility and reconstruction performance compared with the case of no encryption, and the proposed privacy-assured outsourcing of SRDS is highly robust and efficient.},
  archive  = {J},
  author   = {Yushu Zhang and Jiantao Zhou and Yong Xiang and Leo Yu Zhang and Fei Chen and Shaoning Pang and Xiaofeng Liao},
  doi      = {10.1109/TBDATA.2017.2711040},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  number   = {4},
  pages    = {717-728},
  title    = {Computation outsourcing meets lossy channel: Secure sparse robustness decoding service in multi-clouds},
  volume   = {7},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Optimizing share size in efficient and robust secret sharing
scheme for big data. <em>IEEE Transactions on Big Data</em>,
<em>7</em>(4), 703–716. (<a
href="https://doi.org/10.1109/TBDATA.2017.2708085">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Secret sharing scheme has been applied commonly in distributed storage for Big Data. It is a method for protecting outsourced data against data leakage and for securing key management systems. The secret is distributed among a group of participants where each participant holds a share of the secret. The secret can be only reconstructed when a sufficient number of shares are reconstituted. Although many secret sharing schemes have been proposed, they are still inefficient in terms of share size, communication cost and storage cost; and also lack robustness in terms of exact-share repair. In this paper, for the first time, we propose a new secret sharing scheme based on Slepian-Wolf coding. Our scheme can achieve an optimal share size utilizing the simple binning idea of the coding. It also enhances the exact-share repair feature whereby the shares remain consistent even if they are corrupted. We show, through experiments, how our scheme can significantly reduce the communication and storage costs while still being able to support direct share repair leveraging lightweight exclusive-OR (XOR) operation for fast computation.},
  archive  = {J},
  author   = {Tran Phuong Thao and Mohammad Shahriar Rahman and Md Zakirul Alam Bhuiyan and Ayumu Kubota and Shinsaku Kiyomoto and Kazumasa Omote},
  doi      = {10.1109/TBDATA.2017.2708085},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  number   = {4},
  pages    = {703-716},
  title    = {Optimizing share size in efficient and robust secret sharing scheme for big data},
  volume   = {7},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Secure &lt;inline-formula&gt;&lt;tex-math
notation=“LaTeX”&gt;<span
class="math inline"><em>k</em></span>&lt;/tex-math&gt;&lt;mml:math
xmlns:mml=“http://www.w3.org/1998/math/MathML”&gt;&lt;mml:mi&gt;k&lt;/mml:mi&gt;&lt;/mml:math&gt;&lt;inline-graphic
xlink:href=“wang-ieq1-2707552.gif”
xmlns:xlink=“http://www.w3.org/1999/xlink”/&gt;&lt;/inline-formula&gt;-NN
query on encrypted cloud data with multiple keys. <em>IEEE Transactions
on Big Data</em>, <em>7</em>(4), 689–702. (<a
href="https://doi.org/10.1109/TBDATA.2017.2707552">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The k-nearest neighbors (k-NN) query is a fundamental primitive in spatial and multimedia databases. It has extensive applications in location-based services, classification &amp; clustering and so on. With the promise of confidentiality and privacy, massive data are increasingly outsourced to cloud in the encrypted form for enjoying the advantages of cloud computing (e.g., reduce storage and query processing costs). Recently, many schemes have been proposed to support k-NN query on encrypted cloud data. However, prior works have all assumed that the query users (QUs) are fully-trusted and know the key of the data owner (DO), which is used to encrypt and decrypt outsourced data. The assumptions are unrealistic in many situations, since many users are neither trusted nor knowing the key. In this paper, we propose a novel scheme for secure k-NN query on encrypted cloud data with multiple keys, in which the DO and each QU all hold their own different keys, and do not share them with each other; meanwhile, the DO encrypts and decrypts outsourced data using the key of his own. Our scheme is constructed by a distributed two trapdoors public-key cryptosystem (DT-PKC) and a set of protocols of secure two-party computation, which not only preserves the data confidentiality and query privacy but also supports the offline data owner. Our extensive theoretical and experimental evaluations demonstrate the effectiveness of our scheme in terms of security and performance.},
  archive  = {J},
  author   = {Ke Cheng and Liangmin Wang and Yulong Shen and Hua Wang and Yongzhi Wang and Xiaohong Jiang and Hong Zhong},
  doi      = {10.1109/TBDATA.2017.2707552},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  number   = {4},
  pages    = {689-702},
  title    = {Secure &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$k$&lt;/tex-math&gt;&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mml:mi&gt;k&lt;/mml:mi&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=&quot;wang-ieq1-2707552.gif&quot; xmlns:xlink=&quot;http://www.w3.org/1999/xlink&quot;/&gt;&lt;/inline-formula&gt;-NN query on encrypted cloud data with multiple keys},
  volume   = {7},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Privacy-preserving data encryption strategy for big data in
mobile cloud computing. <em>IEEE Transactions on Big Data</em>,
<em>7</em>(4), 678–688. (<a
href="https://doi.org/10.1109/TBDATA.2017.2705807">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Privacy has become a considerable issue when the applications of big data are dramatically growing in cloud computing. The benefits of the implementation for these emerging technologies have improved or changed service models and improve application performances in various perspectives. However, the remarkably growing volume of data sizes has also resulted in many challenges in practice. The execution time of the data encryption is one of the serious issues during the data processing and transmissions. Many current applications abandon data encryptions in order to reach an adoptive performance level companioning with privacy concerns. In this paper, we concentrate on privacy and propose a novel data encryption approach, which is called Dynamic Data Encryption Strategy (D2ES). Our proposed approach aims to selectively encrypt data and use privacy classification methods under timing constraints. This approach is designed to maximize the privacy protection scope by using a selective encryption strategy within the required execution time requirements. The performance of D2ES has been evaluated in our experiments, which provides the proof of the privacy enhancement.},
  archive  = {J},
  author   = {Keke Gai and Meikang Qiu and Hui Zhao},
  doi      = {10.1109/TBDATA.2017.2705807},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  number   = {4},
  pages    = {678-688},
  title    = {Privacy-preserving data encryption strategy for big data in mobile cloud computing},
  volume   = {7},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Secure authentication in cloud big data with hierarchical
attribute authorization structure. <em>IEEE Transactions on Big
Data</em>, <em>7</em>(4), 668–677. (<a
href="https://doi.org/10.1109/TBDATA.2017.2705048">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {With the fast growing demands for the big data, we need to manage and store the big data in the cloud. Since the cloud is not fully trusted and it can be accessed by any users, the data in the cloud may face threats. In this paper, we propose a secure authentication protocol for cloud big data with a hierarchical attribute authorization structure. Our proposed protocol resorts to the tree-based signature to significantly improve the security of attribute authorization. To satisfy the big data requirements, we extend the proposed authentication protocol to support multiple levels in the hierarchical attribute authorization structure. Security analysis shows that our protocol can resist the forgery attack and replay attack. In addition, our protocol can preserve the entities privacy. Comparing with the previous studies, we can show that our protocol has lower computational and communication overhead.},
  archive  = {J},
  author   = {Jian Shen and Dengzhi Liu and Qi Liu and Xingming Sun and Yan Zhang},
  doi      = {10.1109/TBDATA.2017.2705048},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  number   = {4},
  pages    = {668-677},
  title    = {Secure authentication in cloud big data with hierarchical attribute authorization structure},
  volume   = {7},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A pre-authentication approach to proxy re-encryption in big
data context. <em>IEEE Transactions on Big Data</em>, <em>7</em>(4),
657–667. (<a href="https://doi.org/10.1109/TBDATA.2017.2702176">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {With the growing amount of data, the demand of big data storage significantly increases. Through the cloud center, data providers can conveniently share data stored in the center with others. However, one practically important problem in big data storage is privacy. During the sharing process, data is encrypted to be confidential and anonymous. Such operation can protect privacy from being leaked out. To satisfy the practical conditions, data tranmission with multi receivers is also considered. Furthermore, this paper proposes the notion of pre-authentication for the first time, i.e., only users with certain attributes that have already been authenticated can participate in the data transmission. The pre-authentication mechanism combines the advantages of proxy conditional re-encryption multi-sharing mechanism with the attribute-based authentication technique, thus achieving attributes authentication before re-encryption, and ensuring the security of the attributes and data. Finally this paper proves that the system can resist several attacks and the proposed pre-authentication mechanism could significantly enhance the system security level.},
  archive  = {J},
  author   = {Kun Wang and Jiahui Yu and Xiulong Liu and Song Guo},
  doi      = {10.1109/TBDATA.2017.2702176},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  number   = {4},
  pages    = {657-667},
  title    = {A pre-authentication approach to proxy re-encryption in big data context},
  volume   = {7},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Game theory based correlated privacy preserving analysis in
big data. <em>IEEE Transactions on Big Data</em>, <em>7</em>(4),
643–656. (<a href="https://doi.org/10.1109/TBDATA.2017.2701817">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Privacy preservation is one of the greatest concerns in big data. As one of extensive applications in big data, privacy preserving data publication (PPDP) has been an important research field. One of the fundamental challenges in PPDP is the trade-off problem between privacy and utility of the single and independent data set. However, recent research has shown that the advanced privacy mechanism, i.e., differential privacy, is vulnerable when multiple data sets are correlated. In this case, the trade-off problem between privacy and utility is evolved into a game problem, in which payoff of each player is dependent on his and his neighbors&#39; privacy parameters. In this paper, we first present the definition of correlated differential privacy to evaluate the real privacy level of a single data set influenced by the other data sets. Then, we construct a game model of multiple players, in which each publishes data set sanitized by differential privacy. Next, we analyze the existence and uniqueness of the pure Nash Equilibrium. We refer to a notion, i.e., the price of anarchy, to evaluate efficiency of the pure Nash Equilibrium. Finally, we show the correctness of our game analysis via simulation experiments.},
  archive  = {J},
  author   = {Xiaotong Wu and Taotao Wu and Maqbool Khan and Qiang Ni and Wanchun Dou},
  doi      = {10.1109/TBDATA.2017.2701817},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  number   = {4},
  pages    = {643-656},
  title    = {Game theory based correlated privacy preserving analysis in big data},
  volume   = {7},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Security-aware resource allocation for mobile social big
data: A matching-coalitional game solution. <em>IEEE Transactions on Big
Data</em>, <em>7</em>(4), 632–642. (<a
href="https://doi.org/10.1109/TBDATA.2017.2700318">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {As both the scale of mobile networks and the population of mobile users keep increasing, the applications of mobile social big data have emerged where mobile social users can use their mobile devices to exchange and share contents with each other. The security resource is needed to protect mobile social big data during the delivery. However, due to the limited security resource, how to allocate the security resource becomes a new challenge. Therefore, in this paper we propose a joint match-coalitional game based security-aware resource allocation scheme to deliver mobile social big data. In the proposed scheme, first a coalition game model is introduced for base stations (BSs) to form groups to provide both wireless and security resource, where the resource efficiency and profits can be improved. Second, a matching theory based model is employed to determine the selecting process between communities and the coalitions of BSs so that mobile social users can form communities to select the optimal coalition to obtain security resource. Third, a joint matching-coalition algorithm is presented to obtain the stable security-aware resource allocation. At last, the simulation experiments prove that the proposal scheme outperforms other existing schemes.},
  archive  = {J},
  author   = {Zhou Su and Qichao Xu},
  doi      = {10.1109/TBDATA.2017.2700318},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  number   = {4},
  pages    = {632-642},
  title    = {Security-aware resource allocation for mobile social big data: A matching-coalitional game solution},
  volume   = {7},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Velocity-aware parallel encryption algorithm with low energy
consumption for streams. <em>IEEE Transactions on Big Data</em>,
<em>7</em>(4), 619–631. (<a
href="https://doi.org/10.1109/TBDATA.2017.2697446">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In the environment of cloud computing, the data produced by massive users form a data stream and need to be protected by encryption for maintaining confidentiality. Traditional serial encryption algorithms are poor in performance and consume more energy without considering the property of streams. Therefore, we propose a velocity-aware parallel encryption algorithm with low energy consumption (LECPAES) for streams in cloud computing. The algorithm parallelizes Advanced Encryption Standard (AES) based on heterogeneous many-core architecture, adopts a sliding window to stabilize burst flows, senses the velocity of streams using the thresholds of the window computed by frequency ratios, and dynamically scales the frequency of Graphics Processing Units (GPUs) to lower down energy consumption. The experiments for streams at different velocities and the comparisons with other related algorithms show that the algorithm can reduce energy consumption, but only slightly increases retransmission rate and slightly decreases throughput. Therefore, LECPAES is an excellent algorithm for fast and energy-saving stream encryption.},
  archive  = {J},
  author   = {Xiongwei Fei and Kenli Li and Wangdong Yang and Keqin Li},
  doi      = {10.1109/TBDATA.2017.2697446},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  number   = {4},
  pages    = {619-631},
  title    = {Velocity-aware parallel encryption algorithm with low energy consumption for streams},
  volume   = {7},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dynamics of public opinions in an online and offline social
network. <em>IEEE Transactions on Big Data</em>, <em>7</em>(4), 610–618.
(<a href="https://doi.org/10.1109/TBDATA.2017.2676810">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {With the development of the information and Internet technology, the public opinions with big data will rapidly emerge in an online-offline social network, and an inefficient management of public opinions often will lead to the security crisis for either firms or governments. To unveil the interaction mechanism among a large number of agents between the online and offline social networks, in this paper we propose the public opinion dynamics model in an online-offline social network context. Next, in the theory aspect we investigate the analytical conditions to form a consensus in the public opinion dynamics model. Furthermore, we conduct the extensive simulations to investigate how the online agents impact the dynamics of public opinion formation, and unfold that the online agents shorten the steady-state time, decrease the number of opinion clusters, and smoothen the opinion changes in the opinion dynamics. The increase in the size of the online agents often enhances these effects. The results in this paper can provide a basis for the management of the public opinions in the Internet age.},
  archive  = {J},
  author   = {Yucheng Dong and Zhaogang Ding and Francisco Chiclana and Enrique Herrera-Viedma},
  doi      = {10.1109/TBDATA.2017.2676810},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  number   = {4},
  pages    = {610-618},
  title    = {Dynamics of public opinions in an online and offline social network},
  volume   = {7},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fast and positive definite estimation of large covariance
matrix for high-dimensional data analysis. <em>IEEE Transactions on Big
Data</em>, <em>7</em>(3), 603–609. (<a
href="https://doi.org/10.1109/TBDATA.2019.2937785">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Large covariance matrix estimation is a fundamental problem in many high-dimensional statistical analysis applications arises in economics and finance, bioinformatics, social networks, and climate studies. To achieve reliable estimation in the high-dimensional setting, an effective technique is to exploit the intrinsic structure of the covariance matrix, e.g., by sparsity regularization. For sparsity regularization, the lasso penalty is popular and convenient due to its convexity but has a bias problem. A nonconvex penalty can alleviate the bias problem, but the involved nonconvex problem under positive-definiteness constraint is generally difficult to solve. In this work, we propose an efficient algorithm for positive-definiteness constrained covariance estimation by combining the iteratively reweighted method and the alternative direction method of multipliers (ADMM). The iterative reweighting scheme can achieve better sparsity regularization than the lasso method. Meanwhile, the proposed algorithm solves convex subproblems in each iteration and hence is easy to converge. The efficiency and effectiveness of the proposed algorithm has been demonstrated by both simulation study and a gene clustering example for tumor tissues. Code for reproducing the results is available at https://github.com/FWen/pdlc.git .},
  archive  = {J},
  author   = {Fei Wen and Lei Chu and Rendong Ying and Peilin Liu},
  doi      = {10.1109/TBDATA.2019.2937785},
  journal  = {IEEE Transactions on Big Data},
  month    = {7},
  number   = {3},
  pages    = {603-609},
  title    = {Fast and positive definite estimation of large covariance matrix for high-dimensional data analysis},
  volume   = {7},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Efficient graph processing with invalid update filtration.
<em>IEEE Transactions on Big Data</em>, <em>7</em>(3), 590–602. (<a
href="https://doi.org/10.1109/TBDATA.2019.2921358">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Most of existing graph processing systems essentially follow pull-based computation model to handle compute-intensive parts of graph iteration for high parallelism. Considering all vertices and edges are processed in each iteration, pull model may suffers from a large number of invalid (vertex/edge) operations that do not contribute to graph convergence, leading to potential performance degradation. In this paper, we have the insight that these invalid operations can be filtered by leveraging a small fraction of critical information. However, most of critical information are often beyond the visibility of active vertices being processed. We present two novel filtration approaches to (cooperatively) identify out-of-visibility critical information with boundary-cut heuristics and speculative prediction for many graph algorithms. We have integrated both approaches and their hybrid solution into three state-of-art graph processing systems (including Ligra, Gemini, and Polymer). Experimental results using a wide variety of graph algorithms on both real-world and synthetic graph datasets show that neither of these approaches can have an absolute win for all graph algorithms. Boundary-cut, predictive, and hybrid approaches can improve the performance by 115.1, 38.1, and 136.6 percent on average.},
  archive  = {J},
  author   = {Long Zheng and Xianliang Li and Xi Ge and Xiaofei Liao and Zhiyuan Shao and Hai Jin and Qiang-Sheng Hua},
  doi      = {10.1109/TBDATA.2019.2921358},
  journal  = {IEEE Transactions on Big Data},
  month    = {7},
  number   = {3},
  pages    = {590-602},
  title    = {Efficient graph processing with invalid update filtration},
  volume   = {7},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). To your surprise: Identifying serendipitous collaborators.
<em>IEEE Transactions on Big Data</em>, <em>7</em>(3), 574–589. (<a
href="https://doi.org/10.1109/TBDATA.2019.2921567">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Scientific collaboration has become a universal phenomenon in recent years. Meanwhile, scholars tend to hunt for surprising collaborators for broadening their horizons. Serendipity initially denotes the fortunate discovery. Although a lot of literature is available on the topic of serendipity, little research has investigated serendipity in scientific collaborations. The objective of this paper is to identify serendipitous scientific collaborators of target scholars based on their collaboration data. First, we induce the definition of serendipitous scientific collaborators by three components, which are relevance, unexpectedness, and value, respectively. They are quantified as three intuitive indices corresponding to the network proximity, topic diversity, and collaborator influence, respectively. Second, we propose a classification model, called RUVMod, to classify all collaborators based on the analysis of three indices in definition. The serendipitous collaborator has lower network proximity, higher topic diversity and higher influence than his/her target scholar relatively. Finally, we cluster all collaborators via Self Organizing Maps and identify the serendipitous collaborator class according to the classes divided in our RUVMod. We apply our definition to the scientific collaborators extracted from DBLP dataset. The evaluation from the serendipity-based metrics suggests that RUVMod is effective in identifying serendipitous scientific collaborators.},
  archive  = {J},
  author   = {Liangtian Wan and Yuyuan Yuan and Feng Xia and Huan Liu},
  doi      = {10.1109/TBDATA.2019.2921567},
  journal  = {IEEE Transactions on Big Data},
  month    = {7},
  number   = {3},
  pages    = {574-589},
  title    = {To your surprise: Identifying serendipitous collaborators},
  volume   = {7},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multiple distance-based coding: Toward scalable feature
matching for large-scale web image search. <em>IEEE Transactions on Big
Data</em>, <em>7</em>(3), 559–573. (<a
href="https://doi.org/10.1109/TBDATA.2019.2919570">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {For scalable feature matching in large-scale web image search, the bag-of-visual-words-based (BOW) approaches generally code local features as visual words to construct an inverted index file to match features efficiently. Both the popular feature coding techniques, i.e., K-means-based vector quantization and scalar quantization, directly quantize features to generate visual words. K-means-based vector quantization requires expensive visual codebook training, whereas scalar quantization leads to the miss of many matches due to the low stability of individual components of feature vectors. To address the above issues, we demonstrate that the corresponding sub-vectors of similar features generally have similar distances to multiple reference points in feature subspace and propose a multiple distance-based feature coding scheme for scalable feature matching. Specifically, based on the distances between the sub-vectors and multiple distinct reference points, we transform each feature to a set of feature codes, where one code is treated as a visual word required to construct the inverted index file whereas the others are embedded into the index file to further verify the feature matching based on the visual words. The proposed coding scheme does not need visual codebook training and shows desirable stability and discriminability. Moreover, in the matching verification, a feature-distance estimation method is proposed to estimate the Euclidean distances between features for an accurate matching verification. Extensive experimental results demonstrate the superiority of the proposed approach in comparison to the other approaches using recent feature quantization methods for large-scale web image search.},
  archive  = {J},
  author   = {Zhili Zhou and Q. M. Jonathan Wu and Xingming Sun},
  doi      = {10.1109/TBDATA.2019.2919570},
  journal  = {IEEE Transactions on Big Data},
  month    = {7},
  number   = {3},
  pages    = {559-573},
  title    = {Multiple distance-based coding: Toward scalable feature matching for large-scale web image search},
  volume   = {7},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A new approach of exploiting self-adjoint matrix polynomials
of large random matrices for anomaly detection and fault location.
<em>IEEE Transactions on Big Data</em>, <em>7</em>(3), 548–558. (<a
href="https://doi.org/10.1109/TBDATA.2019.2920350">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Synchronized measurements of a large power grid enable an unprecedented opportunity to study the spatial-temporal correlations. Statistical analytics for those massive datasets start with high-dimensional data matrices. Uncertainty is ubiquitous in a future&#39;s power grid. These data matrices are recognized as random matrices. This new point of view is fundamental in our theoretical analysis since true covariance matrices cannot be estimated accurately in a high-dimensional regime. As an alternative, we consider large-dimensional sample covariance matrices in the asymptotic regime to replace the true covariance matrices. The self-adjoint polynomials of large-dimensional random matrices are studied as statistics for big data analytics. The calculation of the asymptotic spectrum distribution (ASD) for such a matrix polynomial is understandably challenging. This task is made possible by a recent breakthrough in free probability, an active research branch in random matrix theory. This is the very reason why the work of this paper is inspired initially. The new approach is interesting in many aspects. The mathematical reason may be most critical. The real-world problems can be solved using this approach, however.},
  archive  = {J},
  author   = {Zenan Ling and Robert C. Qiu and Xing He and Lei Chu},
  doi      = {10.1109/TBDATA.2019.2920350},
  journal  = {IEEE Transactions on Big Data},
  month    = {7},
  number   = {3},
  pages    = {548-558},
  title    = {A new approach of exploiting self-adjoint matrix polynomials of large random matrices for anomaly detection and fault location},
  volume   = {7},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Billion-scale similarity search with GPUs. <em>IEEE
Transactions on Big Data</em>, <em>7</em>(3), 535–547. (<a
href="https://doi.org/10.1109/TBDATA.2019.2921572">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Similarity search finds application in database systems handling complex data such as images or videos, which are typically represented by high-dimensional features and require specific indexing structures. This paper tackles the problem of better utilizing GPUs for this task. While GPUs excel at data parallel tasks such as distance computation, prior approaches in this domain are bottlenecked by algorithms that expose less parallelism, such as k -min selection, or make poor use of the memory hierarchy. We propose a novel design for k -selection. We apply it in different similarity search scenarios, by optimizing brute-force, approximate and compressed-domain search based on product quantization. In all these setups, we outperform the state of the art by large margins. Our implementation operates at up to 55 percent of theoretical peak performance, enabling a nearest neighbor implementation that is 8.5 × faster than prior GPU state of the art. It enables the construction of a high accuracy k -NN graph on 95 million images from the Yfcc100M dataset in 35 minutes, and of a graph connecting 1 billion vectors in less than 12 hours on 4 Maxwell Titan X GPUs. We have open-sourced our approach for the sake of comparison and reproducibility.},
  archive  = {J},
  author   = {Jeff Johnson and Matthijs Douze and Hervé Jégou},
  doi      = {10.1109/TBDATA.2019.2921572},
  journal  = {IEEE Transactions on Big Data},
  month    = {7},
  number   = {3},
  pages    = {535-547},
  title    = {Billion-scale similarity search with GPUs},
  volume   = {7},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Visualization of big spatial data using coresets for kernel
density estimates. <em>IEEE Transactions on Big Data</em>,
<em>7</em>(3), 524–534. (<a
href="https://doi.org/10.1109/TBDATA.2019.2913655">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The size of large, geo-located datasets has reached scales where visualization of all data points is inefficient. Random sampling is a method to reduce the size of a dataset, yet it can introduce unwanted errors. We describe a method for subsampling of spatial data suitable for creating kernel density estimates from very large data and demonstrate that it results in less error than random sampling. We also introduce a method to ensure that thresholding of low values based on sampled data does not omit any regions above the desired threshold when working with sampled data. We demonstrate the effectiveness of our approach using both, artificial and real-world large geospatial datasets.},
  archive  = {J},
  author   = {Yan Zheng and Yi Ou and Alexander Lex and Jeff M. Phillips},
  doi      = {10.1109/TBDATA.2019.2913655},
  journal  = {IEEE Transactions on Big Data},
  month    = {7},
  number   = {3},
  pages    = {524-534},
  title    = {Visualization of big spatial data using coresets for kernel density estimates},
  volume   = {7},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Modeling and visualizing student flow. <em>IEEE Transactions
on Big Data</em>, <em>7</em>(3), 510–523. (<a
href="https://doi.org/10.1109/TBDATA.2018.2840986">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In this work, we present a data science system to model and visualize student flow patterns based on electronic student data of a university. Our system is called eCamp. The datasets used by eCamp were previously disconnected and only maintained and accessed in a siloed manner by independent campus offices. At a campus-level, our models and visualization show how students make choices among hundreds of potential majors, as students gradually progress towards their sophomore, junior, and senior year. At a department-level, the student flow patterns revealed by eCamp show how each course plays a different role within a curriculum. eCamp further dives down to the granularity of the exact classes offered in each semester. At that level, eCamp shows how students navigate from one set of classes in one semester to another set in a subsequent semester. Previously, comprehensive information about student progression patterns at all of these level was simply unavailable. To that end, we also demonstrate how insights into such student flow patterns can support analytical tasks involving student outcomes, student retention, and curriculum design.},
  archive  = {J},
  author   = {Mohammad Raji and John Duggan and Blaise DeCotes and Jian Huang and Bradley Vander Zanden},
  doi      = {10.1109/TBDATA.2018.2840986},
  journal  = {IEEE Transactions on Big Data},
  month    = {7},
  number   = {3},
  pages    = {510-523},
  title    = {Modeling and visualizing student flow},
  volume   = {7},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Visual analysis of spatio-temporal event predictions:
Investigating the spread dynamics of invasive species. <em>IEEE
Transactions on Big Data</em>, <em>7</em>(3), 497–509. (<a
href="https://doi.org/10.1109/TBDATA.2018.2877352">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Invasive species are a major cause of ecological damage and commercial losses. A current problem spreading in North America and Europe is the vinegar fly Drosophila suzukii. Unlike other Drosophila, it infests non-rotting and healthy fruits and is therefore of concern to fruit growers, such as vintners. Consequently, large amounts of data about the occurrence of D. suzukii have been collected in recent years. However, there is a lack of interactive methods to investigate this data. We employ ensemble-based classification to predict areas susceptible to the occurrence of D. suzukii and bring them into a spatio-temporal context using maps and glyph-based visualizations. Following the information-seeking mantra, we provide a visual analysis system Drosophigator for spatio-temporal event predictions, enabling the investigation of the spread dynamics of invasive species. We demonstrate the usefulness of our approach in three use cases and an evaluation with more than 30 domain experts.},
  archive  = {J},
  author   = {Daniel Seebacher and Johannes Häußler and Michael Hundt and Manuel Stein and Hannes Müller and Ulrich Engelke and Daniel A. Keim},
  doi      = {10.1109/TBDATA.2018.2877352},
  journal  = {IEEE Transactions on Big Data},
  month    = {7},
  number   = {3},
  pages    = {497-509},
  title    = {Visual analysis of spatio-temporal event predictions: Investigating the spread dynamics of invasive species},
  volume   = {7},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Integrating data and model space in ensemble learning by
visual analytics. <em>IEEE Transactions on Big Data</em>, <em>7</em>(3),
483–496. (<a href="https://doi.org/10.1109/TBDATA.2018.2877350">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Ensembles of classifier models typically deliver superior performance and can outperform single classifier models given a dataset and classification task at hand. However, the gain in performance comes together with the lack of comprehensibility, posing a challenge to understand how each model affects the classification outputs and from where the errors come. We propose a tight visual integration of the data and the model space for exploring and combining classifier models. We introduce an interactive workflow that builds upon the visual integration and enables the effective exploration of classification outputs and models. The involvement of the user is key to our approach. Therefore, we elaborate on the role of the human and connect our approach to theoretical frameworks on human-centered machine learning. We showcase the usefulness of our approach and the integration of the user via binary and multiclass classification problems. Based on ensembles automatically selected by a standard ensemble selection algorithm, the user can manipulate models and alternative combinations.},
  archive  = {J},
  author   = {Bruno Schneider and Dominik Jäckle and Florian Stoffel and Alexandra Diehl and Johannes Fuchs and Daniel Keim},
  doi      = {10.1109/TBDATA.2018.2877350},
  journal  = {IEEE Transactions on Big Data},
  month    = {7},
  number   = {3},
  pages    = {483-496},
  title    = {Integrating data and model space in ensemble learning by visual analytics},
  volume   = {7},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A new methodology for storing consistent fuzzy geospatial
data in big data environment. <em>IEEE Transactions on Big Data</em>,
<em>7</em>(2), 468–482. (<a
href="https://doi.org/10.1109/TBDATA.2017.2725904">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In this era of big data, as relational databases are inefficient, NoSQL databases are a workable solution for data storage. In this context, one of the key issues is the veracity and therefore the data quality. Indeed, as with classic data, geospatial big data are generally fuzzy even though they are stored as crisp data (perfect data). Hence, if data are geospatial and fuzzy, additional complexities appear because of the complex syntax and semantic features of such data. The NoSQL databases do not offer strict data consistency. Therefore, new challenges are needed to be overcome to develop efficient methods that simultaneously ensure the performance and the consistency in storing fuzzy geospatial big data. This paper presents a new methodology that tackles the storage issues and validates the fuzzy spatial entities&#39; consistency in a document-based NoSQL system. Consequently, first, to better express the structure of fuzzy geospatial data in such a system, we present a logical model called Fuzzy GeoJSON schema. Second, for consistent storage, we implement a schema-driven pipeline based on the Fuzzy GeoJSON schema and semantic constraints.},
  archive  = {J},
  author   = {Besma Khalfi and Cyril de Runz and Sami Faiz and Herman Akdag},
  doi      = {10.1109/TBDATA.2017.2725904},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {2},
  pages    = {468-482},
  title    = {A new methodology for storing consistent fuzzy geospatial data in big data environment},
  volume   = {7},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A machine learning based framework for verification and
validation of massive scale image data. <em>IEEE Transactions on Big
Data</em>, <em>7</em>(2), 451–467. (<a
href="https://doi.org/10.1109/TBDATA.2017.2680460">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Big data validation and system verification are crucial for ensuring the quality of big data applications. However, a rigorous technique for such tasks is yet to emerge. During the past decade, we have developed a big data system called CMA for investigating the classification of biological cells based on cell morphology which is captured in diffraction images. CMA includes a collection of scientific software tools, machine learning algorithms, and a large-scale cell image repository. In order to ensure the quality of big data system CMA, we developed a framework for rigorously validating the massive scale image data as well as adequately verifying both the software tools and machine learning algorithms. The validation of big data is conducted by iteratively selecting the data using a machine learning approach. An experimental approach guided by a feature selection algorithm is introduced in the framework to select an optimal feature set for improving the machine learning performance. The verification of software and algorithms is developed on the iterative metamorphic testing approach due to the non-testable property of the software and algorithms. A machine learning approach is introduced for developing test oracles iteratively to ensure the adequacy of the test coverage criteria. Performance of the machine learning algorithm is evaluated with a stratified N-fold cross validation and confusion matrix. We describe the design of the proposed big data verification and validation framework with CMA as the case study, and demonstrate its effectiveness through verifying and validating the dataset, the software and the algorithms in CMA.},
  archive  = {J},
  author   = {Junhua Ding and Xin-Hua Hu and Venkat Gudivada},
  doi      = {10.1109/TBDATA.2017.2680460},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {2},
  pages    = {451-467},
  title    = {A machine learning based framework for verification and validation of massive scale image data},
  volume   = {7},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Semantic-based and entity-resolution fusion to enhance
quality of big RDF data. <em>IEEE Transactions on Big Data</em>,
<em>7</em>(2), 436–450. (<a
href="https://doi.org/10.1109/TBDATA.2017.2710346">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Within an organisation, the quality in big data is a cornerstone to operational, transactional processes and to the reliability of business analytics for decision making. In fact, as organizations are harnessing multi-sources data to rise the benefits of their business, the quality of data becomes important and crucial. This paper presents a new approach to query big data sources using Resource Description Framework (RDF) representation to ensure data quality by harvesting more relevant and complete query results. Our approach handles two important types of heterogeneity over multiple data sources: semantic heterogeneity and URI-based entity identification. It proposes (1) a semantic entity resolution method based on inference mechanism using rules to manage the misunderstanding of data, in real world entities (2) Data Quality enhancement using MapReduce-based query rewriting approach includes the entity resolution results to infer and adds implicit data into query results (3) a parallel combination of MapReduce jobs of saturation and query rewriting inferences to handle transitive and cyclic rules for a richer rules&#39; expression language (4) experiments to assess the efficiency of the proposed approach over real big RDF data originating from insurance and synthetic data sets.},
  archive  = {J},
  author   = {Salima Benbernou and Xin Huang and Mourad Ouziri},
  doi      = {10.1109/TBDATA.2017.2710346},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {2},
  pages    = {436-450},
  title    = {Semantic-based and entity-resolution fusion to enhance quality of big RDF data},
  volume   = {7},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Towards dynamic verifiable pattern matching. <em>IEEE
Transactions on Big Data</em>, <em>7</em>(2), 421–435. (<a
href="https://doi.org/10.1109/TBDATA.2018.2868657">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Verifiable pattern matching enables users to obtain authenticated query results over outsourced data on an untrusted remote server. It is a fundamental problem in many security-critical big data applications, including big database search, human genome data search, text search, etc., especially when these applications are outsourced to third-party clouds. However, the state-of-the-art schemes do not yet support efficient data updates. In this work, we propose the first dynamic verifiable pattern matching scheme to support efficient data updates. The proposed scheme is built on two ideas: one is to embed unique randomness to decouple the character and its index in the outsourced data, enabling efficient data updates; the other is to reduce the verifiable pattern matching problem to a discrete set membership testing problem, which relies on the decoupling introduced in the first idea. Based on these two ideas, the proposed scheme first employs the suffix array index structure to search pattern matching queries. The scheme then authenticates the outsourced text using a newly designed authenticated data structure based on the RSA accumulator, which guarantees the verifiability of pattern matching query results. Data update is naturally supported using the RSA accumulator working on discrete sets. Based on the proposed design, we have prototyped a proof-of-concept for the proposed scheme and have conducted an extensive experimental evaluation. In addition to supporting efficient data update, our experimental results show that the proposed scheme incurs reduced verification cost in comparison with the baseline state-of-the-art scheme.},
  archive  = {J},
  author   = {Fei Chen and Donghong Wang and Qiuzhen Lin and Jianyong Chen and Zhong Ming and Wei Yu and Jing Qin},
  doi      = {10.1109/TBDATA.2018.2868657},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {2},
  pages    = {421-435},
  title    = {Towards dynamic verifiable pattern matching},
  volume   = {7},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Cost-efficient heterogeneous worker recruitment under
coverage requirement in spatial crowdsourcing. <em>IEEE Transactions on
Big Data</em>, <em>7</em>(2), 407–420. (<a
href="https://doi.org/10.1109/TBDATA.2018.2865755">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {With the progress of mobile devices and the successful forms of using the wisdom of crowds, spatial crowdsourcing has attracted much attention from the research community. The idea of spatial crowdsourcing is recruiting a set of available crowds to finish the spatial tasks located in crowdsourcing locations, e.g., landmarks, by using their handheld devices. This paper addresses the worker recruitment problem in spatial crowdsourcing under the coverage and workload-balancing requirements. The coverage constraint means that any crowdsourcing location should be visited by at least one of the recruited workers to satisfy the Quality-of-Service requirement, e.g., traffic monitoring or climate forecast. In addition, we argue that each crowdsourcing operation has a cost in reality, e.g., data traffic or energy consumption and the resource may be limited at each crowdsourcing location. The objective of this paper is to solve a Coverage and Balanced Crowdsourcing Recruiting (CBCR) problem, which ensures the coverage requirement and minimizes the maximum crowdsourcing cost for any crowdsourcing location. We prove that the CBCR problem is NP-hard in the general case. Then, we discuss the CBCR problem in the 1-D scenario. In the 1-D scenario, we first propose a directionally coverage scheme and further extend it to a Polynomial-Time Approximation Scheme (PTAS) to trade-off the computation complexity and the performance. The performance can be bounded to 2 + ε, where ε can be an arbitrary small value. Then, we found that there exists a sub-optimal structure, and thus the dynamic programming approach is proposed to find the optimal solution in the 1-D scenario. In the general 2-D scenario, we first prove that it has a sub-modular property and thus the naive greedy algorithm has an approximation ratio of ln n + 1. In addition, we propose a randomized rounding algorithm with an expectation bound of O(log n / log log n ). Extensive experiments on realistic traces demonstrate the effectiveness of the proposed algorithms.},
  archive  = {J},
  author   = {Ning Wang and Jie Wu},
  doi      = {10.1109/TBDATA.2018.2865755},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {2},
  pages    = {407-420},
  title    = {Cost-efficient heterogeneous worker recruitment under coverage requirement in spatial crowdsourcing},
  volume   = {7},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Altrumetrics: Inferring altruism propensity based on mobile
phone use patterns. <em>IEEE Transactions on Big Data</em>,
<em>7</em>(2), 397–406. (<a
href="https://doi.org/10.1109/TBDATA.2018.2873346">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Altruism, i.e., an act that one does at their own expense that tends to enhance others well-being, is a fundamental human behavior with implications for personal and societal welfare. Hence, modeling altruism is an important building block in designing human-centered computing systems. Traditional methods for understanding an individual&#39;s altruistic propensities have been surveys and lab experiments. However, the emerging “personal big data” coming from mobile and ubiquitous devices allows for creation of lower-cost, quicker, automated methods for modeling human behaviors and propensities. We propose a new methodology to model altruism using phone data. Based on analysis of data from a 10-week field study (N = 55 N = 55 &lt;; inline-graphic xlink:href=&quot;bati-ieq1-2873346.gif&quot;/&gt; participants), we report that: (1) multiple phone-based features are associated with users&#39; altruistic propensities; (2) phone features-based altruism prediction model yielded significantly better performance than a demography-based model. The results pave way for utilizing “personal big data” to model altruism in multiple commercial and social applications.},
  archive  = {J},
  author   = {Ghassan F. Bati and Vivek K. Singh},
  doi      = {10.1109/TBDATA.2018.2873346},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {2},
  pages    = {397-406},
  title    = {Altrumetrics: Inferring altruism propensity based on mobile phone use patterns},
  volume   = {7},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Opportunistic discovery of personal places using
multi-source sensor data. <em>IEEE Transactions on Big Data</em>,
<em>7</em>(2), 383–396. (<a
href="https://doi.org/10.1109/TBDATA.2018.2872585">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Modern smartphones and wearables are able to continuously collect significant amounts of sensor data, where such data can be helpful to study a user&#39;s mobility or social interaction patterns, but also to deliver various services based on a user&#39;s presence at different places during certain times of the day. Therefore, it is important to accurately identify personal places of interest (POIs), such as a user&#39;s workplace or home. Such places are usually determined using segmentation of location traces, but frequent gaps in the data (i.e., missing location readings) can result in a large number of small and incomplete segments that should actually be grouped together into a single large segment. This paper presents a segmentation approach that utilizes a user&#39;s personal data obtained from multiple sensor sources and devices such as the battery recharge behavior (measured on smartphones), step counts, and sleep patterns (measured by wearables), to opportunistically fill gaps in the user&#39;s location traces. Using the data from a mobile crowd sensing study of more than 450 users over a 2-year period, we show that our approach is able to generate fewer, but more complete segments compared to the state of the art.},
  archive  = {J},
  author   = {Sudip Vhaduri and Christian Poellabauer},
  doi      = {10.1109/TBDATA.2018.2872585},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {2},
  pages    = {383-396},
  title    = {Opportunistic discovery of personal places using multi-source sensor data},
  volume   = {7},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). BusBeat: Early event detection with real-time bus GPS
trajectories. <em>IEEE Transactions on Big Data</em>, <em>7</em>(2),
371–382. (<a href="https://doi.org/10.1109/TBDATA.2018.2872532">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Large-scale events attracting many participants might have a strong negative impact on productivity, mobility, comfort, and safety in a city. Within the few years, serious accidents led by congestion have occurred, especially during sports events, religious ceremonies, festivals, and so on. To alleviate these serious accidents, predicting the occurrence of a large-scale event is much significant. When we know an event occurrence in advance, some of those who are not interested in the event might change their plans and/or might take a detour to avoid to get involved in a heavy congestion. In this paper, we present an early event detection technique named BusBeat. BusBeat uses GPS trajectory data collected from periodic-cars that are vehicles periodically traveling on a pre-scheduled route with a pre-determined departure time, such as a transit bus, shuttle, garbage truck, or municipal patrol car. BusBeat interpolates the missing GPS data by using the features of the periodic-cars. In addition, BusBeat uses the network-based analysis with a Time-dependent Congestion Network (TCN) in order to detect geo-spatial events. BusBeat achieves early event detection without incurring any privacy invasion, by using the continuous trajectories of the periodic-cars that provide the real-time traffic flow and speed. Since traffic towards an event venue would be slow before the event starts, BusBeat detects the geo-spatial events before the attendees gather. We evaluate our BusBeat using over 7,000-bus data collected in Beijing for 5 months and compare with the check-in data collected from a social network service.},
  archive  = {J},
  author   = {Shunsuke Aoki and Kaoru Sezaki and Nicholas Jing Yuan and Xing Xie},
  doi      = {10.1109/TBDATA.2018.2872532},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {2},
  pages    = {371-382},
  title    = {BusBeat: Early event detection with real-time bus GPS trajectories},
  volume   = {7},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fusing location data for depression prediction. <em>IEEE
Transactions on Big Data</em>, <em>7</em>(2), 355–370. (<a
href="https://doi.org/10.1109/TBDATA.2018.2872569">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Recent studies have demonstrated that geographic location features collected using smartphones can be a powerful predictor for depression. While location information can be conveniently gathered by GPS, typical datasets suffer from significant periods of missing data due to various factors (e.g., phone power dynamics, limitations of GPS). A common approach is to remove the time periods with significant missing data before data analysis. In this paper, we develop an approach that fuses location data collected from two sources: GPS and WiFi association records, on smartphones, and evaluate its performance using a dataset collected from 79 college students. Our evaluation demonstrates that our data fusion approach leads to significantly more complete data. In addition, the features extracted from the more complete data present stronger correlation with self-report depression scores, and lead to depression prediction with much higher &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$F_1$&lt;/tex-math&gt;&lt;/inline-formula&gt; scores (up to 0.76 compared to 0.5 before data fusion). We further investigate the scenario when including an additional data source, i.e., the data collected from a WiFi network infrastructure. Our results show that, while this additional data source leads to even more complete data, the resultant &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$F_1$&lt;/tex-math&gt;&lt;/inline-formula&gt; scores are similar to those when only using the location data (i.e., GPS and WiFi association records) from the phones.},
  archive  = {J},
  author   = {Chaoqun Yue and Shweta Ware and Reynaldo Morillo and Jin Lu and Chao Shang and Jinbo Bi and Jayesh Kamath and Alexander Russell and Athanasios Bamis and Bing Wang},
  doi      = {10.1109/TBDATA.2018.2872569},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {2},
  pages    = {355-370},
  title    = {Fusing location data for depression prediction},
  volume   = {7},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An incremental tensor-train decomposition for
cyber-physical-social big data. <em>IEEE Transactions on Big Data</em>,
<em>7</em>(2), 341–354. (<a
href="https://doi.org/10.1109/TBDATA.2018.2867485">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Cyber-physical-social big data generated from ubiquitous devices and diverse spaces generally are multi-source, heterogeneous, and deeply intertwined. To efficiently analyze and handle the ubiquitous cyber-physical-social big data, tensor is considered as an effective tool, but the curse of dimensionality is still the main bottleneck of tensor-based big data analysis. Tensor networks can considerably alleviate or overcome it through the tensor approximate theory. Therefore, this paper focuses on developing an efficient big data processing framework based on tensor networks and providing an incremental tensor train decomposition approach for the streaming big data. Concretely, this paper first presents a hierarchical cyber-physical-social big data processing framework composed of three planes, namely, data representation and decomposition, data storage and processing, and data analysis and service, in which tensor train (TT) and quantized TT decompositions are particularly introduced to remarkably overcome the curse of dimensionality. Besides, to efficiently handle the continuous streaming big data and avoid the repeated decomposition for the history data, an incremental tensor train decomposition (ITTD) approach is proposed and the complexities are further analyzed in detail. Experimental results demonstrate that ITTD demonstrably outperforms the nonincremental TT decomposition in execution time on the precise of guaranteeing the nearly equal approximation error.},
  archive  = {J},
  author   = {Huazhong Liu and Laurence T. Yang and Yimu Guo and Xia Xie and Jianhua Ma},
  doi      = {10.1109/TBDATA.2018.2867485},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {2},
  pages    = {341-354},
  title    = {An incremental tensor-train decomposition for cyber-physical-social big data},
  volume   = {7},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A user-oriented taxi ridesharing system with large-scale
urban GPS sensor data. <em>IEEE Transactions on Big Data</em>,
<em>7</em>(2), 327–340. (<a
href="https://doi.org/10.1109/TBDATA.2018.2872450">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Ridesharing is a challenging topic in the urban computing paradigm, which utilizes urban sensors to generate a wealth of benefits and thus is an important branch in ubiquitous computing. Traditionally, ridesharing is achieved by mainly considering the received user ridesharing requests and then returns solutions to users. However, there lack research efforts of examining user acceptance to the proposed solutions. To our knowledge, user decisions in accepting/rejecting a rideshare is one of the crucial, yet not well studied, factors in the context of dynamic ridesharing. Moreover, existing research attention is mainly paid to find the nearest taxi, whilst in reality the nearest taxi may not be the optimal answer. In this paper, we tackle the above un-addressed issues while preserving the scalability of the system. We present a scalable framework, namely TRIPS, which supports the probability of accepting each request by the companion passengers and minimizes users&#39; efforts. In TRIPS, we propose three search techniques to increase the efficiency of the proposed ridesharing service. We also reformulate the criteria for searching and ranking ridesharing alternatives and propose indexing techniques to optimize the process. Our approach is validated using a real, large-scale dataset of 10,357 GPS-equipped taxis in the city of Beijing, China and showcases its effectiveness on the ridesharing task.},
  archive  = {J},
  author   = {Wei Emma Zhang and Ali Shemshadi and Quan Z. Sheng and Yongrui Qin and Xiujuan Xu and Jian Yang},
  doi      = {10.1109/TBDATA.2018.2872450},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {2},
  pages    = {327-340},
  title    = {A user-oriented taxi ridesharing system with large-scale urban GPS sensor data},
  volume   = {7},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Data-driven digital advertising with uncertain demand model
in metro networks. <em>IEEE Transactions on Big Data</em>,
<em>7</em>(2), 313–326. (<a
href="https://doi.org/10.1109/TBDATA.2017.2725913">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Nowadays most metro advertising systems schedule advertising slots on digital advertising screens to achieve the maximum exposure to passengers by exploring passenger demand models. However, our empirical results show that these passenger demand models experience uncertainty at fine temporal granularity (e.g., per min). As a result, for fine-grained advertisements (shorter than one minute), a scheduling based on these demand models cannot achieve the maximum advertisement exposure. To address this issue, we propose an online advertising approach, called FineUDM, based on the uncertain passenger demand modeling for both entering passengers and exiting passengers. FineUDM combines coarse-grained statistical demand modeling and fine-grained real-time demand modeling by leveraging historical passenger demands, real-time card-swiping records, and passenger mobility patterns. Based on this uncertain demand model, it schedules advertising time online based on robust receding horizon control to maximize the advertisement exposure. We evaluate the proposed approach based on an one-month sample from our 530 GB real-world metro fare dataset with 16 million cards. The results show that our approach provides a 61.5 percent lower traffic prediction error and 20 percent improvement on advertising efficiency on average.},
  archive  = {J},
  author   = {Ruobing Jiang and Zhenni Feng and Desheng Zhang and Shuai Wang and Yanmin Zhu and Fan Zhang and Tian He},
  doi      = {10.1109/TBDATA.2017.2725913},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {2},
  pages    = {313-326},
  title    = {Data-driven digital advertising with uncertain demand model in metro networks},
  volume   = {7},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). ScaleJoin: A deterministic, disjoint-parallel and
skew-resilient stream join. <em>IEEE Transactions on Big Data</em>,
<em>7</em>(2), 299–312. (<a
href="https://doi.org/10.1109/TBDATA.2016.2624274">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The inherently large and varying volumes of information generated in large scale systems demand near real-time processing of data streams. In this context, data streaming is imperative for data-intensive processing infrastructures. Stream joins, the streaming counterpart of database joins, compare tuples coming from different streams and constitute one of the most important and expensive data streaming operators. Algorithmic implementations of stream joins have to be capable of efficiently processing bursty and rate-varying data streams in a deterministic and skew-resilient fashion. To leverage the design of modern multicore architectures, scalability and parallelism need to be addressed also in the algorithmic design. In this paper we present ScaleJoin, an algorithmic construction for deterministic and parallel stream joins that guarantees all the above properties, thus filling in a gap in the existing state-of-the-art. Key to the novelty of ScaleJoin is the ScaleGate data structure and its lock-free implementation. ScaleGate facilitates concurrent data exchange and balances independent actions among processing threads; enabling fine-grain parallelism and deterministic processing. It allows ScaleJoin to run on an arbitrary number of processing threads, evenly sharing the overall comparisons run in parallel and achieving disjoint and skew-resilient high processing throughput and low processing latency.},
  archive  = {J},
  author   = {Vincenzo Gulisano and Yiannis Nikolakopoulos and Marina Papatriantafilou and Philippas Tsigas},
  doi      = {10.1109/TBDATA.2016.2624274},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {2},
  pages    = {299-312},
  title    = {ScaleJoin: A deterministic, disjoint-parallel and skew-resilient stream join},
  volume   = {7},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Managing big interval data with CINTIA: The checkpoint
INTerval array. <em>IEEE Transactions on Big Data</em>, <em>7</em>(2),
285–298. (<a href="https://doi.org/10.1109/TBDATA.2017.2691719">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Intervals have become prominent in data management as they are the main data structure to represent a number of key data types such as temporal or genomic data. Yet, there exists no solution to compactly store and efficiently query big interval data. In this paper we introduce CINTIA-the Checkpoint INTerval Index Array-an efficient data structure to store and query interval data, which achieves high memory locality and outperforms state-of-the art solutions. We also propose a low-latency, Big Data system that implements CINTIA on top of a popular distributed file system and efficiently manages large interval data on clusters of commodity machines. Our system can easily be scaled-out and was designed to accommodate large delays between the various components of a distributed infrastructure. We experimentally evaluate the performance of our approach on several datasets and show that it outperforms current solutions by several orders of magnitude in distributed settings.},
  archive  = {J},
  author   = {Ruslan Mavlyutov and Philippe Cudre-Mauroux},
  doi      = {10.1109/TBDATA.2017.2691719},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {2},
  pages    = {285-298},
  title    = {Managing big interval data with CINTIA: The checkpoint INTerval array},
  volume   = {7},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A deep learning-based data minimization algorithm for fast
and secure transfer of big genomic datasets. <em>IEEE Transactions on
Big Data</em>, <em>7</em>(2), 271–284. (<a
href="https://doi.org/10.1109/TBDATA.2018.2805687">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In the age of Big Genomics Data, institutions such as the National Human Genome Research Institute (NHGRI) are challenged in their efforts to share volumes of data between researchers, a process that has been plagued by unreliable transfers and slow speeds. These occur due to throughput bottlenecks of traditional transfer technologies. Two factors that affect the efficiency of data transmission are the channel bandwidth and the amount of data. Increasing the bandwidth is one way to transmit data efficiently, but might not always be possible due to resource limitations. Another way to maximize channel utilization is by decreasing the bits needed for transmission of a dataset. Traditionally, transmission of big genomic data between two geographical locations is done using general-purpose protocols, such as hypertext transfer protocol (HTTP) and file transfer protocol (FTP) secure. In this paper, we present a novel deep learning-based data minimization algorithm that 1) minimizes the datasets during transfer over the carrier channels; 2) protects the data from the man-in-the-middle (MITM) and other attacks by changing the binary representation (content-encoding) several times for the same dataset: we assign different codewords to the same character in different parts of the dataset. Our data minimization strategy exploits the alphabet limitation of DNA sequences and modifies the binary representation (codeword) of dataset characters using deep learning-based convolutional neural network (CNN) to ensure a minimum of code word uses to the high frequency characters at different time slots during the transfer time. This algorithm ensures transmission of big genomic DNA datasets with minimal bits and latency and yields an efficient and expedient process. Our tested heuristic model, simulation, and real implementation results indicate that the proposed data minimization algorithm is up to 99 times faster and more secure than the currently used content-encoding scheme used in HTTP of the HTTP content-encoding scheme and 96 times faster than FTP on tested datasets. The developed protocol in C# will be available to the wider genomics community and domain scientists.},
  archive  = {J},
  author   = {Mohammed Aledhari and Marianne Di Pierro and Mohamed Hefeida and Fahad Saeed},
  doi      = {10.1109/TBDATA.2018.2805687},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {2},
  pages    = {271-284},
  title    = {A deep learning-based data minimization algorithm for fast and secure transfer of big genomic datasets},
  volume   = {7},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A large-scale study of android malware development
phenomenon on public malware submission and scanning platform. <em>IEEE
Transactions on Big Data</em>, <em>7</em>(2), 255–270. (<a
href="https://doi.org/10.1109/TBDATA.2018.2790439">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {With the steady growth of Android malware, we suspect that, during the malware development phase, some Android malware writers use the popular public scanning services (e.g., VirusTotal) for testing the evasion capability of their malware samples, which we name Android malware development cases (AMDs). In this work, we design an AMD hunter in the context of VirusTotal to hunt for AMDs and reveal new threats for Android. First, the AMD hunter sifts through millions of file submissions on VirusTotal efficiently and alert more suspicious submission traces. Second, it performs package level analysis, static code and dynamic analyses on the APKs of the suspicious submissions to validate the AMDs. The implemented hunter has been used in a leading security company for 4 months, which processed 153 million of submissions on VirusTotal, and identified 1,623 AMDs with 13,855 samples from 83 countries. We also performed case studies on 890 malware samples selected from the identified AMDs, which revealed lots of new threats, including the development cases of fake system/banking phishing app, new rooting exploits, new JavaScript based threats, new evasions and AV probing malware. We wrote industry research articles about some AMDs and notified other security vendors to help patch their false negatives. Besides raising the awareness of the existence of AMDs, more importantly, our research provides the first systematic and efficient way to study the malware development phenomenon on VirusTotal. We will share all the samples of the identified AMDs with the research community.},
  archive  = {J},
  author   = {Heqing Huang and Cong Zheng and Junyuan Zeng and Wu Zhou and Sencun Zhu and Peng Liu and Ian Molloy and Suresh Chari and Ce Zhang and Quanlong Guan},
  doi      = {10.1109/TBDATA.2018.2790439},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {2},
  pages    = {255-270},
  title    = {A large-scale study of android malware development phenomenon on public malware submission and scanning platform},
  volume   = {7},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A novel hilbert curve for cache-locality preserving loops.
<em>IEEE Transactions on Big Data</em>, <em>7</em>(2), 241–254. (<a
href="https://doi.org/10.1109/TBDATA.2018.2830378">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Modern microprocessors offer a rich memory hierarchy including various levels of cache and registers. Some of these memories (like main memory, L3 cache) are big but slow and shared among all cores. Others (registers, L1 cache) are fast and exclusively assigned to a single core but small. Only if the data accesses have a high locality, we can avoid excessive data transfers between the memory hierarchy. In this paper we consider fundamental algorithms like matrix multiplication, K-Means, Cholesky decomposition as well as the algorithm by Floyd and Warshall typically operating in two or three nested loops. We propose to traverse these loops whenever possible not in the canonical order but in an order defined by a space-filling curve. This traversal order dramatically improves data locality over a wide granularity allowing not only to efficiently support a cache of a single, known size (cache conscious) but also a hierarchy of various caches where the effective size available to our algorithms may even be unknown (cache oblivious). We propose a new space-filling curve called Fast Unrestricted (FUR) Hilbert with the following advantages: (1) we overcome the usual limitation to square-like grid sizes where the side-length is a power of 2 or 3. Instead, our approach allows arbitrary loop boundaries for all variables. (2) FUR-Hilbert is non-recursive with a guaranteed constant worst case time complexity per loop iteration (in contrast to O(log(grid-size)) for previous methods). (3) Our non-recursive approach makes the application of our cache-oblivious loops in any host algorithm as easy as conventional loops and facilitates automatic optimization by the compiler. (4) We demonstrate that crucial algorithms like Cholesky decomposition as well as the algorithm by Floyd and Warshall by can be efficiently supported. (5) Extensive experiments on runtime efficiency, cache usage and energy consumption demonstrate the profit of our approach. We believe that future compilers could translate nested loops into cache-oblivious loops either fully automatic or by a user-guided analysis of the data dependency.},
  archive  = {J},
  author   = {Christian Böhm and Martin Perdacher and Claudia Plant},
  doi      = {10.1109/TBDATA.2018.2830378},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {2},
  pages    = {241-254},
  title    = {A novel hilbert curve for cache-locality preserving loops},
  volume   = {7},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Algorithms of unconstrained non-negative latent factor
analysis for recommender systems. <em>IEEE Transactions on Big
Data</em>, <em>7</em>(1), 227–240. (<a
href="https://doi.org/10.1109/TBDATA.2019.2916868">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Non-negativity is vital for a latent factor (LF)-based model to preserve the important feature of a high-dimensional and sparse (HiDS) matrix in recommender systems, i.e., none of its entries is negative. Current non-negative models rely on constraints-combined training schemes. However, they lack flexibility, scalability, or compatibility with general training schemes. This work aims to perform unconstrained non-negative latent factor analysis (UNLFA) on HiDS matrices. To do so, we innovatively transfer the non-negativity constraints from the decision parameters to the output LFs, and connect them through a single-element-dependent mapping function. Then we theoretically prove that by making a mapping function fulfill specific conditions, the resultant model is able to represent the original one precisely. We subsequently design highly efficient UNLFA algorithms for recommender systems. Experimental results on four industrial-size HiDS matrices demonstrate that compared with four state-of-the-art non-negative models, a UNLFA-based model obtains advantage in prediction accuracy for missing data and computational efficiency. Moreover, such high performance is achieved through its unconstrained training process which is compatible with various general training schemes, on the premise of fulfilling non-negativity constraints. Hence, UNLFA algorithms are highly valuable for industrial applications with the need of performing non-negative latent factor analysis on HiDS matrices.},
  archive  = {J},
  author   = {Xin Luo and Mengchu Zhou and Shuai Li and Di Wu and Zhigang Liu and Mingsheng Shang},
  doi      = {10.1109/TBDATA.2019.2916868},
  journal  = {IEEE Transactions on Big Data},
  month    = {3},
  number   = {1},
  pages    = {227-240},
  title    = {Algorithms of unconstrained non-negative latent factor analysis for recommender systems},
  volume   = {7},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). RoD: Evaluating the risk of data disclosure using noise
estimation for differential privacy. <em>IEEE Transactions on Big
Data</em>, <em>7</em>(1), 214–226. (<a
href="https://doi.org/10.1109/TBDATA.2019.2916108">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Differential privacy is a paradigm of big data privacy protection that offers protection even when an attacker has arbitrary background knowledge in advance. Consequently, it is viewed as a reliable protection mechanism for sensitive information. Differential privacy introduces noise, such as Laplace noise, to obfuscate the true values in a data set while preserving its statistical properties. However, a large amount of Laplace noise added into a data set is typically defined by the discursive scale parameter of Laplace distribution. The privacy budget &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$\varepsilon$&lt;/tex-math&gt;&lt;/inline-formula&gt; in differential privacy has been theoretically interpreted, but the implication on the risk of data disclosure ( RoD ) in practice has not yet been well studied. Moreover, choosing an appropriate value for &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$\varepsilon$&lt;/tex-math&gt;&lt;/inline-formula&gt; is not straightforward because it considerably affects the level of privacy in a data set. In this paper, we define and evaluate RoD in a data set with either numerical or binary attributes for numerical or counting queries with multiple attributes based on noise estimation. Through confidence probability of noise estimation, we provide a simple method to select the privacy budget &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$\varepsilon$&lt;/tex-math&gt;&lt;/inline-formula&gt; for differential privacy and associate differential privacy with &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$k$&lt;/tex-math&gt;&lt;/inline-formula&gt; -anonymization. Finally, we show the relationship between the RoD and &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$\varepsilon$&lt;/tex-math&gt;&lt;/inline-formula&gt; as well as between &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$\varepsilon$&lt;/tex-math&gt;&lt;/inline-formula&gt; and &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$k$&lt;/tex-math&gt;&lt;/inline-formula&gt; in our experimental results. To the best of our knowledge, this is the first study using the quantity of noise as a bridge to evaluate RoD for multiple attributes (either numerical or binary data) and determine the relationship between differential privacy and &lt;inline-formula&gt;&lt;tex-math notation=&quot;LaTeX&quot;&gt;$k$&lt;/tex-math&gt;&lt;/inline-formula&gt; -anonymization.},
  archive  = {J},
  author   = {Yao-Tung Tsou and Hung-Li Chen and Jia-Yang Chen},
  doi      = {10.1109/TBDATA.2019.2916108},
  journal  = {IEEE Transactions on Big Data},
  month    = {3},
  number   = {1},
  pages    = {214-226},
  title    = {RoD: Evaluating the risk of data disclosure using noise estimation for differential privacy},
  volume   = {7},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Research on escaping the big-data traps in O2O service
recommendation strategy. <em>IEEE Transactions on Big Data</em>,
<em>7</em>(1), 199–213. (<a
href="https://doi.org/10.1109/TBDATA.2019.2915798">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Internet business can be divided into two categories: pure online business and Online to Offline (O2O) business. Currently, the recommendation technology for online business is maturing, such as news, movies, products, and so forth. However, traditional recommendation technology can easily cause the overcrowding at some O2O services because of the big data traps. In the end, the users’ experience with the O2O service recommendation is useless or very poor because they have to wait for a long time and can&#39;t enjoy the service immediately. Hence, how to improve the performance of O2O service recommendation has become a vital problem. To solve the problem, this paper proposes a research framework based on the continuous feedback learning mechanism between cyber layer and social layer. Then, the continuous feedback ideas are implemented in the design of the O2O service recommendation strategy step by step. Furthermore, the computational experiment system is constructed to perform performance analysis of these service strategies. The results show that our research framework is conductive to help O2O service recommendation to escape the big-data traps and to improve user experience.},
  archive  = {J},
  author   = {Xiao Xue and Shuai Huangfu and Lejun Zhang and Shufang Wang},
  doi      = {10.1109/TBDATA.2019.2915798},
  journal  = {IEEE Transactions on Big Data},
  month    = {3},
  number   = {1},
  pages    = {199-213},
  title    = {Research on escaping the big-data traps in O2O service recommendation strategy},
  volume   = {7},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Effective and efficient content redundancy detection of web
videos. <em>IEEE Transactions on Big Data</em>, <em>7</em>(1), 187–198.
(<a href="https://doi.org/10.1109/TBDATA.2019.2913674">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Currently, an unprecedentedly vast amount of videos are hosted on the Internet and shared by users across the world. Within these videos, a considerable portion is duplicate or near-duplicate. Consequently, building an effective yet efficient content-based redundancy detection system is of importance, as this research would be beneficial to a variety of applications. Despite the progress in this field, designing a practical detection system for web videos continues to be difficult, because of the contradictions between the accuracy and speed requirements. In this paper, we propose a novel near-duplicate video detection system, CompoundEyes, whose design philosophy deviates from the conventional feature-centered paradigm. Instead, the focus of our system has been shifted from the design of an advanced feature representation to the design of system architecture. This design methodology not only ensures a decent detection accuracy by the collaboration of the classifiers but also substantially accelerates the detection speed due to the low dimensionality of the feature representations and the exploitation of the parallelism among the components. Experiments have been conducted to demonstrate that the CompoundEyes is both accurate and fast.},
  archive  = {J},
  author   = {Yixin Chen and Dongsheng Li and Yu Hua and Wenbo He},
  doi      = {10.1109/TBDATA.2019.2913674},
  journal  = {IEEE Transactions on Big Data},
  month    = {3},
  number   = {1},
  pages    = {187-198},
  title    = {Effective and efficient content redundancy detection of web videos},
  volume   = {7},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Incorporating data context to cost-effectively automate
end-to-end data wrangling. <em>IEEE Transactions on Big Data</em>,
<em>7</em>(1), 169–186. (<a
href="https://doi.org/10.1109/TBDATA.2019.2907588">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The process of preparing potentially large and complex data sets for further analysis or manual examination is often called data wrangling. In classical warehousing environments, the steps in such a process are carried out using Extract-Transform-Load platforms, with significant manual involvement in specifying, configuring or tuning many of them. In typical big data applications, we need to ensure that all wrangling steps, including web extraction, selection, integration and cleaning, benefit from automation wherever possible. Towards this goal, in the paper we: (i) introduce a notion of data context, which associates portions of a target schema with extensional data of types that are commonly available; (ii) define a scalable methodology to bootstrap an end-to-end data wrangling process based on data profiling; (iii) describe how data context is used to inform automation in several steps within wrangling, specifically, matching, value format transformation, data repair, and mapping generation and selection to optimise the accuracy, consistency and relevance of the result; and (iv) we evaluate the approach with real estate data and financial data, showing substantial improvements in the results of automated wrangling.},
  archive  = {J},
  author   = {Martin Koehler and Edward Abel and Alex Bogatu and Cristina Civili and Lacramioara Mazilu and Nikolaos Konstantinou and Alvaro A.A. Fernandes and John Keane and Leonid Libkin and Norman W. Paton},
  doi      = {10.1109/TBDATA.2019.2907588},
  journal  = {IEEE Transactions on Big Data},
  month    = {3},
  number   = {1},
  pages    = {169-186},
  title    = {Incorporating data context to cost-effectively automate end-to-end data wrangling},
  volume   = {7},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fast communication-efficient spectral clustering over
distributed data. <em>IEEE Transactions on Big Data</em>, <em>7</em>(1),
158–168. (<a href="https://doi.org/10.1109/TBDATA.2019.2907985">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The last decades have seen a surge of interests in distributed computing thanks to advances in clustered computing and big data technology. Existing distributed algorithms typically assume all the data are already in one place , and divide the data and conquer on multiple machines. However, it is increasingly often that the data are located at a number of distributed sites, and one wishes to compute over all the data with low communication overhead. For spectral clustering, we propose a novel framework that enables its computation over such distributed data, with “minimal” communications while a major speedup in computation. The loss in accuracy is negligible compared to the non-distributed setting. Our approach allows local parallel computing at where the data are located, thus turns the distributed nature of the data into a blessing; the speedup is most substantial when the data are evenly distributed across sites. Experiments on synthetic and large UC Irvine datasets show almost no loss in accuracy with our approach while a 2x speedup under various settings with two distributed sites. As the transmitted data need not be in their original form, our framework readily addresses the privacy concern for data sharing in distributed computing.},
  archive  = {J},
  author   = {Donghui Yan and Yingjie Wang and Jin Wang and Guodong Wu and Honggang Wang},
  doi      = {10.1109/TBDATA.2019.2907985},
  journal  = {IEEE Transactions on Big Data},
  month    = {3},
  number   = {1},
  pages    = {158-168},
  title    = {Fast communication-efficient spectral clustering over distributed data},
  volume   = {7},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). K-nearest neighbor search by random projection forests.
<em>IEEE Transactions on Big Data</em>, <em>7</em>(1), 147–157. (<a
href="https://doi.org/10.1109/TBDATA.2019.2908178">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {K-nearest neighbor (kNN) search is an important problem in data mining and knowledge discovery. Inspired by the huge success of tree-based methodology and ensemble methods over the last decades, we propose a new method for kNN search, random projection forests (rpForests). rpForests finds nearest neighbors by combining multiple kNN-sensitive trees with each constructed recursively through a series of random projections. As demonstrated by experiments on a wide collection of real datasets, our method achieves a remarkable accuracy in terms of fast decaying missing rate of kNNs and that of discrepancy in the k-th nearest neighbor distances. rpForests has a very low computational complexity as a tree-based methodology. The ensemble nature of rpForests makes it easily parallelized to run on clustered or multicore computers; the running time is expected to be nearly inversely proportional to the number of cores or machines. We give theoretical insights on rpForests by showing the exponential decay of neighboring points being separated by ensemble random projection trees when the ensemble size increases. Our theory can also be used to refine the choice of random projections in the growth of rpForests; experiments show that the effect is remarkable.},
  archive  = {J},
  author   = {Donghui Yan and Yingjie Wang and Jin Wang and Honggang Wang and Zhenpeng Li},
  doi      = {10.1109/TBDATA.2019.2908178},
  journal  = {IEEE Transactions on Big Data},
  month    = {3},
  number   = {1},
  pages    = {147-157},
  title    = {K-nearest neighbor search by random projection forests},
  volume   = {7},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). AMIC: An adaptive information theoretic method to identify
multi-scale temporal correlations in big time series data. <em>IEEE
Transactions on Big Data</em>, <em>7</em>(1), 128–146. (<a
href="https://doi.org/10.1109/TBDATA.2019.2907987">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Recent development in computing, sensing and crowd-sourced data have resulted in an explosion in the availability of quantitative information. The possibilities of analyzing this so-called Big Data to inform research and the decision-making process are virtually endless. In general, analyses have to be done across multiple data sets in order to bring out the most value of Big Data. A first important step is to identify temporal correlations between data sets. Given the characteristics of Big Data in terms of volume and velocity, techniques that identify correlations not only need to be fast and scalable, but also need to help users in ordering the correlations across temporal scales so that they can focus on important relationships. In this paper, we present AMIC (Adaptive Mutual Information-based Correlation), a method based on mutual information to identify correlations at multiple temporal scales in large time series. Discovered correlations are suggested to users in an order based on the strength of the relationships. Our method supports an adaptive streaming technique that minimizes duplicated computation and is implemented on top of Apache Spark for scalability. We also provide a comprehensive evaluation on the effectiveness and the scalability of AMIC using both synthetic and real-world data sets.},
  archive  = {J},
  author   = {Nguyen Ho and Huy Vo and Mai Vu and Torben Bach Pedersen},
  doi      = {10.1109/TBDATA.2019.2907987},
  journal  = {IEEE Transactions on Big Data},
  month    = {3},
  number   = {1},
  pages    = {128-146},
  title    = {AMIC: An adaptive information theoretic method to identify multi-scale temporal correlations in big time series data},
  volume   = {7},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deadline-aware cost optimization for spark. <em>IEEE
Transactions on Big Data</em>, <em>7</em>(1), 115–127. (<a
href="https://doi.org/10.1109/TBDATA.2019.2908188">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We present OptEx, a closed-form model of job execution on Apache Spark, a popular parallel processing engine. To the best of our knowledge, OptEx is the first work that analytically models job completion time on Spark. The model can be used to estimate the completion time of a given Spark job on a cloud, with respect to the size of the input dataset, the number of iterations, and the number of nodes comprising the underlying cluster. Experimental results demonstrate that OptEx yields a mean relative error of 6 percent in estimating the job completion time. Furthermore, the model can be applied for estimating the cost-optimal cluster composition for running a given Spark job on a cloud under a completion deadline specified in the SLO (i.e., Service Level Objective). We show experimentally that OptEx is able to correctly estimate the required cluster composition for running a given Spark job under a given SLO deadline with an accuracy of 98 percent. We also provide a tool which can classify Spark jobs into job categories based on bisimilarity analysis on lineage graphs collected from the given jobs.},
  archive  = {J},
  author   = {Subhajit Sidhanta and Wojciech Golab and Supratik Mukhopadhyay},
  doi      = {10.1109/TBDATA.2019.2908188},
  journal  = {IEEE Transactions on Big Data},
  month    = {3},
  number   = {1},
  pages    = {115-127},
  title    = {Deadline-aware cost optimization for spark},
  volume   = {7},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hierarchical density-based clustering using MapReduce.
<em>IEEE Transactions on Big Data</em>, <em>7</em>(1), 102–114. (<a
href="https://doi.org/10.1109/TBDATA.2019.2907624">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Hierarchical density-based clustering is a powerful tool for exploratory data analysis, which can play an important role in the understanding and organization of datasets. However, its applicability to large datasets is limited because the computational complexity of hierarchical clustering methods has a quadratic lower bound in the number of objects to be clustered. MapReduce is a popular programming model to speed up data mining and machine learning algorithms operating on large, possibly distributed datasets. In the literature, there have been attempts to parallelize algorithms such as Single-Linkage, which in principle can also be extended to the broader scope of hierarchical density-based clustering, but hierarchical clustering algorithms are inherently difficult to parallelize with MapReduce. In this paper, we discuss why adapting previous approaches to parallelize Single-Linkage clustering using MapReduce leads to very inefficient solutions when one wants to compute density-based clustering hierarchies. Preliminarily, we discuss one such solution, which is based on an exact, yet very computationally demanding, random blocks parallelization scheme. To be able to efficiently apply hierarchical density-based clustering to large datasets using MapReduce, we then propose a different parallelization scheme that computes an approximate clustering hierarchy based on a much faster, recursive sampling approach. This approach is based on HDBSCAN*, the state-of-the-art hierarchical density-based clustering algorithm, combined with a data summarization technique called data bubbles. The proposed method is evaluated in terms of both runtime and quality of the approximation on a number of datasets, showing its effectiveness and scalability.},
  archive  = {J},
  author   = {Joelson Antônio dos Santos and Talat Iqbal Syed and Murilo C. Naldi and Ricardo J. G. B. Campello and Joerg Sander},
  doi      = {10.1109/TBDATA.2019.2907624},
  journal  = {IEEE Transactions on Big Data},
  month    = {3},
  number   = {1},
  pages    = {102-114},
  title    = {Hierarchical density-based clustering using MapReduce},
  volume   = {7},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Automated labeling for robotic autonomous navigation through
multi-sensory semi-supervised learning on big data. <em>IEEE
Transactions on Big Data</em>, <em>7</em>(1), 93–101. (<a
href="https://doi.org/10.1109/TBDATA.2019.2892462">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Imitation learning holds the promise to address challenging robotic tasks such as autonomous navigation. It however requires a human supervisor to oversee the training process and send correct control commands to robots without feedback, which is always prone to error and expensive. To minimize human involvement and avoid manual labeling of data in the robotic autonomous navigation with imitation learning, this paper proposes a novel semi-supervised imitation learning solution based on a multi-sensory design. This solution includes a suboptimal sensor policy based on sensor fusion to automatically label states encountered by a robot to avoid human supervision during training. In addition, a recording policy is developed to throttle the adversarial affect of learning too much from the suboptimal sensor policy. As a result, this solution allows the robot to learn a navigation policy in a self-supervised manner without human intervention after the initial data collection. With extensive experiments in indoor environments, this solution can achieve near human performance in most of the tasks and even surpasses human performance in case of unexpected events such as hardware failures or human operation errors. To best of our knowledge, this is the first work that synthesizes sensor fusion and imitation learning to enable robotic autonomous navigation in the real world without human supervision.},
  archive  = {J},
  author   = {Junhong Xu and Shangyue Zhu and Hanqing Guo and Shaoen Wu},
  doi      = {10.1109/TBDATA.2019.2892462},
  journal  = {IEEE Transactions on Big Data},
  month    = {3},
  number   = {1},
  pages    = {93-101},
  title    = {Automated labeling for robotic autonomous navigation through multi-sensory semi-supervised learning on big data},
  volume   = {7},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Country image in COVID-19 pandemic: A case study of china.
<em>IEEE Transactions on Big Data</em>, <em>7</em>(1), 81–92. (<a
href="https://doi.org/10.1109/TBDATA.2020.3023459">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Country image has a profound influence on international relations and economic development. In the worldwide outbreak of COVID-19, countries and their people display different reactions, resulting in diverse perceived images among foreign public. Therefore, in this article, we take China as a specific and typical case and investigate its image with aspect-based sentiment analysis on a large-scale Twitter dataset. To our knowledge, this is the first study to explore country image in such a fine-grained way. To perform the analysis, we first build a manually-labeled Twitter dataset with aspect-level sentiment annotations. Afterward, we conduct the aspect-based sentiment analysis with BERT to explore the image of China. We discover an overall sentiment change from non-negative to negative in the general public, and explain it with the increasing mentions of negative ideology-related aspects and decreasing mentions of non-negative fact-based aspects. Further investigations into different groups of Twitter users, including U.S. Congress members, English media, and social bots, reveal different patterns in their attitudes toward China. This article provides a deeper understanding of the changing image of China in COVID-19 pandemic. Our research also demonstrates how aspect-based sentiment analysis can be applied in social science researches to deliver valuable insights.},
  archive  = {J},
  author   = {Huimin Chen and Zeyu Zhu and Fanchao Qi and Yining Ye and Zhiyuan Liu and Maosong Sun and Jianbin Jin},
  doi      = {10.1109/TBDATA.2020.3023459},
  journal  = {IEEE Transactions on Big Data},
  month    = {3},
  number   = {1},
  pages    = {81-92},
  title    = {Country image in COVID-19 pandemic: A case study of china},
  volume   = {7},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Misinformation during the COVID-19 outbreak in china:
Cultural, social and political entanglements. <em>IEEE Transactions on
Big Data</em>, <em>7</em>(1), 69–80. (<a
href="https://doi.org/10.1109/TBDATA.2021.3055758">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Not only did COVID-19 give rise to a global pandemic, but also it resulted in an infodemic comprising misinformation, rumor, and propaganda. The consequences of this infodemic can erode public trust, impede the containment of the virus, and outlive the pandemic itself. The evolving and fragmented media landscape, particularly the extensive use of social media, is a crucial driver of the spread of misinformation. Focusing on the Chinese social media Weibo, we collected four million tweets, from December 9, 2019, to April 4, 2020, examining misinformation identified by the fact-checking platform Tencent–a leading Chinese tech giant. Our results show that the evolution of misinformation follows an issue-attention cycle pertaining to topics such as city lockdown, cures and preventive measures, school reopening, and foreign countries. Sensational and emotionally reassuring misinformation characterizes the whole issue-attention cycle, with misinformation on cures and prevention flooding social media. We also study the evolution of sentiment and observe that positive sentiment dominated over the course of Covid, which may be due to the unique characteristic of “positive energy” on Chinese social media. Lastly, we study the media landscape during Covid via a case study on a controversial unproven cure known as Shuanghuanglian, which testifies to the importance of scientific communication in a plague. Our findings shed light on the distinct characteristics of misinformation and its cultural, social, and political implications, during the COVID-19 pandemic. The study also offers insights into combating misinformation in China and across the world at large.},
  archive  = {J},
  author   = {Yan Leng and Yujia Zhai and Shaojing Sun and Yifei Wu and Jordan Selzer and Sharon Strover and Hezhao Zhang and Anfan Chen and Ying Ding},
  doi      = {10.1109/TBDATA.2021.3055758},
  journal  = {IEEE Transactions on Big Data},
  month    = {3},
  number   = {1},
  pages    = {69-80},
  title    = {Misinformation during the COVID-19 outbreak in china: Cultural, social and political entanglements},
  volume   = {7},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Measuring human and economic activity from satellite imagery
to support city-scale decision-making during COVID-19 pandemic. <em>IEEE
Transactions on Big Data</em>, <em>7</em>(1), 56–68. (<a
href="https://doi.org/10.1109/TBDATA.2020.3032839">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The COVID-19 outbreak forced governments worldwide to impose lockdowns and quarantines to prevent virus transmission. As a consequence, there are disruptions in human and economic activities all over the globe. The recovery process is also expected to be rough. Economic activities impact social behaviors, which leave signatures in satellite images that can be automatically detected and classified. Satellite imagery can support the decision-making of analysts and policymakers by providing a different kind of visibility into the unfolding economic changes. In this article, we use a deep learning approach that combines strategic location sampling and an ensemble of lightweight convolutional neural networks (CNNs) to recognize specific elements in satellite images that could be used to compute economic indicators based on it, automatically. This CNN ensemble framework ranked third place in the US Department of Defense xView challenge, the most advanced benchmark for object detection in satellite images. We show the potential of our framework for temporal analysis using the US IARPA Function Map of the World (fMoW) dataset. We also show results on real examples of different sites before and after the COVID-19 outbreak to illustrate different measurable indicators. Our code and annotated high-resolution aerial scenes before and after the outbreak are available on GitHub. 1 1.https://github.com/maups/covid19-satellite-analysis. },
  archive  = {J},
  author   = {Rodrigo Minetto and Maurício Pamplona Segundo and Gilbert Rotich and Sudeep Sarkar},
  doi      = {10.1109/TBDATA.2020.3032839},
  journal  = {IEEE Transactions on Big Data},
  month    = {3},
  number   = {1},
  pages    = {56-68},
  title    = {Measuring human and economic activity from satellite imagery to support city-scale decision-making during COVID-19 pandemic},
  volume   = {7},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An epidemiological neural network exploiting dynamic graph
structured data applied to the COVID-19 outbreak. <em>IEEE Transactions
on Big Data</em>, <em>7</em>(1), 45–55. (<a
href="https://doi.org/10.1109/TBDATA.2020.3032755">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {With the recent COVID-19 outbreak, we have assisted to the development of new epidemic models or the application of existing methodologies to predict the virus spread and to analyze how the different lock-down strategies can effectively influence the epidemic diffusion. In this paper, we propose a novel machine learning based framework able to estimate the parameters of any epidemiological model, such as contact rates and recovery rates, based on static and dynamic features of places. In particular, we model mobility data through a graph series whose spatial and temporal features are investigated by combining Graph Convolutional Neural Networks (GCNs) and Long short-term memories (LSTMs) in order to infer the parameters of SIR and SIRD models. We evaluate the proposed approach using data related to the COVID-19 dynamics in Italy and we compare the forecasts of the trained model with available data about the epidemic spread.},
  archive  = {J},
  author   = {Valerio La Gatta and Vincenzo Moscato and Marco Postiglione and Giancarlo Sperlí},
  doi      = {10.1109/TBDATA.2020.3032755},
  journal  = {IEEE Transactions on Big Data},
  month    = {3},
  number   = {1},
  pages    = {45-55},
  title    = {An epidemiological neural network exploiting dynamic graph structured data applied to the COVID-19 outbreak},
  volume   = {7},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Relational learning improves prediction of mortality in
COVID-19 in the intensive care unit. <em>IEEE Transactions on Big
Data</em>, <em>7</em>(1), 38–44. (<a
href="https://doi.org/10.1109/TBDATA.2020.3048644">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Traditional Machine Learning (ML) models have had limited success in predicting Coronoavirus-19 (COVID-19) outcomes using Electronic Health Record (EHR) data partially due to not effectively capturing the inter-connectivity patterns between various data modalities. In this work, we propose a novel framework that utilizes relational learning based on a heterogeneous graph model (HGM) for predicting mortality at different time windows in COVID-19 patients within the intensive care unit (ICU). We utilize the EHRs of one of the largest and most diverse patient populations across five hospitals in major health system in New York City. In our model, we use an LSTM for processing time varying patient data and apply our proposed relational learning strategy in the final output layer along with other static features. Here, we replace the traditional softmax layer with a Skip-Gram relational learning strategy to compare the similarity between a patient and outcome embedding representation. We demonstrate that the construction of a HGM can robustly learn the patterns classifying patient representations of outcomes through leveraging patterns within the embeddings of similar patients. Our experimental results show that our relational learning-based HGM model achieves higher area under the receiver operating characteristic curve (auROC) than both comparator models in all prediction time windows, with dramatic improvements to recall.},
  archive  = {J},
  author   = {Tingyi Wanyan and Akhil Vaid and Jessica K De Freitas and Sulaiman Somani and Riccardo Miotto and Girish N. Nadkarni and Ariful Azad and Ying Ding and Benjamin S. Glicksberg},
  doi      = {10.1109/TBDATA.2020.3048644},
  journal  = {IEEE Transactions on Big Data},
  month    = {3},
  number   = {1},
  pages    = {38-44},
  title    = {Relational learning improves prediction of mortality in COVID-19 in the intensive care unit},
  volume   = {7},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Leveraging structured biological knowledge for
counterfactual inference: A case study of viral pathogenesis. <em>IEEE
Transactions on Big Data</em>, <em>7</em>(1), 25–37. (<a
href="https://doi.org/10.1109/TBDATA.2021.3050680">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Counterfactual inference is a useful tool for comparing outcomes of interventions on complex systems. It requires us to represent the system in form of a structural causal model, complete with a causal diagram, probabilistic assumptions on exogenous variables, and functional assignments. Specifying such models can be extremely difficult in practice. The process requires substantial domain expertise, and does not scale easily to large systems, multiple systems, or novel system modifications. At the same time, many application domains, such as molecular biology, are rich in structured causal knowledge that is qualitative in nature. This article proposes a general approach for querying a causal biological knowledge graph, and converting the qualitative result into a quantitative structural causal model that can learn from data to answer the question. We demonstrate the feasibility, accuracy and versatility of this approach using two case studies in systems biology. The first demonstrates the appropriateness of the underlying assumptions and the accuracy of the results. The second demonstrates the versatility of the approach by querying a knowledge base for the molecular determinants of a severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)-induced cytokine storm, and performing counterfactual inference to estimate the causal effect of medical countermeasures for severely ill patients.},
  archive  = {J},
  author   = {Jeremy Zucker and Kaushal Paneri and Sara Mohammad-Taheri and Somya Bhargava and Pallavi Kolambkar and Craig Bakker and Jeremy Teuton and Charles Tapley Hoyt and Kristie Oxford and Robert Ness and Olga Vitek},
  doi      = {10.1109/TBDATA.2021.3050680},
  journal  = {IEEE Transactions on Big Data},
  month    = {3},
  number   = {1},
  pages    = {25-37},
  title    = {Leveraging structured biological knowledge for counterfactual inference: A case study of viral pathogenesis},
  volume   = {7},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). COVID-19 chest CT image segmentation network by multi-scale
fusion and enhancement operations. <em>IEEE Transactions on Big
Data</em>, <em>7</em>(1), 13–24. (<a
href="https://doi.org/10.1109/TBDATA.2021.3056564">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {A novel coronavirus disease 2019 (COVID-19) was detected and has spread rapidly across various countries around the world since the end of the year 2019. Computed Tomography (CT) images have been used as a crucial alternative to the time-consuming RT-PCR test. However, pure manual segmentation of CT images faces a serious challenge with the increase of suspected cases, resulting in urgent requirements for accurate and automatic segmentation of COVID-19 infections. Unfortunately, since the imaging characteristics of the COVID-19 infection are diverse and similar to the backgrounds, existing medical image segmentation methods cannot achieve satisfactory performance. In this article, we try to establish a new deep convolutional neural network tailored for segmenting the chest CT images with COVID-19 infections. We first maintain a large and new chest CT image dataset consisting of 165,667 annotated chest CT images from 861 patients with confirmed COVID-19. Inspired by the observation that the boundary of the infected lung can be enhanced by adjusting the global intensity, in the proposed deep CNN, we introduce a feature variation block which adaptively adjusts the global properties of the features for segmenting COVID-19 infection. The proposed FV block can enhance the capability of feature representation effectively and adaptively for diverse cases. We fuse features at different scales by proposing Progressive Atrous Spatial Pyramid Pooling to handle the sophisticated infection areas with diverse appearance and shapes. The proposed method achieves state-of-the-art performance. Dice similarity coefficients are 0.987 and 0.726 for lung and COVID-19 segmentation, respectively. We conducted experiments on the data collected in China and Germany and show that the proposed deep CNN can produce impressive performance effectively. The proposed network enhances the segmentation ability of the COVID-19 infection, makes the connection with other techniques and contributes to the development of remedying COVID-19 infection.},
  archive  = {J},
  author   = {Qingsen Yan and Bo Wang and Dong Gong and Chuan Luo and Wei Zhao and Jianhu Shen and Jingyang Ai and Qinfeng Shi and Yanning Zhang and Shuo Jin and Liang Zhang and Zheng You},
  doi      = {10.1109/TBDATA.2021.3056564},
  journal  = {IEEE Transactions on Big Data},
  month    = {3},
  number   = {1},
  pages    = {13-24},
  title    = {COVID-19 chest CT image segmentation network by multi-scale fusion and enhancement operations},
  volume   = {7},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). COVID-19-CT-CXR: A freely accessible and weakly labeled
chest x-ray and CT image collection on COVID-19 from biomedical
literature. <em>IEEE Transactions on Big Data</em>, <em>7</em>(1), 3–12.
(<a href="https://doi.org/10.1109/TBDATA.2020.3035935">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The latest threat to global health is the COVID-19 outbreak. Although there exist large datasets of chest X-rays (CXR) and computed tomography (CT) scans, few COVID-19 image collections are currently available due to patient privacy. At the same time, there is a rapid growth of COVID-19-relevant articles in the biomedical literature, including those that report findings on radiographs. Here, we present COVID-19-CT-CXR, a public database of COVID-19 CXR and CT images, which are automatically extracted from COVID-19-relevant articles from the PubMed Central Open Access (PMC-OA) Subset. We extracted figures, associated captions, and relevant figure descriptions in the article and separated compound figures into subfigures. Because a large portion of figures in COVID-19 articles are not CXR or CT, we designed a deep-learning model to distinguish them from other figure types and to classify them accordingly. The final database includes 1,327 CT and 263 CXR images (as of May 9, 2020) with their relevant text. To demonstrate the utility of COVID-19-CT-CXR, we conducted four case studies. (1) We show that COVID-19-CT-CXR, when used as additional training data, is able to contribute to improved deep-learning (DL) performance for the classification of COVID-19 and non-COVID-19 CT. (2) We collected CT images of influenza, another common infectious respiratory illness that may present similarly to COVID-19, and fine-tuned a baseline deep neural network to distinguish a diagnosis of COVID-19, influenza, or normal or other types of diseases on CT. (3) We fine-tuned an unsupervised one-class classifier from non-COVID-19 CXR and performed anomaly detection to detect COVID-19 CXR. (4) From text-mined captions and figure descriptions, we compared 15 clinical symptoms and 20 clinical findings of COVID-19 versus those of influenza to demonstrate the disease differences in the scientific publications. Our database is unique, as the figures are retrieved along with relevant text with fine-grained descriptions, and it can be extended easily in the future. We believe that our work is complementary to existing resources and hope that it will contribute to medical image analysis of the COVID-19 pandemic. The dataset, code, and DL models are publicly available at https://github.com/ncbi-nlp/COVID-19-CT-CXR .},
  archive  = {J},
  author   = {Yifan Peng and Yuxing Tang and Sungwon Lee and Yingying Zhu and Ronald M. Summers and Zhiyong Lu},
  doi      = {10.1109/TBDATA.2020.3035935},
  journal  = {IEEE Transactions on Big Data},
  month    = {3},
  number   = {1},
  pages    = {3-12},
  title    = {COVID-19-CT-CXR: A freely accessible and weakly labeled chest X-ray and CT image collection on COVID-19 from biomedical literature},
  volume   = {7},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Guest editorial: AI for COVID-19. <em>IEEE Transactions on
Big Data</em>, <em>7</em>(1), 1–2. (<a
href="https://doi.org/10.1109/TBDATA.2021.3057121">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The eight articles in this special section aim to provide a forum for AI and data science research in understanding, predicting, forecasting, and modeling the COVID-19 crisis and potential future outbreaks. The accepted papers present diverse perspectives on understanding and combating the COVID-19 pandemic using the AI toolbox, including how to extract knowledge about COVID-19 from scientific publications, model and control the pandemic’s spread, and understand the diffusion of false information throughout online social media.},
  archive  = {J},
  author   = {Yuxiao Dong and Marinka Zitnik},
  doi      = {10.1109/TBDATA.2021.3057121},
  journal  = {IEEE Transactions on Big Data},
  month    = {3},
  number   = {1},
  pages    = {1-2},
  title    = {Guest editorial: AI for COVID-19},
  volume   = {7},
  year     = {2021},
}
</textarea>
</details></li>
</ul>

</body>
</html>
