<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TAP_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tap---18">TAP - 18</h2>
<ul>
<li><details>
<summary>
(2021). Introduction to the special issue on SAP 2021. <em>TAP</em>,
<em>18</em>(4), 1–2. (<a href="https://doi.org/10.1145/3486577">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_TAP},
  doi          = {10.1145/3486577},
  journal      = {ACM Transactions on Applied Perception},
  month        = {10},
  number       = {4},
  pages        = {1-2},
  shortjournal = {ACM Trans. Appl. Perc.},
  title        = {Introduction to the special issue on SAP 2021},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Facial feature manipulation for trait portrayal in realistic
and cartoon-rendered characters. <em>TAP</em>, <em>18</em>(4), 1–8. (<a
href="https://doi.org/10.1145/3486579">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Previous perceptual studies on human faces have shown that specific facial features have consistent effects on perceived personality and appeal, but it remains unclear if and how findings relate to perception of virtual characters. For example, wider human faces have been found to appear more aggressive and dominant, whereas studies on virtual characters have shown opposite trends but have suffered from significant eeriness of exaggerated features. In this study, we use highly realistic virtual faces obtained from 3D scanning, as well as cartoon-rendered counterparts retaining facial proportions. We assess the effects of facial width and eye size on perceptions of appeal, trustworthiness, aggressiveness, dominance, and eeriness. Our manipulations did not affect eeriness, and we find the same perceptual trends previously reported for human faces.},
  archive      = {J_TAP},
  doi          = {10.1145/3486579},
  journal      = {ACM Transactions on Applied Perception},
  month        = {10},
  number       = {4},
  pages        = {1-8},
  shortjournal = {ACM Trans. Appl. Perc.},
  title        = {Facial feature manipulation for trait portrayal in realistic and cartoon-rendered characters},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Do prosody and embodiment influence the perceived
naturalness of conversational agents’ speech? <em>TAP</em>,
<em>18</em>(4), 1–15. (<a
href="https://doi.org/10.1145/3486580">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For conversational agents’ speech, either all possible sentences have to be prerecorded by voice actors or the required utterances can be synthesized. While synthesizing speech is more flexible and economic in production, it also potentially reduces the perceived naturalness of the agents among others due to mistakes at various linguistic levels. In our article, we are interested in the impact of adequate and inadequate prosody, here particularly in terms of accent placement, on the perceived naturalness and aliveness of the agents. We compare (1) inadequate prosody, as generated by off-the-shelf text-to-speech (TTS) engines with synthetic output; (2) the same inadequate prosody imitated by trained human speakers; and (3) adequate prosody produced by those speakers. The speech was presented either as audio-only or by embodied, anthropomorphic agents, to investigate the potential masking effect by a simultaneous visual representation of those virtual agents. To this end, we conducted an online study with 40 participants listening to four different dialogues each presented in the three Speech levels and the two Embodiment levels. Results confirmed that adequate prosody in human speech is perceived as more natural (and the agents are perceived as more alive) than inadequate prosody in both human (2) and synthetic speech (1). Thus, it is not sufficient to just use a human voice for an agents’ speech to be perceived as natural—it is decisive whether the prosodic realisation is adequate or not. Furthermore, and surprisingly, we found no masking effect by speaker embodiment, since neither a human voice with inadequate prosody nor a synthetic voice was judged as more natural, when a virtual agent was visible compared to the audio-only condition. On the contrary, the human voice was even judged as less “alive” when accompanied by a virtual agent. In sum, our results emphasize, on the one hand, the importance of adequate prosody for perceived naturalness, especially in terms of accents being placed on important words in the phrase, while showing, on the other hand, that the embodiment of virtual agents plays a minor role in the naturalness ratings of voices.},
  archive      = {J_TAP},
  doi          = {10.1145/3486580},
  journal      = {ACM Transactions on Applied Perception},
  month        = {10},
  number       = {4},
  pages        = {1-15},
  shortjournal = {ACM Trans. Appl. Perc.},
  title        = {Do prosody and embodiment influence the perceived naturalness of conversational agents’ speech?},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Stealth updates of visual information by leveraging change
blindness and computational visual morphing. <em>TAP</em>,
<em>18</em>(4), 1–17. (<a
href="https://doi.org/10.1145/3486581">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present an approach for covert visual updates by leveraging change blindness with computationally generated morphed images. To clarify the design parameters for intentionally suppressing change detection with morphing visuals, we investigated the visual change detection in three temporal behaviors: visual blank, eye-blink, and step-sequential changes. The results showed a robust trend of change blindness with a blank of more than 33.3 ms and with eye blink. Our sequential change study revealed that participants did not recognize changes until an average of 57% morphing toward another face in small change steps. In addition, changes went unnoticed until the end of morphing in more than 10% of all trials. Our findings should contribute to the design of covert visual updates without consuming users’ attention by leveraging change blindness with computational visual morphing.},
  archive      = {J_TAP},
  doi          = {10.1145/3486581},
  journal      = {ACM Transactions on Applied Perception},
  month        = {10},
  number       = {4},
  pages        = {1-17},
  shortjournal = {ACM Trans. Appl. Perc.},
  title        = {Stealth updates of visual information by leveraging change blindness and computational visual morphing},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Evaluating grasping visualizations and control modes in a VR
game. <em>TAP</em>, <em>18</em>(4), 1–14. (<a
href="https://doi.org/10.1145/3486582">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A primary goal of the Virtual Reality ( VR ) community is to build fully immersive and presence-inducing environments with seamless and natural interactions. To reach this goal, researchers are investigating how to best directly use our hands to interact with a virtual environment using hand tracking. Most studies in this field require participants to perform repetitive tasks. In this article, we investigate if results of such studies translate into a real application and game-like experience. We designed a virtual escape room in which participants interact with various objects to gather clues and complete puzzles. In a between-subjects study, we examine the effects of two input modalities (controllers vs. hand tracking) and two grasping visualizations (continuously tracked hands vs. virtual hands that disappear when grasping) on ownership, realism, efficiency, enjoyment, and presence. Our results show that ownership, realism, enjoyment, and presence increased when using hand tracking compared to controllers. Visualizing the tracked hands during grasps leads to higher ratings in one of our ownership questions and one of our enjoyment questions compared to having the virtual hands disappear during grasps as is common in many applications. We also confirm some of the main results of two studies that have a repetitive design in a more realistic gaming scenario that might be closer to a typical user experience.},
  archive      = {J_TAP},
  doi          = {10.1145/3486582},
  journal      = {ACM Transactions on Applied Perception},
  month        = {10},
  number       = {4},
  pages        = {1-14},
  shortjournal = {ACM Trans. Appl. Perc.},
  title        = {Evaluating grasping visualizations and control modes in a VR game},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An evaluation of screen parallax, haptic feedback, and
sensory-motor mismatch on near-field perception-action coordination in
VR. <em>TAP</em>, <em>18</em>(4), 1–16. (<a
href="https://doi.org/10.1145/3486583">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual reality ( VR ) displays have factors such as vergence-accommodation conflicts that negatively impact depth perception and cause users to misjudge distances to select objects. In addition, popular large-screen immersive displays present the depth of any target rendered through screen parallax information of points, which are encapsulated within stereoscopic voxels that are a distinct unit of space dictating how far an object is placed in front of or behind the screen. As they emanate from the viewers’ eyes (left and right center of projection), the density of voxels is higher in front of the screen (in regions of negative screen parallax) than it is behind the screen (in regions of positive screen parallax), implying a higher spatial resolution of depth in front of the screen than behind the screen. Our experiment implements a near-field fine-motor pick-and-place task in which users pick up a ring and place it around a targeted peg. The targets are arranged in a linear configuration of 3, 5, and 7 pegs along the front-and-back axis with the center peg placed in the same depth as the screen. We use this to evaluate how users manipulate objects in positive versus negative screen parallax space by the metrics of efficiency, accuracy, and economy of movement. In addition, we evaluate how users’ performance is moderated by haptic feedback and mismatch between visual and proprioceptive information. Our results reveal that users perform more efficiently in negative screen parallax space and that haptic feedback and visuo-proprioceptive mismatch have effects on placement efficiency. The implications of these findings are described in the later sections of the article.},
  archive      = {J_TAP},
  doi          = {10.1145/3486583},
  journal      = {ACM Transactions on Applied Perception},
  month        = {10},
  number       = {4},
  pages        = {1-16},
  shortjournal = {ACM Trans. Appl. Perc.},
  title        = {An evaluation of screen parallax, haptic feedback, and sensory-motor mismatch on near-field perception-action coordination in VR},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MovEcho: A gesture-sound interface allowing blind
manipulations in a driving context. <em>TAP</em>, <em>18</em>(3), 1–19.
(<a href="https://doi.org/10.1145/3464692">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most recent vehicles are equipped with touchscreens, which replace arrays of buttons that control secondary driving functions, such as temperature level, strength of ventilation, GPS, or choice of radio stations. While driving, manipulating such interfaces can be problematic in terms of safety, because they require the drivers’ sight. In this article, we develop an innovative interface, MovEcho, which is piloted with gestures and associated with sounds that are used as informational feedback. We compare this interface to a touchscreen in a perceptual experiment that took place in a driving simulator. The results show that MovEcho allows for a better visual task completion related to traffic and is preferred by the participants. These promising results in a simulator condition have to be confirmed in future studies, in a real vehicle with a comparable expertise for both interfaces.},
  archive      = {J_TAP},
  doi          = {10.1145/3464692},
  journal      = {ACM Transactions on Applied Perception},
  month        = {8},
  number       = {3},
  pages        = {1-19},
  shortjournal = {ACM Trans. Appl. Perc.},
  title        = {MovEcho: A gesture-sound interface allowing blind manipulations in a driving context},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). When scents help me remember my password. <em>TAP</em>,
<em>18</em>(3), 1–18. (<a
href="https://doi.org/10.1145/3469889">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current authentication processes overwhelmingly rely on audiovisual data, comprising images, text or audio. However, the use of olfactory data (scents) has remained unexploited in the authentication process, notwithstanding their verified potential to act as cues for information recall. Accordingly, in this paper, a new authentication process is proposed in which olfactory media are used as cues in the login phase. To this end, PassSmell , a proof of concept authentication application, is developed in which words and olfactory media act as passwords and olfactory passwords, respectively. In order to evaluate the potential of PassSmell, two different versions were developed, namely one which was olfactory-enhanced and another which did not employ olfactory media. Forty-two participants were invited to take part in the experiment, evenly split into a control and experimental group. For assessment purposes, we recorded the time taken to logon as well as the number of failed/successful login attempts; we also asked users to complete a Quality of Experience (QoE) questionnaire. In terms of time taken, a significant difference was found between the experimental and the control groups, as determined by an independent sample t-test. Similar results were found with respect to average scores and the number of successful attempts. Regarding user QoE, having olfactory media with words influenced the users positively, emphasizing the potential of using this kind of authentication application in the future.},
  archive      = {J_TAP},
  doi          = {10.1145/3469889},
  journal      = {ACM Transactions on Applied Perception},
  month        = {8},
  number       = {3},
  pages        = {1-18},
  shortjournal = {ACM Trans. Appl. Perc.},
  title        = {When scents help me remember my password},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Identification of words and phrases through a phonemic-based
haptic display: Effects of inter-phoneme and inter-word interval
durations. <em>TAP</em>, <em>18</em>(3), 1–22. (<a
href="https://doi.org/10.1145/3458725">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stand-alone devices for tactile speech reception serve a need as communication aids for persons with profound sensory impairments as well as in applications such as human-computer interfaces and remote communication when the normal auditory and visual channels are compromised or overloaded. The current research is concerned with perceptual evaluations of a phoneme-based tactile speech communication device in which a unique tactile code was assigned to each of the 24 consonants and 15 vowels of English. The tactile phonemic display was conveyed through an array of 24 tactors that stimulated the dorsal and ventral surfaces of the forearm. Experiments examined the recognition of individual words as a function of the inter-phoneme interval (Study 1) and two-word phrases as a function of the inter-word interval (Study 2). Following an average training period of 4.3 hrs on phoneme and word recognition tasks, mean scores for the recognition of individual words in Study 1 ranged from 87.7% correct to 74.3% correct as the inter-phoneme interval decreased from 300 to 0 ms. In Study 2, following an average of 2.5 hours of training on the two-word phrase task, both words in the phrase were identified with an accuracy of 75% correct using an inter-word interval of 1 sec and an inter-phoneme interval of 150 ms. Effective transmission rates achieved on this task were estimated to be on the order of 30 to 35 words/min.},
  archive      = {J_TAP},
  doi          = {10.1145/3458725},
  journal      = {ACM Transactions on Applied Perception},
  month        = {7},
  number       = {3},
  pages        = {1-22},
  shortjournal = {ACM Trans. Appl. Perc.},
  title        = {Identification of words and phrases through a phonemic-based haptic display: Effects of inter-phoneme and inter-word interval durations},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Computational model for global contour precedence based on
primary visual cortex mechanisms. <em>TAP</em>, <em>18</em>(3), 1–21.
(<a href="https://doi.org/10.1145/3459999">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The edges of an image contains rich visual cognitive cues. However, the edge information of a natural scene usually is only a set of disorganized unorganized pixels for a computer. In psychology, the phenomenon of quickly perceiving global information from a complex pattern is called the global precedence effect (GPE). For example, when one observes the edge map of an image, some contours seem to automatically “pop out” from the complex background. This is a manifestation of GPE on edge information and is called global contour precedence (GCP). The primary visual cortex (V1) is closely related to the processing of edges. In this article, a neural computational model to simulate GCP based on the mechanisms of V1 is presented. There are three layers in the proposed model: the representation of line segments, organization of edges, and perception of global contours. In experiments, the ability to group edges is tested on the public dataset BSDS500. The results show that the grouping performance, robustness, and time cost of the proposed model are superior to those of other methods. In addition, the outputs of the proposed model can also be applied to the generation of object proposals, which indicates that the proposed model can contribute significantly to high-level visual tasks.},
  archive      = {J_TAP},
  doi          = {10.1145/3459999},
  journal      = {ACM Transactions on Applied Perception},
  month        = {7},
  number       = {3},
  pages        = {1-21},
  shortjournal = {ACM Trans. Appl. Perc.},
  title        = {Computational model for global contour precedence based on primary visual cortex mechanisms},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An extended analysis on the benefits of dark mode user
interfaces in optical see-through head-mounted displays. <em>TAP</em>,
<em>18</em>(3), 1–22. (<a
href="https://doi.org/10.1145/3456874">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Light-on-dark color schemes, so-called “Dark Mode,” are becoming more and more popular over a wide range of display technologies and application fields. Many people who have to look at computer screens for hours at a time, such as computer programmers and computer graphics artists, indicate a preference for switching colors on a computer screen from dark text on a light background to light text on a dark background due to perceived advantages related to visual comfort and acuity, specifically when working in low-light environments. In this article, we investigate the effects of dark mode color schemes in the field of optical see-through head-mounted displays (OST-HMDs), where the characteristic “additive” light model implies that bright graphics are visible but dark graphics are transparent . We describe two human-subject studies in which we evaluated a normal and inverted color mode in front of different physical backgrounds and different lighting conditions. Our results indicate that dark mode graphics displayed on the HoloLens have significant benefits for visual acuity and usability, while user preferences depend largely on the lighting in the physical environment. We discuss the implications of these effects on user interfaces and applications.},
  archive      = {J_TAP},
  doi          = {10.1145/3456874},
  journal      = {ACM Transactions on Applied Perception},
  month        = {5},
  number       = {3},
  pages        = {1-22},
  shortjournal = {ACM Trans. Appl. Perc.},
  title        = {An extended analysis on the benefits of dark mode user interfaces in optical see-through head-mounted displays},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Eye tracking interaction on unmodified mobile VR headsets
using the selfie camera. <em>TAP</em>, <em>18</em>(3), 1–20. (<a
href="https://doi.org/10.1145/3456875">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Input methods for interaction in smartphone-based virtual and mixed reality (VR/MR) are currently based on uncomfortable head tracking controlling a pointer on the screen. User fixations are a fast and natural input method for VR/MR interaction. Previously, eye tracking in mobile VR suffered from low accuracy, long processing time, and the need for hardware add-ons such as anti-reflective lens coating and infrared emitters. We present an innovative mobile VR eye tracking methodology utilizing only the eye images from the front-facing (selfie) camera through the headset’s lens, without any modifications. Our system first enhances the low-contrast, poorly lit eye images by applying a pipeline of customised low-level image enhancements suppressing obtrusive lens reflections. We then propose an iris region-of-interest detection algorithm that is run only once. This increases the iris tracking speed by reducing the iris search space in mobile devices. We iteratively fit a customised geometric model to the iris to refine its coordinates. We display a thin bezel of light at the top edge of the screen for constant illumination. A confidence metric calculates the probability of successful iris detection. Calibration and linear gaze mapping between the estimated iris centroid and physical pixels on the screen results in low latency, real-time iris tracking. A formal study confirmed that our system’s accuracy is similar to eye trackers in commercial VR headsets in the central part of the headset’s field-of-view. In a VR game, gaze-driven user completion time was as fast as with head-tracked interaction, without the need for consecutive head motions. In a VR panorama viewer, users could successfully switch between panoramas using gaze.},
  archive      = {J_TAP},
  doi          = {10.1145/3456875},
  journal      = {ACM Transactions on Applied Perception},
  month        = {5},
  number       = {3},
  pages        = {1-20},
  shortjournal = {ACM Trans. Appl. Perc.},
  title        = {Eye tracking interaction on unmodified mobile VR headsets using the selfie camera},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The role of subsurface scattering in glossiness perception.
<em>TAP</em>, <em>18</em>(3), 1–26. (<a
href="https://doi.org/10.1145/3458438">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study investigates the potential impact of subsurface light transport on gloss perception for the purposes of broadening our understanding of visual appearance in computer graphics applications. Gloss is an important attribute for characterizing material appearance. We hypothesize that subsurface scattering of light impacts the glossiness perception. However, gloss has been traditionally studied as a surface-related quality and the findings in the state-of-the-art are usually based on fully opaque materials, although the visual cues of glossiness can be impacted by light transmission as well. To address this gap and to test our hypothesis, we conducted psychophysical experiments and found that subjects are able to tell the difference in terms of gloss between stimuli that differ in subsurface light transport but have identical surface qualities and object shape. This gives us a clear indication that subsurface light transport contributes to a glossy appearance. Furthermore, we conducted additional experiments and found that the contribution of subsurface scattering to gloss varies across different shapes and levels of surface roughness. We argue that future research on gloss should include transparent and translucent media and to extend the perceptual models currently limited to surface scattering to more general ones inclusive of subsurface light transport.},
  archive      = {J_TAP},
  doi          = {10.1145/3458438},
  journal      = {ACM Transactions on Applied Perception},
  month        = {5},
  number       = {3},
  pages        = {1-26},
  shortjournal = {ACM Trans. Appl. Perc.},
  title        = {The role of subsurface scattering in glossiness perception},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Field-of-view restriction to reduce VR sickness does not
impede spatial learning in women. <em>TAP</em>, <em>18</em>(2), 1–17.
(<a href="https://doi.org/10.1145/3448304">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Women are more likely to experience virtual reality (VR) sickness than men, which could pose a major challenge to the mass market success of VR. Because VR sickness often results from a visual-vestibular conflict, an effective strategy to mitigate conflict is to restrict the user’s field-of-view (FOV) during locomotion. Sex differences in spatial cognition have been well researched, with several studies reporting that men exhibit better spatial navigation performance in desktop three-dimensional environments than women. However, additional research suggests that this sex difference can be mitigated by providing a larger FOV as this increases the availability of landmarks, which women tend to rely on more than men. Though FOV restriction is already a widely used strategy for VR headsets to minimize VR sickness, it is currently not well understood if it impedes spatial learning in women due to decreased availability of landmarks. Our study (n=28, 14 men and 14 women) found that a dynamic FOV restrictor was equally effective in reducing VR sickness in both sexes, and no sex differences in VR sickness incidence were found. Our study did find a sex difference in spatial learning ability, but an FOV restrictor did not impede spatial learning in either sex.},
  archive      = {J_TAP},
  doi          = {10.1145/3448304},
  journal      = {ACM Transactions on Applied Perception},
  month        = {5},
  number       = {2},
  pages        = {1-17},
  shortjournal = {ACM Trans. Appl. Perc.},
  title        = {Field-of-view restriction to reduce VR sickness does not impede spatial learning in women},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Spot the difference: Accuracy of numerical simulations via
the human visual system. <em>TAP</em>, <em>18</em>(2), 1–15. (<a
href="https://doi.org/10.1145/3449064">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Comparative evaluation lies at the heart of science, and determining the accuracy of a computational method is crucial for evaluating its potential as well as for guiding future efforts. However, metrics that are typically used have inherent shortcomings when faced with the under-resolved solutions of real-world simulation problems. We show how to leverage the human visual system in conjunction with crowd-sourced user studies to address the fundamental problems of widely used classical evaluation metrics. We demonstrate that such user studies driven by visual perception yield a very robust metric and consistent answers for complex phenomena without any requirements for proficiency regarding the physics at hand. This holds even for cases away from convergence where traditional metrics often end up with inconclusive results. More specifically, we evaluate results of different essentially non-oscillatory (ENO) schemes in different fluid flow settings. Our methodology represents a novel and practical approach for scientific evaluations that can give answers for previously unsolved problems.},
  archive      = {J_TAP},
  doi          = {10.1145/3449064},
  journal      = {ACM Transactions on Applied Perception},
  month        = {5},
  number       = {2},
  pages        = {1-15},
  shortjournal = {ACM Trans. Appl. Perc.},
  title        = {Spot the difference: Accuracy of numerical simulations via the human visual system},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Effects of tactile textures on preference in visuo-tactile
exploration. <em>TAP</em>, <em>18</em>(2), 1–13. (<a
href="https://doi.org/10.1145/3449065">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The use of haptic technologies has recently become immensely essential in Human-Computer Interaction to improve user experience and performance. With the introduction of tactile feedback on a touchscreen device, commonly known as surface haptics, several applications and interaction paradigms have become a reality. However, the effects of tactile feedback on the preference of 2D images in visuo-tactile exploration task on touchscreen devices remain largely unknown. In this study, we investigated differences of preference score (the tendency of participants to like/dislike a 2D image based on its visual and tactile properties), reach time, interaction time, and response time under four conditions of feedback: no tactile feedback, high-quality of tactile information (sharp tactile texture), low-quality of tactile information (blurred tactile texture), and incorrect tactile information (mismatch tactile texture). The tactile feedback is rendered in the form of roughness that is simulated by modulating the friction between the finger and the surface and is derived from the 2D image. Thirty-six participants completed visuo-tactile exploration tasks for a total of 36 trials (3 2D images × 4 tactile textures × 3 repetitions). Results showed that the presence of tactile feedback enhanced users’ preference (tactile feedback conditions were rated significantly higher than the no tactile feedback condition for preference regardless of the quality/correctness of tactile feedback). This finding is also supported through results from self-reporting where 88.89% of participants preferred to experience the 2D image with tactile feedback. Additionally, the presence of tactile feedback resulted in significantly larger interaction time and response time compared to the no tactile feedback condition. Furthermore, the quality and correctness of tactile information significantly impacted the preference rating (sharp tactile textures were rated statistically higher than blurred tactile and mismatched tactile textures). All of these findings demonstrate that tactile feedback plays a crucial role in users’ preference and thus motivates further the development of surface haptic technologies.},
  archive      = {J_TAP},
  doi          = {10.1145/3449065},
  journal      = {ACM Transactions on Applied Perception},
  month        = {5},
  number       = {2},
  pages        = {1-13},
  shortjournal = {ACM Trans. Appl. Perc.},
  title        = {Effects of tactile textures on preference in visuo-tactile exploration},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Does what we see shape history? Examining workload history
as a function of performance and ambient/focal visual attention.
<em>TAP</em>, <em>18</em>(2), 1–17. (<a
href="https://doi.org/10.1145/3449066">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Changes in task demands can have delayed adverse impacts on performance. This phenomenon, known as the workload history effect, is especially of concern in dynamic work domains where operators manage fluctuating task demands. The existing workload history literature does not depict a consistent picture regarding how these effects manifest, prompting research to consider measures that are informative on the operator&#39;s process. One promising measure is visual attention patterns, due to its informativeness on various cognitive processes. To explore its ability to explain workload history effects, participants completed a task in an unmanned aerial vehicle command and control testbed where workload transitioned gradually and suddenly. The participants’ performance and visual attention patterns were studied over time to identify workload history effects. The eye-tracking analysis consisted of using a recently developed eye-tracking metric called coefficient K , as it indicates whether visual attention is more focal or ambient. The performance results found workload history effects, but it depended on the workload level, time elapsed, and performance measure. The eye-tracking analysis suggested performance suffered when focal attention was deployed during low workload, which was an unexpected finding. When synthesizing these results, they suggest unexpected visual attention patterns can impact performance immediately over time. Further research is needed; however, this work shows the value of including a real-time visual attention measure, such as coefficient K , as a means to understand how the operator manages varying task demands in complex work environments.},
  archive      = {J_TAP},
  doi          = {10.1145/3449066},
  journal      = {ACM Transactions on Applied Perception},
  month        = {5},
  number       = {2},
  pages        = {1-17},
  shortjournal = {ACM Trans. Appl. Perc.},
  title        = {Does what we see shape history? examining workload history as a function of performance and Ambient/Focal visual attention},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Estimating distances in action space in augmented reality.
<em>TAP</em>, <em>18</em>(2), 1–16. (<a
href="https://doi.org/10.1145/3449067">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Augmented reality ( AR ) is important for training complex tasks, such as navigation, assembly, and medical procedures. The effectiveness of such training may depend on accurate spatial localization of AR objects in the environment. This article presents two experiments that test egocentric distance perception in augmented reality within and at the boundaries of action space (up to 35 m) in comparison with distance perception in a matched real-world ( RW ) environment. Using the Microsoft HoloLens, in Experiment 1, participants in two different RW settings judged egocentric distances (ranging from 10 to 35 m) to an AR avatar or a real person using a visual matching measure. Distances to augmented targets were underestimated compared to real targets in the two indoor, RW contexts. Experiment 2 aimed to generalize the results to an absolute distance measure using verbal reports in one of the indoor environments. Similar to Experiment 1, distances to augmented targets were underestimated compared to real targets. We discuss these findings with respect to the importance of methodologies that directly compare performance in real and mediated environments, as well as the inherent differences present in mediated environments that are “matched” to the real world.},
  archive      = {J_TAP},
  doi          = {10.1145/3449067},
  journal      = {ACM Transactions on Applied Perception},
  month        = {5},
  number       = {2},
  pages        = {1-16},
  shortjournal = {ACM Trans. Appl. Perc.},
  title        = {Estimating distances in action space in augmented reality},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
</ul>

</body>
</html>
