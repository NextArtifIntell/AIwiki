<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TOCS_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tocs---290">TOCS - 290</h2>
<ul>
<li><details>
<summary>
(2021). Computational design of knit templates. <em>TOG</em>,
<em>41</em>(2), 1–16. (<a
href="https://doi.org/10.1145/3488006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present an interactive design system for knitting that allows users to create template patterns that can be fabricated using an industrial knitting machine. Our interactive design tool is novel in that it allows direct control of key knitting design axes we have identified in our formative study and does so consistently across the variations of an input parametric template geometry. This is achieved with two key technical advances. First, we present an interactive meshing tool that lets users build a coarse quadrilateral mesh that adheres to their knit design guidelines. This solution ensures consistency across the parameter space for further customization over shape variations and avoids helices, promoting knittability. Second, we lift and formalize low-level machine knitting constraints to the level of this coarse quad mesh. This enables us to not only guarantee hand- and machine-knittability, but also provides automatic design assistance through auto-completion and suggestions. We show the capabilities through a set of fabricated examples that illustrate the effectiveness of our approach in creating a wide variety of objects and interactively exploring the space of design variations.},
  archive      = {J_TOG},
  doi          = {10.1145/3488006},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {2},
  pages        = {1-16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Computational design of knit templates},
  volume       = {41},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Optimal dual schemes for adaptive grid based hexmeshing.
<em>TOG</em>, <em>41</em>(2), 1–14. (<a
href="https://doi.org/10.1145/3494456">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hexahedral meshes are a ubiquitous domain for the numerical resolution of partial differential equations. Computing a pure hexahedral mesh from an adaptively refined grid is a prominent approach to automatic hexmeshing, and requires the ability to restore the all hex property around the hanging nodes that arise at the interface between cells having different size. The most advanced tools to accomplish this task are based on mesh dualization. These approaches use topological schemes to regularize the valence of inner vertices and edges, such that dualizing the grid yields a pure hexahedral mesh. In this article, we study in detail the dual approach, and propose four main contributions to it: (i) We enumerate all the possible transitions that dual methods must be able to handle, showing that prior schemes do not natively cover all of them; (ii) We show that schemes are internally asymmetric, therefore not only their construction is ambiguous, but different implementative choices lead to hexahedral meshes with different singular structure; (iii) We explore the combinatorial space of dual schemes, selecting the minimum set that covers all the possible configurations and also yields the simplest singular structure in the output hexmesh; (iv) We enlarge the class of adaptive grids that can be transformed into pure hexahedral meshes, relaxing one of the tight topological requirements imposed by previous approaches. Our extensive experiments show that our transition schemes consistently outperform prior art in terms of ability to converge to a valid solution, amount and distribution of singular mesh edges, and element count. Last but not least, we publicly release our code and reveal a conspicuous amount of technical details that were overlooked in previous literature, lowering an entry barrier that was hard to overcome for practitioners in the field.},
  archive      = {J_TOG},
  doi          = {10.1145/3494456},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {2},
  pages        = {1-14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Optimal dual schemes for adaptive grid based hexmeshing},
  volume       = {41},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Escherization with large deformations based on
as-rigid-as-possible shape modeling. <em>TOG</em>, <em>41</em>(2), 1–16.
(<a href="https://doi.org/10.1145/3487017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Escher tiling is well known as a tiling that consists of one or a few recognizable figures, such as animals. The Escherization problem involves finding the most similar shape to a given goal figure that can tile the plane. However, it is easy to imagine that there is no similar tile shape for complex goal shapes. This article devises a method for finding a satisfactory tile shape in such a situation. To obtain a satisfactory tile shape, the tile shape is generated by deforming the goal shape to a considerable extent while retaining the characteristics of the original shape. To achieve this, both goal and tile shapes are represented as triangular meshes to consider not only the contours but also the internal similarity of the shapes. To measure the naturalness of the deformation, energy functions based on traditional as-rigid-as-possible shape modeling are incorporated into a recently developed framework of the exhaustive search of the templates for the Escherization problem. The developed algorithms find satisfactory tile shapes with natural deformations for fairly complex goal shapes.},
  archive      = {J_TOG},
  doi          = {10.1145/3487017},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {2},
  pages        = {1-16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Escherization with large deformations based on as-rigid-as-possible shape modeling},
  volume       = {41},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). GeoTangle: Interactive design of geodesic tangle patterns on
surfaces. <em>TOG</em>, <em>41</em>(2), 1–17. (<a
href="https://doi.org/10.1145/3487909">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tangles are complex patterns, which are often used to decorate the surface of real-world artisanal objects. They consist of arrangements of simple shapes organized into nested hierarchies, obtained by recursively splitting regions to add progressively finer details. In this article, we show that 3D digital shapes can be decorated with tangles by working interactively in the intrinsic metric of the surface. Our tangles are generated by the recursive application of only four operators, which are derived from tracing the isolines or the integral curves of geodesics fields generated from selected seeds on the surface. Based on this formulation, we present an interactive application that lets designers model complex recursive patterns directly on the object surface without relying on parametrization. We reach interactive speed on meshes of a few million triangles by relying on an efficient approximate graph-based geodesic solver.},
  archive      = {J_TOG},
  doi          = {10.1145/3487909},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {2},
  pages        = {1-17},
  shortjournal = {ACM Trans. Graph.},
  title        = {GeoTangle: Interactive design of geodesic tangle patterns on surfaces},
  volume       = {41},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DiffPD: Differentiable projective dynamics. <em>TOG</em>,
<em>41</em>(2), 1–21. (<a
href="https://doi.org/10.1145/3490168">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel, fast differentiable simulator for soft-body learning and control applications. Existing differentiable soft-body simulators can be classified into two categories based on their time integration methods: Simulators using explicit timestepping schemes require tiny timesteps to avoid numerical instabilities in gradient computation, and simulators using implicit time integration typically compute gradients by employing the adjoint method and solving the expensive linearized dynamics. Inspired by Projective Dynamics ( PD ), we present Differentiable Projective Dynamics ( DiffPD ), an efficient differentiable soft-body simulator based on PD with implicit time integration. The key idea in DiffPD is to speed up backpropagation by exploiting the prefactorized Cholesky decomposition in forward PD simulation. In terms of contact handling, DiffPD supports two types of contacts: a penalty-based model describing contact and friction forces and a complementarity-based model enforcing non-penetration conditions and static friction. We evaluate the performance of DiffPD and observe it is 4–19 times faster compared with the standard Newton’s method in various applications including system identification, inverse design problems, trajectory optimization, and closed-loop control. We also apply DiffPD in a reality-to-simulation ( real-to-sim ) example with contact and collisions and show its capability of reconstructing a digital twin of real-world scenes.},
  archive      = {J_TOG},
  doi          = {10.1145/3490168},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {2},
  pages        = {1-21},
  shortjournal = {ACM Trans. Graph.},
  title        = {DiffPD: Differentiable projective dynamics},
  volume       = {41},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A compact representation of measured BRDFs using neural
processes. <em>TOG</em>, <em>41</em>(2), 1–15. (<a
href="https://doi.org/10.1145/3490385">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we introduce a compact representation for measured BRDFs by leveraging Neural Processes (NPs). Unlike prior methods that express those BRDFs as discrete high-dimensional matrices or tensors, our technique considers measured BRDFs as continuous functions and works in corresponding function spaces . Specifically, provided the evaluations of a set of BRDFs, such as ones in MERL and EPFL datasets, our method learns a low-dimensional latent space as well as a few neural networks to encode and decode these measured BRDFs or new BRDFs into and from this space in a non-linear fashion. Leveraging this latent space and the flexibility offered by the NPs formulation, our encoded BRDFs are highly compact and offer a level of accuracy better than prior methods. We demonstrate the practical usefulness of our approach via two important applications, BRDF compression and editing. Additionally, we design two alternative post-trained decoders to, respectively, achieve better compression ratio for individual BRDFs and enable importance sampling of BRDFs.},
  archive      = {J_TOG},
  doi          = {10.1145/3490385},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {2},
  pages        = {1-15},
  shortjournal = {ACM Trans. Graph.},
  title        = {A compact representation of measured BRDFs using neural processes},
  volume       = {41},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Ensemble metropolis light transport. <em>TOG</em>,
<em>41</em>(1), 1–15. (<a
href="https://doi.org/10.1145/3472294">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a Markov Chain Monte Carlo ( MCMC ) rendering algorithm based on a family of guided transition kernels. The kernels exploit properties of ensembles of light transport paths, which are distributed according to the lighting in the scene, and utilize this information to make informed decisions for guiding local path sampling. Critically, our approach does not require caching distributions in world space, saving time and memory, yet it is able to make guided sampling decisions based on whole paths. We show how this can be implemented efficiently by organizing the paths in each ensemble and designing transition kernels for MCMC rendering based on a carefully chosen subset of paths from the ensemble. This algorithm is easy to parallelize and leads to improvements in variance when rendering a variety of scenes.},
  archive      = {J_TOG},
  doi          = {10.1145/3472294},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {1},
  pages        = {1-15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Ensemble metropolis light transport},
  volume       = {41},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). High-fidelity 3D digital human head creation from RGB-d
selfies. <em>TOG</em>, <em>41</em>(1), 1–21. (<a
href="https://doi.org/10.1145/3472954">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a fully automatic system that can produce high-fidelity, photo-realistic three-dimensional (3D) digital human heads with a consumer RGB-D selfie camera. The system only needs the user to take a short selfie RGB-D video while rotating his/her head and can produce a high-quality head reconstruction in less than 30 s. Our main contribution is a new facial geometry modeling and reflectance synthesis procedure that significantly improves the state of the art. Specifically, given the input video a two-stage frame selection procedure is first employed to select a few high-quality frames for reconstruction. Then a differentiable renderer-based 3D Morphable Model (3DMM) fitting algorithm is applied to recover facial geometries from multiview RGB-D data, which takes advantages of a powerful 3DMM basis constructed with extensive data generation and perturbation. Our 3DMM has much larger expressive capacities than conventional 3DMM, allowing us to recover more accurate facial geometry using merely linear basis. For reflectance synthesis, we present a hybrid approach that combines parametric fitting and Convolutional Neural Networks (CNNs) to synthesize high-resolution albedo/normal maps with realistic hair/pore/wrinkle details. Results show that our system can produce faithful 3D digital human faces with extremely realistic details. The main code and the newly constructed 3DMM basis is publicly available.},
  archive      = {J_TOG},
  doi          = {10.1145/3472954},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {1},
  pages        = {1-21},
  shortjournal = {ACM Trans. Graph.},
  title        = {High-fidelity 3D digital human head creation from RGB-D selfies},
  volume       = {41},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Photon-driven neural reconstruction for path guiding.
<em>TOG</em>, <em>41</em>(1), 1–15. (<a
href="https://doi.org/10.1145/3476828">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although Monte Carlo path tracing is a simple and effective algorithm to synthesize photo-realistic images, it is often very slow to converge to noise-free results when involving complex global illumination. One of the most successful variance-reduction techniques is path guiding, which can learn better distributions for importance sampling to reduce pixel noise. However, previous methods require a large number of path samples to achieve reliable path guiding. We present a novel neural path guiding approach that can reconstruct high-quality sampling distributions for path guiding from a sparse set of samples, using an offline trained neural network. We leverage photons traced from light sources as the primary input for sampling density reconstruction, which is effective for challenging scenes with strong global illumination. To fully make use of our deep neural network, we partition the scene space into an adaptive hierarchical grid, in which we apply our network to reconstruct high-quality sampling distributions for any local region in the scene. This allows for effective path guiding for arbitrary path bounce at any location in path tracing. We demonstrate that our photon-driven neural path guiding approach can generalize to diverse testing scenes, often achieving better rendering results than previous path guiding approaches and opening up interesting future directions.},
  archive      = {J_TOG},
  doi          = {10.1145/3476828},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {1},
  pages        = {1-15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Photon-driven neural reconstruction for path guiding},
  volume       = {41},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). TightCap: 3D human shape capture with clothing tightness
field. <em>TOG</em>, <em>41</em>(1), 1–17. (<a
href="https://doi.org/10.1145/3478518">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we present TightCap, a data-driven scheme to capture both the human shape and dressed garments accurately with only a single three-dimensional (3D) human scan, which enables numerous applications such as virtual try-on, biometrics, and body evaluation. To break the severe variations of the human poses and garments, we propose to model the clothing tightness field—the displacements from the garments to the human shape implicitly in the global UV texturing domain. To this end, we utilize an enhanced statistical human template and an effective multi-stage alignment scheme to map the 3D scan into a hybrid 2D geometry image. Based on this 2D representation, we propose a novel framework to predict clothing tightness field via a novel tightness formulation, as well as an effective optimization scheme to further reconstruct multi-layer human shape and garments under various clothing categories and human postures. We further propose a new clothing tightness dataset of human scans with a large variety of clothing styles, poses, and corresponding ground-truth human shapes to stimulate further research. Extensive experiments demonstrate the effectiveness of our TightCap to achieve the high-quality human shape and dressed garments reconstruction, as well as the further applications for clothing segmentation, retargeting, and animation.},
  archive      = {J_TOG},
  doi          = {10.1145/3478518},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {1},
  pages        = {1-17},
  shortjournal = {ACM Trans. Graph.},
  title        = {TightCap: 3D human shape capture with clothing tightness field},
  volume       = {41},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). PCEDNet: A lightweight neural network for fast and
interactive edge detection in 3D point clouds. <em>TOG</em>,
<em>41</em>(1), 1–21. (<a
href="https://doi.org/10.1145/3481804">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, Convolutional Neural Networks (CNN) have proven to be efficient analysis tools for processing point clouds, e.g., for reconstruction, segmentation, and classification. In this article, we focus on the classification of edges in point clouds, where both edges and their surrounding are described. We propose a new parameterization adding to each point a set of differential information on its surrounding shape reconstructed at different scales. These parameters, stored in a Scale-Space Matrix (SSM) , provide a well-suited information from which an adequate neural network can learn the description of edges and use it to efficiently detect them in acquired point clouds. After successfully applying a multi-scale CNN on SSMs for the efficient classification of edges and their neighborhood, we propose a new lightweight neural network architecture outperforming the CNN in learning time, processing time, and classification capabilities. Our architecture is compact, requires small learning sets, is very fast to train, and classifies millions of points in seconds.},
  archive      = {J_TOG},
  doi          = {10.1145/3481804},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {1},
  pages        = {1-21},
  shortjournal = {ACM Trans. Graph.},
  title        = {PCEDNet: A lightweight neural network for fast and interactive edge detection in 3D point clouds},
  volume       = {41},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SGN: Sparse gauss-newton for accelerated sensitivity
analysis. <em>TOG</em>, <em>41</em>(1), 1–10. (<a
href="https://doi.org/10.1145/3470005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a sparse Gauss-Newton solver for accelerated sensitivity analysis with applications to a wide range of equilibrium-constrained optimization problems. Dense Gauss-Newton solvers have shown promising convergence rates for inverse problems, but the cost of assembling and factorizing the associated matrices has so far been a major stumbling block. In this work, we show how the dense Gauss-Newton Hessian can be transformed into an equivalent sparse matrix that can be assembled and factorized much more efficiently. This leads to drastically reduced computation times for many inverse problems, which we demonstrate on a diverse set of examples. We furthermore show links between sensitivity analysis and nonlinear programming approaches based on Lagrange multipliers and prove equivalence under specific assumptions that apply for our problem setting.},
  archive      = {J_TOG},
  doi          = {10.1145/3470005},
  journal      = {ACM Transactions on Graphics},
  month        = {9},
  number       = {1},
  pages        = {1-10},
  shortjournal = {ACM Trans. Graph.},
  title        = {SGN: Sparse gauss-newton for accelerated sensitivity analysis},
  volume       = {41},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Computational object-wrapping rope nets. <em>TOG</em>,
<em>41</em>(1), 1–16. (<a
href="https://doi.org/10.1145/3476829">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wrapping objects using ropes is a common practice in our daily life. However, it is difficult to design and tie ropes on a 3D object with complex topology and geometry features while ensuring wrapping security and easy operation. In this article, we propose to compute a rope net that can tightly wrap around various 3D shapes. Our computed rope net not only immobilizes the object but also maintains the load balance during lifting. Based on the key observation that if every knot of the net has four adjacent curve edges, then only a single rope is needed to construct the entire net. We reformulate the rope net computation problem into a constrained curve network optimization. We propose a discrete-continuous optimization approach, where the topological constraints are satisfied in the discrete phase and the geometrical goals are achieved in the continuous stage. We also develop a hoist planning to pick anchor points so that the rope net equally distributes the load during hoisting. Furthermore, we simulate the wrapping process and use it to guide the physical rope net construction process. We demonstrate the effectiveness of our method on 3D objects with varying geometric and topological complexity. In addition, we conduct physical experiments to demonstrate the practicability of our method.},
  archive      = {J_TOG},
  doi          = {10.1145/3476829},
  journal      = {ACM Transactions on Graphics},
  month        = {9},
  number       = {1},
  pages        = {1-16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Computational object-wrapping rope nets},
  volume       = {41},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Spatial-temporal motion control via composite cam-follower
mechanisms. <em>TOG</em>, <em>40</em>(6), 1–15. (<a
href="https://doi.org/10.1145/3478513.3480477">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motion control, both on the trajectory and timing, is crucial for mechanical automata to perform functionalities such as walking and entertaining. We present composite cam-follower mechanisms that can control their spatial-temporal motions to exactly follow trajectories and timings specified by users, and propose a computational technique to model, design, and optimize these mechanisms. The building blocks of our mechanisms are a new kind of cam-follower mechanism with a modified joint, in which the follower can perform spatial motion on a planar, cylindrical, or spherical surface controlled by the 3D cam&#39;s profile. We parameterize the geometry of these cam-follower mechanisms, formulate analytical equations to model their kinematics and dynamics, and present a method to combine multiple cam-follower mechanisms into a working mechanism. Taking this modeling as a foundation, we propose a computational approach to designing and optimizing the geometry and layout of composite cam-follower mechanisms, with an objective of performing target spatial-temporal motions driving by a small motor torque. We demonstrate the effectiveness of our technique by designing different kinds of personalized automata and showing results not attainable by conventional mechanisms.},
  archive      = {J_TOG},
  doi          = {10.1145/3478513.3480477},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Spatial-temporal motion control via composite cam-follower mechanisms},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Synthesizing scene-aware virtual reality teleport graphs.
<em>TOG</em>, <em>40</em>(6), 1–15. (<a
href="https://doi.org/10.1145/3478513.3480478">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel approach for synthesizing scene-aware virtual reality teleport graphs, which facilitate navigation in indoor virtual environments by suggesting desirable teleport positions. Our approach analyzes panoramic views at candidate teleport positions by extracting scene perception graphs, which encode scene perception relationships between the observer and the surrounding objects, and predict how desirable the views at these positions are. We train a graph convolutional model to predict the scene perception scores of different teleport positions. Based on such predictions, we apply an optimization approach to sample a set of desirable teleport positions while considering other navigation properties such as coverage and connectivity to synthesize a teleport graph. Using teleport graphs, users can navigate virtual environments efficaciously. We demonstrate our approach for synthesizing teleport graphs for common indoor scenes. By conducting a user study, we validate the efficacy and desirability of navigating virtual environments via the synthesized teleport graphs. We also extend our approach to cope with different constraints, user preferences, and practical scenarios.},
  archive      = {J_TOG},
  doi          = {10.1145/3478513.3480478},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Synthesizing scene-aware virtual reality teleport graphs},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). PBNS: Physically based neural simulation for unsupervised
garment pose space deformation. <em>TOG</em>, <em>40</em>(6), 1–14. (<a
href="https://doi.org/10.1145/3478513.3480479">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a methodology to automatically obtain Pose Space Deformation (PSD) basis for rigged garments through deep learning. Classical approaches rely on Physically Based Simulations (PBS) to animate clothes. These are general solutions that, given a sufficiently fine-grained discretization of space and time, can achieve highly realistic results. However, they are computationally expensive and any scene modification prompts the need of re-simulation. Linear Blend Skinning (LBS) with PSD offers a lightweight alternative to PBS, though, it needs huge volumes of data to learn proper PSD. We propose using deep learning, formulated as an implicit PBS, to un-supervisedly learn realistic cloth Pose Space Deformations in a constrained scenario: dressed humans. Furthermore, we show it is possible to train these models in an amount of time comparable to a PBS of a few sequences. To the best of our knowledge, we are the first to propose a neural simulator for cloth. While deep-based approaches in the domain are becoming a trend, these are data-hungry models. Moreover, authors often propose complex formulations to better learn wrinkles from PBS data. Supervised learning leads to physically inconsistent predictions that require collision solving to be used. Also, dependency on PBS data limits the scalability of these solutions, while their formulation hinders its applicability and compatibility. By proposing an unsupervised methodology to learn PSD for LBS models (3D animation standard), we overcome both of these drawbacks. Results obtained show cloth-consistency in the animated garments and meaningful pose-dependant folds and wrinkles. Our solution is extremely efficient, handles multiple layers of cloth, allows unsupervised outfit resizing and can be easily applied to any custom 3D avatar.},
  archive      = {J_TOG},
  doi          = {10.1145/3478513.3480479},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-14},
  shortjournal = {ACM Trans. Graph.},
  title        = {PBNS: Physically based neural simulation for unsupervised garment pose space deformation},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep3DLayout: 3D reconstruction of an indoor layout from a
spherical panoramic image. <em>TOG</em>, <em>40</em>(6), 1–12. (<a
href="https://doi.org/10.1145/3478513.3480480">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recovering the 3D shape of the bounding permanent surfaces of a room from a single image is a key component of indoor reconstruction pipelines. In this article, we introduce a novel deep learning technique capable to produce, at interactive rates, a tessellated bounding 3D surface from a single 360° image. Differently from prior solutions, we fully address the problem in 3D, significantly expanding the reconstruction space of prior solutions. A graph convolutional network directly infers the room structure as a 3D mesh by progressively deforming a graph-encoded tessellated sphere mapped to the spherical panorama, leveraging perceptual features extracted from the input image. Important 3D properties of indoor environments are exploited in our design. In particular, gravity-aligned features are actively incorporated in the graph in a projection layer that exploits the recent concept of multi head self-attention, and specialized losses guide towards plausible solutions even in presence of massive clutter and occlusions. Extensive experiments demonstrate that our approach outperforms current state of the art methods in terms of accuracy and capability to reconstruct more complex environments.},
  archive      = {J_TOG},
  doi          = {10.1145/3478513.3480480},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-12},
  shortjournal = {ACM Trans. Graph.},
  title        = {Deep3DLayout: 3D reconstruction of an indoor layout from a spherical panoramic image},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). VR social copresence with light field displays.
<em>TOG</em>, <em>40</em>(6), 1–13. (<a
href="https://doi.org/10.1145/3478513.3480481">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As virtual reality (VR) devices become increasingly commonplace, asymmetric interactions between people with and without headsets are becoming more frequent. Existing video pass-through VR headsets solve one side of these asymmetric interactions by showing the user a live reconstruction of the outside world. This paper further advocates for reverse pass-through VR , wherein a three-dimensional view of the user&#39;s face and eyes is presented to any number of outside viewers in a perspective-correct manner using a light field display. Tying together research in social telepresence and copresence, autostereoscopic displays, and facial capture, reverse pass-through VR enables natural eye contact and other important non-verbal cues in a wider range of interaction scenarios, providing a path to potentially increase the utility and social acceptability of VR headsets in shared and public spaces.},
  archive      = {J_TOG},
  doi          = {10.1145/3478513.3480481},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-13},
  shortjournal = {ACM Trans. Graph.},
  title        = {VR social copresence with light field displays},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Cascaded sobol’ sampling. <em>TOG</em>, <em>40</em>(6),
1–13. (<a href="https://doi.org/10.1145/3478513.3480482">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rendering quality is largely influenced by the samplers used in Monte Carlo integration. Important factors include sample uniformity (e.g., low discrepancy) in the high-dimensional integration domain, sample uniformity in lower-dimensional projections, and lack of dominant structures that could result in aliasing artifacts. A widely used and successful construction is the Sobol&#39; sequence that guarantees good high-dimensional uniformity and consequently results in faster convergence of quasi-Monte Carlo integration. We show that this sequence exhibits low uniformity and dominant structures in low-dimensional projections. These structures impair quality in the context of rendering, as they precisely occur in the 2-dimensional projections used for sampling light sources, reflectance functions, or the camera lens or sensor. We propose a new cascaded construction, which, despite dropping the sequential aspect of Sobol&#39; samples, produces point sets exhibiting provably perfect dyadic partitioning (and therefore, excellent uniformity) in consecutive 2-dimensional projections, while preserving good high-dimensional uniformity. By optimizing the initialization parameters and performing Owen scrambling at finer levels of binary representations, we further improve over Sobol&#39;s integration convergence rate. Our method does not incur any overhead as compared to the generation of the Sobol&#39; sequence, is compatible with Owen scrambling and can be used in rendering applications.},
  archive      = {J_TOG},
  doi          = {10.1145/3478513.3480482},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Cascaded sobol&#39; sampling},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Continuous aerial path planning for 3D urban scene
reconstruction. <em>TOG</em>, <em>40</em>(6), 1–15. (<a
href="https://doi.org/10.1145/3478513.3480483">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce the first path-oriented drone trajectory planning algorithm, which performs continuous (i.e., dense ) image acquisition along an aerial path and explicitly factors path quality into an optimization along with scene reconstruction quality. Specifically, our method takes as input a rough 3D scene proxy and produces a drone trajectory and image capturing setup, which efficiently yields a high-quality reconstruction of the 3D scene based on three optimization objectives: one to maximize the amount of 3D scene information that can be acquired along the entirety of the trajectory, another to optimize the scene capturing efficiency by maximizing the scene information that can be acquired per unit length along the aerial path, and the last one to minimize the total turning angles along the aerial path, so as to reduce the number of sharp turns. Our search scheme is based on the rapidly-exploring random tree framework, resulting in a final trajectory as a single path through the search tree. Unlike state-of-the-art works, our joint optimization for view selection and path planning is performed in a single step. We comprehensively evaluate our method not only on benchmark virtual datasets as in existing works but also on several large-scale real urban scenes. We demonstrate that the continuous paths optimized by our method can effectively reduce onsite acquisition cost using drones, while achieving high-fidelity 3D reconstruction, compared to existing planning methods and oblique photography, a mature and popular industry solution.},
  archive      = {J_TOG},
  doi          = {10.1145/3478513.3480483},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Continuous aerial path planning for 3D urban scene reconstruction},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Live speech portraits: Real-time photorealistic talking-head
animation. <em>TOG</em>, <em>40</em>(6), 1–17. (<a
href="https://doi.org/10.1145/3478513.3480484">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To the best of our knowledge, we first present a live system that generates personalized photorealistic talking-head animation only driven by audio signals at over 30 fps. Our system contains three stages. The first stage is a deep neural network that extracts deep audio features along with a manifold projection to project the features to the target person&#39;s speech space. In the second stage, we learn facial dynamics and motions from the projected audio features. The predicted motions include head poses and upper body motions, where the former is generated by an autoregressive probabilistic model which models the head pose distribution of the target person. Upper body motions are deduced from head poses. In the final stage, we generate conditional feature maps from previous predictions and send them with a candidate image set to an image-to-image translation network to synthesize photorealistic renderings. Our method generalizes well to wild audio and successfully synthesizes high-fidelity personalized facial details, e.g., wrinkles, teeth. Our method also allows explicit control of head poses. Extensive qualitative and quantitative evaluations, along with user studies, demonstrate the superiority of our method over state-of-the-art techniques.},
  archive      = {J_TOG},
  doi          = {10.1145/3478513.3480484},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-17},
  shortjournal = {ACM Trans. Graph.},
  title        = {Live speech portraits: Real-time photorealistic talking-head animation},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Time-travel rephotography. <em>TOG</em>, <em>40</em>(6),
1–12. (<a href="https://doi.org/10.1145/3478513.3480485">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many historical people were only ever captured by old, faded, black and white photos, that are distorted due to the limitations of early cameras and the passage of time. This paper simulates traveling back in time with a modern camera to rephotograph famous subjects. Unlike conventional image restoration filters which apply independent operations like denoising, colorization, and superresolution, we leverage the StyleGAN2 framework to project old photos into the space of modern high-resolution photos, achieving all of these effects in a unified framework. A unique challenge with this approach is retaining the identity and pose of the subject in the original photo, while discarding the many artifacts frequently seen in low-quality antique photos. Our comparisons to current state-of-the-art restoration filters show significant improvements and compelling results for a variety of important historical people. Please go to time-travell-rephotography.github.io for many more results.},
  archive      = {J_TOG},
  doi          = {10.1145/3478513.3480485},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-12},
  shortjournal = {ACM Trans. Graph.},
  title        = {Time-travel rephotography},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). TreePartNet: Neural decomposition of point clouds for 3D
tree reconstruction. <em>TOG</em>, <em>40</em>(6), 1–16. (<a
href="https://doi.org/10.1145/3478513.3480486">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present TreePartNet , a neural network aimed at reconstructing tree geometry from point clouds obtained by scanning real trees. Our key idea is to learn a natural neural decomposition exploiting the assumption that a tree comprises locally cylindrical shapes. In particular, reconstruction is a two-step process. First, two networks are used to detect priors from the point clouds. One detects semantic branching points, and the other network is trained to learn a cylindrical representation of the branches. In the second step, we apply a neural merging module to reduce the cylindrical representation to a final set of generalized cylinders combined by branches. We demonstrate results of reconstructing realistic tree geometry for a variety of input models and with varying input point quality, e.g., noise, outliers, and incompleteness. We evaluate our approach extensively by using data from both synthetic and real trees and comparing it with alternative methods.},
  archive      = {J_TOG},
  doi          = {10.1145/3478513.3480486},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-16},
  shortjournal = {ACM Trans. Graph.},
  title        = {TreePartNet: Neural decomposition of point clouds for 3D tree reconstruction},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). HyperNeRF: A higher-dimensional representation for
topologically varying neural radiance fields. <em>TOG</em>,
<em>40</em>(6), 1–12. (<a
href="https://doi.org/10.1145/3478513.3480487">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural Radiance Fields (NeRF) are able to reconstruct scenes with unprecedented fidelity, and various recent works have extended NeRF to handle dynamic scenes. A common approach to reconstruct such non-rigid scenes is through the use of a learned deformation field mapping from coordinates in each input image into a canonical template coordinate space. However, these deformation-based approaches struggle to model changes in topology, as topological changes require a discontinuity in the deformation field, but these deformation fields are necessarily continuous. We address this limitation by lifting NeRFs into a higher dimensional space, and by representing the 5D radiance field corresponding to each individual input image as a slice through this &quot;hyper-space&quot;. Our method is inspired by level set methods, which model the evolution of surfaces as slices through a higher dimensional surface. We evaluate our method on two tasks: (i) interpolating smoothly between &quot;moments&quot;, i.e., configurations of the scene, seen in the input images while maintaining visual plausibility, and (ii) novel-view synthesis at fixed moments. We show that our method, which we dub HyperNeRF , outperforms existing methods on both tasks. Compared to Nerfies, HyperNeRF reduces average error rates by 4.1% for interpolation and 8.6% for novel-view synthesis, as measured by LPIPS. Additional videos, results, and visualizations are available at hypernerf.github.io.},
  archive      = {J_TOG},
  doi          = {10.1145/3478513.3480487},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-12},
  shortjournal = {ACM Trans. Graph.},
  title        = {HyperNeRF: A higher-dimensional representation for topologically varying neural radiance fields},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DeepVecFont: Synthesizing high-quality vector fonts via
dual-modality learning. <em>TOG</em>, <em>40</em>(6), 1–15. (<a
href="https://doi.org/10.1145/3478513.3480488">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic font generation based on deep learning has aroused a lot of interest in the last decade. However, only a few recently-reported approaches are capable of directly generating vector glyphs and their results are still far from satisfactory. In this paper, we propose a novel method, DeepVecFont, to effectively resolve this problem. Using our method, for the first time, visually-pleasing vector glyphs whose quality and compactness are both comparable to human-designed ones can be automatically generated. The key idea of our DeepVecFont is to adopt the techniques of image synthesis, sequence modeling and differentiable rasterization to exhaustively exploit the dual-modality information (i.e., raster images and vector outlines) of vector fonts. The highlights of this paper are threefold. First, we design a dual-modality learning strategy which utilizes both image-aspect and sequence-aspect features of fonts to synthesize vector glyphs. Second, we provide a new generative paradigm to handle unstructured data (e.g., vector glyphs) by randomly sampling plausible synthesis results to get the optimal one which is further refined under the guidance of generated structured data (e.g., glyph images). Finally, qualitative and quantitative experiments conducted on a publicly-available dataset demonstrate that our method obtains high-quality synthesis results in the applications of vector font generation and interpolation, significantly outperforming the state of the art.},
  archive      = {J_TOG},
  doi          = {10.1145/3478513.3480488},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-15},
  shortjournal = {ACM Trans. Graph.},
  title        = {DeepVecFont: Synthesizing high-quality vector fonts via dual-modality learning},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Differentiable time-gated rendering. <em>TOG</em>,
<em>40</em>(6), 1–16. (<a
href="https://doi.org/10.1145/3478513.3480489">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The continued advancements of time-of-flight imaging devices have enabled new imaging pipelines with numerous applications. Consequently, several forward rendering techniques capable of accurately and efficiently simulating these devices have been introduced. However, general-purpose differentiable rendering techniques that estimate derivatives of time-of-flight images are still lacking. In this paper, we introduce a new theory of differentiable time-gated rendering that enjoys the generality of differentiating with respect to arbitrary scene parameters. Our theory also allows the design of advanced Monte Carlo estimators capable of handling cameras with near-delta or discontinuous time gates. We validate our theory by comparing derivatives generated with our technique and finite differences. Further, we demonstrate the usefulness of our technique using a few proof-of-concept inverse-rendering examples that simulate several time-of-flight imaging scenarios.},
  archive      = {J_TOG},
  doi          = {10.1145/3478513.3480489},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Differentiable time-gated rendering},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Project starline: A high-fidelity telepresence system.
<em>TOG</em>, <em>40</em>(6), 1–16. (<a
href="https://doi.org/10.1145/3478513.3480490">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a real-time bidirectional communication system that lets two people, separated by distance, experience a face-to-face conversation as if they were copresent. It is the first telepresence system that is demonstrably better than 2D videoconferencing, as measured using participant ratings (e.g., presence, attentiveness, reaction-gauging, engagement), meeting recall, and observed nonverbal behaviors (e.g., head nods, eyebrow movements). This milestone is reached by maximizing audiovisual fidelity and the sense of copresence in all design elements, including physical layout, lighting, face tracking, multi-view capture, microphone array, multi-stream compression, loudspeaker output, and lenticular display. Our system achieves key 3D audiovisual cues (stereopsis, motion parallax, and spatialized audio) and enables the full range of communication cues (eye contact, hand gestures, and body language), yet does not require special glasses or body-worn microphones/headphones. The system consists of a head-tracked autostereoscopic display, high-resolution 3D capture and rendering subsystems, and network transmission using compressed color and depth video streams. Other contributions include a novel image-based geometry fusion algorithm, free-space dereverberation, and talker localization.},
  archive      = {J_TOG},
  doi          = {10.1145/3478513.3480490},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Project starline: A high-fidelity telepresence system},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Aerial path planning for online real-time exploration and
offline high-quality reconstruction of large-scale urban scenes.
<em>TOG</em>, <em>40</em>(6), 1–16. (<a
href="https://doi.org/10.1145/3478513.3480491">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing approaches have shown that, through carefully planning flight trajectories, images captured by Unmanned Aerial Vehicles (UAVs) can be used to reconstruct high-quality 3D models for real environments. These approaches greatly simplify and cut the cost of large-scale urban scene reconstruction. However, to properly capture height discontinuities in urban scenes, all state-of-the-art methods require prior knowledge on scene geometry and hence, additional prepossessing steps are needed before performing the actual image acquisition flights. To address this limitation and to make urban modeling techniques even more accessible, we present a real-time explore-and-reconstruct planning algorithm that does not require any prior knowledge for the scenes. Using only captured 2D images, we estimate 3D bounding boxes for buildings on-the-fly and use them to guide online path planning for both scene exploration and building observation. Experimental results demonstrate that the aerial paths planned by our algorithm in realtime for unknown environments support reconstructing 3D models with comparable qualities and lead to shorter flight air time.},
  archive      = {J_TOG},
  doi          = {10.1145/3478513.3480491},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Aerial path planning for online real-time exploration and offline high-quality reconstruction of large-scale urban scenes},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Predicting high-resolution turbulence details in space and
time. <em>TOG</em>, <em>40</em>(6), 1–16. (<a
href="https://doi.org/10.1145/3478513.3480492">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting the fine and intricate details of a turbulent flow field in both space and time from a coarse input remains a major challenge despite the availability of modern machine learning tools. In this paper, we present a simple and effective dictionary-based approach to spatio-temporal upsampling of fluid simulation. We demonstrate that our neural network approach can reproduce the visual complexity of turbulent flows from spatially and temporally coarse velocity fields even when using a generic training set. Moreover, since our method generates finer spatial and/or temporal details through embarrassingly-parallel upsampling of small local patches, it can efficiently predict high-resolution turbulence details across a variety of grid resolutions. As a consequence, our method offers a whole range of applications varying from fluid flow upsampling to fluid data compression. We demonstrate the efficiency and generalizability of our method for synthesizing turbulent flows on a series of complex examples, highlighting dramatically better results in spatio-temporal upsampling and flow data compression than existing methods as assessed by both qualitative and quantitative comparisons.},
  archive      = {J_TOG},
  doi          = {10.1145/3478513.3480492},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Predicting high-resolution turbulence details in space and time},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fast and versatile fluid-solid coupling for turbulent flow
simulation. <em>TOG</em>, <em>40</em>(6), 1–18. (<a
href="https://doi.org/10.1145/3478513.3480493">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The intricate motions and complex vortical structures generated by the interaction between fluids and solids are visually fascinating. However, reproducing such a two-way coupling between thin objects and turbulent fluids numerically is notoriously challenging and computationally costly: existing approaches such as cut-cell or immersed-boundary methods have difficulty achieving physical accuracy, or even visual plausibility, of simulations involving fast-evolving flows with immersed objects of arbitrary shapes. In this paper, we propose an efficient and versatile approach for simulating two-way fluid-solid coupling within the kinetic (lattice-Boltzmann) fluid simulation framework, valid for both laminar and highly turbulent flows, and for both thick and thin objects. We introduce a novel hybrid approach to fluid-solid coupling which systematically involves a mesoscopic double-sided bounce-back scheme followed by a cut-cell velocity correction for a more robust and plausible treatment of turbulent flows near moving (thin) solids, preventing flow penetration and reducing boundary artifacts significantly. Coupled with an efficient approximation to simplify geometric computations, the whole boundary treatment method preserves the inherent massively parallel computational nature of the kinetic method. Moreover, we propose simple GPU optimizations of the core LBM algorithm which achieve an even higher computational efficiency than the state-of-the-art kinetic fluid solvers in graphics. We demonstrate the accuracy and efficacy of our two-way coupling through various challenging simulations involving a variety of rigid body solids and fluids at both high and low Reynolds numbers. Finally, comparisons to existing methods on benchmark data and real experiments further highlight the superiority of our method.},
  archive      = {J_TOG},
  doi          = {10.1145/3478513.3480493},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-18},
  shortjournal = {ACM Trans. Graph.},
  title        = {Fast and versatile fluid-solid coupling for turbulent flow simulation},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Intuitive and efficient roof modeling for reconstruction and
synthesis. <em>TOG</em>, <em>40</em>(6), 1–17. (<a
href="https://doi.org/10.1145/3478513.3480494">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel and flexible roof modeling approach that can be used for constructing planar 3D polygon roof meshes. Our method uses a graph structure to encode roof topology and enforces the roof validity by optimizing a simple but effective planarity metric we propose. This approach is significantly more efficient than using general purpose 3D modeling tools such as 3ds Max or SketchUp, and more powerful and expressive than specialized tools such as the straight skeleton. Our optimization-based formulation is also flexible and can accommodate different styles and user preferences for roof modeling. We showcase two applications. The first application is an interactive roof editing framework that can be used for roof design or roof reconstruction from aerial images. We highlight the efficiency and generality of our approach by constructing a mesh-image paired dataset consisting of 2539 roofs. Our second application is a generative model to synthesize new roof meshes from scratch. We use our novel dataset to combine machine learning and our roof optimization techniques, by using transformers and graph convolutional networks to model roof topology, and our roof optimization methods to enforce the planarity constraint.},
  archive      = {J_TOG},
  doi          = {10.1145/3478513.3480494},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-17},
  shortjournal = {ACM Trans. Graph.},
  title        = {Intuitive and efficient roof modeling for reconstruction and synthesis},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Ships, splashes, and waves on a vast ocean. <em>TOG</em>,
<em>40</em>(6), 1–15. (<a
href="https://doi.org/10.1145/3478513.3480495">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The simulation of large open water surface is challenging using a uniform volumetric discretization of the Navier-Stokes equations. Simulating water splashes near moving objects, which height field methods for water waves cannot capture, necessitates high resolutions. Such simulations can be carried out using the Fluid-Implicit-Particle (FLIP) method. However, the FLIP method is not efficient for the long-lasting water waves that propagate to long distances, which require sufficient depth for a correct dispersion relationship. This paper presents a new method to tackle this dilemma through an efficient hybridization of volumetric and surface-based advection-projection discretizations. We design a hybrid time-stepping algorithm that combines a FLIP domain and an adaptively remeshed Boundary Element Method (BEM) domain for the incompressible Euler equations. The resulting framework captures the detailed water splashes near moving objects with the FLIP method, and produces convincing water waves with correct dispersion relationships at modest additional costs.},
  archive      = {J_TOG},
  doi          = {10.1145/3478513.3480495},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Ships, splashes, and waves on a vast ocean},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). NeRFactor: Neural factorization of shape and reflectance
under an unknown illumination. <em>TOG</em>, <em>40</em>(6), 1–18. (<a
href="https://doi.org/10.1145/3478513.3480496">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We address the problem of recovering the shape and spatially-varying reflectance of an object from multi-view images (and their camera poses) of an object illuminated by one unknown lighting condition. This enables the rendering of novel views of the object under arbitrary environment lighting and editing of the object&#39;s material properties. The key to our approach, which we call Neural Radiance Factorization (NeRFactor), is to distill the volumetric geometry of a Neural Radiance Field (NeRF) [Mildenhall et al. 2020] representation of the object into a surface representation and then jointly refine the geometry while solving for the spatially-varying reflectance and environment lighting. Specifically, NeRFactor recovers 3D neural fields of surface normals, light visibility, albedo, and Bidirectional Reflectance Distribution Functions (BRDFs) without any supervision, using only a re-rendering loss, simple smoothness priors, and a data-driven BRDF prior learned from real-world BRDF measurements. By explicitly modeling light visibility, NeRFactor is able to separate shadows from albedo and synthesize realistic soft or hard shadows under arbitrary lighting conditions. NeRFactor is able to recover convincing 3D models for free-viewpoint relighting in this challenging and underconstrained capture setup for both synthetic and real scenes. Qualitative and quantitative experiments show that NeRFactor outperforms classic and deep learning-based state of the art across various tasks. Our videos, code, and data are available at people.csail.mit.edu/xiuming/projects/nerfactor/.},
  archive      = {J_TOG},
  doi          = {10.1145/3478513.3480496},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-18},
  shortjournal = {ACM Trans. Graph.},
  title        = {NeRFactor: Neural factorization of shape and reflectance under an unknown illumination},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dynamic neural garments. <em>TOG</em>, <em>40</em>(6), 1–15.
(<a href="https://doi.org/10.1145/3478513.3480497">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A vital task of the wider digital human effort is the creation of realistic garments on digital avatars, both in the form of characteristic fold patterns and wrinkles in static frames as well as richness of garment dynamics under avatars&#39; motion. Existing workflow of modeling, simulation, and rendering closely replicates the physics behind real garments, but is tedious and requires repeating most of the workflow under changes to characters&#39; motion, camera angle, or garment resizing. Although data-driven solutions exist, they either focus on static scenarios or only handle dynamics of tight garments. We present a solution that, at test time, takes in body joint motion to directly produce realistic dynamic garment image sequences. Specifically, given the target joint motion sequence of an avatar, we propose dynamic neural garments to synthesize plausible dynamic garment appearance from a desired viewpoint. Technically, our solution generates a coarse garment proxy sequence, learns deep dynamic features attached to this template, and neurally renders the features to produce appearance changes such as folds, wrinkles, and silhouettes. We demonstrate generalization behavior to both unseen motion and unseen camera views. Further, our network can be fine-tuned to adopt to new body shape and/or background images. We demonstrate our method on a wide range of real and synthetic garments. We also provide comparisons against existing neural rendering and image sequence translation approaches, and report clear quantitative and qualitative improvements. Project page: http://geometry.cs.ucl.ac.uk/projects/2021/DynamicNeuralGarments/},
  archive      = {J_TOG},
  doi          = {10.1145/3478513.3480497},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Dynamic neural garments},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Differentiable transient rendering. <em>TOG</em>,
<em>40</em>(6), 1–11. (<a
href="https://doi.org/10.1145/3478513.3480498">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent differentiable rendering techniques have become key tools to tackle many inverse problems in graphics and vision. Existing models, however, assume steady-state light transport, i.e., infinite speed of light. While this is a safe assumption for many applications, recent advances in ultrafast imaging leverage the wealth of information that can be extracted from the exact time of flight of light. In this context, physically-based transient rendering allows to efficiently simulate and analyze light transport considering that the speed of light is indeed finite. In this paper, we introduce a novel differentiable transient rendering framework, to help bring the potential of differentiable approaches into the transient regime. To differentiate the transient path integral we need to take into account that scattering events at path vertices are no longer independent; instead, tracking the time of flight of light requires treating such scattering events at path vertices jointly as a multidimensional, evolving manifold. We thus turn to the generalized transport theorem, and introduce a novel correlated importance term, which links the time-integrated contribution of a path to its light throughput, and allows us to handle discontinuities in the light and sensor functions. Last, we present results in several challenging scenarios where the time of flight of light plays an important role such as optimizing indices of refraction, non-line-of-sight tracking with nonplanar relay walls, and non-line-of-sight tracking around two corners.},
  archive      = {J_TOG},
  doi          = {10.1145/3478513.3480498},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-11},
  shortjournal = {ACM Trans. Graph.},
  title        = {Differentiable transient rendering},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fast volume rendering with spatiotemporal reservoir
resampling. <em>TOG</em>, <em>40</em>(6), 1–18. (<a
href="https://doi.org/10.1145/3478513.3480499">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Volume rendering under complex, dynamic lighting is challenging, especially if targeting real-time. To address this challenge, we extend a recent direct illumination sampling technique, spatiotemporal reservoir resampling, to multi-dimensional path space for volumetric media. By fully evaluating just a single path sample per pixel, our volumetric path tracer shows unprecedented convergence. To achieve this, we properly estimate the chosen sample&#39;s probability via approximate perfect importance sampling with spatiotemporal resampling. A key observation is recognizing that applying cheaper, biased techniques to approximate scattering along candidate paths (during resampling) does not add bias when shading. This allows us to combine transmittance evaluation techniques: cheap approximations where evaluations must occur many times for reuse, and unbiased methods for final, per-pixel evaluation. With this reformulation, we achieve low-noise, interactive volumetric path tracing with arbitrary dynamic lighting, including volumetric emission, and maintain interactive performance even on high-resolution volumes. When paired with denoising, our low-noise sampling helps preserve smaller-scale volumetric details.},
  archive      = {J_TOG},
  doi          = {10.1145/3478513.3480499},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-18},
  shortjournal = {ACM Trans. Graph.},
  title        = {Fast volume rendering with spatiotemporal reservoir resampling},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Joint computational design of workspaces and workplans.
<em>TOG</em>, <em>40</em>(6), 1–16. (<a
href="https://doi.org/10.1145/3478513.3480500">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Humans assume different production roles in a workspace. On one hand, humans design workplans to complete tasks as efficiently as possible in order to improve productivity. On the other hand, a nice workspace is essential to facilitate teamwork. In this way, workspace design and workplan design complement each other. Inspired by such observations, we propose an automatic approach to jointly design a workspace and a workplan. Taking staff properties, a space, and work equipment as input, our approach jointly optimizes a workspace and a workplan, considering performance factors such as time efficiency and congestion avoidance, as well as workload factors such as walk effort, turn effort, and workload balances. To enable exploration of design trade-offs, our approach generates a set of Pareto-optimal design solutions with strengths on different objectives, which can be adopted for different work scenarios. We apply our approach to synthesize workspaces and workplans for different workplaces such as a fast food kitchen and a supermarket. We also extend our approach to incorporate other common work considerations such as dynamic work demands and accommodating staff members with different physical capabilities. Evaluation experiments with simulations validate the efficacy of our approach for synthesizing effective workspaces and workplans.},
  archive      = {J_TOG},
  doi          = {10.1145/3478513.3480500},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Joint computational design of workspaces and workplans},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Large steps in inverse rendering of geometry. <em>TOG</em>,
<em>40</em>(6), 1–13. (<a
href="https://doi.org/10.1145/3478513.3480501">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inverse reconstruction from images is a central problem in many scientific and engineering disciplines. Recent progress on differentiable rendering has led to methods that can efficiently differentiate the full process of image formation with respect to millions of parameters to solve such problems via gradient-based optimization. At the same time, the availability of cheap derivatives does not necessarily make an inverse problem easy to solve. Mesh-based representations remain a particular source of irritation: an adverse gradient step involving vertex positions could turn parts of the mesh inside-out, introduce numerous local self-intersections, or lead to inadequate usage of the vertex budget due to distortion. These types of issues are often irrecoverable in the sense that subsequent optimization steps will further exacerbate them. In other words, the optimization lacks robustness due to an objective function with substantial non-convexity. Such robustness issues are commonly mitigated by imposing additional regularization, typically in the form of Laplacian energies that quantify and improve the smoothness of the current iterate. However, regularization introduces its own set of problems: solutions must now compromise between solving the problem and being smooth. Furthermore, gradient steps involving a Laplacian energy resemble Jacobi&#39;s iterative method for solving linear equations that is known for its exceptionally slow convergence. We propose a simple and practical alternative that casts differentiable rendering into the framework of preconditioned gradient descent. Our pre-conditioner biases gradient steps towards smooth solutions without requiring the final solution to be smooth. In contrast to Jacobi-style iteration, each gradient step propagates information among all variables, enabling convergence using fewer and larger steps. Our method is not restricted to meshes and can also accelerate the reconstruction of other representations, where smooth solutions are generally expected. We demonstrate its superior performance in the context of geometric optimization and texture reconstruction.},
  archive      = {J_TOG},
  doi          = {10.1145/3478513.3480501},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Large steps in inverse rendering of geometry},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SketchHairSalon: Deep sketch-based hair image synthesis.
<em>TOG</em>, <em>40</em>(6), 1–16. (<a
href="https://doi.org/10.1145/3478513.3480502">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent deep generative models allow real-time generation of hair images from sketch inputs. Existing solutions often require a user-provided binary mask to specify a target hair shape. This not only costs users extra labor but also fails to capture complicated hair boundaries. Those solutions usually encode hair structures via orientation maps, which, however, are not very effective to encode complex structures. We observe that colored hair sketches already implicitly define target hair shapes as well as hair appearance and are more flexible to depict hair structures than orientation maps. Based on these observations, we present SketchHairSalon , a two-stage framework for generating realistic hair images directly from freehand sketches depicting desired hair structure and appearance. At the first stage, we train a network to predict a hair matte from an input hair sketch, with an optional set of non-hair strokes. At the second stage, another network is trained to synthesize the structure and appearance of hair images from the input sketch and the generated matte. To make the networks in the two stages aware of long-term dependency of strokes, we apply self-attention modules to them. To train these networks, we present a new dataset containing thousands of annotated hair sketch-image pairs and corresponding hair mattes. Two efficient methods for sketch completion are proposed to automatically complete repetitive braided parts and hair strokes, respectively, thus reducing the workload of users. Based on the trained networks and the two sketch completion strategies, we build an intuitive interface to allow even novice users to design visually pleasing hair images exhibiting various hair structures and appearance via freehand sketches. The qualitative and quantitative evaluations show the advantages of the proposed system over the existing or alternative solutions.},
  archive      = {J_TOG},
  doi          = {10.1145/3478513.3480502},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-16},
  shortjournal = {ACM Trans. Graph.},
  title        = {SketchHairSalon: Deep sketch-based hair image synthesis},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). TM-NET: Deep generative networks for textured meshes.
<em>TOG</em>, <em>40</em>(6), 1–15. (<a
href="https://doi.org/10.1145/3478513.3480503">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce TM-NET, a novel deep generative model for synthesizing textured meshes in a part-aware manner. Once trained, the network can generate novel textured meshes from scratch or predict textures for a given 3D mesh, without image guidance. Plausible and diverse textures can be generated for the same mesh part, while texture compatibility between parts in the same shape is achieved via conditional generation. Specifically, our method produces texture maps for individual shape parts, each as a deformable box, leading to a natural UV map with limited distortion. The network separately embeds part geometry (via a PartVAE) and part texture (via a TextureVAE) into their respective latent spaces, so as to facilitate learning texture probability distributions conditioned on geometry. We introduce a conditional autoregressive model for texture generation, which can be conditioned on both part geometry and textures already generated for other parts to achieve texture compatibility. To produce high-frequency texture details, our TextureVAE operates in a high-dimensional latent space via dictionary-based vector quantization. We also exploit transparencies in the texture as an effective means to model complex shape structures including topological details. Extensive experiments demonstrate the plausibility, quality, and diversity of the textures and geometries generated by our network, while avoiding inconsistency issues that are common to novel view synthesis methods.},
  archive      = {J_TOG},
  doi          = {10.1145/3478513.3480503},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-15},
  shortjournal = {ACM Trans. Graph.},
  title        = {TM-NET: Deep generative networks for textured meshes},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Human dynamics from monocular video with dynamic camera
movements. <em>TOG</em>, <em>40</em>(6), 1–14. (<a
href="https://doi.org/10.1145/3478513.3480504">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new method that reconstructs 3D human motion from in-the-wild video by making full use of prior knowledge on the laws of physics. Previous studies focus on reconstructing joint angles and positions in the body local coordinate frame. Body translations and rotations in the global reference frame are partially reconstructed only when the video has a static camera view. We are interested in overcoming this static view limitation to deal with dynamic view videos. The camera may pan, tilt, and zoom to track the moving subject. Since we do not assume any limitations on camera movements, body translations and rotations from the video do not correspond to absolute positions in the reference frame. The key technical challenge is inferring body translations and rotations from a sequence of 3D full-body poses, assuming the absence of root motion. This inference is possible because human motion obeys the law of physics. Our reconstruction algorithm produces a control policy that simulates 3D human motion imitating the one in the video. Our algorithm is particularly useful for reconstructing highly dynamic movements, such as sports, dance, gymnastics, and parkour actions.},
  archive      = {J_TOG},
  doi          = {10.1145/3478513.3480504},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Human dynamics from monocular video with dynamic camera movements},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Interactive cutting and tearing in projective dynamics with
progressive cholesky updates. <em>TOG</em>, <em>40</em>(6), 1–12. (<a
href="https://doi.org/10.1145/3478513.3480505">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new algorithm for updating a Cholesky factorization which speeds up Projective Dynamics simulations with topological changes. Our approach addresses an important limitation of the original Projective Dynamics, i.e., that topological changes such as cutting, fracturing, or tearing require full refactorization which compromises computation speed, especially in real-time applications. Our method progressively modifies the Cholesky factor of the system matrix in the global step instead of computing it from scratch. Only a small amount of overhead is added since most of the topological changes in typical simulations are continuous and gradual. Our method is based on the update and downdate routine in CHOLMOD, but unlike recent related work, supports dynamic sizes of the system matrix and the addition of new vertices. Our approach allows us to introduce clean cuts and perform interactive remeshing. Our experiments show that our method works particularly well in simulation scenarios involving cutting, tearing, and local remeshing operations.},
  archive      = {J_TOG},
  doi          = {10.1145/3478513.3480505},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-12},
  shortjournal = {ACM Trans. Graph.},
  title        = {Interactive cutting and tearing in projective dynamics with progressive cholesky updates},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). I♥LA: Compilable markdown for linear algebra. <em>TOG</em>,
<em>40</em>(6), 1–14. (<a
href="https://doi.org/10.1145/3478513.3480506">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Communicating linear algebra in written form is challenging: mathematicians must choose between writing in languages that produce well-formatted but semantically-underdefined representations such as LaTeX; or languages with well-defined semantics but notation unlike conventional math, such as C++/Eigen. In both cases, the underlying linear algebra is obfuscated by the requirements of esoteric language syntax (as in LaTeX) or awkward APIs due to language semantics (as in C++). The gap between representations results in communication challenges, including underspecified and irrepro-ducible research results, difficulty teaching math concepts underlying complex numerical code, as well as repeated, redundant, and error-prone translations from communicated linear algebra to executable code. We introduce I♥LA, a language with syntax designed to closely mimic conventionally-written linear algebra, while still ensuring an unambiguous, compilable interpretation. Inspired by Markdown, a language for writing naturally-structured plain text files that translate into valid HTML, I♥LA allows users to write linear algebra in text form and compile the same source into LaTeX, C++/Eigen, Python/NumPy/SciPy, and MATLAB, with easy extension to further math programming environments. We outline the principles of our language design and highlight design decisions that balance between readability and precise semantics, and demonstrate through case studies the ability for I♥LA to bridge the semantic gap between conventionally-written linear algebra and unambiguous interpretation in math programming environments.},
  archive      = {J_TOG},
  doi          = {10.1145/3478513.3480506},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-14},
  shortjournal = {ACM Trans. Graph.},
  title        = {I♥LA: Compilable markdown for linear algebra},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Generative modelling of BRDF textures from flash images.
<em>TOG</em>, <em>40</em>(6), 1–13. (<a
href="https://doi.org/10.1145/3478513.3480507">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We learn a latent space for easy capture, consistent interpolation, and efficient reproduction of visual material appearance. When users provide a photo of a stationary natural material captured under flashlight illumination, first it is converted into a latent material code. Then, in the second step, conditioned on the material code, our method produces an infinite and diverse spatial field of BRDF model parameters (diffuse albedo, normals, roughness, specular albedo) that subsequently allows rendering in complex scenes and illuminations, matching the appearance of the input photograph. Technically, we jointly embed all flash images into a latent space using a convolutional encoder, and -conditioned on these latent codes- convert random spatial fields into fields of BRDF parameters using a convolutional neural network (CNN). We condition these BRDF parameters to match the visual characteristics (statistics and spectra of visual features) of the input under matching light. A user study compares our approach favorably to previous work, even those with access to BRDF supervision. Project webpage: https://henzler.github.io/publication/neuralmaterial/.},
  archive      = {J_TOG},
  doi          = {10.1145/3478513.3480507},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Generative modelling of BRDF textures from flash images},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Generalized adaptive refinement for grid-based hexahedral
meshing. <em>TOG</em>, <em>40</em>(6), 1–13. (<a
href="https://doi.org/10.1145/3478513.3480508">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to their nice numerical properties, conforming hexahedral meshes are considered a prominent computational domain for simulation tasks. However, the automatic decomposition of a general 3D volume into a small number of hexahedral elements is very challenging. Methods that create an adaptive Cartesian grid and convert it into a conforming mesh offer superior robustness and are the only ones concretely used in the industry. Topological schemes that permit this conversion can be applied only if precise compatibility conditions among grid elements are observed. Some of these conditions are local, hence easy to formulate; others are not and are much harder to satisfy. State-of-the-art approaches fulfill these conditions by prescribing additional refinement based on special building rules for octrees. These methods operate in a restricted space of solutions and are prone to severely over-refine the input grids, creating a bottleneck in the simulation pipeline. In this article, we introduce a novel approach to transform a general adaptive grid into a new grid meeting hexmeshing criteria, without resorting to tree rules. Our key insight is that we can formulate all compatibility conditions as linear constraints in an integer programming problem by choosing the proper set of unknowns. Since we operate in a broader solution space, we are able to meet topological hexmeshing criteria at a much coarser scale than methods using octrees, also supporting generalized grids of any shape or topology. We demonstrate the superiority of our approach for both traditional grid-based hexmeshing and adaptive polycube-based hexmeshing. In all our experiments, our method never prescribed more refinement than the prior art and, in the average case, it introduced close to half the number of extra cells.},
  archive      = {J_TOG},
  doi          = {10.1145/3478513.3480508},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Generalized adaptive refinement for grid-based hexahedral meshing},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Rendering with style: Combining traditional and neural
approaches for high-quality face rendering. <em>TOG</em>,
<em>40</em>(6), 1–14. (<a
href="https://doi.org/10.1145/3478513.3480509">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For several decades, researchers have been advancing techniques for creating and rendering 3D digital faces, where a lot of the effort has gone into geometry and appearance capture, modeling and rendering techniques. This body of research work has largely focused on facial skin, with much less attention devoted to peripheral components like hair, eyes and the interior of the mouth. As a result, even with the best technology for facial capture and rendering, in most high-end productions a lot of artist time is still spent modeling the missing components and fine-tuning the rendering parameters to combine everything into photo-real digital renders. In this work we propose to combine incomplete, high-quality renderings showing only facial skin with recent methods for neural rendering of faces, in order to automatically and seamlessly create photo-realistic full-head portrait renders from captured data without the need for artist intervention. Our method begins with traditional face rendering, where the skin is rendered with the desired appearance, expression, viewpoint, and illumination. These skin renders are then projected into the latent space of a pre-trained neural network that can generate arbitrary photo-real face images (StyleGAN2). The result is a sequence of realistic face images that match the identity and appearance of the 3D character at the skin level, but is completed naturally with synthesized hair, eyes, inner mouth and surroundings. Notably, we present the first method for multi-frame consistent projection into this latent space, allowing photo-realistic rendering and preservation of the identity of the digital human over an animated performance sequence, which can depict different expressions, lighting conditions and viewpoints. Our method can be used in new face rendering pipelines and, importantly, in other deep learning applications that require large amounts of realistic training data with ground-truth 3D geometry, appearance maps, lighting, and viewpoint.},
  archive      = {J_TOG},
  doi          = {10.1145/3478513.3480509},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Rendering with style: Combining traditional and neural approaches for high-quality face rendering},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Ensemble denoising for monte carlo renderings. <em>TOG</em>,
<em>40</em>(6), 1–17. (<a
href="https://doi.org/10.1145/3478513.3480510">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Various denoising methods have been proposed to clean up the noise in Monte Carlo (MC) renderings, each having different advantages, disadvantages, and applicable scenarios. In this paper, we present Ensemble Denoising , an optimization-based technique that combines multiple individual MC denoisers. The combined image is modeled as a per-pixel weighted sum of output images from the individual denoisers. Computation of the optimal weights is formulated as a constrained quadratic programming problem, where we apply a dual-buffer strategy to estimate the overall MSE. We further propose an iterative solver to overcome practical issues involved in the optimization. Besides nice theoretical properties, our ensemble denoiser is demonstrated to be effective and robust, and outperforms any individual denoiser across dozens of scenes and different levels of sample rates. We also perform a comprehensive analysis on the selection of individual denoisers to be combined, providing important and practical guides for users.},
  archive      = {J_TOG},
  doi          = {10.1145/3478513.3480510},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-17},
  shortjournal = {ACM Trans. Graph.},
  title        = {Ensemble denoising for monte carlo renderings},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). AdaptiBrush: Adaptive general and predictable VR ribbon
brush. <em>TOG</em>, <em>40</em>(6), 1–15. (<a
href="https://doi.org/10.1145/3478513.3480511">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual reality drawing applications let users draw 3D shapes using brushes that form ribbon shaped, or ruled-surface, strokes. Each ribbon is uniquely defined by its user-specified ruling length, path, and the ruling directions at each point along this path. Existing brushes use the trajectory of a handheld controller in 3D space as the ribbon path, and compute the ruling directions using a fixed mapping from a specific controller coordinate-frame axis. This fixed mapping forces users to rotate the controller and thus their wrists to change ribbon normal or ruling directions, and requires substantial physical effort to draw even medium complexity ribbons. Since human ability to rotate their wrists continuously is heavily restricted, the space of ribbon geometries users can comfortably draw using these brushes is limited. These brushes can be unpredictable, producing ribbons with unexpectedly varying width or flipped and wobbly normals in response to seemingly natural hand gestures. Our AdaptiBrush ribbon brush system dramatically extends the space of ribbon geometries users can comfortably draw while enabling them to accurately predict the ribbon shape that a given hand motion produces. We achieve this by introducing a novel adaptive ruling direction computation method, enabling users to easily change ribbon ruling and normal orientation using predominantly translational controller, and thus wrist, motion. We facilitate ease-of-use by computing predictable ruling directions that smoothly change in both world and controller coordinate systems, and facilitate ease-of-learning by prioritizing ruling directions which are well-aligned with one of the controller coordinate system axes. Our comparative user studies confirm that our more general and predictable ruling computation leads to significant improvements in brush usability and effectiveness compared to all prior brushes; in a head to head comparison users preferred AdaptiBrush over the next-best brush by a margin of 2 to 1.},
  archive      = {J_TOG},
  doi          = {10.1145/3478513.3480511},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-15},
  shortjournal = {ACM Trans. Graph.},
  title        = {AdaptiBrush: Adaptive general and predictable VR ribbon brush},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Motion recommendation for online character control.
<em>TOG</em>, <em>40</em>(6), 1–16. (<a
href="https://doi.org/10.1145/3478513.3480512">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reinforcement learning (RL) has been proven effective in many scenarios, including environment exploration and motion planning. However, its application in data-driven character control has produced relatively simple motion results compared to recent approaches that have used large complex motion data without RL. In this paper, we provide a real-time motion control method that can generate high-quality and complex motion results from various sets of unstructured data while retaining the advantage of using RL, which is the discovery of optimal behaviors by trial and error. We demonstrate the results for a character achieving different tasks, from simple direction control to complex avoidance of moving obstacles. Our system works equally well on biped/quadruped characters, with motion data ranging from 1 to 48 minutes, without any manual intervention. To achieve this, we exploit a finite set of discrete actions, where each action represents full-body future motion features. We first define a subset of actions that can be selected in each state and store these pieces of information in databases during the preprocessing step. The use of this subset of actions enables the effective learning of control policy even from a large set of motion data. To achieve interactive performance at run-time, we adopt a proposal network and a k-nearest neighbor action sampler.},
  archive      = {J_TOG},
  doi          = {10.1145/3478513.3480512},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Motion recommendation for online character control},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Reproducing reality with a high-dynamic-range multi-focal
stereo display. <em>TOG</em>, <em>40</em>(6), 1–14. (<a
href="https://doi.org/10.1145/3478513.3480513">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With well-established methods for producing photo-realistic results, the next big challenge of graphics and display technologies is to achieve perceptual realism --- producing imagery indistinguishable from real-world 3D scenes. To deliver all necessary visual cues for perceptual realism, we built a High-Dynamic-Range Multi-Focal Stereo Display that achieves high resolution, accurate color, a wide dynamic range, and most depth cues, including binocular presentation and a range of focal depth. The display and associated imaging system have been designed to capture and reproduce a small near-eye three-dimensional object and to allow for a direct comparison between virtual and real scenes. To assess our reproduction of realism and demonstrate the capability of the display and imaging system, we conducted an experiment in which the participants were asked to discriminate between a virtual object and its physical counterpart. Our results indicate that the participants can only detect the discrepancy with a probability of 0.44. With such a level of perceptual realism, our display apparatus can facilitate a range of visual experiments that require the highest fidelity of reproduction while allowing for the full control of the displayed stimuli.},
  archive      = {J_TOG},
  doi          = {10.1145/3478513.3480513},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Reproducing reality with a high-dynamic-range multi-focal stereo display},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Perceptual model for adaptive local shading and refresh
rate. <em>TOG</em>, <em>40</em>(6), 1–18. (<a
href="https://doi.org/10.1145/3478513.3480514">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When the rendering budget is limited by power or time, it is necessary to find the combination of rendering parameters, such as resolution and refresh rate, that could deliver the best quality. Variable-rate shading (VRS), introduced in the last generations of GPUs, enables fine control of the rendering quality, in which each 16×16 image tile can be rendered with a different ratio of shader executions. We take advantage of this capability and propose a new method for adaptive control of local shading and refresh rate. The method analyzes texture content, on-screen velocities, luminance, and effective resolution and suggests the refresh rate and a VRS state map that maximizes the quality of animated content under a limited budget. The method is based on the new content-adaptive metric of judder, aliasing, and blur, which is derived from the psychophysical models of contrast sensitivity. To calibrate and validate the metric, we gather data from literature and also collect new measurements of motion quality under variable shading rates, different velocities of motion, texture content, and display capabilities, such as refresh rate, persistence, and angular resolution. The proposed metric and adaptive shading method is implemented as a game engine plugin. Our experimental validation shows a substantial increase in preference of our method over rendering with a fixed resolution and refresh rate, and an existing motion-adaptive technique.},
  archive      = {J_TOG},
  doi          = {10.1145/3478513.3480514},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-18},
  shortjournal = {ACM Trans. Graph.},
  title        = {Perceptual model for adaptive local shading and refresh rate},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Semi-supervised video-driven facial animation transfer for
production. <em>TOG</em>, <em>40</em>(6), 1–18. (<a
href="https://doi.org/10.1145/3478513.3480515">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a simple algorithm for automatic transfer of facial expressions, from videos to a 3D character, as well as between distinct 3D characters through their rendered animations. Our method begins by learning a common, semantically-consistent latent representation for the different input image domains using an unsupervised image-to-image translation model. It subsequently learns, in a supervised manner, a linear mapping from the character images&#39; encoded representation to the animation coefficients. At inference time, given the source domain (i.e., actor footage), it regresses the corresponding animation coefficients for the target character. Expressions are automatically remapped between the source and target identities despite differences in physiognomy. We show how our technique can be used in the context of markerless motion capture with controlled lighting conditions, for one actor and for multiple actors. Additionally, we show how it can be used to automatically transfer facial animation between distinct characters without consistent mesh parameterization and without engineered geometric priors. We compare our method with standard approaches used in production and with recent state-of-the-art models on single camera face tracking.},
  archive      = {J_TOG},
  doi          = {10.1145/3478513.3480515},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-18},
  shortjournal = {ACM Trans. Graph.},
  title        = {Semi-supervised video-driven facial animation transfer for production},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Generalized deployable elastic geodesic grids. <em>TOG</em>,
<em>40</em>(6), 1–15. (<a
href="https://doi.org/10.1145/3478513.3480516">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given a designer created free-form surface in 3d space, our method computes a grid composed of elastic elements which are completely planar and straight. Only by fixing the ends of the planar elements to appropriate locations, the 2d grid bends and approximates the given 3d surface. Our method is based purely on the notions from differential geometry of curves and surfaces and avoids any physical simulations. In particular, we introduce a well-defined elastic grid energy functional that allows identifying networks of curves that minimize the bending energy and at the same time nestle to the provided input surface well. Further, we generalize the concept of such grids to cases where the surface boundary does not need to be convex, which allows for the creation of sophisticated and visually pleasing shapes. The algorithm finally ensures that the 2d grid is perfectly planar, making the resulting gridshells inexpensive, easy to fabricate, transport, assemble, and finally also to deploy. Additionally, since the whole structure is pre-strained, it also comes with load-bearing capabilities. We evaluate our method using physical simulation and we also provide a full fabrication pipeline for desktop-size models and present multiple examples of surfaces with elliptic and hyperbolic curvature regions. Our method is meant as a tool for quick prototyping for designers, architects, and engineers since it is very fast and results can be obtained in a matter of seconds.},
  archive      = {J_TOG},
  doi          = {10.1145/3478513.3480516},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Generalized deployable elastic geodesic grids},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Polarimetric spatio-temporal light transport probing.
<em>TOG</em>, <em>40</em>(6), 1–18. (<a
href="https://doi.org/10.1145/3478513.3480517">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Light emitted from a source into a scene can undergo complex interactions with multiple scene surfaces of different material types before being reflected towards a detector. During this transport, every surface reflection and propagation is encoded in the properties of the photons that ultimately reach the detector, including travel time, direction, intensity, wavelength and polarization. Conventional imaging systems capture intensity by integrating over all other dimensions of the incident light into a single quantity, hiding this rich scene information in these aggregate measurements. Existing methods are capable of untangling these measurements into their spatial and temporal dimensions, fueling geometric scene understanding tasks. However, examining polarimetric material properties jointly with geometric properties is an open challenge that could enable unprecedented capabilities beyond geometric scene understanding, allowing for material-dependent scene understanding and imaging through complex transport, such as macroscopic scattering. In this work, we close this gap, and propose a computational light transport imaging method that captures the spatially- and temporally-resolved complete polarimetric response of a scene, which encodes rich material properties. Our method hinges on a novel 7D tensor theory of light transport. We discover low-rank structure in the polarimetric tensor dimension and propose a data-driven rotating ellipsometry method that learns to exploit redundancy of polarimetric structure. We instantiate our theory with two imaging prototypes: spatio-polarimetric imaging and coaxial temporal-polarimetric imaging. This allows us, for the first time, to decompose scene light transport into temporal, spatial, and complete polarimetric dimensions that unveil scene properties hidden to conventional methods. We validate the applicability of our method on diverse tasks, including shape reconstruction with subsurface scattering, seeing through scattering media, untangling multi-bounce light transport, breaking metamerism with polarization, and spatio-polarimetric decomposition of crystals.},
  archive      = {J_TOG},
  doi          = {10.1145/3478513.3480517},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-18},
  shortjournal = {ACM Trans. Graph.},
  title        = {Polarimetric spatio-temporal light transport probing},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Neural marching cubes. <em>TOG</em>, <em>40</em>(6), 1–15.
(<a href="https://doi.org/10.1145/3478513.3480518">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce Neural Marching Cubes , a data-driven approach for extracting a triangle mesh from a discretized implicit field. We base our meshing approach on Marching Cubes (MC), due to the simplicity of its input, namely a uniform grid of signed distances or occupancies, which frequently arise in surface reconstruction and from neural implicit models. However, classical MC is defined by coarse tessellation templates isolated to individual cubes. While more refined tessellations have been proposed by several MC variants, they all make heuristic assumptions, such as trilinearity, when determining the vertex positions and local mesh topologies in each cube. In principle, none of these approaches can reconstruct geometric features that reveal coherence or dependencies between nearby cubes (e.g., a sharp edge ), as such information is unaccounted for, resulting in poor estimates of the true underlying implicit field. To tackle these challenges, we re-cast MC from a deep learning perspective, by designing tessellation templates more apt at preserving geometric features, and learning the vertex positions and mesh topologies from training meshes, to account for contextual information from nearby cubes. We develop a compact per-cube parameterization to represent the output triangle mesh, while being compatible with neural processing, so that a simple 3D convolutional network can be employed for the training. We show that all topological cases in each cube that are applicable to our design can be easily derived using our representation, and the resulting tessellations can also be obtained naturally and efficiently by following a few design guidelines. In addition, our network learns local features with limited receptive fields, hence it generalizes well to new shapes and new datasets. We evaluate our neural MC approach by quantitative and qualitative comparisons to all well-known MC variants. In particular, we demonstrate the ability of our network to recover sharp features such as edges and corners, a long-standing issue of MC and its variants. Our network also reconstructs local mesh topologies more accurately than previous approaches. Code and data are available at https://github.com/czq142857/NMC.},
  archive      = {J_TOG},
  doi          = {10.1145/3478513.3480518},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Neural marching cubes},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). ICTree: Automatic perceptual metrics for tree models.
<em>TOG</em>, <em>40</em>(6), 1–15. (<a
href="https://doi.org/10.1145/3478513.3480519">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many algorithms for virtual tree generation exist, but the visual realism of the 3D models is unknown. This problem is usually addressed by performing limited user studies or by a side-by-side visual comparison. We introduce an automated system for realism assessment of the tree model based on their perception. We conducted a user study in which 4,000 participants compared over one million pairs of images to collect subjective perceptual scores of a large dataset of virtual trees. The scores were used to train two neural-network-based predictors. A view independent ICTreeF uses the tree model&#39;s geometric features that are easy to extract from any model. The second is ICTreeI that estimates the perceived visual realism of a tree from its image. Moreover, to provide an insight into the problem, we deduce intrinsic attributes and evaluate which features make trees look like real trees. In particular, we show that branching angles, length of branches, and widths are critical for perceived realism. We also provide three datasets: carefully curated 3D tree geometries and tree skeletons with their perceptual scores, multiple views of the tree geometries with their scores, and a large dataset of images with scores suitable for training deep neural networks.},
  archive      = {J_TOG},
  doi          = {10.1145/3478513.3480519},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-15},
  shortjournal = {ACM Trans. Graph.},
  title        = {ICTree: Automatic perceptual metrics for tree models},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Foids: Bio-inspired fish simulation for generating synthetic
datasets. <em>TOG</em>, <em>40</em>(6), 1–15. (<a
href="https://doi.org/10.1145/3478513.3480520">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a bio-inspired fish simulation platform, which we call &quot;Foids&quot;, to generate realistic synthetic datasets for an use in computer vision algorithm training. This is a first-of-its-kind synthetic dataset platform for fish, which generates all the 3D scenes just with a simulation. One of the major challenges in deep learning based computer vision is the preparation of the annotated dataset. It is already hard to collect a good quality video dataset with enough variations; moreover, it is a painful process to annotate a sufficiently large video dataset frame by frame. This is especially true when it comes to a fish dataset because it is difficult to set up a camera underwater and the number of fish (target objects) in the scene can range up to 30,000 in a fish cage on a fish farm. All of these fish need to be annotated with labels such as a bounding box or silhouette, which can take hours to complete manually, even for only a few minutes of video. We solve this challenge by introducing a realistic synthetic dataset generation platform that incorporates details of biology and ecology studied in the aquaculture field. Because it is a simulated scene, it is easy to generate the scene data with annotation labels from the 3D mesh geometry data and transformation matrix. To this end, we develop an automated fish counting system utilizing the part of synthetic dataset that shows comparable counting accuracy to human eyes, which reduces the time compared to the manual process, and reduces physical injuries sustained by the fish.},
  archive      = {J_TOG},
  doi          = {10.1145/3478513.3480520},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Foids: Bio-inspired fish simulation for generating synthetic datasets},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Repulsive surfaces. <em>TOG</em>, <em>40</em>(6), 1–19. (<a
href="https://doi.org/10.1145/3478513.3480521">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Functionals that penalize bending or stretching of a surface play a key role in geometric and scientific computing, but to date have ignored a very basic requirement: in many situations, surfaces must not pass through themselves or each other. This paper develops a numerical framework for optimization of surface geometry while avoiding (self-)collision. The starting point is the tangent-point energy , which effectively pushes apart pairs of points that are close in space but distant along the surface. We develop a discretization of this energy for triangle meshes, and introduce a novel acceleration scheme based on a fractional Sobolev inner product. In contrast to similar schemes developed for curves, we avoid the complexity of building a multiresolution mesh hierarchy by decomposing our preconditioner into two ordinary Poisson equations, plus forward application of a fractional differential operator. We further accelerate this scheme via hierarchical approximation, and describe how to incorporate a variety of constraints (on area, volume, etc. ). Finally, we explore how this machinery might be applied to problems in mathematical visualization, geometric modeling, and geometry processing.},
  archive      = {J_TOG},
  doi          = {10.1145/3478513.3480521},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-19},
  shortjournal = {ACM Trans. Graph.},
  title        = {Repulsive surfaces},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Integer coordinates for intrinsic geometry processing.
<em>TOG</em>, <em>40</em>(6), 1–13. (<a
href="https://doi.org/10.1145/3478513.3480522">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper describes a numerically robust data structure for encoding intrinsic triangulations of polyhedral surfaces. Many applications demand a correspondence between the intrinsic triangulation and the input surface, but existing data structures either rely on floating point values to encode correspondence, or do not support remeshing operations beyond basic edge flips. We instead provide an integer-based data structure that guarantees valid correspondence, even for meshes with near-degenerate elements. Our starting point is the framework of normal coordinates from geometric topology, which we extend to the broader set of operations needed for mesh processing (vertex insertion, edge splits, etc. ). The resulting data structure can be used as a drop-in replacement for earlier schemes, automatically improving reliability across a wide variety of applications. As a stress test, we successfully compute an intrinsic Delaunay refinement and associated subdivision for all manifold meshes in the Thingi10k dataset. In turn, we can compute reliable and highly accurate solutions to partial differential equations even on extremely low-quality meshes.},
  archive      = {J_TOG},
  doi          = {10.1145/3478513.3480522},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Integer coordinates for intrinsic geometry processing},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Q-zip: Singularity editing primitive for quad meshes.
<em>TOG</em>, <em>40</em>(6), 1–13. (<a
href="https://doi.org/10.1145/3478513.3480523">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Singularity editing of a quadrangle mesh consists in shifting singularities around for either improving the quality of the mesh elements or canceling extraneous singularities, so as to increase mesh regularity. However, the particular structure of a quad mesh renders the exploration of allowable connectivity changes non-local and hard to automate. In this paper, we introduce a simple, principled, and general quad-mesh editing primitive with which pairs of arbitrarily distant singularities can be efficiently displaced around a mesh through a deterministic and reversible chain of local topological operations with a minimal footprint. Dubbed Q-zip as it acts as a zipper opening up and collapsing down quad strips, our practical mesh operator for singularity editing can be easily implemented via parallel transport of a reference compass between any two irregular vertices. Batches of Q-zips performed in parallel can then be used for efficient singularity editing.},
  archive      = {J_TOG},
  doi          = {10.1145/3478513.3480523},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Q-zip: Singularity editing primitive for quad meshes},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Kaleidoscopic structured light. <em>TOG</em>,
<em>40</em>(6), 1–15. (<a
href="https://doi.org/10.1145/3478513.3480524">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Full surround 3D imaging for shape acquisition is essential for generating digital replicas of real-world objects. Surrounding an object we seek to scan with a kaleidoscope, that is, a configuration of multiple planar mirrors, produces an image of the object that encodes information from a combinatorially large number of virtual viewpoints. This information is practically useful for the full surround 3D reconstruction of the object, but cannot be used directly, as we do not know what virtual viewpoint each image pixel corresponds---the pixel label. We introduce a structured light system that combines a projector and a camera with a kaleidoscope. We then prove that we can accurately determine the labels of projector and camera pixels, for arbitrary kaleidoscope configurations, using the projector-camera epipolar geometry. We use this result to show that our system can serve as a multi-view structured light system with hundreds of virtual projectors and cameras. This makes our system capable of scanning complex shapes precisely and with full coverage. We demonstrate the advantages of the kaleidoscopic structured light system by scanning objects that exhibit a large range of shapes and reflectances.},
  archive      = {J_TOG},
  doi          = {10.1145/3478513.3480524},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Kaleidoscopic structured light},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning to reconstruct botanical trees from single images.
<em>TOG</em>, <em>40</em>(6), 1–15. (<a
href="https://doi.org/10.1145/3478513.3480525">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a novel method for reconstructing the 3D geometry of botanical trees from single photographs. Faithfully reconstructing a tree from single-view sensor data is a challenging and open problem because many possible 3D trees exist that fit the tree&#39;s shape observed from a single view. We address this challenge by defining a reconstruction pipeline based on three neural networks. The networks simultaneously mask out trees in input photographs, identify a tree&#39;s species, and obtain its 3D radial bounding volume - our novel 3D representation for botanical trees. Radial bounding volumes (RBV) are used to orchestrate a procedural model primed on learned parameters to grow a tree that matches the main branching structure and the overall shape of the captured tree. While the RBV allows us to faithfully reconstruct the main branching structure, we use the procedural model&#39;s morphological constraints to generate realistic branching for the tree crown. This constraints the number of solutions of tree models for a given photograph of a tree. We show that our method reconstructs various tree species even when the trees are captured in front of complex backgrounds. Moreover, although our neural networks have been trained on synthetic data with data augmentation, we show that our pipeline performs well for real tree photographs. We evaluate the reconstructed geometries with several metrics, including leaf area index and maximum radial tree distances.},
  archive      = {J_TOG},
  doi          = {10.1145/3478513.3480525},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Learning to reconstruct botanical trees from single images},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Computing sparse cones with bounded distortion for conformal
parameterizations. <em>TOG</em>, <em>40</em>(6), 1–9. (<a
href="https://doi.org/10.1145/3478513.3480526">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel method to generate sparse cone singularities with bounded distortion constraints for conformal parameterizations. It is formulated as minimizing the ℓ 0 -norm of Gaussian curvature of vertices with hard constraints of bounding the distortion that is measured by the ℓ 2 -norm of the log conformal factor. We use the reweighted ℓ 1 -norm to approximate the ℓ 0 -norm and solve each convex weighted ℓ 1 minimization subproblem by the Douglas-Rachford (DR) splitting scheme. To quickly generate sparse cones, we modify DR splitting by weighting the ℓ 2 -norm of the proximal mapping to force the small Gaussian curvature to quickly approach zero. Accordingly, compared with the conventional DR splitting, the modified method performs one to two orders of magnitude faster. Besides, we perform variable substitution of log conformal factors to simplify the computation process for acceleration. Our algorithm is able to bound distortion to compute sparse cone singularities, so that the resulting conformal parameterizations achieve a favorable tradeoff between the area distortion and the number of cones. We demonstrate its effectiveness and feasibility on a large number of models.},
  archive      = {J_TOG},
  doi          = {10.1145/3478513.3480526},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-9},
  shortjournal = {ACM Trans. Graph.},
  title        = {Computing sparse cones with bounded distortion for conformal parameterizations},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SuperTrack: Motion tracking for physically simulated
characters using supervised learning. <em>TOG</em>, <em>40</em>(6),
1–13. (<a href="https://doi.org/10.1145/3478513.3480527">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we show how the task of motion tracking for physically simulated characters can be solved using supervised learning and optimizing a policy directly via back-propagation. To achieve this we make use of a world model trained to approximate a specific subset of the environment&#39;s transition function, effectively acting as a differentiable physics simulator through which the policy can be optimized to minimize the tracking error. Compared to popular model-free methods of physically simulated character control which primarily make use of Proximal Policy Optimization (PPO) we find direct optimization of the policy via our approach consistently achieves a higher quality of control in a shorter training time, with a reduced sensitivity to the rate of experience gathering, dataset size, and distribution.},
  archive      = {J_TOG},
  doi          = {10.1145/3478513.3480527},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-13},
  shortjournal = {ACM Trans. Graph.},
  title        = {SuperTrack: Motion tracking for physically simulated characters using supervised learning},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Neural actor: Neural free-view synthesis of human actors
with pose control. <em>TOG</em>, <em>40</em>(6), 1–16. (<a
href="https://doi.org/10.1145/3478513.3480528">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose Neural Actor (NA), a new method for high-quality synthesis of humans from arbitrary viewpoints and under arbitrary controllable poses. Our method is developed upon recent neural scene representation and rendering works which learn representations of geometry and appearance from only 2D images. While existing works demonstrated compelling rendering of static scenes and playback of dynamic scenes, photo-realistic reconstruction and rendering of humans with neural implicit methods, in particular under user-controlled novel poses, is still difficult. To address this problem, we utilize a coarse body model as a proxy to unwarp the surrounding 3D space into a canonical pose. A neural radiance field learns pose-dependent geometric deformations and pose- and view-dependent appearance effects in the canonical space from multi-view video input. To synthesize novel views of high-fidelity dynamic geometry and appearance, NA leverages 2D texture maps defined on the body model as latent variables for predicting residual deformations and the dynamic appearance. Experiments demonstrate that our method achieves better quality than the state-of-the-arts on playback as well as novel pose synthesis, and can even generalize well to new poses that starkly differ from the training poses. Furthermore, our method also supports shape control on the free-view synthesis of human actors.},
  archive      = {J_TOG},
  doi          = {10.1145/3478513.3480528},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Neural actor: Neural free-view synthesis of human actors with pose control},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Keypoint-driven line drawing vectorization via PolyVector
flow. <em>TOG</em>, <em>40</em>(6), 1–17. (<a
href="https://doi.org/10.1145/3478513.3480529">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Line drawing vectorization is a daily task in graphic design, computer animation, and engineering, necessary to convert raster images to a set of curves for editing and geometry processing. Despite recent progress in the area, automatic vectorization tools often produce spurious branches or incorrect connectivity around curve junctions; or smooth out sharp corners. These issues detract from the use of such vectorization tools, both from an aesthetic viewpoint and for feasibility of downstream applications (e.g., automatic coloring or inbetweening). We address these problems by introducing a novel line drawing vectorization algorithm that splits the task into three components: (1) finding keypoints, i.e., curve endpoints, junctions, and sharp corners; (2) extracting drawing topology, i.e., finding connections between keypoints; and (3) computing the geometry of those connections. We compute the optimal geometry of the connecting curves via a novel geometric flow --- PolyVector Flow --- that aligns the curves to the drawing, disambiguating directions around Y-, X-, and T-junctions. We show that our system robustly infers both the geometry and topology of detailed complex drawings. We validate our system both quantitatively and qualitatively, demonstrating that our method visually outperforms previous work.},
  archive      = {J_TOG},
  doi          = {10.1145/3478513.3480529},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-17},
  shortjournal = {ACM Trans. Graph.},
  title        = {Keypoint-driven line drawing vectorization via PolyVector flow},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Physical light-matter interaction in hermite-gauss space.
<em>TOG</em>, <em>40</em>(6), 1–17. (<a
href="https://doi.org/10.1145/3478513.3480530">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Our purpose in this paper is two-fold: introduce a computationally-tractable decomposition of the coherence properties of light; and, present a general-purpose light-matter interaction framework for partially-coherent light. In a recent publication, Steinberg and Yan [2021] introduced a framework that generalises the classical radiometry-based light transport to physical optics. This facilitates a qualitative increase in the scope of optical phenomena that can be rendered, however with the additional expressibility comes greater analytic difficulty: This coherence of light, which is the core quantity of physical light transport, depends initially on the characteristics of the light source, and mutates on interaction with matter and propagation. Furthermore, current tools that aim to quantify the interaction of partially-coherent light with matter remain limited to specific materials and are computationally intensive. To practically represent a wide class of coherence functions, we decompose their modal content in Hermite-Gauss space and derive a set of light-matter interaction formulae, which quantify how matter scatters light and affects its coherence properties. Then, we model matter as a locally-stationary random process, generalizing the prevalent deterministic and stationary stochastic descriptions. This gives rise to a framework that is able to formulate the interaction of arbitrary partially-coherent light with a wide class of matter. Indeed, we will show that our presented formalism unifies a few of the state-of-the-art scatter and diffraction formulae into one cohesive theory. This formulae include the sourcing of partially-coherent light, scatter by rough surfaces and microgeometry, diffraction grating and interference by a layered structure.},
  archive      = {J_TOG},
  doi          = {10.1145/3478513.3480530},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-17},
  shortjournal = {ACM Trans. Graph.},
  title        = {Physical light-matter interaction in hermite-gauss space},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). ExtraNet: Real-time extrapolated rendering for low-latency
temporal supersampling. <em>TOG</em>, <em>40</em>(6), 1–16. (<a
href="https://doi.org/10.1145/3478513.3480531">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Both the frame rate and the latency are crucial to the performance of realtime rendering applications such as video games. Spatial supersampling methods, such as the Deep Learning SuperSampling (DLSS), have been proven successful at decreasing the rendering time of each frame by rendering at a lower resolution. But temporal supersampling methods that directly aim at producing more frames on the fly are still not practically available. This is mainly due to both its own computational cost and the latency introduced by interpolating frames from the future. In this paper, we present ExtraNet, an efficient neural network that predicts accurate shading results on an extrapolated frame, to minimize both the performance overhead and the latency. With the help of the rendered auxiliary geometry buffers of the extrapolated frame, and the temporally reliable motion vectors, we train our ExtraNet to perform two tasks simultaneously: irradiance in-painting for regions that cannot find historical correspondences, and accurate ghosting-free shading prediction for regions where temporal information is available. We present a robust hole-marking strategy to automate the classification of these tasks, as well as the data generation from a series of high-quality production-ready scenes. Finally, we use lightweight gated convolutions to enable fast inference. As a result, our ExtraNet is able to produce plausibly extrapolated frames without easily noticeable artifacts, delivering a 1.5× to near 2× increase in frame rates with minimized latency in practice.},
  archive      = {J_TOG},
  doi          = {10.1145/3478513.3480531},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-16},
  shortjournal = {ACM Trans. Graph.},
  title        = {ExtraNet: Real-time extrapolated rendering for low-latency temporal supersampling},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Weatherscapes: Nowcasting heat transfer and water
continuity. <em>TOG</em>, <em>40</em>(6), 1–19. (<a
href="https://doi.org/10.1145/3478513.3480532">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the complex interplay of various meteorological phenomena, simulating weather is a challenging and open research problem. In this contribution, we propose a novel physics-based model that enables simulating weather at interactive rates. By considering atmosphere and pedosphere we can define the hydrologic cycle - and consequently weather - in unprecedented detail. Specifically, our model captures different warm and cold clouds, such as mammatus, hole-punch, multi-layer, and cumulonimbus clouds as well as their dynamic transitions. We also model different precipitation types, such as rain, snow, and graupel by introducing a comprehensive microphysics scheme. The Wegener-Bergeron-Findeisen process is incorporated into our Kessler-type microphysics formulation covering ice crystal growth occurring in mixed-phase clouds. Moreover, we model the water run-off from the ground surface, the infiltration into the soil, and its subsequent evaporation back to the atmosphere. We account for daily temperature changes, as well as heat transfer between pedosphere and atmosphere leading to a complex feedback loop. Our framework enables us to interactively explore various complex weather phenomena. Our results are assessed visually and validated by simulating weatherscapes for various setups covering different precipitation events and environments, by showcasing the hydrologic cycle, and by reproducing common effects such as Foehn winds. We also provide quantitative evaluations creating high-precipitation cumulonimbus clouds by prescribing atmospheric conditions based on infrared satellite observations. With our model we can generate dynamic 3D scenes of weatherscapes with high visual fidelity and even nowcast real weather conditions as simulations by streaming weather data into our framework.},
  archive      = {J_TOG},
  doi          = {10.1145/3478513.3480532},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-19},
  shortjournal = {ACM Trans. Graph.},
  title        = {Weatherscapes: Nowcasting heat transfer and water continuity},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Camera keyframing with style and control. <em>TOG</em>,
<em>40</em>(6), 1–13. (<a
href="https://doi.org/10.1145/3478513.3480533">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel technique that enables 3D artists to synthesize camera motions in virtual environments following a camera style , while enforcing user-designed camera keyframes as constraints along the sequence. To solve this constrained motion in-betweening problem, we design and train a camera motion generator from a collection of temporal cinematic features (camera and actor motions) using a conditioning on target keyframes. We further condition the generator with a style code to control how to perform the interpolation between the keyframes. Style codes are generated by training a second network that encodes different camera behaviors in a compact latent space, the camera style space. Camera behaviors are defined as temporal correlations between actor features and camera motions and can be extracted from real or synthetic film clips. We further extend the system by incorporating a fine control of camera speed and direction via a hidden state mapping technique. We evaluate our method on two aspects: i) the capacity to synthesize style-aware camera trajectories with user defined keyframes; and ii) the capacity to ensure that in-between motions still comply with the reference camera style while satisfying the keyframe constraints. As a result, our system is the first style-aware keyframe in-betweening technique for camera control that balances style-driven automation with precise and interactive control of keyframes.},
  archive      = {J_TOG},
  doi          = {10.1145/3478513.3480533},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Camera keyframing with style and control},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-class inverted stippling. <em>TOG</em>,
<em>40</em>(6), 1–12. (<a
href="https://doi.org/10.1145/3478513.3480534">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce inverted stippling , a method to mimic an inversion technique used by artists when performing stippling. To this end, we extend Linde-Buzo-Gray (LBG) stippling to multi-class LBG (MLBG) stippling with multiple layers. MLBG stippling couples the layers stochastically to optimize for per-layer and overall blue-noise properties. We propose a stipple-based filling method to generate solid color backgrounds for inverting areas. Our experiments demonstrate the effectiveness of MLBG in terms of reducing overlapping and intensity accuracy. In addition, we showcase MLBG with color stippling and dynamic multi-class blue-noise sampling, which is possible due to its support for temporal coherence.},
  archive      = {J_TOG},
  doi          = {10.1145/3478513.3480534},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-12},
  shortjournal = {ACM Trans. Graph.},
  title        = {Multi-class inverted stippling},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Tessellation-free displacement mapping for ray tracing.
<em>TOG</em>, <em>40</em>(6), 1–16. (<a
href="https://doi.org/10.1145/3478513.3480535">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Displacement mapping is a powerful mechanism for adding fine to medium geometric details over a 3D surface using a 2D map encoding them. While GPU rasterization supports it through the hardware tessellation unit, ray tracing surface meshes textured with high quality displacement requires a significant amount of memory. More precisely, the input surface needs to be pre-tessellated at the displacement map resolution before being enriched with its mandatory acceleration data structure. Consequently, designing displacement maps interactively while enjoying a full physically-based rendering is often impossible, as simply tiling multiple times the map quickly saturates the graphics memory. In this work we introduce a new tessellation-free displacement mapping approach for ray tracing. Our key insight is to decouple the displacement from its base domain by mapping a displacement-specific acceleration structures directly on the mesh. As a result, our method shows low memory footprint and fast high resolution displacement rendering, making interactive displacement editing possible.},
  archive      = {J_TOG},
  doi          = {10.1145/3478513.3480535},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Tessellation-free displacement mapping for ray tracing},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Spiral-spectral fluid simulation. <em>TOG</em>,
<em>40</em>(6), 1–16. (<a
href="https://doi.org/10.1145/3478513.3480536">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a fast, expressive method for simulating fluids over radial domains, including discs, spheres, cylinders, ellipses, spheroids, and tori. We do this by generalizing the spectral approach of Laplacian Eigenfunctions, resulting in what we call spiral-spectral fluid simulations. Starting with a set of divergence-free analytical bases for polar and spherical coordinates, we show that their singularities can be removed by introducing a set of carefully selected enrichment functions. Orthogonality is established at minimal cost, viscosity is supported analytically, and we specifically design basis functions that support scalable FFT-based reconstructions. Additionally, we present an efficient way of computing all the necessary advection tensors. Our approach applies to both three-dimensional flows as well as their surface-based, codimensional variants. We establish the completeness of our basis representation, and compare against a variety of existing solvers.},
  archive      = {J_TOG},
  doi          = {10.1145/3478513.3480536},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Spiral-spectral fluid simulation},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Barbershop: GAN-based image compositing using segmentation
masks. <em>TOG</em>, <em>40</em>(6), 1–13. (<a
href="https://doi.org/10.1145/3478513.3480537">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Seamlessly blending features from multiple images is extremely challenging because of complex relationships in lighting, geometry, and partial occlusion which cause coupling between different parts of the image. Even though recent work on GANs enables synthesis of realistic hair or faces, it remains difficult to combine them into a single, coherent, and plausible image rather than a disjointed set of image patches. We present a novel solution to image blending, particularly for the problem of hairstyle transfer, based on GAN-inversion. We propose a novel latent space for image blending which is better at preserving detail and encoding spatial information, and propose a new GAN-embedding algorithm which is able to slightly modify images to conform to a common segmentation mask. Our novel representation enables the transfer of the visual properties from multiple reference images including specific details such as moles and wrinkles, and because we do image blending in a latent-space we are able to synthesize images that are coherent. Our approach avoids blending artifacts present in other approaches and finds a globally consistent image. Our results demonstrate a significant improvement over the current state of the art in a user study, with users preferring our blending solution over 95 percent of the time. Source code for the new approach is available at https://zpdesu.github.io/Barbershop.},
  archive      = {J_TOG},
  doi          = {10.1145/3478513.3480537},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Barbershop: GAN-based image compositing using segmentation masks},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). FreeStyleGAN: Free-view editable portrait rendering with the
camera manifold. <em>TOG</em>, <em>40</em>(6), 1–15. (<a
href="https://doi.org/10.1145/3478513.3480538">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current Generative Adversarial Networks (GANs) produce photorealistic renderings of portrait images. Embedding real images into the latent space of such models enables high-level image editing. While recent methods provide considerable semantic control over the (re-)generated images, they can only generate a limited set of viewpoints and cannot explicitly control the camera. Such 3D camera control is required for 3D virtual and mixed reality applications. In our solution, we use a few images of a face to perform 3D reconstruction, and we introduce the notion of the GAN camera manifold, the key element allowing us to precisely define the range of images that the GAN can reproduce in a stable manner. We train a small face-specific neural implicit representation network to map a captured face to this manifold and complement it with a warping scheme to obtain free-viewpoint novel-view synthesis. We show how our approach - due to its precise camera control - enables the integration of a pre-trained StyleGAN into standard 3D rendering pipelines, allowing e.g., stereo rendering or consistent insertion of faces in synthetic 3D environments. Our solution proposes the first truly free-viewpoint rendering of realistic faces at interactive rates, using only a small number of casual photos as input, while simultaneously allowing semantic editing capabilities, such as facial expression or lighting changes.},
  archive      = {J_TOG},
  doi          = {10.1145/3478513.3480538},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-15},
  shortjournal = {ACM Trans. Graph.},
  title        = {FreeStyleGAN: Free-view editable portrait rendering with the camera manifold},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). FrictionalMonolith: A monolithic optimization-based approach
for granular flow with contact-aware rigid-body coupling. <em>TOG</em>,
<em>40</em>(6), 1–20. (<a
href="https://doi.org/10.1145/3478513.3480539">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose FrictionalMonolith , a monolithic pressure-friction-contact solver for more accurately, robustly, and efficiently simulating two-way interactions of rigid bodies with continuum granular materials or inviscid liquids. By carefully formulating the components of such systems within a single unified minimization problem, our solver can simultaneously handle unilateral incompressibility and implicit integration of friction for the interior of the continuum, frictional contact resolution among the rigid bodies, and mutual force exchanges between the continuum and rigid bodies. Our monolithic approach eliminates various problematic artifacts in existing weakly coupled approaches, including loss of volume in the continuum material, artificial drift and slip of the continuum at solid boundaries, interpenetrations of rigid bodies, and simulation instabilities. To efficiently handle this challenging monolithic minimization problem, we present a customized solver for the resulting quadratically constrained quadratic program that combines elements of staggered projections, augmented Lagrangian methods, inexact projected Newton, and active-set methods. We demonstrate the critical importance of a unified treatment and the effectiveness of our proposed solver in a range of practical scenarios.},
  archive      = {J_TOG},
  doi          = {10.1145/3478513.3480539},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-20},
  shortjournal = {ACM Trans. Graph.},
  title        = {FrictionalMonolith: A monolithic optimization-based approach for granular flow with contact-aware rigid-body coupling},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). EyelashNet: A dataset and a baseline method for eyelash
matting. <em>TOG</em>, <em>40</em>(6), 1–17. (<a
href="https://doi.org/10.1145/3478513.3480540">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Eyelashes play a crucial part in the human facial structure and largely affect the facial attractiveness in modern cosmetic design. However, the appearance and structure of eyelashes can easily induce severe artifacts in high-fidelity multi-view 3D face reconstruction. Unfortunately it is highly challenging to remove eyelashes from portrait images using both traditional and learning-based matting methods due to the delicate nature of eyelashes and the lack of eyelash matting dataset. To this end, we present EyelashNet, the first eyelash matting dataset which contains 5,400 high-quality eyelash matting data captured from real world and 5,272 virtual eyelash matting data created by rendering avatars. Our work consists of a capture stage and an inference stage to automatically capture and annotate eyelashes instead of tedious manual efforts. The capture is based on a specifically-designed fluorescent labeling system. By coloring the eyelashes with a safe and invisible fluorescent substance, our system takes paired photos with colored and normal eyelashes by turning the equipped ultraviolet (UVA) flash on and off. We further correct the alignment between each pair of photos and use a novel alpha matte inference network to extract the eyelash alpha matte. As there is no prior eyelash dataset, we propose a progressive training strategy that progressively fuses captured eyelash data with virtual eyelash data to learn the latent semantics of real eyelashes. As a result, our method can accurately extract eyelash alpha mattes from fuzzy and self-shadow regions such as pupils, which is almost impossible by manual annotations. To validate the advantage of EyelashNet, we present a baseline method based on deep learning that achieves state-of-the-art eyelash matting performance with RGB portrait images as input. We also demonstrate that our work can largely benefit important real applications including high-fidelity personalized avatar and cosmetic design.},
  archive      = {J_TOG},
  doi          = {10.1145/3478513.3480540},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-17},
  shortjournal = {ACM Trans. Graph.},
  title        = {EyelashNet: A dataset and a baseline method for eyelash matting},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A material point method for nonlinearly magnetized
materials. <em>TOG</em>, <em>40</em>(6), 1–13. (<a
href="https://doi.org/10.1145/3478513.3480541">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel numerical scheme to simulate interactions between a magnetic field and nonlinearly magnetized objects immersed in it. Under our nonlinear magnetization framework, the strength of magnetic forces is effectively saturated to produce stable simulations without requiring any parameter tuning. The mathematical model of our approach is based upon Langevin&#39;s nonlinear theory of paramagnetism, which bridges microscopic structures and macroscopic equations after a statistical derivation. We devise a hybrid Eulerian-Lagrangian numerical approach to simulating this strongly nonlinear process by leveraging the discrete material points to transfer both material properties and the number density of magnetic micro-particles in the simulation domain. The magnetic equations can then be built and solved efficiently on a background Cartesian grid, followed by a finite difference method to incorporate magnetic forces. The multi-scale coupling can be processed naturally by employing the established particle-grid interpolation schemes in a conventional MLS-MPM framework. We demonstrate the efficacy of our approach with a host of simulation examples governed by magnetic-mechanical coupling effects, ranging from magnetic deformable bodies to magnetic viscous fluids with nonlinear elastic constitutive laws.},
  archive      = {J_TOG},
  doi          = {10.1145/3478513.3480541},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-13},
  shortjournal = {ACM Trans. Graph.},
  title        = {A material point method for nonlinearly magnetized materials},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Neural 3D holography: Learning accurate wave propagation
models for 3D holographic virtual and augmented reality displays.
<em>TOG</em>, <em>40</em>(6), 1–12. (<a
href="https://doi.org/10.1145/3478513.3480542">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Holographic near-eye displays promise unprecedented capabilities for virtual and augmented reality (VR/AR) systems. The image quality achieved by current holographic displays, however, is limited by the wave propagation models used to simulate the physical optics. We propose a neural network-parameterized plane-to-multiplane wave propagation model that closes the gap between physics and simulation. Our model is automatically trained using camera feedback and it outperforms related techniques in 2D plane-to-plane settings by a large margin. Moreover, it is the first network-parameterized model to naturally extend to 3D settings, enabling high-quality 3D computer-generated holography using a novel phase regularization strategy of the complex-valued wave field. The efficacy of our approach is demonstrated through extensive experimental evaluation with both VR and optical see-through AR display prototypes.},
  archive      = {J_TOG},
  doi          = {10.1145/3478513.3480542},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-12},
  shortjournal = {ACM Trans. Graph.},
  title        = {Neural 3D holography: Learning accurate wave propagation models for 3D holographic virtual and augmented reality displays},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Beyond mie theory: Systematic computation of bulk scattering
parameters based on microphysical wave optics. <em>TOG</em>,
<em>40</em>(6), 1–12. (<a
href="https://doi.org/10.1145/3478513.3480543">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Light scattering in participating media and translucent materials is typically modeled using the radiative transfer theory. Under the assumption of independent scattering between particles, it utilizes several bulk scattering parameters to statistically characterize light-matter interactions at the macroscale. To calculate these parameters based on microscale material properties, the Lorenz-Mie theory has been considered the gold standard. In this paper, we present a generalized framework capable of systematically and rigorously computing bulk scattering parameters beyond the far-field assumption of Lorenz-Mie theory. Our technique accounts for microscale wave-optics effects such as diffraction and interference as well as interactions between nearby particles. Our framework is general, can be plugged in any renderer supporting Lorenz-Mie scattering, and allows arbitrary packing rates and particles correlation; we demonstrate this generality by computing bulk scattering parameters for a wide range of materials, including anisotropic and correlated media.},
  archive      = {J_TOG},
  doi          = {10.1145/3478513.3480543},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-12},
  shortjournal = {ACM Trans. Graph.},
  title        = {Beyond mie theory: Systematic computation of bulk scattering parameters based on microphysical wave optics},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Generalized fluid carving with fast lattice-guided seam
computation. <em>TOG</em>, <em>40</em>(6), 1–15. (<a
href="https://doi.org/10.1145/3478513.3480544">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we introduce a novel method for intelligently resizing a wide range of volumetric data including fluids. Fluid carving, the technique we build upon, only supported particle-based liquid data, and because it was based on image-based techniques, it was constrained to rectangular boundaries. We address these limitations to allow a much more versatile method for volumetric post-processing. By enclosing a region of interest in our lattice structure, users can retarget regions of a volume with non-rectangular boundaries and non-axis-aligned motion. Our approach generalizes to images, videos, liquids, meshes, and even previously unexplored domains such as fire and smoke. We also present a seam computation method that is significantly faster than the previous approach while maintaining the same level of quality, thus making our method more viable for production settings where post-processing workflows are vital.},
  archive      = {J_TOG},
  doi          = {10.1145/3478513.3480544},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Generalized fluid carving with fast lattice-guided seam computation},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Modeling clothing as a separate layer for an animatable
human avatar. <em>TOG</em>, <em>40</em>(6), 1–15. (<a
href="https://doi.org/10.1145/3478513.3480545">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We have recently seen great progress in building photorealistic animatable full-body codec avatars, but generating high-fidelity animation of clothing is still difficult. To address these difficulties, we propose a method to build an animatable clothed body avatar with an explicit representation of the clothing on the upper body from multi-view captured videos. We use a two-layer mesh representation to register each 3D scan separately with the body and clothing templates. In order to improve the photometric correspondence across different frames, texture alignment is then performed through inverse rendering of the clothing geometry and texture predicted by a variational autoencoder. We then train a new two-layer codec avatar with separate modeling of the upper clothing and the inner body layer. To learn the interaction between the body dynamics and clothing states, we use a temporal convolution network to predict the clothing latent code based on a sequence of input skeletal poses. We show photorealistic animation output for three different actors, and demonstrate the advantage of our clothed-body avatars over the single-layer avatars used in previous work. We also show the benefit of an explicit clothing model that allows the clothing texture to be edited in the animation output.},
  archive      = {J_TOG},
  doi          = {10.1145/3478513.3480545},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Modeling clothing as a separate layer for an animatable human avatar},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Layered neural atlases for consistent video editing.
<em>TOG</em>, <em>40</em>(6), 1–12. (<a
href="https://doi.org/10.1145/3478513.3480546">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a method that decomposes, and &quot;unwraps&quot;, an input video into a set of layered 2D atlases , each providing a unified representation of the appearance of an object (or background) over the video. For each pixel in the video, our method estimates its corresponding 2D coordinate in each of the atlases, giving us a consistent parameterization of the video, along with an associated alpha (opacity) value. Importantly, we design our atlases to be interpretable and semantic, which facilitates easy and intuitive editing in the atlas domain, with minimal manual work required. Edits applied to a single 2D atlas (or input video frame) are automatically and consistently mapped back to the original video frames, while preserving occlusions, deformation, and other complex scene effects such as shadows and reflections. Our method employs a coordinate-based Multilayer Perceptron (MLP) representation for mappings, atlases, and alphas, which are jointly optimized on a per-video basis, using a combination of video reconstruction and regularization losses. By operating purely in 2D, our method does not require any prior 3D knowledge about scene geometry or camera poses, and can handle complex dynamic real world videos. We demonstrate various video editing applications, including texture mapping, video style transfer, image-to-video texture transfer, and segmentation/labeling propagation, all automatically produced by editing a single 2D atlas image.},
  archive      = {J_TOG},
  doi          = {10.1145/3478513.3480546},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-12},
  shortjournal = {ACM Trans. Graph.},
  title        = {Layered neural atlases for consistent video editing},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Path graphs: Iterative path space filtering. <em>TOG</em>,
<em>40</em>(6), 1–15. (<a
href="https://doi.org/10.1145/3478513.3480547">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To render higher quality images from the samples generated by path tracing with a low sample count, we propose a novel path reuse approach that processes a fixed collection of paths to iteratively refine and improve radiance estimates throughout the scene. Our method operates on a path graph consisting of the union of the traced paths with additional neighbor edges inserted among clustered nearby vertices. Our approach refines the initial noisy radiance estimates via an aggregation operator, treating vertices within clusters as independent sampling techniques that can be combined using MIS. In a novel step, we also introduce a propagation operator to forward the refined estimates along the paths to successive bounces. We apply the aggregation and propagation operations to the graph iteratively, progressively refining the radiance values, converging to fixed-point radiance estimates with lower variance than the original ones. We also introduce a decorrelation (final gather) step, which uses information already in the graph and is cheap to compute, allowing us to combine the method with standard denoisers. Our approach is lightweight, in the sense that it can be easily plugged into any standard path tracer and neural final image denoiser. Furthermore, it is independent of scene complexity, as the graph size only depends on image resolution and average path depth. We demonstrate that our technique leads to realistic rendering results starting from as low as 1 path per pixel, even in complex indoor scenes dominated by multi-bounce indirect illumination.},
  archive      = {J_TOG},
  doi          = {10.1145/3478513.3480547},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Path graphs: Iterative path space filtering},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Modeling flower pigmentation patterns. <em>TOG</em>,
<em>40</em>(6), 1–14. (<a
href="https://doi.org/10.1145/3478513.3480548">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although many simulation models of natural phenomena have been developed to date, little attention was given to a major contributor to the beauty of nature: the colorful patterns of flowers. We survey typical patterns and propose methods for simulating them inspired by the current understanding of the biology of floral patterning. The patterns are generated directly on geometric models of flowers, using different combinations of key mathematical models of morphogenesis: vascular patterning, positional information, reaction-diffusion, and random pattern generation. The integration of these models makes it possible to capture a wide range of the flower pigmentation patterns observed in nature.},
  archive      = {J_TOG},
  doi          = {10.1145/3478513.3480548},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Modeling flower pigmentation patterns},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Practical pigment mixing for digital painting. <em>TOG</em>,
<em>40</em>(6), 1–11. (<a
href="https://doi.org/10.1145/3478513.3480549">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is a significant flaw in today&#39;s painting software: the colors do not mix like actual paints. E.g., blue and yellow make gray instead of green. This is because the software is built around the RGB representation, which models the mixing of colored lights. Paints, however, get their color from pigments, whose mixing behavior is predicted by the Kubelka-Munk model (K-M). Although it was introduced to computer graphics almost 30 years ago, the K-M model has never been adopted by painting software in practice as it would require giving up the RGB representation, growing the number of per-pixel channels substantially, and depriving the users of painting with arbitrary RGB colors. In this paper, we introduce a practical approach that enables mixing colors with K-M while keeping everything in RGB. We achieve this by establishing a latent color space, where RGB colors are represented as mixtures ofprimary pigments together with additive residuals. The latents can be manipulated with linear operations, leading to expected, plausible results. We describe the conversion between RGB and our latent representation, and show how to implement it efficiently.},
  archive      = {J_TOG},
  doi          = {10.1145/3478513.3480549},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-11},
  shortjournal = {ACM Trans. Graph.},
  title        = {Practical pigment mixing for digital painting},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Physically-based feature line rendering. <em>TOG</em>,
<em>40</em>(6), 1–11. (<a
href="https://doi.org/10.1145/3478513.3480550">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature lines visualize the shape and structure of 3D objects, and are an essential component of many non-photorealistic rendering styles. Existing feature line rendering methods, however, are only able to render feature lines in limited contexts, such as on immediately visible surfaces or in specular reflections. We present a novel, path-based method for feature line rendering that allows for the accurate rendering of feature lines in the presence of complex physical phenomena such as glossy reflection, depth-of-field, and dispersion. Our key insight is that feature lines can be modeled as view-dependent light sources. These light sources can be sampled as a part of ordinary paths , and seamlessly integrate into existing physically-based rendering methods. We illustrate the effectiveness of our method in several real-world rendering scenarios with a variety of different physical phenomena.},
  archive      = {J_TOG},
  doi          = {10.1145/3478513.3480550},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-11},
  shortjournal = {ACM Trans. Graph.},
  title        = {Physically-based feature line rendering},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sum-of-squares geometry processing. <em>TOG</em>,
<em>40</em>(6), 1–13. (<a
href="https://doi.org/10.1145/3478513.3480551">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Geometry processing presents a variety of difficult numerical problems, each seeming to require its own tailored solution. This breadth is largely due to the expansive list of geometric primitives , e.g., splines, triangles, and hexahedra, joined with an ever-expanding variety of objectives one might want to achieve with them. With the recent increase in attention toward higher-order surfaces , we can expect a variety of challenges porting existing solutions that work on triangle meshes to work on these more complex geometry types. In this paper, we present a framework for solving many core geometry processing problems on higher-order surfaces. We achieve this goal through sum-of-squares optimization, which transforms nonlinear polynomial optimization problems into sequences of convex problems whose complexity is captured by a single degree parameter. This allows us to solve a suite of problems on higher-order surfaces, such as continuous collision detection and closest point queries on curved patches, with only minor changes between formulations and geometries.},
  archive      = {J_TOG},
  doi          = {10.1145/3478513.3480551},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Sum-of-squares geometry processing},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Optimizing contact-based assemblies. <em>TOG</em>,
<em>40</em>(6), 1–19. (<a
href="https://doi.org/10.1145/3478513.3480552">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern fabrication methods have greatly simplified manufacturing of complex free-form shapes at an affordable cost, and opened up new possibilities for improving functionality and customization through automatic optimization, shape optimization in particular. However, most existing shape optimization methods focus on single parts. In this work, we focus on supporting shape optimization for assemblies, more specifically, assemblies that are held together by contact and friction. Examples of which include furniture joints, construction set assemblies, certain types of prosthetic devices and many other. To enable this optimization, we present a framework supporting robust and accurate optimization of a number of important functionals, while enforcing constraints essential for assembly functionality: weight, stress, difficulty of putting the assembly together, and how reliably it stays together. Our framework is based on smoothed formulation of elasticity equations with contact, analytically derived shape derivatives, and robust remeshing to enable large changes of shape, and at the same time, maintain accuracy. We demonstrate the improvements it can achieve for a number of computational and experimental examples.},
  archive      = {J_TOG},
  doi          = {10.1145/3478513.3480552},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-19},
  shortjournal = {ACM Trans. Graph.},
  title        = {Optimizing contact-based assemblies},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Neural frame interpolation for rendered content.
<em>TOG</em>, <em>40</em>(6), 1–13. (<a
href="https://doi.org/10.1145/3478513.3480553">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The demand for creating rendered content continues to drastically grow. As it often is extremely computationally expensive and thus costly to render high-quality computer-generated images, there is a high incentive to reduce this computational burden. Recent advances in learning-based frame interpolation methods have shown exciting progress but still have not achieved the production-level quality which would be required to render fewer pixels and achieve savings in rendering times and costs. Therefore, in this paper we propose a method specifically targeted to achieve high-quality frame interpolation for rendered content. In this setting, we assume that we have full input for every n -th frame in addition to auxiliary feature buffers that are cheap to evaluate (e.g. depth, normals, albedo) for every frame. We propose solutions for leveraging such auxiliary features to obtain better motion estimates, more accurate occlusion handling, and to correctly reconstruct non-linear motion between keyframes. With this, our method is able to significantly push the state-of-the-art in frame interpolation for rendered content and we are able to obtain production-level quality results.},
  archive      = {J_TOG},
  doi          = {10.1145/3478513.3480553},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Neural frame interpolation for rendered content},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Differentiable surface triangulation. <em>TOG</em>,
<em>40</em>(6), 1–13. (<a
href="https://doi.org/10.1145/3478513.3480554">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Triangle meshes remain the most popular data representation for surface geometry. This ubiquitous representation is essentially a hybrid one that decouples continuous vertex locations from the discrete topological triangulation. Unfortunately, the combinatorial nature of the triangulation prevents taking derivatives over the space of possible meshings of any given surface. As a result, to date, mesh processing and optimization techniques have been unable to truly take advantage of modular gradient descent components of modern optimization frameworks. In this work, we present a differentiable surface triangulation that enables optimization for any per-vertex or per-face differentiable objective function over the space of underlying surface triangulations. Our method builds on the result that any 2D triangulation can be achieved by a suitably perturbed weighted Delaunay triangulation. We translate this result into a computational algorithm by proposing a soft relaxation of the classical weighted Delaunay triangulation and optimizing over vertex weights and vertex locations. We extend the algorithm to 3D by decomposing shapes into developable sets and differentiably meshing each set with suitable boundary constraints. We demonstrate the efficacy of our method on various planar and surface meshes on a range of difficult-to-optimize objective functions. Our code can be found online: https://github.com/mrakotosaon/diff-surface-triangulation.},
  archive      = {J_TOG},
  doi          = {10.1145/3478513.3480554},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Differentiable surface triangulation},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Volume decomposition for two-piece rigid casting.
<em>TOG</em>, <em>40</em>(6), 1–14. (<a
href="https://doi.org/10.1145/3478513.3480555">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a novel technique to automatically decompose an input object&#39;s volume into a set of parts that can be represented by two opposite height fields. Such decomposition enables the manufacturing of individual parts using two-piece reusable rigid molds. Our decomposition strategy relies on a new energy formulation that utilizes a pre-computed signal on the mesh volume representing the accessibility for a predefined set of extraction directions. Thanks to this novel formulation, our method allows for efficient optimization of a fabrication-aware partitioning of volumes in a completely automatic way. We demonstrate the efficacy of our approach by generating valid volume partitionings for a wide range of complex objects and physically reproducing several of them.},
  archive      = {J_TOG},
  doi          = {10.1145/3478513.3480555},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Volume decomposition for two-piece rigid casting},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Optimizing global injectivity for constrained
parameterization. <em>TOG</em>, <em>40</em>(6), 1–18. (<a
href="https://doi.org/10.1145/3478513.3480556">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Injective parameterizations of triangulated meshes are critical across applications but remain challenging to compute. Existing algorithms to find injectivity either require initialization from an injective starting state, which is currently only possible without positional constraints, or else can only prevent triangle inversion, which is insufficient to ensure injectivity. Here we present, to our knowledge, the first algorithm for recovering a globally injective parameterization from an arbitrary non-injective initial mesh subject to stationary constraints. These initial meshes can be inverted, wound about interior vertices and/or overlapping. Our algorithm in turn enables globally injective mapping for meshes with arbitrary positional constraints. Our key contribution is a new energy, called smooth excess area (SEA), that measures non-injectivity in a map. This energy is well-defined across both injective and non-injective maps and is smooth almost everywhere, making it readily minimizable using standard gradient-based solvers starting from a non-injective initial state. Importantly, we show that maps minimizing SEA are guaranteed to be locally injective and almost globally injective, in the sense that the overlapping area can be made arbitrarily small. Analyzing SEA&#39;s behavior over a new benchmark set designed to test injective mapping, we find that optimizing SEA successfully recovers globally injective maps for 85% of the benchmark and obtains locally injective maps for 90%. In contrast, state-of-the-art methods for removing triangle inversion obtain locally injective maps for less than 6% of the benchmark, and achieve global injectivity (largely by chance as prior methods are not designed to recover it) on less than 4%.},
  archive      = {J_TOG},
  doi          = {10.1145/3478513.3480556},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-18},
  shortjournal = {ACM Trans. Graph.},
  title        = {Optimizing global injectivity for constrained parameterization},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Efficient and robust discrete conformal equivalence with
boundary. <em>TOG</em>, <em>40</em>(6), 1–16. (<a
href="https://doi.org/10.1145/3478513.3480557">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We describe an efficient algorithm to compute a discrete metric with prescribed Gaussian curvature at all interior vertices and prescribed geodesic curvature along the boundary of a mesh. The metric is (discretely) conformally equivalent to the input metric. Its construction is based on theory developed in [Gu et al. 2018b] and [Springborn 2020], relying on results on hyperbolic ideal Delaunay triangulations. Generality is achieved by considering the surface&#39;s intrinsic triangulation as a degree of freedom, and particular attention is paid to the proper treatment of surface boundaries. While via a double cover approach the case with boundary can be reduced to the case without boundary quite naturally, the implied symmetry of the setting causes additional challenges related to stable Delaunay-critical configurations that we address explicitly. We furthermore explore the numerical limits of the approach and derive continuous maps from the discrete metrics.},
  archive      = {J_TOG},
  doi          = {10.1145/3478513.3480557},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Efficient and robust discrete conformal equivalence with boundary},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Pose with style: Detail-preserving pose-guided image
synthesis with conditional StyleGAN. <em>TOG</em>, <em>40</em>(6), 1–11.
(<a href="https://doi.org/10.1145/3478513.3480559">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present an algorithm for re-rendering a person from a single image under arbitrary poses. Existing methods often have difficulties in hallucinating occluded contents photo-realistically while preserving the identity and fine details in the source image. We first learn to inpaint the correspondence field between the body surface texture and the source image with a human body symmetry prior. The inpainted correspondence field allows us to transfer/warp local features extracted from the source to the target view even under large pose changes. Directly mapping the warped local features to an RGB image using a simple CNN decoder often leads to visible artifacts. Thus, we extend the StyleGAN generator so that it takes pose as input (for controlling poses) and introduces a spatially varying modulation for the latent space using the warped local features (for controlling appearances). We show that our method compares favorably against the state-of-the-art algorithms in both quantitative evaluation and visual comparison.},
  archive      = {J_TOG},
  doi          = {10.1145/3478513.3480559},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-11},
  shortjournal = {ACM Trans. Graph.},
  title        = {Pose with style: Detail-preserving pose-guided image synthesis with conditional StyleGAN},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Binaural audio generation via multi-task learning.
<em>TOG</em>, <em>40</em>(6), 1–13. (<a
href="https://doi.org/10.1145/3478513.3480560">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a learning-based approach for generating binaural audio from mono audio using multi-task learning. Our formulation leverages additional information from two related tasks: the binaural audio generation task and the flipped audio classification task. Our learning model extracts spatialization features from the visual and audio input, predicts the left and right audio channels, and judges whether the left and right channels are flipped. First, we extract visual features using ResNet from the video frames. Next, we perform binaural audio generation and flipped audio classification using separate subnetworks based on visual features. Our learning method optimizes the overall loss based on the weighted sum of the losses of the two tasks. We train and evaluate our model on the FAIR-Play dataset and the YouTube-ASMR dataset. We perform quantitative and qualitative evaluations to demonstrate the benefits of our approach over prior techniques.},
  archive      = {J_TOG},
  doi          = {10.1145/3478513.3480560},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Binaural audio generation via multi-task learning},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning to cluster for rendering with many lights.
<em>TOG</em>, <em>40</em>(6), 1–10. (<a
href="https://doi.org/10.1145/3478513.3480561">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present an unbiased online Monte Carlo method for rendering with many lights. Our method adapts both the hierarchical light clustering and the sampling distribution to our collected samples. Designing such a method requires us to make clustering decisions under noisy observation, and making sure that the sampling distribution adapts to our target. Our method is based on two key ideas: a coarse-to-fine clustering scheme that can find good clustering configurations even with noisy samples, and a discrete stochastic successive approximation method that starts from a prior distribution and provably converges to a target distribution. We compare to other state-of-the-art light sampling methods, and show better results both numerically and visually.},
  archive      = {J_TOG},
  doi          = {10.1145/3478513.3480561},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-10},
  shortjournal = {ACM Trans. Graph.},
  title        = {Learning to cluster for rendering with many lights},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). AutoMate: A dataset and learning approach for automatic
mating of CAD assemblies. <em>TOG</em>, <em>40</em>(6), 1–18. (<a
href="https://doi.org/10.1145/3478513.3480562">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Assembly modeling is a core task of computer aided design (CAD), comprising around one third of the work in a CAD workflow. Optimizing this process therefore represents a huge opportunity in the design of a CAD system, but current research of assembly based modeling is not directly applicable to modern CAD systems because it eschews the dominant data structure of modern CAD: parametric boundary representations (BREPs). CAD assembly modeling defines assemblies as a system of pairwise constraints, called mates , between parts, which are defined relative to BREP topology rather than in world coordinates common to existing work. We propose SB-GCN, a representation learning scheme on BREPs that retains the topological structure of parts, and use these learned representations to predict CAD type mates. To train our system, we compiled the first large scale dataset of BREP CAD assemblies, which we are releasing along with benchmark mate prediction tasks. Finally, we demonstrate the compatibility of our model with an existing commercial CAD system by building a tool that assists users in mate creation by suggesting mate completions, with 72.2% accuracy.},
  archive      = {J_TOG},
  doi          = {10.1145/3478513.3480562},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-18},
  shortjournal = {ACM Trans. Graph.},
  title        = {AutoMate: A dataset and learning approach for automatic mating of CAD assemblies},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fast and accurate spherical harmonics products.
<em>TOG</em>, <em>40</em>(6), 1–14. (<a
href="https://doi.org/10.1145/3478513.3480563">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spherical Harmonics (SH) have been proven as a powerful tool for rendering, especially in real-time applications such as Precomputed Radiance Transfer (PRT). Spherical harmonics are orthonormal basis functions and are efficient in computing dot products. However, computations of triple product and multiple product operations are often the bottlenecks that prevent moderately high-frequency use of spherical harmonics. Specifically state-of-the-art methods for accurate SH triple products of order n have a time complexity of O ( n 5 ), which is a heavy burden for most real-time applications. Even worse, a brute-force way to compute k -multiple products would take O ( n 2 k ) time. In this paper, we propose a fast and accurate method for spherical harmonics triple products with the time complexity of only O ( n 3 ), and further extend it for computing k -multiple products with the time complexity of O ( kn 3 + k 2 n 2 log ( kn )). Our key insight is to conduct the triple and multiple products in the Fourier space, in which the multiplications can be performed much more efficiently. To our knowledge, our method is theoretically the fastest for accurate spherical harmonics triple and multiple products. And in practice, we demonstrate the efficiency of our method in rendering applications including mid-frequency relighting and shadow fields.},
  archive      = {J_TOG},
  doi          = {10.1145/3478513.3480563},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Fast and accurate spherical harmonics products},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Convex polyhedral meshing for robust solid modeling.
<em>TOG</em>, <em>40</em>(6), 1–16. (<a
href="https://doi.org/10.1145/3478513.3480564">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a new technique to create a mesh of convex polyhedra representing the interior volume of a triangulated input surface. Our approach is particularly tolerant to defects in the input, which is allowed to self-intersect, to be non-manifold, disconnected, and to contain surface holes and gaps. We guarantee that the input surface is exactly represented as the union of polygonal facets of the output volume mesh. Thanks to our algorithm, traditionally difficult solid modeling operations such as mesh booleans and Minkowski sums become surprisingly robust and easy to implement, even if the input has defects. Our technique leverages on the recent concept of indirect geometric predicate to provide an unprecedented combination of guaranteed robustness and speed, thus enabling the practical implementation of robust though flexible solid modeling systems. We have extensively tested our method on all the 10000 models of the Thingi10k dataset, and concluded that no existing method provides comparable robustness, precision and performances.},
  archive      = {J_TOG},
  doi          = {10.1145/3478513.3480564},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Convex polyhedral meshing for robust solid modeling},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Monte carlo denoising via auxiliary feature guided
self-attention. <em>TOG</em>, <em>40</em>(6), 1–13. (<a
href="https://doi.org/10.1145/3478513.3480565">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While self-attention has been successfully applied in a variety of natural language processing and computer vision tasks, its application in Monte Carlo (MC) image denoising has not yet been well explored. This paper presents a self-attention based MC denoising deep learning network based on the fact that self-attention is essentially non-local means filtering in the embedding space which makes it inherently very suitable for the denoising task. Particularly, we modify the standard self-attention mechanism to an auxiliary feature guided self-attention that considers the by-products (e.g., auxiliary feature buffers) of the MC rendering process. As a critical prerequisite to fully exploit the performance of self-attention, we design a multi-scale feature extraction stage, which provides a rich set of raw features for the later self-attention module. As self-attention poses a high computational complexity, we describe several ways that accelerate it. Ablation experiments validate the necessity and effectiveness of the above design choices. Comparison experiments show that the proposed self-attention based MC denoising method outperforms the current state-of-the-art methods.},
  archive      = {J_TOG},
  doi          = {10.1145/3478513.3480565},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Monte carlo denoising via auxiliary feature guided self-attention},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Aesthetic-guided outward image cropping. <em>TOG</em>,
<em>40</em>(6), 1–13. (<a
href="https://doi.org/10.1145/3478513.3480566">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image cropping is a commonly used post-processing operation for adjusting the scene composition of an input photography, therefore improving its aesthetics. Existing automatic image cropping methods are all bounded by the image border, thus have very limited freedom for aesthetics improvement if the original scene composition is far from ideal, e.g. the main object is too close to the image border. In this paper, we propose a novel, aesthetic-guided outward image cropping method. It can go beyond the image border to create a desirable composition that is unachievable using previous cropping methods. Our method first evaluates the input image to determine how much the content of the image should be extrapolated by a field of view (FOV) evaluation model. We then synthesize the image content in the extrapolated region, and seek an optimal aesthetic crop within the expanded FOV, by jointly considering the aesthetics of the cropped view, and the local image quality of the extrapolated image content. Experimental results show that our method can generate more visually pleasing image composition in cases that are difficult for previous image cropping tools due to the border constraint, and can also automatically degrade to an inward method when high quality image extrapolation is infeasible.},
  archive      = {J_TOG},
  doi          = {10.1145/3478513.3480566},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Aesthetic-guided outward image cropping},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Interactive all-hex meshing via cuboid decomposition.
<em>TOG</em>, <em>40</em>(6), 1–17. (<a
href="https://doi.org/10.1145/3478513.3480568">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Standard PolyCube-based hexahedral (hex) meshing methods aim to deform the input domain into an axis-aligned PolyCube volume with integer corners; if this deformation is bijective, then applying the inverse map to the voxelized PolyCube yields a valid hex mesh. A key challenge in these methods is to maintain the bijectivity of the PolyCube deformation, thus reducing the robustness of these algorithms. In this work, we present an interactive pipeline for hex meshing that sidesteps this challenge by using a new representation of PolyCubes as unions of cuboids. We begin by deforming the input tetrahedral mesh into a near-PolyCube domain whose faces are loosely aligned to the major axis directions. We then build a PolyCube by optimizing the layout of a set of cuboids with user guidance to closely fit the deformed domain. Finally, we construct an inversion-free pullback map from the voxelized PolyCube to the input domain while optimizing for mesh quality metrics. We allow extensive user control over each stage, such as editing the voxelized PolyCube, positioning surface vertices, and exploring the trade-off among competing quality metrics, while also providing automatic alternatives. We validate our method on over one hundred shapes, including models that are challenging for past PolyCube-based and frame-field-based methods. Our pipeline reliably produces hex meshes with quality on par with or better than state-of-the-art. We additionally conduct a user study with 21 participants in which the majority prefer hex meshes they make using our tool to the ones from automatic state-of-the-art methods. This demonstrates the need for intuitive interactive hex meshing tools where the user can dictate the priorities of their mesh.},
  archive      = {J_TOG},
  doi          = {10.1145/3478513.3480568},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-17},
  shortjournal = {ACM Trans. Graph.},
  title        = {Interactive all-hex meshing via cuboid decomposition},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Neural radiosity. <em>TOG</em>, <em>40</em>(6), 1–11. (<a
href="https://doi.org/10.1145/3478513.3480569">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce Neural Radiosity, an algorithm to solve the rendering equation by minimizing the norm of its residual, similar as in classical radiosity techniques. Traditional basis functions used in radiosity, such as piecewise polynomials or meshless basis functions are typically limited to representing isotropic scattering from diffuse surfaces. Instead, we propose to leverage neural networks to represent the full four-dimensional radiance distribution, directly optimizing network parameters to minimize the norm of the residual. Our approach decouples solving the rendering equation from rendering (perspective) images similar as in traditional radiosity techniques, and allows us to efficiently synthesize arbitrary views of a scene. In addition, we propose a network architecture using geometric learnable features that improves convergence of our solver compared to previous techniques. Our approach leads to an algorithm that is simple to implement, and we demonstrate its effectiveness on a variety of scenes with diffuse and non-diffuse surfaces.},
  archive      = {J_TOG},
  doi          = {10.1145/3478513.3480569},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-11},
  shortjournal = {ACM Trans. Graph.},
  title        = {Neural radiosity},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Transflower: Probabilistic autoregressive dance generation
with multimodal attention. <em>TOG</em>, <em>40</em>(6), 1–14. (<a
href="https://doi.org/10.1145/3478513.3480570">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dance requires skillful composition of complex movements that follow rhythmic, tonal and timbral features of music. Formally, generating dance conditioned on a piece of music can be expressed as a problem of modelling a high-dimensional continuous motion signal, conditioned on an audio signal. In this work we make two contributions to tackle this problem. First, we present a novel probabilistic autoregressive architecture that models the distribution over future poses with a normalizing flow conditioned on previous poses as well as music context, using a multimodal transformer encoder. Second, we introduce the currently largest 3D dance-motion dataset, obtained with a variety of motion-capture technologies, and including both professional and casual dancers. Using this dataset, we compare our new model against two baselines, via objective metrics and a user study, and show that both the ability to model a probability distribution, as well as being able to attend over a large motion and music context are necessary to produce interesting, diverse, and realistic dance that matches the music.},
  archive      = {J_TOG},
  doi          = {10.1145/3478513.3480570},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {1-14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Transflower: Probabilistic autoregressive dance generation with multimodal attention},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Computational design of planar multistable compliant
structures. <em>TOG</em>, <em>40</em>(5), 1–16. (<a
href="https://doi.org/10.1145/3453477">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a method for designing planar multistable compliant structures. Given a sequence of desired stable states and the corresponding poses of the structure, we identify the topology and geometric realization of a mechanism—consisting of bars and joints—that is able to physically reproduce the desired multistable behavior. In order to solve this problem efficiently, we build on insights from minimally rigid graph theory to identify simple but effective topologies for the mechanism. We then optimize its geometric parameters, such as joint positions and bar lengths, to obtain correct transitions between the given poses. Simultaneously, we ensure adequate stability of each pose based on an effective approximate error metric related to the elastic energy Hessian of the bars in the mechanism. As demonstrated by our results, we obtain functional multistable mechanisms of manageable complexity that can be fabricated using 3D printing. Further, we evaluated the effectiveness of our method on a large number of examples in the simulation and fabricated several physical prototypes.},
  archive      = {J_TOG},
  doi          = {10.1145/3453477},
  journal      = {ACM Transactions on Graphics},
  month        = {10},
  number       = {5},
  pages        = {1-16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Computational design of planar multistable compliant structures},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Tailored reality: Perception-aware scene restructuring for
adaptive VR navigation. <em>TOG</em>, <em>40</em>(5), 1–15. (<a
href="https://doi.org/10.1145/3470847">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In virtual reality (VR), the virtual scenes are pre-designed by creators. Our physical surroundings, however, comprise significantly varied sizes, layouts, and components. To bridge the gap and further enable natural navigation, recent solutions have been proposed to redirect users or recreate the virtual content. However, they suffer from either interrupted experience or distorted appearances. We present a novel VR-oriented algorithm that automatically restructures a given virtual scene for a user’s physical environment. Different from the previous methods, we introduce neither interrupted walking experience nor curved appearances. Instead, a perception-aware function optimizes our retargeting technique to preserve the fidelity of the virtual scene that appears in VR head-mounted displays. Besides geometric and topological properties, it emphasizes the unique first-person view perceptual factors in VR, such as dynamic visibility and objectwise relationships. We conduct both analytical experiments and subjective studies. The results demonstrate our system’s versatile capability and practicability for natural navigation in VR: It reduces the virtual space by 40% without statistical loss of perceptual identicality.},
  archive      = {J_TOG},
  doi          = {10.1145/3470847},
  journal      = {ACM Transactions on Graphics},
  month        = {10},
  number       = {5},
  pages        = {1-15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Tailored reality: Perception-aware scene restructuring for adaptive VR navigation},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Thallo – scheduling for high-performance large-scale
non-linear least-squares solvers. <em>TOG</em>, <em>40</em>(5), 1–14.
(<a href="https://doi.org/10.1145/3453986">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large-scale optimization problems at the core of many graphics, vision, and imaging applications are often implemented by hand in tedious and error-prone processes in order to achieve high performance (in particular on GPUs), despite recent developments in libraries and DSLs. At the same time, these hand-crafted solver implementations reveal that the key for high performance is a problem-specific schedule that enables efficient usage of the underlying hardware. In this work, we incorporate this insight into Thallo, a domain-specific language for large-scale non-linear least squares optimization problems. We observe various code reorganizations performed by implementers of high-performance solvers in the literature, and then define a set of basic operations that span these scheduling choices, thereby defining a large scheduling space. Users can either specify code transformations in a scheduling language or use an autoscheduler. Thallo takes as input a compact, shader-like representation of an energy function and a (potentially auto-generated) schedule, translating the combination into high-performance GPU solvers. Since Thallo can generate solvers from a large scheduling space, it can handle a large set of large-scale non-linear and non-smooth problems with various degrees of non-locality and compute-to-memory ratios, including diverse applications such as bundle adjustment, face blendshape fitting, and spatially-varying Poisson deconvolution, as seen in Figure 1. Abstracting schedules from the optimization, we outperform state-of-the-art GPU-based optimization DSLs by an average of 16× across all applications introduced in this work, and even some published hand-written GPU solvers by 30%+.},
  archive      = {J_TOG},
  doi          = {10.1145/3453986},
  journal      = {ACM Transactions on Graphics},
  month        = {9},
  number       = {5},
  pages        = {1-14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Thallo – scheduling for high-performance large-scale non-linear least-squares solvers},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Skeletonization via local separators. <em>TOG</em>,
<em>40</em>(5), 1–18. (<a
href="https://doi.org/10.1145/3459233">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new algorithm for curve skeleton computation that differs from previous algorithms by being based on the notion of local separators . The main benefits of this approach are that it is able to capture relatively fine details and that it works robustly on a range of shape representations. Specifically, our method works on shape representations that can be construed as spatially embedded graphs. Such representations include meshes, volumetric shapes, and graphs computed from point clouds. We describe a simple pipeline where geometric data are initially converted to a graph, optionally simplified, local separators are computed and selected, and finally a skeleton is constructed. We test our pipeline on polygonal meshes, volumetric shapes, and point clouds. Finally, we compare our results to other methods for skeletonization according to performance and quality.},
  archive      = {J_TOG},
  doi          = {10.1145/3459233},
  journal      = {ACM Transactions on Graphics},
  month        = {9},
  number       = {5},
  pages        = {1-18},
  shortjournal = {ACM Trans. Graph.},
  title        = {Skeletonization via local separators},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A large-scale benchmark and an inclusion-based algorithm for
continuous collision detection. <em>TOG</em>, <em>40</em>(5), 1–16. (<a
href="https://doi.org/10.1145/3460775">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a large-scale benchmark for continuous collision detection (CCD) algorithms, composed of queries manually constructed to highlight challenging degenerate cases and automatically generated using existing simulators to cover common cases. We use the benchmark to evaluate the accuracy, correctness, and efficiency of state-of-the-art continuous collision detection algorithms, both with and without minimal separation. We discover that, despite the widespread use of CCD algorithms, existing algorithms are (1) correct but impractically slow; (2) efficient but incorrect, introducing false negatives that will lead to interpenetration; or (3) correct but over conservative, reporting a large number of false positives that might lead to inaccuracies when integrated in a simulator. By combining the seminal interval root finding algorithm introduced by Snyder in 1992 with modern predicate design techniques, we propose a simple and efficient CCD algorithm. This algorithm is competitive with state-of-the-art methods in terms of runtime while conservatively reporting the time of impact and allowing explicit tradeoff between runtime efficiency and number of false positives reported.},
  archive      = {J_TOG},
  doi          = {10.1145/3460775},
  journal      = {ACM Transactions on Graphics},
  month        = {9},
  number       = {5},
  pages        = {1-16},
  shortjournal = {ACM Trans. Graph.},
  title        = {A large-scale benchmark and an inclusion-based algorithm for continuous collision detection},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Shading rig: Dynamic art-directable stylised shading for 3D
characters. <em>TOG</em>, <em>40</em>(5), 1–14. (<a
href="https://doi.org/10.1145/3461696">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the popularity of three-dimensional (3D) animation techniques, the style of 2D cel animation is seeing increased use in games and interactive applications. However, conventional 3D toon shading frequently requires manual editing to clean up undesired shadows or add stylistic details based on art direction. This editing is impractical for the frame-by-frame editing in cartoon feature film post-production. For interactive stylised media and games, post-production is unavailable due to real-time constraints, so art-direction must be preserved automatically. For these reasons, artists often resort to mesh and texture edits to mitigate undesired shadows typical of toon shaders. Such edits allow real-time rendering but are limited in resolution, animation quality and lack detail control for stylised shadow design. In our framework, artists build a “shading rig,” a collection of these edits, that allows artists to animate toon shading. Artists pre-animate the shading rig under changing lighting, to dynamically preserve artistic intent in a live application, without manual intervention. We show our method preserves continuous motion and shape interpolation, with fewer keyframes than previous work. Our shading shape interpolation is computationally cheaper than state-of-the-art image interpolation techniques. We achieve these improvements while preserving vector quality rendering, without resorting either to high texture resolution or mesh density.},
  archive      = {J_TOG},
  doi          = {10.1145/3461696},
  journal      = {ACM Transactions on Graphics},
  month        = {9},
  number       = {5},
  pages        = {1-14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Shading rig: Dynamic art-directable stylised shading for 3D characters},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Free-viewpoint indoor neural relighting from multi-view
stereo. <em>TOG</em>, <em>40</em>(5), 1–18. (<a
href="https://doi.org/10.1145/3469842">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a neural relighting algorithm for captured indoors scenes, that allows interactive free-viewpoint navigation. Our method allows illumination to be changed synthetically, while coherently rendering cast shadows and complex glossy materials. We start with multiple images of the scene and a three-dimensional mesh obtained by multi-view stereo (MVS) reconstruction. We assume that lighting is well explained as the sum of a view-independent diffuse component and a view-dependent glossy term concentrated around the mirror reflection direction. We design a convolutional network around input feature maps that facilitate learning of an implicit representation of scene materials and illumination, enabling both relighting and free-viewpoint navigation. We generate these input maps by exploiting the best elements of both image-based and physically based rendering. We sample the input views to estimate diffuse scene irradiance, and compute the new illumination caused by user-specified light sources using path tracing. To facilitate the network&#39;s understanding of materials and synthesize plausible glossy reflections, we reproject the views and compute mirror images . We train the network on a synthetic dataset where each scene is also reconstructed with MVS. We show results of our algorithm relighting real indoor scenes and performing free-viewpoint navigation with complex and realistic glossy reflections, which so far remained out of reach for view-synthesis techniques.},
  archive      = {J_TOG},
  doi          = {10.1145/3469842},
  journal      = {ACM Transactions on Graphics},
  month        = {9},
  number       = {5},
  pages        = {1-18},
  shortjournal = {ACM Trans. Graph.},
  title        = {Free-viewpoint indoor neural relighting from multi-view stereo},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Self-sampling for neural point cloud consolidation.
<em>TOG</em>, <em>40</em>(5), 1–14. (<a
href="https://doi.org/10.1145/3470645">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a novel technique for neural point cloud consolidation which learns from only the input point cloud. Unlike other point up-sampling methods which analyze shapes via local patches, in this work, we learn from global subsets. We repeatedly self-sample the input point cloud with global subsets that are used to train a deep neural network. Specifically, we define source and target subsets according to the desired consolidation criteria (e.g., generating sharp points or points in sparse regions). The network learns a mapping from source to target subsets, and implicitly learns to consolidate the point cloud. During inference, the network is fed with random subsets of points from the input, which it displaces to synthesize a consolidated point set. We leverage the inductive bias of neural networks to eliminate noise and outliers, a notoriously difficult problem in point cloud consolidation. The shared weights of the network are optimized over the entire shape, learning non-local statistics and exploiting the recurrence of local-scale geometries. Specifically, the network encodes the distribution of the underlying shape surface within a fixed set of local kernels, which results in the best explanation of the underlying shape surface. We demonstrate the ability to consolidate point sets from a variety of shapes, while eliminating outliers and noise.},
  archive      = {J_TOG},
  doi          = {10.1145/3470645},
  journal      = {ACM Transactions on Graphics},
  month        = {9},
  number       = {5},
  pages        = {1-14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Self-sampling for neural point cloud consolidation},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Optical aberrations correction in postprocessing using
imaging simulation. <em>TOG</em>, <em>40</em>(5), 1–15. (<a
href="https://doi.org/10.1145/3474088">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the popularity of mobile photography continues to grow, considerable effort is being invested in the reconstruction of degraded images. Due to the spatial variation in optical aberrations, which cannot be avoided during the lens design process, recent commercial cameras have shifted some of these correction tasks from optical design to postprocessing systems. However, without engaging with the optical parameters, these systems only achieve limited correction for aberrations. In this work, we propose a practical method for recovering the degradation caused by optical aberrations. Specifically, we establish an imaging simulation system based on our proposed optical point spread function model. Given the optical parameters of the camera, it generates the imaging results of these specific devices. To perform the restoration, we design a spatial-adaptive network model on synthetic data pairs generated by the imaging simulation system, eliminating the overhead of capturing training data by a large amount of shooting and registration. Moreover, we comprehensively evaluate the proposed method in simulations and experimentally with a customized digital-single-lens-reflex camera lens and HUAWEI HONOR 20, respectively. The experiments demonstrate that our solution successfully removes spatially variant blur and color dispersion. When compared with the state-of-the-art deblur methods, the proposed approach achieves better results with a lower computational overhead. Moreover, the reconstruction technique does not introduce artificial texture and is convenient to transfer to current commercial cameras. Project Page: https://github.com/TanGeeGo/ImagingSimulation .},
  archive      = {J_TOG},
  doi          = {10.1145/3474088},
  journal      = {ACM Transactions on Graphics},
  month        = {9},
  number       = {5},
  pages        = {1-15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Optical aberrations correction in postprocessing using imaging simulation},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Recurrent video deblurring with blur-invariant motion
estimation and pixel volumes. <em>TOG</em>, <em>40</em>(5), 1–18. (<a
href="https://doi.org/10.1145/3453720">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For the success of video deblurring, it is essential to utilize information from neighboring frames. Most state-of-the-art video deblurring methods adopt motion compensation between video frames to aggregate information from multiple frames that can help deblur a target frame. However, the motion compensation methods adopted by previous deblurring methods are not blur-invariant, and consequently, their accuracy is limited for blurry frames with different blur amounts. To alleviate this problem, we propose two novel approaches to deblur videos by effectively aggregating information from multiple video frames. First, we present blur-invariant motion estimation learning to improve motion estimation accuracy between blurry frames. Second, for motion compensation, instead of aligning frames by warping with estimated motions, we use a pixel volume that contains candidate sharp pixels to resolve motion estimation errors. We combine these two processes to propose an effective recurrent video deblurring network that fully exploits deblurred previous frames. Experiments show that our method achieves the state-of-the-art performance both quantitatively and qualitatively compared to recent methods that use deep learning.},
  archive      = {J_TOG},
  doi          = {10.1145/3453720},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {5},
  pages        = {1-18},
  shortjournal = {ACM Trans. Graph.},
  title        = {Recurrent video deblurring with blur-invariant motion estimation and pixel volumes},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fine wrinkling on coarsely meshed thin shells. <em>TOG</em>,
<em>40</em>(5), 1–32. (<a
href="https://doi.org/10.1145/3462758">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new model and algorithm to capture the high-definition statics of thin shells via coarse meshes. This model predicts global, fine-scale wrinkling at frequencies much higher than the resolution of the coarse mesh; moreover, it is grounded in the geometric analysis of elasticity, and does not require manual guidance, a corpus of training examples, nor tuning of ad hoc parameters. We first approximate the coarse shape of the shell using tension field theory, in which material forces do not resist compression. We then augment this base mesh with wrinkles, parameterized by an amplitude and phase field that we solve for over the base mesh, which together characterize the geometry of the wrinkles. We validate our approach against both physical experiments and numerical simulations, and we show that our algorithm produces wrinkles qualitatively similar to those predicted by traditional shell solvers requiring orders of magnitude more degrees of freedom.},
  archive      = {J_TOG},
  doi          = {10.1145/3462758},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {5},
  pages        = {1-32},
  shortjournal = {ACM Trans. Graph.},
  title        = {Fine wrinkling on coarsely meshed thin shells},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). AMP: Adversarial motion priors for stylized physics-based
character control. <em>TOG</em>, <em>40</em>(4), 1–20. (<a
href="https://doi.org/10.1145/3450626.3459670">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Synthesizing graceful and life-like behaviors for physically simulated characters has been a fundamental challenge in computer animation. Data-driven methods that leverage motion tracking are a prominent class of techniques for producing high fidelity motions for a wide range of behaviors. However, the effectiveness of these tracking-based methods often hinges on carefully designed objective functions, and when applied to large and diverse motion datasets, these methods require significant additional machinery to select the appropriate motion for the character to track in a given scenario. In this work, we propose to obviate the need to manually design imitation objectives and mechanisms for motion selection by utilizing a fully automated approach based on adversarial imitation learning. High-level task objectives that the character should perform can be specified by relatively simple reward functions, while the low-level style of the character&#39;s behaviors can be specified by a dataset of unstructured motion clips, without any explicit clip selection or sequencing. For example, a character traversing an obstacle course might utilize a task-reward that only considers forward progress, while the dataset contains clips of relevant behaviors such as running, jumping, and rolling. These motion clips are used to train an adversarial motion prior, which specifies style-rewards for training the character through reinforcement learning (RL). The adversarial RL procedure automatically selects which motion to perform, dynamically interpolating and generalizing from the dataset. Our system produces high-quality motions that are comparable to those achieved by state-of-the-art tracking-based techniques, while also being able to easily accommodate large datasets of unstructured motion clips. Composition of disparate skills emerges automatically from the motion prior, without requiring a high-level motion planner or other task-specific annotations of the motion clips. We demonstrate the effectiveness of our framework on a diverse cast of complex simulated characters and a challenging suite of motor control tasks.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459670},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-20},
  shortjournal = {ACM Trans. Graph.},
  title        = {AMP: Adversarial motion priors for stylized physics-based character control},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). QuanTaichi: A compiler for quantized simulations.
<em>TOG</em>, <em>40</em>(4), 1–16. (<a
href="https://doi.org/10.1145/3450626.3459671">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-resolution simulations can deliver great visual quality, but they are often limited by available memory, especially on GPUs. We present a compiler for physical simulation that can achieve both high performance and significantly reduced memory costs, by enabling flexible and aggressive quantization. Low-precision (&quot;quantized&quot;) numerical data types are used and packed to represent simulation states, leading to reduced memory space and bandwidth consumption. Quantized simulation allows higher resolution simulation with less memory, which is especially attractive on GPUs. Implementing a quantized simulator that has high performance and packs the data tightly for aggressive storage reduction would be extremely labor-intensive and error-prone using a traditional programming language. To make the creation of quantized simulation practical, we have developed a new set of language abstractions and a compilation system. A suite of tailored domain-specific optimizations ensure quantized simulators often run as fast as the full-precision simulators, despite the overhead of encoding-decoding the packed quantized data types. Our programming language and compiler, based on Taichi , allow developers to effortlessly switch between different full-precision and quantized simulators, to explore the full design space of quantization schemes, and ultimately to achieve a good balance between space and precision. The creation of quantized simulation with our system has large benefits in terms of memory consumption and performance, on a variety of hardware, from mobile devices to workstations with high-end GPUs. We can simulate with levels of resolution that were previously only achievable on systems with much more memory, such as multiple GPUs. For example, on a single GPU, we can simulate a Game of Life with 20 billion cells (8× compression per pixel), an Eulerian fluid system with 421 million active voxels (1.6× compression per voxel), and a hybrid Eulerian-Lagrangian elastic object simulation with 235 million particles (1.7× compression per particle). At the same time, quantized simulations create physically plausible results. Our quantization techniques are complementary to existing acceleration approaches of physical simulation: they can be used in combination with these existing approaches, such as sparse data structures, for even higher scalability and performance.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459671},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-16},
  shortjournal = {ACM Trans. Graph.},
  title        = {QuanTaichi: A compiler for quantized simulations},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). BRDF importance sampling for polygonal lights. <em>TOG</em>,
<em>40</em>(4), 1–14. (<a
href="https://doi.org/10.1145/3450626.3459672">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advent of real-time ray tracing, there is an increasing interest in GPU-friendly importance sampling techniques. We present such methods to sample convex polygonal lights approximately proportional to diffuse and specular BRDFs times the cosine term. For diffuse surfaces, we sample the polygons proportional to projected solid angle. Our algorithm partitions the polygon suitably and employs inverse function sampling for each part. Inversion of the distribution function is challenging. Using algebraic geometry, we develop a special iterative procedure and an initialization scheme. Together, they achieve high accuracy in all possible situations with only two iterations. Our implementation is numerically stable and fast. For specular BRDFs, this method enables us to sample the polygon proportional to a linearly transformed cosine. We combine these diffuse and specular sampling strategies through novel variants of optimal multiple importance sampling. Our techniques render direct lighting from Lambertian polygonal lights with almost no variance outside of penumbrae and support shadows and textured emission. Additionally, we propose an algorithm for solid angle sampling of polygons. It is faster and more stable than existing methods.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459672},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-14},
  shortjournal = {ACM Trans. Graph.},
  title        = {BRDF importance sampling for polygonal lights},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Guaranteed-quality higher-order triangular meshing of 2D
domains. <em>TOG</em>, <em>40</em>(4), 1–14. (<a
href="https://doi.org/10.1145/3450626.3459673">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a guaranteed quality mesh generation algorithm for the curvilinear triangulation of planar domains with piecewise polynomial boundary. The resulting mesh consists of higher-order triangular elements which are not only regular (i.e., with injective geometric map) but respect strict bounds on quality measures like scaled Jacobian and MIPS distortion. This also implies that the curved triangles&#39; inner angles are bounded from above and below. These are key quality criteria, for instance, in the field of finite element analysis. The domain boundary is reproduced exactly, without geometric approximation error. The central idea is to transform the curvilinear meshing problem into a linear meshing problem via a carefully constructed transformation of bounded distortion, enabling us to leverage key results on guaranteed-quality straight-edge triangulation. The transformation is based on a simple yet general construction and observations about convergence properties of curves under subdivision. Our algorithm can handle arbitrary polynomial order, arbitrarily sharp corners, feature and interface curves, and can be executed using rational arithmetic for strict reliability.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459673},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Guaranteed-quality higher-order triangular meshing of 2D domains},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). End-to-end complex lens design with differentiate ray
tracing. <em>TOG</em>, <em>40</em>(4), 1–13. (<a
href="https://doi.org/10.1145/3450626.3459674">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Imaging systems have long been designed in separated steps: experience-driven optical design followed by sophisticated image processing. Although recent advances in computational imaging aim to bridge the gap in an end-to-end fashion, the image formation models used in these approaches have been quite simplistic, built either on simple wave optics models such as Fourier transform, or on similar paraxial models. Such models only support the optimization of a single lens surface, which limits the achievable image quality. To overcome these challenges, we propose a general end-to-end complex lens design framework enabled by a differentiable ray tracing image formation model. Specifically, our model relies on the differentiable ray tracing rendering engine to render optical images in the full field by taking into account all on/off-axis aberrations governed by the theory of geometric optics. Our design pipeline can jointly optimize the lens module and the image reconstruction network for a specific imaging task. We demonstrate the effectiveness of the proposed method on two typical applications, including large field-of-view imaging and extended depth-of-field imaging. Both simulation and experimental results show superior image quality compared with conventional lens designs. Our framework offers a competitive alternative for the design of modern imaging systems.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459674},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-13},
  shortjournal = {ACM Trans. Graph.},
  title        = {End-to-end complex lens design with differentiate ray tracing},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Video recoloring via spatial-temporal geometric palettes.
<em>TOG</em>, <em>40</em>(4), 1–16. (<a
href="https://doi.org/10.1145/3450626.3459675">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Color correction and color grading are important steps in film production. Recent palette-based approaches to image recoloring have shown that a small set of representative colors provide an intuitive set of handles for color adjustment. However, a single, static palette cannot represent the time-varying colors in a video. We introduce a spatial-temporal geometry-based approach to video recoloring. Specifically, its core is a 4D skew polytope with a few vertices that approximately encloses the video pixels in color and time, which implicitly defines time-varying palettes through slicing of the 4D skew polytope at specific time values. Our geometric palette is compact, descriptive, and provides a correspondence between colors throughout the video, including topological changes when colors merge or split. Experiments show that our method produces natural, artifact-free recoloring.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459675},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Video recoloring via spatial-temporal geometric palettes},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). ROSEFusion: Random optimization for online dense
reconstruction under fast camera motion. <em>TOG</em>, <em>40</em>(4),
1–17. (<a href="https://doi.org/10.1145/3450626.3459676">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online reconstruction based on RGB-D sequences has thus far been restrained to relatively slow camera motions (&lt;1m/s). Under very fast camera motion (e.g., 3m/s), the reconstruction can easily crumble even for the state-of-the-art methods. Fast motion brings two challenges to depth fusion: 1) the high nonlinearity of camera pose optimization due to large inter-frame rotations and 2) the lack of reliably trackable features due to motion blur. We propose to tackle the difficulties of fast-motion camera tracking in the absence of inertial measurements using random optimization, in particular, the Particle Filter Optimization (PFO). To surmount the computation-intensive particle sampling and update in standard PFO, we propose to accelerate the randomized search via updating a particle swarm template (PST). PST is a set of particles pre-sampled uniformly within the unit sphere in the 6D space of camera pose. Through moving and rescaling the pre-sampled PST guided by swarm intelligence, our method is able to drive tens of thousands of particles to locate and cover a good local optimum extremely fast and robustly. The particles, representing candidate poses, are evaluated with a fitness function defined based on depth-model conformance. Therefore, our method, being depth-only and correspondence-free, mitigates the motion blur impediment as (ToF-based) depths are often resilient to motion blur. Thanks to the efficient template-based particle set evolution and the effective fitness function, our method attains good quality pose tracking under fast camera motion (up to 4m/s) in a realtime framerate without including loop closure or global pose optimization. Through extensive evaluations on public datasets of RGB-D sequences, especially on a newly proposed benchmark of fast camera motion, we demonstrate the significant advantage of our method over the state of the arts.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459676},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-17},
  shortjournal = {ACM Trans. Graph.},
  title        = {ROSEFusion: Random optimization for online dense reconstruction under fast camera motion},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Revisiting integration in the material point method: A
scheme for easier separation and less dissipation. <em>TOG</em>,
<em>40</em>(4), 1–16. (<a
href="https://doi.org/10.1145/3450626.3459678">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The material point method (MPM) recently demonstrated its efficacy at simulating many materials and the coupling between them on a massive scale. However, in scenarios containing debris, MPM manifests more dissipation and numerical viscosity than traditional Lagrangian methods. We have two observations from carefully revisiting existing integration methods used in MPM. First, nearby particles would end up with smoothed velocities without recovering momentum for each particle during the particle-grid-particle transfers. Second, most existing integrators assume continuity in the entire domain and advect particles by directly interpolating the positions from deformed nodal positions, which would trap the particles and make them harder to separate. We propose an integration scheme that corrects particle positions at each time step. We demonstrate our method&#39;s effectiveness with several large-scale simulations involving brittle materials. Our approach effectively reduces diffusion and unphysical viscosity compared to traditional integrators.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459678},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Revisiting integration in the material point method: A scheme for easier separation and less dissipation},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Free-form scanning of non-planar appearance with neural
trace photography. <em>TOG</em>, <em>40</em>(4), 1–13. (<a
href="https://doi.org/10.1145/3450626.3459679">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose neural trace photography, a novel framework to automatically learn high-quality scanning of non-planar, complex anisotropic appearance. Our key insight is that free-form appearance scanning can be cast as a geometry learning problem on unstructured point clouds, each of which represents an image measurement and the corresponding acquisition condition. Based on this connection, we carefully design a neural network, to jointly optimize the lighting conditions to be used in acquisition, as well as the spatially independent reconstruction of reflectance from corresponding measurements. Our framework is not tied to a specific setup, and can adapt to various factors in a data-driven manner. We demonstrate the effectiveness of our framework on a number of physical objects with a wide variation in appearance. The objects are captured with a light-weight mobile device, consisting of a single camera and an RGB LED array. We also generalize the framework to other common types of light sources, including a point, a linear and an area light.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459679},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Free-form scanning of non-planar appearance with neural trace photography},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MOCCA: Modeling and optimizing cone-joints for complex
assemblies. <em>TOG</em>, <em>40</em>(4), 1–14. (<a
href="https://doi.org/10.1145/3450626.3459680">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a computational framework for modeling and optimizing complex assemblies using cone joints. Cone joints are integral joints that generalize traditional single-direction joints such as mortise and tenon joints to support a general cone of directions for assembly. This additional motion flexibility not just reduces the risk of deadlocking for complex joint arrangements, but also simplifies the assembly process, in particular for automatic assembly by robots. On the other hand, compared to planar contacts, cone joints restrict relative part movement for improved structural stability. Cone joints can be realized in the form of curved contacts between associated parts, which have demonstrated good mechanical properties such as reduced stress concentration. To find the best trade-off between assemblability and stability, we propose an optimization approach that first determines the optimal motion cone for each part contact and subsequently derives a geometric realization of each joint to match this motion cone. We demonstrate that our approach can optimize cone joints for assemblies with a variety of geometric forms, and highlight several application examples.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459680},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-14},
  shortjournal = {ACM Trans. Graph.},
  title        = {MOCCA: Modeling and optimizing cone-joints for complex assemblies},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MoCap-solver: A neural solver for optical motion capture
data. <em>TOG</em>, <em>40</em>(4), 1–11. (<a
href="https://doi.org/10.1145/3450626.3459681">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a conventional optical motion capture (MoCap) workflow, two processes are needed to turn captured raw marker sequences into correct skeletal animation sequences. Firstly, various tracking errors present in the markers must be fixed ( cleaning or refining ). Secondly, an agent skeletal mesh must be prepared for the actor/actress, and used to determine skeleton information from the markers ( re-targeting or solving ). The whole process, normally referred to as solving MoCap data, is extremely time-consuming, labor-intensive, and usually the most costly part of animation production. Hence, there is a great demand for automated tools in industry. In this work, we present MoCap-Solver, a production-ready neural solver for optical MoCap data. It can directly produce skeleton sequences and clean marker sequences from raw MoCap markers, without any tedious manual operations. To achieve this goal, our key idea is to make use of neural encoders concerning three key intrinsic components: the template skeleton, marker configuration and motion, and to learn to predict these latent vectors from imperfect marker sequences containing noise and errors. By decoding these components from latent vectors, sequences of clean markers and skeletons can be directly recovered. Moreover, we also provide a novel normalization strategy based on learning a pose-dependent marker reliability function, which greatly improves system robustness. Experimental results demonstrate that our algorithm consistently outperforms the state-of-the-art on both synthetic and real-world datasets.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459681},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-11},
  shortjournal = {ACM Trans. Graph.},
  title        = {MoCap-solver: A neural solver for optical motion capture data},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). RXMesh: A GPU mesh data structure. <em>TOG</em>,
<em>40</em>(4), 1–16. (<a
href="https://doi.org/10.1145/3450626.3459748">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new static high-performance mesh data structure for triangle surface meshes on the GPU. Our data structure is carefully designed for parallel execution while capturing mesh locality and confining data access, as much as possible, within the GPU&#39;s fast &quot;shared memory.&quot; We achieve this by subdividing the mesh into patches and representing these patches compactly using a matrix-based representation. Our patching technique is decorated with ribbons , thin mesh strips around patches that eliminate the need to communicate between different computation thread blocks, resulting in consistent high throughput. We call our data structure RXMesh : Ribbon-matriX Mesh. We hide the complexity of our data structure behind a flexible but powerful programming model that helps deliver high performance by inducing load balance even in highly irregular input meshes. We show the efficacy of our programming model on common geometry processing applications---mesh smoothing and filtering, geodesic distance, and vertex normal computation. For evaluation, we benchmark our data structure against well-optimized GPU and (single and multi-core) CPU data structures and show significant speedups.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459748},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-16},
  shortjournal = {ACM Trans. Graph.},
  title        = {RXMesh: A GPU mesh data structure},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Real-time deep dynamic characters. <em>TOG</em>,
<em>40</em>(4), 1–16. (<a
href="https://doi.org/10.1145/3450626.3459749">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a deep videorealistic 3D human character model displaying highly realistic shape, motion, and dynamic appearance learned in a new weakly supervised way from multi-view imagery. In contrast to previous work, our controllable 3D character displays dynamics, e.g., the swing of the skirt, dependent on skeletal body motion in an efficient data-driven way, without requiring complex physics simulation. Our character model also features a learned dynamic texture model that accounts for photo-realistic motion-dependent appearance details, as well as view-dependent lighting effects. During training, we do not need to resort to difficult dynamic 3D capture of the human; instead we can train our model entirely from multi-view video in a weakly supervised manner. To this end, we propose a parametric and differentiable character representation which allows us to model coarse and fine dynamic deformations, e.g., garment wrinkles, as explicit spacetime coherent mesh geometry that is augmented with high-quality dynamic textures dependent on motion and view point. As input to the model, only an arbitrary 3D skeleton motion is required, making it directly compatible with the established 3D animation pipeline. We use a novel graph convolutional network architecture to enable motion-dependent deformation learning of body and clothing, including dynamics, and a neural generative dynamic texture model creates corresponding dynamic texture maps. We show that by merely providing new skeletal motions, our model creates motion-dependent surface deformations, physically plausible dynamic clothing deformations, as well as video-realistic surface textures at a much higher level of detail than previous state of the art approaches, and even in real-time.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459749},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Real-time deep dynamic characters},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Pareto gamuts: Exploring optimal designs across varying
contexts. <em>TOG</em>, <em>40</em>(4), 1–17. (<a
href="https://doi.org/10.1145/3450626.3459750">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Manufactured parts are meticulously engineered to perform well with respect to several conflicting metrics, like weight, stress, and cost. The best achievable trade-offs reside on the Pareto front , which can be discovered via performance-driven optimization. The objectives that define this Pareto front often incorporate assumptions about the context in which a part will be used, including loading conditions, environmental influences, material properties, or regions that must be preserved to interface with a surrounding assembly. Existing multi-objective optimization tools are only equipped to study one context at a time, so engineers must run independent optimizations for each context of interest. However, engineered parts frequently appear in many contexts: wind turbines must perform well in many wind speeds, and a bracket might be optimized several times with its bolt-holes fixed in different locations on each run. In this paper, we formulate a framework for variable-context multi-objective optimization. We introduce the Pareto gamut , which captures Pareto fronts over a range of contexts. We develop a global/local optimization algorithm to discover the Pareto gamut directly, rather than discovering a single fixed-context &quot;slice&quot; at a time. To validate our method, we adapt existing multi-objective optimization benchmarks to contextual scenarios. We also demonstrate the practical utility of Pareto gamut exploration for several engineering design problems.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459750},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-17},
  shortjournal = {ACM Trans. Graph.},
  title        = {Pareto gamuts: Exploring optimal designs across varying contexts},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fast diffraction pathfinding for dynamic sound propagation.
<em>TOG</em>, <em>40</em>(4), 1–13. (<a
href="https://doi.org/10.1145/3450626.3459751">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the context of geometric acoustic simulation, one of the more perceptually important yet difficult to simulate acoustic effects is diffraction, a phenomenon that allows sound to propagate around obstructions and corners. A significant bottleneck in real-time simulation of diffraction is the enumeration of high-order diffraction propagation paths in scenes with complex geometry (e.g. highly tessellated surfaces). To this end, we present a dynamic geometric diffraction approach that consists of an extensive mesh preprocessing pipeline and complementary runtime algorithm. The preprocessing module identifies a small subset of edges that are important for diffraction using a novel silhouette edge detection heuristic. It also extends these edges with planar diffraction geometry and precomputes a graph data structure encoding the visibility between the edges. The runtime module uses bidirectional path tracing against the diffraction geometry to probabilistically explore potential paths between sources and listeners, then evaluates the intensities for these paths using the Uniform Theory of Diffraction. It uses the edge visibility graph and the A* pathfinding algorithm to robustly and efficiently find additional high-order diffraction paths. We demonstrate how this technique can simulate 10th-order diffraction up to 568 times faster than the previous state of the art, and can efficiently handle large scenes with both high geometric complexity and high numbers of sources.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459751},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Fast diffraction pathfinding for dynamic sound propagation},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Knit sketching: From cut &amp; sew patterns to machine-knit
garments. <em>TOG</em>, <em>40</em>(4), 1–15. (<a
href="https://doi.org/10.1145/3450626.3459752">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel workflow to design and program knitted garments for industrial whole-garment knitting machines. Inspired by traditional garment making based on cutting and sewing, we propose a sketch representation with additional annotations necessary to model the knitting process. Our system bypasses complex editing operations in 3D space, which allows us to achieve interactive editing of both the garment shape and its underlying time process. We provide control of the local knitting direction, the location of important course interfaces, as well as the placement of stitch irregularities that form seams in the final garment. After solving for the constrained knitting time process, the garment sketches are automatically segmented into a minimal set of simple regions that can be knitted using simple knitting procedures. Finally, our system optimizes a stitch graph hierarchically while providing control over the tradeoff between accuracy and simplicity. We showcase different garments created with our web interface.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459752},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Knit sketching: From cut &amp; sew patterns to machine-knit garments},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Medial IPC: Accelerated incremental potential contact with
medial elastics. <em>TOG</em>, <em>40</em>(4), 1–16. (<a
href="https://doi.org/10.1145/3450626.3459753">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a framework of efficient nonlinear deformable simulation with both fast continuous collision detection and robust collision resolution. We name this new framework Medial IPC as it integrates the merits from medial elastics, for an efficient and versatile reduced simulation, as well as incremental potential contact, for a robust collision and contact resolution. We leverage medial axis transform to construct a kinematic subspace. Instead of resorting to projective dynamics, we use classic hyperelastics to embrace real-world nonlinear materials. A novel reduced continuous collision detection algorithm is presented based on the medial mesh. Thanks to unique geometric properties of medial axis and medial primitives, we derive closed-form formulations for identifying between-primitive collision within the reduced medial space. In the meantime, the implicit barrier energy that generates necessary repulsion forces for collision resolution is also formulated with the medial coordinate. In other words, Medial IPC exploits a universal reduced coordinate for simulation, continuous self-/collision detection, and IPC-based collision resolution. Continuous collision detection also allows more aggressive time stepping. In addition, we carefully implement our system with a heterogeneous CPU-GPU deployment such that massively parallelizable computations are carried out on the GPU while few sequential computations are on the CPU. Such implementation also frees us from generating training poses for selecting Cubature points and pre-computing their weights. We have tested our method on complicated deformable models and collision-rich simulation scenarios. Due to the reduced nature of our system, the computation is faster than fullspace IPC or other fullspace methods using continuous collision detection by at least one order. The simulation remains high-quality as the medial subspace captures intriguing and local deformations with sufficient realism.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459753},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Medial IPC: Accelerated incremental potential contact with medial elastics},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). High-order differentiable autoencoder for nonlinear model
reduction. <em>TOG</em>, <em>40</em>(4), 1–15. (<a
href="https://doi.org/10.1145/3450626.3459754">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper provides a new avenue for exploiting deep neural networks to improve physics-based simulation. Specifically, we integrate the classic Lagrangian mechanics with a deep autoencoder to accelerate elastic simulation of deformable solids. Due to the inertia effect, the dynamic equilibrium cannot be established without evaluating the second-order derivatives of the deep autoencoder network. This is beyond the capability of off-the-shelf automatic differentiation packages and algorithms, which mainly focus on the gradient evaluation. Solving the nonlinear force equilibrium is even more challenging if the standard Newton&#39;s method is to be used. This is because we need to compute a third-order derivative of the network to obtain the variational Hessian. We attack those difficulties by exploiting complex-step finite difference, coupled with reverse automatic differentiation. This strategy allows us to enjoy the convenience and accuracy of complex-step finite difference and in the meantime, to deploy complex-value perturbations as collectively as possible to save excessive network passes. With a GPU-based implementation, we are able to wield deep autoencoders (e.g., 10+ layers) with a relatively high-dimension latent space in real-time. Along this pipeline, we also design a sampling network and a weighting network to enable weight-varying Cubature integration in order to incorporate nonlinearity in the model reduction. We believe this work will inspire and benefit future research efforts in nonlinearly reduced physical simulation problems.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459754},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-15},
  shortjournal = {ACM Trans. Graph.},
  title        = {High-order differentiable autoencoder for nonlinear model reduction},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SANM: A symbolic asymptotic numerical solver with
applications in mesh deformation. <em>TOG</em>, <em>40</em>(4), 1–16.
(<a href="https://doi.org/10.1145/3450626.3459755">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Solving nonlinear systems is an important problem. Numerical continuation methods efficiently solve certain nonlinear systems. The Asymptotic Numerical Method (ANM) is a powerful continuation method that usually converges faster than Newtonian methods. ANM explores the landscape of the function by following a parameterized solution curve approximated with a high-order power series. Although ANM has successfully solved a few graphics and engineering problems, prior to our work, applying ANM to new problems required significant effort because the standard ANM assumes quadratic functions, while manually deriving the power series expansion for nonquadratic systems is a tedious and challenging task. This paper presents a novel solver, SANM, that applies ANM to solve symbolically represented nonlinear systems. SANM solves such systems in a fully automated manner. SANM also extends ANM to support many nonquadratic operators, including intricate ones such as singular value decomposition. Furthermore, SANM generalizes ANM to support the implicit homotopy form. Moreover, SANM achieves high computing performance via optimized system design and implementation. We deploy SANM to solve forward and inverse elastic force equilibrium problems and controlled mesh deformation problems with a few constitutive models. Our results show that SANM converges faster than Newtonian solvers, requires little programming effort for new problems, and delivers comparable or better performance than a hand-coded, specialized ANM solver. While we demonstrate on mesh deformation problems, SANM is generic and potentially applicable to many tasks.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459755},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-16},
  shortjournal = {ACM Trans. Graph.},
  title        = {SANM: A symbolic asymptotic numerical solver with applications in mesh deformation},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Editable free-viewpoint video using a layered neural
representation. <em>TOG</em>, <em>40</em>(4), 1–18. (<a
href="https://doi.org/10.1145/3450626.3459756">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generating free-viewpoint videos is critical for immersive VR/AR experience, but recent neural advances still lack the editing ability to manipulate the visual perception for large dynamic scenes. To fill this gap, in this paper, we propose the first approach for editable free-viewpoint video generation for large-scale view-dependent dynamic scenes using only 16 cameras. The core of our approach is a new layered neural representation, where each dynamic entity, including the environment itself, is formulated into a spatio-temporal coherent neural layered radiance representation called ST-NeRF. Such a layered representation supports manipulations of the dynamic scene while still supporting a wide free viewing experience. In our ST-NeRF, we represent the dynamic entity/layer as a continuous function, which achieves the disentanglement of location, deformation as well as the appearance of the dynamic entity in a continuous and self-supervised manner. We propose a scene parsing 4D label map tracking to disentangle the spatial information explicitly and a continuous deform module to disentangle the temporal motion implicitly. An object-aware volume rendering scheme is further introduced for the re-assembling of all the neural layers. We adopt a novel layered loss and motion-aware ray sampling strategy to enable efficient training for a large dynamic scene with multiple performers, Our framework further enables a variety of editing functions, i.e., manipulating the scale and location, duplicating or retiming individual neural layers to create numerous visual effects while preserving high realism. Extensive experiments demonstrate the effectiveness of our approach to achieve high-quality, photo-realistic, and editable free-viewpoint video generation for dynamic scenes.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459756},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-18},
  shortjournal = {ACM Trans. Graph.},
  title        = {Editable free-viewpoint video using a layered neural representation},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Guaranteed globally injective 3D deformation processing.
<em>TOG</em>, <em>40</em>(4), 1–13. (<a
href="https://doi.org/10.1145/3450626.3459757">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We extend recent advances in the numerical time-integration of contacting elastodynamics [Li et al. 2020] to build a new framework, called Injective Deformation Processing (IDP), for the robust solution of a wide range of mesh deformation problems requiring injectivity. IDP solves challenging 3D (and 2D) geometry processing and animation tasks on meshes, via artificial time integration, with guarantees of both non-inversion and non-overlap. To our knowledge IDP is the first framework for 3D deformation processing that can efficiently guarantee globally injective deformation without geometric locking. We demonstrate its application on a diverse set of problems and show its significant improvement over state-of-the-art for globally injective 3D deformation.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459757},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Guaranteed globally injective 3D deformation processing},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A fitted radiance and attenuation model for realistic
atmospheres. <em>TOG</em>, <em>40</em>(4), 1–14. (<a
href="https://doi.org/10.1145/3450626.3459758">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a fitted model of sky dome radiance and attenuation for realistic terrestrial atmospheres. Using scatterer distribution data from atmospheric measurement data, our model considerably improves on the visual realism of existing analytical clear sky models, as well as of interactive methods that are based on approximating atmospheric light transport. We also provide features not found in fitted models so far: radiance patterns for post-sunset conditions, in-scattered radiance and attenuation values for finite viewing distances, an observer altitude resolved model that includes downward-looking viewing directions, as well as polarisation information. We introduce a fully spherical model for in-scattered radiance that replaces the family of hemispherical functions originally introduced by Perez et al., and which was extended for several subsequent analytical models: our model relies on reference image compression via tensor decomposition instead.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459758},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-14},
  shortjournal = {ACM Trans. Graph.},
  title        = {A fitted radiance and attenuation model for realistic atmospheres},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Constrained willmore surfaces. <em>TOG</em>, <em>40</em>(4),
1–17. (<a href="https://doi.org/10.1145/3450626.3459759">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Smooth curves and surfaces can be characterized as minimizers of squared curvature bending energies subject to constraints. In the univariate case with an isometry (length) constraint this leads to classic non-linear splines. For surfaces, isometry is too rigid a constraint and instead one asks for minimizers of the Willmore (squared mean curvature) energy subject to a conformality constraint. We present an efficient algorithm for (conformally) constrained Willmore surfaces using triangle meshes of arbitrary topology with or without boundary. Our conformal class constraint is based on the discrete notion of conformal equivalence of triangle meshes. The resulting non-linear constrained optimization problem can be solved efficiently using the competitive gradient descent method together with appropriate Sobolev metrics. The surfaces can be represented either through point positions or differential coordinates. The latter enable the realization of abstract metric surfaces without an initial immersion. A versatile toolkit for extrinsic conformal geometry processing, suitable for the construction and manipulation of smooth surfaces, results through the inclusion of additional point, area, and volume constraints.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459759},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-17},
  shortjournal = {ACM Trans. Graph.},
  title        = {Constrained willmore surfaces},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DeepFaceEditing: Deep face generation and editing with
disentangled geometry and appearance control. <em>TOG</em>,
<em>40</em>(4), 1–15. (<a
href="https://doi.org/10.1145/3450626.3459760">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent facial image synthesis methods have been mainly based on conditional generative models. Sketch-based conditions can effectively describe the geometry of faces, including the contours of facial components, hair structures, as well as salient edges (e.g., wrinkles) on face surfaces but lack effective control of appearance, which is influenced by color, material, lighting condition, etc. To have more control of generated results, one possible approach is to apply existing disentangling works to disentangle face images into geometry and appearance representations. However, existing disentangling methods are not optimized for human face editing, and cannot achieve fine control of facial details such as wrinkles. To address this issue, we propose DeepFaceEditing, a structured disentanglement framework specifically designed for face images to support face generation and editing with disentangled control of geometry and appearance. We adopt a local-to-global approach to incorporate the face domain knowledge: local component images are decomposed into geometry and appearance representations, which are fused consistently using a global fusion module to improve generation quality. We exploit sketches to assist in extracting a better geometry representation, which also supports intuitive geometry editing via sketching. The resulting method can either extract the geometry and appearance representations from face images, or directly extract the geometry representation from face sketches. Such representations allow users to easily edit and synthesize face images, with decoupled control of their geometry and appearance. Both qualitative and quantitative evaluations show the superior detail and appearance control abilities of our method compared to state-of-the-art methods.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459760},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-15},
  shortjournal = {ACM Trans. Graph.},
  title        = {DeepFaceEditing: Deep face generation and editing with disentangled geometry and appearance control},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Control strategies for physically simulated characters
performing two-player competitive sports. <em>TOG</em>, <em>40</em>(4),
1–11. (<a href="https://doi.org/10.1145/3450626.3459761">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In two-player competitive sports, such as boxing and fencing , athletes often demonstrate efficient and tactical movements during a competition. In this paper, we develop a learning framework that generates control policies for physically simulated athletes who have many degrees-of-freedom. Our framework uses a two step-approach, learning basic skills and learning bout-level strategies, with deep reinforcement learning, which is inspired by the way that people how to learn competitive sports. We develop a policy model based on an encoder-decoder structure that incorporates an autoregressive latent variable, and a mixture-of-experts decoder. To show the effectiveness of our framework, we implemented two competitive sports, boxing and fencing , and demonstrate control policies learned by our framework that can generate both tactical and natural-looking behaviors. We also evaluate the control policies with comparisons to other learning configurations and with ablation studies.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459761},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-11},
  shortjournal = {ACM Trans. Graph.},
  title        = {Control strategies for physically simulated characters performing two-player competitive sports},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DeepFormableTag: End-to-end generation and recognition of
deformable fiducial markers. <em>TOG</em>, <em>40</em>(4), 1–14. (<a
href="https://doi.org/10.1145/3450626.3459762">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fiducial markers have been broadly used to identify objects or embed messages that can be detected by a camera. Primarily, existing detection methods assume that markers are printed on ideally planar surfaces. The size of a message or identification code is limited by the spatial resolution of binary patterns in a marker. Markers often fail to be recognized due to various imaging artifacts of optical/perspective distortion and motion blur. To overcome these limitations, we propose a novel deformable fiducial marker system that consists of three main parts: First, a fiducial marker generator creates a set of free-form color patterns to encode significantly large-scale information in unique visual codes. Second, a differentiable image simulator creates a training dataset of photorealistic scene images with the deformed markers, being rendered during optimization in a differentiable manner. The rendered images include realistic shading with specular reflection, optical distortion, defocus and motion blur, color alteration, imaging noise, and shape deformation of markers. Lastly, a trained marker detector seeks the regions of interest and recognizes multiple marker patterns simultaneously via inverse deformation transformation. The deformable marker creator and detector networks are jointly optimized via the differentiable photorealistic renderer in an end-to-end manner, allowing us to robustly recognize a wide range of deformable markers with high accuracy. Our deformable marker system is capable of decoding 36-bit messages successfully at ~29 fps with severe shape deformation. Results validate that our system significantly outperforms the traditional and data-driven marker methods. Our learning-based marker system opens up new interesting applications of fiducial markers, including cost-effective motion capture of the human body, active 3D scanning using our fiducial markers&#39; array as structured light patterns, and robust augmented reality rendering of virtual objects on dynamic surfaces.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459762},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-14},
  shortjournal = {ACM Trans. Graph.},
  title        = {DeepFormableTag: End-to-end generation and recognition of deformable fiducial markers},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Discrete conformal equivalence of polyhedral surfaces.
<em>TOG</em>, <em>40</em>(4), 1–20. (<a
href="https://doi.org/10.1145/3450626.3459763">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper describes a numerical method for surface parameterization, yielding maps that are locally injective and discretely conformal in an exact sense. Unlike previous methods for discrete conformal parameterization, the method is guaranteed to work for any manifold triangle mesh, with no restrictions on triangulatiothat each task can be formulated as a convex problem where the triangulation is allowed to change---we complete the picture by introducing the machinery needed to actually construct a discrete conformal map. In particular, we introduce a new scheme for tracking correspondence between triangulations based on normal coordinates , and a new interpolation procedure based on layout in the light cone. Stress tests involving difficult cone configurations and near-degenerate triangulations indicate that the method is extremely robust in practice, and provides high-quality interpolation even on meshes with poor elements.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459763},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-20},
  shortjournal = {ACM Trans. Graph.},
  title        = {Discrete conformal equivalence of polyhedral surfaces},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Unified particle system for multiple-fluid flow and porous
material. <em>TOG</em>, <em>40</em>(4), 1–14. (<a
href="https://doi.org/10.1145/3450626.3459764">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Porous materials are common in daily life. They include granular material (e.g. sand) that behaves like liquid flow when mixed with fluid and foam material (e.g. sponge) that deforms like solid when interacting with liquid. The underlying physics is further complicated when multiple fluids interact with porous materials involving coupling between rigid and fluid bodies, which may follow different physics models such as the Darcy&#39;s law and the multiple-fluid Navier-Stokes equations. We propose a unified particle framework for the simulation of multiple-fluid flows and porous materials. A novel virtual phase concept is introduced to avoid explicit particle state tracking and runtime particle deletion/insertion. Our unified model is flexible and stable to cope with multiple fluid interacting with porous materials, and it can ensure consistent mass and momentum transport over the whole simulation space.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459764},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Unified particle system for multiple-fluid flow and porous material},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). PhotoApp: Photorealistic appearance editing of head
portraits. <em>TOG</em>, <em>40</em>(4), 1–16. (<a
href="https://doi.org/10.1145/3450626.3459765">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Photorealistic editing of head portraits is a challenging task as humans are very sensitive to inconsistencies in faces. We present an approach for high-quality intuitive editing of the camera viewpoint and scene illumination (parameterised with an environment map) in a portrait image. This requires our method to capture and control the full reflectance field of the person in the image. Most editing approaches rely on supervised learning using training data captured with setups such as light and camera stages. Such datasets are expensive to acquire, not readily available and do not capture all the rich variations of in-the-wild portrait images. In addition, most supervised approaches only focus on relighting, and do not allow camera viewpoint editing. Thus, they only capture and control a subset of the reflectance field. Recently, portrait editing has been demonstrated by operating in the generative model space of StyleGAN. While such approaches do not require direct supervision, there is a significant loss of quality when compared to the supervised approaches. In this paper, we present a method which learns from limited supervised training data. The training images only include people in a fixed neutral expression with eyes closed, without much hair or background variations. Each person is captured under 150 one-light-at-a-time conditions and under 8 camera poses. Instead of training directly in the image space, we design a supervised problem which learns transformations in the latent space of StyleGAN. This combines the best of supervised learning and generative adversarial modeling. We show that the StyleGAN prior allows for generalisation to different expressions, hairstyles and backgrounds. This produces high-quality photorealistic results for in-the-wild images and significantly outperforms existing methods. Our approach can edit the illumination and pose simultaneously, and runs at interactive rates.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459765},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-16},
  shortjournal = {ACM Trans. Graph.},
  title        = {PhotoApp: Photorealistic appearance editing of head portraits},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SP-GAN: Sphere-guided 3D shape generation and manipulation.
<em>TOG</em>, <em>40</em>(4), 1–12. (<a
href="https://doi.org/10.1145/3450626.3459766">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present SP-GAN, a new unsupervised sphere-guided generative model for direct synthesis of 3D shapes in the form of point clouds. Compared with existing models, SP-GAN is able to synthesize diverse and high-quality shapes with fine details and promote controllability for part-aware shape generation and manipulation, yet trainable without any parts annotations. In SP-GAN, we incorporate a global prior (uniform points on a sphere) to spatially guide the generative process and attach a local prior (a random latent code) to each sphere point to provide local details. The key insight in our design is to disentangle the complex 3D shape generation task into a global shape modeling and a local structure adjustment, to ease the learning process and enhance the shape generation quality. Also, our model forms an implicit dense correspondence between the sphere points and points in every generated shape, enabling various forms of structure-aware shape manipulations such as part editing, part-wise shape interpolation, and multi-shape part composition, etc., beyond the existing generative models. Experimental results, which include both visual and quantitative evaluations, demonstrate that our model is able to synthesize diverse point clouds with fine details and less noise, as compared with the state-of-the-art models.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459766},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-12},
  shortjournal = {ACM Trans. Graph.},
  title        = {SP-GAN: Sphere-guided 3D shape generation and manipulation},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Codimensional incremental potential contact. <em>TOG</em>,
<em>40</em>(4), 1–24. (<a
href="https://doi.org/10.1145/3450626.3459767">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We extend the incremental potential contact (IPC) model [Li et al. 2020a] for contacting elastodynamics to resolve systems composed of codimensional degrees-of-freedoms in arbitrary combination. This enables a unified, interpenetration-free, robust, and stable simulation framework that couples codimension-0,1,2, and 3 geometries seamlessly with frictional contact. Extending the IPC model to thin structures poses new challenges in computing strain, modeling thickness and determining collisions. To address these challenges we propose three corresponding contributions. First, we introduce a C 2 constitutive barrier model that directly enforces strain limiting as an energy potential while preserving rest state. This provides energetically-consistent strain limiting models (both isotropic and anisotropic) for cloth that enable strict satisfaction of strain-limit inequalities with direct coupling to both elastodynamics and contact via minimization of the incremental potential. Second, to capture the geometric thickness of codimensional domains we extend the IPC model to directly enforce distance offsets. Our treatment imposes a strict guarantee that mid-surfaces (respectively mid-lines) of shells (respectively rods) will not move closer than applied thickness values, even as these thicknesses become characteristically small. This enables us to account for thickness in the contact behavior of codimensional structures and so robustly capture challenging contacting geometries; a number of which, to our knowledge, have not been simulated before. Third, codimensional models, especially with modeled thickness, mandate strict accuracy requirements that pose a severe challenge to all existing continuous collision detection (CCD) methods. To address these limitations we develop a new, efficient, simple-to-implement additive CCD (ACCD) method that applies conservative advancement [Mirtich 1996; Zhang et al. 2006] to iteratively refine a lower bound for deforming primitives, converging to time of impact. In combination these contributions enable codimensional IPC (C-IPC). We perform extensive benchmark experiments to validate the efficacy of our method in capturing intricate behaviors of thin-structure contact and resulting bulk effects. In our experiments C-IPC obtains feasible, convergent, and so artifact-free solutions for all time steps, across all tested examples - producing robust simulations. We test C-IPC across extreme deformations, large time steps, and exceedingly close contact over all possible pairings of codimensional domains. Finally, with our strain-limit model, we confirm C-IPC guarantees non-intersection and strain-limit satisfaction for all reasonable (and well below - verified down to 0.1%) strain limits throughout all time steps.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459767},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-24},
  shortjournal = {ACM Trans. Graph.},
  title        = {Codimensional incremental potential contact},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Surface multigrid via intrinsic prolongation. <em>TOG</em>,
<em>40</em>(4), 1–13. (<a
href="https://doi.org/10.1145/3450626.3459768">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a novel geometric multigrid solver for unstructured curved surfaces. Multigrid methods are highly efficient iterative methods for solving systems of linear equations. Despite the success in solving problems defined on structured domains, generalizing multigrid to unstructured curved domains remains a challenging problem. The critical missing ingredient is a prolongation operator to transfer functions across different multigrid levels. We propose a novel method for computing the prolongation for triangulated surfaces based on intrinsic geometry, enabling an efficient geometric multigrid solver for curved surfaces. Our surface multigrid solver achieves better convergence than existing multigrid methods. Compared to direct solvers, our solver is orders of magnitude faster. We evaluate our method on many geometry processing applications and a wide variety of complex shapes with and without boundaries. By simply replacing the direct solver, we upgrade existing algorithms to interactive frame rates, and shift the computational bottleneck away from solving linear systems.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459768},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Surface multigrid via intrinsic prolongation},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Interactive modelling of volumetric musculoskeletal anatomy.
<em>TOG</em>, <em>40</em>(4), 1–13. (<a
href="https://doi.org/10.1145/3450626.3459769">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a new approach for modelling musculoskeletal anatomy. Unlike previous methods, we do not model individual muscle shapes as geometric primitives (polygonal meshes, NURBS etc.). Instead, we adopt a volumetric segmentation approach where every point in our volume is assigned to a muscle, fat, or bone tissue. We provide an interactive modelling tool where the user controls the segmentation via muscle curves and we visualize the muscle shapes using volumetric rendering. Muscle curves enable intuitive yet powerful control over the muscle shapes. This representation allows us to automatically handle intersections between different tissues (muscle-muscle, muscle-bone, and muscle-skin) during the modelling and automates computation of muscle fiber fields. We further introduce a novel algorithm for converting the volumetric muscle representation into tetrahedral or surface geometry for use in downstream tasks. Additionally, we introduce an interactive skeleton authoring tool that allows the users to create skeletal anatomy starting from only a skin mesh using a library of bone parts.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459769},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Interactive modelling of volumetric musculoskeletal anatomy},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). PH-CPF: Planar hexagonal meshing using coordinate power
fields. <em>TOG</em>, <em>40</em>(4), 1–19. (<a
href="https://doi.org/10.1145/3450626.3459770">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a new approach for computing planar hexagonal meshes that approximate a given surface, represented as a triangle mesh. Our method is based on two novel technical contributions. First, we introduce Coordinate Power Fields , which are a pair of tangent vector fields on the surface that fulfill a certain continuity constraint. We prove that the fulfillment of this constraint guarantees the existence of a seamless parameterization with quantized rotational jumps, which we then use to regularly remesh the surface. We additionally propose an optimization framework for finding Coordinate Power Fields, which also fulfill additional constraints, such as alignment, sizing and bijectivity. Second, we build upon this framework to address a challenging meshing problem: planar hexagonal meshing. To this end, we suggest a combination of conjugacy, scaling and alignment constraints, which together lead to planarizable hexagons. We demonstrate our approach on a variety of surfaces, automatically generating planar hexagonal meshes on complicated meshes, which were not achievable with existing methods.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459770},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-19},
  shortjournal = {ACM Trans. Graph.},
  title        = {PH-CPF: Planar hexagonal meshing using coordinate power fields},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). AgileGAN: Stylizing portraits by inversion-consistent
transfer learning. <em>TOG</em>, <em>40</em>(4), 1–13. (<a
href="https://doi.org/10.1145/3450626.3459771">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Portraiture as an art form has evolved from realistic depiction into a plethora of creative styles. While substantial progress has been made in automated stylization, generating high quality stylistic portraits is still a challenge, and even the recent popular Toonify suffers from several artifacts when used on real input images. Such StyleGAN-based methods have focused on finding the best latent inversion mapping for reconstructing input images; however, our key insight is that this does not lead to good generalization to different portrait styles. Hence we propose AgileGAN, a framework that can generate high quality stylistic portraits via inversion-consistent transfer learning. We introduce a novel hierarchical variational autoencoder to ensure the inverse mapped distribution conforms to the original latent Gaussian distribution, while augmenting the original space to a multi-resolution latent space so as to better encode different levels of detail. To better capture attribute-dependent stylization of facial features, we also present an attribute-aware generator and adopt an early stopping strategy to avoid overfitting small training datasets. Our approach provides greater agility in creating high quality and high resolution (1024×1024) portrait stylization models, requiring only a limited number of style exemplars (~100) and short training time (~1 hour). We collected several style datasets for evaluation including 3D cartoons, comics, oil paintings and celebrities. We show that we can achieve superior portrait stylization quality to previous state-of-the-art methods, with comparisons done qualitatively, quantitatively and through a perceptual user study. We also demonstrate two applications of our method, image editing and motion retargeting.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459771},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-13},
  shortjournal = {ACM Trans. Graph.},
  title        = {AgileGAN: Stylizing portraits by inversion-consistent transfer learning},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The shape matching element method: Direct animation of
curved surface models. <em>TOG</em>, <em>40</em>(4), 1–14. (<a
href="https://doi.org/10.1145/3450626.3459772">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a new method for direct physics-based animation of volumetric curved models, represented using NURBS surfaces. Our technical contribution is the Shape Matching Element Method (SEM). SEM is a completely meshless algorithm, the first to simultaneously be robust to gaps and overlaps in geometry, be compatible with standard constitutive models and time integration schemes, support contact and frictional interactions and to preserve feature correspondence during simulation which enables editable simulated output. We demonstrate the efficacy of our algorithm by producing compelling physics-based animations from a variety of curved input models.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459772},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-14},
  shortjournal = {ACM Trans. Graph.},
  title        = {The shape matching element method: Direct animation of curved surface models},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fast median filters using separable sorting networks.
<em>TOG</em>, <em>40</em>(4), 1–11. (<a
href="https://doi.org/10.1145/3450626.3459773">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Median filters are a widely-used tool in graphics, imaging, machine learning, visual effects, and even audio processing. Currently, very-small-support median filters are performed using sorting networks, and large-support median filters are handled by O (1) histogram-based methods. However, the constant factor on these O (1) algorithms is large, and they scale poorly to data types above 8-bit integers. On the other hand, good sorting networks have not been described above the 7 X 7 case, leaving us with no fast way to compute integer median filters of modest size, and no fast way to compute floating point median filters for any size above 7 X 7. This paper describes new sorting networks that efficiently compute median filters of arbitrary size. The key idea is that these networks can be factored to exploit the separability of the sorting problem - they share common work across scanlines, and within small tiles of output. We also describe new ways to run sorting networks efficiently, using a sorting-specific instruction set, compiler, and interpreter. The speed-up over prior work is more than an order of magnitude for a wide range of data types and filter sizes. For 8-bit integers, we describe the fastest median filters for all sizes up to 25 X 25 on CPU, and up to 33 X 33 on GPU. For higher-precision types, we describe the fastest median filters at all sizes tested on both CPU and GPU.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459773},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-11},
  shortjournal = {ACM Trans. Graph.},
  title        = {Fast median filters using separable sorting networks},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning a family of motor skills from a single motion clip.
<em>TOG</em>, <em>40</em>(4), 1–13. (<a
href="https://doi.org/10.1145/3450626.3459774">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a new algorithm that learns a parameterized family of motor skills from a single motion clip. The motor skills are represented by a deep policy network, which produces a stream of motions in physics simulation in response to user input and environment interaction by navigating continuous action space. Three novel technical components play an important role in the success of our algorithm. First, it explicitly constructs motion parameterization that maps action parameters to their corresponding motions. Simultaneous learning of motion parameterization and motor skills significantly improves the performance and visual quality of learned motor skills. Second, continuous-time reinforcement learning is adopted to explore temporal variations as well as spatial variations in motion parameterization. Lastly, we present a new automatic curriculum generation method that explores continuous action space more efficiently. We demonstrate the flexibility and versatility of our algorithm with highly dynamic motor skills that can be parameterized by task goals, body proportions, physical measurements, and environmental conditions.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459774},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Learning a family of motor skills from a single motion clip},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Systematically differentiating parametric discontinuities.
<em>TOG</em>, <em>40</em>(4), 1–18. (<a
href="https://doi.org/10.1145/3450626.3459775">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emerging research in computer graphics, inverse problems, and machine learning requires us to differentiate and optimize parametric discontinuities. These discontinuities appear in object boundaries, occlusion, contact, and sudden change over time. In many domains, such as rendering and physics simulation, we differentiate the parameters of models that are expressed as integrals over discontinuous functions. Ignoring the discontinuities during differentiation often has a significant impact on the optimization process. Previous approaches either apply specialized hand-derived solutions, smooth out the discontinuities, or rely on incorrect automatic differentiation. We propose a systematic approach to differentiating integrals with discontinuous integrands, by developing a new differentiable programming language. We introduce integration as a language primitive and account for the Dirac delta contribution from differentiating parametric discontinuities in the integrand. We formally define the language semantics and prove the correctness and closure under the differentiation, allowing the generation of gradients and higher-order derivatives. We also build a system, Teg, implementing these semantics. Our approach is widely applicable to a variety of tasks, including image stylization, fitting shader parameters, trajectory optimization, and optimizing physical designs.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459775},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-18},
  shortjournal = {ACM Trans. Graph.},
  title        = {Systematically differentiating parametric discontinuities},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dynamic closest color warping to sort and compare palettes.
<em>TOG</em>, <em>40</em>(4), 1–15. (<a
href="https://doi.org/10.1145/3450626.3459776">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A color palette is one of the simplest and most intuitive descriptors that can be extracted from images or videos. This paper proposes a method to assess the similarity between color palettes by sorting colors. While previous palette similarity measures compare only colors without considering the overall palette combination, we sort palettes to minimize the geometric distance between colors and align them to share a common color tendency. We propose dynamic closest color warping (DCCW) to calculate the minimum distance sum between colors and the graph connecting the colors in the other palette. We evaluate the proposed palette sorting and DCCW with several datasets and demonstrate that DCCW outperforms previous methods in terms of accuracy and computing time. We validate the effectiveness of the proposed sorting technique by conducting a perceptual study, which indicates a clear preference for the results of our approach. We also demonstrate useful applications enabled by DCCW, including palette interpolation, palette navigation, and image recoloring.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459776},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Dynamic closest color warping to sort and compare palettes},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). StrokeStrip: Joint parameterization and fitting of stroke
clusters. <em>TOG</em>, <em>40</em>(4), 1–18. (<a
href="https://doi.org/10.1145/3450626.3459777">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When creating freeform drawings, artists routinely employ clusters of overdrawn strokes to convey intended, aggregate curves. The ability to algorithmically fit these intended curves to their corresponding clusters is central to many applications that use artist drawings as inputs. However, while human observers effortlessly envision the intended curves given stroke clusters as input, existing fitting algorithms lack robustness and frequently fail when presented with input stroke clusters with non-trivial geometry or topology. We present StrokeStrip , a new and robust method for fitting intended curves to vector-format stroke clusters. Our method generates fitting outputs consistent with viewer expectations across a vast range of input stroke cluster configurations. We observe that viewers perceive stroke clusters as continuous, varying-width strips whose paths are described by the intended curves. An arc length parameterization of these strips defines a natural mapping from a strip to its path. We recast the curve fitting problem as one of parameterizing the cluster strokes using a joint 1D parameterization that is the restriction of the natural arc length parameterization of this strip to the strokes in the cluster. We simultaneously compute the joint cluster parameterization and implicitly reconstruct the a priori unknown strip geometry by solving a variational problem using a discrete-continuous optimization framework. We use this parameterization to compute parametric aggregate curves whose shape reflects the geometric properties of the cluster strokes at the corresponding isovalues. We demonstrate StrokeStrip outputs to be significantly better aligned with observer preferences compared to those of prior art; in a perceptual study, viewers preferred our fitting outputs by a factor of 12:1 compared to alternatives. We further validate our algorithmic choices via a range of ablation studies; extend our framework to raster data; and illustrate applications that benefit from the parameterizations produced.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459777},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-18},
  shortjournal = {ACM Trans. Graph.},
  title        = {StrokeStrip: Joint parameterization and fitting of stroke clusters},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fast linking numbers for topology verification of loopy
structures. <em>TOG</em>, <em>40</em>(4), 1–19. (<a
href="https://doi.org/10.1145/3450626.3459778">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is increasingly common to model, simulate, and process complex materials based on loopy structures, such as in yarn-level cloth garments, which possess topological constraints between inter-looping curves. While the input model may satisfy specific topological linkages between pairs of closed loops, subsequent processing may violate those topological conditions. In this paper, we explore a family of methods for efficiently computing and verifying linking numbers between closed curves, and apply these to applications in geometry processing, animation, and simulation, so as to verify that topological invariants are preserved during and after processing of the input models. Our method has three stages: (1) we identify potentially interacting loop-loop pairs, then (2) carefully discretize each loop&#39;s spline curves into line segments so as to enable (3) efficient linking number evaluation using accelerated kernels based on either counting projected segment-segment crossings, or by evaluating the Gauss linking integral using direct or fast summation methods (Barnes-Hut or fast multipole methods). We evaluate CPU and GPU implementations of these methods on a suite of test problems, including yarn-level cloth and chainmail, that involve significant processing: physics-based relaxation and animation, user-modeled deformations, curve compression and reparameterization. We show that topology errors can be efficiently identified to enable more robust processing of loopy structures.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459778},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-19},
  shortjournal = {ACM Trans. Graph.},
  title        = {Fast linking numbers for topology verification of loopy structures},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Direct delta mush skinning compression with continuous
examples. <em>TOG</em>, <em>40</em>(4), 1–13. (<a
href="https://doi.org/10.1145/3450626.3459779">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Direct Delta Mush (DDM) is a high-quality, direct skinning method with a low setup cost. However, its storage and run-time computing cost are relatively high for two reasons: its skinning weights are 4 X 4 matrices instead of scalars like other direct skinning methods, and its computation requires one 3 X 3 Singular Value Decomposition per vertex. In this paper, we introduce a compression method that takes a DDM model and splits it into two layers: the first layer is a smaller DDM model that computes a set of virtual bone transformations and the second layer is a Linear Blend Skinning model that computes per-vertex transformations from the output of the first layer. The two-layer model can approximate the deformation of the original DDM model with significantly lower costs. Our main contribution is a novel problem formulation for the DDM compression based on a continuous example-based technique, in which we minimize the compression error on an uncountable set of example poses. This formulation provides an elegant metric for the compression error and simplifies the problem to the common linear matrix factorization. Our formulation also takes into account the skeleton hierarchy of the model, the bind pose, and the range of motions. In addition, we propose a new update rule to optimize DDM weights of the first layer and a modification to resolve the floating-point cancellation issue of DDM.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459779},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Direct delta mush skinning compression with continuous examples},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Swept volumes via spacetime numerical continuation.
<em>TOG</em>, <em>40</em>(4), 1–11. (<a
href="https://doi.org/10.1145/3450626.3459780">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given a solid 3D shape and a trajectory of it over time, we compute its swept volume - the union of all points contained within the shape at some moment in time. We consider the representation of the input and output as implicit functions, and lift the problem to 4D spacetime, where we show the problem gains a continuous structure which avoids expensive global searches. We exploit this structure via a continuation method which marches and reconstructs the zero level set of the swept volume, using the temporal dimension to avoid erroneous solutions. We show that, compared to other methods, our approach is not restricted to a limited class of shapes or trajectories, is extremely robust, and its asymptotic complexity is an order lower than standards used in the industry, enabling its use in applications such as modeling, constructive solid geometry, and path planning.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459780},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-11},
  shortjournal = {ACM Trans. Graph.},
  title        = {Swept volumes via spacetime numerical continuation},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Computing minimal surfaces with differential forms.
<em>TOG</em>, <em>40</em>(4), 1–14. (<a
href="https://doi.org/10.1145/3450626.3459781">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We describe a new algorithm that solves a classical geometric problem: Find a surface of minimal area bordered by an arbitrarily prescribed boundary curve. Existing numerical methods face challenges due to the non-convexity of the problem. Using a representation of curves and surfaces via differential forms on the ambient space, we reformulate this problem as a convex optimization. This change of variables overcomes many difficulties in previous numerical attempts and allows us to find the global minimum across all possible surface topologies. The new algorithm is based on differential forms on the ambient space and does not require handling meshes. We adopt the Alternating Direction Method of Multiplier (ADMM) to find global minimal surfaces. The resulting algorithm is simple and efficient: it boils down to an alternation between a Fast Fourier Transform (FFT) and a pointwise shrinkage operation. We also show other applications of our solver in geometry processing such as surface reconstruction.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459781},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Computing minimal surfaces with differential forms},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Path-space differentiable rendering of participating media.
<em>TOG</em>, <em>40</em>(4), 1–15. (<a
href="https://doi.org/10.1145/3450626.3459782">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Physics-based differentiable rendering---which focuses on estimating derivatives of radiometric detector responses with respect to arbitrary scene parameters---has a diverse array of applications from solving analysis-by-synthesis problems to training machine-learning pipelines incorporating forward-rendering processes. Unfortunately, existing general-purpose differentiable rendering techniques lack either the generality to handle volumetric light transport or the flexibility to devise Monte Carlo estimators capable of handling complex geometries and light transport effects. In this paper, we bridge this gap by showing how generalized path integrals can be differentiated with respect to arbitrary scene parameters. Specifically, we establish the mathematical formulation of generalized differential path integrals that capture both interfacial and volumetric light transport. Our formulation allows the development of advanced differentiable rendering algorithms capable of efficiently handling challenging geometric discontinuities and light transport phenomena such as volumetric caustics. We validate our method by comparing our derivative estimates to those generated using the finite differences. Further, to demonstrate the effectiveness of our technique, we compare both differentiable rendering and inverse rendering performance with state-of-the-art methods.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459782},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Path-space differentiable rendering of participating media},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Antithetic sampling for monte carlo differentiable
rendering. <em>TOG</em>, <em>40</em>(4), 1–12. (<a
href="https://doi.org/10.1145/3450626.3459783">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stochastic sampling of light transport paths is key to Monte Carlo forward rendering, and previous studies have led to mature techniques capable of drawing high-contribution light paths in complex scenes. These sampling techniques have also been applied to differentiable rendering. In this paper, we demonstrate that path sampling techniques developed for forward rendering can become inefficient for differentiable rendering of glossy materials---especially when estimating derivatives with respect to global scene geometries. To address this problem, we introduce antithetic sampling of BSDFs and light-transport paths, allowing significantly faster convergence and can be easily integrated into existing differentiable rendering pipelines. We validate our method by comparing our derivative estimates to those generated with existing unbiased techniques. Further, we demonstrate the effectiveness of our technique by providing equal-quality and equal-time comparisons with existing sampling methods.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459783},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-12},
  shortjournal = {ACM Trans. Graph.},
  title        = {Antithetic sampling for monte carlo differentiable rendering},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A perceptual model for eccentricity-dependent
spatio-temporal flicker fusion and its applications to foveated
graphics. <em>TOG</em>, <em>40</em>(4), 1–11. (<a
href="https://doi.org/10.1145/3450626.3459784">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual and augmented reality (VR/AR) displays strive to provide a resolution, framerate and field of view that matches the perceptual capabilities of the human visual system, all while constrained by limited compute budgets and transmission bandwidths of wearable computing systems. Foveated graphics techniques have emerged that could achieve these goals by exploiting the falloff of spatial acuity in the periphery of the visual field. However, considerably less attention has been given to temporal aspects of human vision, which also vary across the retina. This is in part due to limitations of current eccentricity-dependent models of the visual system. We introduce a new model, experimentally measuring and computationally fitting eccentricity-dependent critical flicker fusion thresholds jointly for both space and time. In this way, our model is unique in enabling the prediction of temporal information that is imperceptible for a certain spatial frequency, eccentricity, and range of luminance levels. We validate our model with an image quality user study, and use it to predict potential bandwidth savings 7X higher than those afforded by current spatial-only foveated models. As such, this work forms the enabling foundation for new temporally foveated graphics techniques.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459784},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-11},
  shortjournal = {ACM Trans. Graph.},
  title        = {A perceptual model for eccentricity-dependent spatio-temporal flicker fusion and its applications to foveated graphics},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Acorn: Adaptive coordinate networks for neural scene
representation. <em>TOG</em>, <em>40</em>(4), 1–13. (<a
href="https://doi.org/10.1145/3450626.3459785">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural representations have emerged as a new paradigm for applications in rendering, imaging, geometric modeling, and simulation. Compared to traditional representations such as meshes, point clouds, or volumes they can be flexibly incorporated into differentiable learning-based pipelines. While recent improvements to neural representations now make it possible to represent signals with fine details at moderate resolutions (e.g., for images and 3D shapes), adequately representing large-scale or complex scenes has proven a challenge. Current neural representations fail to accurately represent images at resolutions greater than a megapixel or 3D scenes with more than a few hundred thousand polygons. Here, we introduce a new hybrid implicit-explicit network architecture and training strategy that adaptively allocates resources during training and inference based on the local complexity of a signal of interest. Our approach uses a multiscale block-coordinate decomposition, similar to a quadtree or octree, that is optimized during training. The network architecture operates in two stages: using the bulk of the network parameters, a coordinate encoder generates a feature grid in a single forward pass. Then, hundreds or thousands of samples within each block can be efficiently evaluated using a lightweight feature decoder. With this hybrid implicit-explicit network architecture, we demonstrate the first experiments that fit gigapixel images to nearly 40 dB peak signal-to-noise ratio. Notably this represents an increase in scale of over 1000X compared to the resolution of previously demonstrated image-fitting experiments. Moreover, our approach is able to represent 3D shapes significantly faster and better than previous techniques; it reduces training times from days to hours or minutes and memory requirements by over an order of magnitude.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459785},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Acorn: Adaptive coordinate networks for neural scene representation},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). TransPose: Real-time 3D human translation and pose
estimation with six inertial sensors. <em>TOG</em>, <em>40</em>(4),
1–13. (<a href="https://doi.org/10.1145/3450626.3459786">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motion capture is facing some new possibilities brought by the inertial sensing technologies which do not suffer from occlusion or wide-range recordings as vision-based solutions do. However, as the recorded signals are sparse and quite noisy, online performance and global translation estimation turn out to be two key difficulties. In this paper, we present TransPose, a DNN-based approach to perform full motion capture (with both global translations and body poses) from only 6 Inertial Measurement Units (IMUs) at over 90 fps. For body pose estimation, we propose a multi-stage network that estimates leaf-to-full joint positions as intermediate results. This design makes the pose estimation much easier, and thus achieves both better accuracy and lower computation cost. For global translation estimation, we propose a supporting-foot-based method and an RNN-based method to robustly solve for the global translations with a confidence-based fusion technique. Quantitative and qualitative comparisons show that our method outperforms the state-of-the-art learning- and optimization-based methods with a large margin in both accuracy and efficiency. As a purely inertial sensor-based approach, our method is not limited by environmental settings (e.g., fixed cameras), making the capture free from common difficulties such as wide-range motion space and strong occlusion.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459786},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-13},
  shortjournal = {ACM Trans. Graph.},
  title        = {TransPose: Real-time 3D human translation and pose estimation with six inertial sensors},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). GPU-based simulation of cloth wrinkles at submillimeter
levels. <em>TOG</em>, <em>40</em>(4), 1–14. (<a
href="https://doi.org/10.1145/3450626.3459787">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study physics-based cloth simulation in a very high resolution setting, presumably at submillimeter levels with millions of vertices, to meet perceptual precision of our human eyes. State-of-the-art simulation techniques, mostly developed for unstructured triangular meshes, can hardly meet this demand due to their large computational costs and memory footprints. We argue that in a very high resolution, it is more plausible to use regular meshes with an underlying grid structure, which can be highly compatible with GPU acceleration like high-resolution images. Based on this idea, we formulate and solve the nonlinear optimization problem for simulating high-resolution wrinkles, by a fast block-based descent method with reduced memory accesses. We also investigate the development of the collision handling component in our system, whose performance benefits greatly from the grid structure. Finally, we explore various issues related to the applications of our system, including initialization for fast convergence and temporal coherence, gathering effects, inflation and stuffing models, and mesh simplification. We can treat our system as a quasistatic wrinkle synthesis tool, run it as a standalone dynamic simulator, or integrate it into a multi-resolution solver as an additional component. The experiment demonstrates the capability, efficiency and flexibility of our system in producing a variety of high-resolution wrinkles effects.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459787},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-14},
  shortjournal = {ACM Trans. Graph.},
  title        = {GPU-based simulation of cloth wrinkles at submillimeter levels},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). 3D weaving with curved ribbons. <em>TOG</em>,
<em>40</em>(4), 1–15. (<a
href="https://doi.org/10.1145/3450626.3459788">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Basket weaving is a traditional craft for creating curved surfaces as an interwoven array of thin, flexible, and initially straight ribbons. The three-dimensional shape of a woven structure emerges through a complex interplay of the elastic bending behavior of the ribbons and the contact forces at their crossings. Curvature can be injected by carefully placing topological singularities in the otherwise regular weaving pattern. However, shape control through topology is highly non-trivial and inherently discrete, which severely limits the range of attainable woven geometries. Here, we demonstrate how to construct arbitrary smooth free-form surface geometries by weaving carefully optimized curved ribbons. We present an optimization-based approach to solving the inverse design problem for such woven structures. Our algorithm computes the ribbons&#39; planar geometry such that their interwoven assembly closely approximates a given target design surface in equilibrium. We systematically validate our approach through a series of physical prototypes to show a broad range of new woven geometries that is not achievable by existing methods. We anticipate our computational approach to significantly enhance the capabilities for the design of new woven structures. Facilitated by modern digital fabrication technology, we see potential applications in material science, bio- and mechanical engineering, art, design, and architecture.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459788},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-15},
  shortjournal = {ACM Trans. Graph.},
  title        = {3D weaving with curved ribbons},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Computational inverse design of surface-based inflatables.
<em>TOG</em>, <em>40</em>(4), 1–14. (<a
href="https://doi.org/10.1145/3450626.3459789">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a computational inverse design method for a new class of surface-based inflatable structure. Our deployable structures are fabricated by fusing together two layers of inextensible sheet material along carefully selected curves. The fusing curves form a network of tubular channels that can be inflated with air or other fluids. When fully inflated, the initially flat surface assumes a programmed double-curved shape and becomes stiff and load-bearing. We present a method that solves for the layout of air channels that, when inflated, best approximate a given input design. For this purpose, we integrate a forward simulation method for inflation with a gradient-based optimization algorithm that continuously adapts the geometry of the air channels to improve the design objectives. To initialize this non-linear optimization, we propose a novel surface flattening algorithm. When a channel is inflated, it approximately maintains its length, but contracts transversally to its main direction. Our algorithm approximates this deformation behavior by computing a mapping from the 3D design surface to the plane that allows for anisotropic metric scaling within the bounds realizable by the physical system. We show a wide variety of inflatable designs and fabricate several prototypes to validate our approach and highlight potential applications.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459789},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Computational inverse design of surface-based inflatables},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). KnitKit: A flexible system for machine knitting of
customizable textiles. <em>TOG</em>, <em>40</em>(4), 1–16. (<a
href="https://doi.org/10.1145/3450626.3459790">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we introduce KnitKit , a flexible and customizable system for the computational design and production of functional, multi-material, and three-dimensional knitted textiles. Our system greatly simplifies the knitting of 3D objects with complex, varying patterns that use multiple yarns and stitch patterns by separating the high-level design specification in terms of geometry, stitch patterns, materials or colors from the low-level, machine-specific knitting instruction generation. Starting from a triangular 3D mesh and a 2D texture that specifies knitting patterns on top of the geometry, our system generates the required machine instructions in three major steps. First, the input is processed and the KnitNet data structure is generated. This graph structure serves as an abstract interface between the high-level geometric and knitting configuration and the low-level, machine-specific knitting instructions. Second, a graph rewriting procedure is applied on the KnitNet that produces a sequence of abstract machine actions. Finally, the low-level machine instructions are generated by adapting those abstract actions to a specific machine context. We showcase the potential of this computational approach by designing and fabricating a variety of objects with complex geometries, multiple yarns, and multiple stitch patterns.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459790},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-16},
  shortjournal = {ACM Trans. Graph.},
  title        = {KnitKit: A flexible system for machine knitting of customizable textiles},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A generic framework for physical light transport.
<em>TOG</em>, <em>40</em>(4), 1–20. (<a
href="https://doi.org/10.1145/3450626.3459791">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Physically accurate rendering often calls for taking the wave nature of light into consideration. In computer graphics, this is done almost exclusively locally, i.e. on a micrometre scale where the diffractive phenomena arise. However, the statistical properties of light, that dictate its coherence characteristics and its capacity to give rise to wave interference effects, evolve globally: these properties change on, e.g., interaction with a surface, diffusion by participating media and simply by propagation. In this paper, we derive the first global light transport framework that is able to account for these properties of light and, therefore, is fully consistent with Maxwell&#39;s electromagnetic theory. We show that our framework is a generalization of the classical, radiometry-based light transport---prominent in computer graphics---and retains some of its attractive properties. Finally, as a proof of concept, we apply the presented framework to a few practical problems in rendering and validate against well-studied methods in optics.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459791},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-20},
  shortjournal = {ACM Trans. Graph.},
  title        = {A generic framework for physical light transport},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Capturing detailed deformations of moving human bodies.
<em>TOG</em>, <em>40</em>(4), 1–18. (<a
href="https://doi.org/10.1145/3450626.3459792">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a new method to capture detailed human motion, sampling more than 1000 unique points on the body. Our method outputs highly accurate 4D (spatio-temporal) point coordinates and, crucially, automatically assigns a unique label to each of the points. The locations and unique labels of the points are inferred from individual 2D input images only, without relying on temporal tracking or any human body shape or skeletal kinematics models. Therefore, our captured point trajectories contain all of the details from the input images, including motion due to breathing, muscle contractions and flesh deformation, and are well suited to be used as training data to fit advanced models of the human body and its motion. The key idea behind our system is a new type of motion capture suit which contains a special pattern with checkerboard-like corners and two-letter codes. The images from our multi-camera system are processed by a sequence of neural networks which are trained to localize the corners and recognize the codes, while being robust to suit stretching and self-occlusions of the body. Our system relies only on standard RGB or monochrome sensors and fully passive lighting and the passive suit, making our method easy to replicate, deploy and use. Our experiments demonstrate highly accurate captures of a wide variety of human poses, including challenging motions such as yoga, gymnastics, or rolling on the ground.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459792},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-18},
  shortjournal = {ACM Trans. Graph.},
  title        = {Capturing detailed deformations of moving human bodies},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Interactive monte carlo denoising using affinity of neural
features. <em>TOG</em>, <em>40</em>(4), 1–13. (<a
href="https://doi.org/10.1145/3450626.3459793">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-quality denoising of Monte Carlo low-sample renderings remains a critical challenge for practical interactive ray tracing. We present a new learning-based denoiser that achieves state-of-the-art quality and runs at interactive rates. Our model processes individual path-traced samples with a lightweight neural network to extract per-pixel feature vectors. The rest of our pipeline operates in pixel space. We define a novel pairwise affinity over the features in a pixel neighborhood, from which we assemble dilated spatial kernels to filter the noisy radiance. Our denoiser is temporally stable thanks to two mechanisms. First, we keep a running average of the noisy radiance and intermediate features, using a per-pixel recursive filter with learned weights. Second, we use a small temporal kernel based on the pairwise affinity between features of consecutive frames. Our experiments show our new affinities lead to higher quality outputs than techniques with comparable computational costs, and better high-frequency details than kernel-predicting approaches. Our model matches or outperfoms state-of-the-art offline denoisers in the low-sample count regime (2--8 samples per pixel), and runs at interactive frame rates at 1080p resolution.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459793},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Interactive monte carlo denoising using affinity of neural features},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Real-time locally injective volumetric deformation.
<em>TOG</em>, <em>40</em>(4), 1–16. (<a
href="https://doi.org/10.1145/3450626.3459794">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a highly efficient method for interactive volumetric meshless shape deformation. Our method operates within a low dimensional sub-space of shape-aware C ∞ harmonic maps, and is the first method that is guaranteed to produce a smooth locally injective deformation in 3D. Unlike mesh-based methods in which local injectivity is enforced on tetrahedral elements, our method enforces injectivity on a sparse set of domain samples. The main difficulty is then to certify the map as locally injective throughout the entire domain. This is done by utilizing the Lipschitz continuity property of the harmonic basis functions. We show a surprising relation between the Lipschitz constant of the smallest singular value of the map Jacobian and the norm of the Hessian. We further carefully derive a Lipschitz constant for the Hessian, and develop a sufficient condition for the injectivity certification. This is done by utilizing the special structure of the harmonic basis functions combined with a novel regularization term that pushes the Lipschitz constants further down. As a result, the injectivity analysis can be performed on a relatively sparse set of samples. Combined with a parallel GPU-based implementation, our method can produce superior deformations with unique quality guarantees at real-time rates which were possible only in 2D so far.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459794},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Real-time locally injective volumetric deformation},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). NeuMIP: Multi-resolution neural materials. <em>TOG</em>,
<em>40</em>(4), 1–13. (<a
href="https://doi.org/10.1145/3450626.3459795">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose NeuMIP, a neural method for representing and rendering a variety of material appearances at different scales. Classical prefiltering (mipmapping) methods work well on simple material properties such as diffuse color, but fail to generalize to normals, self-shadowing, fibers or more complex microstructures and reflectances. In this work, we generalize traditional mipmap pyramids to pyramids of neural textures, combined with a fully connected network. We also introduce neural offsets, a novel method which enables rendering materials with intricate parallax effects without any tessellation. This generalizes classical parallax mapping, but is trained without supervision by any explicit heightfield. Neural materials within our system support a 7-dimensional query, including position, incoming and outgoing direction, and the desired filter kernel size. The materials have small storage (on the order of standard mipmapping except with more texture channels), and can be integrated within common Monte-Carlo path tracing systems. We demonstrate our method on a variety of materials, resulting in complex appearance across levels of detail, with accurate parallax, self-shadowing, and other effects.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459795},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-13},
  shortjournal = {ACM Trans. Graph.},
  title        = {NeuMIP: Multi-resolution neural materials},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). WireRoom: Model-guided explorative design of abstract wire
art. <em>TOG</em>, <em>40</em>(4), 1–13. (<a
href="https://doi.org/10.1145/3450626.3459796">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present WireRoom , a computational framework for the intelligent design of abstract 3D wire art to depict a given 3D model. Our algorithm generates a set of 3D wire shapes from the 3D model with informative, visually pleasing, and concise structures. It is achieved by solving a dynamic travelling salesman problem on the surface of the 3D model with a multi-path expansion approach. We introduce a novel explorative computational design procedure by taking the generated wire shapes as candidates, avoiding manual design of the wire shape structure. We compare our algorithm with a baseline method and conduct a user study to investigate the usability of the framework and the quality of the produced wire shapes. The results of the comparison and user study confirm that our framework is effective for producing informative, visually pleasing, and concise wire shapes.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459796},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-13},
  shortjournal = {ACM Trans. Graph.},
  title        = {WireRoom: Model-guided explorative design of abstract wire art},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). HodgeNet: Learning spectral geometry on triangle meshes.
<em>TOG</em>, <em>40</em>(4), 1–11. (<a
href="https://doi.org/10.1145/3450626.3459797">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Constrained by the limitations of learning toolkits engineered for other applications, such as those in image processing, many mesh-based learning algorithms employ data flows that would be atypical from the perspective of conventional geometry processing. As an alternative, we present a technique for learning from meshes built from standard geometry processing modules and operations. We show that low-order eigenvalue/eigenvector computation from operators parameterized using discrete exterior calculus is amenable to efficient approximate backpropagation, yielding spectral per-element or per-mesh features with similar formulas to classical descriptors like the heat/wave kernel signatures. Our model uses few parameters, generalizes to high-resolution meshes, and exhibits performance and time complexity on par with past work.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459797},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-11},
  shortjournal = {ACM Trans. Graph.},
  title        = {HodgeNet: Learning spectral geometry on triangle meshes},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Neural complex luminaires: Representation and rendering.
<em>TOG</em>, <em>40</em>(4), 1–12. (<a
href="https://doi.org/10.1145/3450626.3459798">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Complex luminaires, such as grand chandeliers, can be extremely costly to render because the light-emitting sources are typically encased in complex refractive geometry, creating difficult light paths that require many samples to evaluate with Monte Carlo approaches. Previous work has attempted to speed up this process, but the methods are either inaccurate, require the storage of very large lightfields, and/or do not fit well into modern path-tracing frameworks. Inspired by the success of deep networks, which can model complex relationships robustly and be evaluated efficiently, we propose to use a machine learning framework to compress a complex luminaire&#39;s lightfield into an implicit neural representation. Our approach can easily plug into conventional renderers, as it works with the standard techniques of path tracing and multiple importance sampling (MIS). Our solution is to train three networks to perform the essential operations for evaluating the complex luminaire at a specific point and view direction, importance sampling a point on the luminaire given a shading location, and blending to determine the transparency of luminaire queries to properly composite them with other scene elements. We perform favorably relative to state-of-the-art approaches and render final images that are close to the high-sample-count reference with only a fraction of the computation and storage costs, with no need to store the original luminaire geometry and materials.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459798},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-12},
  shortjournal = {ACM Trans. Graph.},
  title        = {Neural complex luminaires: Representation and rendering},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Volumetric appearance stylization with stylizing kernel
prediction network. <em>TOG</em>, <em>40</em>(4), 1–15. (<a
href="https://doi.org/10.1145/3450626.3459799">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper aims to efficiently construct the volume of heterogeneous single-scattering albedo for a given medium that would lead to desired color appearance. We achieve this goal by formulating it as a volumetric style transfer problem in which an input 3D density volume is stylized using color features extracted from a reference 2D image. Unlike existing algorithms that require cumbersome iterative optimizations, our method leverages a feed-forward deep neural network with multiple well-designed modules. At the core of our network is a stylizing kernel predictor (SKP) that extracts multi-scale feature maps from a 2D style image and predicts a handful of stylizing kernels as a highly non-linear combination of the feature maps. Each group of stylizing kernels represents a specific style. A volume autoencoder (VolAE) is designed and jointly learned with the SKP to transform a density volume to an albedo volume based on these stylizing kernels. Since the autoencoder does not encode any style information, it can generate different albedo volumes with a wide range of appearance once training is completed. Additionally, a hybrid multi-scale loss function is used to learn plausible color features and guarantee temporal coherence for time-evolving volumes. Through comprehensive experiments, we validate the effectiveness of our method and show its superiority by comparing against state-of-the-arts. We show that with our method a novice user can easily create a diverse set of realistic translucent effects for 3D models (either static or dynamic), neglecting any cumbersome process of parameter tuning.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459799},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Volumetric appearance stylization with stylizing kernel prediction network},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The design space of plane elastic curves. <em>TOG</em>,
<em>40</em>(4), 1–20. (<a
href="https://doi.org/10.1145/3450626.3459800">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Elastic bending of initially flat slender elements allows the realization and economic fabrication of intriguing curved shapes. In this work, we derive an intuitive but rigorous geometric characterization of the design space of plane elastic rods with variable stiffness. It enables designers to determine which shapes are physically viable with active bending by visual inspection alone. Building on these insights, we propose a method for efficiently designing the geometry of a flat elastic rod that realizes a target equilibrium curve, which only requires solving a linear program. We implement this method in an interactive computational design tool that gives feedback about the feasibility of a design, and computes the geometry of the structural elements necessary to realize it within an instant. The tool also offers an iterative optimization routine that improves the fabricability of a model while modifying it as little as possible. In addition, we use our geometric characterization to derive an algorithm for analyzing and recovering the stability of elastic curves that would otherwise snap out of their unstable equilibrium shapes by buckling. We show the efficacy of our approach by designing and manufacturing several physical models that are assembled from flat elements.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459800},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-20},
  shortjournal = {ACM Trans. Graph.},
  title        = {The design space of plane elastic curves},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fast quasi-harmonic weights for geometric data
interpolation. <em>TOG</em>, <em>40</em>(4), 1–15. (<a
href="https://doi.org/10.1145/3450626.3459801">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose quasi-harmonic weights for interpolating geometric data, which are orders of magnitude faster to compute than state-of-the-art. Currently, interpolation (or, skinning) weights are obtained by solving large-scale constrained optimization problems with explicit constraints to suppress oscillative patterns, yielding smooth weights only after a substantial amount of computation time. As an alternative, our weights are obtained as minima of an unconstrained problem that can be optimized quickly using straightforward numerical techniques. We consider weights that can be obtained as solutions to a parameterized family of second-order elliptic partial differential equations. By leveraging the maximum principle and careful parameterization, we pose weight computation as an inverse problem of recovering optimal anisotropic diffusivity tensors. In addition, we provide a customized ADAM solver that significantly reduces the number of gradient steps; our solver only requires inverting tens of linear systems that share the same sparsity pattern. Overall, our approach achieves orders of magnitude acceleration compared to previous methods, allowing weight computation in near real-time.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459801},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Fast quasi-harmonic weights for geometric data interpolation},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Intersection-free rigid body dynamics. <em>TOG</em>,
<em>40</em>(4), 1–16. (<a
href="https://doi.org/10.1145/3450626.3459802">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce the first implicit time-stepping algorithm for rigid body dynamics, with contact and friction, that guarantees intersection-free configurations at every time step. Our algorithm explicitly models the curved trajectories traced by rigid bodies in both collision detection and response. For collision detection, we propose a conservative narrow phase collision detection algorithm for curved trajectories, which reduces the problem to a sequence of linear CCD queries with minimal separation. For time integration and contact response, we extend the recently proposed incremental potential contact framework to reduced coordinates and rigid body dynamics. We introduce a benchmark for rigid body simulation and show that our approach, while less efficient than alternatives, can robustly handle a wide array of complex scenes, which cannot be simulated with competing methods, without requiring per-scene parameter tuning.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459802},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Intersection-free rigid body dynamics},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Path replay backpropagation: Differentiating light paths
using constant memory and linear time. <em>TOG</em>, <em>40</em>(4),
1–14. (<a href="https://doi.org/10.1145/3450626.3459804">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Differentiable physically-based rendering has become an indispensable tool for solving inverse problems involving light. Most applications in this area jointly optimize a large set of scene parameters to minimize an objective function, in which case reverse-mode differentiation is the method of choice for obtaining parameter gradients. However, existing techniques that perform the necessary differentiation step suffer from either statistical bias or a prohibitive cost in terms of memory and computation time. For example, standard techniques for automatic differentiation based on program transformation or Wengert tapes lead to impracticably large memory usage when applied to physically-based rendering algorithms. A recently proposed adjoint method by Nimier-David et al. [2020] reduces this to a constant memory footprint, but the computation time for unbiased gradient estimates then becomes quadratic in the number of scattering events along a light path. This is problematic when the scene contains highly scattering materials like participating media. In this paper, we propose a new unbiased backpropagation algorithm for rendering that only requires constant memory, and whose computation time is linear in the number of scattering events (i.e., just like path tracing). Our approach builds on the invertibility of the local Jacobian at scattering interactions to recover the various quantities needed for reverse-mode differentiation. Our method also extends to specular materials such as smooth dielectrics and conductors that cannot be handled by prior work.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459804},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Path replay backpropagation: Differentiating light paths using constant memory and linear time},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Only a matter of style: Age transformation using a
style-based regression model. <em>TOG</em>, <em>40</em>(4), 1–12. (<a
href="https://doi.org/10.1145/3450626.3459805">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The task of age transformation illustrates the change of an individual&#39;s appearance over time. Accurately modeling this complex transformation over an input facial image is extremely challenging as it requires making convincing, possibly large changes to facial features and head shape, while still preserving the input identity. In this work, we present an image-to-image translation method that learns to directly encode real facial images into the latent space of a pre-trained unconditional GAN (e.g., StyleGAN) subject to a given aging shift. We employ a pre-trained age regression network to explicitly guide the encoder in generating the latent codes corresponding to the desired age. In this formulation, our method approaches the continuous aging process as a regression task between the input age and desired target age, providing fine-grained control over the generated image. Moreover, unlike approaches that operate solely in the latent space using a prior on the path controlling age, our method learns a more disentangled, non-linear path. Finally, we demonstrate that the end-to-end nature of our approach, coupled with the rich semantic latent space of StyleGAN, allows for further editing of the generated images. Qualitative and quantitative evaluations show the advantages of our method compared to state-of-the-art approaches. Code is available at our project page: https://yuval-alaluf.github.io/SAM.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459805},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-12},
  shortjournal = {ACM Trans. Graph.},
  title        = {Only a matter of style: Age transformation using a style-based regression model},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Real-time 3D neural facial animation from binocular video.
<em>TOG</em>, <em>40</em>(4), 1–17. (<a
href="https://doi.org/10.1145/3450626.3459806">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a method for performing real-time facial animation of a 3D avatar from binocular video. Existing facial animation methods fail to automatically capture precise and subtle facial motions for driving a photo-realistic 3D avatar &quot;in-the-wild&quot; (i.e., variability in illumination, camera noise). The novelty of our approach lies in a light-weight process for specializing a personalized face model to new environments that enables extremely accurate real-time face tracking anywhere. Our method uses a pre-trained high-fidelity personalized model of the face that we complement with a novel illumination model to account for variations due to lighting and other factors often encountered in-the-wild (e.g., facial hair growth, makeup, skin blemishes). Our approach comprises two steps. First, we solve for our illumination model&#39;s parameters by applying analysis-by-synthesis on a short video recording. Using the pairs of model parameters (rigid, non-rigid) and the original images, we learn a regression for real-time inference from the image space to the 3D shape and texture of the avatar. Second, given a new video, we fine-tune the real-time regression model with a few-shot learning strategy to adapt the regression model to the new environment. We demonstrate our system&#39;s ability to precisely capture subtle facial motions in unconstrained scenarios, in comparison to competing methods, on a diverse collection of identities, expressions, and real-world environments.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459806},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-17},
  shortjournal = {ACM Trans. Graph.},
  title        = {Real-time 3D neural facial animation from binocular video},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Monte carlo estimators for differential light transport.
<em>TOG</em>, <em>40</em>(4), 1–16. (<a
href="https://doi.org/10.1145/3450626.3459807">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Physically based differentiable rendering algorithms propagate derivatives through realistic light transport simulations and have applications in diverse areas including inverse reconstruction and machine learning. Recent progress has led to unbiased methods that can simultaneously compute derivatives with respect to millions of parameters. At the same time, elementary properties of these methods remain poorly understood. Current algorithms for differentiable rendering are constructed by mechanically differentiating a given primal algorithm. While convenient, such an approach is simplistic because it leaves no room for improvement. Differentiation produces major changes in the integrals that occur throughout the rendering process, which indicates that the primal and differential algorithms should be decoupled so that the latter can suitably adapt. This leads to a large space of possibilities: consider that even the most basic Monte Carlo path tracer already involves several design choices concerning the techniques for sampling materials and emitters, and their combination, e.g. via multiple importance sampling (MIS). Differentiation causes a veritable explosion of this decision tree: should we differentiate only the estimator, or also the sampling technique? Should MIS be applied before or after differentiation? Are specialized derivative sampling strategies of any use? How should visibility-related discontinuities be handled when millions of parameters are differentiated simultaneously? In this paper, we provide a taxonomy and analysis of different estimators for differential light transport to provide intuition about these and related questions.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459807},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Monte carlo estimators for differential light transport},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Kelvin transformations for simulations on infinite domains.
<em>TOG</em>, <em>40</em>(4), 1–15. (<a
href="https://doi.org/10.1145/3450626.3459809">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Solving partial differential equations (PDEs) on infinite domains has been a challenging task in physical simulations and geometry processing. We introduce a general technique to transform a PDE problem on an unbounded domain to a PDE problem on a bounded domain. Our method uses the Kelvin Transform, which essentially inverts the distance from the origin. However, naive application of this coordinate mapping can still result in a singularity at the origin in the transformed domain. We show that by factoring the desired solution into the product of an analytically known (asymptotic) component and another function to solve for, the problem can be made continuous and compact, with solutions significantly more efficient and well-conditioned than traditional finite element and Monte Carlo numerical PDE methods on stretched coordinates. Specifically, we show that every Poisson or Laplace equation on an infinite domain is transformed to another Poisson (Laplace) equation on a compact region. In other words, any existing Poisson solver on a bounded domain is readily an infinite domain Poisson solver after being wrapped by our transformation. We demonstrate the integration of our method with finite difference and Monte Carlo PDE solvers, with applications in the fluid pressure solve and simulating electromagnetism, including visualizations of the solar magnetic field. Our transformation technique also applies to the Helmholtz equation whose solutions oscillate out to infinity. After the transformation, the Helmholtz equation becomes a tractable equation on a bounded domain without infinite oscillation. To our knowledge, this is the first time that the Helmholtz equation on an infinite domain is solved on a bounded grid without requiring an artificial absorbing boundary condition.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459809},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Kelvin transformations for simulations on infinite domains},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hierarchical neural reconstruction for path guiding using
hybrid path and photon samples. <em>TOG</em>, <em>40</em>(4), 1–16. (<a
href="https://doi.org/10.1145/3450626.3459810">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Path guiding is a promising technique to reduce the variance of path tracing. Although existing online path guiding algorithms can eventually learn good sampling distributions given a large amount of time and samples, the speed of learning becomes a major bottleneck. In this paper, we accelerate the learning of sampling distributions by training a light-weight neural network offline to reconstruct from sparse samples. Uniquely, we design our neural network to directly operate convolutions on a sparse quadtree, which regresses a high-quality hierarchical sampling distribution. Our approach can reconstruct reasonably accurate sampling distributions faster, allowing for efficient path guiding and rendering. In contrast to the recent offline neural path guiding techniques that reconstruct low-resolution 2D images for sampling, our novel hierarchical framework enables more fine-grained directional sampling with less memory usage, effectively advancing the practicality and efficiency of neural path guiding. In addition, we take advantage of hybrid bidirectional samples including both path samples and photons, as we have found this more robust to different light transport scenarios compared to using only one type of sample as in previous work. Experiments on diverse testing scenes demonstrate that our approach often improves rendering results with better visual quality and lower errors. Our framework can also provide the proper balance of speed, memory cost, and robustness.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459810},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Hierarchical neural reconstruction for path guiding using hybrid path and photon samples},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Real-time neural radiance caching for path tracing.
<em>TOG</em>, <em>40</em>(4), 1–16. (<a
href="https://doi.org/10.1145/3450626.3459812">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a real-time neural radiance caching method for path-traced global illumination. Our system is designed to handle fully dynamic scenes, and makes no assumptions about the lighting, geometry, and materials. The data-driven nature of our approach sidesteps many difficulties of caching algorithms, such as locating, interpolating, and updating cache points. Since pretraining neural networks to handle novel, dynamic scenes is a formidable generalization challenge, we do away with pretraining and instead achieve generalization via adaptation , i.e. we opt for training the radiance cache while rendering. We employ self-training to provide low-noise training targets and simulate infinite-bounce transport by merely iterating few-bounce training updates. The updates and cache queries incur a mild overhead---about 2.6ms on full HD resolution---thanks to a streaming implementation of the neural network that fully exploits modern hardware. We demonstrate significant noise reduction at the cost of little induced bias, and report state-of-the-art, real-time performance on a number of challenging scenarios.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459812},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Real-time neural radiance caching for path tracing},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The effect of shape and illumination on material perception:
Model and applications. <em>TOG</em>, <em>40</em>(4), 1–16. (<a
href="https://doi.org/10.1145/3450626.3459813">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Material appearance hinges on material reflectance properties but also surface geometry and illumination. The unlimited number of potential combinations between these factors makes understanding and predicting material appearance a very challenging task. In this work, we collect a large-scale dataset of perceptual ratings of appearance attributes with more than 215,680 responses for 42,120 distinct combinations of material, shape, and illumination. The goal of this dataset is twofold. First, we analyze for the first time the effects of illumination and geometry in material perception across such a large collection of varied appearances. We connect our findings to those of the literature, discussing how previous knowledge generalizes across very diverse materials, shapes, and illuminations. Second, we use the collected dataset to train a deep learning architecture for predicting perceptual attributes that correlate with human judgments. We demonstrate the consistent and robust behavior of our predictor in various challenging scenarios, which, for the first time, enables estimating perceived material attributes from general 2D images. Since our predictor relies on the final appearance in an image, it can compare appearance properties across different geometries and illumination conditions. Finally, we demonstrate several applications that use our predictor, including appearance reproduction using 3D printing, BRDF editing by integrating our predictor in a differentiable renderer, illumination design, or material recommendations for scene design.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459813},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-16},
  shortjournal = {ACM Trans. Graph.},
  title        = {The effect of shape and illumination on material perception: Model and applications},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Coarse-to-fine: Facial structure editing of portrait images
via latent space classifications. <em>TOG</em>, <em>40</em>(4), 1–13.
(<a href="https://doi.org/10.1145/3450626.3459814">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial structure editing of portrait images is challenging given the facial variety, the lack of ground-truth, the necessity of jointly adjusting color and shape, and the requirement of no visual artifacts. In this paper, we investigate how to perform chin editing as a case study of editing facial structures. We present a novel method that can automatically remove the double chin effect in portrait images. Our core idea is to train a fine classification boundary in the latent space of the portrait images. This can be used to edit the chin appearance by manipulating the latent code of the input portrait image while preserving the original portrait features. To achieve such a fine separation boundary, we employ a carefully designed training stage based on latent codes of paired synthetic images with and without a double chin. In the testing stage, our method can automatically handle portrait images with only a refinement to subtle misalignment before and after double chin editing. Our model enables alteration to the neck region of the input portrait image while keeping other regions unchanged, and guarantees the rationality of neck structure and the consistency of facial characteristics. To the best of our knowledge, this presents the first effort towards an effective application for editing double chins. We validate the efficacy and efficiency of our approach through extensive experiments and user studies.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459814},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Coarse-to-fine: Facial structure editing of portrait images via latent space classifications},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A non-exponential transmittance model for volumetric scene
representations. <em>TOG</em>, <em>40</em>(4), 1–16. (<a
href="https://doi.org/10.1145/3450626.3459815">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a novel transmittance model to improve the volumetric representation of 3D scenes. The model can represent opaque surfaces in the volumetric light transport framework. Volumetric representations are useful for complex scenes, and become increasingly popular for level of detail and scene reconstruction. The traditional exponential transmittance model found in volumetric light transport cannot capture correlations in visibility across volume elements. When representing opaque surfaces as volumetric density, this leads to both bloating of silhouettes and light leaking artifacts. By introducing a parametric non-exponential transmittance model, we are able to approximate these correlation effects and significantly improve the accuracy of volumetric appearance representation of opaque scenes. Our parametric transmittance model can represent a continuum between the linear transmittance that opaque surfaces exhibit and the traditional exponential transmittance encountered in participating media and unstructured geometries. This covers a large part of the spectrum of geometric structures encountered in complex scenes. In order to handle the spatially varying transmittance correlation effects, we further extend the theory of non-exponential participating media to a heterogeneous transmittance model. Our model is compact in storage and computationally efficient both for evaluation and for reverse-mode gradient computation. Applying our model to optimization algorithms yields significant improvements in volumetric scene appearance quality. We further show improvements for relevant applications, such as scene appearance prefiltering, image-based scene reconstruction using differentiable rendering, neural representations, and compare it to a conventional exponential model.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459815},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-16},
  shortjournal = {ACM Trans. Graph.},
  title        = {A non-exponential transmittance model for volumetric scene representations},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Mechanics-aware deformation of yarn pattern geometry.
<em>TOG</em>, <em>40</em>(4), 1–11. (<a
href="https://doi.org/10.1145/3450626.3459816">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Triangle mesh-based simulations are able to produce satisfying animations of knitted and woven cloth; however, they lack the rich geometric detail of yarn-level simulations. Naive texturing approaches do not consider yarn-level physics, while full yarn-level simulations may become prohibitively expensive for large garments. We propose a method to animate yarn-level cloth geometry on top of an underlying deforming mesh in a mechanics-aware fashion. Using triangle strains to interpolate precomputed yarn geometry, we are able to reproduce effects such as knit loops tightening under stretching. In combination with precomputed mesh animation or real-time mesh simulation, our method is able to animate yarn-level cloth in real-time at large scales.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459816},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-11},
  shortjournal = {ACM Trans. Graph.},
  title        = {Mechanics-aware deformation of yarn pattern geometry},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Discovering diverse athletic jumping strategies.
<em>TOG</em>, <em>40</em>(4), 1–17. (<a
href="https://doi.org/10.1145/3450626.3459817">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a framework that enables the discovery of diverse and natural-looking motion strategies for athletic skills such as the high jump. The strategies are realized as control policies for physics-based characters. Given a task objective and an initial character configuration, the combination of physics simulation and deep reinforcement learning (DRL) provides a suitable starting point for automatic control policy training. To facilitate the learning of realistic human motions, we propose a Pose Variational Autoencoder (P-VAE) to constrain the actions to a subspace of natural poses. In contrast to motion imitation methods, a rich variety of novel strategies can naturally emerge by exploring initial character states through a sample-efficient Bayesian diversity search (BDS) algorithm. A second stage of optimization that encourages novel policies can further enrich the unique strategies discovered. Our method allows for the discovery of diverse and novel strategies for athletic jumping motions such as high jumps and obstacle jumps with no motion examples and less reward engineering than prior work.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459817},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-17},
  shortjournal = {ACM Trans. Graph.},
  title        = {Discovering diverse athletic jumping strategies},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fusion 360 gallery: A dataset and environment for
programmatic CAD construction from human design sequences. <em>TOG</em>,
<em>40</em>(4), 1–24. (<a
href="https://doi.org/10.1145/3450626.3459818">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Parametric computer-aided design (CAD) is a standard paradigm used to design manufactured objects, where a 3D shape is represented as a program supported by the CAD software. Despite the pervasiveness of parametric CAD and a growing interest from the research community, currently there does not exist a dataset of realistic CAD models in a concise programmatic form. In this paper we present the Fusion 360 Gallery , consisting of a simple language with just the sketch and extrude modeling operations, and a dataset of 8,625 human design sequences expressed in this language. We also present an interactive environment called the Fusion 360 Gym , which exposes the sequential construction of a CAD program as a Markov decision process, making it amendable to machine learning approaches. As a use case for our dataset and environment, we define the CAD reconstruction task of recovering a CAD program from a target geometry. We report results of applying state-of-the-art methods of program synthesis with neurally guided search on this task.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459818},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-24},
  shortjournal = {ACM Trans. Graph.},
  title        = {Fusion 360 gallery: A dataset and environment for programmatic CAD construction from human design sequences},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Tracing versus freehand for evaluating computer-generated
drawings. <em>TOG</em>, <em>40</em>(4), 1–12. (<a
href="https://doi.org/10.1145/3450626.3459819">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Non-photorealistic rendering (NPR) and image processing algorithms are widely assumed as a proxy for drawing. However, this assumption is not well assessed due to the difficulty in collecting and registering freehand drawings. Alternatively, tracings are easier to collect and register, but there is no quantitative evaluation of tracing as a proxy for freehand drawing. In this paper, we compare tracing, freehand drawing, and computer-generated drawing approximation (CGDA) to understand their similarities and differences. We collected a dataset of 1,498 tracings and freehand drawings by 110 participants for 100 image prompts. Our drawings are registered to the prompts and include vector-based timestamped strokes collected via stylus input. Comparing tracing and freehand drawing, we found a high degree of similarity in stroke placement and types of strokes used over time. We show that tracing can serve as a viable proxy for freehand drawing because of similar correlations between spatio-temporal stroke features and labeled stroke types. Comparing hand-drawn content and current CGDA output, we found that 60% of drawn pixels corresponded to computer-generated pixels on average. The overlap tended to be commonly drawn content, but people&#39;s artistic choices and temporal tendencies remained largely uncaptured. We present an initial analysis to inform new CGDA algorithms and drawing applications, and provide the dataset for use by the community.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459819},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-12},
  shortjournal = {ACM Trans. Graph.},
  title        = {Tracing versus freehand for evaluating computer-generated drawings},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A unified second-order accurate in time MPM formulation for
simulating viscoelastic liquids with phase change. <em>TOG</em>,
<em>40</em>(4), 1–18. (<a
href="https://doi.org/10.1145/3450626.3459820">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We assume that the viscous forces in any liquid are simultaneously local and non-local , and introduce the extended POM-POM model [McLeish and Larson 1998; Oishi et al. 2012; Verbeeten et al. 2001] to computer graphics to design a unified constitutive model for viscosity that generalizes prior models, such as Oldroyd-B, the Upper-convected Maxwell (UCM) model [Sadeghy et al. 2005], and classical Newtonian viscosity under one umbrella, recovering each of them with different parameter values. Implicit discretization of our model via backward Euler recovers the variational Stokes solver of [Larionov et al. 2017] for Newtonian viscosity. For greater accuracy, however, we introduce the second-order accurate Generalized Single Step Single Solve (GS4) scheme [Tamma et al. 2000; Zhou and Tamma 2004] to computer graphics, which recovers all prior second-order accurate time integration schemes to date. Using GS4 and our generalized constitutive model, we present a Material Point Method (MPM) for simulating various viscoelastic liquid behaviors, such as classical liquid rope coiling, buckling, folding, and shear thinning/thickening. In addition, we show how to couple our viscoelastic liquid simulator with the recently introduced non-Fourier heat diffusion solver [Xue et al. 2020] for simulating problems with phase change, such as melting chocolate and digital fabrication with 3D printing. While the discretization of heat diffusion is slightly different within GS4, we show that it can still be efficiently solved using an assembly-free Multigrid-preconditioned Conjugate Gradients solver. We present end-to-end 3D simulations to demonstrate the versatility of our framework.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459820},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-18},
  shortjournal = {ACM Trans. Graph.},
  title        = {A unified second-order accurate in time MPM formulation for simulating viscoelastic liquids with phase change},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). ShapeMOD: Macro operation discovery for 3D shape programs.
<em>TOG</em>, <em>40</em>(4), 1–16. (<a
href="https://doi.org/10.1145/3450626.3459821">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A popular way to create detailed yet easily controllable 3D shapes is via procedural modeling, i.e. generating geometry using programs. Such programs consist of a series of instructions along with their associated parameter values. To fully realize the benefits of this representation, a shape program should be compact and only expose degrees of freedom that allow for meaningful manipulation of output geometry. One way to achieve this goal is to design higher-level macro operators that, when executed, expand into a series of commands from the base shape modeling language. However, manually authoring such macros, much like shape programs themselves, is difficult and largely restricted to domain experts. In this paper, we present ShapeMOD, an algorithm for automatically discovering macros that are useful across large datasets of 3D shape programs. ShapeMOD operates on shape programs expressed in an imperative, statement-based language. It is designed to discover macros that make programs more compact by minimizing the number of function calls and free parameters required to represent an input shape collection. We run ShapeMOD on multiple collections of programs expressed in a domain-specific language for 3D shape structures. We show that it automatically discovers a concise set of macros that abstract out common structural and parametric patterns that generalize over large shape collections. We also demonstrate that the macros found by ShapeMOD improve performance on downstream tasks including shape generative modeling and inferring programs from point clouds. Finally, we conduct a user study that indicates that ShapeMOD&#39;s discovered macros make interactive shape editing more efficient.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459821},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-16},
  shortjournal = {ACM Trans. Graph.},
  title        = {ShapeMOD: Macro operation discovery for 3D shape programs},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Seamless manga inpainting with semantics awareness.
<em>TOG</em>, <em>40</em>(4), 1–11. (<a
href="https://doi.org/10.1145/3450626.3459822">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Manga inpainting fills up the disoccluded pixels due to the removal of dialogue balloons or &quot;sound effect&quot; text. This process is long needed by the industry for the language localization and the conversion to animated manga. It is mostly done manually, as existing methods (mostly for natural image inpainting) cannot produce satisfying results. Manga inpainting is more tricky than natural image inpainting because its highly abstract illustration using structural lines and screentone patterns, which confuses the semantic interpretation and visual content synthesis. In this paper, we present the first manga inpainting method, a deep learning model, that generates high-quality results. Instead of direct inpainting, we propose to separate the complicated inpainting into two major phases, semantic inpainting and appearance synthesis. This separation eases both the feature understanding and hence the training of the learning model. A key idea is to disentangle the structural line and screentone, that helps the network to better distinguish the structural line and the screentone features for semantic interpretation. Both the visual comparison and the quantitative experiments evidence the effectiveness of our method and justify its superiority over existing state-of-the-art methods in the application of manga inpainting.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459822},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-11},
  shortjournal = {ACM Trans. Graph.},
  title        = {Seamless manga inpainting with semantics awareness},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DAG amendment for inverse control of parametric shapes.
<em>TOG</em>, <em>40</em>(4), 1–14. (<a
href="https://doi.org/10.1145/3450626.3459823">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Parametric shapes model objects as programs producing a geometry based on a few semantic degrees of freedom, called hyper-parameters. These shapes are the typical output of non-destructive modeling, CAD modeling or rigging. However they suffer from the core issue of being manipulated only indirectly, through a series of values rather than the geometry itself. In this paper, we introduce an amendment process of the underlying direct acyclic graph (DAG) of a parametric shape. This amendment enables a local differentiation of the shape w.r.t. its hyper-parameters that we leverage to provide interactive direct manipulation of the output. By acting on the shape synthesis process itself, our method is agnostic to the variations of the connectivity and topology that may occur in its output while changing the input hyper-parameters. Furthermore, our method is oblivious to the internal logic of the DAG nodes. We illustrate our approach on a collection of examples combining the typical nodes found in modern parametric modeling packages - such as deformation, booleans and surfacing operators - for which our method provides the user with inverse control over the hyper-parameters through a brush stroke metaphor.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459823},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-14},
  shortjournal = {ACM Trans. Graph.},
  title        = {DAG amendment for inverse control of parametric shapes},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Low-cost SPAD sensing for non-line-of-sight tracking,
material classification and depth imaging. <em>TOG</em>, <em>40</em>(4),
1–12. (<a href="https://doi.org/10.1145/3450626.3459824">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time-correlated imaging is an emerging sensing modality that has been shown to enable promising application scenarios, including lidar ranging, fluorescence lifetime imaging, and even non-line-of-sight sensing. A leading technology for obtaining time-correlated light measurements are single-photon avalanche diodes (SPADs), which are extremely sensitive and capable of temporal resolution on the order of tens of picoseconds. However, the rare and expensive optical setups used by researchers have so far prohibited these novel sensing techniques from entering the mass market. Fortunately, SPADs also exist in a radically cheaper and more power-efficient version that has been widely deployed as proximity sensors in mobile devices for almost a decade. These commodity SPAD sensors can be obtained at a mere few cents per detector pixel. However, their inferior data quality and severe technical drawbacks compared to their high-end counterparts necessitate the use of additional optics and suitable processing algorithms. In this paper, we adopt an existing evaluation platform for commodity SPAD sensors, and modify it to unlock time-of-flight (ToF) histogramming and hence computational imaging. Based on this platform, we develop and demonstrate a family of hardware/software systems that, for the first time, implement applications that had so far been limited to significantly more advanced, higher-priced setups: direct ToF depth imaging, non-line-of-sight object tracking, and material classification.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459824},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-12},
  shortjournal = {ACM Trans. Graph.},
  title        = {Low-cost SPAD sensing for non-line-of-sight tracking, material classification and depth imaging},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Neural monocular 3D human motion capture with physical
awareness. <em>TOG</em>, <em>40</em>(4), 1–15. (<a
href="https://doi.org/10.1145/3450626.3459825">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a new trainable system for physically plausible markerless 3D human motion capture, which achieves state-of-the-art results in a broad range of challenging scenarios. Unlike most neural methods for human motion capture, our approach, which we dub &quot;physionical&quot;, is aware of physical and environmental constraints. It combines in a fully-differentiable way several key innovations, i.e. , 1) a proportional-derivative controller, with gains predicted by a neural network, that reduces delays even in the presence of fast motions, 2) an explicit rigid body dynamics model and 3) a novel optimisation layer that prevents physically implausible foot-floor penetration as a hard constraint. The inputs to our system are 2D joint keypoints, which are canonicalised in a novel way so as to reduce the dependency on intrinsic camera parameters---both at train and test time. This enables more accurate global translation estimation without generalisability loss. Our model can be finetuned only with 2D annotations when the 3D annotations are not available. It produces smooth and physically-principled 3D motions in an interactive frame rate in a wide variety of challenging scenes, including newly recorded ones. Its advantages are especially noticeable on in-the-wild sequences that significantly differ from common 3D pose estimation benchmarks such as Human 3.6M and MPI-INF-3DHP. Qualitative results are provided in the supplementary video.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459825},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Neural monocular 3D human motion capture with physical awareness},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning time-critical responses for interactive character
control. <em>TOG</em>, <em>40</em>(4), 1–11. (<a
href="https://doi.org/10.1145/3450626.3459826">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Creating agile and responsive characters from a collection of unorganized human motion has been an important problem of constructing interactive virtual environments. Recently, learning-based approaches have successfully been exploited to learn deep network policies for the control of interactive characters. The agility and responsiveness of deep network policies are influenced by many factors, such as the composition of training datasets, the architecture of network models, and learning algorithms that involve many threshold values, weights, and hyper-parameters. In this paper, we present a novel teacher-student framework to learn time-critically responsive policies, which guarantee the time-to-completion between user inputs and their associated responses regardless of the size and composition of the motion databases. We demonstrate the effectiveness of our approach with interactive characters that can respond to the user&#39;s control quickly while performing agile, highly dynamic movements.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459826},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-11},
  shortjournal = {ACM Trans. Graph.},
  title        = {Learning time-critical responses for interactive character control},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Displaced signed distance fields for additive manufacturing.
<em>TOG</em>, <em>40</em>(4), 1–13. (<a
href="https://doi.org/10.1145/3450626.3459827">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose displaced signed distance fields, an implicit shape representation to accurately, efficiently and robustly 3D-print finely detailed and smoothly curved surfaces at native device resolution. As the resolution and accuracy of 3D printers increase, accurate reproduction of such surfaces becomes increasingly realizable from a hardware perspective. However, representing such surfaces with polygonal meshes requires high polygon counts, resulting in excessive storage, transmission and processing costs. These costs increase with print size, and can become exorbitant for large prints. Our implicit formulation simultaneously allows the augmentation of low-polygon meshes with compact meso-scale topographic information, such as displacement maps, and the realization of curved polygons, while leveraging efficient, streaming-compatible, discrete voxel-wise algorithms. Critical for this is careful treatment of the input primitives, their voxel approximation and the displacement to the true surface. We further propose a robust sign estimation to allow for incomplete, non-manifold input, whether human-made for onscreen rendering or directly out of a scanning pipeline. Our framework is efficient both in terms of time and space. The running time is independent of the number of input polygons, the amount of displacement, and is constant per voxel. The storage costs grow sub-linearly with the number of voxels, making our approach suitable for large prints. We evaluate our approach for efficiency and robustness, and show its advantages over standard techniques.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459827},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Displaced signed distance fields for additive manufacturing},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep relightable appearance models for animatable faces.
<em>TOG</em>, <em>40</em>(4), 1–15. (<a
href="https://doi.org/10.1145/3450626.3459829">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a method for building high-fidelity animatable 3D face models that can be posed and rendered with novel lighting environments in real-time. Our main insight is that relightable models trained to produce an image lit from a single light direction can generalize to natural illumination conditions but are computationally expensive to render. On the other hand, efficient, high-fidelity face models trained with point-light data do not generalize to novel lighting conditions. We leverage the strengths of each of these two approaches. We first train an expensive but generalizable model on point-light illuminations, and use it to generate a training set of high-quality synthetic face images under natural illumination conditions. We then train an efficient model on this augmented dataset, reducing the generalization ability requirements. As the efficacy of this approach hinges on the quality of the synthetic data we can generate, we present a study of lighting pattern combinations for dynamic captures and evaluate their suitability for learning generalizable relightable models. Towards achieving the best possible quality, we present a novel approach for generating dynamic relightable faces that exceeds state-of-the-art performance. Our method is capable of capturing subtle lighting effects and can even generate compelling near-field relighting despite being trained exclusively with far-field lighting data. Finally, we motivate the utility of our model by animating it with images captured from VR-headset mounted cameras, demonstrating the first system for face-driven interactions in VR that uses a photorealistic relightable face model.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459829},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Deep relightable appearance models for animatable faces},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). ManipNet: Neural manipulation synthesis with a hand-object
spatial representation. <em>TOG</em>, <em>40</em>(4), 1–14. (<a
href="https://doi.org/10.1145/3450626.3459830">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Natural hand manipulations exhibit complex finger maneuvers adaptive to object shapes and the tasks at hand. Learning dexterous manipulation from data in a brute force way would require a prohibitive amount of examples to effectively cover the combinatorial space of 3D shapes and activities. In this paper, we propose a hand-object spatial representation that can achieve generalization from limited data. Our representation combines the global object shape as voxel occupancies with local geometric details as samples of closest distances. This representation is used by a neural network to regress finger motions from input trajectories of wrists and objects. Specifically, we provide the network with the current finger pose, past and future trajectories, and the spatial representations extracted from these trajectories. The network then predicts a new finger pose for the next frame as an autoregressive model. With a carefully chosen hand-centric coordinate system, we can handle single-handed and two-handed motions in a unified framework. Learning from a small number of primitive shapes and kitchenware objects, the network is able to synthesize a variety of finger gaits for grasping, in-hand manipulation, and bimanual object handling on a rich set of novel shapes and functional tasks. We also demonstrate a live demo of manipulating virtual objects in real-time using a simple physical prop. Our system is useful for offline animation or real-time applications forgiving to a small delay.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459830},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-14},
  shortjournal = {ACM Trans. Graph.},
  title        = {ManipNet: Neural manipulation synthesis with a hand-object spatial representation},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). FovVideoVDP: A visible difference predictor for wide
field-of-view video. <em>TOG</em>, <em>40</em>(4), 1–19. (<a
href="https://doi.org/10.1145/3450626.3459831">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {FovVideoVDP is a video difference metric that models the spatial, temporal, and peripheral aspects of perception. While many other metrics are available, our work provides the first practical treatment of these three central aspects of vision simultaneously. The complex interplay between spatial and temporal sensitivity across retinal locations is especially important for displays that cover a large field-of-view, such as Virtual and Augmented Reality displays, and associated methods, such as foveated rendering. Our metric is derived from psychophysical studies of the early visual system, which model spatio-temporal contrast sensitivity, cortical magnification and contrast masking. It accounts for physical specification of the display (luminance, size, resolution) and viewing distance. To validate the metric, we collected a novel foveated rendering dataset which captures quality degradation due to sampling and reconstruction. To demonstrate our algorithm&#39;s generality, we test it on 3 independent foveated video datasets, and on a large image quality dataset, achieving the best performance across all datasets when compared to the state-of-the-art.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459831},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-19},
  shortjournal = {ACM Trans. Graph.},
  title        = {FovVideoVDP: A visible difference predictor for wide field-of-view video},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DiffAqua: A differentiable computational design pipeline for
soft underwater swimmers with shape interpolation. <em>TOG</em>,
<em>40</em>(4), 1–14. (<a
href="https://doi.org/10.1145/3450626.3459832">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The computational design of soft underwater swimmers is challenging because of the high degrees of freedom in soft-body modeling. In this paper, we present a differentiable pipeline for co-designing a soft swimmer&#39;s geometry and controller. Our pipeline unlocks gradient-based algorithms for discovering novel swimmer designs more efficiently than traditional gradient-free solutions. We propose Wasserstein barycenters as a basis for the geometric design of soft underwater swimmers since it is differentiable and can naturally interpolate between bio-inspired base shapes via optimal transport. By combining this design space with differentiable simulation and control, we can efficiently optimize a soft underwater swimmer&#39;s performance with fewer simulations than baseline methods. We demonstrate the efficacy of our method on various design problems such as fast, stable, and energy-efficient swimming and demonstrate applicability to multi-objective design.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459832},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-14},
  shortjournal = {ACM Trans. Graph.},
  title        = {DiffAqua: A differentiable computational design pipeline for soft underwater swimmers with shape interpolation},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). General virtual sketching framework for vector line art.
<em>TOG</em>, <em>40</em>(4), 1–14. (<a
href="https://doi.org/10.1145/3450626.3459833">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vector line art plays an important role in graphic design, however, it is tedious to manually create. We introduce a general framework to produce line drawings from a wide variety of images, by learning a mapping from raster image space to vector image space. Our approach is based on a recurrent neural network that draws the lines one by one. A differentiable rasterization module allows for training with only supervised raster data. We use a dynamic window around a virtual pen while drawing lines, implemented with a proposed aligned cropping and differentiable pasting modules. Furthermore, we develop a stroke regularization loss that encourages the model to use fewer and longer strokes to simplify the resulting vector image. Ablation studies and comparisons with existing methods corroborate the efficiency of our approach which is able to generate visually better results in less computation time, while generalizing better to a diversity of images and applications.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459833},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-14},
  shortjournal = {ACM Trans. Graph.},
  title        = {General virtual sketching framework for vector line art},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Orienting point clouds with dipole propagation.
<em>TOG</em>, <em>40</em>(4), 1–14. (<a
href="https://doi.org/10.1145/3450626.3459835">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Establishing a consistent normal orientation for point clouds is a notoriously difficult problem in geometry processing, requiring attention to both local and global shape characteristics. The normal direction of a point is a function of the local surface neighborhood; yet, point clouds do not disclose the full underlying surface structure. Even assuming known geodesic proximity, calculating a consistent normal orientation requires the global context. In this work, we introduce a novel approach for establishing a globally consistent normal orientation for point clouds. Our solution separates the local and global components into two different sub-problems. In the local phase, we train a neural network to learn a coherent normal direction per patch ( i.e. , consistently oriented normals within a single patch). In the global phase, we propagate the orientation across all coherent patches using a dipole propagation. Our dipole propagation decides to orient each patch using the electric field defined by all previously orientated patches. This gives rise to a global propagation that is stable, as well as being robust to nearby surfaces, holes, sharp features and noise.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459835},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Orienting point clouds with dipole propagation},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SWAGAN: A style-based wavelet-driven generative model.
<em>TOG</em>, <em>40</em>(4), 1–11. (<a
href="https://doi.org/10.1145/3450626.3459836">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, considerable progress has been made in the visual quality of Generative Adversarial Networks (GANs). Even so, these networks still suffer from degradation in quality for high-frequency content, stemming from a spectrally biased architecture, and similarly unfavorable loss functions. To address this issue, we present a novel general-purpose Style and WAvelet based GAN (SWAGAN) that implements progressive generation in the frequency domain. SWAGAN incorporates wavelets throughout its generator and discriminator architectures, enforcing a frequency-aware latent representation at every step of the way. This approach, designed to directly tackle the spectral bias of neural networks, yields an improvement in the ability to generate medium and high frequency content, including structures which other networks fail to learn. We demonstrate the advantage of our method by integrating it into the SyleGAN2 framework, and verifying that content generation in the wavelet domain leads to more realistic high-frequency content, even when trained for fewer iterations. Furthermore, we verify that our model&#39;s latent space retains the qualities that allow StyleGAN to serve as a basis for a multitude of editing tasks, and show that our frequency-aware approach also induces improved high-frequency performance in downstream tasks.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459836},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-11},
  shortjournal = {ACM Trans. Graph.},
  title        = {SWAGAN: A style-based wavelet-driven generative model},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Geometry and tool motion planning for curvature adapted CNC
machining. <em>TOG</em>, <em>40</em>(4), 1–16. (<a
href="https://doi.org/10.1145/3450626.3459837">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {CNC machining is the leading subtractive manufacturing technology. Although it is in use since decades, it is far from fully solved and still a rich source for challenging problems in geometric computing. We demonstrate this at hand of 5-axis machining of freeform surfaces, where the degrees of freedom in selecting and moving the cutting tool allow one to adapt the tool motion optimally to the surface to be produced. We aim at a high-quality surface finish, thereby reducing the need for hard-to-control post-machining processes such as grinding and polishing. Our work is based on a careful geometric analysis of curvature-adapted machining via so-called second order line contact between tool and target surface. On the geometric side, this leads to a new continuous transition between &quot;dual&quot; classical results in surface theory concerning osculating circles of surface curves and osculating cones of tangentially circumscribed developable surfaces. Practically, it serves as an effective basis for tool motion planning. Unlike previous approaches to curvature-adapted machining, we solve locally optimal tool positioning and motion planning within a single optimization framework and achieve curvature adaptation even for convex surfaces. This is possible with a toroidal cutter that contains a negatively curved cutting area. The effectiveness of our approach is verified at hand of digital models, simulations and machined parts, including a comparison to results generated with commercial software.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459837},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Geometry and tool motion planning for curvature adapted CNC machining},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Designing an encoder for StyleGAN image manipulation.
<em>TOG</em>, <em>40</em>(4), 1–14. (<a
href="https://doi.org/10.1145/3450626.3459838">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, there has been a surge of diverse methods for performing image editing by employing pre-trained unconditional generators. Applying these methods on real images, however, remains a challenge, as it necessarily requires the inversion of the images into their latent space. To successfully invert a real image, one needs to find a latent code that reconstructs the input image accurately, and more importantly, allows for its meaningful manipulation. In this paper, we carefully study the latent space of StyleGAN, the state-of-the-art unconditional generator. We identify and analyze the existence of a distortion-editability tradeoff and a distortion-perception tradeoff within the StyleGAN latent space. We then suggest two principles for designing encoders in a manner that allows one to control the proximity of the inversions to regions that StyleGAN was originally trained on. We present an encoder based on our two principles that is specifically designed for facilitating editing on real images by balancing these tradeoffs. By evaluating its performance qualitatively and quantitatively on numerous challenging domains, including cars and horses, we show that our inversion method, followed by common editing techniques, achieves superior real-image editing quality, with only a small reconstruction accuracy drop.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459838},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Designing an encoder for StyleGAN image manipulation},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Using isometries for computational design and fabrication.
<em>TOG</em>, <em>40</em>(4), 1–12. (<a
href="https://doi.org/10.1145/3450626.3459839">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We solve the task of representing free forms by an arrangement of panels that are manufacturable by precise isometric bending of surfaces made from a small number of molds. In fact we manage to solve the paneling task with surfaces of constant Gaussian curvature alone. This includes the case of developable surfaces which exhibit zero curvature. Our computations are based on an existing discrete model of isometric mappings between surfaces which for this occasion has been refined to obtain higher numerical accuracy. Further topics are interesting connections of the paneling problem with the geometry of Killing vector fields, designing and actuating isometries, curved folding in the double-curved case, and quad meshes with rigid faces that are nevertheless flexible.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459839},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-12},
  shortjournal = {ACM Trans. Graph.},
  title        = {Using isometries for computational design and fabrication},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bijective and coarse high-order tetrahedral meshes.
<em>TOG</em>, <em>40</em>(4), 1–16. (<a
href="https://doi.org/10.1145/3450626.3459840">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a robust and automatic algorithm to convert linear triangle meshes with feature annotated into coarse tetrahedral meshes with curved elements. Our construction guarantees that the high-order meshes are free of element inversion or self-intersection. A user-specified maximal geometrical error from the input mesh controls the faithfulness of the curved approximation. The boundary of the output mesh is in bijective correspondence to the input, enabling attribute transfer between them, such as boundary conditions for simulations, making our curved mesh an ideal replacement or complement for the original input geometry. The availability of a bijective shell around the input surface is employed to ensure robust curving, prevent self-intersections, and compute a bijective map between the linear input and curved output surface. As necessary building blocks of our algorithm, we extend the bijective shell formulation to support features and propose a robust approach for boundary-preserving linear tetrahedral meshing. We demonstrate the robustness and effectiveness of our algorithm by generating high-order meshes for a large collection of complex 3D models.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459840},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Bijective and coarse high-order tetrahedral meshes},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Optimizing UI layouts for deformable face-rig manipulation.
<em>TOG</em>, <em>40</em>(4), 1–12. (<a
href="https://doi.org/10.1145/3450626.3459842">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Complex deformable face-rigs have many independent parameters that control the shape of the object. A human face has upwards of 50 parameters (FACS Action Units), making conventional UI controls hard to find and operate. Animators address this problem by tediously hand-crafting in-situ layouts of UI controls that serve as visual deformation proxies, and facilitate rapid shape exploration. We propose the automatic creation of such in-situ UI control layouts. We distill the design choices made by animators into mathematical objectives that we optimize as the solution to an integer quadratic programming problem. Our evaluation is three-fold: we show the impact of our design principles on the resulting layouts; we show automated UI layouts for complex and diverse face rigs, comparable to animator handcrafted layouts; and we conduct a user study showing our UI layout to be an effective approach to face-rig manipulation, preferable to a baseline slider interface.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459842},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-12},
  shortjournal = {ACM Trans. Graph.},
  title        = {Optimizing UI layouts for deformable face-rig manipulation},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Modeling and fabrication with specified discrete equivalence
classes. <em>TOG</em>, <em>40</em>(4), 1–12. (<a
href="https://doi.org/10.1145/3450626.3459843">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel method to model and fabricate shapes using a small set of specified discrete equivalence classes of triangles. The core of our modeling technique is a fabrication-error-driven remeshing algorithm. Given a triangle and a template triangle, which are coplanar and have one-to-one corresponding vertices, we define their similarity error from a manufacturing point of view as follows: the minimizer of the maximum of the three distances between the corresponding pair of vertices concerning a rigid transformation. To compute the similarity error, we convert it into an easy-to-compute form. Then, a greedy remeshing method is developed to optimize the topology and geometry of the input mesh to minimize the fabrication error defined as the maximum similarity error of all triangles. Besides, constraints are enforced to ensure the similarity between input and output shapes and the smoothness of the resulting shapes. Since the fabrication error has been considered during the modeling process, the fabrication process is easy to proceed. To assist users in performing fabrication using common materials and tools manually, we present a straightforward manufacturing solution. The feasibility and practicability of our method are demonstrated over various examples, including seven physical manufacturing models with only nine template triangles.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459843},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-12},
  shortjournal = {ACM Trans. Graph.},
  title        = {Modeling and fabrication with specified discrete equivalence classes},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A gradient-based framework for 3D print appearance
optimization. <em>TOG</em>, <em>40</em>(4), 1–15. (<a
href="https://doi.org/10.1145/3450626.3459844">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In full-color inkjet 3D printing, a key problem is determining the material configuration for the millions of voxels that a printed object is made of. The goal is a configuration that minimises the difference between desired target appearance and the result of the printing process. So far, the techniques used to find such a configuration have relied on domain-specific methods or heuristic optimization, which allowed only a limited level of control over the resulting appearance. We propose to use differentiable volume rendering in a continuous material-mixture space, which leads to a framework that can be used as a general tool for optimising inkjet 3D printouts. We demonstrate the technical feasibility of this approach, and use it to attain fine control over the fabricated appearance, and high levels of faithfulness to the specified target.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459844},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-15},
  shortjournal = {ACM Trans. Graph.},
  title        = {A gradient-based framework for 3D print appearance optimization},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning meaningful controls for fluids. <em>TOG</em>,
<em>40</em>(4), 1–13. (<a
href="https://doi.org/10.1145/3450626.3459845">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While modern fluid simulation methods achieve high-quality simulation results, it is still a big challenge to interpret and control motion from visual quantities, such as the advected marker density. These visual quantities play an important role in user interactions: Being familiar and meaningful to humans, these quantities have a strong correlation with the underlying motion. We propose a novel data-driven conditional adversarial model that solves the challenging and theoretically ill-posed problem of deriving plausible velocity fields from a single frame of a density field. Besides density modifications, our generative model is the first to enable the control of the results using all of the following control modalities: obstacles, physical parameters, kinetic energy, and vorticity. Our method is based on a new conditional generative adversarial neural network that explicitly embeds physical quantities into the learned latent space, and a new cyclic adversarial network design for control disentanglement. We show the high quality and versatile controllability of our results for density-based inference, realistic obstacle interaction, and sensitive responses to modifications of physical parameters, kinetic energy, and vorticity. Code, models, and results can be found at https://github.com/RachelCmy/den2vel.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459845},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Learning meaningful controls for fluids},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Stream-guided smoke simulations. <em>TOG</em>,
<em>40</em>(4), 1–7. (<a
href="https://doi.org/10.1145/3450626.3459846">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-resolution fluid simulations are computationally expensive, so many post-processing methods have been proposed to add turbulent details to low-resolution flows. Guiding methods are one promising approach for adding naturalistic, detailed motions as a post-process, but can be inefficient. Thus, we propose a novel, efficient method that formulates fluid guidance as a minimization problem in stream function space. Input flows are first converted into stream functions, and a high resolution flow is then computed via optimization. The resulting problem sizes are much smaller than previous approaches, resulting in faster computation times. Additionally, our method does not require an expensive pressure projection, but still preserves mass. The method is both easy to implement and easy to control, as the user can control the degree of guiding with a single, intuitive parameter. We demonstrate the effectiveness of our method across various examples.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459846},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-7},
  shortjournal = {ACM Trans. Graph.},
  title        = {Stream-guided smoke simulations},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Foldover-free maps in 50 lines of code. <em>TOG</em>,
<em>40</em>(4), 1–16. (<a
href="https://doi.org/10.1145/3450626.3459847">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mapping a triangulated surface to 2D space (or a tetrahedral mesh to 3D space) is an important problem in geometry processing. In computational physics, untangling plays an important role in mesh generation: it takes a mesh as an input, and moves the vertices to get rid of foldovers. In fact, mesh untangling can be considered as a special case of mapping where the geometry of the object is to be defined in the map space and the geometric domain is not explicit, supposing that each element is regular. In this paper, we propose a mapping method inspired by the untangling problem and compare its performance to the state of the art. The main advantage of our method is that the untangling aims at producing locally injective maps, which is the major challenge of mapping. In practice, our method produces locally injective maps in very difficult settings, both in 2D and 3D. We demonstrate it on a large reference database as well as on more difficult stress tests. For a better reproducibility, we publish the code in Python for a basic evaluation, and in C++ for more advanced applications.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459847},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Foldover-free maps in 50 lines of code},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Neural scene graph rendering. <em>TOG</em>, <em>40</em>(4),
1–11. (<a href="https://doi.org/10.1145/3450626.3459848">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a neural scene graph---a modular and controllable representation of scenes with elements that are learned from data. We focus on the forward rendering problem, where the scene graph is provided by the user and references learned elements. The elements correspond to geometry and material definitions of scene objects and constitute the leaves of the graph; we store them as high-dimensional vectors. The position and appearance of scene objects can be adjusted in an artist-friendly manner via familiar transformations, e.g. translation, bending, or color hue shift, which are stored in the inner nodes of the graph. In order to apply a (non-linear) transformation to a learned vector, we adopt the concept of linearizing a problem by lifting it into higher dimensions: we first encode the transformation into a high-dimensional matrix and then apply it by standard matrix-vector multiplication. The transformations are encoded using neural networks. We render the scene graph using a streaming neural renderer, which can handle graphs with a varying number of objects, and thereby facilitates scalability. Our results demonstrate a precise control over the learned object representations in a number of animated 2D and 3D scenes. Despite the limited visual complexity, our work presents a step towards marrying traditional editing mechanisms with learned representations, and towards high-quality, controllable neural rendering.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459848},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-11},
  shortjournal = {ACM Trans. Graph.},
  title        = {Neural scene graph rendering},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Scalable image-based indoor scene rendering with
reflections. <em>TOG</em>, <em>40</em>(4), 1–14. (<a
href="https://doi.org/10.1145/3450626.3459849">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a novel scalable image-based rendering (IBR) pipeline for indoor scenes with reflections. We make substantial progress towards three sub-problems in IBR, namely, depth and reflection reconstruction, view selection for temporally coherent view-warping, and smooth rendering refinements. First, we introduce a global-mesh-guided alternating optimization algorithm that robustly extracts a two-layer geometric representation. The front and back layers encode the RGB-D reconstruction and the reflection reconstruction, respectively. This representation minimizes the image composition error under novel views, enabling accurate renderings of reflections. Second, we introduce a novel approach to select adjacent views and compute blending weights for smooth and temporal coherent renderings. The third contribution is a supersampling network with a motion vector rectification module that refines the rendering results to improve the final output&#39;s temporal coherence. These three contributions together lead to a novel system that produces highly realistic rendering results with various reflections. The rendering quality outperforms state-of-the-art IBR or neural rendering algorithms considerably.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459849},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Scalable image-based indoor scene rendering with reflections},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Driving-signal aware full-body avatars. <em>TOG</em>,
<em>40</em>(4), 1–17. (<a
href="https://doi.org/10.1145/3450626.3459850">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a learning-based method for building driving-signal aware full-body avatars. Our model is a conditional variational autoencoder that can be animated with incomplete driving signals, such as human pose and facial keypoints, and produces a high-quality representation of human geometry and view-dependent appearance. The core intuition behind our method is that better drivability and generalization can be achieved by disentangling the driving signals and remaining generative factors, which are not available during animation. To this end, we explicitly account for information deficiency in the driving signal by introducing a latent space that exclusively captures the remaining information, thus enabling the imputation of the missing factors required during full-body animation, while remaining faithful to the driving signal. We also propose a learnable localized compression for the driving signal which promotes better generalization, and helps minimize the influence of global chance-correlations often found in real datasets. For a given driving signal, the resulting variational model produces a compact space of uncertainty for missing factors that allows for an imputation strategy best suited to a particular application. We demonstrate the efficacy of our approach on the challenging problem of full-body animation for virtual telepresence with driving signals acquired from minimal sensors placed in the environment and mounted on a VR-headset.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459850},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-17},
  shortjournal = {ACM Trans. Graph.},
  title        = {Driving-signal aware full-body avatars},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multiscale cholesky preconditioning for ill-conditioned
problems. <em>TOG</em>, <em>40</em>(4), 1–13. (<a
href="https://doi.org/10.1145/3450626.3459851">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many computer graphics applications boil down to solving sparse systems of linear equations. While the current arsenal of numerical solvers available in various specialized libraries and for different computer architectures often allow efficient and scalable solutions to image processing, modeling and simulation applications, an increasing number of graphics problems face large-scale and ill-conditioned sparse linear systems --- a numerical challenge which typically chokes both direct factorizations (due to high memory requirements) and iterative solvers (because of slow convergence). We propose a novel approach to the efficient preconditioning of such problems which often emerge from the discretization over unstructured meshes of partial differential equations with heterogeneous and anisotropic coefficients. Our numerical approach consists in simply performing a fine-to-coarse ordering and a multiscale sparsity pattern of the degrees of freedom, using which we apply an incomplete Cholesky factorization. By further leveraging supernodes for cache coherence, graph coloring to improve parallelism and partial diagonal shifting to remedy negative pivots, we obtain a preconditioner which, combined with a conjugate gradient solver, far exceeds the performance of existing carefully-engineered libraries for graphics problems involving bad mesh elements and/or high contrast of coefficients. We also back the core concepts behind our simple solver with theoretical foundations linking the recent method of operator-adapted wavelets used in numerical homogenization to the traditional Cholesky factorization of a matrix, providing us with a clear bridge between incomplete Cholesky factorization and multiscale analysis that we leverage numerically.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459851},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Multiscale cholesky preconditioning for ill-conditioned problems},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning skeletal articulations with neural blend shapes.
<em>TOG</em>, <em>40</em>(4), 1–15. (<a
href="https://doi.org/10.1145/3450626.3459852">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Animating a newly designed character using motion capture (mocap) data is a long standing problem in computer animation. A key consideration is the skeletal structure that should correspond to the available mocap data, and the shape deformation in the joint regions, which often requires a tailored, pose-specific refinement. In this work, we develop a neural technique for articulating 3D characters using enveloping with a pre-defined skeletal structure which produces high quality pose dependent deformations. Our framework learns to rig and skin characters with the same articulation structure ( e.g. , bipeds or quadrupeds), and builds the desired skeleton hierarchy into the network architecture. Furthermore , we propose neural blend shapes - a set of corrective pose-dependent shapes which improve the deformation quality in the joint regions in order to address the notorious artifacts resulting from standard rigging and skinning. Our system estimates neural blend shapes for input meshes with arbitrary connectivity, as well as weighting coefficients which are conditioned on the input joint rotations. Unlike recent deep learning techniques which supervise the network with ground-truth rigging and skinning parameters, our approach does not assume that the training data has a specific underlying deformation model. Instead, during training, the network observes deformed shapes and learns to infer the corresponding rig, skin and blend shapes using indirect supervision. During inference, we demonstrate that our network generalizes to unseen characters with arbitrary mesh connectivity, including unrigged characters built by 3D artists. Conforming to standard skeletal animation models enables direct plug-and-play in standard animation software, as well as game engines.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459852},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Learning skeletal articulations with neural blend shapes},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A mathematical foundation for foundation paper pieceable
quilts. <em>TOG</em>, <em>40</em>(4), 1–14. (<a
href="https://doi.org/10.1145/3450626.3459853">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Foundation paper piecing is a popular technique for constructing fabric patchwork quilts using printed paper patterns. But, the construction process imposes constraints on the geometry of the pattern and the order in which the fabric pieces are attached to the quilt. Manually designing foundation paper pieceable patterns that meet all of these constraints is challenging. In this work we mathematically formalize the foundation paper piecing process and use this formalization to develop an algorithm that can automatically check if an input pattern geometry is foundation paper pieceable. Our key insight is that we can represent the geometric pattern design using a certain type of dual hypergraph where nodes represent faces and hyperedges represent seams connecting two or more nodes. We show that determining whether the pattern is paper pieceable is equivalent to checking whether this hypergraph is acyclic, and if it is acyclic, we can apply a leaf-plucking algorithm to the hypergraph to generate viable sewing orders for the pattern geometry. We implement this algorithm in a design tool that allows quilt designers to focus on producing the geometric design of their pattern and let the tool handle the tedious task of determining whether the pattern is foundation paper pieceable.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459853},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-14},
  shortjournal = {ACM Trans. Graph.},
  title        = {A mathematical foundation for foundation paper pieceable quilts},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Highlight-aware two-stream network for single-image SVBRDF
acquisition. <em>TOG</em>, <em>40</em>(4), 1–14. (<a
href="https://doi.org/10.1145/3450626.3459854">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the task of estimating spatially-varying reflectance (i.e., SVBRDF) from a single, casually captured image. Central to our method is a highlight-aware (HA) convolution operation and a two-stream neural network equipped with proper training losses. Our HA convolution, as a novel variant of standard (ST) convolution, directly modulates convolution kernels under the guidance of automatically learned masks representing potentially overexposed highlight regions. It helps to reduce the impact of strong specular highlights on diffuse components and at the same time, hallucinates plausible contents in saturated regions. Considering that variation of saturated pixels also contains important cues for inferring surface bumpiness and specular components, we design a two-stream network to extract features from two different branches stacked by HA convolutions and ST convolutions, respectively. These two groups of features are further fused in an attention-based manner to facilitate feature selection of each SVBRDF map. The whole network is trained end to end with a new perceptual adversarial loss which is particularly useful for enhancing the texture details. Such a design also allows the recovered material maps to be disentangled. We demonstrate through quantitative analysis and qualitative visualization that the proposed method is effective to recover clear SVBRDFs from a single casually captured image, and performs favorably against state-of-the-arts. Since we impose very few constraints on the capture process, even a non-expert user can create high-quality SVBRDFs that cater to many graphical applications.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459854},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Highlight-aware two-stream network for single-image SVBRDF acquisition},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An inverse method for the exploration of layered material
appearance. <em>TOG</em>, <em>40</em>(4), 1–15. (<a
href="https://doi.org/10.1145/3450626.3459857">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Layered materials exhibit a wide range of appearance, due to the combined effects of absorption and scattering at and between interfaces. Yet most existing approaches let users set the physical parameters of all layers by hand, a process of trial and error. We introduce an inverse method that provides control over BRDF lobe properties of layered materials, while automatically retrieving compatible physical parameters. Our method permits to explore the space of layered material appearance: it lets users find configurations with nearly indistinguishable appearance, isolate grazing angle effects, and give control over properties such as the color, blur or haze of reflections.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459857},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-15},
  shortjournal = {ACM Trans. Graph.},
  title        = {An inverse method for the exploration of layered material appearance},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Transfer matrix based layered materials rendering.
<em>TOG</em>, <em>40</em>(4), 1–16. (<a
href="https://doi.org/10.1145/3450626.3459859">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A statistical multi-lobe approach was recently introduced in order to efficiently handle layered materials rendering as an alternative to expensive general-purpose approaches. However, this approach poorly supports scattering volumes as the method does not account for back-scattering and resorts to single scattering approximations. In this paper, we address these limitations with an efficient solution based upon a transfer matrix approach which leverages the properties of the Henyey-Greenstein phase function. Under this formalism, each scattering component of the stack is described through a lightweight matrix, layering operations are reduced to simple matrix products and the statistics of each BSDF lobe accounting for multiple scattering effects are obtained through matrix operators. Based on this representation, we leverage the versatility of the transfer matrix approach to efficiently handle forward and backward scattering which occurs in arbitrary layered materials. The resulting model enables the reproduction of a wide range of layered structures embedding scattering volumes of arbitrary depth, in constant computation time and with low variance.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459859},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Transfer matrix based layered materials rendering},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). StyleCariGAN: Caricature generation via StyleGAN feature map
modulation. <em>TOG</em>, <em>40</em>(4), 1–16. (<a
href="https://doi.org/10.1145/3450626.3459860">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a caricature generation framework based on shape and style manipulation using StyleGAN. Our framework, dubbed StyleCariGAN , automatically creates a realistic and detailed caricature from an input photo with optional controls on shape exaggeration degree and color stylization type. The key component of our method is shape exaggeration blocks that are used for modulating coarse layer feature maps of StyleGAN to produce desirable caricature shape exaggerations. We first build a layer-mixed StyleGAN for photo-to-caricature style conversion by swapping fine layers of the StyleGAN for photos to the corresponding layers of the StyleGAN trained to generate caricatures. Given an input photo, the layer-mixed model produces detailed color stylization for a caricature but without shape exaggerations. We then append shape exaggeration blocks to the coarse layers of the layer-mixed model and train the blocks to create shape exaggerations while preserving the characteristic appearances of the input. Experimental results show that our StyleCariGAN generates realistic and detailed caricatures compared to the current state-of-the-art methods. We demonstrate StyleCariGAN also supports other StyleGAN-based image manipulations, such as facial expression control.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459860},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-16},
  shortjournal = {ACM Trans. Graph.},
  title        = {StyleCariGAN: Caricature generation via StyleGAN feature map modulation},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Solid-fluid interaction with surface-tension-dominant
contact. <em>TOG</em>, <em>40</em>(4), 1–12. (<a
href="https://doi.org/10.1145/3450626.3459862">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel three-way coupling method to model the contact interaction between solid and fluid driven by strong surface tension. At the heart of our physical model is a thin liquid membrane that simultaneously couples to both the liquid volume and the rigid objects, facilitating accurate momentum transfer, collision processing, and surface tension calculation. This model is implemented numerically under a hybrid Eulerian-Lagrangian framework where the membrane is modelled as a simplicial mesh and the liquid volume is simulated on a background Cartesian grid. We devise a monolithic solver to solve the interactions among the three systems of liquid, solid, and membrane. We demonstrate the efficacy of our method through an array of rigid-fluid contact simulations dominated by strong surface tension, which enables the faithful modeling of a host of new surface-tension-dominant phenomena including: objects with higher density than water that remains afloat; &#39;Cheerios effect&#39; where floating objects attract one another; and surface tension weakening effect caused by surface-active constituents.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459862},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-12},
  shortjournal = {ACM Trans. Graph.},
  title        = {Solid-fluid interaction with surface-tension-dominant contact},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Mixture of volumetric primitives for efficient neural
rendering. <em>TOG</em>, <em>40</em>(4), 1–13. (<a
href="https://doi.org/10.1145/3450626.3459863">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-time rendering and animation of humans is a core function in games, movies, and telepresence applications. Existing methods have a number of drawbacks we aim to address with our work. Triangle meshes have difficulty modeling thin structures like hair, volumetric representations like Neural Volumes are too low-resolution given a reasonable memory budget, and high-resolution implicit representations like Neural Radiance Fields are too slow for use in real-time applications. We present Mixture of Volumetric Primitives (MVP), a representation for rendering dynamic 3D content that combines the completeness of volumetric representations with the efficiency of primitive-based rendering, e.g., point-based or mesh-based methods. Our approach achieves this by leveraging spatially shared computation with a convolutional architecture and by minimizing computation in empty regions of space with volumetric primitives that can move to cover only occupied regions. Our parameterization supports the integration of correspondence and tracking constraints, while being robust to areas where classical tracking fails, such as around thin or translucent structures and areas with large topological variability. MVP is a hybrid that generalizes both volumetric and primitive-based representations. Through a series of extensive experiments we demonstrate that it inherits the strengths of each, while avoiding many of their limitations. We also compare our approach to several state-of-the-art methods and demonstrate that MVP produces superior results in terms of quality and runtime performance.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459863},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Mixture of volumetric primitives for efficient neural rendering},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Thin-film smoothed particle hydrodynamics fluid.
<em>TOG</em>, <em>40</em>(4), 1–16. (<a
href="https://doi.org/10.1145/3450626.3459864">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a particle-based method to simulate thin-film fluid that jointly facilitates aggressive surface deformation and vigorous tangential flows. We build our dynamics model from the surface tension driven Navier-Stokes equation with the dimensionality reduced using the asymptotic lubrication theory and customize a set of differential operators based on the weakly compressible Smoothed Particle Hydrodynamics (SPH) for evolving pointset surfaces. The key insight is that the compressible nature of SPH, which is unfavorable in its typical usage, is helpful in our application to co-evolve the thickness, calculate the surface tension, and enforce the fluid incompressibility on a thin film. In this way, we are able to two-way couple the surface deformation with the in-plane flows in a physically based manner. We can simulate complex vortical swirls, fingering effects due to Rayleigh-Taylor instability, capillary waves, Newton&#39;s interference fringes, and the Marangoni effect on liberally deforming surfaces by presenting both realistic visual results and numerical validations. The particle-based nature of our system also enables it to conveniently handle topology changes and codimension transitions, allowing us to marry the thin-film simulation with a wide gamut of 3D phenomena, such as pinch-off of unstable catenoids, dripping under gravity, merging of droplets, as well as bubble rupture.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459864},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Thin-film smoothed particle hydrodynamics fluid},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Incompressible flow simulation on vortex segment clouds.
<em>TOG</em>, <em>40</em>(4), 1–12. (<a
href="https://doi.org/10.1145/3450626.3459865">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel Lagrangian geometric representation using segment clouds to simulate incompressible fluid exhibiting strong anisotropic vortical features. The central component of our approach is a cloud of discrete segments enhanced by a set of local segment reseeding operations to facilitate both the geometrical evolution and the topological updates of vortical flow. We build a vortex dynamics solver with the support for dynamic solid boundaries based on discrete segment primitives. We demonstrate the efficacy of our approach by simulating a broad range of challenging flow phenomena, such as reconnection of non-closed vortex tubes and vortex shedding behind a rotating object.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459865},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-12},
  shortjournal = {ACM Trans. Graph.},
  title        = {Incompressible flow simulation on vortex segment clouds},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Clebsch gauge fluid. <em>TOG</em>, <em>40</em>(4), 1–11. (<a
href="https://doi.org/10.1145/3450626.3459866">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel gauge fluid solver based on Clebsch wave functions to solve incompressible fluid equations. Our method combines the expressive power of Clebsch wave functions to represent coherent vortical structures and the generality of gauge methods to accommodate a broad array of fluid phenomena. By evolving a transformed wave function as the system&#39;s gauge variable enhanced by an additional projection step to enforce pressure jumps on the free boundaries, our method can significantly improve the vorticity generation and preservation ability for a broad range of gaseous and liquid phenomena. Our approach can be easily implemented by modifying a standard grid-based fluid simulator. It can be used to solve various fluid dynamics, including complex vortex filament dynamics, fluids with different obstacles, and surface-tension flow.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459866},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-11},
  shortjournal = {ACM Trans. Graph.},
  title        = {Clebsch gauge fluid},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Designing actuation systems for animatronic figures via
globally optimal discrete search. <em>TOG</em>, <em>40</em>(4), 1–10.
(<a href="https://doi.org/10.1145/3450626.3459867">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present an algorithmic approach to designing animatronic figures - expressive robotic characters whose movements are driven by a large number of actuators. The input to our design system provides a high-level specification of the space of motions the character should be able to perform. The output consists of a fully functional mechatronic blueprint. We cast the design task as a search problem in a vast combinatorial space of possible solutions. To find an optimal design in this space, we propose an efficient best-first search algorithm that is guided by an admissible heuristic. The objectives guiding the search process demand that the design remains free of singularities and self-collisions at any point in the high-dimensional space of motions the character is expected to be able to execute. To identify worst-case self-collision scenarios for multi degree-of-freedom closed-loop mechanisms, we additionally develop an elegant technique inspired by the concept of adversarial attacks. We demonstrate the efficacy of our approach by creating designs for several animatronic figures of varying complexity.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459867},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-10},
  shortjournal = {ACM Trans. Graph.},
  title        = {Designing actuation systems for animatronic figures via globally optimal discrete search},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Knitting 4D garments with elasticity controlled for body
motion. <em>TOG</em>, <em>40</em>(4), 1–16. (<a
href="https://doi.org/10.1145/3450626.3459868">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present a new computational pipeline for designing and fabricating 4D garments as knitwear that considers comfort during body movement. This is achieved by careful control of elasticity distribution to reduce uncomfortable pressure and unwanted sliding caused by body motion. We exploit the ability to knit patterns in different elastic levels by single-jersey jacquard (SJJ) with two yarns. We design the distribution of elasticity for a garment by physics-based computation, the optimized elasticity on the garment is then converted into instructions for a digital knitting machine by two algorithms proposed in this paper. Specifically, a graph-based algorithm is proposed to generate knittable stitch meshes that can accurately capture the 3D shape of a garment, and a tiling algorithm is employed to assign SJJ patterns on the stitch mesh to realize the designed distribution of elasticity. The effectiveness of our approach is verified on simulation results and on specimens physically fabricated by knitting machines.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459868},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Knitting 4D garments with elasticity controlled for body motion},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Boundary-sampled halfspaces: A new representation for
constructive solid modeling. <em>TOG</em>, <em>40</em>(4), 1–15. (<a
href="https://doi.org/10.1145/3450626.3459870">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel representation of solid models for shape design. Like Constructive Solid Geometry (CSG), the solid shape is constructed from a set of halfspaces without the need for an explicit boundary structure. Instead of using Boolean expressions as in CSG, the shape is defined by sparsely placed samples on the boundary of each halfspace. This representation, called Boundary-Sampled Halfspaces (BSH), affords greater agility and expressiveness than CSG while simplifying the reverse engineering process. We discuss theoretical properties of the representation and present practical algorithms for boundary extraction and conversion from other representations. Our algorithms are demonstrated on both 2D and 3D examples.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459870},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Boundary-sampled halfspaces: A new representation for constructive solid modeling},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Consistent depth of moving objects in video. <em>TOG</em>,
<em>40</em>(4), 1–12. (<a
href="https://doi.org/10.1145/3450626.3459871">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a method to estimate depth of a dynamic scene, containing arbitrary moving objects, from an ordinary video captured with a moving camera. We seek a geometrically and temporally consistent solution to this under-constrained problem: the depth predictions of corresponding points across frames should induce plausible, smooth motion in 3D. We formulate this objective in a new test-time training framework where a depth-prediction CNN is trained in tandem with an auxiliary scene-flow prediction MLP over the entire input video. By recursively unrolling the scene-flow prediction MLP over varying time steps, we compute both short-range scene flow to impose local smooth motion priors directly in 3D, and long-range scene flow to impose multi-view consistency constraints with wide baselines. We demonstrate accurate and temporally coherent results on a variety of challenging videos containing diverse moving objects (pets, people, cars), as well as camera motion. Our depth maps give rise to a number of depth-and-motion aware video editing effects such as object and lighting insertion.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459871},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-12},
  shortjournal = {ACM Trans. Graph.},
  title        = {Consistent depth of moving objects in video},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Total relighting: Learning to relight portraits for
background replacement. <em>TOG</em>, <em>40</em>(4), 1–21. (<a
href="https://doi.org/10.1145/3450626.3459872">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel system for portrait relighting and background replacement, which maintains high-frequency boundary details and accurately synthesizes the subject&#39;s appearance as lit by novel illumination, thereby producing realistic composite images for any desired scene. Our technique includes foreground estimation via alpha matting, relighting, and compositing. We demonstrate that each of these stages can be tackled in a sequential pipeline without the use of priors (e.g. known background or known illumination) and with no specialized acquisition techniques, using only a single RGB portrait image and a novel, target HDR lighting environment as inputs. We train our model using relit portraits of subjects captured in a light stage computational illumination system, which records multiple lighting conditions, high quality geometry, and accurate alpha mattes. To perform realistic relighting for compositing, we introduce a novel per-pixel lighting representation in a deep learning framework, which explicitly models the diffuse and the specular components of appearance, producing relit portraits with convincingly rendered non-Lambertian effects like specular highlights. Multiple experiments and comparisons show the effectiveness of the proposed approach when applied to in-the-wild images.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459872},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-21},
  shortjournal = {ACM Trans. Graph.},
  title        = {Total relighting: Learning to relight portraits for background replacement},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Unsupervised learning for cuboid shape abstraction via joint
segmentation from point clouds. <em>TOG</em>, <em>40</em>(4), 1–11. (<a
href="https://doi.org/10.1145/3450626.3459873">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Representing complex 3D objects as simple geometric primitives, known as shape abstraction, is important for geometric modeling, structural analysis, and shape synthesis. In this paper, we propose an unsupervised shape abstraction method to map a point cloud into a compact cuboid representation. We jointly predict cuboid allocation as part segmentation and cuboid shapes and enforce the consistency between the segmentation and shape abstraction for self-learning. For the cuboid abstraction task, we transform the input point cloud into a set of parametric cuboids using a variational auto-encoder network. The segmentation network allocates each point into a cuboid considering the point-cuboid affinity. Without manual annotations of parts in point clouds, we design four novel losses to jointly supervise the two branches in terms of geometric similarity and cuboid compactness. We evaluate our method on multiple shape collections and demonstrate its superiority over existing shape abstraction methods. Moreover, based on our network architecture and learned representations, our approach supports various applications including structured shape generation, shape interpolation, and structural shape clustering.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459873},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-11},
  shortjournal = {ACM Trans. Graph.},
  title        = {Unsupervised learning for cuboid shape abstraction via joint segmentation from point clouds},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A momentum-conserving implicit material point method for
surface tension with contact angles and spatial gradients. <em>TOG</em>,
<em>40</em>(4), 1–16. (<a
href="https://doi.org/10.1145/3450626.3459874">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel Material Point Method (MPM) discretization of surface tension forces that arise from spatially varying surface energies. These variations typically arise from surface energy dependence on temperature and/or concentration. Furthermore, since the surface energy is an interfacial property depending on the types of materials on either side of an interface, spatial variation is required for modeling the contact angle at the triple junction between a liquid, solid and surrounding air. Our discretization is based on the surface energy itself, rather than on the associated traction condition most commonly used for discretization with particle methods. Our energy based approach automatically captures surface gradients without the explicit need to resolve them as in traction condition based approaches. We include an implicit discretization of thermomechanical material coupling with a novel particle-based enforcement of Robin boundary conditions associated with convective heating. Lastly, we design a particle resampling approach needed to achieve perfect conservation of linear and angular momentum with Affine-Particle-In-Cell (APIC) [Jiang et al. 2015]. We show that our approach enables implicit time stepping for complex behaviors like the Marangoni effect and hydrophobicity/hydrophilicity. We demonstrate the robustness and utility of our method by simulating materials that exhibit highly diverse degrees of surface tension and thermomechanical effects, such as water, wine and wax.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459874},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-16},
  shortjournal = {ACM Trans. Graph.},
  title        = {A momentum-conserving implicit material point method for surface tension with contact angles and spatial gradients},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning contact corrections for handle-based subspace
dynamics. <em>TOG</em>, <em>40</em>(4), 1–12. (<a
href="https://doi.org/10.1145/3450626.3459875">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a novel subspace method for the simulation of dynamic deformations. The method augments existing linear handle-based subspace formulations with nonlinear learning-based corrections parameterized by the same subspace. Together, they produce a compact nonlinear model that combines the fast dynamics and overall contact-based interaction of subspace methods, with the highly detailed deformations of learning-based methods. We propose a formulation of the model with nonlinear corrections applied on the local undeformed setting, and decoupling internal and external contact-driven corrections. We define a simple mapping of these corrections to the global setting, an efficient implementation for dynamic simulation, and a training pipeline to generate examples that efficiently cover the interaction space. Altogether, the method achieves unprecedented combination of speed and contact-driven deformation detail.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459875},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-12},
  shortjournal = {ACM Trans. Graph.},
  title        = {Learning contact corrections for handle-based subspace dynamics},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Weakly-supervised contrastive learning in path manifold for
monte carlo image reconstruction. <em>TOG</em>, <em>40</em>(4), 1–14.
(<a href="https://doi.org/10.1145/3450626.3459876">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image-space auxiliary features such as surface normal have significantly contributed to the recent success of Monte Carlo (MC) reconstruction networks. However, path-space features, another essential piece of light propagation, have not yet been sufficiently explored. Due to the curse of dimensionality, information flow between a regression loss and high-dimensional path-space features is sparse, leading to difficult training and inefficient usage of path-space features in a typical reconstruction framework. This paper introduces a contrastive manifold learning framework to utilize path-space features effectively. The proposed framework employs weakly-supervised learning that converts reference pixel colors to dense pseudo labels for light paths. A convolutional path-embedding network then induces a low-dimensional manifold of paths by iteratively clustering intra-class embeddings, while discriminating inter-class embeddings using gradient descent. The proposed framework facilitates path-space exploration of reconstruction networks by extracting low-dimensional yet meaningful embeddings within the features. We apply our framework to the recent image- and sample-space models and demonstrate considerable improvements, especially on the sample space. The source code is available at https://github.com/Mephisto405/WCMC.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459876},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Weakly-supervised contrastive learning in path manifold for monte carlo image reconstruction},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Constrained projective dynamics: Real-time simulation of
deformable objects with energy-momentum conservation. <em>TOG</em>,
<em>40</em>(4), 1–12. (<a
href="https://doi.org/10.1145/3450626.3459878">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a novel energy-momentum conserving integration method. Adopting Projective Dynamics, the proposed method extends its unconstrained minimization for time integration into the constrained form with the position-based energy-momentum constraints. This resolves the well-known problem of unwanted dissipation of energy and momenta without compromising the real-time performance and simulation stability. The proposed method also enables users to directly control the energy and momenta so as to easily create the vivid deformable and global motions they want, which is a fascinating feature for many real-time applications such as virtual/augmented reality and games.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459878},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-12},
  shortjournal = {ACM Trans. Graph.},
  title        = {Constrained projective dynamics: Real-time simulation of deformable objects with energy-momentum conservation},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A time-independent deformer for elastic contacts.
<em>TOG</em>, <em>40</em>(4), 1–14. (<a
href="https://doi.org/10.1145/3450626.3459879">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a purely geometric, time-independent deformer resolving local contacts between elastic objects, including self-collisions between adjacent parts of the same object that often occur in character skinning animation. Starting from multiple meshes in intersection, our deformer first computes the parts of the surfaces remaining in contact, and then applies a procedural displacement with volume preservation. Although our deformer processes each frame independently, it achieves temporally continuous deformations with artistic control of the bulge through few pseudo-stiffness parameters. The plausibility of the deformation is further enhanced by anisotropically spreading the volume-preserving bulge. The result is a robust, real-time deformer that can handle complex geometric configurations such as a ball squashed by a hand, colliding lips, bending fingers, etc.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459879},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-14},
  shortjournal = {ACM Trans. Graph.},
  title        = {A time-independent deformer for elastic contacts},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Optimizing dyadic nets. <em>TOG</em>, <em>40</em>(4), 1–17.
(<a href="https://doi.org/10.1145/3450626.3459880">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We explore the space of (0, m , 2)-nets in base 2 commonly used for sampling. We present a novel constructive algorithm that can exhaustively generate all nets --- up to m -bit resolution --- and thereby compute the exact number of distinct nets. We observe that the construction algorithm holds the key to defining a transformation operation that lets us transform one valid net into another one. This enables the optimization of digital nets using arbitrary objective functions. For example, we define an analytic energy function for blue noise, and use it to generate nets with high-quality blue-noise frequency power spectra. We also show that the space of (0, 2)-sequences is significantly smaller than nets with the same number of points, which drastically limits the optimizability of sequences.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459880},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-17},
  shortjournal = {ACM Trans. Graph.},
  title        = {Optimizing dyadic nets},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Neural animation layering for synthesizing martial arts
movements. <em>TOG</em>, <em>40</em>(4), 1–16. (<a
href="https://doi.org/10.1145/3450626.3459881">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interactively synthesizing novel combinations and variations of character movements from different motion skills is a key problem in computer animation. In this paper, we propose a deep learning framework to produce a large variety of martial arts movements in a controllable manner from raw motion capture data. Our method imitates animation layering using neural networks with the aim to overcome typical challenges when mixing, blending and editing movements from unaligned motion sources. The framework can synthesize novel movements from given reference motions and simple user controls, and generate unseen sequences of locomotion, punching, kicking, avoiding and combinations thereof, but also reconstruct signature motions of different fighters, as well as close-character interactions such as clinching and carrying by learning the spatial joint relationships. To achieve this goal, we adopt a modular framework which is composed of the motion generator and a set of different control modules. The motion generator functions as a motion manifold that projects novel mixed/edited trajectories to natural full-body motions, and synthesizes realistic transitions between different motions. The control modules are task dependent and can be developed and trained separately by engineers to include novel motion tasks, which greatly reduces network iteration time when working with large-scale datasets. Our modular framework provides a transparent control interface for animators that allows modifying or combining movements after network training, and enables iterative adding of control modules for different motion tasks and behaviors. Our system can be used for offline and online motion generation alike, and is relevant for real-time applications such as computer games.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459881},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Neural animation layering for synthesizing martial arts movements},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Eliminating topological errors in neural network rotation
estimation using self-selecting ensembles. <em>TOG</em>, <em>40</em>(4),
1–21. (<a href="https://doi.org/10.1145/3450626.3459882">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many problems in computer graphics and computer vision applications involves inferring a rotation from a variety of different forms of inputs. With the increasing use of deep learning, neural networks have been employed to solve such problems. However, the traditional representations for 3D rotations, the quaternions and Euler angles, are found to be problematic for neural networks in practice, producing seemingly unavoidable large estimation errors. Previous researches has identified the discontinuity of the mapping from SO(3) to the quaternions or Euler angles as the source of such errors, and to solve it, embeddings of SO(3) have been proposed as the output representation of rotation estimation networks instead. In this paper, we argue that the argument against quaternions and Euler angles from local discontinuities of the mappings from SO(3) is flawed, and instead provide a different argument from the global topological properties of SO(3) that also establishes the lower bound of maximum error when using quaternions and Euler angles for rotation estimation networks. Extending from this view, we discover that rotation symmetries in the input object causes additional topological problems that even using embeddings of SO(3) as the output representation would not correctly handle. We propose the self-selecting ensemble, a topologically motivated approach, where the network makes multiple predictions and assigns weights to them. We show theoretically and with experiments that our methods can be combined with a wide range of different rotation representations and can handle all kinds of finite symmetries in 3D rotation estimation problems.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459882},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-21},
  shortjournal = {ACM Trans. Graph.},
  title        = {Eliminating topological errors in neural network rotation estimation using self-selecting ensembles},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning active quasistatic physics-based models from data.
<em>TOG</em>, <em>40</em>(4), 1–14. (<a
href="https://doi.org/10.1145/3450626.3459883">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Humans and animals can control their bodies to generate a wide range of motions via low-dimensional action signals representing high-level goals. As such, human bodies and faces are prime examples of active objects, which can affect their shape via an internal actuation mechanism. This paper explores the following proposition: given a training set of example poses of an active deformable object, can we learn a low-dimensional control space that could reproduce the training set and generalize to new poses? In contrast to popular machine learning methods for dimensionality reduction such as auto-encoders, we model our active objects in a physics-based way. We utilize a differentiable, quasistatic, physics-based simulation layer and combine it with a decoder-type neural network. Our differentiable physics layer naturally fits into deep learning frameworks and allows the decoder network to learn actuations that reach the desired poses after physics-based simulation. In contrast to modeling approaches where users build anatomical models from first principles, medical literature or medical imaging, we do not presume knowledge of the underlying musculature, but learn the structure and control of the actuation mechanism directly from the input data. We present a training paradigm and several scalability-oriented enhancements that allow us to train effectively while accommodating high-resolution volumetric models, with as many as a quarter million simulation elements. The prime demonstration of the efficacy of our example-driven modeling framework targets facial animation, where we train on a collection of input expressions while generalizing to unseen poses, drive detailed facial animation from sparse motion capture input, and facilitate expression sculpting via direct manipulation.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459883},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Learning active quasistatic physics-based models from data},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). TryOnGAN: Body-aware try-on via layered interpolation.
<em>TOG</em>, <em>40</em>(4), 1–10. (<a
href="https://doi.org/10.1145/3450626.3459884">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given a pair of images---target person and garment on another person---we automatically generate the target person in the given garment. Previous methods mostly focused on texture transfer via paired data training, while overlooking body shape deformations, skin color, and seamless blending of garment with the person. This work focuses on those three components, while also not requiring paired data training. We designed a pose conditioned StyleGAN2 architecture with a clothing segmentation branch that is trained on images of people wearing garments. Once trained, we propose a new layered latent space interpolation method that allows us to preserve and synthesize skin color and target body shape while transferring the garment from a different person. We demonstrate results on high resolution 512 × 512 images, and extensively compare to state of the art in try-on on both latent space generated and real images.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459884},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-10},
  shortjournal = {ACM Trans. Graph.},
  title        = {TryOnGAN: Body-aware try-on via layered interpolation},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Physical validation of simulators in computer graphics: A
new framework dedicated to slender elastic structures and frictional
contact. <em>TOG</em>, <em>40</em>(4), 1–19. (<a
href="https://doi.org/10.1145/3450626.3459931">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a selected set of protocols inspired from the Soft Matter Physics community in order to validate Computer Graphics simulators of slender elastic structures possibly subject to dry frictional contact. Although these simulators were primarily intended for feature film animation and visual effects, they are more and more used as virtual design tools for predicting the shape and deformation of real objects; hence the need for a careful, quantitative validation. Our tests, experimentally verified, are designed to evaluate carefully the predictability of these simulators on various aspects, such as bending elasticity, bend-twist coupling, and frictional contact. We have passed a number of popular codes of Computer Graphics through our benchmarks by defining a rigorous, consistent, and as fair as possible methodology. Our results show that while some popular simulators for plates/shells and frictional contact fail even on the simplest scenarios, more recent ones, as well as well-known codes for rods, generally perform well and sometimes even better than some reference commercial tools of Mechanical Engineering. To make our validation protocols easily applicable to any simulator, we provide an extensive description of our methodology, and we shall distribute all the necessary model data to be compared against.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459931},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-19},
  shortjournal = {ACM Trans. Graph.},
  title        = {Physical validation of simulators in computer graphics: A new framework dedicated to slender elastic structures and frictional contact},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). ChoreoMaster: Choreography-oriented music-driven dance
synthesis. <em>TOG</em>, <em>40</em>(4), 1–13. (<a
href="https://doi.org/10.1145/3450626.3459932">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite strong demand in the game and film industry, automatically synthesizing high-quality dance motions remains a challenging task. In this paper, we present ChoreoMaster, a production-ready music-driven dance motion synthesis system. Given a piece of music, ChoreoMaster can automatically generate a high-quality dance motion sequence to accompany the input music in terms of style, rhythm and structure. To achieve this goal, we introduce a novel choreography-oriented choreomusical embedding framework, which successfully constructs a unified choreomusical embedding space for both style and rhythm relationships between music and dance phrases. The learned choreomusical embedding is then incorporated into a novel choreography-oriented graph-based motion synthesis framework, which can robustly and efficiently generate high-quality dance motions following various choreographic rules. Moreover, as a production-ready system, ChoreoMaster is sufficiently controllable and comprehensive for users to produce desired results. Experimental results demonstrate that dance motions generated by ChoreoMaster are accepted by professional artists.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459932},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-13},
  shortjournal = {ACM Trans. Graph.},
  title        = {ChoreoMaster: Choreography-oriented music-driven dance synthesis},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Unconventional patterns on surfaces. <em>TOG</em>,
<em>40</em>(4), 1–16. (<a
href="https://doi.org/10.1145/3450626.3459933">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a unified method to meshing surfaces with unconventional patterns, both periodic and aperiodic. These patterns, which have so far been studied on the plane, are patterns comprising a small number of tiles, that do not necessarily exhibit translational periodicity. Our method generalizes the de Bruijn multigrid method to the discrete setting, and thus reduces the problem to the computation of N -Directional fields on triangle meshes. We work with all cases of directional symmetries that have been little studied, including odd and high N. We address the properties of such patterns on surfaces and the challenges in their construction, including order-preservation, seamlessness, duality, and singularities. We show how our method allows for the design of original and unconventional meshes that can be applied to architectural, industrial, and recreational design.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459933},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Unconventional patterns on surfaces},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Endless loops: Detecting and animating periodic patterns in
still images. <em>TOG</em>, <em>40</em>(4), 1–12. (<a
href="https://doi.org/10.1145/3450626.3459935">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present an algorithm for producing a seamless animated loop from a single image. The algorithm detects periodic structures, such as the windows of a building or the steps of a staircase, and generates a non-trivial displacement vector field that maps each segment of the structure onto a neighboring segment along a user- or auto-selected main direction of motion. This displacement field is used, together with suitable temporal and spatial smoothing, to warp the image and produce the frames of a continuous animation loop. Our cinemagraphs are created in under a second on a mobile device. Over 140,000 users downloaded our app and exported over 350,000 cinemagraphs. Moreover, we conducted two user studies that show that users prefer our method for creating surreal and structured cinemagraphs compared to more manual approaches and compared to previous methods.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459935},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-12},
  shortjournal = {ACM Trans. Graph.},
  title        = {Endless loops: Detecting and animating periodic patterns in still images},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning an animatable detailed 3D face model from
in-the-wild images. <em>TOG</em>, <em>40</em>(4), 1–13. (<a
href="https://doi.org/10.1145/3450626.3459936">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While current monocular 3D face reconstruction methods can recover fine geometric details, they suffer several limitations. Some methods produce faces that cannot be realistically animated because they do not model how wrinkles vary with expression. Other methods are trained on high-quality face scans and do not generalize well to in-the-wild images. We present the first approach that regresses 3D face shape and animatable details that are specific to an individual but change with expression. Our model, DECA (Detailed Expression Capture and Animation), is trained to robustly produce a UV displacement map from a low-dimensional latent representation that consists of person-specific detail parameters and generic expression parameters, while a regressor is trained to predict detail, shape, albedo, expression, pose and illumination parameters from a single image. To enable this, we introduce a novel detail-consistency loss that disentangles person-specific details from expression-dependent wrinkles. This disentanglement allows us to synthesize realistic person-specific wrinkles by controlling expression parameters while keeping person-specific details unchanged. DECA is learned from in-the-wild images with no paired 3D supervision and achieves state-of-the-art shape reconstruction accuracy on two benchmarks. Qualitative results on in-the-wild data demonstrate DECA&#39;s robustness and its ability to disentangle identity- and expression-dependent details enabling animation of reconstructed faces. The model and code are publicly available at https://deca.is.tue.mpg.de.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459936},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Learning an animatable detailed 3D face model from in-the-wild images},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An unbiased ray-marching transmittance estimator.
<em>TOG</em>, <em>40</em>(4), 1–20. (<a
href="https://doi.org/10.1145/3450626.3459937">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present an in-depth analysis of the sources of variance in state-of-the-art unbiased volumetric transmittance estimators, and propose several new methods for improving their efficiency. These combine to produce a single estimator that is universally optimal relative to prior work, with up to several orders of magnitude lower variance at the same cost, and has zero variance for any ray with non-varying extinction. We first reduce the variance of truncated power-series estimators using a novel efficient application of U-statistics. We then greatly reduce the average expansion order of the power series and redistribute density evaluations to filter the optical depth estimates with an equidistant sampling comb. Combined with the use of an online control variate built from a sampled mean density estimate, the resulting estimator effectively performs ray marching most of the time while using rarely-sampled higher-order terms to correct the bias.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459937},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-20},
  shortjournal = {ACM Trans. Graph.},
  title        = {An unbiased ray-marching transmittance estimator},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Computational design of weingarten surfaces. <em>TOG</em>,
<em>40</em>(4), 1–11. (<a
href="https://doi.org/10.1145/3450626.3459939">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we study Weingarten surfaces and explore their potential for fabrication-aware design in freeform architecture. Weingarten surfaces are characterized by a functional relation between their principal curvatures that implicitly defines approximate local congruences on the surface. These symmetries can be exploited to simplify surface paneling of double-curved architectural skins through mold re-use. We present an optimization approach to find a Weingarten surface that is close to a given input design. Leveraging insights from differential geometry, our method aligns curvature isolines of the surface in order to contract the curvature diagram from a 2D region into a 1D curve. The unknown functional curvature relation then emerges as the result of the optimization. We show how a robust and efficient numerical shape approximation method can be implemented using a guided projection approach on a high-order B-spline representation. This algorithm is applied in several design studies to illustrate how Weingarten surfaces define a versatile shape space for fabrication-aware exploration in freeform architecture. Our optimization algorithm provides the first practical tool to compute general Weingarten surfaces with arbitrary curvature relation, thus enabling new investigations into a rich, but as of yet largely unexplored class of surfaces.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459939},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-11},
  shortjournal = {ACM Trans. Graph.},
  title        = {Computational design of weingarten surfaces},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bistable auxetic surface structures. <em>TOG</em>,
<em>40</em>(4), 1–9. (<a
href="https://doi.org/10.1145/3450626.3459940">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present Bistable Auxetic Surface Structures , a novel deployable material system based on optimized bistable auxetic cells. Such a structure can be flat-fabricated from elastic sheet material, then deployed towards a desired double-curved target shape by activating the bistable mechanism of its component cells. A unique feature is that the deployed model is by design in a stable state. This facilitates deployment without the need of complex external supports or boundary constraints. We introduce a computational solution for the inverse design of our Bistable Auxetic Surface Structures. Our algorithm first precomputes a library of bistable auxetic cells to cover a range of in-plane expansion / contraction ratios, while maximizing the bistability and stiffness of the cell to ensure robust deployment. We then use metric distortion analysis of the target surface to compute the planar fabrication state as a composition of cells that best matches the desired deployment deformation. As each cell expands or contracts during deployment, metric frustration forces the surface towards its target equilibrium state. We validate our method with several physical prototypes.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459940},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-9},
  shortjournal = {ACM Trans. Graph.},
  title        = {Bistable auxetic surface structures},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Reliable feature-line driven quad-remeshing. <em>TOG</em>,
<em>40</em>(4), 1–17. (<a
href="https://doi.org/10.1145/3450626.3459941">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a new algorithm for the semi-regular quadrangulation of an input surface, driven by its line features, such as sharp creases. We define a perfectly feature-aligned cross-field and a coarse layout of polygonal-shaped patches where we strictly ensure that all the feature-lines are represented as patch boundaries. To be able to consistently do so, we allow non-quadrilateral patches and T-junctions in the layout; the key is the ability to constrain the layout so that it still admits a globally consistent, T-junction-free, and pure-quad internal tessellation of its patches. This requires the insertion of additional irregular-vertices inside patches, but the regularity of the final-mesh is safeguarded by optimizing for both their number and for their reciprocal alignment. In total, our method guarantees the reproduction of feature-lines by construction, while still producing good quality, isometric, pure-quad, conforming meshes, making it an ideal candidate for CAD models. Moreover, the method is fully automatic, requiring no user intervention, and remarkably reliable, requiring little assumptions on the input mesh, as we demonstrate by batch processing the entire Thingi10K repository, with less than 0.5% of the attempted cases failing to produce a usable mesh.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459941},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-17},
  shortjournal = {ACM Trans. Graph.},
  title        = {Reliable feature-line driven quad-remeshing},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). WRAPD: Weighted rotation-aware ADMM for parameterization and
deformation. <em>TOG</em>, <em>40</em>(4), 1–14. (<a
href="https://doi.org/10.1145/3450626.3459942">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Local-global solvers such as ADMM for elastic simulation and geometry optimization struggle to resolve large rotations such as bending and twisting modes, and large distortions in the presence of barrier energies. We propose two improvements to address these challenges. First, we introduce a novel local-global splitting based on the polar decomposition that separates the geometric nonlinearity of rotations from the material nonlinearity of the deformation energy. The resulting ADMM-based algorithm is a combination of an L-BFGS solve in the global step and proximal updates of element stretches in the local step. We also introduce a novel method for dynamic reweighting that is used to adjust element weights at runtime for improved convergence. With both improved rotation handling and element weighting, our algorithm is considerably faster than state-of-the-art approaches for quasi-static simulations. It is also much faster at making early progress in parameterization problems, making it valuable as an initializer to jump-start second-order algorithms.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459942},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-14},
  shortjournal = {ACM Trans. Graph.},
  title        = {WRAPD: Weighted rotation-aware ADMM for parameterization and deformation},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Beyond blur: Real-time ventral metamers for foveated
rendering. <em>TOG</em>, <em>40</em>(4), 1–14. (<a
href="https://doi.org/10.1145/3450626.3459943">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To peripheral vision, a pair of physically different images can look the same. Such pairs are metamers relative to each other, just as physically-different spectra of light are perceived as the same color. We propose a real-time method to compute such ventral metamers for foveated rendering where, in particular for near-eye displays, the largest part of the framebuffer maps to the periphery. This improves in quality over state-of-the-art foveation methods which blur the periphery. Work in Vision Science has established how peripheral stimuli are ventral metamers if their statistics are similar. Existing methods, however, require a costly optimization process to find such metamers. To this end, we propose a novel type of statistics particularly well-suited for practical real-time rendering: smooth moments of steerable filter responses. These can be extracted from images in time constant in the number of pixels and in parallel over all pixels using a GPU. Further, we show that they can be compressed effectively and transmitted at low bandwidth. Finally, computing realizations of those statistics can again be performed in constant time and in parallel. This enables a new level of quality for foveated applications such as such as remote rendering, level-of-detail and Monte-Carlo denoising. In a user study, we finally show how human task performance increases and foveation artifacts are less suspicious, when using our method compared to common blurring.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459943},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Beyond blur: Real-time ventral metamers for foveated rendering},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Authoring consistent landscapes with flora and fauna.
<em>TOG</em>, <em>40</em>(4), 1–13. (<a
href="https://doi.org/10.1145/3450626.3459952">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel method for authoring landscapes with flora and fauna while considering their mutual interactions. Our algorithm outputs a steady-state ecosystem in the form of density maps for each species, their daily circuits, and a modified terrain with eroded trails from a terrain, climatic conditions, and species with related biological information. We introduce the Resource Access Graph, a new data structure that encodes both interactions between food chain levels and animals traveling between resources over the terrain. A novel competition algorithm operating on this data progressively computes a steady-state solution up the food chain, from plants to carnivores. The user can explore the resulting landscape, where plants and animals are instantiated on the fly, and interactively edit it by over-painting the maps. Our results show that our system enables the authoring of consistent landscapes where the impact of wildlife is visible through animated animals, clearings in the vegetation, and eroded trails. We provide quantitative validation with existing ecosystems and a user-study with expert paleontologist end-users, showing that our system enables them to author and compare different ecosystems illustrating climate changes over the same terrain while enabling relevant visual immersion into consistent landscapes.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459952},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Authoring consistent landscapes with flora and fauna},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fire in paradise: Mesoscale simulation of wildfires.
<em>TOG</em>, <em>40</em>(4), 1–15. (<a
href="https://doi.org/10.1145/3450626.3459954">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Resulting from changing climatic conditions, wildfires have become an existential threat across various countries around the world. The complex dynamics paired with their often rapid progression renders wildfires an often disastrous natural phenomenon that is difficult to predict and to counteract. In this paper we present a novel method for simulating wildfires with the goal to realistically capture the combustion process of individual trees and the resulting propagation of fires at the scale of forests. We rely on a state-of-the-art modeling approach for large-scale ecosystems that enables us to represent each plant as a detailed 3D geometric model. We introduce a novel mathematical formulation for the combustion process of plants - also considering effects such as heat transfer, char insulation, and mass loss - as well as for the propagation of fire through the entire ecosystem. Compared to other wildfire simulations which employ geometric representations of plants such as cones or cylinders, our detailed 3D tree models enable us to simulate the interplay of geometric variations of branching structures and the dynamics of fire and wood combustion. Our simulation runs at interactive rates and thereby provides a convenient way to explore different conditions that affect wildfires, ranging from terrain elevation profiles and ecosystem compositions to various measures against wildfires, such as cutting down trees as firebreaks, the application of fire retardant, or the simulation of rain.},
  archive      = {J_TOG},
  doi          = {10.1145/3450626.3459954},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Fire in paradise: Mesoscale simulation of wildfires},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Real-time global illumination decomposition of videos.
<em>TOG</em>, <em>40</em>(3), 1–16. (<a
href="https://doi.org/10.1145/3374753">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose the first approach for the decomposition of a monocular color video into direct and indirect illumination components in real time. We retrieve, in separate layers, the contribution made to the scene appearance by the scene reflectance, the light sources, and the reflections from various coherent scene regions to one another. Existing techniques that invert global light transport require image capture under multiplexed controlled lighting or only enable the decomposition of a single image at slow off-line frame rates. In contrast, our approach works for regular videos and produces temporally coherent decomposition layers at real-time frame rates. At the core of our approach are several sparsity priors that enable the estimation of the per-pixel direct and indirect illumination layers based on a small set of jointly estimated base reflectance colors. The resulting variational decomposition problem uses a new formulation based on sparse and dense sets of non-linear equations that we solve efficiently using a novel alternating data-parallel optimization strategy. We evaluate our approach qualitatively and quantitatively and show improvements over the state-of-the-art in this field, in both quality and runtime. In addition, we demonstrate various real-time appearance editing applications for videos with consistent illumination.},
  archive      = {J_TOG},
  doi          = {10.1145/3374753},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {3},
  pages        = {1-16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Real-time global illumination decomposition of videos},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Iterative text-based editing of talking-heads using neural
retargeting. <em>TOG</em>, <em>40</em>(3), 1–14. (<a
href="https://doi.org/10.1145/3449063">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a text-based tool for editing talking-head video that enables an iterative editing workflow. On each iteration users can edit the wording of the speech, further refine mouth motions if necessary to reduce artifacts, and manipulate non-verbal aspects of the performance by inserting mouth gestures (e.g., a smile) or changing the overall performance style (e.g., energetic, mumble). Our tool requires only 2 to 3 minutes of the target actor video and it synthesizes the video for each iteration in about 40 seconds, allowing users to quickly explore many editing possibilities as they iterate. Our approach is based on two key ideas. (1) We develop a fast phoneme search algorithm that can quickly identify phoneme-level subsequences of the source repository video that best match a desired edit. This enables our fast iteration loop. (2) We leverage a large repository of video of a source actor and develop a new self-supervised neural retargeting technique for transferring the mouth motions of the source actor to the target actor. This allows us to work with relatively short target actor videos, making our approach applicable in many real-world editing scenarios. Finally, our, refinement and performance controls give users the ability to further fine-tune the synthesized results.},
  archive      = {J_TOG},
  doi          = {10.1145/3449063},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {3},
  pages        = {1-14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Iterative text-based editing of talking-heads using neural retargeting},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SketchGNN: Semantic sketch segmentation with graph neural
networks. <em>TOG</em>, <em>40</em>(3), 1–13. (<a
href="https://doi.org/10.1145/3450284">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce SketchGNN , a convolutional graph neural network for semantic segmentation and labeling of freehand vector sketches. We treat an input stroke-based sketch as a graph with nodes representing the sampled points along input strokes and edges encoding the stroke structure information. To predict the per-node labels, our SketchGNN uses graph convolution and a static-dynamic branching network architecture to extract the features at three levels, i.e., point-level, stroke-level, and sketch-level. SketchGNN significantly improves the accuracy of the state-of-the-art methods for semantic sketch segmentation (by 11.2% in the pixel-based metric and 18.2% in the component-based metric over a large-scale challenging SPG dataset) and has magnitudes fewer parameters than both image-based and sequence-based methods.},
  archive      = {J_TOG},
  doi          = {10.1145/3450284},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {3},
  pages        = {1-13},
  shortjournal = {ACM Trans. Graph.},
  title        = {SketchGNN: Semantic sketch segmentation with graph neural networks},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Supervoxel convolution for online 3D semantic segmentation.
<em>TOG</em>, <em>40</em>(3), 1–15. (<a
href="https://doi.org/10.1145/3453485">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online 3D semantic segmentation, which aims to perform real-time 3D scene reconstruction along with semantic segmentation, is an important but challenging topic. A key challenge is to strike a balance between efficiency and segmentation accuracy. There are very few deep-learning-based solutions to this problem, since the commonly used deep representations based on volumetric-grids or points do not provide efficient 3D representation and organization structure for online segmentation. Observing that on-surface supervoxels, i.e., clusters of on-surface voxels, provide a compact representation of 3D surfaces and brings efficient connectivity structure via supervoxel clustering, we explore a supervoxel-based deep learning solution for this task. To this end, we contribute a novel convolution operation (SVConv) directly on supervoxels. SVConv can efficiently fuse the multi-view 2D features and 3D features projected on supervoxels during the online 3D reconstruction, and leads to an effective supervoxel-based convolutional neural network, termed as Supervoxel-CNN , enabling 2D-3D joint learning for 3D semantic prediction. With the Supervoxel-CNN , we propose a clustering-then-prediction online 3D semantic segmentation approach. The extensive evaluations on the public 3D indoor scene datasets show that our approach significantly outperforms the existing online semantic segmentation systems in terms of efficiency or accuracy.},
  archive      = {J_TOG},
  doi          = {10.1145/3453485},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {3},
  pages        = {1-15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Supervoxel convolution for online 3D semantic segmentation},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Imaging with local speckle intensity correlations: Theory
and practice. <em>TOG</em>, <em>40</em>(3), 1–22. (<a
href="https://doi.org/10.1145/3447392">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in computational imaging have significantly expanded our ability to image through scattering layers such as biological tissues by exploiting the auto-correlation properties of captured speckle intensity patterns. However, most experimental demonstrations of this capability focus on the far-field imaging setting, where obscured light sources are very far from the scattering layer. By contrast, medical imaging applications such as fluorescent imaging operate in the near-field imaging setting, where sources are inside the scattering layer. We provide a theoretical and experimental study of the similarities and differences between the two settings, highlighting the increased challenges posed by the near-field setting. We then draw insights from this analysis to develop a new algorithm for imaging through scattering that is tailored to the near-field setting by taking advantage of unique properties of speckle patterns formed under this setting, such as their local support. We present a theoretical analysis of the advantages of our algorithm and perform real experiments in both far-field and near-field configurations, showing an order-of magnitude expansion in both the range and the density of the obscured patterns that can be recovered.},
  archive      = {J_TOG},
  doi          = {10.1145/3447392},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {3},
  pages        = {1-22},
  shortjournal = {ACM Trans. Graph.},
  title        = {Imaging with local speckle intensity correlations: Theory and practice},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Primary-space adaptive control variates using
piecewise-polynomial approximations. <em>TOG</em>, <em>40</em>(3), 1–15.
(<a href="https://doi.org/10.1145/3450627">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present an unbiased numerical integration algorithm that handles both low-frequency regions and high-frequency details of multidimensional integrals. It combines quadrature and Monte Carlo integration by using a quadrature-based approximation as a control variate of the signal. We adaptively build the control variate constructed as a piecewise polynomial, which can be analytically integrated, and accurately reconstructs the low-frequency regions of the integrand. We then recover the high-frequency details missed by the control variate by using Monte Carlo integration of the residual. Our work leverages importance sampling techniques by working in primary space, allowing the combination of multiple mappings; this enables multiple importance sampling in quadrature-based integration. Our algorithm is generic and can be applied to any complex multidimensional integral. We demonstrate its effectiveness with four applications with low dimensionality: transmittance estimation in heterogeneous participating media, low-order scattering in homogeneous media, direct illumination computation, and rendering of distribution effects. Finally, we show how our technique is extensible to integrands of higher dimensionality by computing the control variate on Monte Carlo estimates of the high-dimensional signal, and accounting for such additional dimensionality on the residual as well. In all cases, we show accurate results and faster convergence compared to previous approaches.},
  archive      = {J_TOG},
  doi          = {10.1145/3450627},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {3},
  pages        = {1-15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Primary-space adaptive control variates using piecewise-polynomial approximations},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning multimodal affinities for textual editing in
images. <em>TOG</em>, <em>40</em>(3), 1–16. (<a
href="https://doi.org/10.1145/3451340">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, as cameras are rapidly adopted in our daily routine, images of documents are becoming both abundant and prevalent. Unlike natural images that capture physical objects, document-images contain a significant amount of text with critical semantics and complicated layouts. In this work, we devise a generic unsupervised technique to learn multimodal affinities between textual entities in a document-image, considering their visual style, the content of their underlying text, and their geometric context within the image. We then use these learned affinities to automatically cluster the textual entities in the image into different semantic groups. The core of our approach is a deep optimization scheme dedicated for an image provided by the user that detects and leverages reliable pairwise connections in the multimodal representation of the textual elements to properly learn the affinities. We show that our technique can operate on highly varying images spanning a wide range of documents and demonstrate its applicability for various editing operations manipulating the content, appearance, and geometry of the image.},
  archive      = {J_TOG},
  doi          = {10.1145/3451340},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {3},
  pages        = {1-16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Learning multimodal affinities for textual editing in images},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Single depth view based real-time reconstruction of
hand-object interactions. <em>TOG</em>, <em>40</em>(3), 1–12. (<a
href="https://doi.org/10.1145/3451341">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reconstructing hand-object interactions is a challenging task due to strong occlusions and complex motions. This article proposes a real-time system that uses a single depth stream to simultaneously reconstruct hand poses, object shape, and rigid/non-rigid motions. To achieve this, we first train a joint learning network to segment the hand and object in a depth image, and to predict the 3D keypoints of the hand. With most layers shared by the two tasks, computation cost is saved for the real-time performance. A hybrid dataset is constructed here to train the network with real data (to learn real-world distributions) and synthetic data (to cover variations of objects, motions, and viewpoints). Next, the depth of the two targets and the keypoints are used in a uniform optimization to reconstruct the interacting motions. Benefitting from a novel tangential contact constraint, the system not only solves the remaining ambiguities but also keeps the real-time performance. Experiments show that our system handles different hand and object shapes, various interactive motions, and moving cameras.},
  archive      = {J_TOG},
  doi          = {10.1145/3451341},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {3},
  pages        = {1-12},
  shortjournal = {ACM Trans. Graph.},
  title        = {Single depth view based real-time reconstruction of hand-object interactions},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Vectorization for fast, analytic, and differentiable
visibility. <em>TOG</em>, <em>40</em>(3), 1–21. (<a
href="https://doi.org/10.1145/3452097">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Computer Graphics, the two main approaches to rendering and visibility involve ray tracing and rasterization. However, a limitation of both approaches is that they essentially use point sampling. This is the source of noise and aliasing, and also leads to significant difficulties for differentiable rendering. In this work, we present a new rendering method, which we call vectorization, that computes 2D point-to-region integrals analytically, thus eliminating point sampling in the 2D integration domain such as for pixel footprints and area lights. Our vectorization revisits the concept of beam tracing, and handles the hidden surface removal problem robustly and accurately. That is, for each intersecting triangle inserted into the viewport of a beam in an arbitrary order, we are able to maintain all the visible regions formed by intersections and occlusions, thanks to our Visibility Bounding Volume Hierarchy structure. As a result, our vectorization produces perfectly anti-aliased visibility, accurate and analytic shading and shadows, and most important, fast and noise-free gradients with Automatic Differentiation or Finite Differences that directly enables differentiable rendering without any changes to our rendering pipeline. Our results are inherently high-quality and noise-free, and our gradients are one to two orders of magnitude faster than those computed with existing differentiable rendering methods.},
  archive      = {J_TOG},
  doi          = {10.1145/3452097},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {3},
  pages        = {1-21},
  shortjournal = {ACM Trans. Graph.},
  title        = {Vectorization for fast, analytic, and differentiable visibility},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Self-similar fractal drawings inspired by m. C. Escher’s
print square limit. <em>TOG</em>, <em>40</em>(3), 1–34. (<a
href="https://doi.org/10.1145/3456298">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A fractal tiling ( f -tiling) is a kind of rarely explored tiling by similar polygonal tiles which possesses self-similarity and the boundary of which is a fractal. Based on a tiling by similar isosceles right triangles, Dutch graphic artist M. C. Escher created an ingenious print Square Limit in which fish are uniformly reduced in size as they approach the boundaries of the tiling. In this article, we present four families of f -tilings and propose an easy-to-implement method to achieve similar Escher-like drawings. By systematically investigating the local star-shaped structure of f -tilings, we first enumerate four families of f -tilings admitted by kite-shaped or dart-shaped prototiles. Then, we establish a fast binning algorithm for visualising f -tilings. To facilitate the creation of Escher-like drawings on the reported f -tilings, we next introduce one-to-one mappings between the square, and kite and dart, respectively. This treatment allows a pre-designed square template to be deformed into all prototiles considered in the article. Finally, we specify some technical implementations and present a gallery of the resulting Escher-like drawings. The method established in this article is thus able to generate a great variety of exotic Escher-like drawings.},
  archive      = {J_TOG},
  doi          = {10.1145/3456298},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {3},
  pages        = {1-34},
  shortjournal = {ACM Trans. Graph.},
  title        = {Self-similar fractal drawings inspired by m. c. escher’s print square limit},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Mid-air drawing of curves on 3D surfaces in virtual reality.
<em>TOG</em>, <em>40</em>(3), 1–17. (<a
href="https://doi.org/10.1145/3459090">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Complex 3D curves can be created by directly drawing mid-air in immersive environments (Augmented and Virtual Realities). Drawing mid-air strokes precisely on the surface of a 3D virtual object, however, is difficult, necessitating a projection of the mid-air stroke onto the user “intended” surface curve. We present the first detailed investigation of the fundamental problem of 3D stroke projection in VR. An assessment of the design requirements of real-time drawing of curves on 3D objects in VR is followed by the definition and classification of multiple techniques for 3D stroke projection. We analyze the advantages and shortcomings of these approaches both theoretically and via practical pilot testing. We then formally evaluate the two most promising techniques spraycan and mimicry with 20 users in VR. The study shows a strong qualitative and quantitative user preference for our novel stroke mimicry projection algorithm. We further illustrate the effectiveness and utility of stroke mimicry to draw complex 3D curves on surfaces for various artistic and functional design applications.},
  archive      = {J_TOG},
  doi          = {10.1145/3459090},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {3},
  pages        = {1-17},
  shortjournal = {ACM Trans. Graph.},
  title        = {Mid-air drawing of curves on 3D surfaces in virtual reality},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Unsupervised shape completion via deep prior in the neural
tangent kernel perspective. <em>TOG</em>, <em>40</em>(3), 1–17. (<a
href="https://doi.org/10.1145/3459234">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel approach for completing and reconstructing 3D shapes from incomplete scanned data by using deep neural networks. Rather than being trained on supervised completion tasks and applied on a testing shape, the network is optimized from scratch on the single testing shape to fully adapt to the shape and complete the missing data using contextual guidance from the known regions. The ability to complete missing data by an untrained neural network is usually referred to as the deep prior . In this article, we interpret the deep prior from a neural tangent kernel (NTK) perspective and show that the completed shape patches by the trained CNN are naturally similar to existing patches, as they are proximate in the kernel feature space induced by NTK. The interpretation allows us to design more efficient network structures and learning mechanisms for the shape completion and reconstruction task. Being more aware of structural regularities than both traditional and other unsupervised learning-based reconstruction methods, our approach completes large missing regions with plausible shapes and complements supervised learning-based methods that use database priors by requiring no extra training dataset and showing flexible adaptation to a particular shape instance.},
  archive      = {J_TOG},
  doi          = {10.1145/3459234},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {3},
  pages        = {1-17},
  shortjournal = {ACM Trans. Graph.},
  title        = {Unsupervised shape completion via deep prior in the neural tangent kernel perspective},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dirty pixels: Towards end-to-end image processing and
perception. <em>TOG</em>, <em>40</em>(3), 1–15. (<a
href="https://doi.org/10.1145/3446918">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-world, imaging systems acquire measurements that are degraded by noise, optical aberrations, and other imperfections that make image processing for human viewing and higher-level perception tasks challenging. Conventional cameras address this problem by compartmentalizing imaging from high-level task processing. As such, conventional imaging involves processing the RAW sensor measurements in a sequential pipeline of steps, such as demosaicking, denoising, deblurring, tone-mapping, and compression. This pipeline is optimized to obtain a visually pleasing image. High-level processing, however, involves steps such as feature extraction, classification, tracking, and fusion. While this silo-ed design approach allows for efficient development, it also dictates compartmentalized performance metrics without knowledge of the higher-level task of the camera system. For example, today’s demosaicking and denoising algorithms are designed using perceptual image quality metrics but not with domain-specific tasks such as object detection in mind. We propose an end-to-end differentiable architecture that jointly performs demosaicking, denoising, deblurring, tone-mapping, and classification (see Figure 1). The architecture does not require any intermediate losses based on perceived image quality and learns processing pipelines whose outputs differ from those of existing ISPs optimized for perceptual quality, preserving fine detail at the cost of increased noise and artifacts. We show that state-of-the-art ISPs discard information that is essential in corner cases, such as extremely low-light conditions, where conventional imaging and perception stacks fail. We demonstrate on captured and simulated data that our model substantially improves perception in low light and other challenging conditions, which is imperative for real-world applications such as autonomous driving, robotics, and surveillance. Finally, we found that the proposed model also achieves state-of-the-art accuracy when optimized for image reconstruction in low-light conditions, validating the architecture itself as a potentially useful drop-in network for reconstruction and analysis tasks beyond the applications demonstrated in this work. Our proposed models, datasets, and calibration data are available at https://github.com/princeton-computational-imaging/DirtyPixels .},
  archive      = {J_TOG},
  doi          = {10.1145/3446918},
  journal      = {ACM Transactions on Graphics},
  month        = {5},
  number       = {3},
  pages        = {1-15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Dirty pixels: Towards end-to-end image processing and perception},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). StyleFlow: Attribute-conditioned exploration of
StyleGAN-generated images using conditional continuous normalizing
flows. <em>TOG</em>, <em>40</em>(3), 1–21. (<a
href="https://doi.org/10.1145/3447648">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-quality, diverse, and photorealistic images can now be generated by unconditional GANs (e.g., StyleGAN). However, limited options exist to control the generation process using (semantic) attributes while still preserving the quality of the output. Further, due to the entangled nature of the GAN latent space, performing edits along one attribute can easily result in unwanted changes along other attributes. In this article, in the context of conditional exploration of entangled latent spaces, we investigate the two sub-problems of attribute-conditioned sampling and attribute-controlled editing. We present StyleFlow as a simple, effective, and robust solution to both the sub-problems by formulating conditional exploration as an instance of conditional continuous normalizing flows in the GAN latent space conditioned by attribute features. We evaluate our method using the face and the car latent space of StyleGAN, and demonstrate fine-grained disentangled edits along various attributes on both real photographs and StyleGAN generated images. For example, for faces, we vary camera pose, illumination variation, expression, facial hair, gender, and age. Finally, via extensive qualitative and quantitative comparisons, we demonstrate the superiority of StyleFlow over prior and several concurrent works. Project Page and Video: https://rameenabdal.github.io/StyleFlow .},
  archive      = {J_TOG},
  doi          = {10.1145/3447648},
  journal      = {ACM Transactions on Graphics},
  month        = {5},
  number       = {3},
  pages        = {1-21},
  shortjournal = {ACM Trans. Graph.},
  title        = {StyleFlow: Attribute-conditioned exploration of StyleGAN-generated images using conditional continuous normalizing flows},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Vid2Player: Controllable video sprites that behave and
appear like professional tennis players. <em>TOG</em>, <em>40</em>(3),
1–16. (<a href="https://doi.org/10.1145/3448978">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a system that converts annotated broadcast video of tennis matches into interactively controllable video sprites that behave and appear like professional tennis players. Our approach is based on controllable video textures and utilizes domain knowledge of the cyclic structure of tennis rallies to place clip transitions and accept control inputs at key decision-making moments of point play. Most importantly, we use points from the video collection to model a player’s court positioning and shot selection decisions during points. We use these behavioral models to select video clips that reflect actions the real-life player is likely to take in a given match-play situation, yielding sprites that behave realistically at the macro level of full points, not just individual tennis motions. Our system can generate novel points between professional tennis players that resemble Wimbledon broadcasts, enabling new experiences, such as the creation of matchups between players that have not competed in real life or interactive control of players in the Wimbledon final. According to expert tennis players, the rallies generated using our approach are significantly more realistic in terms of player behavior than video sprite methods that only consider the quality of motion transitions during video synthesis. The supplementary material/video are available at our https://cs.stanford.edu/~haotianz/research/vid2player/ project website.},
  archive      = {J_TOG},
  doi          = {10.1145/3448978},
  journal      = {ACM Transactions on Graphics},
  month        = {4},
  number       = {3},
  pages        = {1-16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Vid2Player: Controllable video sprites that behave and appear like professional tennis players},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Modeling of personalized anatomy using plastic strains.
<em>TOG</em>, <em>40</em>(2), 1–21. (<a
href="https://doi.org/10.1145/3443703">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a method for modeling solid objects undergoing large spatially varying and/or anisotropic strains, and use it to reconstruct human anatomy from medical images. Our novel shape deformation method uses plastic strains and the finite element method to successfully model shapes undergoing large and/or anisotropic strains, specified by sparse point constraints on the boundary of the object. We extensively compare our method to standard second-order shape deformation methods, variational methods, and surface-based methods, and demonstrate that our method avoids the spikiness, wiggliness, and other artifacts of previous methods. We demonstrate how to perform such shape deformation both for attached and un-attached (“free flying”) objects, using a novel method to solve linear systems with singular matrices with a known nullspace. Although our method is applicable to general large-strain shape deformation modeling, we use it to create personalized 3D triangle and volumetric meshes of human organs, based on magnetic resonance imaging or computed tomography scans. Given a medically accurate anatomy template of a generic individual, we optimize the geometry of the organ to match the magnetic resonance imaging or computed tomography scan of a specific individual. Our examples include human hand muscles, a liver, a hip bone, and a gluteus medius muscle (“hip abductor”).},
  archive      = {J_TOG},
  doi          = {10.1145/3443703},
  journal      = {ACM Transactions on Graphics},
  month        = {6},
  number       = {2},
  pages        = {1-21},
  shortjournal = {ACM Trans. Graph.},
  title        = {Modeling of personalized anatomy using plastic strains},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Differentiable compound optics and processing pipeline
optimization for end-to-end camera design. <em>TOG</em>, <em>40</em>(2),
1–19. (<a href="https://doi.org/10.1145/3446791">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most modern commodity imaging systems we use directly for photography—or indirectly rely on for downstream applications—employ optical systems of multiple lenses that must balance deviations from perfect optics, manufacturing constraints, tolerances, cost, and footprint. Although optical designs often have complex interactions with downstream image processing or analysis tasks, today’s compound optics are designed in isolation from these interactions. Existing optical design tools aim to minimize optical aberrations, such as deviations from Gauss’ linear model of optics, instead of application-specific losses, precluding joint optimization with hardware image signal processing (ISP) and highly parameterized neural network processing. In this article, we propose an optimization method for compound optics that lifts these limitations. We optimize entire lens systems jointly with hardware and software image processing pipelines, downstream neural network processing, and application-specific end-to-end losses. To this end, we propose a learned, differentiable forward model for compound optics and an alternating proximal optimization method that handles function compositions with highly varying parameter dimensions for optics, hardware ISP, and neural nets. Our method integrates seamlessly atop existing optical design tools, such as Zemax . We can thus assess our method across many camera system designs and end-to-end applications. We validate our approach in an automotive camera optics setting—together with hardware ISP post processing and detection—outperforming classical optics designs for automotive object detection and traffic light state detection. For human viewing tasks, we optimize optics and processing pipelines for dynamic outdoor scenarios and dynamic low-light imaging. We outperform existing compartmentalized design or fine-tuning methods qualitatively and quantitatively, across all domain-specific applications tested.},
  archive      = {J_TOG},
  doi          = {10.1145/3446791},
  journal      = {ACM Transactions on Graphics},
  month        = {6},
  number       = {2},
  pages        = {1-19},
  shortjournal = {ACM Trans. Graph.},
  title        = {Differentiable compound optics and processing pipeline optimization for end-to-end camera design},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Repulsive curves. <em>TOG</em>, <em>40</em>(2), 1–21. (<a
href="https://doi.org/10.1145/3439429">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Curves play a fundamental role across computer graphics, physical simulation, and mathematical visualization, yet most tools for curve design do nothing to prevent crossings or self-intersections. This article develops efficient algorithms for (self-)repulsion of plane and space curves that are well-suited to problems in computational design. Our starting point is the so-called tangent-point energy , which provides an infinite barrier to self-intersection. In contrast to local collision detection strategies used in, e.g., physical simulation, this energy considers interactions between all pairs of points, and is hence useful for global shape optimization: local minima tend to be aesthetically pleasing, physically valid, and nicely distributed in space. A reformulation of gradient descent based on a Sobolev-Slobodeckij inner product enables us to make rapid progress toward local minima—independent of curve resolution. We also develop a hierarchical multigrid scheme that significantly reduces the per-step cost of optimization. The energy is easily integrated with a variety of constraints and penalties (e.g., inextensibility, or obstacle avoidance), which we use for applications including curve packing, knot untangling, graph embedding, non-crossing spline interpolation, flow visualization, and robotic path planning.},
  archive      = {J_TOG},
  doi          = {10.1145/3439429},
  journal      = {ACM Transactions on Graphics},
  month        = {5},
  number       = {2},
  pages        = {1-21},
  shortjournal = {ACM Trans. Graph.},
  title        = {Repulsive curves},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A re-examination of dichoptic tone mapping. <em>TOG</em>,
<em>40</em>(2), 1–15. (<a
href="https://doi.org/10.1145/3443702">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dichoptic tone mapping methods aim to leverage stereoscopic displays to increase visual detail and contrast in images and videos. These methods, which have been called both binocular tone mapping and dichoptic contrast enhancement , selectively emphasize contrast differently in the two eyes’ views. The visual system integrates these contrast differences into a unified percept, which is theorized to contain more contrast overall than each eye’s view on its own. As stereoscopic displays become increasingly common for augmented and virtual reality (AR/VR), dichoptic tone mapping is an appealing technique for imaging pipelines. We sought to examine whether a standard photographic technique, exposure bracketing, could be modified to enhance contrast similarly to dichoptic tone mapping. While assessing the efficacy of this technique with user studies, we also re-evaluated existing dichoptic tone mapping methods. Across several user studies; however, we did not find evidence that either dichoptic tone mapping or dichoptic exposures consistently increased subjective image preferences. We also did not observe improvements in subjective or objective measures of detail visibility. We did find evidence that dichoptic methods enhanced subjective 3D impressions. Here, we present these results and evaluate the potential contributions and current limitations of dichoptic methods for applications in stereoscopic displays.},
  archive      = {J_TOG},
  doi          = {10.1145/3443702},
  journal      = {ACM Transactions on Graphics},
  month        = {4},
  number       = {2},
  pages        = {1-15},
  shortjournal = {ACM Trans. Graph.},
  title        = {A re-examination of dichoptic tone mapping},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Locking-proof tetrahedra. <em>TOG</em>, <em>40</em>(2),
1–17. (<a href="https://doi.org/10.1145/3444949">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The simulation of incompressible materials suffers from locking when using the standard finite element method (FEM) and coarse linear tetrahedral meshes. Locking increases as the Poisson ratio ν gets close to 0.5 and often lower Poisson ratio values are used to reduce locking, affecting volume preservation. We propose a novel mixed FEM approach to simulating incompressible solids that alleviates the locking problem for tetrahedra. Our method uses linear shape functions for both displacements and pressure, and adds one scalar per node. It can accommodate nonlinear isotropic materials described by a Young’s modulus and any Poisson ratio value by enforcing a volumetric constitutive law. The most realistic such material is Neo-Hookean, and we focus on adapting it to our method. For ν = 0.5, we can obtain full volume preservation up to any desired numerical accuracy. We show that standard Neo-Hookean simulations using tetrahedra are often locking, which, in turn, affects accuracy. We show that our method gives better results and that our Newton solver is more robust. As an alternative, we propose a dual ascent solver that is simple and has a good convergence rate. We validate these results using numerical experiments and quantitative analysis.},
  archive      = {J_TOG},
  doi          = {10.1145/3444949},
  journal      = {ACM Transactions on Graphics},
  month        = {4},
  number       = {2},
  pages        = {1-17},
  shortjournal = {ACM Trans. Graph.},
  title        = {Locking-proof tetrahedra},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Frictional contact on smooth elastic solids. <em>TOG</em>,
<em>40</em>(2), 1–17. (<a
href="https://doi.org/10.1145/3446663">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Frictional contact between deformable elastic objects remains a difficult simulation problem in computer graphics. Traditionally, contact has been resolved using sophisticated collision detection schemes and methods that build on the assumption that contact happens between polygons. While polygonal surfaces are an efficient representation for solids, they lack some intrinsic properties that are important for contact resolution. Generally, polygonal surfaces are not equipped with an intrinsic inside and outside partitioning or a smooth distance field close to the surface. Here we propose a new method for resolving frictional contacts against deforming implicit surface representations that addresses these problems. We augment a moving least squares (MLS) implicit surface formulation with a local kernel for resolving contacts, and develop a simple parallel transport approximation to enable transfer of frictional impulses. Our variational formulation of dynamics and elasticity enables us to naturally include contact constraints, which are resolved as one Newton-Raphson solve with linear inequality constraints. We extend this formulation by forwarding friction impulses from one time step to the next, used as external forces in the elasticity solve. This maintains the decoupling of friction from elasticity thus allowing for different solvers to be used in each step. In addition, we develop a variation of staggered projections, that relies solely on a non-linear optimization without constraints and does not require a discretization of the friction cone. Our results compare favorably to a popular industrial elasticity solver (used for visual effects), as well as recent academic work in frictional contact, both of which rely on polygons for contact resolution. We present examples of coupling between rigid bodies, cloth and elastic solids.},
  archive      = {J_TOG},
  doi          = {10.1145/3446663},
  journal      = {ACM Transactions on Graphics},
  month        = {4},
  number       = {2},
  pages        = {1-17},
  shortjournal = {ACM Trans. Graph.},
  title        = {Frictional contact on smooth elastic solids},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Temporally adaptive shading reuse for real-time rendering
and virtual reality. <em>TOG</em>, <em>40</em>(2), 1–14. (<a
href="https://doi.org/10.1145/3446790">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal coherence has the potential to enable a huge reduction of shading costs in rendering. Existing techniques focus either only on spatial shading reuse or cannot adaptively choose temporal shading frequencies. We find that temporal shading reuse is possible for extended periods of time for a majority of samples, and we show under which circumstances users perceive temporal artifacts. Our analysis implies that we can approximate shading gradients to efficiently determine when and how long shading can be reused. Whereas visibility usually stays temporally coherent from frame to frame for more than 90%, we find that even in heavily animated game scenes with advanced shading, typically more than 50% of shading is also temporally coherent. To exploit this potential, we introduce a temporally adaptive shading framework and apply it to two real-time methods. Its application saves more than 57% of the shader invocations, reducing overall rendering times up to in virtual reality applications without a noticeable loss in visual quality. Overall, our work shows that there is significantly more potential for shading reuse than currently exploited.},
  archive      = {J_TOG},
  doi          = {10.1145/3446790},
  journal      = {ACM Transactions on Graphics},
  month        = {4},
  number       = {2},
  pages        = {1-14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Temporally adaptive shading reuse for real-time rendering and virtual reality},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). PanoMan: Sparse localized components–based model for full
human motions. <em>TOG</em>, <em>40</em>(2), 1–17. (<a
href="https://doi.org/10.1145/3447244">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Parameterizing Variations of human shapes and motions is a long-standing problem in computer graphics and vision. Most of the existing methods only deal with a specific kind of motion, such as body poses, facial expressions, or hand gestures. We propose PanoMan (sParse locAlized compoNents based mOdel for full huMAn motioNs) to handle shape variation and full-motion across body, face, and hand in a unified framework. Like previous approaches, we factor shape variation into principal components to obtain a human shape space that approximates the shape of arbitrary identity. We then analyze sparse localized components in terms of relative edge length and dihedral angle to capture full motions of body poses, facial expressions, and hand gestures. The final piece of our model is a multilayer perceptron (MLP) that fits the residual between the ground truth and the aforementioned two-level approximation. As an application, we employ the discrete-shell deformation to drive the model to fit sparse constraints such as joint positions and surface feature points. We thoroughly evaluate PanoMan on body, face, and hand motion benchmarks as well as scanned data. The existing skinning-based techniques suffer from joint collapsing when encountering twisting motion of joints. Experiments show that PanoMan can capture all kinds of full human motions with high quality and is easier than the state-of-the-art models in recovering poses with wide joint twisting and complex hand gestures.},
  archive      = {J_TOG},
  doi          = {10.1145/3447244},
  journal      = {ACM Transactions on Graphics},
  month        = {4},
  number       = {2},
  pages        = {1-17},
  shortjournal = {ACM Trans. Graph.},
  title        = {PanoMan: Sparse localized components–based model for full human motions},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Selective region-based photo color adjustment for graphic
designs. <em>TOG</em>, <em>40</em>(2), 1–16. (<a
href="https://doi.org/10.1145/3447647">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When adding a photo onto a graphic design, professional graphic designers often adjust its colors based on some target colors obtained from the brand or product to make the entire design more memorable to audiences and establish a consistent brand identity. However, adjusting the colors of a photo in the context of a graphic design is a difficult task, with two major challenges: (1) Locality: The color is often adjusted locally to preserve the semantics and atmosphere of the original image; and (2) Naturalness: The modified region needs to be carefully chosen and recolored to obtain a semantically valid and visually natural result. To address these challenges, we propose a learning-based approach to photo color adjustment for graphic designs, which maps an input photo along with the target colors to a recolored result. Our method decomposes the color adjustment process into two successive stages: modifiable region selection and target color propagation. The first stage aims to solve the core, challenging problem of which local image region(s) should be adjusted, which requires not only a common sense of colors appearing in our visual world but also understanding of subtle visual design heuristics. To this end, we capitalize on both natural photos and graphic designs to train a region selection network, which detects the most likely regions to be adjusted to the target colors. The second stage trains a recoloring network to naturally propagate the target colors in the detected regions. Through extensive experiments and a user study, we demonstrate the effectiveness of our selective region-based photo recoloring framework.},
  archive      = {J_TOG},
  doi          = {10.1145/3447647},
  journal      = {ACM Transactions on Graphics},
  month        = {4},
  number       = {2},
  pages        = {1-16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Selective region-based photo color adjustment for graphic designs},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). GPU accelerated path tracing of massive scenes.
<em>TOG</em>, <em>40</em>(2), 1–17. (<a
href="https://doi.org/10.1145/3447807">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a solution to path tracing of massive scenes on multiple GPUs. Our approach analyzes the memory access pattern of a path tracer and defines how the scene data should be distributed across up to 16 GPUs with minimal effect on performance. The key concept is that the parts of the scene that have the highest amount of memory accesses are replicated on all GPUs. We propose two methods for maximizing the performance of path tracing when working with partially distributed scene data. Both methods work on the memory management level and therefore path tracer data structures do not have to be redesigned, making our approach applicable to other path tracers with only minor changes in their code. As a proof of concept, we have enhanced the open-source Blender Cycles path tracer. The approach was validated on scenes of sizes up to 169 GB. We show that only 1–5% of the scene data needs to be replicated to all machines for such large scenes. On smaller scenes we have verified that the performance is very close to rendering a fully replicated scene. In terms of scalability we have achieved a parallel efficiency of over 94% using up to 16 GPUs.},
  archive      = {J_TOG},
  doi          = {10.1145/3447807},
  journal      = {ACM Transactions on Graphics},
  month        = {4},
  number       = {2},
  pages        = {1-17},
  shortjournal = {ACM Trans. Graph.},
  title        = {GPU accelerated path tracing of massive scenes},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
</ul>

</body>
</html>
