<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TOMS_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="toms---32">TOMS - 32</h2>
<ul>
<li><details>
<summary>
(2021). Algorithm 1019: A task-based multi-shift QR/QZ algorithm
with aggressive early deflation. <em>TOMS</em>, <em>48</em>(1), 11:1–36.
(<a href="https://doi.org/10.1145/3372419">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The QR algorithm is one of the three phases in the process of computing the eigenvalues and the eigenvectors of a dense nonsymmetric matrix. This paper describes a task-based QR algorithm for reducing an upper Hessenberg matrix to real Schur form. The task-based algorithm also supports generalized eigenvalue problems (QZ algorithm) but this paper concentrates on the standard case. The task-based algorithm adopts previous algorithmic improvements, such as tightly-coupled multi-shifts and Aggressive Early Deflation (AED), and also incorporates several new ideas that significantly improve the performance. This includes, but is not limited to, the elimination of several synchronization points, the dynamic merging of previously separate computational steps, the shortening and the prioritization of the critical path, and experimental GPU support. The task-based implementation is demonstrated to be multiple times faster than multi-threaded LAPACK and ScaLAPACK in both single-node and multi-node configurations on two different machines based on Intel and AMD CPUs. The implementation is built on top of the StarPU runtime system and is part of the open-source StarNEig library.},
  archive  = {J_TOMS},
  author   = {Myllykoski, Mirko},
  doi      = {10.1145/3372419},
  journal  = {ACM Trans. Math. Softw.},
  number   = {1},
  pages    = {11:1-36},
  title    = {Algorithm 1019: A task-based multi-shift QR/QZ algorithm with aggressive early deflation},
  volume   = {48},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Remark on algorithm 982: Explicit solutions of triangular
systems of first-order linear initial-value ordinary differential
equations with constant coefficients. <em>TOMS</em>, <em>48</em>(1),
10:1–4. (<a href="https://doi.org/10.1145/3372419">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Algorithm 982: Explicit solutions of triangular systems of first-order linear initial-value ordinary differential equations with constant coefficients provides an explicit solution for an homogeneous system, and a brief description of how to compute a solution for the inhomogeneous case. The method described is not directly useful if the coefficient matrix is singular. This remark explains more completely how to compute the solution for the inhomogeneous case and for the singular coefficient matrix case.},
  archive  = {J_TOMS},
  author   = {Van Snyder, W.},
  doi      = {10.1145/3372419},
  journal  = {ACM Trans. Math. Softw.},
  number   = {1},
  pages    = {10:1-4},
  title    = {Remark on algorithm 982: Explicit solutions of triangular systems of first-order linear initial-value ordinary differential equations with constant coefficients},
  volume   = {48},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Algorithm 1018: FaVeST—fast vector spherical harmonic
transforms. <em>TOMS</em>, <em>47</em>(4), 39:1–24. (<a
href="https://doi.org/10.1145/3372419">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Vector spherical harmonics on the unit sphere of ℝ3 have broad applications in geophysics, quantum mechanics, and astrophysics. In the representation of a tangent vector field, one needs to evaluate the expansion and the Fourier coefficients of vector spherical harmonics. In this article, we develop fast algorithms (FaVeST) for vector spherical harmonic transforms on these evaluations. The forward FaVeST evaluates the Fourier coefficients and has a computational cost proportional to N log √N for N number of evaluation points. The adjoint FaVeST, which evaluates a linear combination of vector spherical harmonics with a degree up to ⊡M for M evaluation points, has cost proportional to M log √M. Numerical examples of simulated tangent fields illustrate the accuracy, efficiency, and stability of FaVeST.},
  archive  = {J_TOMS},
  author   = {Le Gia, Quoc T. and Li, Ming and Wang, Yu Guang},
  doi      = {10.1145/3372419},
  journal  = {ACM Trans. Math. Softw.},
  number   = {4},
  pages    = {39:1-24},
  title    = {Algorithm 1018: FaVeST—Fast vector spherical harmonic transforms},
  volume   = {47},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Remark on algorithm 992: An OpenGL- and c++-based function
library for curve and surface modeling in a large class of extended
chebyshev spaces. <em>TOMS</em>, <em>47</em>(4), 38:1–2. (<a
href="https://doi.org/10.1145/3372419">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We provide a number of corrections to the software component that accompanied this Algorithm submission [3]. An updated version of the code is available from the ACM Collected Algorithms site [1].},
  archive  = {J_TOMS},
  author   = {R\&#39;{o}th, \&#39;{A}goston},
  doi      = {10.1145/3372419},
  journal  = {ACM Trans. Math. Softw.},
  number   = {4},
  pages    = {38:1-2},
  title    = {Remark on algorithm 992: An OpenGL- and c++-based function library for curve and surface modeling in a large class of extended chebyshev spaces},
  volume   = {47},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Corrigendum: Remark on algorithm 723: Fresnel integrals.
<em>TOMS</em>, <em>47</em>(4), 37:1–1. (<a
href="https://doi.org/10.1145/3372419">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {There are mistakes and typographical errors in Remark on Algorithm&amp;nbsp;723: Fresnel Integrals, which appeared in ACM Transactions on Mathematical Software 22, 4 (December 1996). This remark corrects those errors. The software provided to Collected Algorithms of the ACM was correct.},
  archive  = {J_TOMS},
  author   = {Van Snyder, W.},
  doi      = {10.1145/3372419},
  journal  = {ACM Trans. Math. Softw.},
  number   = {4},
  pages    = {37:1-1},
  title    = {Corrigendum: remark on algorithm&amp;nbsp;723: fresnel integrals},
  volume   = {47},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Scrambled linear pseudorandom number generators.
<em>TOMS</em>, <em>47</em>(4), 36:1–32. (<a
href="https://doi.org/10.1145/3372419">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {F2-linear pseudorandom number generators are very popular due to their high speed, to the ease with which generators with a sizable state space can be created, and to their provable theoretical properties. However, they suffer from linear artifacts that show as failures in linearity-related statistical tests such as the binary-rank and the linear-complexity test. In this article, we give two new contributions. First, we introduce two new F2-linear transformations that have been handcrafted to have good statistical properties and at the same time to be programmable very efficiently on superscalar processors, or even directly in hardware. Then, we describe some scramblers, that is, nonlinear functions applied to the state array that reduce or delete the linear artifacts, and propose combinations of linear transformations and scramblers that give extremely fast pseudorandom number generators of high quality. A novelty in our approach is that we use ideas from the theory of filtered linear-feedback shift registers to prove some properties of our scramblers, rather than relying purely on heuristics. In the end, we provide simple, extremely fast generators that use a few hundred bits of memory, have provable properties, and pass strong statistical tests.},
  archive  = {J_TOMS},
  author   = {Blackman, David and Vigna, Sebastiano},
  doi      = {10.1145/3372419},
  journal  = {ACM Trans. Math. Softw.},
  number   = {4},
  pages    = {36:1-32},
  title    = {Scrambled linear pseudorandom number generators},
  volume   = {47},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Exactly computing the tail of the poisson-binomial
distribution. <em>TOMS</em>, <em>47</em>(4), 35:1–19. (<a
href="https://doi.org/10.1145/3372419">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We present ShiftConvolvePoibin, a fast exact method to compute the tail of a Poisson-binomial distribution (PBD). Our method employs an exponential shift to retain its accuracy when computing a tail probability, and in practice we find that it is immune to the significant relative errors that other methods, exact or approximate, can suffer from when computing very small tail probabilities of the PBD. The accompanying R package is also competitive with the fastest implementations for computing the entire PBD.},
  archive  = {J_TOMS},
  author   = {Peres, Noah and Lee, Andrew Ray and Keich, Uri},
  doi      = {10.1145/3372419},
  journal  = {ACM Trans. Math. Softw.},
  number   = {4},
  pages    = {35:1-19},
  title    = {Exactly computing the tail of the poisson-binomial distribution},
  volume   = {47},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). PySPH: A python-based framework for smoothed particle
hydrodynamics. <em>TOMS</em>, <em>47</em>(4), 34:1–38. (<a
href="https://doi.org/10.1145/3372419">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {PySPH is an open-source, Python-based, framework for particle methods in general and Smoothed Particle Hydrodynamics (SPH) in particular. PySPH allows a user to define a complete SPH simulation using pure Python. High-performance code is generated from this high-level Python code and executed on either multiple cores, or on GPUs, seamlessly. It also supports distributed execution using MPI. PySPH supports a wide variety of SPH schemes and formulations. These include, incompressible and compressible fluid flow, elastic dynamics, rigid body dynamics, shallow water equations, and other problems. PySPH supports a variety of boundary conditions including mirror, periodic, solid wall, and inlet/outlet boundary conditions. The package is written to facilitate reuse and reproducibility. This article discusses the overall design of PySPH and demonstrates many of its features. Several example results are shown to demonstrate the range of features that PySPH provides.},
  archive  = {J_TOMS},
  author   = {Ramachandran, Prabhu and Bhosale, Aditya and Puri, Kunal and Negi, Pawan and Muta, Abhinav and Dinesh, A. and Menon, Dileep and Govind, Rahul and Sanka, Suraj and Sebastian, Amal S. and Sen, Ananyo and Kaushik, Rohan and Kumar, Anshuman and Kurapati, Vikas and Patil, Mrinalgouda and Tavker, Deep and Pandey, Pankaj and Kaushik, Chandrashekhar and Dutt, Arkopal and Agarwal, Arpit},
  doi      = {10.1145/3372419},
  journal  = {ACM Trans. Math. Softw.},
  number   = {4},
  pages    = {34:1-38},
  title    = {PySPH: A python-based framework for smoothed particle hydrodynamics},
  volume   = {47},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hyper.deal: An efficient, matrix-free finite-element library
for high-dimensional partial differential equations. <em>TOMS</em>,
<em>47</em>(4), 33:1–34. (<a
href="https://doi.org/10.1145/3372419">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This work presents the efficient, matrix-free finite-element library hyper.deal for solving partial differential equations in two up to six dimensions with high-order discontinuous Galerkin methods. It builds upon the low-dimensional finite-element library deal.II to create complex low-dimensional meshes and to operate on them individually. These meshes are combined via a tensor product on the fly, and the library provides new special-purpose highly optimized matrix-free functions exploiting domain decomposition as well as shared memory via MPI-3.0 features. Both node-level performance analyses and strong/weak-scaling studies on up to 147,456 CPU cores confirm the efficiency of the implementation. Results obtained with the library hyper.deal are reported for high-dimensional advection problems and for the solution of the Vlasov–Poisson equation in up to six-dimensional phase space.},
  archive  = {J_TOMS},
  author   = {Munch, Peter and Kormann, Katharina and Kronbichler, Martin},
  doi      = {10.1145/3372419},
  journal  = {ACM Trans. Math. Softw.},
  number   = {4},
  pages    = {33:1-34},
  title    = {Hyper.Deal: An efficient, matrix-free finite-element library for high-dimensional partial differential equations},
  volume   = {47},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Propagating geometry information to finite element
computations. <em>TOMS</em>, <em>47</em>(4), 32:1–30. (<a
href="https://doi.org/10.1145/3372419">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The traditional workflow in continuum mechanics simulations is that a geometry description —for example obtained using Constructive Solid Geometry (CSG) or Computer Aided Design (CAD) tools—forms the input for a mesh generator. The mesh is then used as the sole input for the finite element, finite volume, and finite difference solver, which at this point no longer has access to the original, “underlying” geometry. However, many modern techniques—for example, adaptive mesh refinement and the use of higher order geometry approximation methods—really do need information about the underlying geometry to realize their full potential. We have undertaken an exhaustive study of where typical finite element codes use geometry information, with the goal of determining what information geometry tools would have to provide. Our study shows that nearly all geometry-related needs inside the simulators can be satisfied by just two “primitives”: elementary queries posed by the simulation software to the geometry description. We then show that it is possible to provide these primitives in all of the frequently used ways in which geometries are described in common industrial workflows, and illustrate our solutions using a number of examples.},
  archive  = {J_TOMS},
  author   = {Heltai, Luca and Bangerth, Wolfgang and Kronbichler, Martin and Mola, Andrea},
  doi      = {10.1145/3372419},
  journal  = {ACM Trans. Math. Softw.},
  number   = {4},
  pages    = {32:1-30},
  title    = {Propagating geometry information to finite element computations},
  volume   = {47},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Abstractions and automated algorithms for mixed domain
finite element methods. <em>TOMS</em>, <em>47</em>(4), 31:1–36. (<a
href="https://doi.org/10.1145/3372419">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Mixed dimensional partial differential equations (PDEs) are equations coupling unknown fields defined over domains of differing topological dimension. Such equations naturally arise in a wide range of scientific fields including geology, physiology, biology, and fracture mechanics. Mixed dimensional PDEs are also commonly encountered when imposing non-standard conditions over a subspace of lower dimension, e.g., through a Lagrange multiplier. In this article, we present general abstractions and algorithms for finite element discretizations of mixed domain and mixed dimensional PDEs of codimension up to one (i.e., nD-mD with |n-m| ≤ 1). We introduce high-level mathematical software abstractions together with lower-level algorithms for expressing and efficiently solving such coupled systems. The concepts introduced here have also been implemented in the context of the FEniCS finite element software. We illustrate the new features through a range of examples, including a constrained Poisson problem, a set of Stokes-type flow models, and a model for ionic electrodiffusion.},
  archive  = {J_TOMS},
  author   = {Daversin-Catty, C\&#39;{e}cile and Richardson, Chris N. and Ellingsrud, Ada J. and Rognes, Marie E.},
  doi      = {10.1145/3372419},
  journal  = {ACM Trans. Math. Softw.},
  number   = {4},
  pages    = {31:1-36},
  title    = {Abstractions and automated algorithms for mixed domain finite element methods},
  volume   = {47},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Irksome: Automating runge–kutta time-stepping for finite
element methods. <em>TOMS</em>, <em>47</em>(4), 30:1–26. (<a
href="https://doi.org/10.1145/3372419">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {While implicit Runge–Kutta (RK) methods possess high order accuracy and important stability properties, implementation difficulties and the high expense of solving the coupled algebraic system at each time step are frequently cited as impediments. We present Irksome, a high-level library for manipulating UFL (Unified Form Language) expressions of semidiscrete variational forms to obtain UFL expressions for the coupled Runge–Kutta stage equations at each time step. Irksome works with the Firedrake package to enable the efficient solution of the resulting coupled algebraic systems. Numerical examples confirm the efficacy of the software and our solver techniques for various problems.},
  archive  = {J_TOMS},
  author   = {Farrell, Patrick E. and Kirby, Robert C. and Marchena-Men\&#39;{e}ndez, Jorge},
  doi      = {10.1145/3372419},
  journal  = {ACM Trans. Math. Softw.},
  number   = {4},
  pages    = {30:1-26},
  title    = {Irksome: Automating Runge–Kutta time-stepping for finite element methods},
  volume   = {47},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Algorithm 1017: Fuzzyreg: An r package for fitting fuzzy
regression models. <em>TOMS</em>, <em>47</em>(3), 29:1–18. (<a
href="https://doi.org/10.1145/3372419">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Fuzzy regression provides an alternative to statistical regression when the model is indefinite, the relationships between model parameters are vague, the sample size is low, or the data are hierarchically structured. Such cases allow to consider the choice of a regression model based on the fuzzy set theory. In fuzzyreg, we implement fuzzy linear regression methods that differ in the expectations of observational data types, outlier handling, and parameter estimation method. We provide a wrapper function that prepares data for fitting fuzzy linear models with the respective methods from a syntax established in R for fitting regression models. The function fuzzylm thus provides a novel functionality for R through standardized operations with fuzzy numbers. Additional functions allow for conversion of real-value variables to be fuzzy numbers, printing, summarizing, model plotting, and calculation of model predictions from new data using supporting functions that perform arithmetic operations with triangular fuzzy numbers. Goodness of fit and total error of the fit measures allow model comparisons. The package contains a dataset named bats with measurements of temperatures of hibernating bats and the mean annual surface temperature reflecting the climate at the sampling sites. The predictions from fuzzy linear models fitted to this dataset correspond well to the observed biological phenomenon. Fuzzy linear regression has great potential in predictive modeling where the data structure prevents statistical analysis and the modeled process exhibits inherent fuzziness.},
  archive  = {J_TOMS},
  author   = {\v{S}krab\&#39;{a}nek, Pavel and Mart\&#39;{\i}nkov\&#39;{a}, Nat\&#39;{a}lia},
  doi      = {10.1145/3372419},
  journal  = {ACM Trans. Math. Softw.},
  number   = {3},
  pages    = {29:1-18},
  title    = {Algorithm 1017: fuzzyreg: an r package for fitting fuzzy regression models},
  volume   = {47},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Medusa: A c++ library for solving PDEs using strong form
mesh-free methods. <em>TOMS</em>, <em>47</em>(3), 28:1–25. (<a
href="https://doi.org/10.1145/3372419">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Medusa, a novel library for implementation of non-particle strong form mesh-free methods, such as GFDM or RBF-FD, is described. We identify and present common parts and patterns among many such methods reported in the literature, such as node positioning, stencil selection, and stencil weight computation. Many different algorithms exist for each part and the possible combinations offer a plethora of possibilities for improvements of solution procedures that are far from fully understood. As a consequence there are still many unanswered questions in the mesh-free community resulting in vivid ongoing research in the field. Medusa implements the core mesh-free elements as independent blocks, which offers users great flexibility in experimenting with the method they are developing, as well as easily comparing it with other existing methods. The article describes the chosen abstractions and their usage, illustrates aspects of the philosophy and design, offers some executions time benchmarks and demonstrates the application of the library on cases from linear elasticity and fluid flow in irregular 2D and 3D domains.},
  archive  = {J_TOMS},
  author   = {Slak, Jure and Kosec, Gregor},
  doi      = {10.1145/3372419},
  journal  = {ACM Trans. Math. Softw.},
  number   = {3},
  pages    = {28:1-25},
  title    = {Medusa: A c++ library for solving PDEs using strong form mesh-free methods},
  volume   = {47},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). HyperNOMAD: Hyperparameter optimization of deep neural
networks using mesh adaptive direct search. <em>TOMS</em>,
<em>47</em>(3), 27:1–27. (<a
href="https://doi.org/10.1145/3372419">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The performance of deep neural networks is highly sensitive to the choice of the hyperparameters that define the structure of the network and the learning process. When facing a new application, tuning a deep neural network is a tedious and time-consuming process that is often described as a “dark art.” This explains the necessity of automating the calibration of these hyperparameters. Derivative-free optimization is a field that develops methods designed to optimize time-consuming functions without relying on derivatives. This work introduces the HyperNOMAD package, an extension of the NOMAD software that applies the MADS algorithm&amp;nbsp;[7] to simultaneously tune the hyperparameters responsible for both the architecture and the learning process of a deep neural network (DNN). This generic approach allows for an important flexibility in the exploration of the search space by taking advantage of categorical variables. HyperNOMAD is tested on the MNIST, Fashion-MNIST, and CIFAR-10 datasets and achieves results comparable to the current state of the art.},
  archive  = {J_TOMS},
  author   = {Lakhmiri, Dounia and Digabel, S\&#39;{e}bastien Le and Tribes, Christophe},
  doi      = {10.1145/3372419},
  journal  = {ACM Trans. Math. Softw.},
  number   = {3},
  pages    = {27:1-27},
  title    = {HyperNOMAD: Hyperparameter optimization of deep neural networks using mesh adaptive direct search},
  volume   = {47},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). FAME: Fast algorithms for maxwell’s equations for
three-dimensional photonic crystals. <em>TOMS</em>, <em>47</em>(3),
26:1–24. (<a href="https://doi.org/10.1145/3372419">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In this article, we propose the Fast Algorithms for Maxwell’s Equations (FAME) package for solving Maxwell’s equations for modeling three-dimensional photonic crystals. FAME combines the null-space free method with fast Fourier transform (FFT)-based matrix-vector multiplications to solve the generalized eigenvalue problems (GEPs) arising from Yee’s discretization. The GEPs are transformed into a null-space free standard eigenvalue problem with a Hermitian positive-definite coefficient matrix. The computation times for FFT-based matrix-vector multiplications with matrices of dimension 7 million are only 0.33 and 3.6 \texttimes{} 10 − 3 seconds using MATLAB with an Intel Xeon CPU and CUDA C++ programming with a single NVIDIA Tesla P100 GPU, respectively. Such multiplications significantly reduce the computational costs of the conjugate gradient method for solving linear systems. We successfully use FAME on a single P100 GPU to solve a set of GEPs with matrices of dimension more than 19 million, in 127 to 191 seconds per problem. These results demonstrate the potential of our proposed package to enable large-scale numerical simulations for novel physical discoveries and engineering applications of photonic crystals.},
  archive  = {J_TOMS},
  author   = {Lyu, Xing-long and Li, Tiexiang and Huang, Tsung-ming and Lin, Jia-wei and Lin, Wen-wei and Wang, Sheng},
  doi      = {10.1145/3372419},
  journal  = {ACM Trans. Math. Softw.},
  number   = {3},
  pages    = {26:1-24},
  title    = {FAME: Fast algorithms for maxwell’s equations for three-dimensional photonic crystals},
  volume   = {47},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). PCPATCH: Software for the topological construction of
multigrid relaxation methods. <em>TOMS</em>, <em>47</em>(3), 25:1–22.
(<a href="https://doi.org/10.1145/3372419">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Effective relaxation methods are necessary for good multigrid convergence. For many equations, standard Jacobi and Gau\ss{}–Seidel are inadequate, and more sophisticated space decompositions are required; examples include problems with semidefinite terms or saddle point structure. In this article, we present a unifying software abstraction, PCPATCH, for the topological construction of space decompositions for multigrid relaxation methods. Space decompositions are specified by collecting topological entities in a mesh (such as all vertices or faces) and applying a construction rule (such as taking all degrees of freedom in the cells around each entity). The software is implemented in PETSc and facilitates the elegant expression of a wide range of schemes merely by varying solver options at runtime. In turn, this allows for the very rapid development of fast solvers for difficult problems.},
  archive  = {J_TOMS},
  author   = {Farrell, Patrick E. and Knepley, Matthew G. and Mitchell, Lawrence and Wechsung, Florian},
  doi      = {10.1145/3372419},
  journal  = {ACM Trans. Math. Softw.},
  number   = {3},
  pages    = {25:1-22},
  title    = {PCPATCH: Software for the topological construction of multigrid relaxation methods},
  volume   = {47},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fast matching pursuit with multi-gabor dictionaries.
<em>TOMS</em>, <em>47</em>(3), 24:1–20. (<a
href="https://doi.org/10.1145/3372419">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Finding the best K-sparse approximation of a signal in a redundant dictionary is an NP-hard problem. Suboptimal greedy matching pursuit algorithms are generally used for this task. In this work, we present an acceleration technique and an implementation of the matching pursuit algorithm acting on a multi-Gabor dictionary, i.e., a concatenation of several Gabor-type time-frequency dictionaries, each of which consists of translations and modulations of a possibly different window and time and frequency shift parameters. The technique is based on pre-computing and thresholding inner products between atoms and on updating the residual directly in the coefficient domain, i.e., without the round-trip to the signal domain. Since the proposed acceleration technique involves an approximate update step, we provide theoretical and experimental results illustrating the convergence of the resulting algorithm. The implementation is written in C (compatible with C99 and C++11), and we also provide Matlab and GNU Octave interfaces. For some settings, the implementation is up to 70 times faster than the standard Matching Pursuit Toolkit.},
  archive  = {J_TOMS},
  author   = {Pr\r{u}\v{s}a, Zden\v{e}k and Holighaus, Nicki and Balazs, Peter},
  doi      = {10.1145/3372419},
  journal  = {ACM Trans. Math. Softw.},
  number   = {3},
  pages    = {24:1-20},
  title    = {Fast matching pursuit with multi-gabor dictionaries},
  volume   = {47},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). NEP: A module for the parallel solution of nonlinear
eigenvalue problems in SLEPc. <em>TOMS</em>, <em>47</em>(3), 23:1–29.
(<a href="https://doi.org/10.1145/3372419">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {SLEPc is a parallel library for the solution of various types of large-scale eigenvalue problems. Over the past few years, we have been developing a module within SLEPc, called NEP, that is intended for solving nonlinear eigenvalue problems. These problems can be defined by means of a matrix-valued function that depends nonlinearly on a single scalar parameter. We do not consider the particular case of polynomial eigenvalue problems (which are implemented in a different module in SLEPc) and focus here on rational eigenvalue problems and other general nonlinear eigenproblems involving square roots or any other nonlinear function. The article discusses how the NEP module has been designed to fit the needs of applications and provides a description of the available solvers, including some implementation details such as parallelization. Several test problems coming from real applications are used to evaluate the performance and reliability of the solvers.},
  archive  = {J_TOMS},
  author   = {Campos, Carmen and Roman, Jose E.},
  doi      = {10.1145/3372419},
  journal  = {ACM Trans. Math. Softw.},
  number   = {3},
  pages    = {23:1-29},
  title    = {NEP: A module for the parallel solution of nonlinear eigenvalue problems in SLEPc},
  volume   = {47},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Linnea: Automatic generation of efficient linear algebra
programs. <em>TOMS</em>, <em>47</em>(3), 22:1–26. (<a
href="https://doi.org/10.1145/3372419">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The translation of linear algebra computations into efficient sequences of library calls is a non-trivial task that requires expertise in both linear algebra and high-performance computing. Almost all high-level languages and libraries for matrix computations (e.g., Matlab, Eigen) internally use optimized kernels such as those provided by BLAS and LAPACK; however, their translation algorithms are often too simplistic and thus lead to a suboptimal use of said kernels, resulting in significant performance losses. To combine the productivity offered by high-level languages, and the performance of low-level kernels, we are developing Linnea, a code generator for linear algebra problems. As input, Linnea takes a high-level description of a linear algebra problem; as output, it returns an efficient sequence of calls to high-performance kernels. Linnea uses a custom best-first search algorithm to find a first solution in less than a second, and increasingly better solutions when given more time. In 125 test problems, the code generated by Linnea almost always outperforms Matlab, Julia, Eigen, and Armadillo, with speedups up to and exceeding 10\texttimes{}.},
  archive  = {J_TOMS},
  author   = {Barthels, Henrik and Psarras, Christos and Bientinesi, Paolo},
  doi      = {10.1145/3372419},
  journal  = {ACM Trans. Math. Softw.},
  number   = {3},
  pages    = {22:1-26},
  title    = {Linnea: Automatic generation of efficient linear algebra programs},
  volume   = {47},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A set of batched basic linear algebra subprograms and LAPACK
routines. <em>TOMS</em>, <em>47</em>(3), 21:1–23. (<a
href="https://doi.org/10.1145/3372419">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This article describes a standard API for a set of Batched Basic Linear Algebra Subprograms (Batched BLAS or BBLAS). The focus is on many independent BLAS operations on small matrices that are grouped together and processed by a single routine, called a Batched BLAS routine. The matrices are grouped together in uniformly sized groups, with just one group if all the matrices are of equal size. The aim is to provide more efficient, but portable, implementations of algorithms on high-performance many-core platforms. These include multicore and many-core CPU processors, GPUs and coprocessors, and other hardware accelerators with floating-point compute facility. As well as the standard types of single and double precision, we also include half and quadruple precision in the standard. In particular, half precision is used in many very large scale applications, such as those associated with machine learning.},
  archive  = {J_TOMS},
  author   = {Abdelfattah, Ahmad and Costa, Timothy and Dongarra, Jack and Gates, Mark and Haidar, Azzam and Hammarling, Sven and Higham, Nicholas J. and Kurzak, Jakub and Luszczek, Piotr and Tomov, Stanimire and Zounon, Mawussi},
  doi      = {10.1145/3372419},
  journal  = {ACM Trans. Math. Softw.},
  number   = {3},
  pages    = {21:1-23},
  title    = {A set of batched basic linear algebra subprograms and LAPACK routines},
  volume   = {47},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). PLANC: Parallel low-rank approximation with nonnegativity
constraints. <em>TOMS</em>, <em>47</em>(3), 20:1–37. (<a
href="https://doi.org/10.1145/3372419">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We consider the problem of low-rank approximation of massive dense nonnegative tensor data, for example, to discover latent patterns in video and imaging applications. As the size of data sets grows, single workstations are hitting bottlenecks in both computation time and available memory. We propose a distributed-memory parallel computing solution to handle massive data sets, loading the input data across the memories of multiple nodes, and performing efficient and scalable parallel algorithms to compute the low-rank approximation. We present a software package called Parallel Low-rank Approximation with Nonnegativity Constraints, which implements our solution and allows for extension in terms of data (dense or sparse, matrices or tensors of any order), algorithm (e.g., from multiplicative updating techniques to alternating direction method of multipliers), and architecture (we exploit GPUs to accelerate the computation in this work). We describe our parallel distributions and algorithms, which are careful to avoid unnecessary communication and computation, show how to extend the software to include new algorithms and/or constraints, and report efficiency and scalability results for both synthetic and real-world data sets.},
  archive  = {J_TOMS},
  author   = {Eswar, Srinivas and Hayashi, Koby and Ballard, Grey and Kannan, Ramakrishnan and Matheson, Michael A. and Park, Haesun},
  doi      = {10.1145/3372419},
  journal  = {ACM Trans. Math. Softw.},
  number   = {3},
  pages    = {20:1-37},
  title    = {PLANC: Parallel low-rank approximation with nonnegativity constraints},
  volume   = {47},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Algorithm 1016: PyMGRIT: A python package for the
parallel-in-time method MGRIT. <em>TOMS</em>, <em>47</em>(2), 19:1–22.
(<a href="https://doi.org/10.1145/3372419">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In this article, we introduce the Python framework PyMGRIT, which implements the multigrid-reduction-in-time (MGRIT) algorithm for solving (non-)linear systems arising from the discretization of time-dependent problems. The MGRIT algorithm is a reduction-based iterative method that allows parallel-in-time simulations, i.e., calculating multiple time steps simultaneously in a simulation, using a time-grid hierarchy. The PyMGRIT framework includes many different variants of the MGRIT algorithm, ranging from different multigrid cycle types and relaxation schemes, various coarsening strategies, including time-only and space-time coarsening, and the ability to utilize different time integrators on different levels in the multigrid hierachy. The comprehensive documentation with tutorials and many examples and the fully documented code allow an easy start into the work with the package. The functionality of the code is ensured by automated serial and parallel tests using continuous integration. PyMGRIT supports serial runs suitable for prototyping and testing of new approaches, as well as parallel runs using the Message Passing Interface (MPI). In this manuscript, we describe the implementation of the MGRIT algorithm in PyMGRIT and present the usage from both a user and a developer point of view. Three examples illustrate different aspects of the package itself, especially running tests with pure time parallelism, as well as space-time parallelism through the coupling of PyMGRIT with PETSc or Firedrake.},
  archive  = {J_TOMS},
  author   = {Hahne, Jens and Friedhoff, Stephanie and Bolten, Matthias},
  doi      = {10.1145/3372419},
  journal  = {ACM Trans. Math. Softw.},
  number   = {2},
  pages    = {19:1-22},
  title    = {Algorithm 1016: PyMGRIT: a python package for the parallel-in-time method MGRIT},
  volume   = {47},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Algorithm 1015: A fast scalable solver for the dense linear
(sum) assignment problem. <em>TOMS</em>, <em>47</em>(2), 18:1–27. (<a
href="https://doi.org/10.1145/3372419">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We present a new algorithm for solving the dense linear (sum) assignment problem and an efficient, parallel implementation that is based on the successive shortest path algorithm. More specifically, we introduce the well-known epsilon scaling approach used in the Auction algorithm to approximate the dual variables of the successive shortest path algorithm prior to solving the assignment problem to limit the complexity of the path search. This improves the runtime by several orders of magnitude for hard-to-solve real-world problems, making the runtime virtually independent of how hard the assignment is to find. In addition, our approach allows for using accelerators and/or external compute resources to calculate individual rows of the cost matrix. This enables us to solve problems that are larger than what has been reported in the past, including the ability to efficiently solve problems whose cost matrix exceeds the available systems memory. To our knowledge, this is the first implementation that is able to solve problems with more than one trillion arcs in less than 100 hours on a single machine.},
  archive  = {J_TOMS},
  author   = {Guthe, Stefan and Thuerck, Daniel},
  doi      = {10.1145/3372419},
  journal  = {ACM Trans. Math. Softw.},
  number   = {2},
  pages    = {18:1-27},
  title    = {Algorithm&amp;nbsp;1015: A fast scalable solver for the dense linear (Sum) assignment problem},
  volume   = {47},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). FenicsR13: A tensorial mixed finite element solver for the
linear r13 equations using the FEniCS computing platform. <em>TOMS</em>,
<em>47</em>(2), 17:1–29. (<a
href="https://doi.org/10.1145/3372419">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We present a mixed finite element solver for the linearized regularized 13-moment equations of non-equilibrium gas dynamics. The Python implementation builds upon the software tools provided by the FEniCS computing platform. We describe a new tensorial approach utilizing the extension capabilities of FEniCS’ Unified Form Language to define required differential operators for tensors above second degree. The presented solver serves as an example for implementing tensorial variational formulations in FEniCS, for which the documentation and literature seem to be very sparse. Using the software abstraction levels provided by the Unified Form Language allows an almost one-to-one correspondence between the underlying mathematics and the resulting source code. Test cases support the correctness of the proposed method using validation with exact solutions. To justify the usage of extended gas flow models, we discuss typical application cases involving rarefaction effects. We provide the documented and validated solver publicly.},
  archive  = {J_TOMS},
  author   = {Theisen, Lambert and Torrilhon, Manuel},
  doi      = {10.1145/3372419},
  journal  = {ACM Trans. Math. Softw.},
  number   = {2},
  pages    = {17:1-29},
  title    = {FenicsR13: A tensorial mixed finite element solver for the linear r13 equations using the FEniCS computing platform},
  volume   = {47},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). HIPPYlib: An extensible software framework for large-scale
inverse problems governed by PDEs: Part i: Deterministic inversion and
linearized bayesian inference. <em>TOMS</em>, <em>47</em>(2), 16:1–34.
(<a href="https://doi.org/10.1145/3372419">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We present an extensible software framework, hIPPYlib, for solution of large-scale deterministic and Bayesian inverse problems governed by partial differential equations (PDEs) with (possibly) infinite-dimensional parameter fields (which are high-dimensional after discretization). hIPPYlib overcomes the prohibitively expensive nature of Bayesian inversion for this class of problems by implementing state-of-the-art scalable algorithms for PDE-based inverse problems that exploit the structure of the underlying operators, notably the Hessian of the log-posterior. The key property of the algorithms implemented in hIPPYlib is that the solution of the inverse problem is computed at a cost, measured in linearized forward PDE solves, that is independent of the parameter dimension. The mean of the posterior is approximated by the MAP point, which is found by minimizing the negative log-posterior with an inexact matrix-free Newton-CG method. The posterior covariance is approximated by the inverse of the Hessian of the negative log posterior evaluated at the MAP point. The construction of the posterior covariance is made tractable by invoking a low-rank approximation of the Hessian of the log-likelihood. Scalable tools for sample generation are also discussed. hIPPYlib makes all of these advanced algorithms easily accessible to domain scientists and provides an environment that expedites the development of new algorithms.},
  archive  = {J_TOMS},
  author   = {Villa, Umberto and Petra, Noemi and Ghattas, Omar},
  doi      = {10.1145/3372419},
  journal  = {ACM Trans. Math. Softw.},
  number   = {2},
  pages    = {16:1-34},
  title    = {HIPPYlib: an extensible software framework for large-scale inverse problems governed by PDEs: part i: deterministic inversion and linearized bayesian inference},
  volume   = {47},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Replicated computational results (RCR) report for “adaptive
precision block-jacobi for high performance preconditioning in the
ginkgo linear algebra software.” <em>TOMS</em>, <em>47</em>(2), 15:1–4.
(<a href="https://doi.org/10.1145/3372419">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The article by Flegar et&amp;nbsp;al. titled “Adaptive Precision Block-Jacobi for High Performance Preconditioning in the Ginkgo Linear Algebra Software” presents a novel, practical implementation of an adaptive precision block-Jacobi preconditioner. Performance results using state-of-the-art GPU architectures for the block-Jacobi preconditioner generation and application demonstrate the practical usability of the method, compared to a traditional full-precision block-Jacobi preconditioner. A production-ready implementation is provided in the Ginkgo numerical linear algebra library.In this report, the Ginkgo library is reinstalled and performance results are generated to perform a comparison to the original results when using Ginkgo’s Conjugate Gradient solver with either the full or the adaptive precision block-Jacobi preconditioner for a suite of test problems on an NVIDIA GPU accelerator. After completing this process, the published results are deemed reproducible.},
  archive  = {J_TOMS},
  author   = {Osborn, Sarah},
  doi      = {10.1145/3372419},
  journal  = {ACM Trans. Math. Softw.},
  number   = {2},
  pages    = {15:1-4},
  title    = {Replicated computational results (RCR) report for “Adaptive precision block-jacobi for high performance preconditioning in the ginkgo linear algebra software”},
  volume   = {47},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptive precision block-jacobi for high performance
preconditioning in the ginkgo linear algebra software. <em>TOMS</em>,
<em>47</em>(2), 14:1–28. (<a
href="https://doi.org/10.1145/3372419">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The use of mixed precision in numerical algorithms is a promising strategy for accelerating scientific applications. In particular, the adoption of specialized hardware and data formats for low-precision arithmetic in high-end GPUs (graphics processing units) has motivated numerous efforts aiming at carefully reducing the working precision in order to speed up the computations. For algorithms whose performance is bound by the memory bandwidth, the idea of compressing its data before (and after) memory accesses has received considerable attention. One idea is to store an approximate operator–like a preconditioner–in lower than working precision hopefully without impacting the algorithm output. We realize the first high-performance implementation of an adaptive precision block-Jacobi preconditioner which selects the precision format used to store the preconditioner data on-the-fly, taking into account the numerical properties of the individual preconditioner blocks. We implement the adaptive block-Jacobi preconditioner as production-ready functionality in the Ginkgo linear algebra library, considering not only the precision formats that are part of the IEEE standard, but also customized formats which optimize the length of the exponent and significand to the characteristics of the preconditioner blocks. Experiments run on a state-of-the-art GPU accelerator show that our implementation offers attractive runtime savings.},
  archive  = {J_TOMS},
  author   = {Flegar, Goran and Anzt, Hartwig and Cojean, Terry and Quintana-Ort\&#39;{\i}, Enrique S.},
  doi      = {10.1145/3372419},
  journal  = {ACM Trans. Math. Softw.},
  number   = {2},
  pages    = {14:1-28},
  title    = {Adaptive precision block-jacobi for high performance preconditioning in the ginkgo linear algebra software},
  volume   = {47},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Recycling krylov subspaces and truncating deflation
subspaces for solving sequence of linear systems. <em>TOMS</em>,
<em>47</em>(2), 13:1–30. (<a
href="https://doi.org/10.1145/3372419">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This article presents deflation strategies related to recycling Krylov subspace methods for solving one or a sequence of linear systems of equations. Besides well-known strategies of deflation, Ritz-, and harmonic Ritz-based deflation, we introduce an Singular Value Decomposition based deflation technique. We consider the recycling in two contexts: recycling the Krylov subspace between the restart cycles and recycling a deflation subspace when the matrix changes in a sequence of linear systems. Numerical experiments on real-life reservoir simulation demonstrate the impact of our proposed strategy.},
  archive  = {J_TOMS},
  author   = {Daas, Hussam Al and Grigori, Laura and H\&#39;{e}non, Pascal and Ricoux, Philippe},
  doi      = {10.1145/3372419},
  journal  = {ACM Trans. Math. Softw.},
  number   = {2},
  pages    = {13:1-30},
  title    = {Recycling krylov subspaces and truncating deflation subspaces for solving sequence of linear systems},
  volume   = {47},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Supporting mixed-domain mixed-precision matrix
multiplication within the BLIS framework. <em>TOMS</em>, <em>47</em>(2),
12:1–26. (<a href="https://doi.org/10.1145/3372419">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We approach the problem of implementing mixed-datatype support within the general matrix multiplication (gemm) operation of the BLAS-like Library Instantiation Software framework, whereby each matrix operand A, B, and C may be stored as single- or double-precision real or complex values. Another factor of complexity, whereby the matrix product and accumulation are allowed to take place in a precision different from the storage precisions of either A or B, is also discussed. We first break the problem into orthogonal dimensions, considering the mixing of domains separately from mixing precisions. Support for all combinations of matrix operands stored in either the real or complex domain is mapped out by enumerating the cases and describing an implementation approach for each. Supporting all combinations of storage and computation precisions is handled by typecasting the matrices at key stages of the computation—during packing and/or accumulation, as needed. Several optional optimizations are also documented. Performance results gathered on a 56-core Marvell ThunderX2 and a 52-core Intel Xeon Platinum demonstrate that high performance is mostly preserved, with modest slowdowns incurred from unavoidable typecast instructions. The mixed-datatype implementation confirms that combinatorial intractability is avoided, with the framework relying on only two assembly microkernels to implement 128 datatype combinations.},
  archive  = {J_TOMS},
  author   = {Van Zee, Field G. and Parikh, Devangi N. and Geijn, Robert A. Van De},
  doi      = {10.1145/3372419},
  journal  = {ACM Trans. Math. Softw.},
  number   = {2},
  pages    = {12:1-26},
  title    = {Supporting mixed-domain mixed-precision matrix multiplication within the BLIS framework},
  volume   = {47},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Improved arithmetic of complex fans. <em>TOMS</em>,
<em>47</em>(2), 11:1–10. (<a
href="https://doi.org/10.1145/3372419">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Complex fans are sets of complex numbers whose magnitudes and angles range in closed intervals. The fact that the sum of two fans is a disordered shape gives rise to the need for computational methods to find the minimal enclosing fan. Cases where the sum of two fans contains the origin of the complex plane as a boundary point are of special interest. The result of the addition is then enclosed by circles in current methods, but under certain circumstances this turns out to be an overestimate. The focus of this article is the diagnosis and treatment of such cases.},
  archive  = {J_TOMS},
  author   = {Soylu, G\&quot;{u}ltekin},
  doi      = {10.1145/3372419},
  journal  = {ACM Trans. Math. Softw.},
  number   = {2},
  pages    = {11:1-10},
  title    = {Improved arithmetic of complex fans},
  volume   = {47},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Confidence intervals for stochastic arithmetic.
<em>TOMS</em>, <em>47</em>(2), 10:1–33. (<a
href="https://doi.org/10.1145/3372419">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Quantifying errors and losses due to the use of Floating-point (FP) calculations in industrial scientific computing codes is an important part of the Verification, Validation, and Uncertainty Quantification process. Stochastic Arithmetic is one way to model and estimate FP losses of accuracy, which scales well to large, industrial codes. It exists in different flavors, such as CESTAC or MCA, implemented in various tools such as CADNA, Verificarlo, or Verrou. These methodologies and tools are based on the idea that FP losses of accuracy can be modeled via randomness. Therefore, they share the same need to perform a statistical analysis of programs results to estimate the significance of the results.In this article, we propose a framework to perform a solid statistical analysis of Stochastic Arithmetic. This framework unifies all existing definitions of the number of significant digits (CESTAC and MCA), and also proposes a new quantity of interest: the number of digits contributing to the accuracy of the results. Sound confidence intervals are provided for all estimators, both in the case of normally distributed results, and in the general case. The use of this framework is demonstrated by two case studies of industrial codes: Europlexus and code_aster.},
  archive  = {J_TOMS},
  author   = {Sohier, Devan and Castro, Pablo De Oliveira and F\&#39;{e}votte, Fran\c{c}ois and Lathuili\`{e}re, Bruno and Petit, Eric and Jamond, Olivier},
  doi      = {10.1145/3372419},
  journal  = {ACM Trans. Math. Softw.},
  number   = {2},
  pages    = {10:1-33},
  title    = {Confidence intervals for stochastic arithmetic},
  volume   = {47},
  year     = {2021},
}
</textarea>
</details></li>
</ul>

</body>
</html>
