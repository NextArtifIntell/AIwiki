<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TIIS_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tiis---32">TIIS - 32</h2>
<ul>
<li><details>
<summary>
(2021). Toward responsible AI: An overview of federated learning for
user-centered privacy-preserving computing. <em>TIIS</em>,
<em>11</em>(3-4), 1–22. (<a
href="https://doi.org/10.1145/3485875">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid advances of Artificial Intelligence (AI) technologies and applications, an increasing concern is on the development and application of responsible AI technologies. Building AI technologies or machine-learning models often requires massive amounts of data, which may include sensitive, user private information to be collected from different sites or countries. Privacy, security, and data governance constraints rule out a brute force process in the acquisition and integration of these data. It is thus a serious challenge to protect user privacy while achieving high-performance models. This article reviews recent progress of federated learning in addressing this challenge in the context of privacy-preserving computing. Federated learning allows global AI models to be trained and used among multiple decentralized data sources with high security and privacy guarantees, as well as sound incentive mechanisms. This article presents the background, motivations, definitions, architectures, and applications of federated learning as a new paradigm for building privacy-preserving, responsible AI ecosystems.},
  archive      = {J_TIIS},
  doi          = {10.1145/3485875},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {10},
  number       = {3-4},
  pages        = {1-22},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {Toward responsible AI: An overview of federated learning for user-centered privacy-preserving computing},
  volume       = {11},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Introduction to the special column for human-centered
artificial intelligence. <em>TIIS</em>, <em>11</em>(3-4), 1. (<a
href="https://doi.org/10.1145/3490553">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {No abstract available.},
  archive      = {J_TIIS},
  doi          = {10.1145/3490553},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {10},
  number       = {3-4},
  pages        = {1},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {Introduction to the special column for human-centered artificial intelligence},
  volume       = {11},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Projection path explorer: Exploring visual patterns in
projected decision-making paths. <em>TIIS</em>, <em>11</em>(3-4), 1–29.
(<a href="https://doi.org/10.1145/3387165">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In problem-solving, a path towards a solutions can be viewed as a sequence of decisions. The decisions, made by humans or computers, describe a trajectory through a high-dimensional representation space of the problem. By means of dimensionality reduction, these trajectories can be visualized in lower-dimensional space. Such embedded trajectories have previously been applied to a wide variety of data, but analysis has focused almost exclusively on the self-similarity of single trajectories. In contrast, we describe patterns emerging from drawing many trajectories—for different initial conditions, end states, and solution strategies—in the same embedding space. We argue that general statements about the problem-solving tasks and solving strategies can be made by interpreting these patterns. We explore and characterize such patterns in trajectories resulting from human and machine-made decisions in a variety of application domains: logic puzzles (Rubik’s cube), strategy games (chess), and optimization problems (neural network training). We also discuss the importance of suitably chosen representation spaces and similarity metrics for the embedding.},
  archive      = {J_TIIS},
  doi          = {10.1145/3387165},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {8},
  number       = {3-4},
  pages        = {1-29},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {Projection path explorer: Exploring visual patterns in projected decision-making paths},
  volume       = {11},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A multidisciplinary survey and framework for design and
evaluation of explainable AI systems. <em>TIIS</em>, <em>11</em>(3-4),
1–45. (<a href="https://doi.org/10.1145/3387166">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The need for interpretable and accountable intelligent systems grows along with the prevalence of artificial intelligence ( AI ) applications used in everyday life. Explainable AI ( XAI ) systems are intended to self-explain the reasoning behind system decisions and predictions. Researchers from different disciplines work together to define, design, and evaluate explainable systems. However, scholars from different disciplines focus on different objectives and fairly independent topics of XAI research, which poses challenges for identifying appropriate design and evaluation methodology and consolidating knowledge across efforts. To this end, this article presents a survey and framework intended to share knowledge and experiences of XAI design and evaluation methods across multiple disciplines. Aiming to support diverse design goals and evaluation methods in XAI research, after a thorough review of XAI related papers in the fields of machine learning, visualization, and human-computer interaction, we present a categorization of XAI design goals and evaluation methods. Our categorization presents the mapping between design goals for different XAI user groups and their evaluation methods. From our findings, we develop a framework with step-by-step design guidelines paired with evaluation methods to close the iterative design and evaluation cycles in multidisciplinary XAI teams. Further, we provide summarized ready-to-use tables of evaluation methods and recommendations for different goals in XAI research.},
  archive      = {J_TIIS},
  doi          = {10.1145/3387166},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {8},
  number       = {3-4},
  pages        = {1-45},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {A multidisciplinary survey and framework for design and evaluation of explainable AI systems},
  volume       = {11},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MI3: Machine-initiated intelligent interaction for
interactive classification and data reconstruction. <em>TIIS</em>,
<em>11</em>(3-4), 1–34. (<a
href="https://doi.org/10.1145/3412848">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many applications, while machine learning (ML) can be used to derive algorithmic models to aid decision processes, it is often difficult to learn a precise model when the number of similar data points is limited. One example of such applications is data reconstruction from historical visualizations, many of which encode precious data, but their numerical records are lost. On the one hand, there is not enough similar data for training an ML model. On the other hand, manual reconstruction of the data is both tedious and arduous. Hence, a desirable approach is to train an ML model dynamically using interactive classification, and hopefully, after some training, the model can complete the data reconstruction tasks with less human interference. For this approach to be effective, the number of annotated data objects used for training the ML model should be as small as possible, while the number of data objects to be reconstructed automatically should be as large as possible. In this article, we present a novel technique for the machine to initiate intelligent interactions to reduce the user’s interaction cost in interactive classification tasks. The technique of machine-initiated intelligent interaction (MI3) builds on a generic framework featuring active sampling and default labeling. To demonstrate the MI3 approach, we use the well-known cholera map visualization by John Snow as an example, as it features three instances of MI3 pipelines. The experiment has confirmed the merits of the MI3 approach.},
  archive      = {J_TIIS},
  doi          = {10.1145/3412848},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {8},
  number       = {3-4},
  pages        = {1-34},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {MI3: Machine-initiated intelligent interaction for interactive classification and data reconstruction},
  volume       = {11},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). BONNIE: Building online narratives from noteworthy
interaction events. <em>TIIS</em>, <em>11</em>(3-4), 1–31. (<a
href="https://doi.org/10.1145/3423048">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, we have access to data of unprecedented volume, high dimensionality, and complexity. To extract novel insights from such complex and dynamic data, we need effective and efficient strategies. One such strategy is to combine data analysis and visualization techniques, which are the essence of visual analytics applications. After the knowledge discovery process, a major challenge is to filter the essential information that has led to a discovery and to communicate the findings to other people, explaining the decisions they may have made based on the data. We propose to record and use the trace left by the exploratory data analysis, in the form of user interaction history, to aid this process. With the trace, users can choose the desired interaction steps and create a narrative, sharing the acquired knowledge with readers. To achieve our goal, we have developed the BONNIE ( Building Online Narratives from Noteworthy Interaction Events ) framework. BONNIE comprises a log model to register the interaction events, auxiliary code to help developers instrument their own code, and an environment to view users’ own interaction history and build narratives. This article presents our proposal for communicating discoveries in visual analytics applications, the BONNIE framework, and the studies we conducted to evaluate our solution. After two user studies (the first one focused on history visualization and the second one focused on narrative creation), our solution has showed to be promising, with mostly positive feedback and results from a Technology Acceptance Model ( TAM ) questionnaire.},
  archive      = {J_TIIS},
  doi          = {10.1145/3423048},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {8},
  number       = {3-4},
  pages        = {1-31},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {BONNIE: Building online narratives from noteworthy interaction events},
  volume       = {11},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). VADAF: Visualization for abnormal client detection and
analysis in federated learning. <em>TIIS</em>, <em>11</em>(3-4), 1–23.
(<a href="https://doi.org/10.1145/3426866">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Learning (FL) provides a powerful solution to distributed machine learning on a large corpus of decentralized data. It ensures privacy and security by performing computation on devices (which we refer to as clients) based on local data to improve the shared global model. However, the inaccessibility of the data and the invisibility of the computation make it challenging to interpret and analyze the training process, especially to distinguish potential client anomalies. Identifying these anomalies can help experts diagnose and improve FL models. For this reason, we propose a visual analytics system, VADAF, to depict the training dynamics and facilitate analyzing potential client anomalies. Specifically, we design a visualization scheme that supports massive training dynamics in the FL environment. Moreover, we introduce an anomaly detection method to detect potential client anomalies, which are further analyzed based on both the client model’s visual and objective estimation. Three case studies have demonstrated the effectiveness of our system in understanding the FL training process and supporting abnormal client detection and analysis.},
  archive      = {J_TIIS},
  doi          = {10.1145/3426866},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {8},
  number       = {3-4},
  pages        = {1-23},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {VADAF: Visualization for abnormal client detection and analysis in federated learning},
  volume       = {11},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). QuestionComb: A gamification approach for the visual
explanation of linguistic phenomena through interactive labeling.
<em>TIIS</em>, <em>11</em>(3-4), 1–38. (<a
href="https://doi.org/10.1145/3429448">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Linguistic insight in the form of high-level relationships and rules in text builds the basis of our understanding of language. However, the data-driven generation of such structures often lacks labeled resources that can be used as training data for supervised machine learning. The creation of such ground-truth data is a time-consuming process that often requires domain expertise to resolve text ambiguities and characterize linguistic phenomena. Furthermore, the creation and refinement of machine learning models is often challenging for linguists as the models are often complex, in-transparent, and difficult to understand. To tackle these challenges, we present a visual analytics technique for interactive data labeling that applies concepts from gamification and explainable Artificial Intelligence (XAI) to support complex classification tasks. The visual-interactive labeling interface promotes the creation of effective training data. Visual explanations of learned rules unveil the decisions of the machine learning model and support iterative and interactive optimization. The gamification-inspired design guides the user through the labeling process and provides feedback on the model performance. As an instance of the proposed technique, we present QuestionComb , a workspace tailored to the task of question classification (i.e., in information-seeking vs. non-information-seeking questions). Our evaluation studies confirm that gamification concepts are beneficial to engage users through continuous feedback, offering an effective visual analytics technique when combined with active learning and XAI.},
  archive      = {J_TIIS},
  doi          = {10.1145/3429448},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {8},
  number       = {3-4},
  pages        = {1-38},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {QuestionComb: A gamification approach for the visual explanation of linguistic phenomena through interactive labeling},
  volume       = {11},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). AOI-shapes: An efficient footprint algorithm to support
visualization of user-defined urban areas of interest. <em>TIIS</em>,
<em>11</em>(3-4), 1–32. (<a
href="https://doi.org/10.1145/3431817">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding urban areas of interest (AOIs) is essential in many real-life scenarios, and such AOIs can be computed based on the geographic points that satisfy user queries. In this article, we study the problem of efficient and effective visualization of user-defined urban AOIs in an interactive manner. In particular, we first define the problem of user-defined AOI visualization based on a real estate data visualization scenario, and we illustrate why a novel footprint method is needed to support the visualization. After extensively reviewing existing “footprint” methods, we propose a parameter-free footprint method, named AOI-shapes, to capture the boundary information of a user-defined urban AOI. Next, to allow interactive query refinements by the user, we propose two efficient and scalable algorithms to incrementally generate urban AOIs by reusing existing visualization results. Finally, we conduct extensive experiments with both synthetic and real-world datasets to demonstrate the quality and efficiency of the proposed methods.},
  archive      = {J_TIIS},
  doi          = {10.1145/3431817},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {8},
  number       = {3-4},
  pages        = {1-32},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {AOI-shapes: An efficient footprint algorithm to support visualization of user-defined urban areas of interest},
  volume       = {11},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A taxonomy of property measures to unify active learning and
human-centered approaches to data labeling. <em>TIIS</em>,
<em>11</em>(3-4), 1–42. (<a
href="https://doi.org/10.1145/3439333">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Strategies for selecting the next data instance to label, in service of generating labeled data for machine learning, have been considered separately in the machine learning literature on active learning and in the visual analytics literature on human-centered approaches. We propose a unified design space for instance selection strategies to support detailed and fine-grained analysis covering both of these perspectives. We identify a concise set of 15 properties, namely measureable characteristics of datasets or of machine learning models applied to them, that cover most of the strategies in these literatures. To quantify these properties, we introduce Property Measures (PM) as fine-grained building blocks that can be used to formalize instance selection strategies. In addition, we present a taxonomy of PMs to support the description, evaluation, and generation of PMs across four dimensions: machine learning (ML) Model Output , Instance Relations , Measure Functionality , and Measure Valence . We also create computational infrastructure to support qualitative visual data analysis: a visual analytics explainer for PMs built around an implementation of PMs using cascades of eight atomic functions. It supports eight analysis tasks, covering the analysis of datasets and ML models using visual comparison within and between PMs and groups of PMs, and over time during the interactive labeling process. We iteratively refined the PM taxonomy, the explainer, and the task abstraction in parallel with each other during a two-year formative process, and show evidence of their utility through a summative evaluation with the same infrastructure. This research builds a formal baseline for the better understanding of the commonalities and differences of instance selection strategies, which can serve as the stepping stone for the synthesis of novel strategies in future work.},
  archive      = {J_TIIS},
  doi          = {10.1145/3439333},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {8},
  number       = {3-4},
  pages        = {1-42},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {A taxonomy of property measures to unify active learning and human-centered approaches to data labeling},
  volume       = {11},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Developing conversational agents for use in criminal
investigations. <em>TIIS</em>, <em>11</em>(3-4), 1–35. (<a
href="https://doi.org/10.1145/3444369">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The adoption of artificial intelligence (AI) systems in environments that involve high risk and high consequence decision-making is severely hampered by critical design issues. These issues include system transparency and brittleness, where transparency relates to (i) the explainability of results and (ii) the ability of a user to inspect and verify system goals and constraints; and brittleness, (iii) the ability of a system to adapt to new user demands. Transparency is a particular concern for criminal intelligence analysis, where there are significant ethical and trust issues that arise when algorithmic and system processes are not adequately understood by a user. This prevents adoption of potentially useful technologies in policing environments. In this article, we present a novel approach to designing a conversational agent (CA) AI system for intelligence analysis that tackles these issues. We discuss the results and implications of three different studies; a Cognitive Task Analysis to understand analyst thinking when retrieving information in an investigation, Emergent Themes Analysis to understand the explanation needs of different system components, and an interactive experiment with a prototype conversational agent. Our prototype conversational agent, named Pan, demonstrates transparency provision and mitigates brittleness by evolving new CA intentions. We encode interactions with the CA with human factors principles for situation recognition and use interactive visual analytics to support analyst reasoning. Our approach enables complex AI systems, such as Pan, to be used in sensitive environments, and our research has broader application than the use case discussed.},
  archive      = {J_TIIS},
  doi          = {10.1145/3444369},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {8},
  number       = {3-4},
  pages        = {1-35},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {Developing conversational agents for use in criminal investigations},
  volume       = {11},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Effect of adaptive guidance and visualization literacy on
gaze attentive behaviors and sequential patterns on magazine-style
narrative visualizations. <em>TIIS</em>, <em>11</em>(3-4), 1–46. (<a
href="https://doi.org/10.1145/3447992">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the effectiveness of adaptive interventions at helping users process textual documents with embedded visualizations, a form of multimodal documents known as Magazine-Style Narrative Visualizations (MSNVs). The interventions are meant to dynamically highlight in the visualization the datapoints that are described in the textual sentence currently being read by the user, as captured by eye-tracking. These interventions were previously evaluated in two user studies that involved 98 participants reading excerpts of real-world MSNVs during a 1-hour session. Participants’ outcomes included their subjective feedback about the guidance, and well as their reading time and score on a set of comprehension questions. Results showed that the interventions can increase comprehension of the MSNV excerpts for users with lower levels of a cognitive skill known as visualization literacy. In this article, we aim to further investigate this result by leveraging eye-tracking to analyze in depth how the participants processed the interventions depending on their levels of visualization literacy. We first analyzed summative gaze metrics that capture how users process and integrate the key components of the narrative visualizations. Second, we mined the salient patterns in the users’ scanpaths to contextualize how users sequentially process these components. Results indicate that the interventions succeed in guiding attention to salient components of the narrative visualizations, especially by generating more transitions between key components of the visualization (i.e., datapoints, labels, and legend), as well as between the two modalities (text and visualization). We also show that the interventions help users with lower levels of visualization literacy to better map datapoints to the legend, which likely contributed to their improved comprehension of the documents. These findings shed light on how adaptive interventions help users with different levels of visualization literacy, informing the design of personalized narrative visualizations.},
  archive      = {J_TIIS},
  doi          = {10.1145/3447992},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {8},
  number       = {3-4},
  pages        = {1-46},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {Effect of adaptive guidance and visualization literacy on gaze attentive behaviors and sequential patterns on magazine-style narrative visualizations},
  volume       = {11},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). After-action review for AI (AAR/AI). <em>TIIS</em>,
<em>11</em>(3-4), 1–35. (<a
href="https://doi.org/10.1145/3453173">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Explainable AI is growing in importance as AI pervades modern society, but few have studied how explainable AI can directly support people trying to assess an AI agent. Without a rigorous process, people may approach assessment in ad hoc ways—leading to the possibility of wide variations in assessment of the same agent due only to variations in their processes. AAR, or After-Action Review, is a method some military organizations use to assess human agents, and it has been validated in many domains. Drawing upon this strategy, we derived an After-Action Review for AI (AAR/AI), to organize ways people assess reinforcement learning agents in a sequential decision-making environment. We then investigated what AAR/AI brought to human assessors in two qualitative studies. The first investigated AAR/AI to gather formative information, and the second built upon the results, and also varied the type of explanation (model-free vs. model-based) used in the AAR/AI process. Among the results were the following: (1) participants reporting that AAR/AI helped to organize their thoughts and think logically about the agent, (2) AAR/AI encouraged participants to reason about the agent from a wide range of perspectives , and (3) participants were able to leverage AAR/AI with the model-based explanations to falsify the agent’s predictions.},
  archive      = {J_TIIS},
  doi          = {10.1145/3453173},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {8},
  number       = {3-4},
  pages        = {1-35},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {After-action review for AI (AAR/AI)},
  volume       = {11},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Promoting energy-efficient behavior by depicting social
norms in a recommender interface. <em>TIIS</em>, <em>11</em>(3-4), 1–32.
(<a href="https://doi.org/10.1145/3460005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How can recommender interfaces help users to adopt new behaviors? In the behavioral change literature, social norms and other nudges are studied to understand how people can be convinced to take action (e.g., towel re-use is boosted when stating that “75% of hotel guests” do so), but most of these nudges are not personalized. In contrast, recommender systems know what to recommend in a personalized way, but not much human-computer interaction ( HCI ) research has considered how personalized advice should be presented to help users to change their current habits. We examine the value of depicting normative messages (e.g., “75% of users do X”), based on actual user data, in a personalized energy recommender interface called “Saving Aid.” In a study among 207 smart thermostat owners, we compared three different normative explanations (“Global.” “Similar,” and “Experienced” norm rates) to a non-social baseline (“kWh savings”). Although none of the norms increased the total number of chosen measures directly, we show that depicting high peer adoption rates alongside energy-saving measures increased the likelihood that they would be chosen from a list of recommendations. In addition, we show that depicting social norms positively affects a user’s evaluation of a recommender interface.},
  archive      = {J_TIIS},
  doi          = {10.1145/3460005},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {8},
  number       = {3-4},
  pages        = {1-32},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {Promoting energy-efficient behavior by depicting social norms in a recommender interface},
  volume       = {11},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learn, generate, rank, explain: A case study of visual
explanation by generative machine learning. <em>TIIS</em>,
<em>11</em>(3-4), 1–34. (<a
href="https://doi.org/10.1145/3465407">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While the computer vision problem of searching for activities in videos is usually addressed by using discriminative models, their decisions tend to be opaque and difficult for people to understand. We propose a case study of a novel machine learning approach for generative searching and ranking of motion capture activities with visual explanation. Instead of directly ranking videos in the database given a text query, our approach uses a variant of Generative Adversarial Networks (GANs) to generate exemplars based on the query and uses them to search for the activity of interest in a large database. Our model is able to achieve comparable results to its discriminative counterpart, while being able to dynamically generate visual explanations. In addition to our searching and ranking method, we present an explanation interface that enables the user to successfully explore the model’s explanations and its confidence by revealing query-based, model-generated motion capture clips that contributed to the model’s decision. Finally, we conducted a user study with 44 participants to show that by using our model and interface, participants benefit from a deeper understanding of the model’s conceptualization of the search query. We discovered that the XAI system yielded a comparable level of efficiency, accuracy, and user-machine synchronization as its black-box counterpart, if the user exhibited a high level of trust for AI explanation.},
  archive      = {J_TIIS},
  doi          = {10.1145/3465407},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {8},
  number       = {3-4},
  pages        = {1-34},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {Learn, generate, rank, explain: A case study of visual explanation by generative machine learning},
  volume       = {11},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Special issue on interactive visual analytics for making
explainable and accountable decisions. <em>TIIS</em>, <em>11</em>(3-4),
1–4. (<a href="https://doi.org/10.1145/3471903">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_TIIS},
  doi          = {10.1145/3471903},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {8},
  number       = {3-4},
  pages        = {1-4},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {Special issue on interactive visual analytics for making explainable and accountable decisions},
  volume       = {11},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Humanized recommender systems: State-of-the-art and research
issues. <em>TIIS</em>, <em>11</em>(2), 1–41. (<a
href="https://doi.org/10.1145/3446906">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Psychological factors such as personality, emotions, social connections , and decision biases can significantly affect the outcome of a decision process. These factors are also prevalent in the existing literature related to the inclusion of psychological aspects in recommender system development. Personality and emotions of users have strong connections with their interests and decision-making behavior. Hence, integrating these factors into recommender systems can help to better predict users’ item preferences and increase the satisfaction with recommended items. In scenarios where decisions are made by groups (e.g., selecting a tourism destination to visit with friends), group composition and social connections among group members can affect the outcome of a group decision. Decision biases often occur in a recommendation process, since users usually apply heuristics when making a decision. These biases can result in low-quality decisions. In this article, we provide a rigorous review of existing research on the influence of the mentioned psychological factors on recommender systems. These factors are not only considered in single-user recommendation scenarios but, importantly, also in group recommendation ones, where groups of users are involved in a decision-making process. We include working examples to provide a deeper understanding of how to take into account these factors in recommendation processes. The provided examples go beyond single-user recommendation scenarios by also considering specific aspects of group recommendation settings.},
  archive      = {J_TIIS},
  doi          = {10.1145/3446906},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {7},
  number       = {2},
  pages        = {1-41},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {Humanized recommender systems: State-of-the-art and research issues},
  volume       = {11},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Understanding, discovering, and mitigating habitual
smartphone use in young adults. <em>TIIS</em>, <em>11</em>(2), 1–34. (<a
href="https://doi.org/10.1145/3447991">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {People, especially young adults, often use their smartphones out of habit: They compulsively browse social networks, check emails, and play video-games with little or no awareness at all. While previous studies analyzed this phenomena qualitatively , e.g., by showing that users perceive it as meaningless and addictive, yet our understanding of how to discover smartphone habits and mitigate their disruptive effects is limited. Being able to automatically assess habitual smartphone use, in particular, might have different applications, e.g., to design better “digital wellbeing” solutions for mitigating meaningless habitual use. To close this gap, we first define a data analytic methodology based on clustering and association rules mining to automatically discover complex smartphone habits from mobile usage data. We assess the methodology over more than 130,000 phone usage sessions collected from users aged between 16 and 33, and we show evidence that smartphone habits of young adults can be characterized by various types of links between contextual situations and usage sessions, which are highly diversified and differently perceived across users. We then apply the proposed methodology in Socialize, a digital wellbeing app that (i) monitors habitual smartphone behaviors in real time and (ii) uses proactive notifications and just-in-time reminders to encourage users to avoid any identified smartphone habits they consider as meaningless. An in-the-wild study with 20 users (ages 19–31) demonstrates that Socialize can assist young adults in better controlling their smartphone usage with a significant reduction of their unwanted smartphone habits.},
  archive      = {J_TIIS},
  doi          = {10.1145/3447991},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {7},
  number       = {2},
  pages        = {1-34},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {Understanding, discovering, and mitigating habitual smartphone use in young adults},
  volume       = {11},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Expressive cognitive architecture for a curious social
robot. <em>TIIS</em>, <em>11</em>(2), 1–25. (<a
href="https://doi.org/10.1145/3451531">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial curiosity, based on developmental psychology concepts wherein an agent attempts to maximize its learning progress, has gained much attention in recent years. Similarly, social robots are slowly integrating into our daily lives, in schools, factories, and in our homes. In this contribution, we integrate recent advances in artificial curiosity and social robots into a single expressive cognitive architecture. It is composed of artificial curiosity and social expressivity modules and their unique link, i.e., the robot verbally and non-verbally communicates its internally estimated learning progress, or learnability, to its human companion. We implemented this architecture in an interaction where a fully autonomous robot took turns with a child trying to select and solve tangram puzzles on a tablet. During the curious robot’s turn, it selected its estimated most learnable tangram to play, communicated its selection to the child, and then attempted at solving it. We validated the implemented architecture and showed that the robot learned, estimated its learnability, and improved when its selection was based on its learnability estimation. Moreover, we ran a comparison study between curious and non-curious robots, and showed that the robot’s curiosity-based behavior influenced the child’s selections. Based on the artificial curiosity module of the robot, we have formulated an equation that estimates each child’s moment-by-moment curiosity based on their selections. This analysis revealed an overall significant decrease in estimated curiosity during the interaction. However, this drop in estimated curiosity was significantly larger with the non-curious robot, compared to the curious one. These results suggest that the new architecture is a promising new approach to integrate state-of-the-art curiosity-based algorithms to the growing field of social robots.},
  archive      = {J_TIIS},
  doi          = {10.1145/3451531},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {7},
  number       = {2},
  pages        = {1-25},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {Expressive cognitive architecture for a curious social robot},
  volume       = {11},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Artificial intelligence for modeling complex systems: Taming
the complexity of expert models to improve decision making.
<em>TIIS</em>, <em>11</em>(2), 1–49. (<a
href="https://doi.org/10.1145/3453172">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Major societal and environmental challenges involve complex systems that have diverse multi-scale interacting processes. Consider, for example, how droughts and water reserves affect crop production and how agriculture and industrial needs affect water quality and availability. Preventive measures, such as delaying planting dates and adopting new agricultural practices in response to changing weather patterns, can reduce the damage caused by natural processes. Understanding how these natural and human processes affect one another allows forecasting the effects of undesirable situations and study interventions to take preventive measures. For many of these processes, there are expert models that incorporate state-of-the-art theories and knowledge to quantify a system&#39;s response to a diversity of conditions. A major challenge for efficient modeling is the diversity of modeling approaches across disciplines and the wide variety of data sources available only in formats that require complex conversions. Using expert models for particular problems requires integration of models with third-party data as well as integration of models across disciplines. Modelers face significant heterogeneity that requires resolving semantic, spatiotemporal, and execution mismatches, which are largely done by hand today and may take more than 2 years of effort. We are developing a modeling framework that uses artificial intelligence (AI) techniques to reduce modeling effort while ensuring utility for decision making. Our work to date makes several innovative contributions: (1) an intelligent user interface that guides analysts to frame their modeling problem and assists them by suggesting relevant choices and automating steps along the way; (2) semantic metadata for models, including their modeling variables and constraints, that ensures model relevance and proper use for a given decision-making problem; and (3) semantic representations of datasets in terms of modeling variables that enable automated data selection and data transformations. This framework is implemented in the MINT (Model INTegration) framework, and currently includes data and models to analyze the interactions between natural and human systems involving climate, water availability, agricultural production, and markets. Our work to date demonstrates the utility of AI techniques to accelerate modeling to support decision-making and uncovers several challenging directions for future work.},
  archive      = {J_TIIS},
  doi          = {10.1145/3453172},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {7},
  number       = {2},
  pages        = {1-49},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {Artificial intelligence for modeling complex systems: Taming the complexity of expert models to improve decision making},
  volume       = {11},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Socially aware navigation: A non-linear multi-objective
optimization approach. <em>TIIS</em>, <em>11</em>(2), 1–26. (<a
href="https://doi.org/10.1145/3453445">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile robots are increasingly populating homes, hospitals, shopping malls, factory floors, and other human environments. Human society has social norms that people mutually accept; obeying these norms is an essential signal that someone is participating socially with respect to the rest of the population. For robots to be socially compatible with humans, it is crucial for robots to obey these social norms. In prior work, we demonstrated a Socially-Aware Navigation (SAN) planner, based on Pareto Concavity Elimination Transformation (PaCcET), in a hallway scenario, optimizing two objectives so the robot does not invade the personal space of people. This article extends our PaCcET-based SAN planner to multiple scenarios with more than two objectives. We modified the Robot Operating System’s (ROS) navigation stack to include PaCcET in the local planning task. We show that our approach can accommodate multiple Human-Robot Interaction (HRI) scenarios. Using the proposed approach, we achieved successful HRI in multiple scenarios such as hallway interactions, an art gallery, waiting in a queue, and interacting with a group. We implemented our method on a simulated PR2 robot in a 2D simulator (Stage) and a pioneer-3DX mobile robot in the real-world to validate all the scenarios. A comprehensive set of experiments shows that our approach can handle multiple interaction scenarios on both holonomic and non-holonomic robots; hence, it can be a viable option for a Unified Socially-Aware Navigation (USAN).},
  archive      = {J_TIIS},
  doi          = {10.1145/3453445},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {7},
  number       = {2},
  pages        = {1-26},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {Socially aware navigation: A non-linear multi-objective optimization approach},
  volume       = {11},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Emerging ExG-based NUI inputs in extended realities: A
bottom-up survey. <em>TIIS</em>, <em>11</em>(2), 1–49. (<a
href="https://doi.org/10.1145/3457950">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Incremental and quantitative improvements of two-way interactions with e x tended realities (XR) are contributing toward a qualitative leap into a state of XR ecosystems being efficient, user-friendly, and widely adopted. However, there are multiple barriers on the way toward the omnipresence of XR; among them are the following: computational and power limitations of portable hardware, social acceptance of novel interaction protocols, and usability and efficiency of interfaces. In this article, we overview and analyse novel natural user interfaces based on sensing electrical bio-signals that can be leveraged to tackle the challenges of XR input interactions. Electroencephalography-based brain-machine interfaces that enable thought-only hands-free interaction, myoelectric input methods that track body gestures employing electromyography, and gaze-tracking electrooculography input interfaces are the examples of electrical bio-signal sensing technologies united under a collective concept of ExG. ExG signal acquisition modalities provide a way to interact with computing systems using natural intuitive actions enriching interactions with XR. This survey will provide a bottom-up overview starting from (i) underlying biological aspects and signal acquisition techniques, (ii) ExG hardware solutions, (iii) ExG-enabled applications, (iv) discussion on social acceptance of such applications and technologies, as well as (v) research challenges, application directions, and open problems; evidencing the benefits that ExG-based Natural User Interfaces inputs can introduce to the area of XR.},
  archive      = {J_TIIS},
  doi          = {10.1145/3457950},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {7},
  number       = {2},
  pages        = {1-49},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {Emerging ExG-based NUI inputs in extended realities: A bottom-up survey},
  volume       = {11},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Photo sequences of varying emotion: Optimization with a
valence-arousal annotated dataset. <em>TIIS</em>, <em>11</em>(2), 1–19.
(<a href="https://doi.org/10.1145/3458844">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Synthesizing photo products such as photo strips and slideshows using a database of images is a time-consuming and tedious process that requires significant manual work. To overcome this limitation, we developed a method that automatically synthesizes photo sequences based on several design parameters. Our method considers the valence and arousal ratings of images in conjunction with parameters related to both the visual consistency of the synthesized photo sequence and the progression of valence and arousal throughout the photo sequence. Our method encodes valence, arousal, and visual consistency parameters as cost terms into a total cost function while applying a Markov chain Monte Carlo optimization techniques called simulated annealing to synthesize the photo sequence based on user-defined target objectives in a few seconds. As our method was developed for the synthesis of photo sequences using the valence-arousal emotional model, a user study was conducted to evaluate the efficacy of the synthesized photo sequences in triggering valence-arousal ratings as expected. Our results indicate that the proposed method synthesizes photo sequences in which valence and arousal dimensions are perceived as expected by participants; however, valence may be more appropriately perceived than arousal.},
  archive      = {J_TIIS},
  doi          = {10.1145/3458844},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {7},
  number       = {2},
  pages        = {1-19},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {Photo sequences of varying emotion: Optimization with a valence-arousal annotated dataset},
  volume       = {11},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Predicting visual search task success from eye gaze data as
a basis for user-adaptive information visualization systems.
<em>TIIS</em>, <em>11</em>(2), 1–25. (<a
href="https://doi.org/10.1145/3446638">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Information visualizations are an efficient means to support the users in understanding large amounts of complex, interconnected data; user comprehension, however, depends on individual factors such as their cognitive abilities. The research literature provides evidence that user-adaptive information visualizations positively impact the users’ performance in visualization tasks. This study attempts to contribute toward the development of a computational model to predict the users’ success in visual search tasks from eye gaze data and thereby drive such user-adaptive systems. State-of-the-art deep learning models for time series classification have been trained on sequential eye gaze data obtained from 40 study participants’ interaction with a circular and an organizational graph. The results suggest that such models yield higher accuracy than a baseline classifier and previously used models for this purpose. In particular, a Multivariate Long Short Term Memory Fully Convolutional Network shows encouraging performance for its use in online user-adaptive systems. Given this finding, such a computational model can infer the users’ need for support during interaction with a graph and trigger appropriate interventions in user-adaptive information visualization systems. This facilitates the design of such systems since further interaction data like mouse clicks is not required.},
  archive      = {J_TIIS},
  doi          = {10.1145/3446638},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {5},
  number       = {2},
  pages        = {1-25},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {Predicting visual search task success from eye gaze data as a basis for user-adaptive information visualization systems},
  volume       = {11},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Exploring the role of common model of cognition in designing
adaptive coaching interactions for health behavior change.
<em>TIIS</em>, <em>11</em>(1), 1–30. (<a
href="https://doi.org/10.1145/3375790">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Our research aims to develop intelligent collaborative agents that are human-aware : They can model, learn, and reason about their human partner’s physiological, cognitive, and affective states. In this article, we study how adaptive coaching interactions can be designed to help people develop sustainable healthy behaviors. We leverage the common model of cognition (CMC) [31] as a framework for unifying several behavior change theories that are known to be useful in human–human coaching. We motivate a set of interactive system desiderata based on the CMC-based view of behavior change. Then, we propose PARCoach, an interactive system that addresses the desiderata. PARCoach helps a trainee pick a relevant health goal, set an implementation intention, and track their behavior. During this process, the trainee identifies a specific goal-directed behavior as well as the situational context in which they will perform it. PARCCoach uses this information to send notifications to the trainee, reminding them of their chosen behavior and the context. We report the results from a 4-week deployment with 60 participants. Our results support the CMC-based view of behavior change and demonstrate that the desiderata for proposed interactive system design is useful in producing behavior change.},
  archive      = {J_TIIS},
  doi          = {10.1145/3375790},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {4},
  number       = {1},
  pages        = {1-30},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {Exploring the role of common model of cognition in designing adaptive coaching interactions for health behavior change},
  volume       = {11},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). PRIME: A personalized recommender system for information
visualization methods via extended matrix completion. <em>TIIS</em>,
<em>11</em>(1), 1–30. (<a
href="https://doi.org/10.1145/3366484">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adapting user interface designs for specific tasks performed by different users is a challenging yet important problem. Automatically adapting visualization designs to users and contexts (e.g., tasks, display devices, environments, etc.) can theoretically improve human–computer interaction to acquire insights from complex datasets. However, effectiveness of any specific visualization is moderated by individual differences in knowledge, skills, and abilities for different contexts. A modeling framework called P ersonalized R ecommender System for I nformation visualization M ethods via E xtended matrix completion (PRIME) is proposed for recommending the optimal visualization designs for individual users in different contexts. PRIME quantitatively models covariates (e.g., psychological and behavioral measurements) to predict recommendation scores (e.g., perceived complexity, mental workload, etc.) for users to adapt the visualization specific to the context. An evaluation study was conducted and showed that PRIME can achieve satisfactory recommendation accuracy for adapting visualization, even when there are limited historical data. PRIME can make accurate recommendations even for new users or new tasks based on historical wearable sensor signals and recommendation scores. This capability contributes to designing a new generation of visualization systems that will adapt to users’ states. PRIME can support researchers in reducing the sample size requirements to quantify individual differences, and practitioners in adapting visualizations according to user states and contexts.},
  archive      = {J_TIIS},
  doi          = {10.1145/3366484},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {3},
  number       = {1},
  pages        = {1-30},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {PRIME: A personalized recommender system for information visualization methods via extended matrix completion},
  volume       = {11},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The shoutcasters, the game enthusiasts, and the AI: Foraging
for explanations of real-time strategy players. <em>TIIS</em>,
<em>11</em>(1), 1–46. (<a
href="https://doi.org/10.1145/3396047">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Assessing and understanding intelligent agents is a difficult task for users who lack an AI background. “Explainable AI” (XAI) aims to address this problem, but what should be in an explanation? One route toward answering this question is to turn to theories of how humans try to obtain information they seek. Information Foraging Theory (IFT) is one such theory. In this article, we present a series of studies 1 using IFT: the first investigates how expert explainers supply explanations in the RTS domain, the second investigates what explanations domain experts demand from agents in the RTS domain, and the last focuses on how both populations try to explain a state-of-the-art AI. Our results show that RTS environments like StarCraft offer so many options that change so rapidly, foraging tends to be very costly. Ways foragers attempted to manage such costs included “satisficing” approaches to reduce their cognitive load, such as focusing more on What information than on Why information, strategic use of language to communicate a lot of nuanced information in a few words, and optimizing their environment when possible to make their most valuable information patches readily available. Further, when a real AI entered the picture, even very experienced domain experts had difficulty understanding and judging some of the AI’s unconventional behaviors. Finally, our results reveal ways Information Foraging Theory can inform future XAI interactive explanation environments, and also how XAI can inform IFT.},
  archive      = {J_TIIS},
  doi          = {10.1145/3396047},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {3},
  number       = {1},
  pages        = {1-46},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {The shoutcasters, the game enthusiasts, and the AI: Foraging for explanations of real-time strategy players},
  volume       = {11},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On the detection of structural aesthetic defects of android
mobile user interfaces with a metrics-based tool. <em>TIIS</em>,
<em>11</em>(1), 1–27. (<a
href="https://doi.org/10.1145/3410468">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Smartphone users are striving for easy-to-learn and use mobile apps user interfaces. Accomplishing these qualities demands an iterative evaluation of the Mobile User Interface (MUI). Several studies stress the value of providing a MUI with a pleasing look and feel to engaging end-users. The MUI, therefore, needs to be free from all kinds of structural aesthetic defects. Such defects are indicators of poor design decisions interfering with the consistency of a MUI and making it more difficult to use. To this end, we are proposing a tool (Aesthetic Defects DEtection Tool (ADDET)) to determine the structural aesthetic dimension of MUIs. Automating this process is useful to designers in evaluating the quality of their designs. Our approach is composed of two modules. (1) Metrics assessment is based on the static analysis of a tree-structured layout of the MUI. We used 15 geometric metrics (also known as structural or aesthetic metrics) to check various structural properties before a defect is triggered. (2) Defects detection: The manual combination of metrics and defects are time-consuming and user-dependent when determining a detection rule. Thus, we perceive the process of identification of defects as an optimization problem. We aim to automatically combine the metrics related to a particular defect and optimize the accuracy of the rules created by assigning a weight, representing the metric importance in detecting a defect. We conducted a quantitative and qualitative analysis to evaluate the accuracy of the proposed tool in computing metrics and detecting defects. The findings affirm the tool’s reliability when assessing a MUI’s structural design problems with 71% accuracy.},
  archive      = {J_TIIS},
  doi          = {10.1145/3410468},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {3},
  number       = {1},
  pages        = {1-27},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {On the detection of structural aesthetic defects of android mobile user interfaces with a metrics-based tool},
  volume       = {11},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A real-time interactive visualizer for large classroom.
<em>TIIS</em>, <em>11</em>(1), 1–26. (<a
href="https://doi.org/10.1145/3418529">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In improving the teaching and learning experience in a classroom environment, it is crucial for a teacher to have a fair idea about the students who need help during a lecture. However, teachers of large classes usually face difficulties in identifying the students who are in a critical state. The current methods for classroom visualization are limited in showing both the status and location of a large number of students in a limited display area. Additionally, comprehension of the states adds cognitive load on the teacher working in a time-constrained classroom environment. In this article, we propose a two-level visualizer for large classrooms to address the challenges. In the first level, the visualizer generates a colored matrix representation of the classroom. The colored matrix is a quantitative illustration of the status of the class in terms of student clusters. We use three colors: red, yellow, and green, indicating the most critical, less critical, and the normal cluster on the screen, respectively. With tap/click on the first level, detailed information for a cluster is visualized as the second level. We conducted extensive studies for our visualizer in a simulated classroom with 12 tasks and 27 teacher participants. The results show that the visualizer is efficient and usable.},
  archive      = {J_TIIS},
  doi          = {10.1145/3418529},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {3},
  number       = {1},
  pages        = {1-26},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {A real-time interactive visualizer for large classroom},
  volume       = {11},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). I know what you know: What hand movements reveal about
domain expertise. <em>TIIS</em>, <em>11</em>(1), 1–26. (<a
href="https://doi.org/10.1145/3423049">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This research investigates whether students’ level of domain expertise can be detected during authentic learning activities by analyzing their physical activity patterns. More expert students reduced their manual activity by a substantial 50%, which was evident in fine-grained signal analyses and total rate of gesturing. The quality of experts’ discrete hand movements also averaged shorter in distance, briefer in duration, and slower in velocity than those of non-experts. Interestingly, experts adapted by nearly eliminating gestures on easier problems, while selectively increasing them on harder ones. They also strategically produced 62% more iconic gestures, which serve to retain spatial information in working memory while extracting inferences required to solve problems correctly. These findings highlight the close relation between hand movements and mental state and, more specifically, that hand movements provide an unusually clear window on students’ level of domain expertise. Embodied Cognition and Limited Resource theories only partially account for the present findings, which specify future directions for theoretical work.},
  archive      = {J_TIIS},
  doi          = {10.1145/3423049},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {3},
  number       = {1},
  pages        = {1-26},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {I know what you know: What hand movements reveal about domain expertise},
  volume       = {11},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Theoretical, measured, and subjective responsibility in
aided decision making. <em>TIIS</em>, <em>11</em>(1), 1–37. (<a
href="https://doi.org/10.1145/3425732">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When humans interact with intelligent systems, their causal responsibility for outcomes becomes equivocal. We analyze the descriptive abilities of a newly developed responsibility quantification model (ResQu) to predict actual human responsibility and perceptions of responsibility in the interaction with intelligent systems. In two laboratory experiments, participants performed a classification task. They were aided by classification systems with different capabilities. We compared the predicted theoretical responsibility values to the actual measured responsibility participants took on and to their subjective rankings of responsibility. The model predictions were strongly correlated with both measured and subjective responsibility. Participants’ behavior with each system was influenced by the system and human capabilities, but also by the subjective perceptions of these capabilities and the perception of the participant&#39;s own contribution. A bias existed only when participants with poor classification capabilities relied less than optimally on a system that had superior classification capabilities and assumed higher-than-optimal responsibility. The study implies that when humans interact with advanced intelligent systems, with capabilities that greatly exceed their own, their comparative causal responsibility will be small, even if formally the human is assigned major roles. Simply putting a human into the loop does not ensure that the human will meaningfully contribute to the outcomes. The results demonstrate the descriptive value of the ResQu model to predict behavior and perceptions of responsibility by considering the characteristics of the human, the intelligent system, the environment, and some systematic behavioral biases. The ResQu model is a new quantitative method that can be used in system design and can guide policy and legal decisions regarding human responsibility in events involving intelligent systems.},
  archive      = {J_TIIS},
  doi          = {10.1145/3425732},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {3},
  number       = {1},
  pages        = {1-37},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {Theoretical, measured, and subjective responsibility in aided decision making},
  volume       = {11},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Holistic transfer to rank for top-n recommendation.
<em>TIIS</em>, <em>11</em>(1), 1. (<a
href="https://doi.org/10.1145/3434360">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommender systems have been a valuable component in various online services such as e-commerce and entertainment. To provide an accurate top-N recommendation list of items for each target user, we have to answer a very basic question of how to model users’ feedback effectively. In this article, we focus on studying users’ explicit feedback, which is usually assumed to contain more preference information than the counterpart, i.e., implicit feedback. In particular, we follow two very recent transfer to rank algorithms by converting the original feedback to three different but related views of examinations, scores, and purchases, and then propose a novel solution called holistic transfer to rank (HoToR), which is able to address the uncertainty challenge and the inconvenience challenge in the existing works. More specifically, we take the rating scores as a weighting strategy to alleviate the uncertainty of the examinations, and we design a holistic one-stage solution to address the inconvenience of the two/three-stage training and prediction procedures in previous works. We then conduct extensive empirical studies in a direct comparison with the two closely related transfer learning algorithms and some very competitive factorization- and neighborhood-based methods on three public datasets and find that our HoToR performs significantly better than the other methods in terms of several ranking-oriented evaluation metrics.},
  archive      = {J_TIIS},
  doi          = {10.1145/3434360},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {3},
  number       = {1},
  pages        = {1},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {Holistic transfer to rank for top-N recommendation},
  volume       = {11},
  year         = {2021},
}
</textarea>
</details></li>
</ul>

</body>
</html>
