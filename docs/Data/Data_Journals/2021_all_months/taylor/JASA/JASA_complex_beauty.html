<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>JASA_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="jasa---178">JASA - 178</h2>
<ul>
<li><details>
<summary>
(2021). Balancing inferential integrity and disclosure risk via
model targeted masking and multiple imputation. <em>JASA</em>,
<em>117</em>(537), 52–66. (<a
href="https://doi.org/10.1080/01621459.2021.1909597">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is a growing expectation that data collected by government-funded studies should be openly available to ensure research reproducibility, which also increases concerns about data privacy. A strategy to protect individuals’ identity is to release multiply imputed (MI) synthetic datasets with masked sensitivity values. However, information loss or incorrectly specified imputation models can weaken or invalidate the inferences obtained from the MI-datasets. We propose a new masking framework with a data-augmentation (DA) component and a tuning mechanism that balances protecting identity disclosure against preserving data utility. Applying it to a restricted-use Canadian Scleroderma Research Group (CSRG) dataset, we found that this DA-MI strategy achieved a 0\% identity disclosure risk and preserved all inferential conclusions. It yielded 95\% confidence intervals (CIs) that had overlaps of 98.5\% (95.5\%) on average with the CIs constructed using the full, unmasked CSRG dataset in a work-disability (interstitial lung disease) study. The CI-overlaps were lower for several other methods considered, ranging from 73.9\% to 91.9\% on average with the lowest value being 28.1\%; such low CI-overlaps further led to some incorrect inferential conclusions. These findings indicate that the DA-MI masking framework facilitates sharing of useful research data while protecting participants’ identities. Supplementary materials for this article, including a standardized description of the materials available for reproducing the work, are available as an online supplement.},
  archive      = {J_JASA},
  author       = {Bei Jiang and Adrian E. Raftery and Russell J. Steele and Naisyin Wang},
  doi          = {10.1080/01621459.2021.1909597},
  journal      = {Journal of the American Statistical Association},
  number       = {537},
  pages        = {52-66},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Balancing inferential integrity and disclosure risk via model targeted masking and multiple imputation},
  volume       = {117},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Correction. <em>JASA</em>, <em>116</em>(536), 2100. (<a
href="https://doi.org/10.1080/01621459.2021.1969237">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  doi          = {10.1080/01621459.2021.1969237},
  journal      = {Journal of the American Statistical Association},
  number       = {536},
  pages        = {2100},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Correction},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Advanced survival models. <em>JASA</em>, <em>116</em>(536),
2098–2099. (<a
href="https://doi.org/10.1080/01621459.2021.1997014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Sangwook Kang},
  doi          = {10.1080/01621459.2021.1997014},
  journal      = {Journal of the American Statistical Association},
  number       = {536},
  pages        = {2098-2099},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Advanced survival models},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). What are the most important statistical ideas of the past 50
years? <em>JASA</em>, <em>116</em>(536), 2087–2097. (<a
href="https://doi.org/10.1080/01621459.2021.1938081">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We review the most important statistical ideas of the past half century, which we categorize as: counterfactual causal inference, bootstrapping and simulation-based inference, overparameterized models and regularization, Bayesian multilevel models, generic computation algorithms, adaptive decision analysis, robust inference, and exploratory data analysis. We discuss key contributions in these subfields, how they relate to modern computing and big data, and how they might be developed and extended in future decades. The goal of this article is to provoke thought and discussion regarding the larger themes of research in statistics and data science.},
  archive      = {J_JASA},
  author       = {Andrew Gelman and Aki Vehtari},
  doi          = {10.1080/01621459.2021.1938081},
  journal      = {Journal of the American Statistical Association},
  number       = {536},
  pages        = {2087-2097},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {What are the most important statistical ideas of the past 50 years?},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sampling algorithms for discrete markov random fields and
related graphical models. <em>JASA</em>, <em>116</em>(536), 2065–2086.
(<a href="https://doi.org/10.1080/01621459.2021.1898410">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Discrete Markov random fields are undirected graphical models in which the nodes of a graph are discrete random variables with values usually represented by colors. Typically, graphs are taken to be square lattices, although more general graphs are also of interest. Such discrete MRFs have been studied in many disciplines. We describe the two most popular types of discrete MRFs, namely the two-state Ising model and the q -state Potts model, and variations such as the cellular automaton, the cellular Potts model, and the random cluster model, the latter of which is a continuous generalization of both the Ising and Potts models. Research interest is usually focused on providing algorithms for simulating from these models because the partition function is so computationally intractable that statistical inference for the parameters of the appropriate probability distribution becomes very complicated. Substantial improvements to the Metropolis algorithm have appeared in the form of cluster algorithms, such as the Swendsen–Wang and Wolff algorithms. We study the simulation processes of these algorithms, which update the color of a cluster of nodes at each iteration.},
  archive      = {J_JASA},
  author       = {Alan Julian Izenman},
  doi          = {10.1080/01621459.2021.1898410},
  journal      = {Journal of the American Statistical Association},
  number       = {536},
  pages        = {2065-2086},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Sampling algorithms for discrete markov random fields and related graphical models},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The impact of churn on client value in health insurance,
evaluation using a random forest under various censoring mechanisms.
<em>JASA</em>, <em>116</em>(536), 2053–2064. (<a
href="https://doi.org/10.1080/01621459.2020.1764364">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract– In the insurance broker market, commissions received by brokers are closely related to so-called “customer value”: the longer a policyholder keeps their contract, the more profit there is for the company and therefore the broker. Hence, predicting the time at which a potential policyholder will surrender their contract is essential to optimize a commercial process and define a prospect scoring. In this article, we propose a weighted random forest model to address this problem. Our model is designed to compensate for the impact of random censoring. We investigate different types of assumptions on the censoring, studying both the cases where it is independent or not from the covariates. We compare our approach with other standard methods which apply in our setting, using simulated and real data analysis. We show that our approach is very competitive in terms of quadratic error in addressing the given problem. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Guillaume Gerber and Yohann Le Faou and Olivier Lopez and Michael Trupin},
  doi          = {10.1080/01621459.2020.1764364},
  journal      = {Journal of the American Statistical Association},
  number       = {536},
  pages        = {2053-2064},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {The impact of churn on client value in health insurance, evaluation using a random forest under various censoring mechanisms},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Efficiently backtesting conditional value-at-risk and
conditional expected shortfall. <em>JASA</em>, <em>116</em>(536),
2041–2052. (<a
href="https://doi.org/10.1080/01621459.2020.1763804">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract– Given the importance of backtesting risk models and forecasts for financial institutions and regulators, we develop an efficient empirical likelihood backtest for either conditional value-at-risk or conditional expected shortfall when the given risk variable is modeled by an ARMA-GARCH process. Using a two-step procedure, the proposed backtests require less finite moments than existing backtests, allowing for robustness to heavier tails. Furthermore, we add a constraint on the goodness of fit of the error distribution to provide more accurate risk forecasts and improved test power. A simulation study confirms the good finite sample performance of the new backtests, and empirical analyses demonstrate the usefulness of these efficient backtests for monitoring financial crises.},
  archive      = {J_JASA},
  author       = {Qihui Su and Zhongling Qin and Liang Peng and Gengsheng Qin},
  doi          = {10.1080/01621459.2020.1763804},
  journal      = {Journal of the American Statistical Association},
  number       = {536},
  pages        = {2041-2052},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Efficiently backtesting conditional value-at-risk and conditional expected shortfall},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Modeling network populations via graph distances.
<em>JASA</em>, <em>116</em>(536), 2023–2040. (<a
href="https://doi.org/10.1080/01621459.2020.1763803">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article introduces a new class of models for multiple networks. The core idea is to parameterize a distribution on labeled graphs in terms of a Fréchet mean graph (which depends on a user-specified choice of metric or graph distance) and a parameter that controls the concentration of this distribution about its mean. Entropy is the natural parameter for such control, varying from a point mass concentrated on the Fréchet mean itself to a uniform distribution over all graphs on a given vertex set. We provide a hierarchical Bayesian approach for exploiting this construction, along with straightforward strategies for sampling from the resultant posterior distribution. We conclude by demonstrating the efficacy of our approach via simulation studies and two multiple-network data analysis examples: one drawn from systems biology and the other from neuroscience. This article has online supplementary materials .},
  archive      = {J_JASA},
  author       = {Simón Lunagómez and Sofia C. Olhede and Patrick J. Wolfe},
  doi          = {10.1080/01621459.2020.1763803},
  journal      = {Journal of the American Statistical Association},
  number       = {536},
  pages        = {2023-2040},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Modeling network populations via graph distances},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A new coefficient of correlation. <em>JASA</em>,
<em>116</em>(536), 2009–2022. (<a
href="https://doi.org/10.1080/01621459.2020.1758115">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract– Is it possible to define a coefficient of correlation which is (a) as simple as the classical coefficients like Pearson’s correlation or Spearman’s correlation, and yet (b) consistently estimates some simple and interpretable measure of the degree of dependence between the variables, which is 0 if and only if the variables are independent and 1 if and only if one is a measurable function of the other, and (c) has a simple asymptotic theory under the hypothesis of independence, like the classical coefficients? This article answers this question in the affirmative, by producing such a coefficient. No assumptions are needed on the distributions of the variables. There are several coefficients in the literature that converge to 0 if and only if the variables are independent, but none that satisfy any of the other properties mentioned above. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Sourav Chatterjee},
  doi          = {10.1080/01621459.2020.1758115},
  journal      = {Journal of the American Statistical Association},
  number       = {536},
  pages        = {2009-2022},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {A new coefficient of correlation},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sparse learning and structure identification for
ultrahigh-dimensional image-on-scalar regression. <em>JASA</em>,
<em>116</em>(536), 1994–2008. (<a
href="https://doi.org/10.1080/01621459.2020.1753523">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article considers high-dimensional image-on-scalar regression, where the spatial heterogeneity of covariate effects on imaging responses is investigated via a flexible partially linear spatially varying coefficient model. To tackle the challenges of spatial smoothing over the imaging response’s complex domain consisting of regions of interest, we approximate the spatially varying coefficient functions via bivariate spline functions over triangulation. We first study estimation when the active constant coefficients and varying coefficient functions are known in advance. We then further develop a unified approach for simultaneous sparse learning and model structure identification in the presence of ultrahigh-dimensional covariates. Our method can identify zero, nonzero constant, and spatially varying components correctly and efficiently. The estimators of constant coefficients and varying coefficient functions are consistent and asymptotically normal for constant coefficient estimators. The method is evaluated by Monte Carlo simulation studies and applied to a dataset provided by the Alzheimer’s Disease Neuroimaging Initiative. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Xinyi Li and Li Wang and Huixia Judy Wang and for the Alzheimer’s Disease Neuroimaging Initiative},
  doi          = {10.1080/01621459.2020.1753523},
  journal      = {Journal of the American Statistical Association},
  number       = {536},
  pages        = {1994-2008},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Sparse learning and structure identification for ultrahigh-dimensional image-on-scalar regression},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Smooth backfitting of proportional hazards with
multiplicative components. <em>JASA</em>, <em>116</em>(536), 1983–1993.
(<a href="https://doi.org/10.1080/01621459.2020.1753520">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Smooth backfitting has proven to have a number of theoretical and practical advantages in structured regression. By projecting the data down onto the structured space of interest smooth backfitting provides a direct link between data and estimator. This article introduces the ideas of smooth backfitting to survival analysis in a proportional hazard model, where we assume an underlying conditional hazard with multiplicative components. We develop asymptotic theory for the estimator. In a comprehensive simulation study, we show that our smooth backfitting estimator successfully circumvents the curse of dimensionality and outperforms existing estimators. This is especially the case in difficult situations like high number of covariates and/or high correlation between the covariates, where other estimators tend to break down. We use the smooth backfitter in a practical application where we extend recent advances of in-sample forecasting methodology by allowing more information to be incorporated, while still obeying the structured requirements of in-sample forecasting. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Munir Hiabu and Enno Mammen and M. Dolores Martínez-Miranda and Jens P. Nielsen},
  doi          = {10.1080/01621459.2020.1753520},
  journal      = {Journal of the American Statistical Association},
  number       = {536},
  pages        = {1983-1993},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Smooth backfitting of proportional hazards with multiplicative components},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bayesian projected calibration of computer models.
<em>JASA</em>, <em>116</em>(536), 1965–1982. (<a
href="https://doi.org/10.1080/01621459.2020.1753519">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a Bayesian approach called the Bayesian projected calibration to address the problem of calibrating an imperfect computer model using observational data from an unknown complex physical system. The calibration parameter and the physical system are parameterized in an identifiable fashion via the L 2 -projection. The physical system is imposed a Gaussian process prior distribution, which naturally induces a prior distribution on the calibration parameter through the L 2 -projection constraint. The calibration parameter is estimated through its posterior distribution, serving as a natural and nonasymptotic approach for the uncertainty quantification. We provide rigorous large sample justifications of the proposed approach by establishing the asymptotic normality of the posterior of the calibration parameter with the efficient covariance matrix. In addition to the theoretical analysis, two convenient computational algorithms based on stochastic approximation are designed with strong theoretical support. Through extensive simulation studies and the analyses of two real-world datasets, we show that the proposed Bayesian projected calibration can accurately estimate the calibration parameters, calibrate the computer models well, and compare favorably to alternative approaches. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Fangzheng Xie and Yanxun Xu},
  doi          = {10.1080/01621459.2020.1753519},
  journal      = {Journal of the American Statistical Association},
  number       = {536},
  pages        = {1965-1982},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Bayesian projected calibration of computer models},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Spherical regression under mismatch corruption with
application to automated knowledge translation. <em>JASA</em>,
<em>116</em>(536), 1953–1964. (<a
href="https://doi.org/10.1080/01621459.2020.1752219">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by a series of applications in data integration, language translation, bioinformatics, and computer vision, we consider spherical regression with two sets of unit-length vectors when the data are corrupted by a small fraction of mismatch in the response-predictor pairs. We propose a three-step algorithm in which we initialize the parameters by solving an orthogonal Procrustes problem to estimate a translation matrix W ignoring the mismatch. We then estimate a mapping matrix aiming to correct the mismatch using hard-thresholding to induce sparsity, while incorporating potential group information. We eventually obtain a refined estimate for W by removing the estimated mismatched pairs. We derive the error bound for the initial estimate of W in both fixed and high-dimensional setting. We demonstrate that the refined estimate of W achieves an error rate that is as good as if no mismatch is present. We show that our mapping recovery method not only correctly distinguishes one-to-one and one-to-many correspondences, but also consistently identifies the matched pairs and estimates the weight vector for combined correspondence. We examine the finite sample performance of the proposed method via extensive simulation studies, and with application to the unsupervised translation of medical codes using electronic health records data. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Xu Shi and Xiaoou Li and Tianxi Cai},
  doi          = {10.1080/01621459.2020.1752219},
  journal      = {Journal of the American Statistical Association},
  number       = {536},
  pages        = {1953-1964},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Spherical regression under mismatch corruption with application to automated knowledge translation},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Extrapolating treatment effects in multi-cutoff regression
discontinuity designs. <em>JASA</em>, <em>116</em>(536), 1941–1952. (<a
href="https://doi.org/10.1080/01621459.2020.1751646">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract– In nonexperimental settings, the regression discontinuity (RD) design is one of the most credible identification strategies for program evaluation and causal inference. However, RD treatment effect estimands are necessarily local, making statistical methods for the extrapolation of these effects a key area for development. We introduce a new method for extrapolation of RD effects that relies on the presence of multiple cutoffs, and is therefore design-based. Our approach employs an easy-to-interpret identifying assumption that mimics the idea of “common trends” in difference-in-differences designs. We illustrate our methods with data on a subsidized loan program on post-education attendance in Colombia, and offer new evidence on program effects for students with test scores away from the cutoff that determined program eligibility. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Matias D. Cattaneo and Luke Keele and Rocío Titiunik and Gonzalo Vazquez-Bare},
  doi          = {10.1080/01621459.2020.1751646},
  journal      = {Journal of the American Statistical Association},
  number       = {536},
  pages        = {1941-1952},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Extrapolating treatment effects in multi-cutoff regression discontinuity designs},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Estimating mixed memberships with sharp eigenvector
deviations. <em>JASA</em>, <em>116</em>(536), 1928–1940. (<a
href="https://doi.org/10.1080/01621459.2020.1751645">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of estimating community memberships of nodes in a network, where every node is associated with a vector determining its degree of membership in each community. Existing provably consistent algorithms often require strong assumptions about the population, are computationally expensive, and only provide an overall error bound for the whole community membership matrix. This article provides uniform rates of convergence for the inferred community membership vector of each node in a network generated from the mixed membership stochastic blockmodel (MMSB); to our knowledge, this is the first work to establish per-node rates for overlapping community detection in networks. We achieve this by establishing sharp row-wise eigenvector deviation bounds for MMSB. Based on the simplex structure inherent in the eigen-decomposition of the population matrix, we build on established corner-finding algorithms from the optimization community to infer the community membership vectors. Our results hold over a broad parameter regime where the average degree only grows poly-logarithmically with the number of nodes. Using experiments with simulated and real datasets, we show that our method achieves better error with lower variability over competing methods, and processes real world networks of up to 100,000 nodes within tens of seconds. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Xueyu Mao and Purnamrita Sarkar and Deepayan Chakrabarti},
  doi          = {10.1080/01621459.2020.1751645},
  journal      = {Journal of the American Statistical Association},
  number       = {536},
  pages        = {1928-1940},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Estimating mixed memberships with sharp eigenvector deviations},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Integrating multisource block-wise missing data in model
selection. <em>JASA</em>, <em>116</em>(536), 1914–1927. (<a
href="https://doi.org/10.1080/01621459.2020.1751176">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For multisource data, blocks of variable information from certain sources are likely missing. Existing methods for handling missing data do not take structures of block-wise missing data into consideration. In this article, we propose a multiple block-wise imputation (MBI) approach, which incorporates imputations based on both complete and incomplete observations. Specifically, for a given missing pattern group, the imputations in MBI incorporate more samples from groups with fewer observed variables in addition to the group with complete observations. We propose to construct estimating equations based on all available information, and integrate informative estimating functions to achieve efficient estimators. We show that the proposed method has estimation and model selection consistency under both fixed-dimensional and high-dimensional settings. Moreover, the proposed estimator is asymptotically more efficient than the estimator based on a single imputation from complete observations only. In addition, the proposed method is not restricted to missing completely at random. Numerical studies and ADNI data application confirm that the proposed method outperforms existing variable selection methods under various missing mechanisms. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Fei Xue and Annie Qu},
  doi          = {10.1080/01621459.2020.1751176},
  journal      = {Journal of the American Statistical Association},
  number       = {536},
  pages        = {1914-1927},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Integrating multisource block-wise missing data in model selection},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Randomization tests for weak null hypotheses in randomized
experiments. <em>JASA</em>, <em>116</em>(536), 1898–1913. (<a
href="https://doi.org/10.1080/01621459.2020.1750415">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Fisher randomization test (FRT) is appropriate for any test statistic, under a sharp null hypothesis that can recover all missing potential outcomes. However, it is often sought after to test a weak null hypothesis that the treatment does not affect the units on average. To use the FRT for a weak null hypothesis, we must address two issues. First, we need to impute the missing potential outcomes although the weak null hypothesis cannot determine all of them. Second, we need to choose a proper test statistic. For a general weak null hypothesis, we propose an approach to imputing missing potential outcomes under a compatible sharp null hypothesis. Building on this imputation scheme, we advocate a studentized statistic. The resulting FRT has multiple desirable features. First, it is model-free. Second, it is finite-sample exact under the sharp null hypothesis that we use to impute the potential outcomes. Third, it conservatively controls large-sample Type I error under the weak null hypothesis of interest. Therefore, our FRT is agnostic to the treatment effect heterogeneity. We establish a unified theory for general factorial experiments and extend it to stratified and clustered experiments. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Jason Wu and Peng Ding},
  doi          = {10.1080/01621459.2020.1750415},
  journal      = {Journal of the American Statistical Association},
  number       = {536},
  pages        = {1898-1913},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Randomization tests for weak null hypotheses in randomized experiments},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Depth for curve data and applications. <em>JASA</em>,
<em>116</em>(536), 1881–1897. (<a
href="https://doi.org/10.1080/01621459.2020.1745815">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In 1975, John W. Tukey defined statistical data depth as a function that determines the centrality of an arbitrary point with respect to a data cloud or to a probability measure. During the last decades, this seminal idea of data depth evolved into a powerful tool proving to be useful in various fields of science. Recently, extending the notion of data depth to the functional setting attracted a lot of attention among theoretical and applied statisticians. We go further and suggest a notion of data depth suitable for data represented as curves, or trajectories, which is independent of the parameterization. We show that our curve depth satisfies theoretical requirements of general depth functions that are meaningful for trajectories. We apply our methodology to diffusion tensor brain images and also to pattern recognition of handwritten digits and letters. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Pierre Lafaye de Micheaux and Pavlo Mozharovskyi and Myriam Vimond},
  doi          = {10.1080/01621459.2020.1745815},
  journal      = {Journal of the American Statistical Association},
  number       = {536},
  pages        = {1881-1897},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Depth for curve data and applications},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Prediction intervals for synthetic control methods.
<em>JASA</em>, <em>116</em>(536), 1865–1880. (<a
href="https://doi.org/10.1080/01621459.2021.1979561">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Uncertainty quantification is a fundamental problem in the analysis and interpretation of synthetic control (SC) methods. We develop conditional prediction intervals in the SC framework, and provide conditions under which these intervals offer finite-sample probability guarantees. Our method allows for covariate adjustment and nonstationary data. The construction begins by noting that the statistical uncertainty of the SC prediction is governed by two distinct sources of randomness: one coming from the construction of the (likely misspecified) SC weights in the pretreatment period, and the other coming from the unobservable stochastic error in the post-treatment period when the treatment effect is analyzed. Accordingly, our proposed prediction intervals are constructed taking into account both sources of randomness. For implementation, we propose a simulation-based approach along with finite-sample-based probability bound arguments, naturally leading to principled sensitivity analysis methods. We illustrate the numerical performance of our methods using empirical applications and a small simulation study. Python, R and Stata software packages implementing our methodology are available. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Matias D. Cattaneo and Yingjie Feng and Rocio Titiunik},
  doi          = {10.1080/01621459.2021.1979561},
  journal      = {Journal of the American Statistical Association},
  number       = {536},
  pages        = {1865-1880},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Prediction intervals for synthetic control methods},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An exact and robust conformal inference method for
counterfactual and synthetic controls. <em>JASA</em>, <em>116</em>(536),
1849–1864. (<a
href="https://doi.org/10.1080/01621459.2021.1920957">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce new inference procedures for counterfactual and synthetic control methods for policy evaluation. We recast the causal inference problem as a counterfactual prediction and a structural breaks testing problem. This allows us to exploit insights from conformal prediction and structural breaks testing to develop permutation inference procedures that accommodate modern high-dimensional estimators, are valid under weak and easy-to-verify conditions, and are provably robust against misspecification. Our methods work in conjunction with many different approaches for predicting counterfactual mean outcomes in the absence of the policy intervention. Examples include synthetic controls, difference-in-differences, factor and matrix completion models, and (fused) time series panel data models. Our approach demonstrates an excellent small-sample performance in simulations and is taken to a data application where we re-evaluate the consequences of decriminalizing indoor prostitution. Open-source software for implementing our conformal inference methods is available.},
  archive      = {J_JASA},
  author       = {Victor Chernozhukov and Kaspar Wüthrich and Yinchu Zhu},
  doi          = {10.1080/01621459.2021.1920957},
  journal      = {Journal of the American Statistical Association},
  number       = {536},
  pages        = {1849-1864},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {An exact and robust conformal inference method for counterfactual and synthetic controls},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Randomization tests in observational studies with staggered
adoption of treatment. <em>JASA</em>, <em>116</em>(536), 1835–1848. (<a
href="https://doi.org/10.1080/01621459.2021.1974458">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article considers the problem of inference in observational studies with time-varying adoption of treatment. In addition to an unconfoundedness assumption that the potential outcomes are independent of the times at which units adopt treatment conditional on the units’ observed characteristics, our analysis assumes that the time at which each unit adopts treatment follows a Cox proportional hazards model. This assumption permits the time at which each unit adopts treatment to depend on the observed characteristics of the unit, but imposes the restriction that the probability of multiple units adopting treatment at the same time is zero. In this context, we study randomization tests of a null hypothesis that specifies that there is no treatment effect for all units and all time periods in a distributional sense. We first show that an infeasible test that treats the parameters of the Cox model as known has rejection probability under the null hypothesis no greater than the nominal level in finite samples. Since these parameters are unknown in practice, this result motivates a feasible test that replaces these parameters with consistent estimators. While the resulting test does not need to have the same finite-sample validity as the infeasible test, we show that it has limiting rejection probability under the null hypothesis no greater than the nominal level. In a simulation study, we examine the practical relevance of our theoretical results, including robustness to misspecification of the model for the time at which each unit adopts treatment. Finally, we provide an empirical application of our methodology using the synthetic control-based test statistic and tobacco legislation data found in Abadie, Diamond and Hainmueller. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Azeem M. Shaikh and Panos Toulis},
  doi          = {10.1080/01621459.2021.1974458},
  journal      = {Journal of the American Statistical Association},
  number       = {536},
  pages        = {1835-1848},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Randomization tests in observational studies with staggered adoption of treatment},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A penalized synthetic control estimator for disaggregated
data. <em>JASA</em>, <em>116</em>(536), 1817–1834. (<a
href="https://doi.org/10.1080/01621459.2021.1971535">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Synthetic control methods are commonly applied in empirical research to estimate the effects of treatments or interventions on aggregate outcomes. A synthetic control estimator compares the outcome of a treated unit to the outcome of a weighted average of untreated units that best resembles the characteristics of the treated unit before the intervention. When disaggregated data are available, constructing separate synthetic controls for each treated unit may help avoid interpolation biases. However, the problem of finding a synthetic control that best reproduces the characteristics of a treated unit may not have a unique solution. Multiplicity of solutions is a particularly daunting challenge when the data include many treated and untreated units. To address this challenge, we propose a synthetic control estimator that penalizes the pairwise discrepancies between the characteristics of the treated units and the characteristics of the units that contribute to their synthetic controls. The penalization parameter trades off pairwise matching discrepancies with respect to the characteristics of each unit in the synthetic control against matching discrepancies with respect to the characteristics of the synthetic control unit as a whole. We study the properties of this estimator and propose data-driven choices of the penalization parameter.},
  archive      = {J_JASA},
  author       = {Alberto Abadie and Jérémy L’Hour},
  doi          = {10.1080/01621459.2021.1971535},
  journal      = {Journal of the American Statistical Association},
  number       = {536},
  pages        = {1817-1834},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {A penalized synthetic control estimator for disaggregated data},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Combining matching and synthetic control to tradeoff biases
from extrapolation and interpolation. <em>JASA</em>, <em>116</em>(536),
1804–1816. (<a
href="https://doi.org/10.1080/01621459.2021.1979562">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The synthetic control (SC) method is widely used in comparative case studies to adjust for differences in pretreatment characteristics. SC limits extrapolation bias at the potential expense of interpolation bias, whereas traditional matching estimators have the opposite properties. This complementarity motives us to propose a matching and synthetic control (or MASC) estimator as a model averaging estimator that combines the standard SC and matching estimators. We show how to use a rolling-origin cross-validation procedure to train the MASC to resolve tradeoffs between interpolation and extrapolation bias. We use a series of empirically based placebo and Monte Carlo simulations to shed light on when the SC, matching, MASC and penalized SC estimators do (and do not) perform well. Then, we apply these estimators to examine the economic costs of conflicts in the context of Spain.},
  archive      = {J_JASA},
  author       = {Maxwell Kellogg and Magne Mogstad and Guillaume A. Pouliot and Alexander Torgovitsky},
  doi          = {10.1080/01621459.2021.1979562},
  journal      = {Journal of the American Statistical Association},
  number       = {536},
  pages        = {1804-1816},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Combining matching and synthetic control to tradeoff biases from extrapolation and interpolation},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The augmented synthetic control method. <em>JASA</em>,
<em>116</em>(536), 1789–1803. (<a
href="https://doi.org/10.1080/01621459.2021.1929245">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The synthetic control method (SCM) is a popular approach for estimating the impact of a treatment on a single unit in panel data settings. The “synthetic control” is a weighted average of control units that balances the treated unit’s pretreatment outcomes and other covariates as closely as possible. A critical feature of the original proposal is to use SCM only when the fit on pretreatment outcomes is excellent. We propose Augmented SCM as an extension of SCM to settings where such pretreatment fit is infeasible. Analogous to bias correction for inexact matching, augmented SCM uses an outcome model to estimate the bias due to imperfect pretreatment fit and then de-biases the original SCM estimate. Our main proposal, which uses ridge regression as the outcome model, directly controls pretreatment fit while minimizing extrapolation from the convex hull. This estimator can also be expressed as a solution to a modified synthetic controls problem that allows negative weights on some donor units. We bound the estimation error of this approach under different data-generating processes, including a linear factor model, and show how regularization helps to avoid over-fitting to noise. We demonstrate gains from Augmented SCM with extensive simulation studies and apply this framework to estimate the impact of the 2012 Kansas tax cuts on economic growth. We implement the proposed method in the new augsynth R package.},
  archive      = {J_JASA},
  author       = {Eli Ben-Michael and Avi Feller and Jesse Rothstein},
  doi          = {10.1080/01621459.2021.1929245},
  journal      = {Journal of the American Statistical Association},
  number       = {536},
  pages        = {1789-1803},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {The augmented synthetic control method},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Counterfactual analysis with artificial controls: Inference,
high dimensions, and nonstationarity. <em>JASA</em>, <em>116</em>(536),
1773–1788. (<a
href="https://doi.org/10.1080/01621459.2021.1964978">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, there has been growing interest in developing statistical tools to conduct counterfactual analysis with aggregate data when a single “treated” unit suffers an intervention, such as a policy change, and there is no obvious control group. Usually, the proposed methods are based on the construction of an artificial counterfactual from a pool of “untre ated” peers, organized in a panel data structure. In this article, we consider a general framework for counterfactual analysis for high-dimensional, nonstationary data with either deterministic and/or stochastic trends, which nests well-established methods, such as the synthetic control. We propose a resampling procedure to test intervention effects that does not rely on postintervention asymptotics and that can be used even if there is only a single observation after the intervention. A simulation study is provided as well as an empirical application. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Ricardo Masini and Marcelo C. Medeiros},
  doi          = {10.1080/01621459.2021.1964978},
  journal      = {Journal of the American Statistical Association},
  number       = {536},
  pages        = {1773-1788},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Counterfactual analysis with artificial controls: Inference, high dimensions, and nonstationarity},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On the properties of the synthetic control estimator with
many periods and many controls. <em>JASA</em>, <em>116</em>(536),
1764–1772. (<a
href="https://doi.org/10.1080/01621459.2021.1965613">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the asymptotic properties of the synthetic control (SC) estimator when both the number of pretreatment periods and control units are large. If potential outcomes follow a linear factor model, we provide conditions under which the SC unit asymptotically recovers the factor structure of the treated unit, even when the pretreatment fit is imperfect. This happens when there are weights diluted among an increasing number of control units such that a weighted average of the factor structure of the control units asymptotically reconstructs the factor structure of the treated unit. In this case, the SC estimator is asymptotically unbiased even when treatment assignment is correlated with time-varying unobservables. Supplementary materials for this article, including a standardized description of the materials available for reproducing the work, are available as an online supplement.},
  archive      = {J_JASA},
  author       = {Bruno Ferman},
  doi          = {10.1080/01621459.2021.1965613},
  journal      = {Journal of the American Statistical Association},
  number       = {536},
  pages        = {1764-1772},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {On the properties of the synthetic control estimator with many periods and many controls},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Matrix completion, counterfactuals, and factor analysis of
missing data. <em>JASA</em>, <em>116</em>(536), 1746–1763. (<a
href="https://doi.org/10.1080/01621459.2021.1967163">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes an imputation procedure that uses the factors estimated from a tall block along with the re-rotated loadings estimated from a wide block to impute missing values in a panel of data. Assuming that a strong factor structure holds for the full panel of data and its sub-blocks, it is shown that the common component can be consistently estimated at four different rates of convergence without requiring regularization or iteration. An asymptotic analysis of the estimation error is obtained. An application of our analysis is estimation of counterfactuals when potential outcomes have a factor structure. We study the estimation of average and individual treatment effects on the treated and establish a normal distribution theory that can be useful for hypothesis testing.},
  archive      = {J_JASA},
  author       = {Jushan Bai and Serena Ng},
  doi          = {10.1080/01621459.2021.1967163},
  journal      = {Journal of the American Statistical Association},
  number       = {536},
  pages        = {1746-1763},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Matrix completion, counterfactuals, and factor analysis of missing data},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On robustness of principal component regression.
<em>JASA</em>, <em>116</em>(536), 1731–1745. (<a
href="https://doi.org/10.1080/01621459.2021.1928513">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Principal component regression (PCR) is a simple, but powerful and ubiquitously utilized method. Its effectiveness is well established when the covariates exhibit low-rank structure. However, its ability to handle settings with noisy, missing, and mixed-valued, that is, discrete and continuous, covariates is not understood and remains an important open challenge. As the main contribution of this work, we establish the robustness of PCR, without any change, in this respect and provide meaningful finite-sample analysis. To do so, we establish that PCR is equivalent to performing linear regression after preprocessing the covariate matrix via hard singular value thresholding (HSVT). As a result, in the context of counterfactual analysis using observational data, we show PCR is equivalent to the recently proposed robust variant of the synthetic control method, known as robust synthetic control (RSC). As an immediate consequence, we obtain finite-sample analysis of the RSC estimator that was previously absent. As an important contribution to the synthetic controls literature, we establish that an (approximate) linear synthetic control exists in the setting of a generalized factor model, or latent variable model; traditionally in the literature, the existence of a synthetic control needs to be assumed to exist as an axiom. We further discuss a surprising implication of the robustness property of PCR with respect to noise, that is, PCR can learn a good predictive model even if the covariates are tactfully transformed to preserve differential privacy. Finally, this work advances the state-of-the-art analysis for HSVT by establishing stronger guarantees with respect to the l 2 , ∞ -norm rather than the Frobenius norm as is commonly done in the matrix estimation literature, which may be of interest in its own right.},
  archive      = {J_JASA},
  author       = {Anish Agarwal and Devavrat Shah and Dennis Shen and Dogyoon Song},
  doi          = {10.1080/01621459.2021.1928513},
  journal      = {Journal of the American Statistical Association},
  number       = {536},
  pages        = {1731-1745},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {On robustness of principal component regression},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Matrix completion methods for causal panel data models.
<em>JASA</em>, <em>116</em>(536), 1716–1730. (<a
href="https://doi.org/10.1080/01621459.2021.1891924">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we study methods for estimating causal effects in settings with panel data, where some units are exposed to a treatment during some periods and the goal is estimating counterfactual (untreated) outcomes for the treated unit/period combinations. We propose a class of matrix completion estimators that uses the observed elements of the matrix of control outcomes corresponding to untreated unit/periods to impute the “missing” elements of the control outcome matrix, corresponding to treated units/periods. This leads to a matrix that well-approximates the original (incomplete) matrix, but has lower complexity according to the nuclear norm for matrices. We generalize results from the matrix completion literature by allowing the patterns of missing data to have a time series dependency structure that is common in social science applications. We present novel insights concerning the connections between the matrix completion literature, the literature on interactive fixed effects models and the literatures on program evaluation under unconfoundedness and synthetic control methods. We show that all these estimators can be viewed as focusing on the same objective function. They differ solely in the way they deal with identification, in some cases solely through regularization (our proposed nuclear norm matrix completion estimator) and in other cases primarily through imposing hard restrictions (the unconfoundedness and synthetic control approaches). The proposed method outperforms unconfoundedness-based or synthetic control estimators in simulations based on real data.},
  archive      = {J_JASA},
  author       = {Susan Athey and Mohsen Bayati and Nikolay Doudchenko and Guido Imbens and Khashayar Khosravi},
  doi          = {10.1080/01621459.2021.1891924},
  journal      = {Journal of the American Statistical Association},
  number       = {536},
  pages        = {1716-1730},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Matrix completion methods for causal panel data models},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Introduction to the special section on synthetic control
methods. <em>JASA</em>, <em>116</em>(536), 1713–1715. (<a
href="https://doi.org/10.1080/01621459.2021.2002600">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Alberto Abadie and Matias D. Cattaneo},
  doi          = {10.1080/01621459.2021.2002600},
  journal      = {Journal of the American Statistical Association},
  number       = {536},
  pages        = {1713-1715},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Introduction to the special section on synthetic control methods},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A bayesian hierarchical CACE model accounting for incomplete
noncompliance with application to a meta-analysis of epidural analgesia
on cesarean section. <em>JASA</em>, <em>116</em>(536), 1700–1712. (<a
href="https://doi.org/10.1080/01621459.2021.1900859">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Noncompliance with assigned treatments is a common challenge in analyzing and interpreting randomized clinical trials (RCTs). One way to handle noncompliance is to estimate the complier-average causal effect (CACE), the intervention’s efficacy in the subpopulation that complies with assigned treatment. In a two-step meta-analysis, one could first estimate CACE for each study, then combine them to estimate the population-averaged CACE. However, when some trials do not report noncompliance data, the two-step meta-analysis can be less efficient and potentially biased by excluding these trials. This article proposes a flexible Bayesian hierarchical CACE framework to simultaneously account for heterogeneous and incomplete noncompliance data in a meta-analysis of RCTs. The models are motivated by and used for a meta-analysis estimating the CACE of epidural analgesia on cesarean section, in which only 10 of 27 trials reported complete noncompliance data. The new analysis includes all 27 studies and the results present new insights on the causal effect after accounting for noncompliance. Compared to the estimated risk difference of 0.8\% (95\% CI: –0.3\%, 1.9\%) given by the two-step intention-to-treat meta-analysis, the estimated CACE is 4.1\% (95\% CrI: –0.3\%, 10.5\%). We also report simulation studies to evaluate the performance of the proposed method. Supplementary materials for this article, including a standardized description of the materials available for reproducing the work, are available as an online supplement.},
  archive      = {J_JASA},
  author       = {Jincheng Zhou and James S. Hodges and Haitao Chu},
  doi          = {10.1080/01621459.2021.1900859},
  journal      = {Journal of the American Statistical Association},
  number       = {536},
  pages        = {1700-1712},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {A bayesian hierarchical CACE model accounting for incomplete noncompliance with application to a meta-analysis of epidural analgesia on cesarean section},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Graph-based equilibrium metrics for dynamic supply–demand
systems with applications to ride-sourcing platforms. <em>JASA</em>,
<em>116</em>(536), 1688–1699. (<a
href="https://doi.org/10.1080/01621459.2021.1898409">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How to dynamically measure the local-to-global spatio-temporal coherence between demand and supply networks is a fundamental task for ride-sourcing platforms, such as DiDi. Such coherence measurement is critically important for the quantification of the market efficiency and the comparison of different platform policies, such as dispatching. The aim of this paper is to introduce a graph-based equilibrium metric (GEM) to quantify the distance between demand and supply networks based on a weighted graph structure. We formulate GEM as the optimal objective value of an unbalanced optimal transport problem, which can be formulated as an equivalent linear programming and efficiently solved. We examine how the GEM can help solve three operational tasks of ride-sourcing platforms. The first one is that GEM achieves up to 70.6\% reduction in root-mean-square error over the second-best distance measurement for the prediction accuracy of order answer rate. The second one is that the use of GEM for designing order dispatching policy increases drivers’ revenue for more than 1\%, representing a huge improvement in number. The third one is that GEM can serve as an endpoint for comparing different platform policies in AB test. Supplementary materials for this article, including a standardized description of the materials available for reproducing the work, are available as an online supplement.},
  archive      = {J_JASA},
  author       = {Fan Zhou and Shikai Luo and Xiaohu Qie and Jieping Ye and Hongtu Zhu},
  doi          = {10.1080/01621459.2021.1898409},
  journal      = {Journal of the American Statistical Association},
  number       = {536},
  pages        = {1688-1699},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Graph-based equilibrium metrics for dynamic Supply–Demand systems with applications to ride-sourcing platforms},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Investigating clustering and violence interruption in
gang-related violent crime data using spatial–temporal point processes
with covariates. <em>JASA</em>, <em>116</em>(536), 1674–1687. (<a
href="https://doi.org/10.1080/01621459.2021.1898408">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reported gang-related violent crimes in Los Angeles, California, from 1/1/14 to 12/31/17 are modeled using spatial–temporal marked Hawkes point processes with covariates. We propose an algorithm to estimate the spatial-temporally varying background rate nonparametrically as a function of demographic covariates. Kernel smoothing and generalized additive models are used in an attempt to model the background rate as closely as possible in an effort to differentiate inhomogeneity in the background rate from causal clustering or triggering of events. The models are fit to data from 2014 to 2016 and evaluated using data from 2017, based on log-likelihood and superthinned residuals. The impact of nonrandomized violence interruption performed by The City of Los Angeles M ayor’s Office of Gang Reduction Youth Development (GRYD) Incident Response (IR) Program is assessed by comparing the triggering associated with GRYD IR Program events to the triggering associated with sub-sampled non-GRYD events selected to have a similar spatial–temporal distribution. The results suggest that GRYD IR Program violence interruption yields a reduction of approximately 18.3\% in the retaliation rate in locations more than 130 m from the original reported crimes, and a reduction of 14.2\% in retaliations within 130 m.},
  archive      = {J_JASA},
  author       = {Junhyung Park and Frederic Paik Schoenberg and Andrea L. Bertozzi and P. Jeffrey Brantingham},
  doi          = {10.1080/01621459.2021.1898408},
  journal      = {Journal of the American Statistical Association},
  number       = {536},
  pages        = {1674-1687},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Investigating clustering and violence interruption in gang-related violent crime data using Spatial–Temporal point processes with covariates},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Forecasting unemployment using internet search data via
PRISM. <em>JASA</em>, <em>116</em>(536), 1662–1673. (<a
href="https://doi.org/10.1080/01621459.2021.1883436">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Big data generated from the Internet offer great potential for predictive analysis. Here we focus on using online users’ Internet search data to forecast unemployment initial claims weeks into the future, which provides timely insights into the direction of the economy. To this end, we present a novel method Penalized Regression with Inferred Seasonality Module (PRISM), which uses publicly available online search data from Google. PRISM is a semiparametric method, motivated by a general state-space formulation, and employs nonparametric seasonal decomposition and penalized regression. For forecasting unemployment initial claims, PRISM outperforms all previously available methods, including forecasting during the 2008–2009 financial crisis period and near-future forecasting during the COVID-19 pandemic period, when unemployment initial claims both rose rapidly. The timely and accurate unemployment forecasts by PRISM could aid government agencies and financial institutions to assess the economic trend and make well-informed decisions, especially in the face of economic turbulence.},
  archive      = {J_JASA},
  author       = {Dingdong Yi and Shaoyang Ning and Chia-Jung Chang and S. C. Kou},
  doi          = {10.1080/01621459.2021.1883436},
  journal      = {Journal of the American Statistical Association},
  number       = {536},
  pages        = {1662-1673},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Forecasting unemployment using internet search data via PRISM},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A semiparametric kernel independence test with application
to mutational signatures. <em>JASA</em>, <em>116</em>(536), 1648–1661.
(<a href="https://doi.org/10.1080/01621459.2020.1871357">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cancers arise owing to somatic mutations, and the characteristic combinations of somatic mutations form mutational signatures. Despite many mutational signatures being identified, mutational processes underlying a number of mutational signatures remain unknown, which hinders the identification of interventions that may reduce somatic mutation burdens and prevent the development of cancer. We demonstrate that the unknown cause of a mutational signature can be inferred by the associated signatures with known etiology. However, existing association tests are not statistically powerful due to excess zeros in mutational signatures data. To address this limitation, we propose a semiparametric kernel independence test (SKIT). The SKIT statistic is defined as the integrated squared distance between mixed probability distributions and is decomposed into four disjoint components to pinpoint the source of dependency. We derive the asymptotic null distribution and prove the asymptotic convergence of power. Due to slow convergence to the asymptotic null distribution, a bootstrap method is employed to compute p -values. Simulation studies demonstrate that when zeros are prevalent, SKIT is more resilient to power loss than existing tests and robust to random errors. We applied SKIT to The Cancer Genome Atlas mutational signatures data for over 9000 tumors across 32 cancer types, and identified a novel association between signature 17 curated in the Catalogue of Somatic Mutations in Cancer and apolipoprotein B mRNA editing enzyme (APOBEC) signatures in gastrointestinal cancers. It indicates that APOBEC activity is likely associated with the unknown cause of signature 17. Supplementary materials for this article, including a standardized description of the materials available for reproducing the work, are available as an online supplement.},
  archive      = {J_JASA},
  author       = {DongHyuk Lee and Bin Zhu},
  doi          = {10.1080/01621459.2020.1871357},
  journal      = {Journal of the American Statistical Association},
  number       = {536},
  pages        = {1648-1661},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {A semiparametric kernel independence test with application to mutational signatures},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A bayesian state-space approach to mapping directional brain
networks. <em>JASA</em>, <em>116</em>(536), 1637–1647. (<a
href="https://doi.org/10.1080/01621459.2020.1865985">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The human brain is a directional network system of brain regions involving directional connectivity. Seizures are a directional network phenomenon as abnormal neuronal activities start from a seizure onset zone (SOZ) and propagate to otherwise healthy regions. To localize the SOZ of an epileptic patient, clinicians use intracranial electroencephalography (iEEG) to record the patient’s intracranial brain activity in many small regions. iEEG data are high-dimensional multivariate time series. We build a state-space multivariate autoregression (SSMAR) for iEEG data to model the underlying directional brain network. To produce scientifically interpretable network results, we incorporate into the SSMAR the scientific knowledge that the underlying brain network tends to have a cluster structure. Specifically, we assign to the SSMAR parameters a stochastic-blockmodel-motivated prior, which reflects the cluster structure. We develop a Bayesian framework to estimate the SSMAR, infer directional connections, and identify clusters for the unobserved network edges. The new method is robust to violations of model assumptions and outperforms existing network methods. By applying the new method to an epileptic patient’s iEEG data, we reveal seizure initiation and propagation in the patient’s directional brain network and discover a unique directional connectivity property of the SOZ. Overall, the network results obtained in this study bring new insights into epileptic patients’ normal and abnormal epileptic brain mechanisms and have the potential to assist neurologists and clinicians in localizing the SOZ—a long-standing research focus in epilepsy diagnosis and treatment. Supplementary materials for this article, including a standardized description of the materials available for reproducing the work, are available as an online supplement.},
  archive      = {J_JASA},
  author       = {Huazhang Li and Yaotian Wang and Guofen Yan and Yinge Sun and Seiji Tanabe and Chang-Chia Liu and Mark S. Quigg and Tingting Zhang},
  doi          = {10.1080/01621459.2020.1865985},
  journal      = {Journal of the American Statistical Association},
  number       = {536},
  pages        = {1637-1647},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {A bayesian state-space approach to mapping directional brain networks},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Biased encouragements and heterogeneous effects in an
instrumental variable study of emergency general surgical outcomes.
<em>JASA</em>, <em>116</em>(536), 1625–1636. (<a
href="https://doi.org/10.1080/01621459.2020.1863220">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate the efficacy of surgical versus nonsurgical management for two gastrointestinal conditions, colitis and diverticulitis, using observational data. We deploy an instrumental variable design with surgeons’ tendencies to operate as an instrument. Assuming instrument validity, we find that nonsurgical alternatives can reduce both hospital length of stay and the risk of complications, with estimated effects larger for septic patients than for nonseptic patients. The validity of our instrument is plausible but not ironclad, necessitating a sensitivity analysis. Existing sensitivity analyses for IV designs assume effect homogeneity, unlikely to hold here because of patient-specific physiology. We develop a new sensitivity analysis that accommodates arbitrary effect heterogeneity and exploits components explainable by observed features. We find that the results for nonseptic patients prove more robust to hidden bias despite having smaller estimated effects. For nonseptic patients, two individuals with identical observed characteristics would have to differ in their odds of assignment to a high tendency to operate surgeon by a factor of 2.34 to overturn our finding of a benefit for nonsurgical management in reducing length of stay. For septic patients, this value is only 1.64. Simulations illustrate that this phenomenon may be explained by differences in within-group heterogeneity. Supplementary materials for this article, including a standardized description of the materials available for reproducing the work, are available as an online supplement.},
  archive      = {J_JASA},
  author       = {Colin B. Fogarty and Kwonsang Lee and Rachel R. Kelz and Luke J. Keele},
  doi          = {10.1080/01621459.2020.1863220},
  journal      = {Journal of the American Statistical Association},
  number       = {536},
  pages        = {1625-1636},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Biased encouragements and heterogeneous effects in an instrumental variable study of emergency general surgical outcomes},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Topic modeling on triage notes with semiorthogonal
nonnegative matrix factorization. <em>JASA</em>, <em>116</em>(536),
1609–1624. (<a
href="https://doi.org/10.1080/01621459.2020.1862667">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emergency department (ED) crowding is a universal health issue that affects the efficiency of hospital management and patient care quality. ED crowding frequently occurs when a request for a ward-bed for a patient is delayed until a doctor makes an admission decision. In this case study, we build a classifier to predict the disposition of patients using manually typed nurse notes collected during triage as provided by the Alberta Medical Center. These predictions can potentially be incorporated to early bed coordination and fast track streaming strategies to alleviate overcrowding and waiting times in the ED. However, these triage notes involve high dimensional, noisy, and sparse text data, which make model-fitting and interpretation difficult. To address this issue, we propose a novel semiorthogonal nonnegative matrix factorization for both continuous and binary predictors to reduce the dimensionality and derive word topics. The triage notes can then be interpreted as a non-subtractive linear combination of orthogonal basis topic vectors. Our real data analysis shows that the triage notes contain strong predictive information toward classifying the disposition of patients for certain medical complaints, such as altered consciousness or stroke. Additionally, we show that the document-topic vectors generated by our method can be used as features to further improve classification accuracy by up to 1\% across different medical complaints, for example, 74.3\%–75.3\% accuracy for patients with stroke symptoms. This improvement could be clinically impactful for certain patients, especially when the scale of hospital patients is large. Furthermore, the generated word-topic vectors provide a bi-clustering interpretation under each topic due to the orthogonal formulation, which can be beneficial for hospitals in better understanding the symptoms and reasons behind patients’ visits. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Yutong Li and Ruoqing Zhu and Annie Qu and Han Ye and Zhankun Sun},
  doi          = {10.1080/01621459.2020.1862667},
  journal      = {Journal of the American Statistical Association},
  number       = {536},
  pages        = {1609-1624},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Topic modeling on triage notes with semiorthogonal nonnegative matrix factorization},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). IFAA: Robust association identification and inference for
absolute abundance in microbiome analyses. <em>JASA</em>,
<em>116</em>(536), 1595–1608. (<a
href="https://doi.org/10.1080/01621459.2020.1860770">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The target of inference in microbiome analyses is usually relative abundance (RA) because RA in a sample (e.g., stool) can be considered as an approximation of RA in an entire ecosystem (e.g., gut). However, inference on RA suffers from the fact that RA are calculated by dividing absolute abundances (AAs) over the common denominator (CD), the summation of all AA (i.e., library size). Because of that, perturbation in one taxon will result in a change in the CD and thus cause false changes in RA of all other taxa, and those false changes could lead to false positive/negative findings. We propose a novel analysis approach (IFAA) to make robust inference on AA of an ecosystem that can circumvent the issues induced by the CD problem and compositional structure of RA. IFAA can also address the issues of overdispersion and handle zero-inflated data structures. IFAA identifies microbial taxa associated with the covariates in Phase 1 and estimates the association parameters by employing an independent reference taxon in Phase 2. Two real data applications are presented and extensive simulations show that IFAA outperforms other established existing approaches by a big margin in the presence of unbalanced library size. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Zhigang Li and Lu Tian and A. James O’Malley and Margaret R. Karagas and Anne G. Hoen and Brock C. Christensen and Juliette C. Madan and Quran Wu and Raad Z. Gharaibeh and Christian Jobin and Hongzhe Li},
  doi          = {10.1080/01621459.2020.1860770},
  journal      = {Journal of the American Statistical Association},
  number       = {536},
  pages        = {1595-1608},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {IFAA: Robust association identification and inference for absolute abundance in microbiome analyses},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Rejoinder: Regression models for understanding COVID-19
epidemic dynamics with incomplete data. <em>JASA</em>,
<em>116</em>(536), 1591–1594. (<a
href="https://doi.org/10.1080/01621459.2021.2001340">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Corbin Quick and Rounak Dey and Xihong Lin},
  doi          = {10.1080/01621459.2021.2001340},
  journal      = {Journal of the American Statistical Association},
  number       = {536},
  pages        = {1591-1594},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Rejoinder: Regression models for understanding COVID-19 epidemic dynamics with incomplete data},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Discussion of “regression models for understanding COVID-19
epidemic dynamics with incomplete data.” <em>JASA</em>,
<em>116</em>(536), 1587–1590. (<a
href="https://doi.org/10.1080/01621459.2021.1982722">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Natalie Dean and Yang Yang},
  doi          = {10.1080/01621459.2021.1982722},
  journal      = {Journal of the American Statistical Association},
  number       = {536},
  pages        = {1587-1590},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Discussion of “Regression models for understanding COVID-19 epidemic dynamics with incomplete data”},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Discussion on “regression models for understanding COVID-19
epidemic dynamics with incomplete data.” <em>JASA</em>,
<em>116</em>(536), 1583–1586. (<a
href="https://doi.org/10.1080/01621459.2021.1982721">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Jyotishka Datta and Bhramar Mukherjee},
  doi          = {10.1080/01621459.2021.1982721},
  journal      = {Journal of the American Statistical Association},
  number       = {536},
  pages        = {1583-1586},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Discussion on “Regression models for understanding COVID-19 epidemic dynamics with incomplete data”},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Statistical models for COVID-19 incidence, cumulative
prevalence, and r t. <em>JASA</em>, <em>116</em>(536), 1578–1582. (<a
href="https://doi.org/10.1080/01621459.2021.1983436">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Nicholas P. Jewell},
  doi          = {10.1080/01621459.2021.1983436},
  journal      = {Journal of the American Statistical Association},
  number       = {536},
  pages        = {1578-1582},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Statistical models for COVID-19 incidence, cumulative prevalence, and r t},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Regression models for understanding COVID-19 epidemic
dynamics with incomplete data. <em>JASA</em>, <em>116</em>(536),
1561–1577. (<a
href="https://doi.org/10.1080/01621459.2021.2001339">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modeling infectious disease dynamics has been critical throughout the COVID-19 pandemic. Of particular interest are the incidence, prevalence, and effective reproductive number ( R t ). Estimating these quantities is challenging due to under-ascertainment, unreliable reporting, and time lags between infection, onset, and testing. We propose a Multilevel Epidemic Regression Model to Account for Incomplete Data (MERMAID) to jointly estimate R t , ascertainment rates, incidence, and prevalence over time in one or multiple regions. Specifically, MERMAID allows for a flexible regression model of R t that can incorporate geographic and time-varying covariates. To account for under-ascertainment, we (a) model the ascertainment probability over time as a function of testing metrics and (b) jointly model data on confirmed infections and population-based serological surveys. To account for delays between infection, onset, and reporting, we model stochastic lag times as missing data, and develop an EM algorithm to estimate the model parameters. We evaluate the performance of MERMAID in simulation studies, and assess its robustness by conducting sensitivity analyses in a range of scenarios of model misspecifications. We apply the proposed method to analyze COVID-19 daily confirmed infection counts, PCR testing data, and serological survey data across the United States. Based on our model, we estimate an overall COVID-19 prevalence of 12.5\% (ranging from 2.4\% in Maine to 20.2\% in New York) and an overall ascertainment rate of 45.5\% (ranging from 22.5\% in New York to 81.3\% in Rhode Island) in the United States from March to December 2020. Supplementary materials for this article, including a standardized description of the materials available for reproducing the work, are available as an online supplement.},
  archive      = {J_JASA},
  author       = {Corbin Quick and Rounak Dey and Xihong Lin},
  doi          = {10.1080/01621459.2021.2001339},
  journal      = {Journal of the American Statistical Association},
  number       = {536},
  pages        = {1561-1577},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Regression models for understanding COVID-19 epidemic dynamics with incomplete data},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Correction. <em>JASA</em>, <em>116</em>(535), 1560. (<a
href="https://doi.org/10.1080/01621459.2021.1957322">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  doi          = {10.1080/01621459.2021.1957322},
  journal      = {Journal of the American Statistical Association},
  number       = {535},
  pages        = {1560},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Correction},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Thirty years of the network scale-up method. <em>JASA</em>,
<em>116</em>(535), 1548–1559. (<a
href="https://doi.org/10.1080/01621459.2021.1935267">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimating the size of hard-to-reach populations is an important problem for many fields. The network scale-up method (NSUM) is a relatively new approach to estimate the size of these hard-to-reach populations by asking respondents the question, “How many X’s do you know,” where X is the population of interest (e.g., “How many female sex workers do you know?”). The answers to these questions form aggregated relational data (ARD). The NSUM has been used to estimate the size of a variety of subpopulations, including female sex workers, drug users, and even children who have been hospitalized for choking. Within the network scale-up methodology, there are a multitude of estimators for the size of the hidden population, including direct estimators, maximum likelihood estimators, and Bayesian estimators. In this article, we first provide an in-depth analysis of ARD properties and the techniques to collect the data. Then, we comprehensively review different estimation methods in terms of the assumptions behind each model, the relationships between the estimators, and the practical considerations of implementing the methods. We apply many of the models discussed in the review to one canonical dataset and compare their performance and unique features, presented in the supplementary materials . Finally, we provide a summary of the dominant methods and an extensive list of the applications, and discuss the open problems and potential research directions in this area.},
  archive      = {J_JASA},
  author       = {Ian Laga and Le Bao and Xiaoyue Niu},
  doi          = {10.1080/01621459.2021.1935267},
  journal      = {Journal of the American Statistical Association},
  number       = {535},
  pages        = {1548-1559},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Thirty years of the network scale-up method},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Count time series: A methodological review. <em>JASA</em>,
<em>116</em>(535), 1533–1547. (<a
href="https://doi.org/10.1080/01621459.2021.1904957">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A growing interest in non-Gaussian time series, particularly in series comprised of nonnegative integers (counts), is taking place in today’s statistics literature. Count series naturally arise in fields, such as agriculture, economics, epidemiology, finance, geology, meteorology, and sports. Unlike stationary Gaussian series where autoregressive moving-averages are the primary modeling vehicle, no single class of models dominates the count landscape. As such, the literature has evolved somewhat ad-hocly, with different model classes being developed to tackle specific situations. This article is an attempt to summarize the current state of count time series modeling. The article first reviews models having prescribed marginal distributions, including some recent developments. This is followed by a discussion of state-space approaches. Multivariate extensions of the methods are then studied and Bayesian approaches to the problem are considered. The intent is to inform researchers and practitioners about the various types of count time series models arising in the modern literature. While estimation issues are not pursued in detail, reference to this literature is made.},
  archive      = {J_JASA},
  author       = {Richard A. Davis and Konstantinos Fokianos and Scott H. Holan and Harry Joe and James Livsey and Robert Lund and Vladas Pipiras and Nalini Ravishanker},
  doi          = {10.1080/01621459.2021.1904957},
  journal      = {Journal of the American Statistical Association},
  number       = {535},
  pages        = {1533-1547},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Count time series: A methodological review},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bayesian factor analysis for inference on interactions.
<em>JASA</em>, <em>116</em>(535), 1521–1532. (<a
href="https://doi.org/10.1080/01621459.2020.1745813">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article is motivated by the problem of inference on interactions among chemical exposures impacting human health outcomes. Chemicals often co-occur in the environment or in synthetic mixtures and as a result exposure levels can be highly correlated. We propose a latent factor joint model, which includes shared factors in both the predictor and response components while assuming conditional independence. By including a quadratic regression in the latent variables in the response component, we induce flexible dimension reduction in characterizing main effects and interactions. We propose a Bayesian approach to inference under this factor analysis for interactions (FIN) framework. Through appropriate modifications of the factor modeling structure, FIN can accommodate higher order interactions. We evaluate the performance using a simulation study and data from the National Health and Nutrition Examination Survey. Code is available on GitHub. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Federico Ferrari and David B. Dunson},
  doi          = {10.1080/01621459.2020.1745813},
  journal      = {Journal of the American Statistical Association},
  number       = {535},
  pages        = {1521-1532},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Bayesian factor analysis for inference on interactions},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Training neural networks as learning data-adaptive kernels:
Provable representation and approximation benefits. <em>JASA</em>,
<em>116</em>(535), 1507–1520. (<a
href="https://doi.org/10.1080/01621459.2020.1745812">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Consider the problem: given the data pair ( x , y ) ( x , y ) (x,y) drawn from a population with f ∗ ( x ) = E [ y ∣ ∣ x = x ] f * ( x ) = E [ y | x = x ] f*(x)=E[y|x=x] , specify a neural network model and run gradient flow on the weights over time until reaching any stationarity. How does f t , the function computed by the neural network at time t , relate to f ∗ f * f* , in terms of approximation and representation? What are the provable benefits of the adaptive representation by neural networks compared to the pre-specified fixed basis representation in the classical nonparametric literature? We answer the above questions via a dynamic reproducing kernel Hilbert space (RKHS) approach indexed by the training process of neural networks. Firstly, we show that when reaching any local stationarity, gradient flow learns an adaptive RKHS representation and performs the global least-squares projection onto the adaptive RKHS, simultaneously. Secondly, we prove that as the RKHS is data-adaptive and task-specific, the residual for f * lies in a subspace that is potentially much smaller than the orthogonal complement of the RKHS. The result formalizes the representation and approximation benefits of neural networks. Lastly, we show that the neural network function computed by gradient flow converges to the kernel ridgeless regression with an adaptive kernel, in the limit of vanishing regularization. The adaptive kernel viewpoint provides new angles of studying the approximation, representation, generalization, and optimization advantages of neural networks.},
  archive      = {J_JASA},
  author       = {Xialiang Dou and Tengyuan Liang},
  doi          = {10.1080/01621459.2020.1745812},
  journal      = {Journal of the American Statistical Association},
  number       = {535},
  pages        = {1507-1520},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Training neural networks as learning data-adaptive kernels: Provable representation and approximation benefits},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Inference on selected subgroups in clinical trials.
<em>JASA</em>, <em>116</em>(535), 1498–1506. (<a
href="https://doi.org/10.1080/01621459.2020.1740096">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When existing clinical trial data suggest a promising subgroup, we must address the question of how good the selected subgroup really is. The usual statistical inference applied to the selected subgroup, assuming that the subgroup is chosen independent of the data, may lead to an overly optimistic evaluation of the selected subgroup. In this article, we address the issue of selection bias and develop a de-biasing bootstrap inference procedure for the best selected subgroup effect. The proposed inference procedure is model-free, easy to compute, and asymptotically sharp. We demonstrate the merit of our proposed method by reanalyzing the MONET1 trial and show that how the subgroup is selected post hoc should play an important role in any statistical analysis. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Xinzhou Guo and Xuming He},
  doi          = {10.1080/01621459.2020.1740096},
  journal      = {Journal of the American Statistical Association},
  number       = {535},
  pages        = {1498-1506},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Inference on selected subgroups in clinical trials},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-goal prior selection: A way to reconcile bayesian and
classical approaches for random effects models. <em>JASA</em>,
<em>116</em>(535), 1487–1497. (<a
href="https://doi.org/10.1080/01621459.2020.1737532">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The two-level normal hierarchical model has played an important role in statistical theory and applications. In this article, we first introduce a general adjusted maximum likelihood method for estimating the unknown variance component of the model and the associated empirical best linear unbiased predictor of the random effects. We then discuss a new idea for selecting prior for the hyperparameters. The prior, called a multi-goal prior, produces Bayesian solutions for hyperparmeters and random effects that match (in the higher order asymptotic sense) the corresponding classical solution in linear mixed model with respect to several properties. Moreover, we establish for the first time an analytical equivalence of the posterior variances under the proposed multi-goal prior and the corresponding parametric bootstrap second-order mean squared error estimates in the context of a random effects model.},
  archive      = {J_JASA},
  author       = {Masayo Y. Hirose and Partha Lahiri},
  doi          = {10.1080/01621459.2020.1737532},
  journal      = {Journal of the American Statistical Association},
  number       = {535},
  pages        = {1487-1497},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Multi-goal prior selection: A way to reconcile bayesian and classical approaches for random effects models},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Targeted inference involving high-dimensional data using
nuisance penalized regression. <em>JASA</em>, <em>116</em>(535),
1472–1486. (<a
href="https://doi.org/10.1080/01621459.2020.1737079">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Analysis of high-dimensional data has received considerable and increasing attention in statistics. In practice, we may not be interested in every variable that is observed. Instead, often some of the variables are of particular interest, and the remaining variables are nuisance. To this end, we propose the nuisance penalized regression which does not penalize the parameters of interest. When the coherence between interest parameters and nuisance parameters is negligible, we show that resulting estimator can be directly used for inference without any correction. When the coherence is not negligible, we propose an iterative procedure to further refine the estimate of interest parameters, based on which we propose a modified profile likelihood based statistic for hypothesis testing. The utilities of our general results are demonstrated in three specific examples. Numerical studies lend further support to our method.},
  archive      = {J_JASA},
  author       = {Qiang Sun and Heping Zhang},
  doi          = {10.1080/01621459.2020.1737079},
  journal      = {Journal of the American Statistical Association},
  number       = {535},
  pages        = {1472-1486},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Targeted inference involving high-dimensional data using nuisance penalized regression},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Analogues on the sphere of the affine-equivariant spatial
median. <em>JASA</em>, <em>116</em>(535), 1457–1471. (<a
href="https://doi.org/10.1080/01621459.2020.1733582">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robust estimation of location for data on the unit sphere 𝒮 p − 1 S p − 1 Sp−1 is an important problem in directional statistics even though the influence functions of the sample mean direction and other location estimators are bounded. A significant limitation of previous literature on this topic is that robust estimators and procedures have been developed under the assumption that the underlying population is rotationally symmetric. This assumption often does not hold with real data and in these cases there is a needless loss of efficiency in the estimator. In this article, we propose two estimators for spherical data, both of which are analogous to the affine-equivariant spatial median in Euclidean space. The influence functions of the new location estimators are obtained under a new semiparametric elliptical symmetry model on the sphere and are shown to be standardized bias robust in the highly concentrated case; the influence function of the companion scatter matrix is also obtained. An iterative algorithm that computes both estimators is described. Asymptotic results, including consistency and asymptotic normality, are also derived for the location estimators that result from applying a fixed number of steps in this algorithm. Numerical studies demonstrate that both location estimators may be expected to perform well in practice in terms of efficiency and robustness. A brief example application from the geophysics literature is also provided. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Janice L. Scealy and Andrew T. A. Wood},
  doi          = {10.1080/01621459.2020.1733582},
  journal      = {Journal of the American Statistical Association},
  number       = {535},
  pages        = {1457-1471},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Analogues on the sphere of the affine-equivariant spatial median},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Principal component analysis of spatially indexed functions.
<em>JASA</em>, <em>116</em>(535), 1444–1456. (<a
href="https://doi.org/10.1080/01621459.2020.1732395">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop an expansion, similar in some respects to the Karhunen–Loève expansion, but which is more suitable for functional data indexed by spatial locations on a grid. Unlike the traditional Karhunen–Loève expansion, it takes into account the spatial dependence between the functions. By doing so, it provides a more efficient dimension reduction tool, both theoretically and in finite samples, for functional data with moderate spatial dependence. For such data, it also possesses other theoretical and practical advantages over the currently used approach. The article develops complete asymptotic theory and estimation methodology. The performance of the method is examined by a simulation study and data analysis. The new tools are implemented in an R package. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Thomas Kuenzer and Siegfried Hörmann and Piotr Kokoszka},
  doi          = {10.1080/01621459.2020.1732395},
  journal      = {Journal of the American Statistical Association},
  number       = {535},
  pages        = {1444-1456},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Principal component analysis of spatially indexed functions},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Extreme and inference for tail gini functionals with
applications in tail risk measurement. <em>JASA</em>, <em>116</em>(535),
1428–1443. (<a
href="https://doi.org/10.1080/01621459.2020.1730855">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tail risk analysis focuses on the problem of risk measurement on the tail regions of financial variables. As one crucial task in tail risk analysis for risk management, the measurement of tail risk variability is less addressed in the literature. Neither the theoretical results nor inference methods are fully developed, which results in the difficulty of modeling implementation. Practitioners are then short of measurement methods to understand and evaluate tail risks, even when they have large amounts of valuable data in hand. In this article, we consider the measurement of tail variability under the tail scenarios of a systemic variable by extending the Gini’s methodology. As we are very interested in the limit of the proposed measures as the risk level approaches to the extreme status, we showed, by using extreme value techniques, how the tail dependence structure and marginal risk severity have influences on the limit of the proposed tail variability measures. We construct a nonparametric estimator, and its asymptotic behavior is explored. Furthermore, to provide practitioners with more measures for tail risk, we construct three coefficients/measures for tail risks from different views toward tail risks and illustrate them in a real data analysis. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Yanxi Hou and Xing Wang},
  doi          = {10.1080/01621459.2020.1730855},
  journal      = {Journal of the American Statistical Association},
  number       = {535},
  pages        = {1428-1443},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Extreme and inference for tail gini functionals with applications in tail risk measurement},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Metropolized knockoff sampling. <em>JASA</em>,
<em>116</em>(535), 1413–1427. (<a
href="https://doi.org/10.1080/01621459.2020.1729163">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Model-X knockoffs is a wrapper that transforms essentially any feature importance measure into a variable selection algorithm, which discovers true effects while rigorously controlling the expected fraction of false positives. A frequently discussed challenge to apply this method is to construct knockoff variables, which are synthetic variables obeying a crucial exchangeability property with the explanatory variables under study. This article introduces techniques for knockoff generation in great generality: we provide a sequential characterization of all possible knockoff distributions, which leads to a Metropolis–Hastings formulation of an exact knockoff sampler. We further show how to use conditional independence structure to speed up computations. Combining these two threads, we introduce an explicit set of sequential algorithms and empirically demonstrate their effectiveness. Our theoretical analysis proves that our algorithms achieve near-optimal computational complexity in certain cases. The techniques we develop are sufficiently rich to enable knockoff sampling in challenging models including cases where the covariates are continuous and heavy-tailed, and follow a graphical model such as the Ising model. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Stephen Bates and Emmanuel Candès and Lucas Janson and Wenshuo Wang},
  doi          = {10.1080/01621459.2020.1729163},
  journal      = {Journal of the American Statistical Association},
  number       = {535},
  pages        = {1413-1427},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Metropolized knockoff sampling},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fast calibrated additive quantile regression. <em>JASA</em>,
<em>116</em>(535), 1402–1412. (<a
href="https://doi.org/10.1080/01621459.2020.1725521">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel framework for fitting additive quantile regression models, which provides well-calibrated inference about the conditional quantiles and fast automatic estimation of the smoothing parameters, for model structures as diverse as those usable with distributional generalized additive models, while maintaining equivalent numerical efficiency and stability. The proposed methods are at once statistically rigorous and computationally efficient, because they are based on the general belief updating framework of Bissiri, Holmes, and Walker to loss based inference, but compute by adapting the stable fitting methods of Wood, Pya, and Säfken. We show how the pinball loss is statistically suboptimal relative to a novel smooth generalization, which also gives access to fast estimation methods. Further, we provide a novel calibration method for efficiently selecting the “learning rate” balancing the loss with the smoothing priors during inference, thereby obtaining reliable quantile uncertainty estimates. Our work was motivated by a probabilistic electricity load forecasting application, used here to demonstrate the proposed approach. The methods described here are implemented by the qgam R package, available on the Comprehensive R Archive Network (CRAN). Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Matteo Fasiolo and Simon N. Wood and Margaux Zaffran and Raphaël Nedellec and Yannig Goude},
  doi          = {10.1080/01621459.2020.1725521},
  journal      = {Journal of the American Statistical Association},
  number       = {535},
  pages        = {1402-1412},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Fast calibrated additive quantile regression},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Estimating the covariance of fragmented and other related
types of functional data. <em>JASA</em>, <em>116</em>(535), 1383–1401.
(<a href="https://doi.org/10.1080/01621459.2020.1723597">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of estimating the covariance function of functional data which are only observed on a subset of their domain, such as fragments observed on small intervals or related types of functional data. We focus on situations where the data enable to compute the empirical covariance function or smooth versions of it only on a subset of its domain which contains a diagonal band. We show that estimating the covariance function consistently outside that subset is possible as long as the curves are sufficiently smooth. We establish conditions under which the covariance function is identifiable on its entire domain and propose a tensor product series approach for estimating it consistently. We derive asymptotic properties of our estimator and illustrate its finite sample properties on simulated and real data. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Aurore Delaigle and Peter Hall and Wei Huang and Alois Kneip},
  doi          = {10.1080/01621459.2020.1723597},
  journal      = {Journal of the American Statistical Association},
  number       = {535},
  pages        = {1383-1401},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Estimating the covariance of fragmented and other related types of functional data},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Using maximum entry-wise deviation to test the goodness of
fit for stochastic block models. <em>JASA</em>, <em>116</em>(535),
1373–1382. (<a
href="https://doi.org/10.1080/01621459.2020.1722676">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The stochastic block model is widely used for detecting community structures in network data. How to test the goodness of fit of the model is one of the fundamental problems and has gained growing interests in recent years. In this article, we propose a novel goodness-of-fit test based on the maximum entry of the centered and rescaled adjacency matrix for the stochastic block model. One noticeable advantage of the proposed test is that the number of communities can be allowed to grow linearly with the number of nodes ignoring a logarithmic factor. We prove that the null distribution of the test statistic converges in distribution to a Gumbel distribution, and we show that both the number of communities and the membership vector can be tested via the proposed method. Furthermore, we show that the proposed test has asymptotic power guarantee against a class of alternatives. We also demonstrate that the proposed method can be extended to the degree-corrected stochastic block model. Both simulation studies and real-world data examples indicate that the proposed method works well. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Jianwei Hu and Jingfei Zhang and Hong Qin and Ting Yan and Ji Zhu},
  doi          = {10.1080/01621459.2020.1722676},
  journal      = {Journal of the American Statistical Association},
  number       = {535},
  pages        = {1373-1382},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Using maximum entry-wise deviation to test the goodness of fit for stochastic block models},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Optimal permutation recovery in permuted monotone matrix
model. <em>JASA</em>, <em>116</em>(535), 1358–1372. (<a
href="https://doi.org/10.1080/01621459.2020.1713794">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by recent research on quantifying bacterial growth dynamics based on genome assemblies, we consider a permuted monotone matrix model Y = Θ Π + Z Y = Θ Π + Z Y=ΘΠ+Z , where the rows represent different samples, the columns represent contigs in genome assemblies and the elements represent log-read counts after preprocessing steps and Guanine-Cytosine (GC) adjustment. In this model, Θ is an unknown mean matrix with monotone entries for each row, Π is a permutation matrix that permutes the columns of Θ, and Z is a noise matrix. This article studies the problem of estimation/recovery of Π given the observed noisy matrix Y . We propose an estimator based on the best linear projection, which is shown to be minimax rate-optimal for both exact recovery, as measured by the 0-1 loss, and partial recovery, as quantified by the normalized Kendall’s tau distance. Simulation studies demonstrate the superior empirical performance of the proposed estimator over alternative methods. We demonstrate the methods using a synthetic metagenomics dataset of 45 closely related bacterial species and a real metagenomic dataset to compare the bacterial growth dynamics between the responders and the nonresponders of the IBD patients after 8 weeks of treatment. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Rong Ma and T. Tony Cai and Hongzhe Li},
  doi          = {10.1080/01621459.2020.1713794},
  journal      = {Journal of the American Statistical Association},
  number       = {535},
  pages        = {1358-1372},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Optimal permutation recovery in permuted monotone matrix model},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Nonparametric estimation of conditional expectation with
auxiliary information and dimension reduction. <em>JASA</em>,
<em>116</em>(535), 1346–1357. (<a
href="https://doi.org/10.1080/01621459.2020.1713793">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nonparametric estimation of the conditional expectation E ( Y | U ) E ( Y | U ) E(Y|U) of an outcome Y given a covariate vector U is of primary importance in many statistical applications such as prediction and personalized medicine. In some problems, there is an additional auxiliary variable Z in the training dataset used to construct estimators, but Z is not available for future prediction or selecting patient treatment in personalized medicine. For example, in the training dataset longitudinal outcomes are observed, but only the last outcome Y is concerned in the future prediction or analysis. The longitudinal outcomes other than the last point is then the variable Z that is observed and related with both Y and U . Previous work on how to make use of Z in the estimation of E ( Y | U ) mainly focused on using Z in the construction of a linear function of U to reduce covariate dimension for better estimation. Using E ( Y | U ) = E { E ( Y | U , Z ) | U } , we propose a two-step estimation of inner and outer expectations, respectively, with sufficient dimension reduction for kernel estimation in both steps. The information from Z is utilized not only in dimension reduction, but also directly in the estimation. Because of the existence of different ways for dimension reduction, we construct two estimators that may improve the estimator without using Z . The improvements are shown in the convergence rate of estimators as the sample size increases to infinity as well as in the finite sample simulation performance. A real data analysis about the selection of mammography intervention is presented for illustration.},
  archive      = {J_JASA},
  author       = {Bingying Xie and Jun Shao},
  doi          = {10.1080/01621459.2020.1713793},
  journal      = {Journal of the American Statistical Association},
  number       = {535},
  pages        = {1346-1357},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Nonparametric estimation of conditional expectation with auxiliary information and dimension reduction},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Regression models and multivariate life tables.
<em>JASA</em>, <em>116</em>(535), 1330–1345. (<a
href="https://doi.org/10.1080/01621459.2020.1713792">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semiparametric, multiplicative-form regression models are specified for marginal single and double failure hazard rates for the regression analysis of multivariate failure time data. Cox-type estimating functions are specified for single and double failure hazard ratio parameter estimation, and corresponding Aalen–Breslow estimators are specified for baseline hazard rates. Generalization to allow classification of failure times into a smaller set of failure types, with failures of the same type having common baseline hazard functions, is also included. Asymptotic distribution theory arises by generalization of the marginal single failure hazard rate estimation results of Lin et al. The Péano series representation for the bivariate survival function in terms of corresponding marginal single and double failure hazard rates leads to novel estimators for pairwise bivariate survival functions and pairwise dependency functions, at specified covariate history. Related asymptotic distribution theory follows from that for the marginal single and double failure hazard rates and the continuity, compact differentiability of the Péano series transformation and bootstrap applicability. Simulation evaluation of the proposed estimation procedures is presented, and an application to multiple clinical outcomes in the Women’s Health Initiative Dietary Modification Trial is provided. Higher dimensional marginal hazard rate regression modeling is briefly mentioned. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Ross L. Prentice and Shanshan Zhao},
  doi          = {10.1080/01621459.2020.1713792},
  journal      = {Journal of the American Statistical Association},
  number       = {535},
  pages        = {1330-1345},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Regression models and multivariate life tables},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Semiparametric inference for the functional cox model.
<em>JASA</em>, <em>116</em>(535), 1319–1329. (<a
href="https://doi.org/10.1080/01621459.2019.1710155">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article studies penalized semiparametric maximum partial likelihood estimation and hypothesis testing for the functional Cox model in analyzing right-censored data with both functional and scalar predictors. Deriving the asymptotic joint distribution of finite-dimensional and infinite-dimensional estimators is a very challenging theoretical problem due to the complexity of semiparametric models. For the problem, we construct the Sobolev space equipped with a special inner product and discover a new joint Bahadur representation of estimators of the unknown slope function and coefficients. Using this key tool, we establish the asymptotic joint normality of the proposed estimators and the weak convergence of the estimated slope function, and then construct local and global confidence intervals for an unknown slope function. Furthermore, we study a penalized partial likelihood ratio test, show that the test statistic enjoys the Wilks phenomenon, and also verify the optimality of the test. The theoretical results are examined through simulation studies, and a right-censored data example from the Improving Care of Acute Lung Injury Patients study is provided for illustration. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Meiling Hao and Kin-yat Liu and Wei Xu and Xingqiu Zhao},
  doi          = {10.1080/01621459.2019.1710155},
  journal      = {Journal of the American Statistical Association},
  number       = {535},
  pages        = {1319-1329},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Semiparametric inference for the functional cox model},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Statistical inference for high-dimensional models via
recursive online-score estimation. <em>JASA</em>, <em>116</em>(535),
1307–1318. (<a
href="https://doi.org/10.1080/01621459.2019.1710154">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we develop a new estimation and valid inference method for single or low-dimensional regression coefficients in high-dimensional generalized linear models. The number of the predictors is allowed to grow exponentially fast with respect to the sample size. The proposed estimator is computed by solving a score function. We recursively conduct model selection to reduce the dimensionality from high to a moderate scale and construct the score equation based on the selected variables. The proposed confidence interval (CI) achieves valid coverage without assuming consistency of the model selection procedure. When the selection consistency is achieved, we show the length of the proposed CI is asymptotically the same as the CI of the “oracle” method which works as well as if the support of the control variables were known. In addition, we prove the proposed CI is asymptotically narrower than the CIs constructed based on the desparsified Lasso estimator and the decorrelated score statistic. Simulation studies and real data applications are presented to back up our theoretical findings. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Chengchun Shi and Rui Song and Wenbin Lu and Runze Li},
  doi          = {10.1080/01621459.2019.1710154},
  journal      = {Journal of the American Statistical Association},
  number       = {535},
  pages        = {1307-1318},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Statistical inference for high-dimensional models via recursive online-score estimation},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multiple systems estimation for sparse capture data:
Inferential challenges when there are nonoverlapping lists.
<em>JASA</em>, <em>116</em>(535), 1297–1306. (<a
href="https://doi.org/10.1080/01621459.2019.1708748">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple systems estimation strategies have recently been applied to quantify hard-to-reach populations, particularly when estimating the number of victims of human trafficking and modern slavery. In such contexts, it is not uncommon to see sparse or even no overlap between some of the lists on which the estimates are based. These create difficulties in model fitting and selection, and we develop inference procedures to address these challenges. The approach is based on Poisson log-linear regression modeling. Issues investigated in detail include taking proper account of data sparsity in the estimation procedure, as well as the existence and identifiability of maximum likelihood estimates. A stepwise method for choosing the most suitable parameters is developed, together with a bootstrap approach to finding confidence intervals for the total population size. We apply the strategy to two empirical datasets of trafficking in US regions, and find that the approach results in stable, reasonable estimates. An accompanying R software implementation has been made publicly available. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Lax Chan and Bernard W. Silverman and Kyle Vincent},
  doi          = {10.1080/01621459.2019.1708748},
  journal      = {Journal of the American Statistical Association},
  number       = {535},
  pages        = {1297-1306},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Multiple systems estimation for sparse capture data: Inferential challenges when there are nonoverlapping lists},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Individualized multidirectional variable selection.
<em>JASA</em>, <em>116</em>(535), 1280–1296. (<a
href="https://doi.org/10.1080/01621459.2019.1705308">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose a heterogeneous modeling framework which achieves individual-wise feature selection and heterogeneous covariates’ effects subgrouping simultaneously. In contrast to conventional model selection approaches, the new approach constructs a separation penalty with multidirectional shrinkages, which facilitates individualized modeling to distinguish strong signals from noisy ones and selects different relevant variables for different individuals. Meanwhile, the proposed model identifies subgroups among which individuals share similar covariates’ effects, and thus improves individualized estimation efficiency and feature selection accuracy. Moreover, the proposed model also incorporates within-individual correlation for longitudinal data to gain extra efficiency. We provide a general theoretical foundation under a double-divergence modeling framework where the number of individuals and the number of individual-wise measurements can both diverge, which enables inference on both an individual level and a population level. In particular, we establish a strong oracle property for the individualized estimator to ensure its optimal large sample property under various conditions. An efficient ADMM algorithm is developed for computational scalability. Simulation studies and applications to post-trauma mental disorder analysis with genetic variation and an HIV longitudinal treatment study are illustrated to compare the new approach to existing methods. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Xiwei Tang and Fei Xue and Annie Qu},
  doi          = {10.1080/01621459.2019.1705308},
  journal      = {Journal of the American Statistical Association},
  number       = {535},
  pages        = {1280-1296},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Individualized multidirectional variable selection},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Trends in extreme value indices. <em>JASA</em>,
<em>116</em>(535), 1265–1279. (<a
href="https://doi.org/10.1080/01621459.2019.1705307">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider extreme value analysis for independent but nonidentically distributed observations. In particular, the observations do not share the same extreme value index. Assuming continuously changing extreme value indices, we provide a nonparametric estimate for the functional extreme value index. Besides estimating the extreme value index locally, we also provide a global estimator for the trend and its joint asymptotic theory. The asymptotic theory for the global estimator can be used for testing a prespecified parametric trend in the extreme value indices. In particular, it can be applied to test whether the extreme value index remains at a constant level across all observations.},
  archive      = {J_JASA},
  author       = {Laurens de Haan and Chen Zhou},
  doi          = {10.1080/01621459.2019.1705307},
  journal      = {Journal of the American Statistical Association},
  number       = {535},
  pages        = {1265-1279},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Trends in extreme value indices},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Complier stochastic direct effects: Identification and
robust estimation. <em>JASA</em>, <em>116</em>(535), 1254–1264. (<a
href="https://doi.org/10.1080/01621459.2019.1704292">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mediation analysis is critical to understanding the mechanisms underlying exposure-outcome relationships. In this article, we identify the instrumental variable-direct effect of the exposure on the outcome not through the mediator, using randomization of the instrument. We call this estimand the complier stochastic direct effect (CSDE). To our knowledge, such an estimand has not previously been considered or estimated. We propose and evaluate several estimators for the CSDE: a ratio of inverse-probability of treatment-weighted estimators (IPTW), a ratio of estimating equation estimators (EE), a ratio of targeted minimum loss-based estimators (TMLE), and a TMLE that targets the CSDE directly. These estimators are applicable for a variety of study designs, including randomized encouragement trials, like the Moving to Opportunity housing voucher experiment we consider as an illustrative example, treatment discontinuities, and Mendelian randomization. We found the IPTW estimator to be the most sensitive to finite sample bias, resulting in bias of over 40\% even when all models were correctly specified in a sample size of N = 100. In contrast, the EE estimator and TMLE that targets the CSDE directly were far less sensitive. The EE and TML estimators also have advantages in terms of efficiency and reduced reliance on correct parametric model specification. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Kara E. Rudolph and Oleg Sofrygin and Mark J. van der Laan},
  doi          = {10.1080/01621459.2019.1704292},
  journal      = {Journal of the American Statistical Association},
  number       = {535},
  pages        = {1254-1264},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Complier stochastic direct effects: Identification and robust estimation},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Optimal estimation of wasserstein distance on a tree with an
application to microbiome studies. <em>JASA</em>, <em>116</em>(535),
1237–1253. (<a
href="https://doi.org/10.1080/01621459.2019.1699422">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The weighted UniFrac distance, a plug-in estimator of the Wasserstein distance of read counts on a tree, has been widely used to measure the microbial community difference in microbiome studies. Our investigation however shows that such a plug-in estimator, although intuitive and commonly used in practice, suffers from potential bias. Motivated by this finding, we study the problem of optimal estimation of the Wasserstein distance between two distributions on a tree from the sampled data in the high-dimensional setting. The minimax rate of convergence is established. To overcome the bias problem, we introduce a new estimator, referred to as the moment-screening estimator on a tree (MET), by using implicit best polynomial approximation that incorporates the tree structure. The new estimator is computationally efficient and is shown to be minimax rate-optimal. Numerical studies using both simulated and real biological datasets demonstrate the practical merits of MET, including reduced biases and statistically more significant differences in microbiome between the inactive Crohn’s disease patients and the normal controls. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Shulei Wang and T. Tony Cai and Hongzhe Li},
  doi          = {10.1080/01621459.2019.1699422},
  journal      = {Journal of the American Statistical Association},
  number       = {535},
  pages        = {1237-1253},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Optimal estimation of wasserstein distance on a tree with an application to microbiome studies},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). ALMOND: Adaptive latent modeling and optimization via neural
networks and langevin diffusion. <em>JASA</em>, <em>116</em>(535),
1224–1236. (<a
href="https://doi.org/10.1080/01621459.2019.1691563">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Latent variable models cover a broad range of statistical and machine learning models, such as Bayesian models, linear mixed models, and Gaussian mixture models. Existing methods often suffer from two major challenges in practice: (a) a proper latent variable distribution is difficult to be specified; (b) making an exact likelihood inference is formidable due to the intractable computation. We propose a novel framework for the inference of latent variable models that overcomes these two limitations. This new framework allows for a fully data-driven latent variable distribution via deep neural networks, and the proposed stochastic gradient method, combined with the Langevin algorithm, is efficient and suitable for complex models and big data. We provide theoretical results for the Langevin algorithm, and establish the convergence analysis of the optimization method. This framework has demonstrated superior practical performance through simulation studies and a real data analysis. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Yixuan Qiu and Xiao Wang},
  doi          = {10.1080/01621459.2019.1691563},
  journal      = {Journal of the American Statistical Association},
  number       = {535},
  pages        = {1224-1236},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {ALMOND: Adaptive latent modeling and optimization via neural networks and langevin diffusion},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Scalable collaborative ranking for personalized prediction.
<em>JASA</em>, <em>116</em>(535), 1215–1223. (<a
href="https://doi.org/10.1080/01621459.2019.1691562">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Personalized prediction presents an important yet challenging task, which predicts user-specific preferences on a large number of items given limited information. It is often modeled as certain recommender systems focusing on ordinal or continuous ratings, as in collaborative filtering and content-based filtering. In this article, we propose a new collaborative ranking system to predict most-preferred items for each user given search queries. Particularly, we propose a ψ -ranker based on ranking functions incorporating information on users, items, and search queries through latent factor models. Moreover, we show that the proposed nonconvex surrogate pairwise ψ -loss performs well under four popular bipartite ranking losses, such as the sum loss, pairwise zero-one loss, discounted cumulative gain, and mean average precision. We develop a parallel computing strategy to optimize the intractable loss of two levels of nonconvex components through difference of convex programming and block successive upper-bound minimization. Theoretically, we establish a probabilistic error bound for the ψ -ranker and show that its ranking error has a sharp rate of convergence in the general framework of bipartite ranking, even when the dimension of the model parameters diverges with the sample size. Consequently, this result also indicates that the ψ -ranker performs better than two major approaches in bipartite ranking: pairwise ranking and scoring. Finally, we demonstrate the utility of the ψ -ranker by comparing it with some strong competitors in the literature through simulated examples as well as Expedia booking data. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Ben Dai and Xiaotong Shen and Junhui Wang and Annie Qu},
  doi          = {10.1080/01621459.2019.1691562},
  journal      = {Journal of the American Statistical Association},
  number       = {535},
  pages        = {1215-1223},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Scalable collaborative ranking for personalized prediction},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Rejoinder—a gibbs sampler for a class of random convex
polytopes. <em>JASA</em>, <em>116</em>(535), 1211–1214. (<a
href="https://doi.org/10.1080/01621459.2021.1945458">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We are very grateful to all commenters for their stimulating remarks, questions, as well as useful pointers to the literature which span a wide range of statistical methods over decades of research. We have neither the space nor the knowledge to answer many of the questions raised, and we only aim to offer some clarifications. We hope that readers will be as enthusiastic as ourselves about research on the topics discussed by the commenters. In the following, we refer to Diaconis and Wang as DW, Hoffman, Hannig and Zhang as HHZ, Lawrence and Vander Wiel as LV, Ruggeri as R, Shafer as S, and Williams as W.},
  archive      = {J_JASA},
  author       = {Pierre E. Jacob and Ruobin Gong and Paul T. Edlefsen and Arthur P. Dempster},
  doi          = {10.1080/01621459.2021.1945458},
  journal      = {Journal of the American Statistical Association},
  number       = {535},
  pages        = {1211-1214},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Rejoinder—A gibbs sampler for a class of random convex polytopes},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Comments on “a gibbs sampler for a class of random convex
polytopes.” <em>JASA</em>, <em>116</em>(535), 1206–1210. (<a
href="https://doi.org/10.1080/01621459.2021.1950002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Kentaro Hoffman and Jan Hannig and Kai Zhang},
  doi          = {10.1080/01621459.2021.1950002},
  journal      = {Journal of the American Statistical Association},
  number       = {535},
  pages        = {1206-1210},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Comments on “A gibbs sampler for a class of random convex polytopes”},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Comment on “a gibbs sampler for a class of random convex
polytopes” by p.e. Jacob, r. Gong, p.t. Edlefsen and a.p. dempster.
<em>JASA</em>, <em>116</em>(535), 1204–1205. (<a
href="https://doi.org/10.1080/01621459.2021.1946404">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Fabrizio Ruggeri},
  doi          = {10.1080/01621459.2021.1946404},
  journal      = {Journal of the American Statistical Association},
  number       = {535},
  pages        = {1204-1205},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Comment on “A gibbs sampler for a class of random convex polytopes” by P.E. jacob, r. gong, P.T. edlefsen and A.P. dempster},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Comment on “a gibbs sampler for a class of random convex
polytopes.” <em>JASA</em>, <em>116</em>(535), 1201–1203. (<a
href="https://doi.org/10.1080/01621459.2021.1947305">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Earl Lawrence and Scott Vander Wiel},
  doi          = {10.1080/01621459.2021.1947305},
  journal      = {Journal of the American Statistical Association},
  number       = {535},
  pages        = {1201-1203},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Comment on “A gibbs sampler for a class of random convex polytopes”},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Discussion of “a gibbs sampler for a class of random convex
polytopes.” <em>JASA</em>, <em>116</em>(535), 1198–1200. (<a
href="https://doi.org/10.1080/01621459.2021.1946405">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Jonathan P Williams},
  doi          = {10.1080/01621459.2021.1946405},
  journal      = {Journal of the American Statistical Association},
  number       = {535},
  pages        = {1198-1200},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Discussion of “A gibbs sampler for a class of random convex polytopes”},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Comment on “a gibbs sampler for a class of random convex
polytopes,” by pierre e. Jacob, ruobin gong, paul t. Edlefsen, and
arthur p. dempster. <em>JASA</em>, <em>116</em>(535), 1196–1197. (<a
href="https://doi.org/10.1080/01621459.2021.1950001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Glenn Shafer},
  doi          = {10.1080/01621459.2021.1950001},
  journal      = {Journal of the American Statistical Association},
  number       = {535},
  pages        = {1196-1197},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Comment on “A gibbs sampler for a class of random convex polytopes,” by pierre e. jacob, ruobin gong, paul t. edlefsen, and arthur p. dempster},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Discussion of “a gibbs sampler for a class of random convex
polytopes.” <em>JASA</em>, <em>116</em>(535), 1193–1195. (<a
href="https://doi.org/10.1080/01621459.2021.1950000">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Persi Diaconis and Guanyang Wang},
  doi          = {10.1080/01621459.2021.1950000},
  journal      = {Journal of the American Statistical Association},
  number       = {535},
  pages        = {1193-1195},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Discussion of “A gibbs sampler for a class of random convex polytopes”},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). A gibbs sampler for a class of random convex polytopes.
<em>JASA</em>, <em>116</em>(535), 1181–1192. (<a
href="https://doi.org/10.1080/01621459.2021.1881523">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a Gibbs sampler for the Dempster–Shafer (DS) approach to statistical inference for categorical distributions. The DS framework extends the Bayesian approach, allows in particular the use of partial prior information, and yields three-valued uncertainty assessments representing probabilities “for,” “against,” and “don’t know” about formal assertions of interest. The proposed algorithm targets the distribution of a class of random convex polytopes which encapsulate the DS inference. The sampler relies on an equivalence between the iterative constraints of the vertex configuration and the nonnegativity of cycles in a fully connected directed graph. Illustrations include the testing of independence in 2 × 2 contingency tables and parameter estimation of the linkage model.},
  archive      = {J_JASA},
  author       = {Pierre E. Jacob and Ruobin Gong and Paul T. Edlefsen and Arthur P. Dempster},
  doi          = {10.1080/01621459.2021.1881523},
  journal      = {Journal of the American Statistical Association},
  number       = {535},
  pages        = {1181-1192},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {A gibbs sampler for a class of random convex polytopes},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Constrained functional regression of national forest
inventory data over time using remote sensing observations.
<em>JASA</em>, <em>116</em>(535), 1168–1180. (<a
href="https://doi.org/10.1080/01621459.2020.1860769">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The USDA Forest Service uses satellite imagery, along with a sample of national forest inventory field plots, to monitor and predict changes in forest conditions over time throughout the United States. We specifically focus on a 230,400 ha region in north-central Wisconsin between 2003 and 2012. The auxiliary data from the satellite imagery of this region are relatively dense in space and time, and can be used to learn how forest conditions changed over that decade. However, these records have a significant proportion of missing values due to weather conditions and system failures that we fill in first using a spatiotemporal model. Subsequently, we use the complete imagery as functional predictors in a two-component mixture model to capture the spatial variation in yearly average live tree basal area, an attribute of interest measured on field plots. We further modify the regression equation to accommodate a biophysical constraint on how plot-level live tree basal area can change from one year to the next. Findings from our analysis, represented with a series of maps, match known spatial patterns across the landscape. Supplementary materials for this article, including a standardized description of the materials available for reproducing the work, are available as an online supplement.},
  archive      = {J_JASA},
  author       = {Md Kamrul Hasan Khan and Avishek Chakraborty and Giovanni Petris and Barry T. Wilson},
  doi          = {10.1080/01621459.2020.1860769},
  journal      = {Journal of the American Statistical Association},
  number       = {535},
  pages        = {1168-1180},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Constrained functional regression of national forest inventory data over time using remote sensing observations},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Marginalized frailty-based illness-death model: Application
to the UK-biobank survival data. <em>JASA</em>, <em>116</em>(535),
1155–1167. (<a
href="https://doi.org/10.1080/01621459.2020.1831922">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The UK Biobank is a large-scale health resource comprising genetic, environmental, and medical information on approximately 500,000 volunteer participants in the United Kingdom, recruited at ages 40–69 during the years 2006–2010. The project monitors the health and well-being of its participants. This work demonstrates how these data can be used to yield the building blocks for an interpretable risk-prediction model, in a semiparametric fashion, based on known genetic and environmental risk factors of various chronic diseases, such as colorectal cancer. An illness-death model is adopted, which inherently is a semi-competing risks model, since death can censor the disease, but not vice versa. Using a shared-frailty approach to account for the dependence between time to disease diagnosis and time to death, we provide a new illness-death model that assumes Cox models for the marginal hazard functions. The recruitment procedure used in this study introduces delayed entry to the data. An additional challenge arising from the recruitment procedure is that information coming from both prevalent and incident cases must be aggregated. Lastly, we do not observe any deaths prior to the minimal recruitment age, 40. In this work, we provide an estimation procedure for our new illness-death model that overcomes all the above challenges. Supplementary materials for this article, including a standardized description of the materials available for reproducing the work, are available as an online supplement.},
  archive      = {J_JASA},
  author       = {Malka Gorfine and Nir Keret and Asaf Ben Arie and David Zucker and Li Hsu},
  doi          = {10.1080/01621459.2020.1831922},
  journal      = {Journal of the American Statistical Association},
  number       = {535},
  pages        = {1155-1167},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Marginalized frailty-based illness-death model: Application to the UK-biobank survival data},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Evaluation of the health impacts of the 1990 clean air act
amendments using causal inference and machine learning. <em>JASA</em>,
<em>116</em>(535), 1128–1139. (<a
href="https://doi.org/10.1080/01621459.2020.1803883">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a causal inference approach to estimate the number of adverse health events that were prevented due to changes in exposure to multiple pollutants attributable to a large-scale air quality intervention/regulation, with a focus on the 1990 Clean Air Act Amendments (CAAA). We introduce a causal estimand called the Total Events Avoided (TEA) by the regulation, defined as the difference in the number of health events expected under the no-regulation pollution exposures and the number observed with-regulation. We propose matching and machine learning methods that leverage population-level pollution and health data to estimate the TEA. Our approach improves upon traditional methods for regulation health impact analyses by formalizing causal identifying assumptions, utilizing population-level data, minimizing parametric assumptions, and collectively analyzing multiple pollutants. To reduce model-dependence, our approach estimates cumulative health impacts in the subset of regions with projected no-regulation features lying within the support of the observed with-regulation data, thereby providing a conservative but data-driven assessment to complement traditional parametric approaches. We analyze the health impacts of the CAAA in the US Medicare population in the year 2000, and our estimates suggest that large numbers of cardiovascular and dementia-related hospitalizations were avoided due to CAAA-attributable changes in pollution exposure.},
  archive      = {J_JASA},
  author       = {Rachel C. Nethery and Fabrizia Mealli and Jason D. Sacks and Francesca Dominici},
  doi          = {10.1080/01621459.2020.1803883},
  journal      = {Journal of the American Statistical Association},
  number       = {535},
  pages        = {1128-1139},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Evaluation of the health impacts of the 1990 clean air act amendments using causal inference and machine learning},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bayesian semiparametric longitudinal drift-diffusion mixed
models for tone learning in adults. <em>JASA</em>, <em>116</em>(535),
1114–1127. (<a
href="https://doi.org/10.1080/01621459.2020.1801448">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding how adult humans learn nonnative speech categories such as tone information has shed novel insights into the mechanisms underlying experience-dependent brain plasticity. Scientists have traditionally examined these questions using longitudinal learning experiments under a multi-category decision making paradigm. Drift-diffusion processes are popular in such contexts for their ability to mimic underlying neural mechanisms. Motivated by these problems, we develop a novel Bayesian semiparametric inverse Gaussian drift-diffusion mixed model for multi-alternative decision making in longitudinal settings. We design a Markov chain Monte Carlo algorithm for posterior computation. We evaluate the method’s empirical performances through synthetic experiments. Applied to our motivating longitudinal tone learning study, the method provides novel insights into how the biologically interpretable model parameters evolve with learning, differ between input-response tone combinations, and differ between well and poorly performing adults. Supplementary materials for this article, including a standardized description of the materials available for reproducing the work, are available as an online supplement.},
  archive      = {J_JASA},
  author       = {Giorgio Paulon and Fernando Llanos and Bharath Chandrasekaran and Abhra Sarkar},
  doi          = {10.1080/01621459.2020.1801448},
  journal      = {Journal of the American Statistical Association},
  number       = {535},
  pages        = {1114-1127},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Bayesian semiparametric longitudinal drift-diffusion mixed models for tone learning in adults},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Evaluating proxy influence in assimilated paleoclimate
reconstructions—testing the exchangeability of two ensembles of spatial
processes. <em>JASA</em>, <em>116</em>(535), 1100–1113. (<a
href="https://doi.org/10.1080/01621459.2020.1799810">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Climate field reconstructions (CFRs) attempt to estimate spatiotemporal fields of climate variables in the past using climate proxies such as tree rings, ice cores, and corals. Data assimilation (DA) methods are a recent and promising new means of deriving CFRs that optimally fuse climate proxies with climate model output. Despite the growing application of DA-based CFRs, little is understood about how much the assimilated proxies change the statistical properties of the climate model data. To address this question, we propose a robust and computationally efficient method, based on functional data depth, to evaluate differences in the distributions of two spatiotemporal processes. We apply our test to study global and regional proxy influence in DA-based CFRs by comparing the background and analysis states, which are treated as two samples of spatiotemporal fields. We find that the analysis states are significantly altered from the climate-model-based background states due to the assimilation of proxies. Moreover, the difference between the analysis and background states increases with the number of proxies, even in regions far beyond proxy collection sites. Our approach allows us to characterize the added value of proxies, indicating where and when the analysis states are distinct from the background states. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Trevor Harris and Bo Li and Nathan J. Steiger and Jason E. Smerdon and Naveen Narisetty and J. Derek Tucker},
  doi          = {10.1080/01621459.2020.1799810},
  journal      = {Journal of the American Statistical Association},
  number       = {535},
  pages        = {1100-1113},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Evaluating proxy influence in assimilated paleoclimate Reconstructions—Testing the exchangeability of two ensembles of spatial processes},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Nonparametric estimation of galaxy cluster emissivity and
detection of point sources in astrophysics with two lasso penalties.
<em>JASA</em>, <em>116</em>(535), 1088–1099. (<a
href="https://doi.org/10.1080/01621459.2020.1796676">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Astrophysicists are interested in recovering the three-dimensional gas emissivity of a galaxy cluster from a two-dimensional telescope image. Blurring and point sources make this inverse problem harder to solve. The conventional approach requires in a first step to identify and mask the point sources. Instead we model all astrophysical components in a single Poisson generalized linear model. To enforce sparsity on the parameters, maximum likelihood estimation is regularized with two l 1 penalties with weights λ 1 for the radial emissivity and λ 2 for the point sources. The method has the advantage of not employing cross-validation to select λ 1 and λ 2 . To judge the significance of interesting features, we quantify uncertainty with the bootstrap. We apply our method to two X-ray telescopes (XMM-Newton and Chandra) data to estimate gas emissivity. The results are more stable and seems less biased than the conventional method, in particular in the outskirt of galaxy clusters. Supplementary materials for this article, including a standardized description of the materials available for reproducing the work, are available as an online supplement.},
  archive      = {J_JASA},
  author       = {Jairo Diaz-Rodriguez and Dominique Eckert and Hatef Monajemi and Stéphane Paltani and Sylvain Sardy},
  doi          = {10.1080/01621459.2020.1796676},
  journal      = {Journal of the American Statistical Association},
  number       = {535},
  pages        = {1088-1099},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Nonparametric estimation of galaxy cluster emissivity and detection of point sources in astrophysics with two lasso penalties},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bayesian copula density deconvolution for zero-inflated data
in nutritional epidemiology. <em>JASA</em>, <em>116</em>(535),
1075–1087. (<a
href="https://doi.org/10.1080/01621459.2020.1782220">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimating the marginal and joint densities of the long-term average intakes of different dietary components is an important problem in nutritional epidemiology. Since these variables cannot be directly measured, data are usually collected in the form of 24-hr recalls of the intakes, which show marked patterns of conditional heteroscedasticity. Significantly compounding the challenges, the recalls for episodically consumed dietary components also include exact zeros. The problem of estimating the density of the latent long-time intakes from their observed measurement error contaminated proxies is then a problem of deconvolution of densities with zero-inflated data. We propose a Bayesian semiparametric solution to the problem, building on a novel hierarchical latent variable framework that translates the problem to one involving continuous surrogates only. Crucial to accommodating important aspects of the problem, we then design a copula based approach to model the involved joint distributions, adopting different modeling strategies for the marginals of the different dietary components. We design efficient Markov chain Monte Carlo algorithms for posterior inference and illustrate the efficacy of the proposed method through simulation experiments. Applied to our motivating nutritional epidemiology problems, compared to other approaches, our method provides more realistic estimates of the consumption patterns of episodically consumed dietary components. Supplementary materials for this article, including a standardized description of the materials available for reproducing the work, are available as an online supplement.},
  archive      = {J_JASA},
  author       = {Abhra Sarkar and Debdeep Pati and Bani K. Mallick and Raymond J. Carroll},
  doi          = {10.1080/01621459.2020.1782220},
  journal      = {Journal of the American Statistical Association},
  number       = {535},
  pages        = {1075-1087},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Bayesian copula density deconvolution for zero-inflated data in nutritional epidemiology},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Network dependence can lead to spurious associations and
invalid inference. <em>JASA</em>, <em>116</em>(535), 1060–1074. (<a
href="https://doi.org/10.1080/01621459.2020.1782219">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Researchers across the health and social sciences generally assume that observations are independent, even while relying on convenience samples that draw subjects from one or a small number of communities, schools, hospitals, etc. A paradigmatic example of this is the Framingham Heart Study (FHS). Many of the limitations of such samples are well-known, but the issue of statistical dependence due to social network ties has not previously been addressed. We show that, along with anticonservative variance estimation, this can result in spurious associations due to network dependence . Using a statistical test that we adapted from one developed for spatial autocorrelation, we test for network dependence in several of the thousands of influential papers that have been published using FHS data. Results suggest that some of the many decades of research on coronary heart disease, other health outcomes, and peer influence using FHS data may suffer from spurious associations, error-prone point estimates, and anticonservative inference due to unacknowledged network dependence. These issues are not unique to the FHS; as researchers in psychology, medicine, and beyond grapple with replication failures, this unacknowledged source of invalid statistical inference should be part of the conversation. Supplementary materials for this article, including a standardized description of the materials available for reproducing the work, are available as an online supplement.},
  archive      = {J_JASA},
  author       = {Youjin Lee and Elizabeth L. Ogburn},
  doi          = {10.1080/01621459.2020.1782219},
  journal      = {Journal of the American Statistical Association},
  number       = {535},
  pages        = {1060-1074},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Network dependence can lead to spurious associations and invalid inference},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multivariate postprocessing methods for high-dimensional
seasonal weather forecasts. <em>JASA</em>, <em>116</em>(535), 1048–1059.
(<a href="https://doi.org/10.1080/01621459.2020.1769634">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Seasonal weather forecasts are crucial for long-term planning in many practical situations and skillful forecasts may have substantial economic and humanitarian implications. Current seasonal forecasting models require statistical postprocessing of the output to correct systematic biases and unrealistic uncertainty assessments. We propose a multivariate postprocessing approach using covariance tapering, combined with a dimension reduction step based on principal component analysis for efficient computation. Our proposed technique can correctly and efficiently handle nonstationary, non-isotropic and negatively correlated spatial error patterns, and is applicable on a global scale. Further, a moving average approach to marginal postprocessing is shown to flexibly handle trends in biases caused by global warming, and short training periods. In an application to global sea surface temperature forecasts issued by the Norwegian climate prediction model, our proposed methodology is shown to outperform known reference methods. Supplementary materials for this article, including a standardized description of the materials available for reproducing the work, are available as an online supplement.},
  archive      = {J_JASA},
  author       = {Claudio Heinrich and Kristoffer H. Hellton and Alex Lenkoski and Thordis L. Thorarinsdottir},
  doi          = {10.1080/01621459.2020.1769634},
  journal      = {Journal of the American Statistical Association},
  number       = {535},
  pages        = {1048-1059},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Multivariate postprocessing methods for high-dimensional seasonal weather forecasts},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Back to our future: Text analytics insights. <em>JASA</em>,
<em>116</em>(535), 1039–1047. (<a
href="https://doi.org/10.1080/01621459.2021.1960760">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Each year, the Journal of the American Statistical Association ( ASA ) publishes the presidential address from the Joint Statistical Meetings (JSM). Here, we present the 2020 address verbatim save for the addition of references and a few minor editorial corrections.},
  archive      = {J_JASA},
  author       = {Wendy L. Martinez},
  doi          = {10.1080/01621459.2021.1960760},
  journal      = {Journal of the American Statistical Association},
  number       = {535},
  pages        = {1039-1047},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Back to our future: Text analytics insights},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021c). Correction. <em>JASA</em>, <em>116</em>(534), 1039. (<a
href="https://doi.org/10.1080/01621459.2021.1915023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  doi          = {10.1080/01621459.2021.1915023},
  journal      = {Journal of the American Statistical Association},
  number       = {534},
  pages        = {1039},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Correction},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Design of experiments for generalized linear models.
<em>JASA</em>, <em>116</em>(534), 1038. (<a
href="https://doi.org/10.1080/01621459.2021.1921472">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Youngjun Choe},
  doi          = {10.1080/01621459.2021.1921472},
  journal      = {Journal of the American Statistical Association},
  number       = {534},
  pages        = {1038},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Design of experiments for generalized linear models},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Graphical models for processing missing data. <em>JASA</em>,
<em>116</em>(534), 1023–1037. (<a
href="https://doi.org/10.1080/01621459.2021.1874961">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article reviews recent advances in missing data research using graphical models to represent multivariate dependencies. We first examine the limitations of traditional frameworks from three different perspectives: transparency , estimability , and testability . We then show how procedures based on graphical models can overcome these limitations and provide meaningful performance guarantees even when data are missing not at random (MNAR). In particular, we identify conditions that guarantee consistent estimation in broad categories of missing data problems, and derive procedures for implementing this estimation. Finally, we derive testable implications for missing data models in both missing at random and MNAR categories.},
  archive      = {J_JASA},
  author       = {Karthika Mohan and Judea Pearl},
  doi          = {10.1080/01621459.2021.1874961},
  journal      = {Journal of the American Statistical Association},
  number       = {534},
  pages        = {1023-1037},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Graphical models for processing missing data},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Regression modeling for size-and-shape data based on a
gaussian model for landmarks. <em>JASA</em>, <em>116</em>(534),
1011–1022. (<a
href="https://doi.org/10.1080/01621459.2020.1724115">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose a regression model for size-and-shape response data. So far as we are aware, few such models have been explored in the literature to date. We assume a Gaussian model for labeled landmarks; these landmarks are used to represent the random objects under study. The regression structure, assumed in this article to be linear in the ambient space, enters through the landmark means. Two approaches to parameter estimation are considered. The first approach is based directly on the marginal likelihood for the landmark-based shapes. In the second approach, we treat the orientations of the landmarks as missing data, and we set up a model-consistent estimation procedure for the parameters using the EM algorithm. Both approaches raise challenging computational issues which we explain how to deal with. The usefulness of this regression modeling framework is demonstrated through real-data examples. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Ian L. Dryden and Alfred Kume and Phillip J. Paine and Andrew T. A. Wood},
  doi          = {10.1080/01621459.2020.1724115},
  journal      = {Journal of the American Statistical Association},
  number       = {534},
  pages        = {1011-1022},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Regression modeling for size-and-shape data based on a gaussian model for landmarks},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Grouped heterogeneous mixture modeling for clustered data.
<em>JASA</em>, <em>116</em>(534), 999–1010. (<a
href="https://doi.org/10.1080/01621459.2020.1777136">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustered data are ubiquitous in a variety of scientific fields. In this article, we propose a flexible and interpretable modeling approach, called grouped heterogeneous mixture modeling, for clustered data, which models cluster-wise conditional distributions by mixtures of latent conditional distributions common to all the clusters. In the model, we assume that clusters are divided into a finite number of groups and mixing proportions are the same within the same group. We provide a simple generalized EM algorithm for computing the maximum likelihood estimator, and an information criterion to select the numbers of groups and latent distributions. We also propose structured grouping strategies by introducing penalties on grouping parameters in the likelihood function. Under the settings where both the number of clusters and cluster sizes tend to infinity, we present asymptotic properties of the maximum likelihood estimator and the information criterion. We demonstrate the proposed method through simulation studies and an application to crime risk modeling in Tokyo.},
  archive      = {J_JASA},
  author       = {Shonosuke Sugasawa},
  doi          = {10.1080/01621459.2020.1777136},
  journal      = {Journal of the American Statistical Association},
  number       = {534},
  pages        = {999-1010},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Grouped heterogeneous mixture modeling for clustered data},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Global and simultaneous hypothesis testing for
high-dimensional logistic regression models. <em>JASA</em>,
<em>116</em>(534), 984–998. (<a
href="https://doi.org/10.1080/01621459.2019.1699421">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-dimensional logistic regression is widely used in analyzing data with binary outcomes. In this article, global testing and large-scale multiple testing for the regression coefficients are considered in both single- and two-regression settings. A test statistic for testing the global null hypothesis is constructed using a generalized low-dimensional projection for bias correction and its asymptotic null distribution is derived. A lower bound for the global testing is established, which shows that the proposed test is asymptotically minimax optimal over some sparsity range. For testing the individual coefficients simultaneously, multiple testing procedures are proposed and shown to control the false discovery rate and falsely discovered variables asymptotically. Simulation studies are carried out to examine the numerical performance of the proposed tests and their superiority over existing methods. The testing procedures are also illustrated by analyzing a dataset of a metabolomics study that investigates the association between fecal metabolites and pediatric Crohn’s disease and the effects of treatment on such associations. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Rong Ma and T. Tony Cai and Hongzhe Li},
  doi          = {10.1080/01621459.2019.1699421},
  journal      = {Journal of the American Statistical Association},
  number       = {534},
  pages        = {984-998},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Global and simultaneous hypothesis testing for high-dimensional logistic regression models},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Privacy-preserving parametric inference: A case for robust
statistics. <em>JASA</em>, <em>116</em>(534), 969–983. (<a
href="https://doi.org/10.1080/01621459.2019.1700130">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Differential privacy is a cryptographically motivated approach to privacy that has become a very active field of research over the last decade in theoretical computer science and machine learning. In this paradigm, one assumes there is a trusted curator who holds the data of individuals in a database and the goal of privacy is to simultaneously protect individual data while allowing the release of global characteristics of the database. In this setting, we introduce a general framework for parametric inference with differential privacy guarantees. We first obtain differentially private estimators based on bounded influence M-estimators by leveraging their gross-error sensitivity in the calibration of a noise term added to them to ensure privacy. We then show how a similar construction can also be applied to construct differentially private test statistics analogous to the Wald, score, and likelihood ratio tests. We provide statistical guarantees for all our proposals via an asymptotic analysis. An interesting consequence of our results is to further clarify the connection between differential privacy and robust statistics. In particular, we demonstrate that differential privacy is a weaker stability requirement than infinitesimal robustness, and show that robust M-estimators can be easily randomized to guarantee both differential privacy and robustness toward the presence of contaminated data. We illustrate our results both on simulated and real data. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Marco Avella-Medina},
  doi          = {10.1080/01621459.2019.1700130},
  journal      = {Journal of the American Statistical Association},
  number       = {534},
  pages        = {969-983},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Privacy-preserving parametric inference: A case for robust statistics},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Assessing partial association between ordinal variables:
Quantification, visualization, and hypothesis testing. <em>JASA</em>,
<em>116</em>(534), 955–968. (<a
href="https://doi.org/10.1080/01621459.2020.1796394">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Partial association refers to the relationship between variables Y 1 , Y 2 , … , Y K Y 1 , Y 2 , … , Y K Y1,Y2,…,YK while adjusting for a set of covariates X = { X 1 , … , X p } X = { X 1 , … , X p } X={X1,…,Xp} . To assess such an association when Y k ’s are recorded on ordinal scales, a classical approach is to use partial correlation between the latent continuous variables. This so-called polychoric correlation is inadequate, as it requires multivariate normality and it only reflects a linear association. We propose a new framework for studying ordinal-ordinal partial association by using Liu-Zhang’s surrogate residuals. We justify that conditional on X , Y k , and Y l are independent if and only if their corresponding surrogate residual variables are independent. Based on this result, we develop a general measure ϕ to quantify association strength. As opposed to polychoric correlation, ϕ does not rely on normality or models with the probit link, but instead it broadly applies to models with any link functions. It can capture a nonlinear or even nonmonotonic association. Moreover, the measure ϕ gives rise to a general procedure for testing the hypothesis of partial independence. Our framework also permits visualization tools, such as partial regression plots and three-dimensional P-P plots, to examine the association structure, which is otherwise unfeasible for ordinal data. We stress that the whole set of tools (measures, p -values, and graphics) is developed within a single unified framework, which allows a coherent inference. The analyses of the National Election Study ( K = 5) and Big Five Personality Traits ( K = 50) demonstrate that our framework leads to a much fuller assessment of partial association and yields deeper insights for domain researchers. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Dungang Liu and Shaobo Li and Yan Yu and Irini Moustaki},
  doi          = {10.1080/01621459.2020.1796394},
  journal      = {Journal of the American Statistical Association},
  number       = {534},
  pages        = {955-968},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Assessing partial association between ordinal variables: Quantification, visualization, and hypothesis testing},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Structure and sensitivity in differential privacy: Comparing
k-norm mechanisms. <em>JASA</em>, <em>116</em>(534), 935–954. (<a
href="https://doi.org/10.1080/01621459.2020.1773831">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Differential privacy (DP) provides a framework for provable privacy protection against arbitrary adversaries, while allowing the release of summary statistics and synthetic data. We address the problem of releasing a noisy real-valued statistic vector T , a function of sensitive data under DP, via the class of K -norm mechanisms with the goal of minimizing the noise added to achieve privacy. First, we introduce the sensitivity space of T , which extends the concepts of sensitivity polytope and sensitivity hull to the setting of arbitrary statistics T . We then propose a framework consisting of three methods for comparing the K -norm mechanisms: (1) a multivariate extension of stochastic dominance, (2) the entropy of the mechanism, and (3) the conditional variance given a direction, to identify the optimal K -norm mechanism. In all of these criteria, the optimal K -norm mechanism is generated by the convex hull of the sensitivity space. Using our methodology, we extend the objective perturbation and functional mechanisms and apply these tools to logistic and linear regression, allowing for private releases of statistical results. Via simulations and an application to a housing price dataset, we demonstrate that our proposed methodology offers a substantial improvement in utility for the same level of risk.},
  archive      = {J_JASA},
  author       = {Jordan Awan and Aleksandra Slavković},
  doi          = {10.1080/01621459.2020.1773831},
  journal      = {Journal of the American Statistical Association},
  number       = {534},
  pages        = {935-954},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Structure and sensitivity in differential privacy: Comparing K-norm mechanisms},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Predictive inference for locally stationary time series with
an application to climate data. <em>JASA</em>, <em>116</em>(534),
919–934. (<a
href="https://doi.org/10.1080/01621459.2019.1708368">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The model-free prediction principle of Politis has been successfully applied to general regression problems, as well as problems involving stationary time series. However, with long time series, for example, annual temperature measurements spanning over 100 years or daily financial returns spanning several years, it may be unrealistic to assume stationarity throughout the span of the dataset. In this article, we show how model-free prediction can be applied to handle time series that are only locally stationary, that is, they can be assumed to be stationary only over short time-windows. Surprisingly, there is little literature on point prediction for general locally stationary time series even in model-based setups, and there is no literature whatsoever on the construction of prediction intervals of locally stationary time series. We attempt to fill this gap here as well. Both one-step-ahead point predictors and prediction intervals are constructed, and the performance of model-free is compared to model-based prediction using models that incorporate a trend and/or heteroscedasticity. Both aspects of the article, model-free and model-based, are novel in the context of time-series that are locally (but not globally) stationary. We also demonstrate the application of our model-based and model-free prediction methods to speleothem climate data which exhibits local stationarity and show that our best model-free point prediction results outperform that obtained with the RAMPFIT algorithm previously used for analysis of this type of data. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Srinjoy Das and Dimitris N. Politis},
  doi          = {10.1080/01621459.2019.1708368},
  journal      = {Journal of the American Statistical Association},
  number       = {534},
  pages        = {919-934},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Predictive inference for locally stationary time series with an application to climate data},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Identification and estimation of treatment and interference
effects in observational studies on networks. <em>JASA</em>,
<em>116</em>(534), 901–918. (<a
href="https://doi.org/10.1080/01621459.2020.1768100">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract– Causal inference on a population of units connected through a network often presents technical challenges, including how to account for interference. In the presence of interference, for instance, potential outcomes of a unit depend on their treatment as well as on the treatments of other units, such as their neighbors in the network. In observational studies, a further complication is that the typical unconfoundedness assumption must be extended—say, to include the treatment of neighbors, and individual and neighborhood covariates—to guarantee identification and valid inference. Here, we propose new estimands that define treatment and interference effects. We then derive analytical expressions for the bias of a naive estimator that wrongly assumes away interference. The bias depends on the level of interference but also on the degree of association between individual and neighborhood treatments. We propose an extended unconfoundedness assumption that accounts for interference, and we develop new covariate-adjustment methods that lead to valid estimates of treatment and interference effects in observational studies on networks. Estimation is based on a generalized propensity score that balances individual and neighborhood covariates across units under different levels of individual treatment and of exposure to neighbors’ treatment. We carry out simulations, calibrated using friendship networks and covariates in a nationally representative longitudinal study of adolescents in grades 7–12 in the United States, to explore finite-sample performance in different realistic settings. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Laura Forastiere and Edoardo M. Airoldi and Fabrizia Mealli},
  doi          = {10.1080/01621459.2020.1768100},
  journal      = {Journal of the American Statistical Association},
  number       = {534},
  pages        = {901-918},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Identification and estimation of treatment and interference effects in observational studies on networks},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Rare feature selection in high dimensions. <em>JASA</em>,
<em>116</em>(534), 887–900. (<a
href="https://doi.org/10.1080/01621459.2020.1796677">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is common in modern prediction problems for many predictor variables to be counts of rarely occurring events. This leads to design matrices in which many columns are highly sparse. The challenge posed by such “rare features” has received little attention despite its prevalence in diverse areas, ranging from natural language processing (e.g., rare words) to biology (e.g., rare species). We show, both theoretically and empirically, that not explicitly accounting for the rareness of features can greatly reduce the effectiveness of an analysis. We next propose a framework for aggregating rare features into denser features in a flexible manner that creates better predictors of the response. Our strategy leverages side information in the form of a tree that encodes feature similarity. We apply our method to data from TripAdvisor, in which we predict the numerical rating of a hotel based on the text of the associated review. Our method achieves high accuracy by making effective use of rare words; by contrast, the lasso is unable to identify highly predictive words if they are too rare. A companion R package, called rare, implements our new estimator, using the alternating direction method of multipliers. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Xiaohan Yan and Jacob Bien},
  doi          = {10.1080/01621459.2020.1796677},
  journal      = {Journal of the American Statistical Association},
  number       = {534},
  pages        = {887-900},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Rare feature selection in high dimensions},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Transformed dynamic quantile regression on censored data.
<em>JASA</em>, <em>116</em>(534), 874–886. (<a
href="https://doi.org/10.1080/01621459.2019.1695623">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a class of power-transformed linear quantile regression models for time-to-event observations subject to censoring. By introducing a process of power transformation with different transformation parameters at individual quantile levels, our framework relaxes the assumption of logarithmic transformation on survival times and provides dynamic estimation of various quantile levels. With such formulation, our proposal no longer requires the potentially restrictive global linearity assumption imposed on a class of existing inference procedures for censored quantile regression. Uniform consistency and weak convergence of the proposed estimator as a process of quantile levels are established via the martingale-based argument. Numerical studies are presented to illustrate the outperformance of the proposed estimator over existing contenders under various settings.},
  archive      = {J_JASA},
  author       = {Chi Wing Chu and Tony Sit and Gongjun Xu},
  doi          = {10.1080/01621459.2019.1695623},
  journal      = {Journal of the American Statistical Association},
  number       = {534},
  pages        = {874-886},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Transformed dynamic quantile regression on censored data},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Nonbifurcating phylogenetic tree inference via the adaptive
LASSO. <em>JASA</em>, <em>116</em>(534), 858–873. (<a
href="https://doi.org/10.1080/01621459.2020.1778481">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Phylogenetic tree inference using deep DNA sequencing is reshaping our understanding of rapidly evolving systems, such as the within-host battle between viruses and the immune system. Densely sampled phylogenetic trees can contain special features, including sampled ancestors in which we sequence a genotype along with its direct descendants, and polytomies in which multiple descendants arise simultaneously. These features are apparent after identifying zero-length branches in the tree. However, current maximum-likelihood based approaches are not capable of revealing such zero-length branches. In this article, we find these zero-length branches by introducing adaptive-LASSO-type regularization estimators for the branch lengths of phylogenetic trees, deriving their properties, and showing regularization to be a practically useful approach for phylogenetics. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Cheng Zhang and Vu Dinh and Frederick A. Matsen IV},
  doi          = {10.1080/01621459.2020.1778481},
  journal      = {Journal of the American Statistical Association},
  number       = {534},
  pages        = {858-873},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Nonbifurcating phylogenetic tree inference via the adaptive LASSO},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On the length of post-model-selection confidence intervals
conditional on polyhedral constraints. <em>JASA</em>, <em>116</em>(534),
845–857. (<a
href="https://doi.org/10.1080/01621459.2020.1732989">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Valid inference after model selection is currently a very active area of research. The polyhedral method, introduced in an article by Lee et al., allows for valid inference after model selection if the model selection event can be described by polyhedral constraints. In that reference, the method is exemplified by constructing two valid confidence intervals when the Lasso estimator is used to select a model. We here study the length of these intervals. For one of these confidence intervals, which is easier to compute, we find that its expected length is always infinite. For the other of these confidence intervals, whose computation is more demanding, we give a necessary and sufficient condition for its expected length to be infinite. In simulations, we find that this sufficient condition is typically satisfied, unless the selected model includes almost all or almost none of the available regressors. For the distribution of confidence interval length, we find that the κ -quantiles behave like 1 / ( 1 − κ ) for κ close to 1. Our results can also be used to analyze other confidence intervals that are based on the polyhedral method.},
  archive      = {J_JASA},
  author       = {Danijel Kivaranovic and Hannes Leeb},
  doi          = {10.1080/01621459.2020.1732989},
  journal      = {Journal of the American Statistical Association},
  number       = {534},
  pages        = {845-857},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {On the length of post-model-selection confidence intervals conditional on polyhedral constraints},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Auto-g-computation of causal effects on a network.
<em>JASA</em>, <em>116</em>(534), 833–844. (<a
href="https://doi.org/10.1080/01621459.2020.1811098">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Methods for inferring average causal effects have traditionally relied on two key assumptions: (i) the intervention received by one unit cannot causally influence the outcome of another; and (ii) units can be organized into nonoverlapping groups such that outcomes of units in separate groups are independent. In this article, we develop new statistical methods for causal inference based on a single realization of a network of connected units for which neither assumption (i) nor (ii) holds. The proposed approach allows both for arbitrary forms of interference, whereby the outcome of a unit may depend on interventions received by other units with whom a network path through connected units exists; and long range dependence, whereby outcomes for any two units likewise connected by a path in the network may be dependent. Under network versions of consistency and no unobserved confounding, inference is made tractable by an assumption that the networks outcome, treatment and covariate vectors are a single realization of a certain chain graph model. This assumption allows inferences about various network causal effects via the auto-g-computation algorithm , a network generalization of Robins’ well-known g-computation algorithm previously described for causal inference under assumptions (i) and (ii). Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Eric J. Tchetgen Tchetgen and Isabel R. Fulcher and Ilya Shpitser},
  doi          = {10.1080/01621459.2020.1811098},
  journal      = {Journal of the American Statistical Association},
  number       = {534},
  pages        = {833-844},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Auto-G-computation of causal effects on a network},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Intrinsic wavelet regression for curves of hermitian
positive definite matrices. <em>JASA</em>, <em>116</em>(534), 819–832.
(<a href="https://doi.org/10.1080/01621459.2019.1700129">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intrinsic wavelet transforms and wavelet estimation methods are introduced for curves in the non-Euclidean space of Hermitian positive definite matrices, with in mind the application to Fourier spectral estimation of multivariate stationary time series. The main focus is on intrinsic average-interpolation wavelet transforms in the space of positive definite matrices equipped with an affine-invariant Riemannian metric, and convergence rates of linear wavelet thresholding are derived for intrinsically smooth curves of Hermitian positive definite matrices. In the context of multivariate Fourier spectral estimation, intrinsic wavelet thresholding is equivariant under a change of basis of the time series, and nonlinear wavelet thresholding is able to capture localized features in the spectral density matrix across frequency, always guaranteeing positive definite estimates. The finite-sample performance of intrinsic wavelet thresholding is assessed by means of simulated data and compared to several benchmark estimators in the Riemannian manifold. Further illustrations are provided by examining the multivariate spectra of trial-replicated brain signal time series recorded during a learning experiment. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Joris Chau and Rainer von Sachs},
  doi          = {10.1080/01621459.2019.1700129},
  journal      = {Journal of the American Statistical Association},
  number       = {534},
  pages        = {819-832},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Intrinsic wavelet regression for curves of hermitian positive definite matrices},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A distributed and integrated method of moments for
high-dimensional correlated data analysis. <em>JASA</em>,
<em>116</em>(534), 805–818. (<a
href="https://doi.org/10.1080/01621459.2020.1736082">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article is motivated by a regression analysis of electroencephalography (EEG) neuroimaging data with high-dimensional correlated responses with multilevel nested correlations. We develop a divide-and-conquer procedure implemented in a fully distributed and parallelized computational scheme for statistical estimation and inference of regression parameters. Despite significant efforts in the literature, the computational bottleneck associated with high-dimensional likelihoods prevents the scalability of existing methods. The proposed method addresses this challenge by dividing responses into subvectors to be analyzed separately and in parallel on a distributed platform using pairwise composite likelihood. Theoretical challenges related to combining results from dependent data are overcome in a statistically efficient way using a meta-estimator derived from Hansen’s generalized method of moments. We provide a rigorous theoretical framework for efficient estimation, inference, and goodness-of-fit tests. We develop an R package for ease of implementation. We illustrate our method’s performance with simulations and the analysis of the EEG data, and find that iron deficiency is significantly associated with two auditory recognition memory related potentials in the left parietal-occipital region of the brain. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Emily C. Hector and Peter X.-K. Song},
  doi          = {10.1080/01621459.2020.1736082},
  journal      = {Journal of the American Statistical Association},
  number       = {534},
  pages        = {805-818},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {A distributed and integrated method of moments for high-dimensional correlated data analysis},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Inference on a new class of sample average treatment
effects. <em>JASA</em>, <em>116</em>(534), 798–804. (<a
href="https://doi.org/10.1080/01621459.2020.1730854">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We derive new variance formulas for inference on a general class of estimands of causal average treatment effects in a randomized control trial. We generalize the seminal work of Robins and show that when the researcher’s objective is inference on sample average treatment effect of the treated (SATT), a consistent variance estimator exists. Although this estimand is equal to the sample average treatment effect (SATE) in expectation, potentially large differences in both accuracy and coverage can occur by the change of estimand, even asymptotically. Inference on SATE, even using a conservative confidence interval, provides incorrect coverage of SATT. We demonstrate the applicability of the new theoretical results using an empirical application with hundreds of online experiments with an average sample size of approximately 100 million observations per experiment. An R package, estCI, that implements all the proposed estimation procedures is available. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Jasjeet S. Sekhon and Yotam Shem-Tov},
  doi          = {10.1080/01621459.2020.1730854},
  journal      = {Journal of the American Statistical Association},
  number       = {534},
  pages        = {798-804},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Inference on a new class of sample average treatment effects},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Parametric modeling of quantile regression coefficient
functions with longitudinal data. <em>JASA</em>, <em>116</em>(534),
783–797. (<a
href="https://doi.org/10.1080/01621459.2021.1892702">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In ordinary quantile regression, quantiles of different order are estimated one at a time. An alternative approach, which is referred to as quantile regression coefficients modeling ( qrcm ), is to model quantile regression coefficients as parametric functions of the order of the quantile. In this article, we describe how the qrcm qrcm qrcm paradigm can be applied to longitudinal data. We introduce a two-level quantile function, in which two different quantile regression models are used to describe the (conditional) distribution of the within-subject response and that of the individual effects. We propose a novel type of penalized fixed-effects estimator, and discuss its advantages over standard methods based on l 1 and l 2 penalization. We provide model identifiability conditions, derive asymptotic properties, describe goodness-of-fit measures and model selection criteria, present simulation results, and discuss an application. The proposed method has been implemented in the R package qrcm.},
  archive      = {J_JASA},
  author       = {Paolo Frumento and Matteo Bottai and Iván Fernández-Val},
  doi          = {10.1080/01621459.2021.1892702},
  journal      = {Journal of the American Statistical Association},
  number       = {534},
  pages        = {783-797},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Parametric modeling of quantile regression coefficient functions with longitudinal data},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Permutation tests for infection graphs. <em>JASA</em>,
<em>116</em>(534), 770–782. (<a
href="https://doi.org/10.1080/01621459.2019.1700128">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We formulate and analyze a novel hypothesis testing problem for inferring the edge structure of an infection graph. In our model, a disease spreads over a network via contagion or random infection, where the times between successive contagion events are independent exponential random variables with unknown rate parameters. A subset of nodes is also censored uniformly at random. Given the observed infection statuses of nodes in the network, the goal is to determine the underlying graph. We present a procedure based on permutation testing, and we derive sufficient conditions for the validity of our test in terms of automorphism groups of the graphs corresponding to the null and alternative hypotheses. Our test is easy to compute and does not involve estimating unknown parameters governing the process. We also derive risk bounds for our permutation test in a variety of settings, and relate our test statistic to approximate likelihood ratio testing and maximin tests. For graphs not satisfying the necessary symmetries, we provide an additional method for testing the significance of the graph structure, albeit at a higher computational cost. We conclude with an application to real data from an HIV infection network. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Justin Khim and Po-Ling Loh},
  doi          = {10.1080/01621459.2019.1700128},
  journal      = {Journal of the American Statistical Association},
  number       = {534},
  pages        = {770-782},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Permutation tests for infection graphs},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Log-linear bayesian additive regression trees for
multinomial logistic and count regression models. <em>JASA</em>,
<em>116</em>(534), 756–769. (<a
href="https://doi.org/10.1080/01621459.2020.1813587">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce Bayesian additive regression trees (BART) for log-linear models including multinomial logistic regression and count regression with zero-inflation and overdispersion. BART has been applied to nonparametric mean regression and binary classification problems in a range of settings. However, existing applications of BART have been mostly limited to models for Gaussian “data,” either observed or latent. This is primarily because efficient MCMC algorithms are available for Gaussian likelihoods. But while many useful models are naturally cast in terms of latent Gaussian variables, many others are not—including models considered in this article. We develop new data augmentation strategies and carefully specified prior distributions for these new models. Like the original BART prior, the new prior distributions are carefully constructed and calibrated to be flexible while guarding against overfitting. Together the new priors and data augmentation schemes allow us to implement an efficient MCMC sampler outside the context of Gaussian models. The utility of these new methods is illustrated with examples and an application to a previously published dataset. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Jared S. Murray},
  doi          = {10.1080/01621459.2020.1813587},
  journal      = {Journal of the American Statistical Association},
  number       = {534},
  pages        = {756-769},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Log-linear bayesian additive regression trees for multinomial logistic and count regression models},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Inter-subject analysis: A partial gaussian graphical model
approach. <em>JASA</em>, <em>116</em>(534), 746–755. (<a
href="https://doi.org/10.1080/01621459.2020.1841645">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Different from traditional intra-subject analysis, the goal of inter-subject analysis (ISA) is to explore the dependency structure between different subjects with the intra-subject dependency as nuisance. ISA has important applications in neuroscience to study the functional connectivity between brain regions under natural stimuli. We propose a modeling framework for ISA that is based on Gaussian graphical models, under which ISA can be converted to the problem of estimation and inference of a partial Gaussian graphical model. The main statistical challenge is that we do not impose sparsity constraints on the whole precision matrix and we only assume the inter-subject part is sparse. For estimation, we propose to estimate an alternative parameter to get around the nonsparse issue and it can achieve asymptotic consistency even if the intra-subject dependency is dense. For inference, we propose an “untangle and chord” procedure to de-bias our estimator. It is valid without the sparsity assumption on the inverse Hessian of the log-likelihood function. This inferential method is general and can be applied to many other statistical problems, thus it is of independent theoretical interest. Numerical experiments on both simulated and brain imaging data validate our methods and theory. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Cong Ma and Junwei Lu and Han Liu},
  doi          = {10.1080/01621459.2020.1841645},
  journal      = {Journal of the American Statistical Association},
  number       = {534},
  pages        = {746-755},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Inter-subject analysis: A partial gaussian graphical model approach},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Covariate regularized community detection in sparse graphs.
<em>JASA</em>, <em>116</em>(534), 734–745. (<a
href="https://doi.org/10.1080/01621459.2019.1706541">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we investigate community detection in networks in the presence of node covariates. In many instances, covariates and networks individually only give a partial view of the cluster structure. One needs to jointly infer the full cluster structure by considering both. In statistics, an emerging body of work has been focused on combining information from both the edges in the network and the node covariates to infer community memberships. However, so far the theoretical guarantees have been established in the dense regime, where the network can lead to perfect clustering under a broad parameter regime, and hence the role of covariates is often not clear. In this article, we examine sparse networks in conjunction with finite dimensional sub-Gaussian mixtures as covariates under moderate separation conditions. In this setting each individual source can only cluster a nonvanishing fraction of nodes correctly. We propose a simple optimization framework which improves clustering accuracy when the two sources carry partial information about the cluster memberships, and hence perform poorly on their own. Our optimization problem can be solved by scalable convex optimization algorithms. With a variety of simulated and real data examples, we show that the proposed method outperforms other existing methodology. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Bowei Yan and Purnamrita Sarkar},
  doi          = {10.1080/01621459.2019.1706541},
  journal      = {Journal of the American Statistical Association},
  number       = {534},
  pages        = {734-745},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Covariate regularized community detection in sparse graphs},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Inference for multivariate regression model based on
synthetic data generated using plug-in sampling. <em>JASA</em>,
<em>116</em>(534), 720–733. (<a
href="https://doi.org/10.1080/01621459.2021.1900860">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, the authors derive the likelihood-based exact inference for singly and multiply imputed synthetic data in the context of a multivariate regression model. The synthetic data are generated via the Plug-in Sampling method, where the unknown parameters in the model are set equal to the observed values of their point estimators based on the original data, and synthetic data are drawn from this estimated version of the model. Simulation studies are carried out in order to confirm the theoretical results. The authors provide exact test procedures, which in case multiple synthetic datasets are permissible, are compared with the asymptotic results of Reiter. An application using 2000 U.S. Current Population Survey public use data is discussed. Furthermore, properties of the proposed methodology are evaluated in scenarios where some of the conditions that were used to derive the methodology do not hold, namely for nonnormal and discrete distributed random variables, cases in which the inferential procedures developed still show very good performances.},
  archive      = {J_JASA},
  author       = {Ricardo Moura and Martin Klein and John Zylstra and Carlos A. Coelho and Bimal Sinha},
  doi          = {10.1080/01621459.2021.1900860},
  journal      = {Journal of the American Statistical Association},
  number       = {534},
  pages        = {720-733},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Inference for multivariate regression model based on synthetic data generated using plug-in sampling},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Statistical inference for online decision making via
stochastic gradient descent. <em>JASA</em>, <em>116</em>(534), 708–719.
(<a href="https://doi.org/10.1080/01621459.2020.1826325">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online decision making aims to learn the optimal decision rule by making personalized decisions and updating the decision rule recursively. It has become easier than before with the help of big data, but new challenges also come along. Since the decision rule should be updated once per step, an offline update which uses all the historical data is inefficient in computation and storage. To this end, we propose a completely online algorithm that can make decisions and update the decision rule online via stochastic gradient descent. It is not only efficient but also supports all kinds of parametric reward models. Focusing on the statistical inference of online decision making, we establish the asymptotic normality of the parameter estimator produced by our algorithm and the online inverse probability weighted value estimator we used to estimate the optimal value. Online plugin estimators for the variance of the parameter and value estimators are also provided and shown to be consistent, so that interval estimation and hypothesis test are possible using our method. The proposed algorithm and theoretical results are tested by simulations and a real data application to news article recommendation.},
  archive      = {J_JASA},
  author       = {Haoyu Chen and Wenbin Lu and Rui Song},
  doi          = {10.1080/01621459.2020.1826325},
  journal      = {Journal of the American Statistical Association},
  number       = {534},
  pages        = {708-719},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Statistical inference for online decision making via stochastic gradient descent},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Rejoinder: Learning optimal distributionally robust
individualized treatment rules. <em>JASA</em>, <em>116</em>(534),
699–707. (<a
href="https://doi.org/10.1080/01621459.2020.1866581">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Weibin Mo and Zhengling Qi and Yufeng Liu},
  doi          = {10.1080/01621459.2020.1866581},
  journal      = {Journal of the American Statistical Association},
  number       = {534},
  pages        = {699-707},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Rejoinder: Learning optimal distributionally robust individualized treatment rules},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Rejoinder: New objectives for policy learning.
<em>JASA</em>, <em>116</em>(534), 694–698. (<a
href="https://doi.org/10.1080/01621459.2020.1866580">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Nathan Kallus},
  doi          = {10.1080/01621459.2020.1866580},
  journal      = {Journal of the American Statistical Association},
  number       = {534},
  pages        = {694-698},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Rejoinder: New objectives for policy learning},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Discussion of kallus (2020) and mo et al. (2020).
<em>JASA</em>, <em>116</em>(534), 690–693. (<a
href="https://doi.org/10.1080/01621459.2020.1833887">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We discuss the results on improving the generalizability of individualized treatment rule following the work by Kallus and Mo et al. We note that the advocated weights in the work of Kallus are connected to the efficient score of the contrast function. We further propose a likelihood-ratio-based method (LR-ITR) to accommodate covariate shifts, and compare it to the CTE-DR-ITR method proposed by Mo et al. We provide the upper-bound on the risk function of the target population when both the covariate shift and the contrast function shift are present. Numerical studies show that LR-ITR can outperform CTE-DR-ITR when there is only covariate shift. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Muxuan Liang and Ying-Qi Zhao},
  doi          = {10.1080/01621459.2020.1833887},
  journal      = {Journal of the American Statistical Association},
  number       = {534},
  pages        = {690-693},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Discussion of kallus (2020) and mo et al. (2020)},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Discussion of kallus (2020) and mo, qi, and liu (2020): New
objectives for policy learning. <em>JASA</em>, <em>116</em>(534),
680–689. (<a
href="https://doi.org/10.1080/01621459.2020.1837140">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Sijia Li and Xiudi Li and Alex Luedtke},
  doi          = {10.1080/01621459.2020.1837140},
  journal      = {Journal of the American Statistical Association},
  number       = {534},
  pages        = {680-689},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Discussion of kallus (2020) and mo, qi, and liu (2020): New objectives for policy learning},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Discussion of kallus and mo, qi, and liu: New objectives for
policy learning. <em>JASA</em>, <em>116</em>(534), 675–679. (<a
href="https://doi.org/10.1080/01621459.2020.1844718">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Stijn Vansteelandt and Oliver Dukes},
  doi          = {10.1080/01621459.2020.1844718},
  journal      = {Journal of the American Statistical Association},
  number       = {534},
  pages        = {675-679},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Discussion of kallus and mo, qi, and liu: New objectives for policy learning},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Learning optimal distributionally robust individualized
treatment rules. <em>JASA</em>, <em>116</em>(534), 659–674. (<a
href="https://doi.org/10.1080/01621459.2020.1796359">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent development in the data-driven decision science has seen great advances in individualized decision making. Given data with individual covariates, treatment assignments and outcomes, policy makers best individualized treatment rule (ITR) that maximizes the expected outcome, known as the value function. Many existing methods assume that the training and testing distributions are the same. However, the estimated optimal ITR may have poor generalizability when the training and testing distributions are not identical. In this article, we consider the problem of finding an optimal ITR from a restricted ITR class where there are some unknown covariate changes between the training and testing distributions. We propose a novel distributionally robust ITR (DR-ITR) framework that maximizes the worst-case value function across the values under a set of underlying distributions that are “close” to the training distribution. The resulting DR-ITR can guarantee the performance among all such distributions reasonably well. We further propose a calibrating procedure that tunes the DR-ITR adaptively to a small amount of calibration data from a target population. In this way, the calibrated DR-ITR can be shown to enjoy better generalizability than the standard ITR based on our numerical studies. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Weibin Mo and Zhengling Qi and Yufeng Liu},
  doi          = {10.1080/01621459.2020.1796359},
  journal      = {Journal of the American Statistical Association},
  number       = {534},
  pages        = {659-674},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Learning optimal distributionally robust individualized treatment rules},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). More efficient policy learning via optimal retargeting.
<em>JASA</em>, <em>116</em>(534), 646–658. (<a
href="https://doi.org/10.1080/01621459.2020.1788948">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Policy learning can be used to extract individualized treatment regimes from observational data in healthcare, civics, e-commerce, and beyond. One big hurdle to policy learning is a commonplace lack of overlap in the data for different actions, which can lead to unwieldy policy evaluation and poorly performing learned policies. We study a solution to this problem based on retargeting, that is, changing the population on which policies are optimized. We first argue that at the population level, retargeting may induce little to no bias. We then characterize the optimal reference policy and retargeting weights in both binary-action and multi-action settings. We do this in terms of the asymptotic efficient estimation variance of the new learning objective. We further consider weights that additionally control for potential bias due to retargeting. Extensive empirical results in a simulation study and a case study of personalized job counseling demonstrate that retargeting is a fairly easy way to significantly improve any policy learning procedure applied to observational data. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Nathan Kallus},
  doi          = {10.1080/01621459.2020.1788948},
  journal      = {Journal of the American Statistical Association},
  number       = {534},
  pages        = {646-658},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {More efficient policy learning via optimal retargeting},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Introduction to theory and methods special issue on
precision medicine and individualized policy discovery part II.
<em>JASA</em>, <em>116</em>(534), 645. (<a
href="https://doi.org/10.1080/01621459.2021.1916266">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  doi          = {10.1080/01621459.2021.1916266},
  journal      = {Journal of the American Statistical Association},
  number       = {534},
  pages        = {645},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Introduction to theory and methods special issue on precision medicine and individualized policy discovery part II},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Causal inference with interference and noncompliance in
two-stage randomized experiments. <em>JASA</em>, <em>116</em>(534),
632–644. (<a
href="https://doi.org/10.1080/01621459.2020.1775612">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many social science experiments, subjects often interact with each other and as a result one unit’s treatment influences the outcome of another unit. Over the last decade, a significant progress has been made toward causal inference in the presence of such interference between units. Researchers have shown that the two-stage randomization of treatment assignment enables the identification of average direct and spillover effects. However, much of the literature has assumed perfect compliance with treatment assignment. In this article, we establish the nonparametric identification of the complier average direct and spillover effects in two-stage randomized experiments with interference and noncompliance. In particular, we consider the spillover effect of the treatment assignment on the treatment receipt as well as the spillover effect of the treatment receipt on the outcome. We propose consistent estimators and derive their randomization-based variances under the stratified interference assumption. We also prove the exact relationships between the proposed randomization-based estimators and the popular two-stage least squares estimators. The proposed methodology is motivated by and applied to our own randomized evaluation of India’s National Health Insurance Program (RSBY), where we find some evidence of spillover effects. The proposed methods are implemented via an open-source software package. Supplementary materials for this article, including a standardized description of the materials available for reproducing the work, are available as an online supplement.},
  archive      = {J_JASA},
  author       = {Kosuke Imai and Zhichao Jiang and Anup Malani},
  doi          = {10.1080/01621459.2020.1775612},
  journal      = {Journal of the American Statistical Association},
  number       = {534},
  pages        = {632-644},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Causal inference with interference and noncompliance in two-stage randomized experiments},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Do school districts affect NYC house prices? Identifying
border differences using a bayesian nonparametric approach to geographic
regression discontinuity designs. <em>JASA</em>, <em>116</em>(534),
619–631. (<a
href="https://doi.org/10.1080/01621459.2020.1817749">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {What is the premium on house price for a particular school district? To estimate this in New York City we use a novel implementation of a geographic regression discontinuity design (GeoRDD) built from Gaussian processes regression (kriging) to model spatial structure. With a GeoRDD, we specifically examine price differences along borders between “treatment” and “control” school districts. GeoRDDs extend RDDs to multivariate settings; location is the forcing variable and the border between school districts constitutes the discontinuity threshold. We first obtain a Bayesian posterior distribution of the price difference function, our nominal treatment effect, along the border. We then address nuances of having a functional estimand defined on a border with potentially intricate topology, particularly when defining and estimating causal estimands of the local average treatment effect (LATE). We test for nonzero LATE with a calibrated hypothesis test with good frequentist properties, which we further validate using a placebo test. Using our methodology, we identify substantial differences in price across several borders. In one case, a border separating Brooklyn and Queens, we estimate a statistically significant 20\% higher price for a house on the more desirable side. We also find that geographic features can undermine some of these comparisons. Supplementary materials for this article, including a standardized description of the materials available for reproducing the work, are available as an online supplement.},
  archive      = {J_JASA},
  author       = {Maxime Rischard and Zach Branson and Luke Miratrix and Luke Bornn},
  doi          = {10.1080/01621459.2020.1817749},
  journal      = {Journal of the American Statistical Association},
  number       = {534},
  pages        = {619-631},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Do school districts affect NYC house prices? identifying border differences using a bayesian nonparametric approach to geographic regression discontinuity designs},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bayesian structure learning in multilayered genomic
networks. <em>JASA</em>, <em>116</em>(534), 605–618. (<a
href="https://doi.org/10.1080/01621459.2020.1775611">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Integrative network modeling of data arising from multiple genomic platforms provides insight into the holistic picture of the interactive system, as well as the flow of information across many disease domains including cancer. The basic data structure consists of a sequence of hierarchically ordered datasets for each individual subject, which facilitates integration of diverse inputs, such as genomic, transcriptomic, and proteomic data. A primary analytical task in such contexts is to model the layered architecture of networks where the vertices can be naturally partitioned into ordered layers, dictated by multiple platforms, and exhibit both undirected and directed relationships. We propose a multilayered Gaussian graphical model (mlGGM) to investigate conditional independence structures in such multilevel genomic networks in human cancers. We implement a Bayesian node-wise selection (BANS) approach based on variable selection techniques that coherently accounts for the multiple types of dependencies in mlGGM; this flexible strategy exploits edge-specific prior knowledge and selects sparse and interpretable models. Through simulated data generated under various scenarios, we demonstrate that BANS outperforms other existing multivariate regression-based methodologies. Our integrative genomic network analysis for key signaling pathways across multiple cancer types highlights commonalities and differences of p53 integrative networks and epigenetic effects of BRCA2 on p53 and its interaction with T68 phosphorylated CHK2, that may have translational utilities of finding biomarkers and therapeutic targets. Supplementary materials for this article, including a standardized description of the materials available for reproducing the work, are available as an online supplement.},
  archive      = {J_JASA},
  author       = {Min Jin Ha and Francesco Claudio Stingo and Veerabhadran Baladandayuthapani},
  doi          = {10.1080/01621459.2020.1775611},
  journal      = {Journal of the American Statistical Association},
  number       = {534},
  pages        = {605-618},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Bayesian structure learning in multilayered genomic networks},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Recurrent events analysis with data collected at informative
clinical visits in electronic health records. <em>JASA</em>,
<em>116</em>(534), 594–604. (<a
href="https://doi.org/10.1080/01621459.2020.1801447">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although increasingly used as a data resource for assembling cohorts, electronic health records (EHRs) pose many analytic challenges. In particular, a patient’s health status influences when and what data are recorded, generating sampling bias in the collected data. In this article, we consider recurrent event analysis using EHR data. Conventional regression methods for event risk analysis usually require the values of covariates to be observed throughout the follow-up period. In EHR databases, time-dependent covariates are intermittently measured during clinical visits, and the timing of these visits is informative in the sense that it depends on the disease course. Simple methods, such as the last-observation-carried-forward approach, can lead to biased estimation. On the other hand, complex joint models require additional assumptions on the covariate process and cannot be easily extended to handle multiple longitudinal predictors. By incorporating sampling weights derived from estimating the observation time process, we develop a novel estimation procedure based on inverse-rate-weighting and kernel-smoothing for the semiparametric proportional rate model of recurrent events. The proposed methods do not require model specifications for the covariate processes and can easily handle multiple time-dependent covariates. Our methods are applied to a kidney transplant study for illustration. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Yifei Sun and Charles E. McCulloch and Kieren A. Marr and Chiung-Yu Huang},
  doi          = {10.1080/01621459.2020.1801447},
  journal      = {Journal of the American Statistical Association},
  number       = {534},
  pages        = {594-604},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Recurrent events analysis with data collected at informative clinical visits in electronic health records},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bayesian regression with undirected network predictors with
an application to brain connectome data. <em>JASA</em>,
<em>116</em>(534), 581–593. (<a
href="https://doi.org/10.1080/01621459.2020.1772079">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article focuses on the relationship between a measure of creativity and the human brain network for subjects in a brain connectome dataset obtained using a diffusion weighted magnetic resonance imaging procedure. We identify brain regions and interconnections that have a significant effect on creativity. Brain networks are often expressed in terms of symmetric adjacency matrices, with row and column indices of the matrix representing the regions of interest (ROI), and a cell entry signifying the estimated number of fiber bundles connecting the corresponding row and column ROIs. Current statistical practices for regression analysis with the brain network as the predictor and the measure of creativity as the response typically vectorize the network predictor matrices prior to any analysis, thus failing to account for the important structural information in the network. This results in poor inferential and predictive performance in presence of small sample sizes. To answer the scientific questions discussed above, we develop a flexible Bayesian framework that avoids reshaping the network predictor matrix, draws inference on brain ROIs and interconnections significantly related to creativity, and enables accurate prediction of creativity from a brain network. A novel class of network shrinkage priors for the coefficient corresponding to the network predictor is proposed to achieve these goals simultaneously. The Bayesian framework allows characterization of uncertainty in the findings. Empirical results in simulation studies illustrate substantial inferential and predictive gains of the proposed framework in comparison with the ordinary high-dimensional Bayesian shrinkage priors and penalized optimization schemes. Our framework yields new insights into the relationship of brain regions with creativity, also providing the uncertainty associated with the scientific findings. Supplementary materials for this article, including a standardized description of the materials available for reproducing the work, are available as an online supplement.},
  archive      = {J_JASA},
  author       = {Sharmistha Guha and Abel Rodriguez},
  doi          = {10.1080/01621459.2020.1772079},
  journal      = {Journal of the American Statistical Association},
  number       = {534},
  pages        = {581-593},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Bayesian regression with undirected network predictors with an application to brain connectome data},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Discovering heterogeneous exposure effects using
randomization inference in air pollution studies. <em>JASA</em>,
<em>116</em>(534), 569–580. (<a
href="https://doi.org/10.1080/01621459.2020.1870476">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Several studies have provided strong evidence that long-term exposure to air pollution, even at low levels, increases risk of mortality. As regulatory actions are becoming prohibitively expensive, robust evidence to guide the development of targeted interventions to protect the most vulnerable is needed. In this article, we introduce a novel statistical method that (i) discovers subgroups whose effects substantially differ from the population mean, and (ii) uses randomization-based tests to assess discovered heterogeneous effects. Also, we develop a sensitivity analysis method to assess the robustness of the conclusions to unmeasured confounding bias. Via simulation studies and theoretical arguments, we demonstrate that hypothesis testing focusing on the discovered subgroups can substantially increase statistical power to detect heterogeneity of the exposure effects. We apply the proposed de novo method to the data of 1,612,414 Medicare beneficiaries in the New England region in the United States for the period 2000–2006. We find that seniors aged between 81 and 85 with low income and seniors aged 85 and above have statistically significant greater causal effects of long-term exposure to PM 2.5 on 5-year mortality rate compared to the population mean.},
  archive      = {J_JASA},
  author       = {Kwonsang Lee and Dylan S. Small and Francesca Dominici},
  doi          = {10.1080/01621459.2020.1870476},
  journal      = {Journal of the American Statistical Association},
  number       = {534},
  pages        = {569-580},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Discovering heterogeneous exposure effects using randomization inference in air pollution studies},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Modeling multiple time-varying related groups: A dynamic
hierarchical bayesian approach with an application to the health and
retirement study. <em>JASA</em>, <em>116</em>(534), 558–568. (<a
href="https://doi.org/10.1080/01621459.2021.1886105">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the population of the older individuals continues to grow, it is important to study the relationship among the variables measuring financial health and physical health of the older individuals to better understand the demand for healthcare, and health insurance. We propose a semiparametric approach to jointly model these variables. We use data from the Health and Retirement Study which includes a set of correlated longitudinal variables measuring financial and physical health. In particular, we propose a dynamic hierarchical matrix stick-breaking process prior for some of the model parameters to account for the time dependent aspects of our data. This prior introduces dependence among the parameters across different groups which varies over time. A Lasso type shrinkage prior is specified for the covariates with time-invariant effects for selecting the set of covariates with significant effects on the outcomes. Through joint modeling, we are able to study the physical health of the older individuals conditional on their financial health, and vice-versa. Based on our analysis, we find that the health insurance (medicare) provided by the government (of the United States) to the older individuals is very effective, and it covers most of the medical expenditures. However, none of the health insurances conveniently cover the additional medical expenses due to chronic diseases like cancer and heart problem. Simulation studies are performed to assess the operating characteristics of our proposed modeling approach. Supplementary materials for this article, including a standardized description of the materials available for reproducing the work, are available as an online supplement.},
  archive      = {J_JASA},
  author       = {Kiranmoy Das and Pulak Ghosh and Michael J. Daniels},
  doi          = {10.1080/01621459.2021.1886105},
  journal      = {Journal of the American Statistical Association},
  number       = {534},
  pages        = {558-568},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Modeling multiple time-varying related groups: A dynamic hierarchical bayesian approach with an application to the health and retirement study},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On constraining projections of future climate using
observations and simulations from multiple climate models.
<em>JASA</em>, <em>116</em>(534), 546–557. (<a
href="https://doi.org/10.1080/01621459.2020.1851696">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Numerical climate models are used to project future climate change due to both anthropogenic and natural causes. Differences between projections from different climate models are a major source of uncertainty about future climate. Emergent relationships shared by multiple climate models have the potential to constrain our uncertainty when combined with historical observations. We combine projections from 13 climate models with observational data to quantify the impact of emergent relationships on projections of future warming in the Arctic at the end of the 21st century. We propose a hierarchical Bayesian framework based on a coexchangeable representation of the relationship between climate models and the Earth system. We show how emergent constraints fit into the coexchangeable representation, and extend it to account for internal variability simulated by the models and natural variability in the Earth system. Our analysis shows that projected warming in some regions of the Arctic may be more than 2 ° C lower and our uncertainty reduced by up to 30\% when constrained by historical observations. A detailed theoretical comparison with existing multi-model projection frameworks is also provided. In particular, we show that projections may be biased if we do not account for internal variability in climate model predictions. Supplementary materials for this article, including a standardized description of the materials available for reproducing the work, are available as an online supplement.},
  archive      = {J_JASA},
  author       = {Philip G. Sansom and David B. Stephenson and Thomas J. Bracegirdle},
  doi          = {10.1080/01621459.2020.1851696},
  journal      = {Journal of the American Statistical Association},
  number       = {534},
  pages        = {546-557},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {On constraining projections of future climate using observations and simulations from multiple climate models},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Gene-based association testing of dichotomous traits with
generalized functional linear mixed models using extended pedigrees:
Applications to age-related macular degeneration. <em>JASA</em>,
<em>116</em>(534), 531–545. (<a
href="https://doi.org/10.1080/01621459.2020.1799809">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Genetics plays a role in age-related macular degeneration (AMD), a common cause of blindness in the elderly. There is a need for powerful methods for carrying out region-based association tests between a dichotomous trait like AMD and genetic variants on family data. Here, we apply our new generalized functional linear mixed models (GFLMM) developed to test for gene-based association in a set of AMD families. Using common and rare variants, we observe significant association with two known AMD genes: CFH and ARMS2 . Using rare variants, we find suggestive signals in four genes: ASAH1 , CLEC6A , TMEM63C , and SGSM1 . Intriguingly, ASAH1 is down-regulated in AMD aqueous humor, and ASAH1 deficiency leads to retinal inflammation and increased vulnerability to oxidative stress. These findings were made possible by our GFLMM which model the effect of a major gene as a fixed mean, the polygenic contributions as a random variation, and the correlation of pedigree members by kinship coefficients. Simulations indicate that the GFLMM likelihood ratio tests (LRTs) accurately control the Type I error rates. The LRTs have similar or higher power than existing retrospective kernel and burden statistics. Our GFLMM-based statistics provide a new tool for conducting family-based genetic studies of complex diseases. Supplementary materials for this article, including a standardized description of the materials available for reproducing the work, are available as an online supplement.},
  archive      = {J_JASA},
  author       = {Yingda Jiang and Chi-Yang Chiu and Qi Yan and Wei Chen and Michael B. Gorin and Yvette P. Conley and M’Hamed Lajmi Lakhal-Chaieb and Richard J. Cook and Christopher I. Amos and Alexander F. Wilson and Joan E. Bailey-Wilson and Francis J. McMahon and Ana I. Vazquez and Ao Yuan and Xiaogang Zhong and Momiao Xiong and Daniel E. Weeks and Ruzong Fan},
  doi          = {10.1080/01621459.2020.1799809},
  journal      = {Journal of the American Statistical Association},
  number       = {534},
  pages        = {531-545},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Gene-based association testing of dichotomous traits with generalized functional linear mixed models using extended pedigrees: Applications to age-related macular degeneration},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bayesian joint modeling of multiple brain functional
networks. <em>JASA</em>, <em>116</em>(534), 518–530. (<a
href="https://doi.org/10.1080/01621459.2020.1796357">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Investigating the similarity and changes in brain networks under different mental conditions has become increasingly important in neuroscience research. A standard separate estimation strategy fails to pool information across networks and hence has reduced estimation accuracy and power to detect between-network differences. Motivated by an fMRI Stroop task experiment that involves multiple related tasks, we develop an integrative Bayesian approach for jointly modeling multiple brain networks that provides a systematic inferential framework for network comparisons. The proposed approach explicitly models shared and differential patterns via flexible Dirichlet process-based priors on edge probabilities. Conditional on edges, the connection strengths are modeled via Bayesian spike-and-slab prior on the precision matrix off-diagonals. Numerical simulations illustrate that the proposed approach has increased power to detect true differential edges while providing adequate control on false positives and achieves greater network estimation accuracy compared to existing methods. The Stroop task data analysis reveals greater connectivity differences between task and fixation that are concentrated in brain regions previously identified as differentially activated in Stroop task, and more nuanced connectivity differences between exertion and relaxed task. In contrast, penalized modeling approaches involving computationally burdensome permutation tests reveal negligible network differences between conditions that seem biologically implausible. Supplementary materials for this article, including a standardized description of the materials available for reproducing the work, are available as an online supplement.},
  archive      = {J_JASA},
  author       = {Joshua Lukemire and Suprateek Kundu and Giuseppe Pagnoni and Ying Guo},
  doi          = {10.1080/01621459.2020.1796357},
  journal      = {Journal of the American Statistical Association},
  number       = {534},
  pages        = {518-530},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Bayesian joint modeling of multiple brain functional networks},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bias and high-dimensional adjustment in observational
studies of peer effects. <em>JASA</em>, <em>116</em>(534), 507–517. (<a
href="https://doi.org/10.1080/01621459.2020.1796393">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Peer effects, in which an individual’s behavior is affected by peers’ behavior, are posited by multiple theories in the social sciences. Randomized field experiments that identify peer effects, however, are often expensive or infeasible, so many studies of peer effects use observational data, which is expected to suffer from confounding. Here we show, in the context of information and media diffusion, that high-dimensional adjustment of a nonexperimental control group (660 million observations) using propensity score models produces estimates of peer effects statistically indistinguishable from those using a large randomized experiment (215 million observations). Compared with the experiment, naive observational estimators overstate peer effects by over 300\% and commonly available variables (e.g., demographics) offer little bias reduction. Adjusting for a measure of prior behaviors closely related to the focal behavior reduces this bias by 91\%, while models adjusting for over 3700 past behaviors provide additional bias reduction, reducing bias by over 97\%, which is statistically indistinguishable from unbiasedness. This demonstrates how detailed records of behavior can improve studies of social influence, information diffusion, and imitation; these results are encouraging for the credibility of some studies but also cautionary for studies of peer effects in rare or new behaviors. More generally, these results show how large, high-dimensional datasets and statistical learning can be used to improve causal inference. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Dean Eckles and Eytan Bakshy},
  doi          = {10.1080/01621459.2020.1796393},
  journal      = {Journal of the American Statistical Association},
  number       = {534},
  pages        = {507-517},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Bias and high-dimensional adjustment in observational studies of peer effects},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The interplay of demographic variables and social distancing
scores in deep prediction of u.s. COVID-19 cases. <em>JASA</em>,
<em>116</em>(534), 492–506. (<a
href="https://doi.org/10.1080/01621459.2021.1901717">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the severity of the COVID-19 outbreak, we characterize the nature of the growth trajectories of counties in the United States using a novel combination of spectral clustering and the correlation matrix. As the United States and the rest of the world are still suffering from the effects of the virus, the importance of assigning growth membership to counties and understanding the determinants of the growth is increasingly evident. For the two communities (faster versus slower growth trajectories) we cluster the counties into, the average between-group correlation is 88.4\% whereas the average within-group correlations are 95.0\% and 93.8\%. The average growth rate for one group is 0.1589 and 0.1704 for the other, further suggesting that our methodology captures meaningful differences between the nature of the growth across various counties. Subsequently, we select the demographic features that are most statistically significant in distinguishing the communities: number of grocery stores, number of bars, Asian population, White population, median household income, number of people with the bachelor’s degrees, and population density. Lastly, we effectively predict the future growth of a given county with a long short-term memory (LSTM) recurrent neural network using three social distancing scores. The best-performing model achieves a median out-of-sample R 2 of 0.6251 for a four-day ahead prediction and we find that the number of communities and social distancing features play an important role in producing a more accurate forecasting. This comprehensive study captures the nature of the counties’ growth in cases at a very micro-level using growth communities, demographic factors, and social distancing performance to help government agencies utilize known information to make appropriate decisions regarding which potential counties to target resources and funding to. Supplementary materials for this article, including a standardized description of the materials available for reproducing the work, are available as an online supplement.},
  archive      = {J_JASA},
  author       = {Francesca Tang and Yang Feng and Hamza Chiheb and Jianqing Fan},
  doi          = {10.1080/01621459.2021.1901717},
  journal      = {Journal of the American Statistical Association},
  number       = {534},
  pages        = {492-506},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {The interplay of demographic variables and social distancing scores in deep prediction of U.S. COVID-19 cases},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The effects of stringent and mild interventions for
coronavirus pandemic. <em>JASA</em>, <em>116</em>(534), 481–491. (<a
href="https://doi.org/10.1080/01621459.2021.1897015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The pandemic of COVID-19 has caused severe public health consequences around the world. Many interventions of COVID-19 have been implemented. It is of great public health and social importance to evaluate the effects of interventions in the pandemic of COVID-19. With the help of a synthetic control method, the regression discontinuity, and a state-space compartmental model, we evaluated the treatment and stagewise effects of the intervention policies. We found statistically significant treatment effects of broad stringent interventions in Wenzhou and mild interventions in Shanghai to subdue the epidemic’s spread. If those reduction effects were not activated, the expected number of positive individuals would increase by 2.18 times on February 5, 2020, for Wenzhou and 7.69 times on February 4, 2020, for Shanghai, respectively. Alternatively, regression discontinuity elegantly identified the stringent ( p -value: &lt;0.001) and mild interventions ( p -value: 0.024) lowered the severity of the epidemic. Under the compartmental modeling for different interventions, we understood the importance of implementing the interventions. The highest level alert to COVID-19 was practical and crucial at the early stage of the epidemic. Furthermore, the physical/social distancing policy was necessary once the spread of COVID-19 continued. If appropriate control measures were implemented, then epidemic would be under control effectively and early. Supplementary materials for this article, including a standardized description of the materials available for reproducing the work, are available as an online supplement.},
  archive      = {J_JASA},
  author       = {Ting Tian and Jianbin Tan and Wenxiang Luo and Yukang Jiang and Minqiong Chen and Songpan Yang and Canhong Wen and Wenliang Pan and Xueqin Wang},
  doi          = {10.1080/01621459.2021.1897015},
  journal      = {Journal of the American Statistical Association},
  number       = {534},
  pages        = {481-491},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {The effects of stringent and mild interventions for coronavirus pandemic},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Rejoinder for “exponential-family embedding with
application to cell developmental trajectories for single-cell RNA-seq
data.” <em>JASA</em>, <em>116</em>(534), 478–480. (<a
href="https://doi.org/10.1080/01621459.2021.1892701">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Kevin Z. Lin and Jing Lei and Kathryn Roeder},
  doi          = {10.1080/01621459.2021.1892701},
  journal      = {Journal of the American Statistical Association},
  number       = {534},
  pages        = {478-480},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Rejoinder for “Exponential-family embedding with application to cell developmental trajectories for single-cell RNA-seq data”},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Discussion of “exponential-family embedding with application
to cell developmental trajectories for single-cell RNA-seq data.”
<em>JASA</em>, <em>116</em>(534), 475–477. (<a
href="https://doi.org/10.1080/01621459.2021.1880919">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Jian Hu and Mingyao Li},
  doi          = {10.1080/01621459.2021.1880919},
  journal      = {Journal of the American Statistical Association},
  number       = {534},
  pages        = {475-477},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Discussion of “Exponential-family embedding with application to cell developmental trajectories for single-cell RNA-seq data”},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Discussion of “exponential-family embedding with application
to cell developmental trajectories for single-cell RNA-seq data.”
<em>JASA</em>, <em>116</em>(534), 471–474. (<a
href="https://doi.org/10.1080/01621459.2021.1880920">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Zhicheng Ji and Hongkai Ji},
  doi          = {10.1080/01621459.2021.1880920},
  journal      = {Journal of the American Statistical Association},
  number       = {534},
  pages        = {471-474},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Discussion of “Exponential-family embedding with application to cell developmental trajectories for single-cell RNA-seq data”},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Exponential-family embedding with application to cell
developmental trajectories for single-cell RNA-seq data. <em>JASA</em>,
<em>116</em>(534), 457–470. (<a
href="https://doi.org/10.1080/01621459.2021.1886106">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scientists often embed cells into a lower-dimensional space when studying single-cell RNA-seq data for improved downstream analyses such as developmental trajectory analyses, but the statistical properties of such nonlinear embedding methods are often not well understood. In this article, we develop the exponential-family SVD (eSVD), a nonlinear embedding method for both cells and genes jointly with respect to a random dot product model using exponential-family distributions. Our estimator uses alternating minimization, which enables us to have a computationally efficient method, prove the identifiability conditions and consistency of our method, and provide statistically principled procedures to tune our method. All these qualities help advance the single-cell embedding literature, and we provide extensive simulations to demonstrate that the eSVD is competitive compared to other embedding methods. We apply the eSVD via Gaussian distributions where the standard deviations are proportional to the means to analyze a single-cell dataset of oligodendrocytes in mouse brains. Using the eSVD estimated embedding, we then investigate the cell developmental trajectories of the oligodendrocytes. While previous results are not able to distinguish the trajectories among the mature oligodendrocyte cell types, our diagnostics and results demonstrate there are two major developmental trajectories that diverge at mature oligodendrocytes. Supplementary materials for this article, including a standardized description of the materials available for reproducing the work, are available as an online supplementary materials .},
  archive      = {J_JASA},
  author       = {Kevin Z. Lin and Jing Lei and Kathryn Roeder},
  doi          = {10.1080/01621459.2021.1886106},
  journal      = {Journal of the American Statistical Association},
  number       = {534},
  pages        = {457-470},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Exponential-family embedding with application to cell developmental trajectories for single-cell RNA-seq data},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Handbook of environmental and ecological statistics.
<em>JASA</em>, <em>116</em>(533), 453–455. (<a
href="https://doi.org/10.1080/01621459.2021.1880232">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Grace S. Chiu},
  doi          = {10.1080/01621459.2021.1880232},
  journal      = {Journal of the American Statistical Association},
  number       = {533},
  pages        = {453-455},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Handbook of environmental and ecological statistics.},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Handbook of spatial epidemiology. <em>JASA</em>,
<em>116</em>(533), 451–453. (<a
href="https://doi.org/10.1080/01621459.2021.1880230">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Paula Moraga},
  doi          = {10.1080/01621459.2021.1880230},
  journal      = {Journal of the American Statistical Association},
  number       = {533},
  pages        = {451-453},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Handbook of spatial epidemiology},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Stochastic gradient markov chain monte carlo. <em>JASA</em>,
<em>116</em>(533), 433–450. (<a
href="https://doi.org/10.1080/01621459.2020.1847120">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Markov chain Monte Carlo (MCMC) algorithms are generally regarded as the gold standard technique for Bayesian inference. They are theoretically well-understood and conceptually simple to apply in practice. The drawback of MCMC is that performing exact inference generally requires all of the data to be processed at each iteration of the algorithm. For large datasets, the computational cost of MCMC can be prohibitive, which has led to recent developments in scalable Monte Carlo algorithms that have a significantly lower computational cost than standard MCMC. In this article, we focus on a particular class of scalable Monte Carlo algorithms, stochastic gradient Markov chain Monte Carlo (SGMCMC) which utilizes data subsampling techniques to reduce the per-iteration cost of MCMC. We provide an introduction to some popular SGMCMC algorithms and review the supporting theoretical results, as well as comparing the efficiency of SGMCMC algorithms against MCMC on benchmark examples. The supporting R code is available online at https://github.com/chris-nemeth/sgmcmc-review-paper .},
  archive      = {J_JASA},
  author       = {Christopher Nemeth and Paul Fearnhead},
  doi          = {10.1080/01621459.2020.1847120},
  journal      = {Journal of the American Statistical Association},
  number       = {533},
  pages        = {433-450},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Stochastic gradient markov chain monte carlo},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Stochastic tree search for estimating optimal dynamic
treatment regimes. <em>JASA</em>, <em>116</em>(533), 421–432. (<a
href="https://doi.org/10.1080/01621459.2020.1819294">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A dynamic treatment regime (DTR) is a sequence of decision rules that adapt to the time-varying states of an individual. Black-box learning methods have shown great potential in predicting the optimal treatments; however, the resulting DTRs lack interpretability, which is of paramount importance for medical experts to understand and implement. We present a stochastic tree-based reinforcement learning (ST-RL) method for estimating optimal DTRs in a multistage multitreatment setting with data from either randomized trials or observational studies. At each stage, ST-RL constructs a decision tree by first modeling the mean of counterfactual outcomes via nonparametric regression models, and then stochastically searching for the optimal tree-structured decision rule using a Markov chain Monte Carlo algorithm. We implement the proposed method in a backward inductive fashion through multiple decision stages. The proposed ST-RL delivers optimal DTRs with better interpretability and contributes to the existing literature in its non-greedy policy search. Additionally, ST-RL demonstrates stable and outstanding performances even with a large number of covariates, which is especially appealing when data are from large observational studies. We illustrate the performance of ST-RL through simulation studies, and also a real data application using esophageal cancer data collected from 1170 patients at MD Anderson Cancer Center from 1998 to 2012. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Yilun Sun and Lu Wang},
  doi          = {10.1080/01621459.2020.1819294},
  journal      = {Journal of the American Statistical Association},
  number       = {533},
  pages        = {421-432},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Stochastic tree search for estimating optimal dynamic treatment regimes},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Personalized policy learning using longitudinal mobile
health data. <em>JASA</em>, <em>116</em>(533), 410–420. (<a
href="https://doi.org/10.1080/01621459.2020.1785476">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Personalized policy represents a paradigm shift one decision rule for all users to an individualized decision rule for each user. Developing personalized policy in mobile health applications imposes challenges. First, for lack of adherence, data from each user are limited. Second, unmeasured contextual factors can potentially impact on decision making. Aiming to optimize immediate rewards, we propose using a generalized linear mixed modeling framework where population features and individual features are modeled as fixed and random effects, respectively, and synthesized to form the personalized policy. The group lasso type penalty is imposed to avoid overfitting of individual deviations from the population model. We examine the conditions under which the proposed method work in the presence of time-varying endogenous covariates, and provide conditional optimality and marginal consistency results of the expected immediate outcome under the estimated policies. We apply our method to develop personalized push (“prompt”) schedules in 294 app users, with the goal to maximize the prompt response rate given past app usage and other contextual factors. The proposed method compares favorably to existing estimation methods including using the R function “glmer” in a simulation study. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Xinyu Hu and Min Qian and Bin Cheng and Ying Kuen Cheung},
  doi          = {10.1080/01621459.2020.1785476},
  journal      = {Journal of the American Statistical Association},
  number       = {533},
  pages        = {410-420},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Personalized policy learning using longitudinal mobile health data},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning when-to-treat policies. <em>JASA</em>,
<em>116</em>(533), 392–409. (<a
href="https://doi.org/10.1080/01621459.2020.1831925">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many applied decision-making problems have a dynamic component: The policymaker needs not only to choose whom to treat, but also when to start which treatment. For example, a medical doctor may choose between postponing treatment (watchful waiting) and prescribing one of several available treatments during the many visits from a patient. We develop an “advantage doubly robust” estimator for learning such dynamic treatment rules using observational data under the assumption of sequential ignorability. We prove welfare regret bounds that generalize results for doubly robust learning in the single-step setting, and show promising empirical performance in several different contexts. Our approach is practical for policy optimization, and does not need any structural (e.g., Markovian) assumptions. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Xinkun Nie and Emma Brunskill and Stefan Wager},
  doi          = {10.1080/01621459.2020.1831925},
  journal      = {Journal of the American Statistical Association},
  number       = {533},
  pages        = {392-409},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Learning when-to-treat policies},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Off-policy estimation of long-term average outcomes with
applications to mobile health. <em>JASA</em>, <em>116</em>(533),
382–391. (<a
href="https://doi.org/10.1080/01621459.2020.1807993">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the recent advancements in wearables and sensing technology, health scientists are increasingly developing mobile health (mHealth) interventions. In mHealth interventions, mobile devices are used to deliver treatment to individuals as they go about their daily lives. These treatments are generally designed to impact a near time, proximal outcome such as stress or physical activity. The mHealth intervention policies, often called just-in-time adaptive interventions, are decision rules that map an individual’s current state (e.g., individual’s past behaviors as well as current observations of time, location, social activity, stress, and urges to smoke) to a particular treatment at each of many time points. The vast majority of current mHealth interventions deploy expert-derived policies. In this article, we provide an approach for conducting inference about the performance of one or more such policies using historical data collected under a possibly different policy. Our measure of performance is the average of proximal outcomes over a long time period should the particular mHealth policy be followed. We provide an estimator as well as confidence intervals. This work is motivated by HeartSteps, an mHealth physical activity intervention. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Peng Liao and Predrag Klasnja and Susan Murphy},
  doi          = {10.1080/01621459.2020.1807993},
  journal      = {Journal of the American Statistical Association},
  number       = {533},
  pages        = {382-391},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Off-policy estimation of long-term average outcomes with applications to mobile health},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust q-learning. <em>JASA</em>, <em>116</em>(533),
368–381. (<a
href="https://doi.org/10.1080/01621459.2020.1753522">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract– Q-learning is a regression-based approach that is widely used to formalize the development of an optimal dynamic treatment strategy. Finite dimensional working models are typically used to estimate certain nuisance parameters, and misspecification of these working models can result in residual confounding and/or efficiency loss. We propose a robust Q-learning approach which allows estimating such nuisance parameters using data-adaptive techniques. We study the asymptotic behavior of our estimators and provide simulation studies that highlight the need for and usefulness of the proposed method in practice. We use the data from the “Extending Treatment Effectiveness of Naltrexone” multistage randomized trial to illustrate our proposed methods. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Ashkan Ertefaie and James R. McKay and David Oslin and Robert L. Strawderman},
  doi          = {10.1080/01621459.2020.1753522},
  journal      = {Journal of the American Statistical Association},
  number       = {533},
  pages        = {368-381},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Robust Q-learning},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A multi-resolution theory for approximating
infinite-p-zero-n: Transitional inference, individualized predictions,
and a world without bias-variance tradeoff. <em>JASA</em>,
<em>116</em>(533), 353–367. (<a
href="https://doi.org/10.1080/01621459.2020.1844210">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transitional inference is an empiricism concept, rooted and practiced in clinical medicine since ancient Greece. Knowledge and experiences gained from treating one entity (e.g., a disease or a group of patients) are applied to treat a related but distinctively different one (e.g., a similar disease or a new patient). This notion of “transition to the similar” renders individualized treatments an operational meaning, yet its theoretical foundation defies the familiar inductive inference framework. The uniqueness of entities is the result of potentially an infinite number of attributes (hence p = ∞ ), which entails zero direct training sample size (i.e., n = 0) because genuine guinea pigs do not exist. However, the literature on wavelets and on sieve methods for nonparametric estimation suggests a principled approximation theory for transitional inference via a multi-resolution (MR) perspective, where we use the resolution level to index the degree of approximation to ultimate individuality. MR inference seeks a primary resolution indexing an indirect training sample, which provides enough matched attributes to increase the relevance of the results to the target individuals and yet still accumulate sufficient indirect sample sizes for robust estimation. Theoretically, MR inference relies on an infinite-term ANOVA-type decomposition, providing an alternative way to model sparsity via the decay rate of the resolution bias as a function of the primary resolution level. Unexpectedly, this decomposition reveals a world without variance when the outcome is a deterministic function of potentially infinitely many predictors. In this deterministic world, the optimal resolution prefers over-fitting in the traditional sense when the resolution bias decays sufficiently rapidly. Furthermore, there can be many “descents” in the prediction error curve, when the contributions of predictors are inhomogeneous and the ordering of their importance does not align with the order of their inclusion in prediction. These findings may hint at a deterministic approximation theory for understanding the apparently over-fitting resistant phenomenon of some over-saturated models in machine learning.},
  archive      = {J_JASA},
  author       = {Xinran Li and Xiao-Li Meng},
  doi          = {10.1080/01621459.2020.1844210},
  journal      = {Journal of the American Statistical Association},
  number       = {533},
  pages        = {353-367},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {A multi-resolution theory for approximating infinite-p-zero-n: Transitional inference, individualized predictions, and a world without bias-variance tradeoff},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Estimation and validation of ratio-based conditional average
treatment effects using observational data. <em>JASA</em>,
<em>116</em>(533), 335–352. (<a
href="https://doi.org/10.1080/01621459.2020.1772080">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While sample sizes in randomized clinical trials are large enough to estimate the average treatment effect well, they are often insufficient for estimation of treatment-covariate interactions critical to studying data-driven precision medicine. Observational data from real world practice may play an important role in alleviating this problem. One common approach in trials is to predict the outcome of interest with separate regression models in each treatment arm, and estimate the treatment effect based on the contrast of the predictions. Unfortunately, this simple approach may induce spurious treatment-covariate interaction in observational studies when the regression model is misspecified. Motivated by the need of modeling the number of relapses in multiple sclerosis (MS) patients, where the ratio of relapse rates is a natural choice of the treatment effect, we propose to estimate the conditional average treatment effect (CATE) as the ratio of expected potential outcomes, and derive a doubly robust estimator of this CATE in a semiparametric model of treatment-covariate interactions. We also provide a validation procedure to check the quality of the estimator on an independent sample. We conduct simulations to demonstrate the finite sample performance of the proposed methods, and illustrate their advantages on real data by examining the treatment effect of dimethyl fumarate compared to teriflunomide in MS patients. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Steve Yadlowsky and Fabio Pellegrini and Federica Lionetto and Stefan Braune and Lu Tian},
  doi          = {10.1080/01621459.2020.1772080},
  journal      = {Journal of the American Statistical Association},
  number       = {533},
  pages        = {335-352},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Estimation and validation of ratio-based conditional average treatment effects using observational data},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). BAGS: A bayesian adaptive group sequential trial design with
subgroup-specific survival comparisons. <em>JASA</em>,
<em>116</em>(533), 322–334. (<a
href="https://doi.org/10.1080/01621459.2020.1837142">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A Bayesian group sequential design is proposed that performs survival comparisons within patient subgroups in randomized trials where treatment–subgroup interactions may be present. A latent subgroup membership variable is assumed to allow the design to adaptively combine homogeneous subgroups, or split heterogeneous subgroups, to improve the procedure’s within-subgroup power. If a baseline covariate related to survival is available, the design may incorporate this information to improve subgroup identification while basing the comparative test on the average hazard ratio. General guidelines are provided for calibrating prior hyperparameters and design parameters to control the overall Type I error rate and optimize performance. Simulations show that the design is robust under a wide variety of different scenarios. When two or more subgroups are truly homogeneous but differ from the other subgroups, the proposed method is substantially more powerful than tests that either ignore subgroups or conduct a separate test within each subgroup. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Ruitao Lin and Peter F. Thall and Ying Yuan},
  doi          = {10.1080/01621459.2020.1837142},
  journal      = {Journal of the American Statistical Association},
  number       = {533},
  pages        = {322-334},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {BAGS: A bayesian adaptive group sequential trial design with subgroup-specific survival comparisons},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Estimation of optimal individualized treatment rules using a
covariate-specific treatment effect curve with high-dimensional
covariates. <em>JASA</em>, <em>116</em>(533), 309–321. (<a
href="https://doi.org/10.1080/01621459.2020.1865167">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With a large number of baseline covariates, we propose a new semiparametric modeling strategy for heterogeneous treatment effect estimation and individualized treatment selection, which are two major goals in personalized medicine. We achieve the first goal through estimating a covariate-specific treatment effect (CSTE) curve modeled as an unknown function of a weighted linear combination of all baseline covariates. The weight or the coefficient for each covariate is estimated by fitting a sparse semiparametric logistic single-index coefficient model. The CSTE curve is estimated by a spline-backfitted kernel procedure, which enables us to further construct a simultaneous confidence band (SCB) for the CSTE curve under a desired confidence level. Based on the SCB, we find the subgroups of patients that benefit from each treatment, so that we can make individualized treatment selection. The innovations of the proposed method are 3-fold. First, the proposed method can quantify variability associated with the estimated optimal individualized treatment rule with high-dimensional covariates. Second, the proposed method is very flexible to depict both local and global associations between the treatment and baseline covariates in the presence of high-dimensional covariates, and thus it enjoys flexibility while achieving dimensionality reduction. Third, the SCB achieves the nominal confidence level asymptotically, and it provides a uniform inferential tool in making individualized treatment decisions. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Wenchuan Guo and Xiao-Hua Zhou and Shujie Ma},
  doi          = {10.1080/01621459.2020.1865167},
  journal      = {Journal of the American Statistical Association},
  number       = {533},
  pages        = {309-321},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Estimation of optimal individualized treatment rules using a covariate-specific treatment effect curve with high-dimensional covariates},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Selecting and ranking individualized treatment rules with
unmeasured confounding. <em>JASA</em>, <em>116</em>(533), 295–308. (<a
href="https://doi.org/10.1080/01621459.2020.1736083">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is common to compare individualized treatment rules based on the value function, which is the expected potential outcome under the treatment rule. Although the value function is not point-identified when there is unmeasured confounding, it still defines a partial order among the treatment rules under Rosenbaum’s sensitivity analysis model. We first consider how to compare two treatment rules with unmeasured confounding in the single-decision setting and then use this pairwise test to rank multiple treatment rules. We consider how to, among many treatment rules, select the best rules and select the rules that are better than a control rule. The proposed methods are illustrated using two real examples, one about the benefit of malaria prevention programs to different age groups and another about the effect of late retirement on senior health in different gender and occupation groups. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Bo Zhang and Jordan Weiss and Dylan S. Small and Qingyuan Zhao},
  doi          = {10.1080/01621459.2020.1736083},
  journal      = {Journal of the American Statistical Association},
  number       = {533},
  pages        = {295-308},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Selecting and ranking individualized treatment rules with unmeasured confounding},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Improved doubly robust estimation in learning optimal
individualized treatment rules. <em>JASA</em>, <em>116</em>(533),
283–294. (<a
href="https://doi.org/10.1080/01621459.2020.1725522">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Individualized treatment rules (ITRs) recommend treatment according to patient characteristics. There is a growing interest in developing novel and efficient statistical methods in constructing ITRs. We propose an improved doubly robust estimator of the optimal ITRs. The proposed estimator is based on a direct optimization of an augmented inverse-probability weighted estimator of the expected clinical outcome over a class of ITRs. The method enjoys two key properties. First, it is doubly robust, meaning that the proposed estimator is consistent when either the propensity score or the outcome model is correct. Second, it achieves the smallest variance among the class of doubly robust estimators when the propensity score model is correctly specified, regardless of the specification of the outcome model. Simulation studies show that the estimated ITRs obtained from our method yield better results than those obtained from current popular methods. Data from the Sequenced Treatment Alternatives to Relieve Depression study is analyzed as an illustrative example. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Yinghao Pan and Ying-Qi Zhao},
  doi          = {10.1080/01621459.2020.1725522},
  journal      = {Journal of the American Statistical Association},
  number       = {533},
  pages        = {283-294},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Improved doubly robust estimation in learning optimal individualized treatment rules},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning individualized treatment rules for multiple-domain
latent outcomes. <em>JASA</em>, <em>116</em>(533), 269–282. (<a
href="https://doi.org/10.1080/01621459.2020.1817751">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For many mental disorders, latent mental status from multiple-domain psychological or clinical symptoms may perform as a better characterization of the underlying disorder status than a simple summary score of the symptoms, and they may also serve as more reliable and representative features to differentiate treatment responses. Therefore, to address the complexity and heterogeneity of treatment responses for mental disorders, we provide a new paradigm for learning optimal individualized treatment rules (ITRs) by modeling patients’ latent mental status. We first learn the multi-domain latent states at baseline from the observed symptoms under a restricted Boltzmann machine (RBM) model, which encodes patients’ heterogeneous symptoms using an economical number of latent variables and yet remains flexible. We then optimize a value function defined by the latent states after treatment by exploiting a transformation of the observed symptoms based on the RBM without modeling the relationship between the latent mental states before and after treatment. The optimal treatment rules are derived using a weighted large margin classifier. We derive the convergence rate of the proposed estimator under the latent models. Simulation studies are conducted to test the performance of the proposed method. Finally, we apply the developed method to real world studies and we demonstrate the utility and advantage of our method in tailoring treatments for patients with major depression, and identify patient subgroups informative for treatment recommendations. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Yuan Chen and Donglin Zeng and Yuanjia Wang},
  doi          = {10.1080/01621459.2020.1817751},
  journal      = {Journal of the American Statistical Association},
  number       = {533},
  pages        = {269-282},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Learning individualized treatment rules for multiple-domain latent outcomes},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Doubly robust estimation of optimal dosing strategies.
<em>JASA</em>, <em>116</em>(533), 256–268. (<a
href="https://doi.org/10.1080/01621459.2020.1753521">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of precision medicine is to tailor treatment strategies on an individual patient level. Although several estimation techniques have been developed for determining optimal treatment rules, the majority of methods focus on the case of a dichotomous treatment, an example being the dynamic weighted ordinary least squares regression approach of Wallace and Moodie. We propose an extension to the aforementioned framework to allow for a continuous treatment with the ultimate goal of estimating optimal dosing strategies. The proposed method is shown to be doubly robust against model misspecification whenever the implemented weights satisfy a particular balancing condition. A broad class of weight functions can be derived from the balancing condition, providing a flexible regression based estimation method in the context of adaptive treatment strategies for continuous valued treatments. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Juliana Schulz and Erica E. M. Moodie},
  doi          = {10.1080/01621459.2020.1753521},
  journal      = {Journal of the American Statistical Association},
  number       = {533},
  pages        = {256-268},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Doubly robust estimation of optimal dosing strategies},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Statistical inference for online decision making: In a
contextual bandit setting. <em>JASA</em>, <em>116</em>(533), 240–255.
(<a href="https://doi.org/10.1080/01621459.2020.1770098">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online decision making problem requires us to make a sequence of decisions based on incremental information. Common solutions often need to learn a reward model of different actions given the contextual information and then maximize the long-term reward. It is meaningful to know if the posited model is reasonable and how the model performs in the asymptotic sense. We study this problem under the setup of the contextual bandit framework with a linear reward model. The ε -greedy policy is adopted to address the classic exploration-and-exploitation dilemma. Using the martingale central limit theorem, we show that the online ordinary least squares estimator of model parameters is asymptotically normal. When the linear model is misspecified, we propose the online weighted least squares estimator using the inverse propensity score weighting and also establish its asymptotic normality. Based on the properties of the parameter estimators, we further show that the in-sample inverse propensity weighted value estimator is asymptotically normal. We illustrate our results using simulations and an application to a news article recommendation dataset from Yahoo!. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Haoyu Chen and Wenbin Lu and Rui Song},
  doi          = {10.1080/01621459.2020.1770098},
  journal      = {Journal of the American Statistical Association},
  number       = {533},
  pages        = {240-255},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Statistical inference for online decision making: In a contextual bandit setting},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Efficient estimation of optimal regimes under a no direct
effect assumption. <em>JASA</em>, <em>116</em>(533), 224–239. (<a
href="https://doi.org/10.1080/01621459.2020.1856117">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We derive new estimators of an optimal joint testing and treatment regime under the no direct effect (NDE) assumption that a given laboratory, diagnostic, or screening test has no effect on a patient’s clinical outcomes except through the effect of the test results on the choice of treatment. We model the optimal joint strategy with an optimal structural nested mean model (opt-SNMM). The proposed estimators are more efficient than previous estimators of the parameters of an opt-SNMM because they efficiently leverage the “NDE of testing” assumption. Our methods will be of importance to decision scientists who either perform cost-benefit analyses or are tasked with the estimation of the “value of information” supplied by an expensive diagnostic test (such as an MRI to screen for lung cancer). Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Lin Liu and Zach Shahn and James M. Robins and Andrea Rotnitzky},
  doi          = {10.1080/01621459.2020.1856117},
  journal      = {Journal of the American Statistical Association},
  number       = {533},
  pages        = {224-239},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Efficient estimation of optimal regimes under a no direct effect assumption},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A two-part framework for estimating individualized treatment
rules from semicontinuous outcomes. <em>JASA</em>, <em>116</em>(533),
210–223. (<a
href="https://doi.org/10.1080/01621459.2020.1801449">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Health care payments are an important component of health care utilization and are thus a major focus in health services and health policy applications. However, payment outcomes are semicontinuous in that over a given period of time some patients incur no payments and some patients incur large costs. Individualized treatment rules (ITRs) are a major part of the push for tailoring treatments and interventions to patients, yet there is a little work focused on estimating ITRs from semicontinuous outcomes. In this article, we develop a framework for estimation of ITRs based on two-part modeling, wherein the ITR is estimated by separately targeting the zero part of the outcome and the strictly positive part. To improve performance when high-dimensional covariates are available, we leverage a scientifically plausible penalty that simultaneously selects variables and encourages the signs of coefficients for each variable to agree between the two components of the ITR. We develop an efficient algorithm for computation and prove oracle inequalities for the resulting estimation and prediction errors. We demonstrate the effectiveness of our approach in simulated examples and in a study of a health system intervention. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Jared D. Huling and Maureen A. Smith and Guanhua Chen},
  doi          = {10.1080/01621459.2020.1801449},
  journal      = {Journal of the American Statistical Association},
  number       = {533},
  pages        = {210-223},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {A two-part framework for estimating individualized treatment rules from semicontinuous outcomes},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Rejoinder: Optimal individualized decision rules using
instrumental variable methods. <em>JASA</em>, <em>116</em>(533),
207–209. (<a
href="https://doi.org/10.1080/01621459.2020.1865166">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Hongxiang Qiu and Marco Carone and Ekaterina Sadikova and Maria Petukhova and Ronald C. Kessler and Alex Luedtke},
  doi          = {10.1080/01621459.2020.1865166},
  journal      = {Journal of the American Statistical Association},
  number       = {533},
  pages        = {207-209},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Rejoinder: Optimal individualized decision rules using instrumental variable methods},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Machine intelligence for individualized decision making
under a counterfactual world: A rejoinder. <em>JASA</em>,
<em>116</em>(533), 200–206. (<a
href="https://doi.org/10.1080/01621459.2021.1872580">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Yifan Cui and Eric Tchetgen Tchetgen},
  doi          = {10.1080/01621459.2021.1872580},
  journal      = {Journal of the American Statistical Association},
  number       = {533},
  pages        = {200-206},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Machine intelligence for individualized decision making under a counterfactual world: A rejoinder},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Discussion of cui and tchetgen tchetgen (2020) and qiu et
al. (2020). <em>JASA</em>, <em>116</em>(533), 196–199. (<a
href="https://doi.org/10.1080/01621459.2020.1832500">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Bo Zhang and Hongming Pu},
  doi          = {10.1080/01621459.2020.1832500},
  journal      = {Journal of the American Statistical Association},
  number       = {533},
  pages        = {196-199},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Discussion of cui and tchetgen tchetgen (2020) and qiu et al. (2020)},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Comment: Individualized treatment rules under endogeneity.
<em>JASA</em>, <em>116</em>(533), 192–195. (<a
href="https://doi.org/10.1080/01621459.2020.1831923">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Sukjin Han},
  doi          = {10.1080/01621459.2020.1831923},
  journal      = {Journal of the American Statistical Association},
  number       = {533},
  pages        = {192-195},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Comment: Individualized treatment rules under endogeneity},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Optimal individualized decision rules using instrumental
variable methods. <em>JASA</em>, <em>116</em>(533), 174–191. (<a
href="https://doi.org/10.1080/01621459.2020.1745814">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is an extensive literature on the estimation and evaluation of optimal individualized treatment rules in settings where all confounders of the effect of treatment on outcome are observed. We study the development of individualized decision rules in settings where some of these confounders may not have been measured but a valid binary instrument is available for a binary treatment. We first consider individualized treatment rules, which will naturally be most interesting in settings where it is feasible to intervene directly on treatment. We then consider a setting where intervening on treatment is infeasible, but intervening to encourage treatment is feasible. In both of these settings, we also handle the case that the treatment is a limited resource so that optimal interventions focus the available resources on those individuals who will benefit most from treatment. Given a reference rule, we evaluate an optimal individualized rule by its average causal effect relative to a prespecified reference rule. We develop methods to estimate optimal individualized rules and construct asymptotically efficient plug-in estimators of the corresponding average causal effect relative to a prespecified reference rule. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Hongxiang Qiu and Marco Carone and Ekaterina Sadikova and Maria Petukhova and Ronald C. Kessler and Alex Luedtke},
  doi          = {10.1080/01621459.2020.1745814},
  journal      = {Journal of the American Statistical Association},
  number       = {533},
  pages        = {174-191},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Optimal individualized decision rules using instrumental variable methods},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). A semiparametric instrumental variable approach to optimal
treatment regimes under endogeneity. <em>JASA</em>, <em>116</em>(533),
162–173. (<a
href="https://doi.org/10.1080/01621459.2020.1783272">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is a fast-growing literature on estimating optimal treatment regimes based on randomized trials or observational studies under a key identifying condition of no unmeasured confounding. Because confounding by unmeasured factors cannot generally be ruled out with certainty in observational studies or randomized trials subject to noncompliance, we propose a general instrumental variable (IV) approach to learning optimal treatment regimes under endogeneity. Specifically, we establish identification of both value function E [ Y D ( L ) ] for a given regime D and optimal regimes arg max D E [ Y D ( L ) ] with the aid of a binary IV, when no unmeasured confounding fails to hold. We also construct novel multiply robust classification-based estimators. Furthermore, we propose to identify and estimate optimal treatment regimes among those who would comply to the assigned treatment under a monotonicity assumption. In this latter case, we establish the somewhat surprising result that complier optimal regimes can be consistently estimated without directly collecting compliance information and therefore without the complier average treatment effect itself being identified. Our approach is illustrated via extensive simulation studies and a data application on the effect of child rearing on labor participation. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Yifan Cui and Eric Tchetgen Tchetgen},
  doi          = {10.1080/01621459.2020.1783272},
  journal      = {Journal of the American Statistical Association},
  number       = {533},
  pages        = {162-173},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {A semiparametric instrumental variable approach to optimal treatment regimes under endogeneity},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Introduction to the theory and methods special issue on
precision medicine and individualized policy discovery. <em>JASA</em>,
<em>116</em>(533), 159–161. (<a
href="https://doi.org/10.1080/01621459.2020.1863224">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce the Theory and Methods Special Issue on Precision Medicine and Individualized Policy Discovery. The issue consists of four discussion papers, grouped into two pairs, and sixteen regular research papers that cover many important lines of research on data-driven decision making. We hope that the many provocative and original ideas presented herein will inspire further work and development in precision medicine and personalization.},
  archive      = {J_JASA},
  author       = {Michael R. Kosorok and Eric B. Laber and Dylan S. Small and Donglin Zeng},
  doi          = {10.1080/01621459.2020.1863224},
  journal      = {Journal of the American Statistical Association},
  number       = {533},
  pages        = {159-161},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Introduction to the theory and methods special issue on precision medicine and individualized policy discovery},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Brain regions identified as being associated with verbal
reasoning through the use of imaging regression via internal variation.
<em>JASA</em>, <em>116</em>(533), 144–158. (<a
href="https://doi.org/10.1080/01621459.2020.1766468">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract– Brain-imaging data have been increasingly used to understand intellectual disabilities. Despite significant progress in biomedical research, the mechanisms for most of the intellectual disabilities remain unknown. Finding the underlying neurological mechanisms has proved difficult, especially in children due to the rapid development of their brains. We investigate verbal reasoning, which is a reliable measure of an individual’s general intellectual abilities, and develop a class of high-order imaging regression models to identify brain subregions which might be associated with this specific intellectual ability. A key novelty of our method is to take advantage of spatial brain structures, and specifically the piecewise smooth nature of most imaging coefficients in the form of high-order tensors. Our approach provides an effective and urgently needed method for identifying brain subregions potentially underlying certain intellectual disabilities. The idea behind our approach is a carefully constructed concept called internal variation (IV). The IV employs tensor decomposition and provides a computationally feasible substitution for total variation, which has been considered suitable to deal with similar problems but may not be scalable to high-order tensor regression. Before applying our method to analyze the real data, we conduct comprehensive simulation studies to demonstrate the validity of our method in imaging signal identification. Next, we present our results from the analysis of a dataset based on the Philadelphia Neurodevelopmental Cohort for which we preprocessed the data including reorienting, bias-field correcting, extracting, normalizing, and registering the magnetic resonance images from 978 individuals. Our analysis identified a subregion across the cingulate cortex and the corpus callosum as being associated with individuals’ verbal reasoning ability, which, to the best of our knowledge, is a novel region that has not been reported in the literature. This finding is useful in further investigation of functional mechanisms for verbal reasoning. Supplementary materials for this article, including a standardized description of the materials available for reproducing the work, are available as an online supplement.},
  archive      = {J_JASA},
  author       = {Long Feng and Xuan Bi and Heping Zhang},
  doi          = {10.1080/01621459.2020.1766468},
  journal      = {Journal of the American Statistical Association},
  number       = {533},
  pages        = {144-158},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Brain regions identified as being associated with verbal reasoning through the use of imaging regression via internal variation},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A penalized regression framework for building polygenic risk
models based on summary statistics from genome-wide association studies
and incorporating external information. <em>JASA</em>,
<em>116</em>(533), 133–143. (<a
href="https://doi.org/10.1080/01621459.2020.1764849">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large-scale genome-wide association studies (GWAS) provide opportunities for developing genetic risk prediction models that have the potential to improve disease prevention, intervention or treatment. The key step is to develop polygenic risk score (PRS) models with high predictive performance for a given disease, which typically requires a large training dataset for selecting truly associated single nucleotide polymorphisms (SNPs) and estimating effect sizes accurately. Here, we develop a comprehensive penalized regression for fitting l 1 regularized regression models to GWAS summary statistics. We propose incorporating pleiotropy and annotation information into PRS (PANPRS) development through suitable formulation of penalty functions and associated tuning parameters. Extensive simulations show that PANPRS performs equally well or better than existing PRS methods when no functional annotation or pleiotropy is incorporated. When functional annotation data and pleiotropy are informative, PANPRS substantially outperforms existing PRS methods in simulations. Finally, we applied our methods to build PRS for type 2 diabetes and melanoma and found that incorporating relevant functional annotations and GWAS of genetically related traits improved prediction of these two complex diseases. Supplementary materials for this article, including a standardized description of the materials available for reproducing the work, are available as an online supplement.},
  archive      = {J_JASA},
  author       = {Ting-Huei Chen and Nilanjan Chatterjee and Maria Teresa Landi and Jianxin Shi},
  doi          = {10.1080/01621459.2020.1764849},
  journal      = {Journal of the American Statistical Association},
  number       = {533},
  pages        = {133-143},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {A penalized regression framework for building polygenic risk models based on summary statistics from genome-wide association studies and incorporating external information},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Modeling and regionalization of china’s PM2.5 using
spatial-functional mixture models. <em>JASA</em>, <em>116</em>(533),
116–132. (<a
href="https://doi.org/10.1080/01621459.2020.1764363">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract– Severe air pollution affects billions of people around the world, particularly in developing countries such as China. Effective emission control policies rely primarily on a proper assessment of air pollutants and accurate spatial clustering outcomes. Unfortunately, emission patterns are difficult to observe as they are highly confounded by many meteorological and geographical factors. In this study, we propose a novel approach for modeling and clustering PM 2.5 concentrations across China. We model observed concentrations from monitoring stations as spatially dependent functional data and assume latent emission processes originate from a functional mixture model with each component as a spatio-temporal process. Cluster memberships of monitoring stations are modeled as a Markov random field, in which confounding effects are controlled through energy functions. The superior performance of our approach is demonstrated using extensive simulation studies. Our method is effective in dividing China and the Beijing-Tianjin-Hebei region into several regions based on PM 2.5 concentrations, suggesting that separate local emission control policies are needed. Supplementary materials for this article, including a standardized description of the materials available for reproducing the work, are available as an online supplement.},
  archive      = {J_JASA},
  author       = {Decai Liang and Haozhe Zhang and Xiaohui Chang and Hui Huang},
  doi          = {10.1080/01621459.2020.1764363},
  journal      = {Journal of the American Statistical Association},
  number       = {533},
  pages        = {116-132},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Modeling and regionalization of china’s PM2.5 using spatial-functional mixture models},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Incorporating animal movement into distance sampling.
<em>JASA</em>, <em>116</em>(533), 107–115. (<a
href="https://doi.org/10.1080/01621459.2020.1764362">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distance sampling is a popular statistical method to estimate the density of wild animal populations. Conventional distance sampling represents animals as fixed points in space that are detected with an unknown probability that depends on the distance between the observer and the animal. Animal movement can cause substantial bias in density estimation. Methods to correct for responsive animal movement exist, but none account for nonresponsive movement independent of the observer. Here, an explicit animal movement model is incorporated into distance sampling, combining distance sampling survey data with animal telemetry data. Detection probability depends on the entire unobserved path the animal travels. The intractable integration over all possible animal paths is approximated by a hidden Markov model. A simulation study shows the method to be negligibly biased (&lt;5\%) in scenarios where conventional distance sampling overestimates abundance by up to 100\%. The method is applied to line transect surveys (1999–2006) of spotted dolphins ( Stenella attenuata ) in the eastern tropical Pacific where abundance is shown to be positively biased by 21\% on average, which can have substantial impact on the population dynamics estimated from these abundance estimates and on the choice of statistical methodology applied to future surveys. Supplementary materials for this article, including a standardized description of the materials available for reproducing the work, are available as an online supplement.},
  archive      = {J_JASA},
  author       = {R. Glennie and S. T. Buckland and R. Langrock and T. Gerrodette and L. T. Ballance and S. J. Chivers and M. D. Scott},
  doi          = {10.1080/01621459.2020.1764362},
  journal      = {Journal of the American Statistical Association},
  number       = {533},
  pages        = {107-115},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Incorporating animal movement into distance sampling},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A hierarchical max-infinitely divisible spatial model for
extreme precipitation. <em>JASA</em>, <em>116</em>(533), 93–106. (<a
href="https://doi.org/10.1080/01621459.2020.1750414">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding the spatial extent of extreme precipitation is necessary for determining flood risk and adequately designing infrastructure (e.g., stormwater pipes) to withstand such hazards. While environmental phenomena typically exhibit weakening spatial dependence at increasingly extreme levels, limiting max-stable process models for block maxima have a rigid dependence structure that does not capture this type of behavior. We propose a flexible Bayesian model from a broader family of (conditionally) max-infinitely divisible processes that allows for weakening spatial dependence at increasingly extreme levels, and due to a hierarchical representation of the likelihood in terms of random effects, our inference approach scales to large datasets. Therefore, our model not only has a flexible dependence structure, but it also allows for fast, fully Bayesian inference, prediction and conditional simulation in high dimensions. The proposed model is constructed using flexible random basis functions that are estimated from the data, allowing for straightforward inspection of the predominant spatial patterns of extremes. In addition, the described process possesses (conditional) max-stability as a special case, making inference on the tail dependence class possible. We apply our model to extreme precipitation in North-Eastern America, and show that the proposed model adequately captures the extremal behavior of the data. Interestingly, we find that the principal modes of spatial variation estimated from our model resemble observed patterns in extreme precipitation events occurring along the coast (e.g., with localized tropical cyclones and convective storms) and mountain range borders. Our model, which can easily be adapted to other types of environmental datasets, is therefore useful to identify extreme weather patterns and regions at risk. Supplementary materials for this article, including a standardized description of the materials available for reproducing the work, are available as an online supplement.},
  archive      = {J_JASA},
  author       = {Gregory P. Bopp and Benjamin A. Shaby and Raphaël Huser},
  doi          = {10.1080/01621459.2020.1750414},
  journal      = {Journal of the American Statistical Association},
  number       = {533},
  pages        = {93-106},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {A hierarchical max-infinitely divisible spatial model for extreme precipitation},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Reinforced designs: Multiple instruments plus control groups
as evidence factors in an observational study of the effectiveness of
catholic schools. <em>JASA</em>, <em>116</em>(533), 82–92. (<a
href="https://doi.org/10.1080/01621459.2020.1745811">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Absent randomization, causal conclusions gain strength if several independent evidence factors concur. We develop a method for constructing evidence factors from several instruments plus a direct comparison of treated and control groups, and we evaluate the methods performance in terms of design sensitivity and simulation. In the application, we consider the effectiveness of Catholic versus public high schools, constructing three evidence factors from three past strategies for studying this question, namely: (i) having nearby access to a Catholic school as an instrument, (ii) being Catholic as an instrument for attending Catholic school, and (iii) a direct comparison of students in Catholic and public high schools. Although these three analyses use the same data, we: (i) construct three essentially independent statistical tests of no effect that require very different assumptions, (ii) study the sensitivity of each test to the assumptions underlying that test, (iii) examine the degree to which independent tests dependent upon different assumptions concur, (iv) pool evidence across independent factors. In the application, we conclude that the ostensible benefit of Catholic education depends critically on the validity of one instrument, and is therefore quite fragile. Supplementary materials for this article, including a standardized description of the materials available for reproducing the work, are available as an online supplement.},
  archive      = {J_JASA},
  author       = {Bikram Karmakar and Dylan S. Small and Paul R. Rosenbaum},
  doi          = {10.1080/01621459.2020.1745811},
  journal      = {Journal of the American Statistical Association},
  number       = {533},
  pages        = {82-92},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Reinforced designs: Multiple instruments plus control groups as evidence factors in an observational study of the effectiveness of catholic schools},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Intentional control of type i error over unconscious data
distortion: A neyman–pearson approach to text classification.
<em>JASA</em>, <em>116</em>(533), 68–81. (<a
href="https://doi.org/10.1080/01621459.2020.1740711">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article addresses the challenges in classifying textual data obtained from open online platforms, which are vulnerable to distortion. Most existing classification methods minimize the overall classification error and may yield an undesirably large Type I error (relevant textual messages are classified as irrelevant), particularly when available data exhibit an asymmetry between relevant and irrelevant information. Data distortion exacerbates this situation and often leads to fallacious prediction. To deal with inestimable data distortion, we propose the use of the Neyman–Pearson (NP) classification paradigm, which minimizes Type II error under a user-specified Type I error constraint. Theoretically, we show that the NP oracle is unaffected by data distortion when the class conditional distributions remain the same. Empirically, we study a case of classifying posts about worker strikes obtained from a leading Chinese microblogging platform, which are frequently prone to extensive, unpredictable and inestimable censorship. We demonstrate that, even though the training and test data are susceptible to different distortion and therefore potentially follow different distributions, our proposed NP methods control the Type I error on test data at the targeted level. The methods and implementation pipeline proposed in our case study are applicable to many other problems involving data distortion. Supplementary materials for this article, including a standardized description of the materials available for reproducing the work, are available as an online supplement.},
  archive      = {J_JASA},
  author       = {Lucy Xia and Richard Zhao and Yanhui Wu and Xin Tong},
  doi          = {10.1080/01621459.2020.1740711},
  journal      = {Journal of the American Statistical Association},
  number       = {533},
  pages        = {68-81},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Intentional control of type i error over unconscious data distortion: A Neyman–Pearson approach to text classification},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Covariance-based sample selection for heterogeneous data:
Applications to gene expression and autism risk gene detection.
<em>JASA</em>, <em>116</em>(533), 54–67. (<a
href="https://doi.org/10.1080/01621459.2020.1738234">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Risk for autism can be influenced by genetic mutations in hundreds of genes. Based on findings showing that genes with highly correlated gene expressions are functionally interrelated, “guilt by association” methods such as DAWN have been developed to identify these autism risk genes. Previous research analyzes the BrainSpan dataset, which contains gene expression of brain tissues from varying regions and developmental periods. Since the spatiotemporal properties of brain tissue are known to affect the gene expression’s covariance, previous research has focused only on a specific subset of samples to avoid the issue of heterogeneity. This analysis leads to a potential loss of power when detecting risk genes. In this article, we develop a new method called covariance-based sample selection (COBS) to find a larger and more homogeneous subset of samples that share the same population covariance matrix for the downstream DAWN analysis. To demonstrate COBSs effectiveness, we use genetic risk scores from two sequential data freezes obtained in 2014 and 2020. We show COBS improves DAWNs ability to predict risk genes detected in the newer data freeze when using the risk scores of the older data freeze as input. Supplementary materials for this article, including a standardized description of the materials available for reproducing the work, are available as an online supplement.},
  archive      = {J_JASA},
  author       = {Kevin Z. Lin and Han Liu and Kathryn Roeder},
  doi          = {10.1080/01621459.2020.1738234},
  journal      = {Journal of the American Statistical Association},
  number       = {533},
  pages        = {54-67},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Covariance-based sample selection for heterogeneous data: Applications to gene expression and autism risk gene detection},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Triplet matching for estimating causal effects with three
treatment arms: A comparative study of mortality by trauma center level.
<em>JASA</em>, <em>116</em>(533), 44–53. (<a
href="https://doi.org/10.1080/01621459.2020.1737078">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Comparing outcomes across different levels of trauma centers is vital in evaluating regionalized trauma care. With observational data, it is critical to adjust for patient characteristics to render valid causal comparisons. Propensity score matching is a popular method to infer causal relationships in observational studies with two treatment arms. Few studies, however, have used matching designs with more than two groups, due to the complexity of matching algorithms. We fill the gap by developing an iterative matching algorithm for the three-group setting. Our algorithm outperforms the nearest neighbor algorithm and is shown to produce matched samples with total distance no larger than twice the optimal distance. We implement the evidence factors method for binary outcomes, which includes a randomization-based testing strategy and a sensitivity analysis for hidden bias in three-group matched designs. We apply our method to the Nationwide Emergency Department Sample data to compare emergency department mortality among non-trauma, level I, and level II trauma centers. Our tests suggest that the admission to a trauma center has a beneficial effect on mortality, assuming no unmeasured confounding. A sensitivity analysis for hidden bias shows that unmeasured confounders, moderately associated with the type of care received, may change the result qualitatively. Supplementary materials for this article, including a standardized description of the materials available for reproducing the work, are available as an online supplement.},
  archive      = {J_JASA},
  author       = {Giovanni Nattino and Bo Lu and Junxin Shi and Stanley Lemeshow and Henry Xiang},
  doi          = {10.1080/01621459.2020.1737078},
  journal      = {Journal of the American Statistical Association},
  number       = {533},
  pages        = {44-53},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Triplet matching for estimating causal effects with three treatment arms: A comparative study of mortality by trauma center level},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hierarchical probabilistic forecasting of electricity demand
with smart meter data. <em>JASA</em>, <em>116</em>(533), 27–43. (<a
href="https://doi.org/10.1080/01621459.2020.1736081">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decisions regarding the supply of electricity across a power grid must take into consideration the inherent uncertainty in demand. Optimal decision-making requires probabilistic forecasts for demand in a hierarchy with various levels of aggregation, such as substations, cities, and regions. The forecasts should be coherent in the sense that the forecast of the aggregated series should equal the sum of the forecasts of the corresponding disaggregated series. Coherency is essential, since the allocation of electricity at one level of the hierarchy relies on the appropriate amount being provided from the previous level. We introduce a new probabilistic forecasting method for a large hierarchy based on UK residential smart meter data. We find our method provides coherent and accurate probabilistic forecasts, as a result of an effective forecast combination. Furthermore, by avoiding distributional assumptions, we find that our method captures the variety of distributions in the smart meter hierarchy. Finally, the results confirm that, to ensure coherency in our large-scale hierarchy, it is sufficient to model a set of lower-dimension dependencies, rather than modeling the entire joint distribution of all series in the hierarchy. In achieving coherent and accurate hierarchical probabilistic forecasts, this work contributes to improved decision-making for smart grids. Supplementary materials for this article, including a standardized description of the materials available for reproducing the work, are available as an online supplement.},
  archive      = {J_JASA},
  author       = {Souhaib Ben Taieb and James W. Taylor and Rob J. Hyndman},
  doi          = {10.1080/01621459.2020.1736081},
  journal      = {Journal of the American Statistical Association},
  number       = {533},
  pages        = {27-43},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Hierarchical probabilistic forecasting of electricity demand with smart meter data},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Integrating multidimensional data for clustering analysis
with applications to cancer patient data. <em>JASA</em>,
<em>116</em>(533), 14–26. (<a
href="https://doi.org/10.1080/01621459.2020.1730853">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advances in high-throughput genomic technologies coupled with large-scale studies including The Cancer Genome Atlas (TCGA) project have generated rich resources of diverse types of omics data to better understand cancer etiology and treatment responses. Clustering patients into subtypes with similar disease etiologies and/or treatment responses using multiple omics data types has the potential to improve the precision of clustering than using a single data type. However, in practice, patient clustering is still mostly based on a single type of omics data or ad hoc integration of clustering results from individual data types, leading to potential loss of information. By treating each omics data type as a different informative representation from patients, we propose a novel multi-view spectral clustering framework to integrate different omics data types measured from the same subject. We learn the weight of each data type as well as a similarity measure between patients via a nonconvex optimization framework. We solve the proposed nonconvex problem iteratively using the ADMM algorithm and show the convergence of the algorithm. The accuracy and robustness of the proposed clustering method is studied both in theory and through various synthetic data. When our method is applied to the TCGA data, the patient clusters inferred by our method show more significant differences in survival times between clusters than those inferred from existing clustering methods. Supplementary materials for this article, including a standardized description of the materials available for reproducing the work, are available as an online supplement.},
  archive      = {J_JASA},
  author       = {Seyoung Park and Hao Xu and Hongyu Zhao},
  doi          = {10.1080/01621459.2020.1730853},
  journal      = {Journal of the American Statistical Association},
  number       = {533},
  pages        = {14-26},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Integrating multidimensional data for clustering analysis with applications to cancer patient data},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Toward optimal fingerprinting in detection and attribution
of changes in climate extremes. <em>JASA</em>, <em>116</em>(533), 1–13.
(<a href="https://doi.org/10.1080/01621459.2020.1730852">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract– Detection and attribution of climate change plays a central role in establishing the causal relationship between the observed changes in the climate and their possible causes. Optimal fingerprinting has been widely used as a standard method for detection and attribution analysis for mean climate conditions, but there has been no satisfactory analog for climate extremes. Here, we turn an intuitive concept, which incorporates the expected climate responses to external forcings into the location parameters of the marginal generalized extreme value (GEV) distributions of the observed extremes, to a practical and better-understood method. Marginal approaches based on a weighted sum of marginal GEV score equations are promising for no need to specify the dependence structure. The computational efficiency makes them feasible in handling multiple forcings simultaneously. The method under working independence is recommended because it produces robust results where there are errors-in-variables. Our analyses show human influences on temperature extremes at the subcontinental scale. Compared with previous studies, we detected human influences in a slightly smaller number of regions. This is possibly due to the under-coverage of the confidence intervals in existing works, suggesting the need for careful examinations of the properties of the statistical methods in practice. Supplementary materials for this article, including a standardized description of the materials available for reproducing the work, are available as an online supplement.},
  archive      = {J_JASA},
  author       = {Zhuo Wang and Yujing Jiang and Hui Wan and Jun Yan and Xuebin Zhang},
  doi          = {10.1080/01621459.2020.1730852},
  journal      = {Journal of the American Statistical Association},
  number       = {533},
  pages        = {1-13},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Toward optimal fingerprinting in detection and attribution of changes in climate extremes},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
</ul>

</body>
</html>
