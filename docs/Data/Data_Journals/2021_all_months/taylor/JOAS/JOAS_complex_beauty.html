<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>JOAS_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="joas---174">JOAS - 174</h2>
<ul>
<li><details>
<summary>
(2021a). Correction. <em>JOAS</em>, <em>48</em>(16), 3253–3254. (<a
href="https://doi.org/10.1080/02664763.2021.1972659">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JOAS},
  doi          = {10.1080/02664763.2021.1972659},
  journal      = {Journal of Applied Statistics},
  number       = {16},
  pages        = {3253-3254},
  shortjournal = {J. Appl. Stat.},
  title        = {Correction},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Correction to: The weibull fréchet distribution and its
applications. <em>JOAS</em>, <em>48</em>(16), 3251–3252. (<a
href="https://doi.org/10.1080/02664763.2021.1973388">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Afify et al. [ The Weibull Fréchet distribution and its applications , J. Appl. Stat., 43 (2016), pp. 2608–2626] defined and studied a new four-parameter lifetime model called the Weibull Fréchet distribution. They made some mistakes in presenting the log-likelihood function and the components of score vector. In this note, we will correct them.},
  archive      = {J_JOAS},
  author       = {Reza Azimi and Mahdy Esmailian},
  doi          = {10.1080/02664763.2021.1973388},
  journal      = {Journal of Applied Statistics},
  number       = {16},
  pages        = {3251-3252},
  shortjournal = {J. Appl. Stat.},
  title        = {Correction to: The weibull fréchet distribution and its applications},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Estimating engel curves: A new way to improve the SILC-HBS
matching process using GLM methods. <em>JOAS</em>, <em>48</em>(16),
3233–3250. (<a
href="https://doi.org/10.1080/02664763.2020.1796933">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Microdata are required to evaluate the distributive impact of the taxation system as a whole (direct and indirect taxes) on individuals or households. However, in European Union countries this information is usually distributed into two separate surveys: the Household Budget Surveys (HBS), including total household expenditure and its composition, and EU Statistics on Income and Living Conditions (EU-SILC), including detailed information about households&#39; income and direct (but not indirect) taxes paid. We present a parametric statistical matching procedure to merge both surveys. For the first stage of matching, we propose estimating total household expenditure in HBS (Engel curves) using a GLM estimator, instead of the traditionally used OLS method. It is a better alternative, insofar as it can deal with the heteroskedasticity problem of the OLS estimates, while making it unnecessary to retransform the regressors estimated in logarithms. To evaluate these advantages of the GLM estimator, we conducted a computational Monte Carlo simulation. In addition, when an error term is added to the deterministic imputation of expenditure in the EU-SILC, we propose replacing the usual Normal distribution of the error with a Chi-square type, which allows a better approximation to the original expenditures variance in the HBS. An empirical analysis is provided using Spanish surveys for years 2012–2016. In addition, we extend the empirical analysis to the rest of the European Union countries, using the surveys provided by Eurostat (EU-SILC, 2011; HBS, 2010).},
  archive      = {J_JOAS},
  author       = {Julio López-Laborda and Carmen Marín-González and Jorge Onrubia-Fernández},
  doi          = {10.1080/02664763.2020.1796933},
  journal      = {Journal of Applied Statistics},
  number       = {16},
  pages        = {3233-3250},
  shortjournal = {J. Appl. Stat.},
  title        = {Estimating engel curves: A new way to improve the SILC-HBS matching process using GLM methods},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A comparison study on modeling of clustered and
overdispersed count data for multiple comparisons. <em>JOAS</em>,
<em>48</em>(16), 3220–3232. (<a
href="https://doi.org/10.1080/02664763.2020.1788518">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data collected in various scientific fields are count data. One way to analyze such data is to compare the individual levels of the factor treatment using multiple comparisons. However, the measured individuals are often clustered – e.g. according to litter or rearing. This must be considered when estimating the parameters by a repeated measurement model. In addition, ignoring the overdispersion to which count data is prone leads to an increase of the type one error rate. We carry out simulation studies using several different data settings and compare different multiple contrast tests with parameter estimates from generalized estimation equations and generalized linear mixed models in order to observe coverage and rejection probabilities. We generate overdispersed, clustered count data in small samples as can be observed in many biological settings. We have found that the generalized estimation equations outperform generalized linear mixed models if the variance-sandwich estimator is correctly specified. Furthermore, generalized linear mixed models show problems with the convergence rate under certain data settings, but there are model implementations with lower implications exists. Finally, we use an example of genetic data to demonstrate the application of the multiple contrast test and the problems of ignoring strong overdispersion.},
  archive      = {J_JOAS},
  author       = {Jochen Kruppa and Ludwig Hothorn},
  doi          = {10.1080/02664763.2020.1788518},
  journal      = {Journal of Applied Statistics},
  number       = {16},
  pages        = {3220-3232},
  shortjournal = {J. Appl. Stat.},
  title        = {A comparison study on modeling of clustered and overdispersed count data for multiple comparisons},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A coefficient of discrimination for use with nominal and
ordinal regression models. <em>JOAS</em>, <em>48</em>(16), 3208–3219.
(<a href="https://doi.org/10.1080/02664763.2020.1796940">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study introduces a coefficient of discrimination for use with nominal and ordinal regression models. Computation of the coefficient is demonstrated with data from the Pew Research Center’s 25th Anniversary of the Web Omnibus Survey pertaining to cell/home phone ownership, where the coefficient of discrimination indicates that respondent age and gender increased the probability of a correct versus incorrect classification by 13.9\%. Additionally, the coefficient is compared to existing coefficients.},
  archive      = {J_JOAS},
  author       = {Thomas J. Smith and David A. Walker and Cornelius M. McKenna},
  doi          = {10.1080/02664763.2020.1796940},
  journal      = {Journal of Applied Statistics},
  number       = {16},
  pages        = {3208-3219},
  shortjournal = {J. Appl. Stat.},
  title        = {A coefficient of discrimination for use with nominal and ordinal regression models},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multimodal exponential families of circular distributions
with application to daily peak hours of PM2.5 level in a large city.
<em>JOAS</em>, <em>48</em>(16), 3193–3207. (<a
href="https://doi.org/10.1080/02664763.2020.1796938">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose two multimodal circular distributions which are suitable for modeling circular data sets with two or more modes. Both distributions belong to the regular exponential family of distributions and are considered as extensions of the von Mises distribution. Hence, they possess the highly desirable properties, such as the existence of non-trivial sufficient statistics and optimal inferences for their parameters. Fine particulates (PM2.5) are generally emitted from activities such as industrial and residential combustion and from vehicle exhaust. We illustrate the utility of our proposed models using a real data set consisting of fine particulates (PM2.5) pollutant levels in Houston region during Fall season in 2019. Our results provide a strong evidence that its diurnal pattern exhibits four modes; two peaks during morning and evening rush hours and two peaks in between.},
  archive      = {J_JOAS},
  author       = {Sungsu Kim and Ashis SenGupta},
  doi          = {10.1080/02664763.2020.1796938},
  journal      = {Journal of Applied Statistics},
  number       = {16},
  pages        = {3193-3207},
  shortjournal = {J. Appl. Stat.},
  title        = {Multimodal exponential families of circular distributions with application to daily peak hours of PM2.5 level in a large city},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The unit extended weibull families of distributions and its
applications. <em>JOAS</em>, <em>48</em>(16), 3174–3192. (<a
href="https://doi.org/10.1080/02664763.2020.1796936">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, two new general families of distributions supported on the unit interval are introduced. The proposed families include several known models as special cases and define at least twenty (each one) new special models. Since the list of well-being indicators may include several double bounded random variables, the applicability for modeling those is the major practical motivation for introducing the distributions on those families. We propose a parametrization of the new families in terms of the median and develop a shiny application to provide interactive density shape illustrations for some special cases. Various properties of the introduced families are studied. Some special models in the new families are discussed. In particular, the complementary unit Weibull distribution is studied in some detail. The method of maximum likelihood for estimating the model parameters is discussed. An extensive Monte Carlo experiment is conducted to evaluate the performances of these estimators in finite samples. Applications to the literacy rate in Brazilian and Colombian municipalities illustrate the usefulness of the two new families for modeling well-being indicators.},
  archive      = {J_JOAS},
  author       = {Renata Rojas Guerra and Fernando A. Peña-Ramírez and Marcelo Bourguignon},
  doi          = {10.1080/02664763.2020.1796936},
  journal      = {Journal of Applied Statistics},
  number       = {16},
  pages        = {3174-3192},
  shortjournal = {J. Appl. Stat.},
  title        = {The unit extended weibull families of distributions and its applications},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bayesian factor models for multivariate categorical data
obtained from questionnaires. <em>JOAS</em>, <em>48</em>(16), 3150–3173.
(<a href="https://doi.org/10.1080/02664763.2020.1796935">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Factor analysis is a flexible technique for assessment of multivariate dependence and codependence. Besides being an exploratory tool used to reduce the dimensionality of multivariate data, it allows estimation of common factors that often have an interesting theoretical interpretation in real problems. However, standard factor analysis is only applicable when the variables are scaled, which is often inappropriate, for example, in data obtained from questionnaires in the field of psychology, where the variables are often categorical. In this framework, we propose a factor model for the analysis of multivariate ordered and non-ordered polychotomous data. The inference procedure is done under the Bayesian approach via Markov chain Monte Carlo methods. Two Monte Carlo simulation studies are presented to investigate the performance of this approach in terms of estimation bias, precision and assessment of the number of factors. We also illustrate the proposed method to analyze participants&#39; responses to the Motivational State Questionnaire dataset, developed to study emotions in laboratory and field settings.},
  archive      = {J_JOAS},
  author       = {Vitor Capdeville and Kelly C. M. Gonçalves and João B. M. Pereira},
  doi          = {10.1080/02664763.2020.1796935},
  journal      = {Journal of Applied Statistics},
  number       = {16},
  pages        = {3150-3173},
  shortjournal = {J. Appl. Stat.},
  title        = {Bayesian factor models for multivariate categorical data obtained from questionnaires},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Partial least squares regression with compositional response
variables and covariates. <em>JOAS</em>, <em>48</em>(16), 3130–3149. (<a
href="https://doi.org/10.1080/02664763.2020.1795813">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The common approach for regression analysis with compositional variables is to express compositions in log-ratio coordinates (coefficients) and then perform standard statistical processing in real space. Similar to working in real space, the problem is that the standard least squares regression fails when the number of parts of all compositional covariates is higher than the number of observations. The aim of this study is to analyze in detail the partial least squares (PLS) regression which can deal with this problem. In this paper, we focus on the PLS regression between more than one compositional response variable and more than one compositional covariate. First, we give the PLS regression model with log-ratio coordinates of compositional variables, then we express the PLS model directly in the simplex. We also prove that the PLS model is invariant under the change of coordinate system, such as the ilr coordinates with a different contrast matrix or the clr coefficients. Moreover, we give the estimation and inference for parameters in PLS model. Finally, the PLS model with clr coefficients is used to analyze the relationship between the chemical metabolites of Astragali Radix and the plasma metabolites of rat after giving Astragali Radix.},
  archive      = {J_JOAS},
  author       = {Jiajia Chen and Xiaoqin Zhang and Karel Hron},
  doi          = {10.1080/02664763.2020.1795813},
  journal      = {Journal of Applied Statistics},
  number       = {16},
  pages        = {3130-3149},
  shortjournal = {J. Appl. Stat.},
  title        = {Partial least squares regression with compositional response variables and covariates},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Identifying key factors in momentum in basketball games.
<em>JOAS</em>, <em>48</em>(16), 3116–3129. (<a
href="https://doi.org/10.1080/02664763.2020.1795819">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Momentum as elaborated under a recent novel definition has been shown quantitatively to have a significant impact on basketball game outcomes. This paper makes two contributions to the analytical literature on sports momentum: (1) two aspects of the new definition are operationalized so that its practicality becomes evident; and (2) through a dimension-reduction technique (elastic net), key factors associated with momentum are identified. Both technical variables such as field goals, assists, rebounds, etc. and environmental variables such as the spectator attendance rate and player salary dispersion are considered, and the potential for useful real-time analyzes is illustrated.},
  archive      = {J_JOAS},
  author       = {Tao Chen and Qingliang Fan and Kai Liu and Lingshan Le},
  doi          = {10.1080/02664763.2020.1795819},
  journal      = {Journal of Applied Statistics},
  number       = {16},
  pages        = {3116-3129},
  shortjournal = {J. Appl. Stat.},
  title        = {Identifying key factors in momentum in basketball games},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Nonparametric inference for panel count data with competing
risks. <em>JOAS</em>, <em>48</em>(16), 3102–3115. (<a
href="https://doi.org/10.1080/02664763.2020.1795816">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In survival and reliability studies, panel count data arise when we investigate a recurrent event process and each study subject is observed only at discrete time points. If recurrent events of several types are possible, we obtain panel count data with competing risks. Such data arise frequently from transversal studies on recurrent events in demography, epidemiology and reliability experiments where the individuals cannot be observed continuously. In the present paper, we propose an isotonic regression estimator for the cause specific mean function of the underlying recurrent event process of a competing risks panel count data. Further, a nonparametric test is proposed to compare the cause specific mean functions of the panel count competing risks data. Asymptotic properties of the proposed estimator and test statistic are studied. A simulation study is conducted to assess the finite sample behaviour of the proposed estimator and test statistic. Finally, the procedures developed are applied to a real data arising from skin cancer chemo prevention trial.},
  archive      = {J_JOAS},
  author       = {E. P. Sreedevi and P. G. Sankaran},
  doi          = {10.1080/02664763.2020.1795816},
  journal      = {Journal of Applied Statistics},
  number       = {16},
  pages        = {3102-3115},
  shortjournal = {J. Appl. Stat.},
  title        = {Nonparametric inference for panel count data with competing risks},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Box–cox power transformation unconditional quantile
regressions with an application on wage inequality. <em>JOAS</em>,
<em>48</em>(16), 3086–3101. (<a
href="https://doi.org/10.1080/02664763.2020.1795817">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study proposes a semi-parametric estimation method, Box–Cox power transformation unconditional quantile regression, to estimate the impact of changes in the distribution of the explanatory variables on the unconditional quantile of the outcome variable. The proposed method consists of running a nonlinear regression of the recentered influence function (RIF) of the outcome variable on the explanatory variables. We also show the asymptotic properties of the proposed estimator and apply the estimation method to address an existing puzzle in labor economics–why the 50th/10th percentile wage gap has been falling in the USA since the late 1980s. Our results show that declining unionization can explain approximately 10\% of the decline in the 50/10 wage gap in 1990–2000 and 23\% in 2000–2010.},
  archive      = {J_JOAS},
  author       = {Pallab Kumar Ghosh},
  doi          = {10.1080/02664763.2020.1795817},
  journal      = {Journal of Applied Statistics},
  number       = {16},
  pages        = {3086-3101},
  shortjournal = {J. Appl. Stat.},
  title        = {Box–Cox power transformation unconditional quantile regressions with an application on wage inequality},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Linear censored regression models with skew scale mixtures
of normal distributions. <em>JOAS</em>, <em>48</em>(16), 3060–3085. (<a
href="https://doi.org/10.1080/02664763.2020.1795814">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A special source of difficulty in the statistical analysis is the possibility that some subjects may not have a complete observation of the response variable. Such incomplete observation of the response variable is called censoring. Censorship can occur for a variety of reasons, including limitations of measurement equipment, design of the experiment, and non-occurrence of the event of interest until the end of the study. In the presence of censoring, the dependence of the response variable on the explanatory variables can be explored through regression analysis. In this paper, we propose to examine the censorship problem in context of the class of asymmetric, i.e., we have proposed a linear regression model with censored responses based on skew scale mixtures of normal distributions. We develop a Monte Carlo EM (MCEM) algorithm to perform maximum likelihood inference of the parameters in the proposed linear censored regression models with skew scale mixtures of normal distributions. The MCEM algorithm has been discussed with an emphasis on the skew-normal, skew Student-t-normal, skew-slash and skew-contaminated normal distributions. To examine the performance of the proposed method, we present some simulation studies and analyze a real dataset.},
  archive      = {J_JOAS},
  author       = {Daniel C. F. Guzmán and Clécio S. Ferreira and Camila B. Zeller},
  doi          = {10.1080/02664763.2020.1795814},
  journal      = {Journal of Applied Statistics},
  number       = {16},
  pages        = {3060-3085},
  shortjournal = {J. Appl. Stat.},
  title        = {Linear censored regression models with skew scale mixtures of normal distributions},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Regression models to dependence for exceedance.
<em>JOAS</em>, <em>48</em>(16), 3048–3059. (<a
href="https://doi.org/10.1080/02664763.2020.1795088">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extreme Value Theory (EVT) aims to study the tails of probability distributions in order to measure and quantify extreme events of maximum and minimum. In river flow data, an extreme level of a river may be related to the level of a neighboring river that flows into it. In this type of data, it is very common for flooding of a location to have been caused by a very large flow from an affluent river that is tens or hundreds of kilometers from this location. In this sense, an interesting approach is to consider a conditional model for the estimation of a multivariate model. Inspired by this idea, we propose a Bayesian model to describe the dependence of exceedance between rivers, where we considered a conditionally independent structure. In this model, the dependence between rivers is captured by modeling the excess marginally of one river as a consequence of linear functions of the other rivers. The results showed that there is a strong and positive connection between excesses in one river caused by the excesses of the other rivers.},
  archive      = {J_JOAS},
  author       = {Fernando Ferraz do Nascimento and Andreson Almeida Azevedo and Valmaria Rocha da Silva Ferraz},
  doi          = {10.1080/02664763.2020.1795088},
  journal      = {Journal of Applied Statistics},
  number       = {16},
  pages        = {3048-3059},
  shortjournal = {J. Appl. Stat.},
  title        = {Regression models to dependence for exceedance},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A new bivariate poisson distribution via conditional
specification: Properties and applications. <em>JOAS</em>,
<em>48</em>(16), 3025–3047. (<a
href="https://doi.org/10.1080/02664763.2020.1793307">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we discuss a bivariate Poisson distribution whose conditionals are univariate Poisson distributions and the marginals are not Poisson which exhibits negative correlation. Some useful structural properties of this distribution namely marginals, moments, generating functions, stochastic ordering are investigated. Simple proofs of negative correlation, marginal over-dispersion, distribution of sum and conditional given the sum are also derived. The distribution is shown to be a member of the multi-parameter exponential family and some natural but useful consequences are also outlined. Parameter estimation with maximum likelihood is implemented. Copula-based simulation experiments are carried out using Bivariate Normal and the Farlie–Gumbel–Morgenstern copulas to assess how the model behaves in dealing with the situation. Finally, the distribution is fitted to seven bivariate count data sets with an inherent negative correlation to illustrate suitability.},
  archive      = {J_JOAS},
  author       = {Indranil Ghosh and Filipe Marques and Subrata Chakraborty},
  doi          = {10.1080/02664763.2020.1793307},
  journal      = {Journal of Applied Statistics},
  number       = {16},
  pages        = {3025-3047},
  shortjournal = {J. Appl. Stat.},
  title        = {A new bivariate poisson distribution via conditional specification: Properties and applications},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A new class of skew distributions with climate data
analysis. <em>JOAS</em>, <em>48</em>(16), 3002–3024. (<a
href="https://doi.org/10.1080/02664763.2020.1791804">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we develop a new general class of skew distributions with flexibility properties on the tails. Moreover, such class can provide heavy and light tails. Some of its mathematical properties are studied, including the quantile function, the moments, the moment generating function and the mean of deviations. New skew distributions are derived and used to construct new models capturing asymmetry inherent to data. The estimation of the class parameters is investigated by the method of maximum likelihood and the performance of the estimators is assessed by a simulation study. Applications of the proposed distribution are explored for two climate data sets. The first data set concerns the annual heat wave index and the second data set involves temperature and precipitation measures from the meteorological station located at Schiphol, Netherlands. Data fitting results show that our models perform better than the competitors.},
  archive      = {J_JOAS},
  author       = {Hassan S. Bakouch and Meitner Cadena and Christophe Chesneau},
  doi          = {10.1080/02664763.2020.1791804},
  journal      = {Journal of Applied Statistics},
  number       = {16},
  pages        = {3002-3024},
  shortjournal = {J. Appl. Stat.},
  title        = {A new class of skew distributions with climate data analysis},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Accurate estimation for extra-poisson variability assuming
random effect models. <em>JOAS</em>, <em>48</em>(16), 2982–3001. (<a
href="https://doi.org/10.1080/02664763.2020.1789075">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, the components of extra-Poisson variability are estimated assuming random effect models under a Bayesian approach. A standard existing methodology to estimate extra-Poisson variability assumes a negative binomial distribution. The obtained results show that using the proposed random effect model it is possible to get more accurate estimates for the extra-Poisson variability components when compared to the use of a negative binomial distribution where it is possible to estimate only one component of extra-Poisson variability. Some illustrative examples are introduced considering real data sets.},
  archive      = {J_JOAS},
  author       = {Ricardo Puziol de Oliveira and Jorge Alberto Achcar},
  doi          = {10.1080/02664763.2020.1789075},
  journal      = {Journal of Applied Statistics},
  number       = {16},
  pages        = {2982-3001},
  shortjournal = {J. Appl. Stat.},
  title        = {Accurate estimation for extra-poisson variability assuming random effect models},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Understanding students’ evaluations of professors using
non-negative matrix factorization. <em>JOAS</em>, <em>48</em>(13-15),
2961–2981. (<a
href="https://doi.org/10.1080/02664763.2021.1991288">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we use Nonnegative Matrix Factorization (NMF) and several other state of the art statistical machine learning techniques to provide an in-depth study of university professor evaluations by their students. We specifically use the Kullback–Leibler divergence as our loss function in keeping with the type of the data and extract revealing patterns consistent with the educational objectives underlying the questionnaire design. In particular, the application of our techniques to a dataset gathered at Gazi University in Turkey reveals compelling patterns such as the strong association between the student&#39;s seriousness and dedication (measured by attendance) and the kind of scores they tend to assign to the courses and the corresponding professors. Insights emerging from our study suggest that more aspects of students&#39; evaluations should be explored at greater depths.},
  archive      = {J_JOAS},
  author       = {Necla Gündüz and Ernest Fokoué},
  doi          = {10.1080/02664763.2021.1991288},
  journal      = {Journal of Applied Statistics},
  number       = {13-15},
  pages        = {2961-2981},
  shortjournal = {J. Appl. Stat.},
  title        = {Understanding students&#39; evaluations of professors using non-negative matrix factorization},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hierarchical marketing mix models with sign constraints.
<em>JOAS</em>, <em>48</em>(13-15), 2944–2960. (<a
href="https://doi.org/10.1080/02664763.2021.1946020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Marketing mix models (MMMs) are statistical models for measuring the effectiveness of various marketing activities such as promotion, media advertisement, etc. In this research, we propose a comprehensive marketing mix model that captures the hierarchical structure and the carryover, shape and scale effects of certain marketing activities, as well as sign restrictions on certain coefficients that are consistent with common business sense. In contrast to commonly adopted approaches in practice, which estimate parameters in a multi-stage process, the proposed approach estimates all the unknown parameters simultaneously using a constrained maximum likelihood approach and a Hamiltonian Monte Carlo algorithm. We present results on real datasets to illustrate the use of the proposed solution algorithms.},
  archive      = {J_JOAS},
  author       = {Hao Chen and Minguang Zhang and Lanshan Han and Alvin Lim},
  doi          = {10.1080/02664763.2021.1946020},
  journal      = {Journal of Applied Statistics},
  number       = {13-15},
  pages        = {2944-2960},
  shortjournal = {J. Appl. Stat.},
  title        = {Hierarchical marketing mix models with sign constraints},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Circular analyses of dates on patients with gastric
carcinoma. <em>JOAS</em>, <em>48</em>(13-15), 2931–2943. (<a
href="https://doi.org/10.1080/02664763.2021.1977259">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dates have great importance in cancer diseases. However, the date variables themselves are not analyzed. This study aims to evaluate the descriptive statistics of diagnosis, operation, and last examination dates in gastric carcinoma patients by circular analysis methods. Totally 502 gastric carcinoma patients were enrolled in the study. The mean month of diagnosis date was found in nearly November (∼10.86) for females and May (∼5.17) for male patients. The mean month of operation date was found March (∼3.24) for females, and July &amp; August (∼7.79) for males. The mean month of the last examination date was found as February &amp; March (∼2.61) for females, and May (∼4.85) for males. Moreover, the mean day of the week for diagnosis date was found Thursday (∼5.50) for both female and male patients. The fitting of distributions of all variables was checked, also, according to von Mises, Rayleigh, and Kuiper’s tests. When the days and months were analyzed by classical descriptive statistics, the results were obtained completely different from the circular analyses results. Therefore, the dates and times should be analyzed in certain diseases to give an idea for physicians.},
  archive      = {J_JOAS},
  author       = {Adnan Karaibrahimoglu and Seren Ayhan and Mustafa Karaagac and Mehmet Artac},
  doi          = {10.1080/02664763.2021.1977259},
  journal      = {Journal of Applied Statistics},
  number       = {13-15},
  pages        = {2931-2943},
  shortjournal = {J. Appl. Stat.},
  title        = {Circular analyses of dates on patients with gastric carcinoma},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The design of multiple crop insurance in indonesia based on
revenue risk using the copula model approach. <em>JOAS</em>,
<em>48</em>(13-15), 2920–2930. (<a
href="https://doi.org/10.1080/02664763.2021.1897089">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is important for Indonesia as a country with agricultural bases to develop crop insurance. Until now, Indonesia has not had any insurance for horticultural crops other than for corn. This paper discusses horticultural multicrop insurance products based on revenue risk that can be triggered by low prices, low yields, or a combination of both. In designing multicrop insurance products, it is important to model the variability of revenue risk through the implementation of copula toward crop yield and price and to estimate indemnity of the revenue-based multicrop insurance. The analysis employed Gumbel and Clayton copulas to model the dependency structure between crop yield and price of multicrops. Each marginal variable was modeled by using the ARIMA model. The results showed that multicrop revenue insurance tends to reduce the price of agricultural insurance in Indonesia, and thus this program has the potential to have good acceptance in agricultural insurance.},
  archive      = {J_JOAS},
  author       = {H. A. Rusyda and L. Noviyanti and A. Z. Soleh and A. Chadidjah and F. Indrayatna},
  doi          = {10.1080/02664763.2021.1897089},
  journal      = {Journal of Applied Statistics},
  number       = {13-15},
  pages        = {2920-2930},
  shortjournal = {J. Appl. Stat.},
  title        = {The design of multiple crop insurance in indonesia based on revenue risk using the copula model approach},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A comparative study of computation approaches of the
generalized f-test. <em>JOAS</em>, <em>48</em>(13-15), 2906–2919. (<a
href="https://doi.org/10.1080/02664763.2021.1939660">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Generalized F-test is derived based on the Generalized P-value Method to test the equality of normally distributed group means under unequal variances. There are two approaches to compute the p-value of the GF test, based on beta and chi-squared random numbers. From prior art in the literature, it appears that the two computation approaches of the Generalized tests are equivalent. In this study, the equivalence of these approaches is investigated in an extensive Monte-Carlo simulation study in terms of Type I error probability and penalized power. It is found that the equivalence of the computation approaches is not quite correct and that there is a difference between their conclusion, and researchers should decide which one is powerful than the others according to the structure of data, such as sample size, and the number of groups. Also, real data examples are given to show the opposite decisions of the computation approaches.},
  archive      = {J_JOAS},
  author       = {Berna Yazici and Mustafa Cavus},
  doi          = {10.1080/02664763.2021.1939660},
  journal      = {Journal of Applied Statistics},
  number       = {13-15},
  pages        = {2906-2919},
  shortjournal = {J. Appl. Stat.},
  title        = {A comparative study of computation approaches of the generalized F-test},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Taguchi based case study in the automotive industry:
Nonconformity decreasing with use of six sigma methodology.
<em>JOAS</em>, <em>48</em>(13-15), 2889–2905. (<a
href="https://doi.org/10.1080/02664763.2020.1837086">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we applied a conceptual Six Sigma/design of experiment hybrid framework that aims to integrate the Taguchi method and Six Sigma for process improvement in a complex industrial environment. In this context, the Six Sigma methodology was employed on a company operating within the automotive industry to improve a manufacturing process which caused a customer complaint within the company. Studies employing the Taguchi experiment design usually focus on a single variable and neglect the effects of adjustments on remaining quality characteristics. In this study, a multi-response Taguchi design of experiment was preferred, and all of the quality characteristics were taken into account. In our study, define, measure, analysis, improve and control phases were used to reduce the nonconformity rate from 23.940 percent (baseline) to 0.049 percent. As a result of implementing Six Sigma, the sigma level increased from 2.21 (baseline) to 4.80.},
  archive      = {J_JOAS},
  author       = {Atakan Gerger and Ali Riza Firuzan},
  doi          = {10.1080/02664763.2020.1837086},
  journal      = {Journal of Applied Statistics},
  number       = {13-15},
  pages        = {2889-2905},
  shortjournal = {J. Appl. Stat.},
  title        = {Taguchi based case study in the automotive industry: Nonconformity decreasing with use of six sigma methodology},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Efficient experimental design for dose response modelling.
<em>JOAS</em>, <em>48</em>(13-15), 2864–2888. (<a
href="https://doi.org/10.1080/02664763.2021.1880556">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The logit binomial logistic dose response model is commonly used in applied research to model binary outcomes as a function of the dose or concentration of a substance. This model is easily tailored to assess the relative potency of two substances. Consequently, in instances where two such dose response curves are parallel so one substance can be viewed as a dilution of the other, the degree of that dilution is captured in the relative potency model parameter. It is incumbent that experimental researchers working in fields including biomedicine, environmental science, toxicology and applied sciences choose efficient experimental designs to run their studies to both fit their dose response curves and to garner important information regarding drug or substance potency. This article provides far-reaching practical design strategies for dose response model fitting and estimation of relative potency using key illustrations. These results are subsequently extended here to handle situations where the assessment of parallelism and the proper dose-scale are also of interest. Conclusions and recommended strategies are supported by both theoretical and simulation results.},
  archive      = {J_JOAS},
  author       = {Timothy E. O’Brien and Jack Silcox},
  doi          = {10.1080/02664763.2021.1880556},
  journal      = {Journal of Applied Statistics},
  number       = {13-15},
  pages        = {2864-2888},
  shortjournal = {J. Appl. Stat.},
  title        = {Efficient experimental design for dose response modelling},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A method for detection of mode-mixing problem.
<em>JOAS</em>, <em>48</em>(13-15), 2847–2863. (<a
href="https://doi.org/10.1080/02664763.2021.1908969">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classical Empirical Mode Decomposition (EMD) is a data-driven method used to analyze non-linear and non-stationary time series data. Besides being an adaptable method by its nature, EMD assumes that every data consists of oscillations of the intrinsic mode functions (IMF). EMD also requires the condition that IMFs which represent the characteristic structures in the data should show only a unique sub-characteristic of the data. However, in some cases, depending on the way the sub-characteristics which make up a sophisticated data coexist, the IMFs are able to be not unique. This is called the mode-mixing problem. Although there are many studies and successful methods (such as EEMD, CEEMDAN) for eliminating the mode-mixing problem, a limited number of studies exist on determining the presence of the aforementioned problem. In this study, a method for the determination of the mode-mixing problem is proposed. In the suggested method, the Itakura–Saito distance, which is a measurement of the similarity of stationary signals and based on Fourier spectrums, is modified by applying Kaiser filter onto short-time signals. The performance of the method is tested via various applications with simulated and real data, and the results show successful detection of the mode-mixing if it exists in time series.},
  archive      = {J_JOAS},
  author       = {Atacan Erdiş and M. Akif Bakir and Muhammed I. Jaiteh},
  doi          = {10.1080/02664763.2021.1908969},
  journal      = {Journal of Applied Statistics},
  number       = {13-15},
  pages        = {2847-2863},
  shortjournal = {J. Appl. Stat.},
  title        = {A method for detection of mode-mixing problem},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Estimation in the partially nonlinear model by continuous
optimization. <em>JOAS</em>, <em>48</em>(13-15), 2826–2846. (<a
href="https://doi.org/10.1080/02664763.2020.1864816">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A useful model for data analysis is the partially nonlinear model where response variable is represented as the sum of a nonparametric and a parametric component. In this study, we propose a new procedure for estimating the parameters in the partially nonlinear models. Therefore, we consider penalized profile nonlinear least square problem where nonparametric components are expressed as a B-spline basis function, and then estimation problem is expressed in terms of conic quadratic programming which is a continuous optimization problem and solved interior point method. An application study is conducted to evaluate the performance of the proposed method by considering some well-known performance measures. The results are compared against parametric nonlinear model.},
  archive      = {J_JOAS},
  author       = {Fatma Yerlikaya-Özkurt and Pakize Taylan and Müjgan Tez},
  doi          = {10.1080/02664763.2020.1864816},
  journal      = {Journal of Applied Statistics},
  number       = {13-15},
  pages        = {2826-2846},
  shortjournal = {J. Appl. Stat.},
  title        = {Estimation in the partially nonlinear model by continuous optimization},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Threshold single multiplicative neuron artificial neural
networks for non-linear time series forecasting. <em>JOAS</em>,
<em>48</em>(13-15), 2809–2825. (<a
href="https://doi.org/10.1080/02664763.2020.1869702">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single multiplicative neuron artificial neural networks have different importance than many other artificial neural networks because they do not have complex architecture problem, too many parameters and they need more computation time to use. In single multiplicative neuron artificial neural network, it is assumed that there is a one data generation process for time series. Many time series need an assumption that they have two data generation process or more. Based on this idea, the threshold model structure can be employed in a single multiplicative neuron model artificial neural network for taking into considering data generation processes problem. In this study, a new artificial neural network type is proposed and it is called a threshold single multiplicative neuron artificial neural network. It is assumed that time series have two data generation processes according to the architecture of single multiplicative neuron artificial neural network. Training algorithms are proposed based on harmony search algorithm and particle swarm optimization for threshold single multiplicative neuron artificial neural network. The proposed method is tested by various time series data sets and compared with well-known forecasting methods by considering different error measures. Finally, the performance of the proposed method is evaluated by a simulation study.},
  archive      = {J_JOAS},
  author       = {Asiye Nur Yildirim and Eren Bas and Erol Egrioglu},
  doi          = {10.1080/02664763.2020.1869702},
  journal      = {Journal of Applied Statistics},
  number       = {13-15},
  pages        = {2809-2825},
  shortjournal = {J. Appl. Stat.},
  title        = {Threshold single multiplicative neuron artificial neural networks for non-linear time series forecasting},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Mixed lasso estimator for stochastic restricted regression
models. <em>JOAS</em>, <em>48</em>(13-15), 2795–2808. (<a
href="https://doi.org/10.1080/02664763.2021.1922614">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Parameters of a linear regression model can be estimated with the help of traditional methods like generalized least squares and mixed estimator. However, recent developments increased the importance of big data sets, which have much more predictors than observations where some predictors have no impact on the dependent variable. The estimation and model selection problem of big datasets can be solved using the least absolute shrinkage and selection operator (Lasso). However, to the authors’ knowledge, there is no study that incorporates stochastic restrictions, within a Lasso framework. In this paper, we propose a Mixed Lasso (M-Lasso) estimator that incorporates stochastic linear restrictions to big data sets for selecting the true model and estimating parameters simultaneously. We conduct a simulation study to compare the performance of M-Lasso with existing estimators based on mean squared error ( mse ) and model selection performance. Results show that M-Lasso is superior in terms of mse and it generally dominates compared estimators according to the model selection criteria. We employ M-Lasso to estimate parameters of a widely analysed production function under stochastic restrictions raised from economic theory. Our results show that M-Lasso can provide reasonable and more precise estimates of model parameters that are in line with the economic theory.},
  archive      = {J_JOAS},
  author       = {Huseyin Guler and Ebru Ozgur Guler},
  doi          = {10.1080/02664763.2021.1922614},
  journal      = {Journal of Applied Statistics},
  number       = {13-15},
  pages        = {2795-2808},
  shortjournal = {J. Appl. Stat.},
  title        = {Mixed lasso estimator for stochastic restricted regression models},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A two-step machine learning approach to predict s&amp;p 500
bubbles. <em>JOAS</em>, <em>48</em>(13-15), 2776–2794. (<a
href="https://doi.org/10.1080/02664763.2020.1823947">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we are interested in predicting the bubbles in the S&amp;P 500 stock market with a two-step machine learning approach that employs a real-time bubble detection test and support vector machine (SVM). SVM as a nonparametric binary classification technique is already a widely used method in financial time series forecasting. In the literature, a bubble is often defined as a situation where the asset price exceeds its fundamental value. As one of the early warning signals, prediction of bubbles is vital for policymakers and regulators who are responsible to take preemptive measures against the future crises. Therefore, many attempts have been made to understand the main factors in bubble formation and to predict them in their earlier phases. Our analysis consists of two steps. The first step is to identify the bubbles in the S&amp;P 500 index using a widely recognized right-tailed unit root test. Then, SVM is employed to predict the bubbles by macroeconomic indicators. Also, we compare SVM with different supervised learning algorithms by using k -fold cross-validation. The experimental results show that the proposed approach with high predictive power could be a favourable alternative in bubble prediction.},
  archive      = {J_JOAS},
  author       = {Fatma Başoğlu Kabran and Kamil Demirberk Ünlü},
  doi          = {10.1080/02664763.2020.1823947},
  journal      = {Journal of Applied Statistics},
  number       = {13-15},
  pages        = {2776-2794},
  shortjournal = {J. Appl. Stat.},
  title        = {A two-step machine learning approach to predict S&amp;P 500 bubbles},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Some dominance indices to determine market concentration.
<em>JOAS</em>, <em>48</em>(13-15), 2755–2775. (<a
href="https://doi.org/10.1080/02664763.2021.1963421">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study intends to provide a new insight into the concentration and dominance indices as the concerns grow about the increasing concentration in the markets around the world. Most of the studies attempting to measure concentration or dominance in a market employ the popular concentration/dominance indices like Herfindahl–Hirschmann, Hannah–Kay, Rosenbluth–Hall–Tidemann and Concentration ratio. On the other hand, measures of qualitative variation are closely related to entropy, diversity and concentration/dominance measures. In this study, two normalized dominance measures that can be derived from the work of Wilcox on qualitative variation are proposed. The limiting distributions of these normalized dominance measures are formulated. By some simulations, asymptotic behaviors of these indices are analyzed under some assumptions about the market structure. In the end, by an application on the Turkish car sales in 2019, it is determined that the values of dominance indices vary in a considerably large range. Thus one of the dominance indices is determined to have the advantage of having less error in estimation, less sensitivity to smaller market shares, and less sampling variability.},
  archive      = {J_JOAS},
  author       = {Atif Evren and Elif Tuna and Erhan Ustaoglu and Busra Sahin},
  doi          = {10.1080/02664763.2021.1963421},
  journal      = {Journal of Applied Statistics},
  number       = {13-15},
  pages        = {2755-2775},
  shortjournal = {J. Appl. Stat.},
  title        = {Some dominance indices to determine market concentration},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Reject inference methods in credit scoring. <em>JOAS</em>,
<em>48</em>(13-15), 2734–2754. (<a
href="https://doi.org/10.1080/02664763.2021.1929090">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The granting process is based on the probability that the applicant will refund his/her loan given his/her characteristics. This probability, also called score, is learnt based on a dataset in which rejected applicants are excluded. Thus, the population on which the score is used is different from the learning population. Many “reject inference” methods try to exploit the data available from the rejected applicants in the learning process. However, most of these methods are empirical and lack of formalization of their assumptions, and of their expected theoretical properties. We formalize such hidden assumptions in a general missing data setting for some of the most common reject inference methods. It reveals that hidden modelling is mostly incomplete, thus prohibiting to compare existing methods within the general model selection mechanism (except by financing “non-fundable” applicants). So, we assess performance of the methods on both simulated data and real data (from CACF, a major European loan issuer). Unsurprisingly, no method seems uniformly dominant. Both these theoretical and empirical results not only reinforce the idea to carefully use the classical reject inference methods but also to invest in future research works for designing model-based reject inference methods (without financing “non-fundable” applicants).},
  archive      = {J_JOAS},
  author       = {Adrien Ehrhardt and Christophe Biernacki and Vincent Vandewalle and Philippe Heinrich and Sébastien Beben},
  doi          = {10.1080/02664763.2021.1929090},
  journal      = {Journal of Applied Statistics},
  number       = {13-15},
  pages        = {2734-2754},
  shortjournal = {J. Appl. Stat.},
  title        = {Reject inference methods in credit scoring},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Stochastic cusp catastrophe model and its bayesian
computations. <em>JOAS</em>, <em>48</em>(13-15), 2714–2733. (<a
href="https://doi.org/10.1080/02664763.2021.1922993">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper revitalizes the investigation of the classical cusp catastrophe model in catastrophe theory and tackles the unsolved statistical inference problem concerning stochastic cusp differential equation. This model is challenging because its associated transition density hence the likelihood function is analytically intractable. We propose a novel Bayesian approach combining Hamiltonian Monte Carlo with two likelihood approximation methods, namely, Euler approximation and Hermite expansion. We validate this novel approach through a series of simulation studies. We further demonstrate potential application of this novel approach using the real USD/EUR exchange rate.},
  archive      = {J_JOAS},
  author       = {Ding-Geng Chen and Haipeng Gao and Chuanshu Ji and Xinguang Chen},
  doi          = {10.1080/02664763.2021.1922993},
  journal      = {Journal of Applied Statistics},
  number       = {13-15},
  pages        = {2714-2733},
  shortjournal = {J. Appl. Stat.},
  title        = {Stochastic cusp catastrophe model and its bayesian computations},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Assessment of longevity risk: Credibility approach.
<em>JOAS</em>, <em>48</em>(13-15), 2695–2713. (<a
href="https://doi.org/10.1080/02664763.2021.1922613">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To correctly measure the effect of mortality rates on the stability of insurance and pension provider&#39;s financial risk, longevity risk should be considered. This paper aims to investigate the future mortality and longevity risk with different age structures for different countries. Lee–Carter mortality model is used on the historical census data to forecast future mortality rates. Turkey, Germany, and Japan are chosen concerning their expected life and population distributions. Then, the longevity risk on a hypothetical portfolio is assessed based on static and dynamic mortality table approaches. To determine the impact of longevity risk, which is retrieved using a stochastic mortality model, a pension insurance product is taken into account. The net single premium for an annuity is quantified under the proposed set up for the selected countries. Additionally, the credibility approach is proposed to establish a reliable estimate for the annuity net single premium.},
  archive      = {J_JOAS},
  author       = {Bükre Yıldırım Külekci and A. Sevtap Selcuk-Kestel},
  doi          = {10.1080/02664763.2021.1922613},
  journal      = {Journal of Applied Statistics},
  number       = {13-15},
  pages        = {2695-2713},
  shortjournal = {J. Appl. Stat.},
  title        = {Assessment of longevity risk: Credibility approach},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Econometric models of duration data in entrepreneurship with
an application to start-ups’ time-to-funding by venture capitalists
(VCs). <em>JOAS</em>, <em>48</em>(13-15), 2673–2694. (<a
href="https://doi.org/10.1080/02664763.2021.1896686">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Because time is a key determinant of entrepreneurial decision making, time-to-event models are ubiquitous in entrepreneurship. Widespread econometric misconception, however, may cause complicated biases in existing studies. The reason is spurious duration dependency, a complicated form of endogeneity caused by unobserved heterogeneity, which is particularly pronounced in entrepreneurship data. This article discusses the endogeneity problem and methods to ‘debias’ time-to-event models in entrepreneurship. Simulations and empirical evidence indicate that only the frailty approach yields consistently unbiased parameter estimates. An application to start-up firms&#39; time-to-funding shows that other methods lead to dramatic biases. Therefore, this article advocates a paradigm shift in the modeling of time variables in entrepreneurship.},
  archive      = {J_JOAS},
  author       = {Paul P. Momtaz},
  doi          = {10.1080/02664763.2021.1896686},
  journal      = {Journal of Applied Statistics},
  number       = {13-15},
  pages        = {2673-2694},
  shortjournal = {J. Appl. Stat.},
  title        = {Econometric models of duration data in entrepreneurship with an application to start-ups&#39; time-to-funding by venture capitalists (VCs)},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Restricted calibration and weight trimming approaches for
estimation of the population total in business statistics.
<em>JOAS</em>, <em>48</em>(13-15), 2658–2672. (<a
href="https://doi.org/10.1080/02664763.2020.1869703">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Some adjustments are made to design weights to reduce the negative effects of non-response and out-of-scope problems. The calibration approach is a weighting process that agrees with the known population values by using auxiliary information. In this study, alternative calibration approaches and weight trimming process that can be used in large data sets with extreme weights and different correlation structures were analysed. In addition, the effect of the correlation structure of auxiliary variables on the efficiency of the calibration estimators was investigated by a simulation study. The 2017 Annual Industry and Service Statistics data were used in the simulation study and it was seen that restricted calibration estimators were more efficient than the generalized regression estimator in estimating the variables with a high variance such as turnover. Especially in small sample fractions, we recommend the application of restricted calibration estimators, as they are more efficient than the weight trimming in solving the negative and less than one weights problem encountered after the calibration process.},
  archive      = {J_JOAS},
  author       = {Cenker Burak Metin and Sinem Tuğba Şahin Tekin and Yaprak Arzu Özdemir},
  doi          = {10.1080/02664763.2020.1869703},
  journal      = {Journal of Applied Statistics},
  number       = {13-15},
  pages        = {2658-2672},
  shortjournal = {J. Appl. Stat.},
  title        = {Restricted calibration and weight trimming approaches for estimation of the population total in business statistics},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Determining the relationship between stock return and
financial performance: An analysis on turkish deposit banks.
<em>JOAS</em>, <em>48</em>(13-15), 2643–2657. (<a
href="https://doi.org/10.1080/02664763.2020.1849056">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Banks play a very important role in financial markets due to their intermediary function. The availability of financing to businesses and individuals, the prevalence of branches throughout the country as well as the preference status at the collection point as a result of the habits of savings holders, have made deposit banks more active among other financial institutions. Since the banking system affects the whole economy, their performance and their performance evaluation become important. Performance measurement can be defined as one of the most important issues in the financial field. In this study, the relationship between stock return and financial performance of Turkish deposit banks was examined via CRITIC method, TOPSIS method and Spearman’s rank correlation analysis for 2014–2018 periods. According to the results of the analysis, there is no statistically significant correlation between the stock return ranking and financial performance rankings of deposit banks in Turkey.},
  archive      = {J_JOAS},
  author       = {M. Esra Atukalp},
  doi          = {10.1080/02664763.2020.1849056},
  journal      = {Journal of Applied Statistics},
  number       = {13-15},
  pages        = {2643-2657},
  shortjournal = {J. Appl. Stat.},
  title        = {Determining the relationship between stock return and financial performance: An analysis on turkish deposit banks},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The role of social capital in environmental protection
efforts: Evidence from turkey. <em>JOAS</em>, <em>48</em>(13-15),
2626–2642. (<a
href="https://doi.org/10.1080/02664763.2020.1843609">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The existing literature has recognized the role and importance of social capital in natural resource management. Several studies provide empirical evidence that higher levels of social capital may positively affect individuals&#39; behavior towards natural resources management. This study is therefore an attempt to investigate the environmental quality impacts of social capital and central government expenditures on environmental protection, taking spatial dimension into account from 2009 to 2017 for Turkey. A general-to-specific approach has been adopted where spatial variations in the relationships have been examined with a dynamic spatial Durbin model, using the panel data at NUTS3 level. The empirical results do not support the validity of an environmental Kuznets curve, rather a U-shaped environmental Kuznets curve is validated, which exhibits spatial dependence. Estimation results show that industrial production has detrimental effects on the environment, while social capital improves it. The central government expenditures on environmental protection are effective in the abatement of pollution, and its effectiveness is enhanced when social capital is controlled. In addition to spatial spillover effects, our results show the presence of strong path dependency; that is, there is certain pollution inertia. Moreover, environmental protection policies would be more effective if social capital levels are improved.},
  archive      = {J_JOAS},
  author       = {Julide Yildirim and Barış Alpaslan and Erdener Emin Eker},
  doi          = {10.1080/02664763.2020.1843609},
  journal      = {Journal of Applied Statistics},
  number       = {13-15},
  pages        = {2626-2642},
  shortjournal = {J. Appl. Stat.},
  title        = {The role of social capital in environmental protection efforts: Evidence from turkey},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hierarchies in communities of UK stock market from the
perspective of brexit. <em>JOAS</em>, <em>48</em>(13-15), 2607–2625. (<a
href="https://doi.org/10.1080/02664763.2020.1796942">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, increase of analyzing stock markets as complex systems lead graph theory to play a key role. For instance, detecting graph communities is an important task in the analysis of stocks, and as planar maximally filtered graphs let us to get important information for the topology of the market. In this study, we first obtain correlation network representation of UK&#39;s leading stock market network by using a novel threshold method. Then, we determine vertex clusters by using modularity and analyze clusters in planar maximally filtered graph substructures. Our analyze include a new measure called weighted Gini index for measuring the sparsity. The main goal of this paper is to study the hierarchical evolution of the market communities throughout the Brexit referendum, which is known as the stress period for the stock market. Hence, the overall sample is divided into two sub-periods of pre-referendum, and post-referendum to obtain communities and hierarchical structures. Our results indicate that financial companies are leading elements of the clusters. Moreover, the significant changes within the network topologies are observed for insurance, consumer goods, consumer services, mining, and technology sectors whereas oil and gas and health care sectors have not been affected by Brexit stress.},
  archive      = {J_JOAS},
  author       = {Mehmet Ali Balcı and Ömer Akgüller and Serdar Can Güzel},
  doi          = {10.1080/02664763.2020.1796942},
  journal      = {Journal of Applied Statistics},
  number       = {13-15},
  pages        = {2607-2625},
  shortjournal = {J. Appl. Stat.},
  title        = {Hierarchies in communities of UK stock market from the perspective of brexit},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Forecasting drought using neural network approaches with
transformed time series data. <em>JOAS</em>, <em>48</em>(13-15),
2591–2606. (<a
href="https://doi.org/10.1080/02664763.2020.1867829">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Drought is one of the important and costliest disaster all over the world. With the accelerated progress of climate change, its frequency of occurrence and negative impacts are rapidly increasing. It is crucial to initiate and sustain an early warning system to monitor and predict the possible impacts of future droughts. Recently, with the rise of data driven models, various case studies are conducted by using Machine Learning algorithms instead of using pure statistical approaches. The main goal of this paper is to conduct a drought forecasting study for a weather station located in Marmara Region. For that purpose, firstly, widely used univariate drought index, Standardized Precipitation Index is calculated for Bursa station. Thereafter, both the historical information retrieved from time series data and its wavelet transformation are considered to investigate Nonlinear Auto-Regressive and Nonlinear Auto-Regressive with External Input (NARX) type Neural Network (NN) models. According to a pool of Goodness-of-Fit (GOF) tests, the forecasting performance of the models with various number of hidden neurons are compared. The recent findings of the study showed that considering the data with its wavelet transformation under (NARX-NN) has benefits to increase the capacity of forecasting the drought index.},
  archive      = {J_JOAS},
  author       = {O. Ozan Evkaya and Fatma Sevinç Kurnaz},
  doi          = {10.1080/02664763.2020.1867829},
  journal      = {Journal of Applied Statistics},
  number       = {13-15},
  pages        = {2591-2606},
  shortjournal = {J. Appl. Stat.},
  title        = {Forecasting drought using neural network approaches with transformed time series data},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Comparison of forecast accuracy of ata and exponential
smoothing. <em>JOAS</em>, <em>48</em>(13-15), 2580–2590. (<a
href="https://doi.org/10.1080/02664763.2020.1803813">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Forecasting is a crucial step in almost all scientific research and is essential in many areas of industrial, commercial, clinical and economic activity. There are many forecasting methods in the literature; but exponential smoothing stands out due to its simplicity and accuracy. Despite the facts that exponential smoothing is widely used and has been in the literature for a long time, it suffers from some problems that potentially affect the model&#39;s forecast accuracy. An alternative forecasting framework, called Ata, was recently proposed to overcome these problems and to provide improved forecasts. In this study, the forecast accuracy of Ata and exponential smoothing will be compared among data sets with no or linear trend. The results of this study are obtained using simulated data sets with different sample sizes, variances. Forecast errors are compared within both short and long term forecasting horizons. The results show that the proposed approach outperforms exponential smoothing for both types of time series data when forecasting the near and distant future. The methods are implemented on the U.S. annualized monthly interest rates for services data and their forecasting performance are also compared for this data set.},
  archive      = {J_JOAS},
  author       = {Beyza Cetin and Idil Yavuz},
  doi          = {10.1080/02664763.2020.1803813},
  journal      = {Journal of Applied Statistics},
  number       = {13-15},
  pages        = {2580-2590},
  shortjournal = {J. Appl. Stat.},
  title        = {Comparison of forecast accuracy of ata and exponential smoothing},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On the performance of the variance ratio unit root tests
with flexible fourier form. <em>JOAS</em>, <em>48</em>(13-15),
2560–2579. (<a
href="https://doi.org/10.1080/02664763.2020.1796939">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article introduces a new unit root test that combines the variance ratio framework with the Flexible Fourier Form under the generalized least squares detrending mechanism. The advantage of the proposed method against its alternatives can be listed as: (1) it suggests a non-parametric procedure that does not require any parametric or semi-parametric model to remove serial correlation in the innovation process; (2) it can reasonably adapt itself to deal with the multiple structural breaks with various functional specifications. In the simulation exercises, we show that the proposed method exhibits satisfactory performance in the size and size-adjusted power analysis.},
  archive      = {J_JOAS},
  author       = {Burak A. Erog̃lu and Selim Yıldırım},
  doi          = {10.1080/02664763.2020.1796939},
  journal      = {Journal of Applied Statistics},
  number       = {13-15},
  pages        = {2560-2579},
  shortjournal = {J. Appl. Stat.},
  title        = {On the performance of the variance ratio unit root tests with flexible fourier form},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Testing fractional unit roots with non-linear smooth break
approximations using fourier functions. <em>JOAS</em>,
<em>48</em>(13-15), 2542–2559. (<a
href="https://doi.org/10.1080/02664763.2020.1757047">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present a testing procedure for fractional orders of integration in the context of non-linear terms approximated by Fourier functions. The test statistic has an asymptotic standard normal distribution and several Monte Carlo experiments conducted in the paper show that it performs well in finite samples. Various applications using real life time series, such as US unemployment rates, US GNP and Purchasing Power Parity (PPP) of G7 countries are presented at the end of the paper.},
  archive      = {J_JOAS},
  author       = {Luis A. Gil-Alana and OlaOluwa S. Yaya},
  doi          = {10.1080/02664763.2020.1757047},
  journal      = {Journal of Applied Statistics},
  number       = {13-15},
  pages        = {2542-2559},
  shortjournal = {J. Appl. Stat.},
  title        = {Testing fractional unit roots with non-linear smooth break approximations using fourier functions},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A multivariate poisson regression model for count data.
<em>JOAS</em>, <em>48</em>(13-15), 2525–2541. (<a
href="https://doi.org/10.1080/02664763.2021.1877637">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new technique for the study of multivariate count data. The proposed model is applied to the study of the number of individuals several fossil species found in a set of geographical observation points. First, we are proposing a multivariate model based on the Poisson distributions, which allows positive and negative correlations between the components. We are extending the log-linear Poisson model in the multivariate case through the conditional distributions. For this model, we obtain the maximum likelihood estimates and compute several goodness of fit statistics. Finally we illustrate the application of the proposed method over data sets: various simulated data sets and a count data set of various fossil species.},
  archive      = {J_JOAS},
  author       = {J. M. Muñoz-Pichardo and R. Pino-Mejías and J. García-Heras and F. Ruiz-Muñoz and M. Luz González-Regalado},
  doi          = {10.1080/02664763.2021.1877637},
  journal      = {Journal of Applied Statistics},
  number       = {13-15},
  pages        = {2525-2541},
  shortjournal = {J. Appl. Stat.},
  title        = {A multivariate poisson regression model for count data},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The lomax regression model with residual analysis: An
application to insurance data. <em>JOAS</em>, <em>48</em>(13-15),
2515–2524. (<a
href="https://doi.org/10.1080/02664763.2020.1834515">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we introduce a new regression model, called Lomax regression model , as an alternative to the gamma regression model. The maximum-likelihood method is used to estimate the unknown parameters of the proposed model, and the finite sample performance of the maximum-likelihood estimation method is evaluated by means of the Monte-Carlo simulation study. The randomized quantile residuals are used to check the adequacy of the fitted model. The insurance data are analyzed to demonstrate the usefulness of the proposed regression model against the gamma regression model.},
  archive      = {J_JOAS},
  author       = {Emrah Altun},
  doi          = {10.1080/02664763.2020.1834515},
  journal      = {Journal of Applied Statistics},
  number       = {13-15},
  pages        = {2515-2524},
  shortjournal = {J. Appl. Stat.},
  title        = {The lomax regression model with residual analysis: An application to insurance data},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A new alternative estimation method for liu-type logistic
estimator via particle swarm optimization: An application to data of
collapse of turkish commercial banks during the asian financial crisis.
<em>JOAS</em>, <em>48</em>(13-15), 2499–2514. (<a
href="https://doi.org/10.1080/02664763.2020.1837085">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the existence of multicollinearity problem in the logistic model, some important problems may occur in the analysis of the model, such as unstable maximum likelihood estimator with very high standard errors, false inferences. The Liu-type logistic estimator was proposed as two-parameter estimator to overcome multicollinearity problem in the logistic model. In the existing previous studies, the ( k , d ) pair in this shrinkage estimator is estimated by two-phase methods. However, since the different estimators can be utilized in the estimation of d , optimal choice of the ( k , d ) pair provided using the two-phase approaches is not guaranteed to overcome multicollinearity. In this article, a new alternative method based on particle swarm optimization is suggested to estimate ( k , d ) pair in Liu-type logistic estimator, simultaneously. For this purpose, an objective function that eliminates the multicollinearity problem, provides minimization of the bias of the model and improvement of the model’s predictive performance, is developed. Monte Carlo simulation study is conducted to show the performance of the proposed method by comparing it with existing methods. The performance of the proposed method is also demonstrated by the real dataset which is related to the collapse of commercial banks in Turkey during Asian financial crisis.},
  archive      = {J_JOAS},
  author       = {Nuriye Sancar and Deniz Inan},
  doi          = {10.1080/02664763.2020.1837085},
  journal      = {Journal of Applied Statistics},
  number       = {13-15},
  pages        = {2499-2514},
  shortjournal = {J. Appl. Stat.},
  title        = {A new alternative estimation method for liu-type logistic estimator via particle swarm optimization: An application to data of collapse of turkish commercial banks during the asian financial crisis},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Some shrinkage estimators based on median ranked set
sampling. <em>JOAS</em>, <em>48</em>(13-15), 2473–2498. (<a
href="https://doi.org/10.1080/02664763.2021.1895088">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, some shrinkage estimators using a median ranked set sample in the presence of multicollinearity were studied. Initially, we constructed the multiple regression model using median ranked set sampling. We also adapted the Ridge and Liu-type estimators to these multiple regression model. To investigate the efficiency of these estimators, a simulation study was performed for a different number of explanatory variables, sample sizes, correlation coefficients, and error variances in perfect and imperfect ranking cases. In addition, these estimators were compared with other estimators that are based on ranked set sample using simulation study. It is shown that when the collinearity is moderate, Ridge estimator using median ranked set sample performs better than other estimators and when the collinearity increases, Liu-type estimator using median ranked set sample gets better than all other estimators do. When the collinearity is smaller than 0.95, ridge estimator based on median ranked set sample is more efficient than Liu-type estimator based on same sample. However, this threshold increases as the sample size increases and the number of explanatory variables decreases. In addition, real data example is presented to illustrate how collinearity affects the estimators under median ranked set sampling and ranked set sampling.},
  archive      = {J_JOAS},
  author       = {Meral Ebegil and Yaprak Arzu Özdemir and Fikri Gökpinar},
  doi          = {10.1080/02664763.2021.1895088},
  journal      = {Journal of Applied Statistics},
  number       = {13-15},
  pages        = {2473-2498},
  shortjournal = {J. Appl. Stat.},
  title        = {Some shrinkage estimators based on median ranked set sampling},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A new robust ridge parameter estimator based on search
method for linear regression model. <em>JOAS</em>, <em>48</em>(13-15),
2457–2472. (<a
href="https://doi.org/10.1080/02664763.2020.1803814">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A large and wide variety of ridge parameter estimators proposed for linear regression models exist in the literature. Actually proposing new ridge parameter estimator lately proving its efficiency on few cases seems endless. However, so far there is no ridge parameter estimator that can serve best for any sample size or any degree of collinearity among regressors. In this study we propose a new robust ridge parameter estimator that serves best for any case assuring that is free of sample size, number of regressors and degree of collinearity. This is in fact realized by choosing three best from enormous number of ridge parameter estimators performing well in different cases in developing the new ridge parameter estimator in a way of search method providing the smallest mean square error values of regression parameters. After that a simulation study is conducted to show that the proposed parameter is robust. In conclusion, it is found that this ridge parameter estimator is promising in any case. Moreover, a recent data set is used as an example for illustration to show that the proposed ridge parameter estimator is performing better.},
  archive      = {J_JOAS},
  author       = {Atila Göktaş and Özge Akkuş and Aykut Kuvat},
  doi          = {10.1080/02664763.2020.1803814},
  journal      = {Journal of Applied Statistics},
  number       = {13-15},
  pages        = {2457-2472},
  shortjournal = {J. Appl. Stat.},
  title        = {A new robust ridge parameter estimator based on search method for linear regression model},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Binary particle swarm optimization as a detection tool for
influential subsets in linear regression. <em>JOAS</em>,
<em>48</em>(13-15), 2441–2456. (<a
href="https://doi.org/10.1080/02664763.2020.1779196">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An influential observation is any point that has a huge effect on the coefficients of a regression line fitting the data. The presence of such observations in the data set reduces the sensitivity and validity of the statistical analysis. In the literature there are many methods used for identifying influential observations. However, many of those methods are highly influenced by masking and swamping effects and require distributional assumptions. Especially in the presence of influential subsets most of these methods are insufficient to detect these observations. This study aims to develop a new diagnostic tool for identifying influential observations using the meta-heuristic binary particle swarm optimization algorithm. This proposed approach does not require any distributional assumptions and also not affected by masking and swamping effects as the known methods. The performance of the proposed method is analyzed via simulations and real data set applications.},
  archive      = {J_JOAS},
  author       = {G. Deliorman and D. Inan},
  doi          = {10.1080/02664763.2020.1779196},
  journal      = {Journal of Applied Statistics},
  number       = {13-15},
  pages        = {2441-2456},
  shortjournal = {J. Appl. Stat.},
  title        = {Binary particle swarm optimization as a detection tool for influential subsets in linear regression},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A new outlier detection method based on convex optimization:
Application to diagnosis of parkinson’s disease. <em>JOAS</em>,
<em>48</em>(13-15), 2421–2440. (<a
href="https://doi.org/10.1080/02664763.2020.1864815">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neuroscience is a combination of different scientific disciplines which investigate the nervous system for understanding of the biological basis. Recently, applications to the diagnosis of neurodegenerative diseases like Parkinson’s disease have become very promising by considering different statistical regression models. However, well-known statistical regression models may give misleading results for the diagnosis of the neurodegenerative diseases when experimental data contain outlier observations that lie an abnormal distance from the other observation. The main achievements of this study consist of a novel mathematics-supported approach beside statistical regression models to identify and treat the outlier observations without direct elimination for a great and emerging challenge in humankind, such as neurodegenerative diseases. By this approach, a new method named as CM T MSOM is proposed with the contributions of the powerful convex and continuous optimization techniques referred to as conic quadratic programing. This method, based on the mean-shift outlier regression model, is developed by combining robustness of M-estimation and stability of Tikhonov regularization. We apply our method and other parametric models on Parkinson telemonitoring dataset which is a real-world dataset in Neuroscience. Then, we compare these methods by using well-known method-free performance measures. The results indicate that the CM T MSOM method performs better than current parametric models.},
  archive      = {J_JOAS},
  author       = {Pakize Taylan and Fatma Yerlikaya-Özkurt and Burcu Bilgiç Uçak and Gerhard-Wilhelm Weber},
  doi          = {10.1080/02664763.2020.1864815},
  journal      = {Journal of Applied Statistics},
  number       = {13-15},
  pages        = {2421-2440},
  shortjournal = {J. Appl. Stat.},
  title        = {A new outlier detection method based on convex optimization: Application to diagnosis of parkinson’s disease},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). CD-vine model for capturing complex dependence.
<em>JOAS</em>, <em>48</em>(13-15), 2406–2420. (<a
href="https://doi.org/10.1080/02664763.2020.1834519">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Copula based finite mixture models allow us to capture the dependence between random variables more flexibly. Although bivariate case of finite mixture models has been commonly studied, limited efforts have been spent on finite mixture of vines. Instead of using classical mixture models, it is possible to incorporate C-vines into the D-vine model (CD-vine) to understand both the dependence among the variables over different time points. The aim of this study is to create a CD-vine mixture model expressing the dependencies between variables in temporal order. To achieve this, cumulative distribution function values generated within the time components are tied together with D-vine probabilistically. With this approach, dependence structure between variables at each time point is explained by C-vine and the dependence among the time points is captured by the D-vine model. The performance of the proposed CD-vine model is validated using simulated data and applied on four stock market indices.},
  archive      = {J_JOAS},
  author       = {O. Ozan Evkaya and Ceylan Yozgatlıgil and A. Sevtap Selcuk-Kestel},
  doi          = {10.1080/02664763.2020.1834519},
  journal      = {Journal of Applied Statistics},
  number       = {13-15},
  pages        = {2406-2420},
  shortjournal = {J. Appl. Stat.},
  title        = {CD-vine model for capturing complex dependence},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Unit-lindley mixed-effect model for proportion data.
<em>JOAS</em>, <em>48</em>(13-15), 2389–2405. (<a
href="https://doi.org/10.1080/02664763.2020.1823946">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, unit-Lindley distribution and its associated regression models have been developed as an alternative to Beta regression model for which continuous outcome in the unit interval ( 0 , 1 ) . Proportion data usually occur in clinical trials, economics and social studies with hierarchical structures. In this study, unit-Lindley mixed-effect model is proposed and the appropriate likelihood analysis methods for parameter estimation are investigated. In the case of clustered or longitudinal proportion data in mixed-effect models, the full-likelihood function does not have a closed form. Parameter estimations of unit-Lindley mixed-effect model are obtained with Laplace and adaptive Gaussian quadrature approximation methods in this study. We analyzed a dataset on the proportion of households with insufficient water supply and sewage with some sociodemographic variables in the cities of Brazil by using unit-Lindley mixed-effect model including a random intercept as federative states of Brazil. Analysis results indicate that the proposed unit-Lindley mixed-effect model provides better fit than unit-Lindley regression model and beta mixed model. Also, in the simulation study the accuracy of the estimates of approximation methods are evaluated and compared via Monte Carlo simulation study in terms of bias and mean square error.},
  archive      = {J_JOAS},
  author       = {Hatice Tul Kubra Akdur},
  doi          = {10.1080/02664763.2020.1823946},
  journal      = {Journal of Applied Statistics},
  number       = {13-15},
  pages        = {2389-2405},
  shortjournal = {J. Appl. Stat.},
  title        = {Unit-lindley mixed-effect model for proportion data},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Optimal b-robust estimators for the parameters of the power
lindley distribution. <em>JOAS</em>, <em>48</em>(13-15), 2369–2388. (<a
href="https://doi.org/10.1080/02664763.2020.1854201">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Parameters of a distribution are generally estimated by using the classical methods such as maximum likelihood (ML) and least squares (LS) estimation. However, these classical methods are very sensitive to outliers. This study, therefore, proposes the application of the optimal B-robust (OBR) estimation method, which is resistant to outliers, to estimate the parameters of power Lindley (PL) distribution. We also provide a simulation study and a real data example to compare the performance of the OBR estimators with the performances of the ML, LS, and the regression M estimators.},
  archive      = {J_JOAS},
  author       = {Berivan Çakmak and Fatma Zehra Doğru},
  doi          = {10.1080/02664763.2020.1854201},
  journal      = {Journal of Applied Statistics},
  number       = {13-15},
  pages        = {2369-2388},
  shortjournal = {J. Appl. Stat.},
  title        = {Optimal B-robust estimators for the parameters of the power lindley distribution},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Minimizing the expected value of the asymmetric loss
function and an inequality for the variance of the loss. <em>JOAS</em>,
<em>48</em>(13-15), 2348–2368. (<a
href="https://doi.org/10.1080/02664763.2020.1761951">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The coefficients of regression are usually estimated for minimization problems with asymmetric loss functions. In this paper, we rather correct predictions so that the prediction error follows a generalized Gaussian distribution. In our method, we not only minimize the expected value of the asymmetric loss, but also lower the variance of the loss. Predictions usually have errors. Therefore, it is necessary to use predictions in consideration of these errors. Our approach takes into account prediction errors. Furthermore, even if we do not understand the prediction method, which is a possible circumstance in, e.g. deep learning, we can use our method if we know the prediction error distribution and asymmetric loss function. Our method can be applied to procurement of electricity from electricity markets.},
  archive      = {J_JOAS},
  author       = {Naoya Yamaguchi and Yuka Yamaguchi and Ryuei Nishii},
  doi          = {10.1080/02664763.2020.1761951},
  journal      = {Journal of Applied Statistics},
  number       = {13-15},
  pages        = {2348-2368},
  shortjournal = {J. Appl. Stat.},
  title        = {Minimizing the expected value of the asymmetric loss function and an inequality for the variance of the loss},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A novel perspective for parameter estimation of seemingly
unrelated nonlinear regression. <em>JOAS</em>, <em>48</em>(13-15),
2326–2347. (<a
href="https://doi.org/10.1080/02664763.2021.1877638">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nonlinear regression is commonly used as a modeling tool to get a functional form between inputs and response variables when the inputs and the responses have a nonlinear relationship. It should be better to compose the predicted nonlinear models with considering correlation between the responses for multi-response data sets. For this purpose, seemingly unrelated nonlinear regression (SUNR) have been widely used in the literature. The parameter estimation procedure of the SUNR is based on nonlinear least squares ( NLS ) method, based on L 2 -norm. However, it is possible to use different norms for parameter estimation process. The novelty of this study is presenting the applicability of least absolute deviation ( LAD ) method, defined in L 1 -norm, with the NLS method simultaneously for obtaining parameter estimates of the SUNR model in a multi objective perspective. In this study, the proposed multi-objective SUNR model is called MO-SUNR. The optimization of the MO-SUNR model is achieved by using soft computing methods. Two data set examples are given for application purposes of the MO-SUNR model. It is seen from the results that the MO-SUNR provides many alternatively usable compromise parameter estimates through the simultaneous evaluation of the LAD and the NLS methods.},
  archive      = {J_JOAS},
  author       = {Özlem Türkşen},
  doi          = {10.1080/02664763.2021.1877638},
  journal      = {Journal of Applied Statistics},
  number       = {13-15},
  pages        = {2326-2347},
  shortjournal = {J. Appl. Stat.},
  title        = {A novel perspective for parameter estimation of seemingly unrelated nonlinear regression},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Statistical inference for distributions with one poisson
conditional. <em>JOAS</em>, <em>48</em>(13-15), 2306–2325. (<a
href="https://doi.org/10.1080/02664763.2021.1928017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It will be recalled that the classical bivariate normal distributions have normal marginals and normal conditionals. It is natural to ask whether a similar phenomenon can be encountered involving Poisson marginals and conditionals. However, it is known, from research on conditionally specified models, that Poisson marginals will be encountered, together with both conditionals being of the Poisson form, only in the case in which the variables are independent. In order to have a flexible dependent bivariate model with some Poisson components, in the present article, we will be focusing on bivariate distributions with one marginal and the other family of conditionals being of the Poisson form. Such distributions are called Pseudo-Poisson distributions. We discuss distributional features of such models, explore inferential aspects and include an example of applications of the Pseudo-Poisson model to sets of over-dispersed data.},
  archive      = {J_JOAS},
  author       = {Barry C. Arnold and B. G. Manjunath},
  doi          = {10.1080/02664763.2021.1928017},
  journal      = {Journal of Applied Statistics},
  number       = {13-15},
  pages        = {2306-2325},
  shortjournal = {J. Appl. Stat.},
  title        = {Statistical inference for distributions with one poisson conditional},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Tail conditional moment for generalized skew-elliptical
distributions. <em>JOAS</em>, <em>48</em>(13-15), 2285–2305. (<a
href="https://doi.org/10.1080/02664763.2021.1896687">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Substantial changes in the financial markets and insurance companies have needed the development of the structure of the risk benchmark, which is the challenge addressed in this paper. We propose a theorem that expands the tail conditional moment (TCM) measure from elliptical distributions to wider classes of skew-elliptical distributions. This family of distributions is suitable for modeling asymmetric phenomena. We obtain the analytical formula for the n th n th nth TCM for skew-elliptical distributions to help well to figure out the risk behavior along the tail of loss distributions. We derive four significant results and generalize the tail conditional skewness (TCS) and the tail conditional kurtosis (TCK) measures for generalized skew-elliptical distributions, which are used to determine the skewness and the kurtosis in the tail of loss distributions. The proposed TCM measure has been applied to well-known families of generalized skew-elliptical distributions. We also provide a practical example of a portfolio problem by calculating the proposed TCM measure for the weighted sum of generalized skew-elliptical distributions.},
  archive      = {J_JOAS},
  author       = {Esmat Jamshidi Eini and Hamid Khaloozadeh},
  doi          = {10.1080/02664763.2021.1896687},
  journal      = {Journal of Applied Statistics},
  number       = {13-15},
  pages        = {2285-2305},
  shortjournal = {J. Appl. Stat.},
  title        = {Tail conditional moment for generalized skew-elliptical distributions},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Comparison of different estimation methods for extreme value
distribution. <em>JOAS</em>, <em>48</em>(13-15), 2259–2284. (<a
href="https://doi.org/10.1080/02664763.2021.1940109">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The extreme value distribution was developed for modeling extreme-order statistics or extreme events. In this study, we discuss the distribution of the largest extreme. The main objective of this paper is to determine the best estimators of the unknown parameters of the extreme value distribution. Thus, both classical and Bayesian methods are used. The classical estimation methods under consideration are maximum likelihood estimators, moment’s estimators, least squares estimators, and weighted least squares estimators, percentile estimators, the ordinary least squares estimators, best linear unbiased estimators, L-moments estimators, trimmed L-moments estimators, and Bain and Engelhardt estimators. We also propose new estimators for the unknown parameters. Bayesian estimators of the parameters are derived by using Lindley’s approximation and Markov Chain Monte Carlo methods . The asymptotic confidence intervals are considered by using maximum likelihood estimators. The Bayesian credible intervals are also obtained by using Gibbs sampling. The performances of these estimation methods are compared with respect to their biases and mean square errors through a simulation study. The maximum daily flood discharge (annual) data sets of the Meriç River and Feather River are analyzed at the end of the study for a better understanding of the methods presented in this paper.},
  archive      = {J_JOAS},
  author       = {Asuman Yılmaz and Mahmut Kara and Onur Özdemir},
  doi          = {10.1080/02664763.2021.1940109},
  journal      = {Journal of Applied Statistics},
  number       = {13-15},
  pages        = {2259-2284},
  shortjournal = {J. Appl. Stat.},
  title        = {Comparison of different estimation methods for extreme value distribution},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Optimal bandwidth estimators of kernel density functionals
for contaminated data. <em>JOAS</em>, <em>48</em>(13-15), 2239–2258. (<a
href="https://doi.org/10.1080/02664763.2021.1944999">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we provide simulation-based exploration and characterization of the two most crucial kernel density functionals that play a central role in kernel density estimation, considering the probability density functions that are members of the location-scale family. Kernel density functional estimates are known to rely on the choice of preliminary bandwidth. Normal-scale estimators are commonly used to obtain preliminary bandwidth estimates, with the assumption that the data come from normal distribution. Here, we present an alternative approach, called the Cauchy-scale estimators, to obtain preliminary bandwidth estimates. In this approach, data are assumed to come from a Cauchy distribution. Furthermore, analysis results related to the sampling distribution of bandwidth estimators based on the normal- and Cauchy-scale approaches are presented. As a case study, we provide a comprehensive characterization of different contamination levels with a simulation study constructed for the random samples from normal distributions with various parameters and various contamination levels. The proposed preliminary bandwidth selection shows lower variance in both mixture and contaminated data in our simulations. Besides, functional bandwidth presents results similar to the simulation results in the applications we made on the real data set.},
  archive      = {J_JOAS},
  author       = {Necla Gündüz and Celal Aydın},
  doi          = {10.1080/02664763.2021.1944999},
  journal      = {Journal of Applied Statistics},
  number       = {13-15},
  pages        = {2239-2258},
  shortjournal = {J. Appl. Stat.},
  title        = {Optimal bandwidth estimators of kernel density functionals for contaminated data},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Editorial to the special issue: Recent statistical methods
for data analysis, applied economics, business &amp; finance.
<em>JOAS</em>, <em>48</em>(13-15), 2231–2238. (<a
href="https://doi.org/10.1080/02664763.2021.1991180">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JOAS},
  author       = {A. Göktaş and Ö. Akkuş},
  doi          = {10.1080/02664763.2021.1991180},
  journal      = {Journal of Applied Statistics},
  number       = {13-15},
  pages        = {2231-2238},
  shortjournal = {J. Appl. Stat.},
  title        = {Editorial to the special issue: Recent statistical methods for data analysis, applied economics, business &amp; finance},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An elastic-net penalized expectile regression with
applications. <em>JOAS</em>, <em>48</em>(12), 2205–2230. (<a
href="https://doi.org/10.1080/02664763.2020.1787355">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To perform variable selection in expectile regression, we introduce the elastic-net penalty into expectile regression and propose an elastic-net penalized expectile regression (ER-EN) model. We then adopt the semismooth Newton coordinate descent (SNCD) algorithm to solve the proposed ER-EN model in high-dimensional settings. The advantages of ER-EN model are illustrated via extensive Monte Carlo simulations. The numerical results show that the ER-EN model outperforms the elastic-net penalized least squares regression (LSR-EN), the elastic-net penalized Huber regression (HR-EN), the elastic-net penalized quantile regression (QR-EN) and conventional expectile regression (ER) in terms of variable selection and predictive ability, especially for asymmetric distributions. We also apply the ER-EN model to two real-world applications: relative location of CT slices on the axial axis and metabolism of tacrolimus (Tac) drug. Empirical results also demonstrate the superiority of the ER-EN model.},
  archive      = {J_JOAS},
  author       = {Q.F. Xu and X.H. Ding and C.X. Jiang and K.M. Yu and L. Shi},
  doi          = {10.1080/02664763.2020.1787355},
  journal      = {Journal of Applied Statistics},
  number       = {12},
  pages        = {2205-2230},
  shortjournal = {J. Appl. Stat.},
  title        = {An elastic-net penalized expectile regression with applications},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Monitoring coefficient of variation using one-sided run
rules control charts in the presence of measurement errors.
<em>JOAS</em>, <em>48</em>(12), 2178–2204. (<a
href="https://doi.org/10.1080/02664763.2020.1787356">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate, in this paper, the effect of the measurement error (ME) on the performance of Run Rules control charts monitoring the coefficient of variation (CV) squared. The previous Run Rules CV chart in the literature is improved slightly by monitoring the CV squared using two one-sided Run Rules charts instead of monitoring the CV itself using a two-sided chart. The numerical results show that this improvement gives better performance in detecting process shifts. Moreover, we will show through simulation that the precision and accuracy errors do have a negative effect on the performance of the proposed Run Rules charts. We also find out that taking multiple measurements per item is not an effective way to reduce these negative effects. The proposed Run Rules control charts can be applied in the anomaly detection area.},
  archive      = {J_JOAS},
  author       = {Phuong Hanh Tran and Cédric Heuchenne and Huu Du Nguyen and Hélène Marie},
  doi          = {10.1080/02664763.2020.1787356},
  journal      = {Journal of Applied Statistics},
  number       = {12},
  pages        = {2178-2204},
  shortjournal = {J. Appl. Stat.},
  title        = {Monitoring coefficient of variation using one-sided run rules control charts in the presence of measurement errors},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Maximum expected entropy transformed latin hypercube
designs. <em>JOAS</em>, <em>48</em>(12), 2152–2177. (<a
href="https://doi.org/10.1080/02664763.2020.1786674">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing projection designs (e.g. maximum projection designs) attempt to achieve good space-filling properties in all projections. However, when using a Gaussian process (GP), model-based design criteria such as the entropy criterion is more appropriate. We employ the entropy criterion averaged over a set of projections, called expected entropy criterion (EEC), to generate projection designs. We show that maximum EEC designs are invariant to monotonic transformations of the response, i.e. they are optimal for a wide class of stochastic process models. We also demonstrate that transformation of each column of a Latin hypercube design (LHD) based on a monotonic function can substantially improve the EEC. Two types of input transformations are considered: a quantile function of a symmetric Beta distribution chosen to optimize the EEC, and a nonparametric transformation corresponding to the quantile function of a symmetric density chosen to optimize the EEC. Numerical studies show that the proposed transformations of the LHD are efficient and effective for building robust maximum EEC designs. These designs give projections with markedly higher entropies and lower maximum prediction variances (MPV&#39;s) at the cost of small increases in average prediction variances (APV&#39;s) compared to state-of-the-art space-filling designs over wide ranges of covariance parameter values.},
  archive      = {J_JOAS},
  author       = {Chong Sheng and Matthias Hwai Yong Tan and Lu Zou},
  doi          = {10.1080/02664763.2020.1786674},
  journal      = {Journal of Applied Statistics},
  number       = {12},
  pages        = {2152-2177},
  shortjournal = {J. Appl. Stat.},
  title        = {Maximum expected entropy transformed latin hypercube designs},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Modified maximum likelihood estimator under the jones and
faddy’s skew t-error distribution for censored regression model.
<em>JOAS</em>, <em>48</em>(12), 2136–2151. (<a
href="https://doi.org/10.1080/02664763.2020.1786673">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is well-known that classical Tobit estimator of the parameters of the censored regression (CR) model is inefficient in case of non-normal error terms. In this paper, we propose to use the modified maximum likelihood (MML) estimator under the Jones and Faddy&#39;s skew t -error distribution, which covers a wide range of skew and symmetric distributions, for the CR model. The MML estimators, providing an alternative to the Tobit estimator, are explicitly expressed and they are asymptotically equivalent to the maximum likelihood estimator. A simulation study is conducted to compare the efficiencies of the MML estimators with the classical estimators such as the ordinary least squares, Tobit, censored least absolute deviations and symmetrically trimmed least squares estimators. The results of the simulation study show that the MML estimators work well among the others with respect to the root mean square error criterion for the CR model. A real life example is also provided to show the suitability of the MML methodology.},
  archive      = {J_JOAS},
  author       = {Sukru Acitas and Ismail Yenilmez and Birdal Senoglu and Yeliz Mert Kantar},
  doi          = {10.1080/02664763.2020.1786673},
  journal      = {Journal of Applied Statistics},
  number       = {12},
  pages        = {2136-2151},
  shortjournal = {J. Appl. Stat.},
  title        = {Modified maximum likelihood estimator under the jones and faddy&#39;s skew t-error distribution for censored regression model},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Stochastic EM algorithm for generalized exponential cure
rate model and an empirical study. <em>JOAS</em>, <em>48</em>(12),
2112–2135. (<a
href="https://doi.org/10.1080/02664763.2020.1786676">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we consider two well-known parametric long-term survival models, namely, the Bernoulli cure rate model and the promotion time (or Poisson) cure rate model. Assuming the long-term survival probability to depend on a set of risk factors, the main contribution is in the development of the stochastic expectation maximization (SEM) algorithm to determine the maximum likelihood estimates of the model parameters. We carry out a detailed simulation study to demonstrate the performance of the proposed SEM algorithm. For this purpose, we assume the lifetimes due to each competing cause to follow a two-parameter generalized exponential distribution. We also compare the results obtained from the SEM algorithm with those obtained from the well-known expectation maximization (EM) algorithm. Furthermore, we investigate a simplified estimation procedure for both SEM and EM algorithms that allow the objective function to be maximized to split into simpler functions with lower dimensions with respect to model parameters. Moreover, we present examples where the EM algorithm fails to converge but the SEM algorithm still works. For illustrative purposes, we analyze a breast cancer survival data. Finally, we use a graphical method to assess the goodness-of-fit of the model with generalized exponential lifetimes.},
  archive      = {J_JOAS},
  author       = {Katherine Davies and Suvra Pal and Joynob A. Siddiqua},
  doi          = {10.1080/02664763.2020.1786676},
  journal      = {Journal of Applied Statistics},
  number       = {12},
  pages        = {2112-2135},
  shortjournal = {J. Appl. Stat.},
  title        = {Stochastic EM algorithm for generalized exponential cure rate model and an empirical study},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Equal-bin-width histogram versus equal-bin-count histogram.
<em>JOAS</em>, <em>48</em>(12), 2092–2111. (<a
href="https://doi.org/10.1080/02664763.2020.1784853">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The histogram has all its bin widths equal to some non-random number arbitrary set by an analyst (EBWH). In the result, particular bin counts are random variables. This paper presents also a histogram that is constructed in a converse manner. Bin counts are all equal to some non-random number arbitrary set by an analyst (EBCH). In the result, particular bin widths are random variables. The first goal of the paper is a choose of constant bin width (of bin numbers k ) in the EBWH, which maximize the similarity measure in the Monte Carlo simulation. The second goal is a choose of constant bin count in the EBCH, which maximize the similarity measure in the Monte Carlo simulation. The third goal is to present similarity measures between empirical and theoretical data. The fourth goal is the comparative analysis of two histogram methods by means of the frequency formula. The first additional goal is a tip how to proceed in EBCH when modulo( n , k )≠0. The second additional goal is the software in the form of a Mathcad file with the implementation of EBWH and EBCH.},
  archive      = {J_JOAS},
  author       = {Piotr Sulewski},
  doi          = {10.1080/02664763.2020.1784853},
  journal      = {Journal of Applied Statistics},
  number       = {12},
  pages        = {2092-2111},
  shortjournal = {J. Appl. Stat.},
  title        = {Equal-bin-width histogram versus equal-bin-count histogram},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Two new nonparametric kernel distribution estimators based
on a transformation of the data. <em>JOAS</em>, <em>48</em>(12),
2065–2091. (<a
href="https://doi.org/10.1080/02664763.2020.1786675">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose two kernel distribution estimators based on a data transformation. We study the properties of these estimators and we compare them with two conventional estimators. It appears that with an appropriate choice of the parameters of the two proposed estimators, the convergence rate of two estimators will be faster than that of the two conventional estimators and the Mean Integrated Square Error will be smaller than the two conventional estimators. We corroborate these theoretical results through simulations as well as a real data set.},
  archive      = {J_JOAS},
  author       = {Yousri Slaoui},
  doi          = {10.1080/02664763.2020.1786675},
  journal      = {Journal of Applied Statistics},
  number       = {12},
  pages        = {2065-2091},
  shortjournal = {J. Appl. Stat.},
  title        = {Two new nonparametric kernel distribution estimators based on a transformation of the data},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). RETRACTED ARTICLE: A novel alpha power transformed
exponential distribution with real-life applications. <em>JOAS</em>,
<em>48</em>(11), I–XVI. (<a
href="https://doi.org/10.1080/02664763.2020.1870673">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We, the Editor-in-Chief and Publisher of Journal of Applied Statistics have retracted the following article, which was due to appear in a special issue: Muhammad Ijaz, Wali Khan Mashwani, Atilla Göktaş &amp; Yuksel Akay Unvan (2021): A novel alpha power transformed exponential distribution with real-life applications, Journal of Applied Statistics . DOI: 10.1080/02664763.2020.1870673 . The Editor-in-Chief and the Publisher are cognisant of clear evidence that the findings presented are unreliable. The probability distribution is only valid if α &gt; 1 and numerous mathematical properties in Section 2 have been shown to be incorrect. This has then impacted at least two figures in the article. We are further cognisant that the article contained a number of similarities to previously published papers where some of the findings had been published without proper cross-referencing including: Gupta, R.D. and Kundu, D. (2001), Exponentiated Exponential Family: An Alternative to Gamma and Weibull Distributions. Biom. J., 43: 117–130. https://doi.org/10.1002/1521-4036(200102)43:13.0.CO;2-R . We have been informed in our decision-making by our corrections and editorial policies and the Committee on Publication Ethics (COPE) guidelines on retractions. The retracted article will remain online to maintain the scholarly record, but it will be digitally watermarked on each page as ‘Retracted’. The Editor-in-Chief and the Publisher would like to thank the anonymous reader/s for their comments which alerted JAS to these major errors in the first instance.},
  archive      = {J_JOAS},
  author       = {Muhammad Ijaz and Wali Khan Mashwani and Atilla Göktaş and Yuksel Akay Unvan},
  doi          = {10.1080/02664763.2020.1870673},
  journal      = {Journal of Applied Statistics},
  number       = {11},
  pages        = {I-XVI},
  shortjournal = {J. Appl. Stat.},
  title        = {RETRACTED ARTICLE: A novel alpha power transformed exponential distribution with real-life applications},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Statement of retraction: A novel alpha power transformed
exponential distribution with real-life applications. <em>JOAS</em>,
<em>48</em>(11), 2064. (<a
href="https://doi.org/10.1080/02664763.2021.1955521">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JOAS},
  doi          = {10.1080/02664763.2021.1955521},
  journal      = {Journal of Applied Statistics},
  number       = {11},
  pages        = {2064},
  shortjournal = {J. Appl. Stat.},
  title        = {Statement of retraction: A novel alpha power transformed exponential distribution with real-life applications},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Testing for correlation between two time series using a
parametric bootstrap. <em>JOAS</em>, <em>48</em>(11), 2042–2063. (<a
href="https://doi.org/10.1080/02664763.2020.1783519">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the problem of determining if two time series are correlated in the mean and variance. Several test statistics, originally designed for determining the correlation between two mean processes or goodness-of-fit testing, are explored and formally introduced for determining cross-correlation in variance. Simulations demonstrate the theoretical asymptotic distribution can be ineffective in finite samples. Parametric bootstrapping is shown to be an effective tool in such an enterprise. A large simulation study is provided demonstrating the efficacy of the bootstrapping method. Lastly, an empirical example explores a correlation between the Standard &amp; Poor&#39;s 500 index and the Euro/US dollar exchange rate while also demonstrating a level of robustness for the proposed method.},
  archive      = {J_JOAS},
  author       = {Zequn Sun and Thomas J. Fisher},
  doi          = {10.1080/02664763.2020.1783519},
  journal      = {Journal of Applied Statistics},
  number       = {11},
  pages        = {2042-2063},
  shortjournal = {J. Appl. Stat.},
  title        = {Testing for correlation between two time series using a parametric bootstrap},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Feasibility as a mechanism for model identification and
validation. <em>JOAS</em>, <em>48</em>(11), 2022–2041. (<a
href="https://doi.org/10.1080/02664763.2020.1783522">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As new technologies permit the generation of hitherto unprecedented volumes of data (e.g. genome-wide association study data), researchers struggle to keep up with the added complexity and time commitment required for its analysis. For this reason, model selection commonly relies on machine learning and data-reduction techniques, which tend to afford models with obscure interpretations. Even in cases with straightforward explanatory variables, the so-called ‘best’ model produced by a given model-selection technique may fail to capture information of vital importance to the domain-specific questions at hand. Herein we propose a new concept for model selection, feasibility , for use in identifying multiple models that are in some sense optimal and may unite to provide a wider range of information relevant to the topic of interest, including (but not limited to) interaction terms. We further provide an R package and associated Shiny Applications for use in identifying or validating feasible models, the performance of which we demonstrate on both simulated and real-life data.},
  archive      = {J_JOAS},
  author       = {Corrine F. Elliott and Joshua W. Lambert and Arnold J. Stromberg and Pei Wang and Ting Zeng and Katherine L. Thompson},
  doi          = {10.1080/02664763.2020.1783522},
  journal      = {Journal of Applied Statistics},
  number       = {11},
  pages        = {2022-2041},
  shortjournal = {J. Appl. Stat.},
  title        = {Feasibility as a mechanism for model identification and validation},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Augmented-limited regression models with an application to
the study of the risk perceived using continuous scales. <em>JOAS</em>,
<em>48</em>(11), 1998–2021. (<a
href="https://doi.org/10.1080/02664763.2020.1783518">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Studies of risk perceived using continuous scales of [0,100] were recently introduced in psychometrics, which can be transformed to the unit interval, but the presence of zeros or ones are commonly observed. Motivated by this, we introduce a full inferential set of tools that allows for augmented and limited data modeling. We considered parameter estimation, residual analysis, influence diagnostic and model selection for zero-and/or-one augmented beta rectangular (ZOABR) regression models and their particular nested models, which is based on a new parameterization of the beta rectangular distribution. Different from other alternatives, we performed maximum-likelihood estimation using a combination of the EM algorithm (for the continuous part) and Fisher scoring algorithm (for the discrete part). Also, we perform an additional step, by considering other link functions, besides the usual logistic link, for modeling the response mean. By considering randomized quantile residuals, (local) influence diagnostics and model selection tools, we identified that the ZOABR regression model is the best one. We also conducted extensive simulations studies, which indicate that all developed tools work properly. Finally, we discuss the use of this type of models to treat psychometric data. It is worthwhile to mention that applications of the developed methods go beyond to Psychometric data. Indeed, they can be useful when the response variable in bounded, including or not the respective limits.},
  archive      = {J_JOAS},
  author       = {Ana R. S. Silva and Caio L. N. Azevedo and Jorge L. Bazán and Juvêncio S. Nobre},
  doi          = {10.1080/02664763.2020.1783518},
  journal      = {Journal of Applied Statistics},
  number       = {11},
  pages        = {1998-2021},
  shortjournal = {J. Appl. Stat.},
  title        = {Augmented-limited regression models with an application to the study of the risk perceived using continuous scales},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A dependent counting INAR model with serially dependent
innovation. <em>JOAS</em>, <em>48</em>(11), 1975–1997. (<a
href="https://doi.org/10.1080/02664763.2020.1783521">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To provide a more flexible model of count data, we extend the first-order integer-valued autoregressive model with serially dependent innovations based on the dependent thinning operator. This model is appropriate for modelling the number of dependent random events affecting each other when the number of new cases depend on the previous count through a linear functional relationship. Several statistical properties of the model are determined, parameters are estimated by some methods and their properties are studied via simulations. This study was carried out to investigate the efficiency of the new model by two real count data sets, the number of contagious diseases and robbery.},
  archive      = {J_JOAS},
  author       = {Masoumeh Shirozhan and Mehrnaz Mohammadpour},
  doi          = {10.1080/02664763.2020.1783521},
  journal      = {Journal of Applied Statistics},
  number       = {11},
  pages        = {1975-1997},
  shortjournal = {J. Appl. Stat.},
  title        = {A dependent counting INAR model with serially dependent innovation},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Exponentiated odd chen-g family of distributions:
Statistical properties, bayesian and non-bayesian estimation with
applications. <em>JOAS</em>, <em>48</em>(11), 1948–1974. (<a
href="https://doi.org/10.1080/02664763.2020.1783520">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a new flexible generator of distributions is proposed. Some of its fundamental properties including quantile, skewness, kurtosis, hazard rate function, moments, mean deviations, mean time to failure, mean time between failure, availability and reliability function of consecutive linear and circular systems are studied. The hazard rate function can be increasing, decreasing, unimodal-bathtub, unimodal, bathtub, J and inverse J-shaped depending on its parameters values. After introducing the general class, two special models of the new family are discussed in detail. Maximum likelihood and Bayesian methods are used to estimate the model parameters. A detailed simulation study is carried out to examine the bias and mean square error of maximum likelihood and Bayesian estimators. We also illustrate the importance of the new family by means of two distinctive real data sets. It can serve as an alternative model to other lifetime distributions in the existing statistical literature for modeling positive and negative real data in many areas.},
  archive      = {J_JOAS},
  author       = {M. S. Eliwa and M. El-Morshedy and Sajid Ali},
  doi          = {10.1080/02664763.2020.1783520},
  journal      = {Journal of Applied Statistics},
  number       = {11},
  pages        = {1948-1974},
  shortjournal = {J. Appl. Stat.},
  title        = {Exponentiated odd chen-G family of distributions: Statistical properties, bayesian and non-bayesian estimation with applications},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Variance estimation based on blocked 3×2 cross-validation in
high-dimensional linear regression. <em>JOAS</em>, <em>48</em>(11),
1934–1947. (<a
href="https://doi.org/10.1080/02664763.2020.1780571">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In high-dimensional linear regression, the dimension of variables is always greater than the sample size. In this situation, the traditional variance estimation technique based on ordinary least squares constantly exhibits a high bias even under sparsity assumption. One of the major reasons is the high spurious correlation between unobserved realized noise and several predictors. To alleviate this problem, a refitted cross-validation (RCV) method has been proposed in the literature. However, for a complicated model, the RCV exhibits a lower probability that the selected model includes the true model in case of finite samples. This phenomenon may easily result in a large bias of variance estimation. Thus, a model selection method based on the ranks of the frequency of occurrences in six votes from a blocked 3×2 cross-validation is proposed in this study. The proposed method has a considerably larger probability of including the true model in practice than the RCV method. The variance estimation obtained using the model selected by the proposed method also shows a lower bias and a smaller variance. Furthermore, theoretical analysis proves the asymptotic normality property of the proposed variance estimation.},
  archive      = {J_JOAS},
  author       = {Xingli Yang and Yu Wang and Wennan Yan and Jihong Li},
  doi          = {10.1080/02664763.2020.1780571},
  journal      = {Journal of Applied Statistics},
  number       = {11},
  pages        = {1934-1947},
  shortjournal = {J. Appl. Stat.},
  title        = {Variance estimation based on blocked 3×2 cross-validation in high-dimensional linear regression},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Linear discriminant analysis for multiple functional data
analysis. <em>JOAS</em>, <em>48</em>(11), 1917–1933. (<a
href="https://doi.org/10.1080/02664763.2020.1780569">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In multivariate data analysis, Fisher linear discriminant analysis is useful to optimally separate two classes of observations by finding a linear combination of p variables. Functional data analysis deals with the analysis of continuous functions and thus can be seen as a generalisation of multivariate analysis where the dimension of the analysis space p strives to infinity. Several authors propose methods to perform discriminant analysis in this infinite dimensional space. Here, the methodology is introduced to perform discriminant analysis, not on single infinite dimensional functions, but to find a linear combination of p infinite dimensional continuous functions, providing a set of continuous canonical functions which are optimally separated in the canonical space.},
  archive      = {J_JOAS},
  author       = {Sugnet Gardner-Lubbe},
  doi          = {10.1080/02664763.2020.1780569},
  journal      = {Journal of Applied Statistics},
  number       = {11},
  pages        = {1917-1933},
  shortjournal = {J. Appl. Stat.},
  title        = {Linear discriminant analysis for multiple functional data analysis},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Birnbaum–saunders sample selection model. <em>JOAS</em>,
<em>48</em>(11), 1896–1916. (<a
href="https://doi.org/10.1080/02664763.2020.1780570">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The sample selection bias problem occurs when the outcome of interest is only observed according to some selection rule, where there is a dependence structure between the outcome and the selection rule. In a pioneering work, J. Heckman proposed a sample selection model based on a bivariate normal distribution for dealing with this problem. Due to the non-robustness of the normal distribution, many alternatives have been introduced in the literature by assuming extensions of the normal distribution like the Student-t and skew-normal models. One common limitation of the existent sample selection models is that they require a transformation of the outcome of interest, which is common R + -valued, such as income and wage. With this, data are analyzed on a non-original scale which complicates the interpretation of the parameters. In this paper, we propose a sample selection model based on the bivariate Birnbaum–Saunders distribution, which has the same number of parameters that the classical Heckman model. Further, our associated outcome equation is R + -valued. We discuss estimation by maximum likelihood and present some Monte Carlo simulation studies. An empirical application to the ambulatory expenditures data from the 2001 Medical Expenditure Panel Survey is presented.},
  archive      = {J_JOAS},
  author       = {Fernando de Souza Bastos and Wagner Barreto-Souza},
  doi          = {10.1080/02664763.2020.1780570},
  journal      = {Journal of Applied Statistics},
  number       = {11},
  pages        = {1896-1916},
  shortjournal = {J. Appl. Stat.},
  title        = {Birnbaum–Saunders sample selection model},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A clipped gaussian geo-classification model for poverty
mapping. <em>JOAS</em>, <em>48</em>(10), 1882–1895. (<a
href="https://doi.org/10.1080/02664763.2020.1779191">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The importance of discrete spatial models cannot be overemphasized, especially when measuring living standards. The battery of measurements is generally categorical with nearer geo-referenced observations featuring stronger dependencies. This study presents a Clipped Gaussian Geo-Classification (CGG-C) model for spatially-dependent ordered data, and compares its performance with existing methods to classify household poverty using Ghana living standards survey (GLSS 6) data. Bayesian inference was performed on data sampled by MCMC. Model evaluation was based on measures of classification and prediction accuracy. Spatial associations, given some household features, were quantified, and a poverty classification map for Ghana was developed. Overall, the results of estimation showed that many of the statistically significant covariates were generally strongly related with the ordered response variable. Households at specific locations tended to uniformly experience specific levels of poverty, thus, providing an empirical spatial character of poverty in Ghana. A comparative analysis of validation results showed that the CGG-C model (with 14.2\% misclassification rate) outperformed the Cumulative Probit (CP) model with misclassification rate of 17.4\%. This approach to poverty analysis is relevant for policy design and the implementation of cost-effective programmes to reduce category and site-specific poverty incidence, and monitor changes in both category and geographical trends thereof.},
  archive      = {J_JOAS},
  author       = {Richard Puurbalanta},
  doi          = {10.1080/02664763.2020.1779191},
  journal      = {Journal of Applied Statistics},
  number       = {10},
  pages        = {1882-1895},
  shortjournal = {J. Appl. Stat.},
  title        = {A clipped gaussian geo-classification model for poverty mapping},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Methods of assessing categorical agreement between
correlated screening tests in clinical studies. <em>JOAS</em>,
<em>48</em>(10), 1861–1881. (<a
href="https://doi.org/10.1080/02664763.2020.1777394">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advances in breast imaging and other screening tests have prompted studies to evaluate and compare the consistency between experts&#39; ratings of existing with new screening tests. In clinical settings, medical experts make subjective assessments of screening test results such as mammograms. Consistency between experts&#39; ratings is evaluated by measures of inter-rater agreement or association. However, conventional measures, such as Cohen&#39;s and Fleiss&#39; kappas, are unable to be applied or may perform poorly when studies consist of many experts, unbalanced data, or dependencies between experts&#39; ratings exist. Here we assess the performance of existing approaches including recently developed summary measures for assessing the agreement between experts&#39; binary and ordinal ratings when patients undergo two screening procedures. Methods to assess consistency between repeated measurements by the same experts are also described. We present applications to three large-scale clinical screening studies. Properties of these agreement measures are illustrated via simulation studies. Generally, a model-based approach provides several advantages over alternative methods including the ability to flexibly incorporate various measurement scales (i.e. binary or ordinal), large numbers of experts and patients, sparse data, and robustness to prevalence of underlying disease.},
  archive      = {J_JOAS},
  author       = {Thomas J. Zhou and Sughra Raza and Kerrie P. Nelson},
  doi          = {10.1080/02664763.2020.1777394},
  journal      = {Journal of Applied Statistics},
  number       = {10},
  pages        = {1861-1881},
  shortjournal = {J. Appl. Stat.},
  title        = {Methods of assessing categorical agreement between correlated screening tests in clinical studies},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Quantifying the similarity of 2D images using edge pixels:
An application to the forensic comparison of footwear impressions.
<em>JOAS</em>, <em>48</em>(10), 1833–1860. (<a
href="https://doi.org/10.1080/02664763.2020.1779194">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel method to quantify the similarity between an impression ( Q ) from an unknown source and a test impression ( K ) from a known source. Using the property of geometrical congruence in the impressions, the degree of correspondence is quantified using ideas from graph theory and maximum clique (MC). The algorithm uses the x and y coordinates of the edges in the images as the data. We focus on local areas in Q and the corresponding regions in K and extract features for comparison. Using pairs of images with known origin, we train a random forest to classify pairs into mates and non-mates. We collected impressions from 60 pairs of shoes of the same brand and model, worn over six months. Using a different set of very similar shoes, we evaluated the performance of the algorithm in terms of the accuracy with which it correctly classified images into source classes. Using classification error rates and ROC curves, we compare the proposed method to other algorithms in the literature and show that for these data, our method shows good classification performance relative to other methods. The algorithm can be implemented with the R package shoeprintr .},
  archive      = {J_JOAS},
  author       = {Soyoung Park and Alicia Carriquiry},
  doi          = {10.1080/02664763.2020.1779194},
  journal      = {Journal of Applied Statistics},
  number       = {10},
  pages        = {1833-1860},
  shortjournal = {J. Appl. Stat.},
  title        = {Quantifying the similarity of 2D images using edge pixels: An application to the forensic comparison of footwear impressions},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Extension of biplot methodology to multivariate regression
analysis. <em>JOAS</em>, <em>48</em>(10), 1816–1832. (<a
href="https://doi.org/10.1080/02664763.2020.1779192">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {At the core of multivariate statistics is the investigation of relationships between different sets of variables. More precisely, the inter-variable relationships and the causal relationships. The latter is a regression problem, where one set of variables is referred to as the response variables and the other set of variables as the predictor variables. In this situation, the effect of the predictors on the response variables is revealed through the regression coefficients. Results from the resulting regression analysis can be viewed graphically using the biplot. The consequential biplot provides a single graphical representation of the samples together with the predictor variables and response variables. In addition, their effect in terms of the regression coefficients can be visualized, although sub-optimally, in the said biplot.},
  archive      = {J_JOAS},
  author       = {Opeoluwa F. Oyedele},
  doi          = {10.1080/02664763.2020.1779192},
  journal      = {Journal of Applied Statistics},
  number       = {10},
  pages        = {1816-1832},
  shortjournal = {J. Appl. Stat.},
  title        = {Extension of biplot methodology to multivariate regression analysis},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Estimation of common location parameter of several
heterogeneous exponential populations based on generalized order
statistics. <em>JOAS</em>, <em>48</em>(10), 1798–1815. (<a
href="https://doi.org/10.1080/02664763.2020.1777395">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, several independent populations following exponential distribution with common location parameter and unknown and unequal scale parameters are considered. From these populations, several independent samples of generalized order statistics ( gos ) are drawn. Under the setup of gos , the problem of estimation of common location parameter is discussed and various estimators of common location parameter are derived. The authors obtained maximum likelihood estimator (MLE), modified MLE and uniformly minimum variance unbiased estimator of common location parameter. Furthermore, under scaled-squared error loss function, a general inadmissibility result of invariant estimator is proposed. The derived results are further reduced for upper record values which is a special case of gos . Finally, simulation study and real life example are reported to show the performances of various competing estimators in terms of percentage risk improvement.},
  archive      = {J_JOAS},
  author       = {Qazi J. Azhad and Mohd. Arshad and Amit Kumar Misra},
  doi          = {10.1080/02664763.2020.1777395},
  journal      = {Journal of Applied Statistics},
  number       = {10},
  pages        = {1798-1815},
  shortjournal = {J. Appl. Stat.},
  title        = {Estimation of common location parameter of several heterogeneous exponential populations based on generalized order statistics},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Model-based estimation of baseball batting metrics.
<em>JOAS</em>, <em>48</em>(10), 1775–1797. (<a
href="https://doi.org/10.1080/02664763.2020.1775792">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce an approach to model the batting outcomes of baseball batters based on the weighted likelihood approach and make use of our methodology to estimate commonly used baseball batting metrics. The weighted likelihood allows the sharing of relevant information among players. Specifically, this allows the inference on each batter to make use of the batting data from all other players in the league and, in the process, allows for improved inference. MAMSE (Minimum Averaged Mean Squared Error) weights are used as the likelihood weights. For comparison, we implemented a semi-parametric Bayesian approach based on the Dirichlet process, which enables the borrowing of information across batters while providing a natural clustering mechanism. We demonstrate and compare these approaches using 2018 Major League Baseball (MLB) batters data.},
  archive      = {J_JOAS},
  author       = {Lahiru Wickramasinghe and Alexandre Leblanc and Saman Muthukumarana},
  doi          = {10.1080/02664763.2020.1775792},
  journal      = {Journal of Applied Statistics},
  number       = {10},
  pages        = {1775-1797},
  shortjournal = {J. Appl. Stat.},
  title        = {Model-based estimation of baseball batting metrics},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Model-free slice screening for ultrahigh-dimensional
survival data. <em>JOAS</em>, <em>48</em>(10), 1755–1774. (<a
href="https://doi.org/10.1080/02664763.2020.1772734">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For ultrahigh-dimensional data, independent feature screening has been demonstrated both theoretically and empirically to be an effective dimension reduction method with low computational demanding. Motivated by the Buckley–James method to accommodate censoring, we propose a fused Kolmogorov–Smirnov filter to screen out the irrelevant dependent variables for ultrahigh-dimensional survival data. The proposed model-free screening method can work with many types of covariates (e.g. continuous, discrete and categorical variables) and is shown to enjoy the sure independent screening property under mild regularity conditions without requiring any moment conditions on covariates. In particular, the proposed procedure can still be powerful when covariates are strongly dependent on each other. We further develop an iterative algorithm to enhance the performance of our method while dealing with the practical situations where some covariates may be marginally unrelated but jointly related to the response. We conduct extensive simulations to evaluate the finite-sample performance of the proposed method, showing that it has favourable exhibition over the existing typical methods. As an illustration, we apply the proposed method to the diffuse large-B-cell lymphoma study.},
  archive      = {J_JOAS},
  author       = {Jing Zhang and Yanyan Liu},
  doi          = {10.1080/02664763.2020.1772734},
  journal      = {Journal of Applied Statistics},
  number       = {10},
  pages        = {1755-1774},
  shortjournal = {J. Appl. Stat.},
  title        = {Model-free slice screening for ultrahigh-dimensional survival data},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bayesian modeling of factorial time-course data with
applications to a bone aging gene expression study. <em>JOAS</em>,
<em>48</em>(10), 1730–1754. (<a
href="https://doi.org/10.1080/02664763.2020.1772733">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many scientific studies, especially in the biomedical sciences, generate data measured simultaneously over a multitude of units, over a period of time, and under different conditions or combinations of factors. Often, an important question of interest asked relates to which units behave similarly under different conditions, but measuring the variation over time complicates the analysis significantly. In this article we address such a problem arising from a gene expression study relating to bone aging, and develop a Bayesian statistical method that can simultaneously detect and uncover signals on three levels within such data: factorial, longitudinal, and transcriptional. Our model framework considers both cluster and time-point-specific parameters and these parameters uniquely determine the shapes of the temporal gene expression profiles, allowing the discovery and characterization of latent gene clusters based on similar underlying biological mechanisms. Our methodology was successfully applied to discover transcriptional networks in a microarray data set comparing the transcriptomic changes that occurred during bone aging in male and female mice expressing one or both copies of the bromodomain (Brd2) gene, a transcriptional regulator which exhibits an age-dependent sex-linked bone loss phenotype.},
  archive      = {J_JOAS},
  author       = {Joseph Wu and Mayetri Gupta and Amira I. Hussein and Louis Gerstenfeld},
  doi          = {10.1080/02664763.2020.1772733},
  journal      = {Journal of Applied Statistics},
  number       = {10},
  pages        = {1730-1754},
  shortjournal = {J. Appl. Stat.},
  title        = {Bayesian modeling of factorial time-course data with applications to a bone aging gene expression study},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Wavelet threshold based on stein’s unbiased risk estimators
of restricted location parameter in multivariate normal. <em>JOAS</em>,
<em>48</em>(10), 1712–1729. (<a
href="https://doi.org/10.1080/02664763.2020.1772209">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, the problem of estimating the mean vector under non-negative constraints on location vector of the multivariate normal distribution is investigated. The value of the wavelet threshold based on Stein&#39;s unbiased risk estimators is calculated for the shrinkage estimator in restricted parameter space. We suppose that covariance matrix is unknown and we find the dominant class of shrinkage estimators under Balance loss function. The performance evaluation of the proposed class of estimators is checked through a simulation study by using risk and average mean square error values.},
  archive      = {J_JOAS},
  author       = {H. Karamikabir and M. Afshari and F. Lak},
  doi          = {10.1080/02664763.2020.1772209},
  journal      = {Journal of Applied Statistics},
  number       = {10},
  pages        = {1712-1729},
  shortjournal = {J. Appl. Stat.},
  title        = {Wavelet threshold based on stein&#39;s unbiased risk estimators of restricted location parameter in multivariate normal},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Discovering associations between players’ performance
indicators and matches’ results in the european soccer leagues.
<em>JOAS</em>, <em>48</em>(9), 1696–1711. (<a
href="https://doi.org/10.1080/02664763.2020.1772210">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The application of data mining techniques and statistical analysis to the sports field has received increasing attention in the last decade. One of the most famous sports in the world is soccer, and the present work deals with it, using data from the 2009/2010 season to the 2015/2016 season from nine European leagues extracted from the Kaggle European Soccer database. Overall performance indicators of the four roles in a soccer team (forward, midfielder, defender and goalkeeper) for home and away teams are used to investigate the relationships between them and the results of matches, and to predict the wins of the home team. The model used to answer both these demands is the Bayesian Network. This study shows that this model can be very useful for mining the relations between players&#39; performance indicators and for improving knowledge of the game strategies applied by coaches in different leagues. Moreover, it is shown that the ability to predict match results of the proposed Bayesian Network is roughly the same as that of the Naive Bayes model.},
  archive      = {J_JOAS},
  author       = {Maurizio Carpita and Silvia Golia},
  doi          = {10.1080/02664763.2020.1772210},
  journal      = {Journal of Applied Statistics},
  number       = {9},
  pages        = {1696-1711},
  shortjournal = {J. Appl. Stat.},
  title        = {Discovering associations between players&#39; performance indicators and matches&#39; results in the european soccer leagues},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Exploratory data structure comparisons: Three new visual
tools based on principal component analysis. <em>JOAS</em>,
<em>48</em>(9), 1675–1695. (<a
href="https://doi.org/10.1080/02664763.2020.1773772">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Datasets are sometimes divided into distinct subsets, e.g. due to multi-center sampling, or to variations in instruments, questionnaire item ordering or mode of administration, and the data analyst then needs to assess whether a joint analysis is meaningful. The Principal Component Analysis-based Data Structure Comparisons (PCADSC) tools are three new non-parametric, visual diagnostic tools for investigating differences in structure for two subsets of a dataset through covariance matrix comparisons by use of principal component analysis. The PCADCS tools are demonstrated in a data example using European Social Survey data on psychological well-being in three countries, Denmark, Sweden, and Bulgaria. The data structures are found to be different in Denmark and Bulgaria, and thus a comparison of for example mean psychological well-being scores is not meaningful. However, when comparing Denmark and Sweden, very similar data structures, and thus comparable concepts of well-being, are found. Therefore, inter-country comparisons are warranted for these countries.},
  archive      = {J_JOAS},
  author       = {Anne Helby Petersen and Bo Markussen and Karl Bang Christensen},
  doi          = {10.1080/02664763.2020.1773772},
  journal      = {Journal of Applied Statistics},
  number       = {9},
  pages        = {1675-1695},
  shortjournal = {J. Appl. Stat.},
  title        = {Exploratory data structure comparisons: Three new visual tools based on principal component analysis},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An elastic net penalized small area model combining unit-
and area-level data for regional hypertension prevalence estimation.
<em>JOAS</em>, <em>48</em>(9), 1659–1674. (<a
href="https://doi.org/10.1080/02664763.2020.1765323">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hypertension is a highly prevalent cardiovascular disease. It marks a considerable cost factor to many national health systems. Despite its prevalence, regional disease distributions are often unknown and must be estimated from survey data. However, health surveys frequently lack in regional observations due to limited resources. Obtained prevalence estimates suffer from unacceptably large sampling variances and are not reliable. Small area estimation solves this problem by linking auxiliary data from multiple regions in suitable regression models. Typically, either unit- or area-level observations are considered for this purpose. But with respect to hypertension, both levels should be used. Hypertension has characteristic comorbidities and is strongly related to lifestyle features, which are unit-level information. It is also correlated with socioeconomic indicators that are usually measured on the area-level. But the level combination is challenging as it requires multi-level model parameter estimation from small samples. We use a multi-level small area model with level-specific penalization to overcome this issue. Model parameter estimation is performed via stochastic coordinate gradient descent. A jackknife estimator of the mean squared error is presented. The methodology is applied to combine health survey data and administrative records to estimate regional hypertension prevalence in Germany.},
  archive      = {J_JOAS},
  author       = {J. P. Burgard and J. Krause and R. Münnich},
  doi          = {10.1080/02664763.2020.1765323},
  journal      = {Journal of Applied Statistics},
  number       = {9},
  pages        = {1659-1674},
  shortjournal = {J. Appl. Stat.},
  title        = {An elastic net penalized small area model combining unit- and area-level data for regional hypertension prevalence estimation},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Variable selection and importance in presence of high
collinearity: An application to the prediction of lean body mass from
multi-frequency bioelectrical impedance. <em>JOAS</em>, <em>48</em>(9),
1644–1658. (<a
href="https://doi.org/10.1080/02664763.2020.1763930">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In prediction problems both response and covariates may have high correlation with a second group of influential regressors, that can be considered as background variables. An important challenge is to perform variable selection and importance assessment among the covariates in the presence of these variables. A clinical example is the prediction of the lean body mass (response) from bioimpedance (covariates), where anthropometric measures play the role of background variables. We introduce a reduced dataset in which the variables are defined as the residuals with respect to the background, and perform variable selection and importance assessment both in linear and random forest models. Using a clinical dataset of multi-frequency bioimpedance, we show the effectiveness of this method to select the most relevant predictors of the lean body mass beyond anthropometry.},
  archive      = {J_JOAS},
  author       = {Camillo Cammarota and Alessandro Pinto},
  doi          = {10.1080/02664763.2020.1763930},
  journal      = {Journal of Applied Statistics},
  number       = {9},
  pages        = {1644-1658},
  shortjournal = {J. Appl. Stat.},
  title        = {Variable selection and importance in presence of high collinearity: An application to the prediction of lean body mass from multi-frequency bioelectrical impedance},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An optimal control chart for finite matrix sequences at some
unknown change point. <em>JOAS</em>, <em>48</em>(9), 1628–1643. (<a
href="https://doi.org/10.1080/02664763.2020.1772208">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a new measure for evaluating the performance of control charts to detect abrupt changes of finite matrix sequences. The objective is to minimize the probability that the control chart fails to raise the alarm at unknown change point time for a given in-control average run length. We construct and prove the optimal control chart with dynamic control limits in different pre- and post-change distributions. We validate the optimality of the proposed chart by conducting exhaustive experiments on both simulation study and real-world data.},
  archive      = {J_JOAS},
  author       = {Yunfei Ye and Dong Han},
  doi          = {10.1080/02664763.2020.1772208},
  journal      = {Journal of Applied Statistics},
  number       = {9},
  pages        = {1628-1643},
  shortjournal = {J. Appl. Stat.},
  title        = {An optimal control chart for finite matrix sequences at some unknown change point},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Simultaneous statistical modelling of excess zeros,
over/underdispersion, and multimodality with applications in hotel
industry. <em>JOAS</em>, <em>48</em>(9), 1603–1627. (<a
href="https://doi.org/10.1080/02664763.2020.1769577">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose zero-inflated statistical models based on the generalized Hermite distribution for simultaneously modelling of excess zeros, over/underdispersion, and multimodality. These new models are parsimonious yet remarkably flexible allowing the covariates to be introduced directly through the mean, dispersion, and zero-inflated parameters. To accommodate the interval inequality constraint for the dispersion parameter, we present a new link function for the covariate-dependent dispersion regression model. We derive score tests for zero inflation in both covariate-free and covariate-dependent models. Both the score test and the likelihood-ratio test are conducted to examine the validity of zero inflation. The score test provides a useful tool when computing the likelihood-ratio statistic proves to be difficult. We analyse several hotel booking cancellation datasets extracted from two recently published real datasets from a resort hotel and a city hotel. These extracted cancellation datasets reveal complex features of excess zeros, over/underdispersion, and multimodality simultaneously making them difficult to analyse with existing approaches. The application of the proposed methods to the cancellation datasets illustrates the usefulness and flexibility of the models.},
  archive      = {J_JOAS},
  author       = {Kai-Sheng Song},
  doi          = {10.1080/02664763.2020.1769577},
  journal      = {Journal of Applied Statistics},
  number       = {9},
  pages        = {1603-1627},
  shortjournal = {J. Appl. Stat.},
  title        = {Simultaneous statistical modelling of excess zeros, over/underdispersion, and multimodality with applications in hotel industry},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Confidence limits for conformance proportions in normal
mixture models. <em>JOAS</em>, <em>48</em>(9), 1579–1602. (<a
href="https://doi.org/10.1080/02664763.2020.1769578">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conformance proportions are important numerical indices for quality assessments. When the population is characterized by a normal mixture model, estimating conformance proportions can be a practical issue. To account for the inherent structure of normal mixture models, universal and individual conformance proportions are first defined for the purpose of evaluating the overall population and specific subpopulations of interest, respectively. On the basis of generalized fiducial quantities, a systematic method is then proposed in this paper to obtain confidence limits for the two classes of conformance proportions. The simulation results demonstrate that the proposed method can maintain the empirical coverage rate sufficiently close to the nominal level. In addition, two examples are given to illustrate the proposed method.},
  archive      = {J_JOAS},
  author       = {Shin-Fu Tsai and Tse-Le Huang},
  doi          = {10.1080/02664763.2020.1769578},
  journal      = {Journal of Applied Statistics},
  number       = {9},
  pages        = {1579-1602},
  shortjournal = {J. Appl. Stat.},
  title        = {Confidence limits for conformance proportions in normal mixture models},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A new kind of stochastic restricted biased estimator for
logistic regression model. <em>JOAS</em>, <em>48</em>(9), 1559–1578. (<a
href="https://doi.org/10.1080/02664763.2020.1769576">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the logistic regression model, the variance of the maximum likelihood estimator is inflated and unstable when the multicollinearity exists in the data. There are several methods available in literature to overcome this problem. We propose a new stochastic restricted biased estimator. We study the statistical properties of the proposed estimator and compare its performance with some existing estimators in the sense of scalar mean squared criterion. An example and a simulation study are provided to illustrate the performance of the proposed estimator.},
  archive      = {J_JOAS},
  author       = {M. I. Alheety and Kristofer Månsson and B. M. Golam Kibria},
  doi          = {10.1080/02664763.2020.1769576},
  journal      = {Journal of Applied Statistics},
  number       = {9},
  pages        = {1559-1578},
  shortjournal = {J. Appl. Stat.},
  title        = {A new kind of stochastic restricted biased estimator for logistic regression model},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Estimation of the probability content in a specified
interval using fiducial approach. <em>JOAS</em>, <em>48</em>(9),
1541–1558. (<a
href="https://doi.org/10.1080/02664763.2020.1768228">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Statistical methods for constructing confidence intervals for the probability content in a specified interval are proposed. Exact and approximate solutions based on the fiducial approach are described when the measurements on the variable of interest can be modelled by a location-scale (or log-location-scale) distribution. Methods are described for the normal, Weibull, two-parameter exponential and two-parameter Rayleigh distributions. For each case, the solutions are evaluated for their merits. Three examples, where it is desired to estimate the percentages of engineering products meet the specification limits, are provided to illustrate the methods.},
  archive      = {J_JOAS},
  author       = {Ngan Hoang-Nguyen-Thuy and K. Krishnamoorthy},
  doi          = {10.1080/02664763.2020.1768228},
  journal      = {Journal of Applied Statistics},
  number       = {9},
  pages        = {1541-1558},
  shortjournal = {J. Appl. Stat.},
  title        = {Estimation of the probability content in a specified interval using fiducial approach},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Optimal allocation of subjects in a matched pair
cluster-randomized trial with fixed number of heterogeneous clusters.
<em>JOAS</em>, <em>48</em>(9), 1527–1540. (<a
href="https://doi.org/10.1080/02664763.2020.1779195">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In cluster-randomized trials, investigators randomize clusters of individuals such as households, medical practices, schools or classrooms despite the unit of interest are the individuals. It results in the loss of efficiency in terms of the estimation of the unknown parameters as well as the power of the test for testing the treatment effects. To recoup this efficiency loss, some studies pair similar clusters and randomize treatment within pairs. However, the clusters within a treatment arm might be heterogeneous in nature. In this article, we propose a locally optimal design that accounts the clusters heterogeneity and optimally allocates the subjects within each cluster. To address the dependency of design on the unknown parameters, we also discuss Bayesian optimal designs. Performances of proposed designs are investigated numerically through some data examples.},
  archive      = {J_JOAS},
  author       = {Satya Prakash Singh and Pradeep Yadav},
  doi          = {10.1080/02664763.2020.1779195},
  journal      = {Journal of Applied Statistics},
  number       = {9},
  pages        = {1527-1540},
  shortjournal = {J. Appl. Stat.},
  title        = {Optimal allocation of subjects in a matched pair cluster-randomized trial with fixed number of heterogeneous clusters},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Time varying mixed effects model with fused lasso
regularization. <em>JOAS</em>, <em>48</em>(8), 1513–1526. (<a
href="https://doi.org/10.1080/02664763.2020.1791805">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The associations between covariates and the outcomes often vary over time, regardless of whether the covariate is time-varying or time-invariant. For example, we hypothesize that the impact of chronic diseases, such as diabetes and heart disease, on people’s physical functions differ with aging. However, the age-varying effect would be missed if one models the covariate simply as a time-invariant covariate (yes/no) with a time-constant coefficient. We propose a fused lasso-based time-varying linear mixed effect (FTLME) model and an efficient two-stage parameter estimation algorithm to estimate the longitudinal trajectories of fixed-effect coefficients. Simulation studies are presented to demonstrate the efficacy of the method and its computational efficiency in estimating smooth time-varying effects in high dimensional settings. A real data example on the Health and Retirement Study (HRS) analysis is used to demonstrate the practical usage of our method to infer age-varying impact of chronic disease on older people’s physical functions.},
  archive      = {J_JOAS},
  author       = {Jaehong Yu and Hua Zhong},
  doi          = {10.1080/02664763.2020.1791805},
  journal      = {Journal of Applied Statistics},
  number       = {8},
  pages        = {1513-1526},
  shortjournal = {J. Appl. Stat.},
  title        = {Time varying mixed effects model with fused lasso regularization},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Maximum likelihood estimation for the proportional odds
model with mixed interval-censored failure time data. <em>JOAS</em>,
<em>48</em>(8), 1496–1512. (<a
href="https://doi.org/10.1080/02664763.2020.1789077">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article discusses regression analysis of mixed interval-censored failure time data. Such data frequently occur across a variety of settings, including clinical trials, epidemiologic investigations, and many other biomedical studies with a follow-up component. For example, mixed failure times are commonly found in the two largest studies of long-term survivorship after childhood cancer, the datasets that motivated this work. However, most existing methods for failure time data consider only right-censored or only interval-censored failure times, not the more general case where times may be mixed. Additionally, among regression models developed for mixed interval-censored failure times, the proportional hazards formulation is generally assumed. It is well-known that the proportional hazards model may be inappropriate in certain situations, and alternatives are needed to analyze mixed failure time data in such cases. To fill this need, we develop a maximum likelihood estimation procedure for the proportional odds regression model with mixed interval-censored data. We show that the resulting estimators are consistent and asymptotically Gaussian. An extensive simulation study is performed to assess the finite-sample properties of the method, and this investigation indicates that the proposed method works well for many practical situations. We then apply our approach to examine the impact of age at cranial radiation therapy on risk of growth hormone deficiency in long-term survivors of childhood cancer.},
  archive      = {J_JOAS},
  author       = {Liang Zhu and Xingwei Tong and Dingjiao Cai and Yimei Li and Ryan Sun and Deo K. Srivastava and Melissa M. Hudson},
  doi          = {10.1080/02664763.2020.1789077},
  journal      = {Journal of Applied Statistics},
  number       = {8},
  pages        = {1496-1512},
  shortjournal = {J. Appl. Stat.},
  title        = {Maximum likelihood estimation for the proportional odds model with mixed interval-censored failure time data},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Designs for order-of-addition experiments. <em>JOAS</em>,
<em>48</em>(8), 1475–1495. (<a
href="https://doi.org/10.1080/02664763.2020.1801607">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The order-of-addition experiment aims at determining the optimal order of adding components such that the response of interest is optimized. Order of addition has been widely involved in many areas, including bio-chemistry, food science, nutritional science, pharmaceutical science, etc. However, such an important study is rather primitive in statistical literature. In this paper, a thorough study on pair-wise ordering designs for order of addition is provided. The recursive relation between two successive full pair-wise ordering designs is developed. Based on this recursive relation, the full pair-wise ordering design can be obtained without evaluating all the orders of components. The value of the D -efficiency for the full pair-wise ordering model is then derived. It provides a benchmark for choosing the fractional pair-wise ordering designs. To overcome the unaffordability of the full pair-wise ordering design, a new class of minimal-point pair-wise ordering designs is proposed. A job scheduling problem as well as simulation studies are conducted to illustrate the performance of the pair-wise ordering designs for determining the optimal orders. It is shown that the proposed designs are very efficient in determining the optimal order of addition.},
  archive      = {J_JOAS},
  author       = {Yuna Zhao and Dennis K. J. Lin and Min-Qian Liu},
  doi          = {10.1080/02664763.2020.1801607},
  journal      = {Journal of Applied Statistics},
  number       = {8},
  pages        = {1475-1495},
  shortjournal = {J. Appl. Stat.},
  title        = {Designs for order-of-addition experiments},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Determinants of the heavily right-tailed residential housing
price in tianjin. <em>JOAS</em>, <em>48</em>(8), 1457–1474. (<a
href="https://doi.org/10.1080/02664763.2020.1840534">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The housing price in Tianjin, one of the typical monocentric cities of China, exhibits a heavily right-tailed distribution even after the logarithm transformation of the price, which might lead to a biased estimation of the parameters under normal distribution assumption. Therefore, the extended Cox proportional hazards regression model and the generalized concept of relative risk are used to identify factors associated with the housing price. The analysis shows that the implementation dates of the macro regulation policies were related to the price changing trends. Qualities of public elementary and secondary schools were significantly associated with the housing price, and the associations between the structure and neighborhood characteristics and the housing price were influenced by the distance of the residential property to downtown Tianjin.},
  archive      = {J_JOAS},
  author       = {Bojuan Barbara Zhao and Ruijuan Su},
  doi          = {10.1080/02664763.2020.1840534},
  journal      = {Journal of Applied Statistics},
  number       = {8},
  pages        = {1457-1474},
  shortjournal = {J. Appl. Stat.},
  title        = {Determinants of the heavily right-tailed residential housing price in tianjin},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Simultaneous confidence interval construction for
many-to-one comparisons of proportion differences based on correlated
paired data. <em>JOAS</em>, <em>48</em>(8), 1442–1456. (<a
href="https://doi.org/10.1080/02664763.2020.1795815">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In some medical researches such as ophthalmological, orthopaedic and otolaryngologic studies, it is often of interest to compare multiple groups with a control using data collected from paired organs of patients. The major difficulty in performing the data analysis is to adjust the multiplicity between the comparison of multiple groups, and the correlation within the same patient&#39;s paired organs. In this article, we construct asymptotic simultaneous confidence intervals (SCIs) for many-to-one comparisons of proportion differences adjusting for multiplicity and the correlation. The coverage probabilities and widths of the proposed CIs are evaluated by Monte Carlo simulation studies. The methods are illustrated by a real data example.},
  archive      = {J_JOAS},
  author       = {Zhengyu Yang and Guo-Liang Tian and Xiaobin Liu and Chang-Xing Ma},
  doi          = {10.1080/02664763.2020.1795815},
  journal      = {Journal of Applied Statistics},
  number       = {8},
  pages        = {1442-1456},
  shortjournal = {J. Appl. Stat.},
  title        = {Simultaneous confidence interval construction for many-to-one comparisons of proportion differences based on correlated paired data},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bayesian analysis of the box-cox transformation model based
on left-truncated and right-censored data. <em>JOAS</em>,
<em>48</em>(8), 1429–1441. (<a
href="https://doi.org/10.1080/02664763.2020.1784854">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we discuss the inference problem about the Box-Cox transformation model when one faces left-truncated and right-censored data, which often occur in studies, for example, involving the cross-sectional sampling scheme. It is well-known that the Box-Cox transformation model includes many commonly used models as special cases such as the proportional hazards model and the additive hazards model. For inference, a Bayesian estimation approach is proposed and in the method, the piecewise function is used to approximate the baseline hazards function. Also the conditional marginal prior, whose marginal part is free of any constraints, is employed to deal with many computational challenges caused by the constraints on the parameters, and a MCMC sampling procedure is developed. A simulation study is conducted to assess the finite sample performance of the proposed method and indicates that it works well for practical situations. We apply the approach to a set of data arising from a retirement center.},
  archive      = {J_JOAS},
  author       = {Chunjie Wang and Jingjing Jiang and Linlin Luo and Shuying Wang},
  doi          = {10.1080/02664763.2020.1784854},
  journal      = {Journal of Applied Statistics},
  number       = {8},
  pages        = {1429-1441},
  shortjournal = {J. Appl. Stat.},
  title        = {Bayesian analysis of the box-cox transformation model based on left-truncated and right-censored data},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Estimation of the cumulative baseline hazard function for
dependently right-censored failure time data. <em>JOAS</em>,
<em>48</em>(8), 1416–1428. (<a
href="https://doi.org/10.1080/02664763.2020.1795818">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study the properties of a special class of frailty models when the frailty is common to several failure times. The models are closely linked to Archimedean copula models. We establish a useful formula for cumulative baseline hazard functions and develop a new estimator for cumulative baseline hazard functions in bivariate frailty regression models. Based on our proposed estimator, we present a graphical model checking procedure. We fit a leukemia data set using our model and end our paper with some discussions.},
  archive      = {J_JOAS},
  author       = {Antai Wang and Xieyang Jia and Zhezhen Jin},
  doi          = {10.1080/02664763.2020.1795818},
  journal      = {Journal of Applied Statistics},
  number       = {8},
  pages        = {1416-1428},
  shortjournal = {J. Appl. Stat.},
  title        = {Estimation of the cumulative baseline hazard function for dependently right-censored failure time data},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A generalized likelihood ratio test for monitoring profile
data. <em>JOAS</em>, <em>48</em>(8), 1402–1415. (<a
href="https://doi.org/10.1080/02664763.2021.1880555">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Profile data emerges when the quality of a product or process is characterized by a functional relationship among (input and output) variables. In this paper, we focus on the case where each profile has one response variable Y , one explanatory variable x , and the functional relationship between these two variables can be rather arbitrary. The basic concept can be applied to a much wider case, however. We propose a general method based on the Generalized Likelihood Ratio Test (GLRT) for monitoring of profile data. The proposed method uses nonparametric regression to estimate the on-line profiles and thus does not require any functional form for the profiles. Both Shewhart-type and EWMA-type control charts are considered. The average run length (ARL) performance of the proposed method is studied. It is shown that the proposed GLRT-based control chart can efficiently detect both location and dispersion shifts of the on-line profiles from the baseline profile. An upper control limit (UCL) corresponding to a desired in-control ARL value is constructed.},
  archive      = {J_JOAS},
  author       = {Yang Liu and JunJia Zhu and Dennis K. J. Lin},
  doi          = {10.1080/02664763.2021.1880555},
  journal      = {Journal of Applied Statistics},
  number       = {8},
  pages        = {1402-1415},
  shortjournal = {J. Appl. Stat.},
  title        = {A generalized likelihood ratio test for monitoring profile data},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Empirical evaluation of sub-cohort sampling designs for risk
prediction modeling. <em>JOAS</em>, <em>48</em>(8), 1374–1401. (<a
href="https://doi.org/10.1080/02664763.2020.1861225">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sub-cohort sampling designs, such as nested case-control (NCC) and case-cohort (CC) studies, have been widely used to estimate biomarker-disease associations because of their cost effectiveness. These designs have been well studied and shown to maintain relatively high efficiency compared to full-cohort designs, but their performance of building risk prediction models has been less studied. Moreover, sub-cohort sampling designs often use matching (or stratifying) to further control for confounders or to reduce measurement error. Their predictive performance depends on both the design and matching procedures. Based on a dataset from the NYU Women&#39;s Health Study (NYUWHS), we performed Monte Carlo simulations to systematically evaluate risk prediction performance under NCC, CC, and full-cohort studies. Our simulations demonstrate that sub-cohort sampling designs can have predictive accuracy (i.e. discrimination and calibration) similar to that of the full-cohort design, but could be sensitive to the matching procedure used. Our results suggest that researchers can have the option of performing NCC and CC studies with huge potential benefits in cost and resources, but need to pay particular attention to the matching procedure when developing a risk prediction model in biomarker studies.},
  archive      = {J_JOAS},
  author       = {Myeonggyun Lee and Anne Zeleniuch-Jacquotte and Mengling Liu},
  doi          = {10.1080/02664763.2020.1861225},
  journal      = {Journal of Applied Statistics},
  number       = {8},
  pages        = {1374-1401},
  shortjournal = {J. Appl. Stat.},
  title        = {Empirical evaluation of sub-cohort sampling designs for risk prediction modeling},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Network-based survival analysis to discover target genes for
developing cancer immunotherapies and predicting patient survival.
<em>JOAS</em>, <em>48</em>(8), 1352–1373. (<a
href="https://doi.org/10.1080/02664763.2020.1812543">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, cancer immunotherapies have been life-savers; however, only a fraction of treated patients have durable responses. Consequently, statistical methods that enable the discovery of target genes for developing new treatments and predicting the patient survival are of importance. This paper introduced a network-based survival analysis method and applied it to identify candidate genes as possible targets for developing new treatments. RNA-seq data from a mouse study was used to select differentially expressed genes, which were then translated to those in humans. We constructed a gene network and identified gene clusters using a training set of 310 human gliomas. Then we conducted gene set enrichment analysis to select the gene clusters with significant biological function. A penalized Cox model was built to identify a small set of candidate genes to predict survival. An independent set of 690 human glioma samples was used to evaluate a predictive accuracy of the survival model. The areas under time-dependent ROC curves in both the training and validation sets are more than 90\%, indicating a strong association between selected genes and patient survival. Consequently, potential biomedical interventions targeting these genes might be able to alter their expressions and prolong patient survival.},
  archive      = {J_JOAS},
  author       = {Xinwei He and Xiaoqiang Sun and Yongzhao Shao},
  doi          = {10.1080/02664763.2020.1812543},
  journal      = {Journal of Applied Statistics},
  number       = {8},
  pages        = {1352-1373},
  shortjournal = {J. Appl. Stat.},
  title        = {Network-based survival analysis to discover target genes for developing cancer immunotherapies and predicting patient survival},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Editorial to special issue frontiers of data analysis.
<em>JOAS</em>, <em>48</em>(8), 1349–1351. (<a
href="https://doi.org/10.1080/02664763.2021.1922853">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JOAS},
  author       = {Zhezhen Jin and Jianguo Sun},
  doi          = {10.1080/02664763.2021.1922853},
  journal      = {Journal of Applied Statistics},
  number       = {8},
  pages        = {1349-1351},
  shortjournal = {J. Appl. Stat.},
  title        = {Editorial to special issue frontiers of data analysis},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Correction. <em>JOAS</em>, <em>48</em>(7), i. (<a
href="https://doi.org/10.1080/02664763.2020.1807783">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JOAS},
  doi          = {10.1080/02664763.2020.1807783},
  journal      = {Journal of Applied Statistics},
  number       = {7},
  pages        = {i},
  shortjournal = {J. Appl. Stat.},
  title        = {Correction},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Predicting an optimal composite outcome variable for
huntington’s disease clinical trials. <em>JOAS</em>, <em>48</em>(7),
1339–1348. (<a
href="https://doi.org/10.1080/02664763.2020.1759034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While there is no known cure for Huntington&#39;s disease (HD), there are early-phase clinical trials aimed at altering disease progression patterns. There is, however, no obvious single outcome for these trials to evaluate treatment efficacy. Currently used outcomes are, while reasonable, not optimal in any sense. In this paper we derive a method for constructing a composite variable via a linear combination of clinical measures. Our composite variable optimizes the signal-to-noise ratio (SNR) within the context of a longitudinal study design. We also demonstrate how to induce sparsity using a soft-approximation of an L 1 penalty on the coefficients of the composite variable. We applied our method to data from the TRACK-HD study, a longitudinal study aimed at establishing good outcome measures for HD, and found that compared to the existing composite measurement our composite variable provides a larger SNR and allows clinical trials with smaller sample sizes to achieve equivalent power.},
  archive      = {J_JOAS},
  author       = {Daniel K. Sewell and Journey Penney and Melissa Jay and Ying Zhang and Jane S. Paulsen},
  doi          = {10.1080/02664763.2020.1759034},
  journal      = {Journal of Applied Statistics},
  number       = {7},
  pages        = {1339-1348},
  shortjournal = {J. Appl. Stat.},
  title        = {Predicting an optimal composite outcome variable for huntington&#39;s disease clinical trials},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Monitoring of zero-inflated binomial processes with a DEWMA
control chart. <em>JOAS</em>, <em>48</em>(7), 1319–1338. (<a
href="https://doi.org/10.1080/02664763.2020.1761950">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Control charts are widely used for monitoring quality characteristics of high-yield processes. In such processes where a large number of zero observations exists in count data, the zero-inflated binomial (ZIB) models are more appropriate than the ordinary binomial models. In ZIB models, random shocks occur with probability θ , and upon the occurrence of random shocks, the number of non-conforming items in a sample of size n follows the binomial distribution with proportion p . In the present article, we study in more detail the exponentially weighted moving average control chart based on ZIB distribution (ZIB-EWMA) and we also propose a new control chart based on the double exponentially weighted moving average statistic for monitoring ZIB data (ZIB-DEWMA). The two control charts are studied in detecting upward shifts in θ or p individually, as well as in both parameters simultaneously. Through a simulation study, we compare the performance of the proposed chart with the ZIB-Shewhart, ZIB-EWMA and ZIB-CUSUM charts. Finally, an illustrative example is also presented to display the practical application of the ZIB charts.},
  archive      = {J_JOAS},
  author       = {Vasileios Alevizakos and Christos Koukouvinos},
  doi          = {10.1080/02664763.2020.1761950},
  journal      = {Journal of Applied Statistics},
  number       = {7},
  pages        = {1319-1338},
  shortjournal = {J. Appl. Stat.},
  title        = {Monitoring of zero-inflated binomial processes with a DEWMA control chart},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Type-i intermittency from markov binary block visibility
graph perspective. <em>JOAS</em>, <em>48</em>(7), 1303–1318. (<a
href="https://doi.org/10.1080/02664763.2020.1761949">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, the type-I intermittency is studied from the optimized Markov binary visibility graphs perspective. We consider a local Poincaré map such as the logistic map that is a simple model for exhibiting this type of intermittency. To consider the acceptance gate as G ≪ 0.01 G ≪ 0.01 G≪0.01 , we show that the transition between laminar and non-laminar zones in type-I intermittency takes distinct phases and regions. According to their behavioral characteristics, we call them as pure, switching, threshold, trapping, and transforming phases for the laminar zone and initial, terminal reinjection, and chaotic burst regions for non-laminar zone. We investigate their properties based on statistical tools such as the maximum and the mean length of the laminar zone and also length distributions of the laminar zone. For further investigation, we study degree distribution of the complex network generated by type-I intermittency time series and finally, predict various behaviors of phases and regions by proposed theoretical degree distributions.},
  archive      = {J_JOAS},
  author       = {Pejman Bordbar and Sodeif Ahadpour},
  doi          = {10.1080/02664763.2020.1761949},
  journal      = {Journal of Applied Statistics},
  number       = {7},
  pages        = {1303-1318},
  shortjournal = {J. Appl. Stat.},
  title        = {Type-I intermittency from markov binary block visibility graph perspective},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Pairwise influences in dynamic choice: Network-based model
and application. <em>JOAS</em>, <em>48</em>(7), 1269–1302. (<a
href="https://doi.org/10.1080/02664763.2020.1761948">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study the problem of network discovery and influence propagation, and propose an integrated approach for the analysis of lead-lag synchronization in multiple choices. Network models for the processes by which decisions propagate through social interaction have been studied before, but only a few consider unknown structures of interacting agents. In fact, while individual choices are typically observed, inferring individual influences – who influences who – from sequences of dynamic choices requires strong modeling assumptions on the cross-section dependencies of the observed panels. We propose a class of parametric models which extends the vector autoregression to the case of pairwise influences between individual choices over multiple items and supports the analysis of influence propagation. After uncovering a collection of theoretical properties (conditional moments, parameter sensitivity, identifiability and estimation), we provide an economic application to music broadcasting, where a set of songs are diffused over radio stations; we infer station-to-station influences based on the proposed methodology and assess the propagation effect of initial launching stations to maximize songs diffusion. Both on the theoretical and empirical sides, the proposed approach connects fields which are traditionally treated as separated areas: the problem of network discovery and the one of influence propagation.},
  archive      = {J_JOAS},
  author       = {Stefano Nasini and Victor Martínez-de-Albéniz},
  doi          = {10.1080/02664763.2020.1761948},
  journal      = {Journal of Applied Statistics},
  number       = {7},
  pages        = {1269-1302},
  shortjournal = {J. Appl. Stat.},
  title        = {Pairwise influences in dynamic choice: Network-based model and application},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A combined mixed-s-skip sampling strategy to reduce the
effect of autocorrelation on the x̄ scheme with and without measurement
errors. <em>JOAS</em>, <em>48</em>(7), 1243–1268. (<a
href="https://doi.org/10.1080/02664763.2020.1759033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to reduce the effect of autocorrelation on the X ⎯ ⎯ ⎯ X ¯ X¯ monitoring scheme, a new sampling strategy is proposed to form rational subgroup samples of size n . It requires sampling to be done such that: (i) observations from two consecutive samples are merged, and (ii) some consecutive observations are skipped before sampling. This technique which is a generalized version of the mixed samples strategy is shown to yield a better reduction of the negative effect of autocorrelation when monitoring the mean of processes with and without measurement errors. For processes subjected to a combined effect of autocorrelation and measurement errors, the proposed sampling technique, together with multiple measurement strategy, yields an uniformly better zero-state run-length performance than its two main existing competitors for any autocorrelation level. However, in steady-state mode, it yields the best performance only when the monitoring process is subject to a high level of autocorrelation, for any given level of measurement errors. A real life example is used to illustrate the implementation of the proposed sampling strategy.},
  archive      = {J_JOAS},
  author       = {Sandile Charles Shongwe and Jean-Claude Malela-Majika and Philippe Castagliola},
  doi          = {10.1080/02664763.2020.1759033},
  journal      = {Journal of Applied Statistics},
  number       = {7},
  pages        = {1243-1268},
  shortjournal = {J. Appl. Stat.},
  title        = {A combined mixed-s-skip sampling strategy to reduce the effect of autocorrelation on the x̄ scheme with and without measurement errors},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Double and group acceptance sampling plan for truncated life
test based on inverse log-logistic distribution. <em>JOAS</em>,
<em>48</em>(7), 1227–1242. (<a
href="https://doi.org/10.1080/02664763.2020.1759031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a double and group acceptance sampling plans based on time truncated lifetimes when the lifetime of an item follows the inverse log-logistic (ILL) distribution with known shape parameter. The operating characteristic function and average sample number (ASN) values of the double acceptance sampling inspection plan are provided. The values of the minimum number of groups and operating characteristic function for various quality levels are obtained for a group acceptance sampling inspection plan. A comparative study between single acceptance sampling inspection plan and double acceptance sampling inspection plan is carried out in terms of sample size. One simulated example and four real-life examples are discussed to show the applicability of the proposed double and group acceptance sampling inspection plans for ILL distributed quality parameters.},
  archive      = {J_JOAS},
  author       = {Harsh Tripathi and Sanku Dey and Mahendra Saha},
  doi          = {10.1080/02664763.2020.1759031},
  journal      = {Journal of Applied Statistics},
  number       = {7},
  pages        = {1227-1242},
  shortjournal = {J. Appl. Stat.},
  title        = {Double and group acceptance sampling plan for truncated life test based on inverse log-logistic distribution},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Confidence intervals based on l-moments for quantiles of the
GP and GEV distributions with application to market-opening asset prices
data. <em>JOAS</em>, <em>48</em>(7), 1199–1226. (<a
href="https://doi.org/10.1080/02664763.2020.1757046">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a ground-breaking paper published in 1990 by the Journal of the Royal Statistical Society , J.R.M. Hosking defined the L-moment of a random variable as an expectation of certain linear combinations of order statistics. L-moments are an alternative to conventional moments and recently they have been used often in inferential statistics. L-moments have several advantages over the conventional moments, including robustness to the the presence of outliers, which may lead to more accurate estimates in some cases as the characteristics of distributions. In this contribution, asymptotic theory and L-moments are used to derive confidence intervals of the population parameters and quantiles of the three-parametric generalized Pareto and extreme-value distributions. Computer simulations are performed to determine the performance of confidence intervals for the population quantiles based on L-moments and to compare them to those obtained by traditional estimation techniques. The results obtained show that they perform well in comparison to the moments and maximum likelihood methods when the interest is in higher quantiles, or even best. L-moments are especially recommended when the tail of the distribution is rather heavier and the sample size is small. The derived intervals are applied to real economic data, and specifically to market-opening asset prices.},
  archive      = {J_JOAS},
  author       = {T. Šimková},
  doi          = {10.1080/02664763.2020.1757046},
  journal      = {Journal of Applied Statistics},
  number       = {7},
  pages        = {1199-1226},
  shortjournal = {J. Appl. Stat.},
  title        = {Confidence intervals based on L-moments for quantiles of the GP and GEV distributions with application to market-opening asset prices data},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Tests on asymmetry for ordered categorical variables.
<em>JOAS</em>, <em>48</em>(7), 1180–1198. (<a
href="https://doi.org/10.1080/02664763.2020.1757045">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Skewness is a well-established statistical concept for continuous and, to a lesser extent, for discrete quantitative statistical variables. However, for ordered categorical variables, limited literature concerning skewness exists, although this type of variables is common for behavioral, educational, and social sciences. Suitable measures of skewness for ordered categorical variables have to be invariant with respect to the group of strictly increasing, continuous transformations. Therefore, they have to depend on the corresponding maximal-invariants. Based on these maximal-invariants, we propose a new class of skewness functionals, show that members of this class preserve a suitable ordering of skewness and derive the asymptotic distribution of the corresponding skewness statistic. Finally, we show the good power behavior of the corresponding skewness tests and illustrate these tests by applying real data examples.},
  archive      = {J_JOAS},
  author       = {Ingo Klein and Monika Doll},
  doi          = {10.1080/02664763.2020.1757045},
  journal      = {Journal of Applied Statistics},
  number       = {7},
  pages        = {1180-1198},
  shortjournal = {J. Appl. Stat.},
  title        = {Tests on asymmetry for ordered categorical variables},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). EM algorithm for mixture of skew-normal distributions fitted
to grouped data. <em>JOAS</em>, <em>48</em>(7), 1154–1179. (<a
href="https://doi.org/10.1080/02664763.2020.1759032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Grouped data are frequently used in several fields of study. In this work, we use the expectation-maximization (EM) algorithm for fitting the skew-normal (SN) mixture model to the grouped data. Implementing the EM algorithm requires computing the one-dimensional integrals for each group or class. Our simulation study and real data analyses reveal that the EM algorithm not only always converges but also can be implemented in just a few seconds even when the number of components is large, contrary to the Bayesian paradigm that is computationally expensive. The accuracy of the EM algorithm and superiority of the SN mixture model over the traditional normal mixture model in modelling grouped data are demonstrated through the simulation and three real data illustrations. For implementing the EM algorithm, we use the package called ForestFit developed for R environment available at https://cran.r-project.org/web/packages/ForestFit/index.html .},
  archive      = {J_JOAS},
  author       = {Mahdi Teimouri},
  doi          = {10.1080/02664763.2020.1759032},
  journal      = {Journal of Applied Statistics},
  number       = {7},
  pages        = {1154-1179},
  shortjournal = {J. Appl. Stat.},
  title        = {EM algorithm for mixture of skew-normal distributions fitted to grouped data},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Correlated discrete and continuous outcomes with endogeneity
and lagged effects: Past season yield impact on improved corn seed
adoption. <em>JOAS</em>, <em>48</em>(6), 1128–1153. (<a
href="https://doi.org/10.1080/02664763.2020.1757050">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Farmers in Sub-Saharan Africa have lower agricultural technology adoption rates compared to the rest of the world. It is believed that the past season yield affects a farmer&#39;s capacity to take on the riskier improved seed variety; but this effect has not been studied. We quantify the effect of past season yield on improved corn seed use in future seasons while addressing the impact of the seed variety on yield. We develop a maximum likelihood method that addresses the fact that farmers self-select into a technology resulting in its effect on yield being endogenous. The method is unique since it models both lagged and endogenous effects in correlated discrete and continuous outcomes simultaneously. Due to the prescence of the lagged effect in a three year dataset, we also propose a solution to the initial conditions problem and demonstrate with simulations its effectiveness. We used survey longitudinal data collected from Kenyan corn farmers for three years. Our results show that higher past season yield increased the likelihood of adoption in future seasons. The simulation and empirical studies indicate that ignoring the self selection of improved seed use biases the results; we obtain a different sign in the covariance.},
  archive      = {J_JOAS},
  author       = {Rhoda Nandai Muse and Satheesh Aradhyula},
  doi          = {10.1080/02664763.2020.1757050},
  journal      = {Journal of Applied Statistics},
  number       = {6},
  pages        = {1128-1153},
  shortjournal = {J. Appl. Stat.},
  title        = {Correlated discrete and continuous outcomes with endogeneity and lagged effects: Past season yield impact on improved corn seed adoption},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A doubly-inflated poisson regression for correlated count
data. <em>JOAS</em>, <em>48</em>(6), 1111–1127. (<a
href="https://doi.org/10.1080/02664763.2020.1757049">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Count data have emerged in many applied research areas. In recent years, there has been a considerable interest in models for count data. In modelling such data, it is common to face a large frequency of zeroes. The data are regarded as zero-inflated when the frequency of observed zeroes is larger than what is expected from a theoretical distribution such as Poisson distribution, as a standard model for analysing count data. Data analysis, using the simple Poisson model, may lead to over-dispersion. Several classes of different mixture models were proposed for handling zero-inflated data. But they do not apply to cases when inflated counts happen at some other points, in addition to zero. In these cases, a doubly-inflated Poisson model has been suggested which only be used for cross-sectional data and cannot consider correlations between observations. However, correlated count data have a large application, especially in the health and medical fields. The present study aims to introduce a Doubly-Inflated Poisson models with random effect for correlated doubly-inflated data. Then, the best performance of the proposed method is shown via different simulation scenarios. Finally, the proposed model is applied to a dental study.},
  archive      = {J_JOAS},
  author       = {Erfan Ghasemi and Alireza Akbarzadeh Baghban and Farid Zayeri and Asma Pourhoseingholi and Seyed Mohammadreza Safavi},
  doi          = {10.1080/02664763.2020.1757049},
  journal      = {Journal of Applied Statistics},
  number       = {6},
  pages        = {1111-1127},
  shortjournal = {J. Appl. Stat.},
  title        = {A doubly-inflated poisson regression for correlated count data},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A signature enrichment design with bayesian adaptive
randomization. <em>JOAS</em>, <em>48</em>(6), 1091–1110. (<a
href="https://doi.org/10.1080/02664763.2020.1757048">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clinical trials in the era of precision cancer medicine aim to identify and validate biomarker signatures which can guide the assignment of individually optimal treatments to patients. In this article, we propose a group sequential randomized phase II design, which updates the biomarker signature as the trial goes on, utilizes enrichment strategies for patient selection, and uses Bayesian response-adaptive randomization for treatment assignment. To evaluate the performance of the new design, in addition to the commonly considered criteria of Type I error and power, we propose four new criteria measuring the benefits and losses for individuals both inside and outside of the clinical trial. Compared with designs with equal randomization, the proposed design gives trial participants a better chance to receive their personalized optimal treatments and thus results in a higher response rate on the trial. This design increases the chance to discover a successful new drug by an adaptive enrichment strategy, i.e. identification and selective enrollment of a subset of patients who are sensitive to the experimental therapies. Simulation studies demonstrate these advantages of the proposed design. It is illustrated by an example based on an actual clinical trial in non-small-cell lung cancer.},
  archive      = {J_JOAS},
  author       = {Fang Xia and Stephen L. George and Jing Ning and Liang Li and Xuelin Huang},
  doi          = {10.1080/02664763.2020.1757048},
  journal      = {Journal of Applied Statistics},
  number       = {6},
  pages        = {1091-1110},
  shortjournal = {J. Appl. Stat.},
  title        = {A signature enrichment design with bayesian adaptive randomization},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A bivariate discrete inverse resilience family of
distributions with resilience marginals. <em>JOAS</em>, <em>48</em>(6),
1071–1090. (<a
href="https://doi.org/10.1080/02664763.2020.1755618">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a new bivariate discrete generalized exponential distribution, whose marginals are discrete generalized exponential distributions, is studied. It is observed that the proposed bivariate distribution is a flexible distribution whose cumulative distribution function has an analytical structure. In addition, a new bivariate geometric distribution can be obtained as a special case. We study different properties of this distribution and propose estimation of its parameters. We will see that the maximum of the variables involved in the proposed bivariate distribution defines some new classes of univariate discrete distributions, which are interesting in their own sake, and can be used to analyze some Reliability systems whose components are positive dependent. Some important futures of this new univariate family of discrete distributions are also studied in details. In addition, a general class of bivariate discrete distributions, whose marginals are exponentiated discrete distributions, is introduced. Moreover, the analysis of two real bivariate data sets is performed to indicate the effectiveness of the proposed models. Finally, we conclude the paper.},
  archive      = {J_JOAS},
  author       = {Vahid Nekoukhou and Ashkan Khalifeh and Hamid Bidram},
  doi          = {10.1080/02664763.2020.1755618},
  journal      = {Journal of Applied Statistics},
  number       = {6},
  pages        = {1071-1090},
  shortjournal = {J. Appl. Stat.},
  title        = {A bivariate discrete inverse resilience family of distributions with resilience marginals},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Tuning parameter selection for a penalized estimator of
species richness. <em>JOAS</em>, <em>48</em>(6), 1053–1070. (<a
href="https://doi.org/10.1080/02664763.2020.1754359">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Our goal is to estimate the true number of classes in a population, called the species richness. We consider the case where multiple frequency count tables have been collected from a homogeneous population and investigate a penalized maximum likelihood estimator under a negative binomial model. Because high probabilities of unobserved classes increase the variance of species richness estimates, our method penalizes the probability of a class being unobserved. Tuning the penalization parameter is challenging because the true species richness is never known, and so we propose and validate four novel methods for tuning the penalization parameter. We illustrate and contrast the performance of the proposed methods by estimating the strain-level microbial diversity of Lake Champlain over three consecutive years, and global human host-associated species-level microbial richness.},
  archive      = {J_JOAS},
  author       = {Alex Paynter and Amy D. Willis},
  doi          = {10.1080/02664763.2020.1754359},
  journal      = {Journal of Applied Statistics},
  number       = {6},
  pages        = {1053-1070},
  shortjournal = {J. Appl. Stat.},
  title        = {Tuning parameter selection for a penalized estimator of species richness},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Statistical inferences for single-index models with
measurement errors. <em>JOAS</em>, <em>48</em>(6), 1033–1052. (<a
href="https://doi.org/10.1080/02664763.2020.1754358">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An important factor in house prices is its location. However, measurement errors arise frequently in the process of observing variables such as the latitude and longitude of the house. The single-index models with measurement errors are used to study the relationship between house location and house price. We obtain the estimators by a SIMEX method based on the local linear method and the estimating equation. To test the significance of the index coefficient and the linearity of the link function, we establish the generalized likelihood ratio (GLR) tests for the models. We demonstrate that the asymptotic null distributions of the established GLR tests follow χ 2 -distributions which are independent of nuisance parameters or functions. Finally, two simulated examples and a real estate valuation data set are given to illustrate the effect of GLR tests.},
  archive      = {J_JOAS},
  author       = {Zhensheng Huang and Wen Lou},
  doi          = {10.1080/02664763.2020.1754358},
  journal      = {Journal of Applied Statistics},
  number       = {6},
  pages        = {1033-1052},
  shortjournal = {J. Appl. Stat.},
  title        = {Statistical inferences for single-index models with measurement errors},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sample size calculations for noninferiority trials for
time-to-event data using the concept of proportional time.
<em>JOAS</em>, <em>48</em>(6), 1009–1032. (<a
href="https://doi.org/10.1080/02664763.2020.1753026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Noninferiority trials intend to show that a new treatment is ‘not worse&#39; than a standard-of-care active control and can be used as an alternative when it is likely to cause fewer side effects compared to the active control. In the case of time-to-event endpoints, existing methods of sample size calculation are done either assuming proportional hazards between the two study arms, or assuming exponentially distributed lifetimes. In scenarios where these assumptions are not true, there are few reliable methods for calculating the sample sizes for a time-to-event noninferiority trial. Additionally, the choice of the non-inferiority margin is obtained either from a meta-analysis of prior studies, or strongly justifiable ‘expert opinion&#39;, or from a ‘well conducted&#39; definitive large-sample study. Thus, when historical data do not support the traditional assumptions, it would not be appropriate to use these methods to design a noninferiority trial. For such scenarios, an alternate method of sample size calculation based on the assumption of Proportional Time is proposed. This method utilizes the generalized gamma ratio distribution to perform the sample size calculations. A practical example is discussed, followed by insights on choice of the non-inferiority margin, and the indirect testing of superiority of treatment compared to placebo.},
  archive      = {J_JOAS},
  author       = {Milind A. Phadnis and Matthew S. Mayo},
  doi          = {10.1080/02664763.2020.1753026},
  journal      = {Journal of Applied Statistics},
  number       = {6},
  pages        = {1009-1032},
  shortjournal = {J. Appl. Stat.},
  title        = {Sample size calculations for noninferiority trials for time-to-event data using the concept of proportional time},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On the lengths of t-based confidence intervals.
<em>JOAS</em>, <em>48</em>(6), 993–1008. (<a
href="https://doi.org/10.1080/02664763.2020.1754357">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Confidence interval is a basic type of interval estimation in statistics. When dealing with samples from a normal population with the unknown mean and the variance, the traditional method to construct t -based confidence intervals for the mean parameter is to treat the n sampled units as n groups and build the intervals. Here we propose a generalized method. We first divide them into several equal-sized groups and then calculate the confidence intervals with the mean values of these groups. If we define “better” in terms of the expected length of the confidence interval, then the first method is better because the expected length of the confidence interval obtained from the first method is shorter. We prove this intuition theoretically. We also specify when the elements in each group are correlated, the first method is invalid, while the second can give us correct results in terms of the coverage probability. We illustrate this with analytical expressions. In practice, when the data set is extremely large and distributed in several data centers, the second method is a good tool to get confidence intervals, in both independent and correlated cases. Some simulations and real data analyses are presented to verify our theoretical results.},
  archive      = {J_JOAS},
  author       = {Yu Zhang and Xiangzhong Fang},
  doi          = {10.1080/02664763.2020.1754357},
  journal      = {Journal of Applied Statistics},
  number       = {6},
  pages        = {993-1008},
  shortjournal = {J. Appl. Stat.},
  title        = {On the lengths of t-based confidence intervals},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A bayesian semiparametric method for analyzing length-biased
data. <em>JOAS</em>, <em>48</em>(6), 977–992. (<a
href="https://doi.org/10.1080/02664763.2020.1753028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Survival data obtained from prevalent cohort study designs are often subject to length-biased sampling. Frequentist methods including estimating equation approaches, as well as full likelihood methods, are available for assessing covariate effects on survival from such data. Bayesian methods allow a perspective of probability interpretation for the parameters of interest, and may easily provide the predictive distribution for future observations while incorporating weak prior knowledge on the baseline hazard function. There is lack of Bayesian methods for analyzing length-biased data. In this paper, we propose Bayesian methods for analyzing length-biased data under a proportional hazards model. The prior distribution for the cumulative hazard function is specified semiparametrically using I-Splines. Bayesian conditional and full likelihood approaches are developed for analyzing simulated and real data.},
  archive      = {J_JOAS},
  author       = {Nusrat Harun and Bo Cai and Yu Shen},
  doi          = {10.1080/02664763.2020.1753028},
  journal      = {Journal of Applied Statistics},
  number       = {6},
  pages        = {977-992},
  shortjournal = {J. Appl. Stat.},
  title        = {A bayesian semiparametric method for analyzing length-biased data},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Optimal weighted two-sample t-test with partially paired
data in a unified framework. <em>JOAS</em>, <em>48</em>(6), 961–976. (<a
href="https://doi.org/10.1080/02664763.2020.1753027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we provide a unified framework for two-sample t -test with partially paired data. We show that many existing two-sample t -tests with partially paired data can be viewed as special members in our unified framework. Some shortcomings of these t -tests are discussed. We also propose the asymptotically optimal weighted linear combination of the test statistics comparing all four paired and unpaired data sets. Simulation studies are used to illustrate the performance of our proposed asymptotically optimal weighted combinations of test statistics and compare with some existing methods. It is found that our proposed test statistic is generally more powerful. Three real data sets about CD4 count, DNA extraction concentrations, and the quality of sleep are also analyzed by using our newly introduced test statistic.},
  archive      = {J_JOAS},
  author       = {Xu Guo and Yan Wang and Niwen Zhou and Xuehu Zhu},
  doi          = {10.1080/02664763.2020.1753027},
  journal      = {Journal of Applied Statistics},
  number       = {6},
  pages        = {961-976},
  shortjournal = {J. Appl. Stat.},
  title        = {Optimal weighted two-sample t-test with partially paired data in a unified framework},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Identifying influential observations in a bayesian
multi-level mediation model. <em>JOAS</em>, <em>48</em>(5), 943–960. (<a
href="https://doi.org/10.1080/02664763.2020.1748179">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Increasingly complex models are being fit to data these days. This is especially the case for Bayesian modelling making use of Markov chain Monte Carlo methods. Tailored model diagnostics are usually lacking behind. This is also the case for Bayesian mediation models. In this paper, we developed a method for the detection of influential observations for a popular mediation model and its extensions in a Bayesian context. Detection of influential observations is based on the case-deletion principle. Importance sampling with weights which take advantage of the dependence structure in hierarchical models is utilized in order to identify the part of the model which is influenced most. We make use of the variance of log importance sampling weights as the measure of influence. It is demonstrated that this approach is useful when interest lies in the impact of individual observations on a subset of model parameters. The method is illustrated on a three-level data set from the field of nursing research, which was previously used to fit a mediation model of patient satisfaction with care. We focused on influential cases on both the second and the third level of the data.},
  archive      = {J_JOAS},
  author       = {Šárka Večeřová and Arnošt Komárek and Luk Bruyneel and Emmanuel Lesaffre},
  doi          = {10.1080/02664763.2020.1748179},
  journal      = {Journal of Applied Statistics},
  number       = {5},
  pages        = {943-960},
  shortjournal = {J. Appl. Stat.},
  title        = {Identifying influential observations in a bayesian multi-level mediation model},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Improvement of mixed predictors in linear mixed models.
<em>JOAS</em>, <em>48</em>(5), 924–942. (<a
href="https://doi.org/10.1080/02664763.2020.1833182">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we introduce stochastic-restricted Liu predictors which will be defined by combining in a special way the two approaches followed in obtaining the mixed predictors and the Liu predictors in the linear mixed models. Superiorities of the linear combination of the new predictor to the Liu and mixed predictors are done in the sense of mean square error matrix criterion. Finally, numerical examples and a simulation study are done to illustrate the findings. In numerical examples, we took some arbitrary observations from the data as the prior information since we did not have historical data or additional information about the data sets. The results show that this case does the new estimator gain efficiency over the constituent estimators and provide accurate estimation and prediction of the data.},
  archive      = {J_JOAS},
  author       = {Özge Kuran and M. Revan Özkale},
  doi          = {10.1080/02664763.2020.1833182},
  journal      = {Journal of Applied Statistics},
  number       = {5},
  pages        = {924-942},
  shortjournal = {J. Appl. Stat.},
  title        = {Improvement of mixed predictors in linear mixed models},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Interval-censored data with misclassification: A bayesian
approach. <em>JOAS</em>, <em>48</em>(5), 907–923. (<a
href="https://doi.org/10.1080/02664763.2020.1753025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Survival data involving silent events are often subject to interval censoring (the event is known to occur within a time interval) and classification errors if a test with no perfect sensitivity and specificity is applied. Considering the nature of this data plays an important role in estimating the time distribution until the occurrence of the event. In this context, we incorporate validation subsets into the parametric proportional hazard model, and show that this additional data, combined with Bayesian inference, compensate the lack of knowledge about test sensitivity and specificity improving the parameter estimates. The proposed model is evaluated through simulation studies, and Bayesian analysis is conducted within a Gibbs sampling procedure. The posterior estimates obtained under validation subset models present lower bias and standard deviation compared to the scenario with no validation subset or the model that assumes perfect sensitivity and specificity. Finally, we illustrate the usefulness of the new methodology with an analysis of real data about HIV acquisition in female sex workers that have been discussed in the literature.},
  archive      = {J_JOAS},
  author       = {Magda Carvalho Pires and Enrico Antônio Colosimo and Guilherme Augusto Veloso and Raquel de Souza Borges Ferreira},
  doi          = {10.1080/02664763.2020.1753025},
  journal      = {Journal of Applied Statistics},
  number       = {5},
  pages        = {907-923},
  shortjournal = {J. Appl. Stat.},
  title        = {Interval-censored data with misclassification: A bayesian approach},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Time varying factor models with possibly strongly correlated
noises. <em>JOAS</em>, <em>48</em>(5), 887–906. (<a
href="https://doi.org/10.1080/02664763.2020.1753024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In factor models, noises are often assumed to be weakly correlated; otherwise, separation of factors from noises becomes difficult, if not impossible. This paper will address this problem. We utilize an econometric idea, the so called common correlated effects (CCE) to estimate time varying factor models. We first cross sectionally average the covariates and then project the responses to the space spanned by the averaged covariates. By doing so, noises are diminished while factors are distinguished. The advantages of our new estimators are two folds. First, the convergence rates of estimated factors and loadings are independent of cross sectional dimension. Second, our new estimators are robust to the correlation of noises. Hence our new estimators can, on one hand, separate market factors for the stock data set used in this paper even if noises exhibit strong correlations within industries due to industry-specific factors and on the other hand, avoid inappropriately absorbing industry-specific factors into market factors.},
  archive      = {J_JOAS},
  author       = {Mingjing Chen and Xiangyong Tan and Jian Wu},
  doi          = {10.1080/02664763.2020.1753024},
  journal      = {Journal of Applied Statistics},
  number       = {5},
  pages        = {887-906},
  shortjournal = {J. Appl. Stat.},
  title        = {Time varying factor models with possibly strongly correlated noises},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Modified check loss for efficient estimation via model
selection in quantile regression. <em>JOAS</em>, <em>48</em>(5),
866–886. (<a
href="https://doi.org/10.1080/02664763.2020.1753023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The check loss function is used to define quantile regression. In cross-validation, it is also employed as a validation function when the true distribution is unknown. However, our empirical study indicates that validation with the check loss often leads to overfitting the data. In this work, we suggest a modified or L2-adjusted check loss which rounds the sharp corner in the middle of check loss. This has the effect of guarding against overfitting to some extent. The adjustment is devised to shrink to zero as sample size grows. Through various simulation settings of linear and nonlinear regressions, the improvement due to modification of the check loss by quadratic adjustment is examined empirically.},
  archive      = {J_JOAS},
  author       = {Yoonsuh Jung and Steven N. MacEachern and Hang Joon Kim},
  doi          = {10.1080/02664763.2020.1753023},
  journal      = {Journal of Applied Statistics},
  number       = {5},
  pages        = {866-886},
  shortjournal = {J. Appl. Stat.},
  title        = {Modified check loss for efficient estimation via model selection in quantile regression},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Regression analysis of case-cohort studies in the presence
of dependent interval censoring. <em>JOAS</em>, <em>48</em>(5), 846–865.
(<a href="https://doi.org/10.1080/02664763.2020.1752633">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The case-cohort design is widely used as a means of reducing the cost in large cohort studies, especially when the disease rate is low and covariate measurements may be expensive, and has been discussed by many authors. In this paper, we discuss regression analysis of case-cohort studies that produce interval-censored failure time with dependent censoring, a situation for which there does not seem to exist an established approach. For inference, a sieve inverse probability weighting estimation procedure is developed with the use of Bernstein polynomials to approximate the unknown baseline cumulative hazard functions. The proposed estimators are shown to be consistent and the asymptotic normality of the resulting regression parameter estimators is established. A simulation study is conducted to assess the finite sample properties of the proposed approach and indicates that it works well in practical situations. The proposed method is applied to an HIV/AIDS case-cohort study that motivated this investigation.},
  archive      = {J_JOAS},
  author       = {Mingyue Du and Qingning Zhou and Shishun Zhao and Jianguo Sun},
  doi          = {10.1080/02664763.2020.1752633},
  journal      = {Journal of Applied Statistics},
  number       = {5},
  pages        = {846-865},
  shortjournal = {J. Appl. Stat.},
  title        = {Regression analysis of case-cohort studies in the presence of dependent interval censoring},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Significance test for linear regression: How to test without
p-values? <em>JOAS</em>, <em>48</em>(5), 827–845. (<a
href="https://doi.org/10.1080/02664763.2020.1748180">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The discussion on the use and misuse of p -values in 2016 by the American Statistician Association was a timely assertion that statistical concept should be properly used in science. Some researchers, especially the economists, who adopt significance testing and p -values to report their results, may felt confused by the statement, leading to misinterpretations of the statement. In this study, we aim to re-examine the accuracy of the p -value and introduce an alternative way for testing the hypothesis. We conduct a simulation study to investigate the reliability of the p -value. Apart from investigating the performance of p -value, we also introduce some existing approaches, Minimum Bayes Factors and Belief functions, for replacing p -value. Results from the simulation study confirm unreliable p -value in some cases and that our proposed approaches seem to be useful as the substituted tool in the statistical inference. Moreover, our results show that the plausibility approach is more accurate for making decisions about the null hypothesis than the traditionally used p -values when the null hypothesis is true. However, the MBFs of Edwards et al. [ Bayesian statistical inference for psychological research . Psychol. Rev. 70(3) (1963), pp. 193–242]; Vovk [ A logic of probability, with application to the foundations of statistics . J. Royal Statistical Soc. Series B (Methodological) 55 (1993), pp. 317–351] and Sellke et al. [ Calibration of p values for testing precise null hypotheses . Am. Stat. 55(1) (2001), pp. 62–71] provide more reliable results compared to all other methods when the null hypothesis is false.},
  archive      = {J_JOAS},
  author       = {Paravee Maneejuk and Woraphon Yamaka},
  doi          = {10.1080/02664763.2020.1748180},
  journal      = {Journal of Applied Statistics},
  number       = {5},
  pages        = {827-845},
  shortjournal = {J. Appl. Stat.},
  title        = {Significance test for linear regression: How to test without P-values?},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dynamic structural models with covariates for short-term
forecasting of time series with complex seasonal patterns.
<em>JOAS</em>, <em>48</em>(5), 804–826. (<a
href="https://doi.org/10.1080/02664763.2020.1748178">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work presents a framework of dynamic structural models with covariates for short-term forecasting of time series with complex seasonal patterns. The framework is based on the multiple sources of randomness formulation. A noise model is formulated to allow the incorporation of randomness into the seasonal component and to propagate this same randomness in the coefficients of the variant trigonometric terms over time. A unique, recursive and systematic computational procedure based on the maximum likelihood estimation under the hypothesis of Gaussian errors is introduced. The referred procedure combines the Kalman filter with recursive adjustment of the covariance matrices and the selection method of harmonics number in the trigonometric terms. A key feature of this method is that it allows estimating not only the states of the system but also allows obtaining the standard errors of the estimated parameters and the prediction intervals. In addition, this work also presents a non-parametric bootstrap approach to improve the forecasting method based on Kalman filter recursions. The proposed framework is empirically explored with two real time series.},
  archive      = {J_JOAS},
  author       = {António Casimiro Puindi and Maria Eduarda Silva},
  doi          = {10.1080/02664763.2020.1748178},
  journal      = {Journal of Applied Statistics},
  number       = {5},
  pages        = {804-826},
  shortjournal = {J. Appl. Stat.},
  title        = {Dynamic structural models with covariates for short-term forecasting of time series with complex seasonal patterns},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Copula-based markov zero-inflated count time series models
with application. <em>JOAS</em>, <em>48</em>(5), 786–803. (<a
href="https://doi.org/10.1080/02664763.2020.1748581">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Count time series data with excess zeros are observed in several applied disciplines. When these zero-inflated counts are sequentially recorded, they might result in serial dependence. Ignoring the zero-inflation and the serial dependence might produce inaccurate results. In this paper, Markov zero-inflated count time series models based on a joint distribution on consecutive observations are proposed. The joint distribution function of the consecutive observations is constructed through copula functions. First- and second-order Markov chains are considered with the univariate margins of zero-inflated Poisson (ZIP), zero-inflated negative binomial (ZINB), or zero-inflated Conway–Maxwell–Poisson (ZICMP) distributions. Under the Markov models, bivariate copula functions such as the bivariate Gaussian, Frank, and Gumbel are chosen to construct a bivariate distribution of two consecutive observations. Moreover, the trivariate Gaussian and max-infinitely divisible copula functions are considered to build the joint distribution of three consecutive observations. Likelihood-based inference is performed and asymptotic properties are studied. To evaluate the estimation method and the asymptotic results, simulated examples are studied. The proposed class of models are applied to sandstorm counts example. The results suggest that the proposed models have some advantages over some of the models in the literature for modeling zero-inflated count time series data.},
  archive      = {J_JOAS},
  author       = {Mohammed Alqawba and Norou Diawara},
  doi          = {10.1080/02664763.2020.1748581},
  journal      = {Journal of Applied Statistics},
  number       = {5},
  pages        = {786-803},
  shortjournal = {J. Appl. Stat.},
  title        = {Copula-based markov zero-inflated count time series models with application},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). General location multivariate latent variable models for
mixed correlated bounded continuous, ordinal, and nominal responses with
non-ignorable missing data. <em>JOAS</em>, <em>48</em>(5), 765–785. (<a
href="https://doi.org/10.1080/02664763.2020.1745765">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Using a multivariate latent variable approach, this article proposes some new general models to analyze the correlated bounded continuous and categorical (nominal or/and ordinal) responses with and without non-ignorable missing values. First, we discuss regression methods for jointly analyzing continuous, nominal, and ordinal responses that we motivated by analyzing data from studies of toxicity development. Second, using the beta and Dirichlet distributions, we extend the models so that some bounded continuous responses are replaced for continuous responses. The joint distribution of the bounded continuous, nominal and ordinal variables is decomposed into a marginal multinomial distribution for the nominal variable and a conditional multivariate joint distribution for the bounded continuous and ordinal variables given the nominal variable. We estimate the regression parameters under the new general location models using the maximum-likelihood method. Sensitivity analysis is also performed to study the influence of small perturbations of the parameters of the missing mechanisms of the model on the maximal normal curvature. The proposed models are applied to two data sets: BMI, Steatosis and Osteoporosis data and Tehran household expenditure budgets.},
  archive      = {J_JOAS},
  author       = {Elham Tabrizi and Ehsan Bahrami Samani and Mojtaba Ganjali},
  doi          = {10.1080/02664763.2020.1745765},
  journal      = {Journal of Applied Statistics},
  number       = {5},
  pages        = {765-785},
  shortjournal = {J. Appl. Stat.},
  title        = {General location multivariate latent variable models for mixed correlated bounded continuous, ordinal, and nominal responses with non-ignorable missing data},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A multivariate multiple third-variable effect analysis with
an application to explore racial and ethnic disparities in obesity.
<em>JOAS</em>, <em>48</em>(4), 750–764. (<a
href="https://doi.org/10.1080/02664763.2020.1738359">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Third-Variable effect refers to the intervening effect from a third variable (called mediators or confounders) to the observed relationship between an exposure and an outcome. The general multiple third-variable effect analysis method (TVEA) allows consideration of multiple mediators/confounders (MC) simultaneously and the use of linear and nonlinear predictive models for estimating MC effects. Previous studies have found that compared with non-Hispanic White population, Blacks and Hispanic Whites suffered disproportionally more with obesity and related chronic diseases. In this paper, we extend the general TVEA to deal with multivariate/multi-categorical predictors and multivariate response variables. We designed algorithms and an R package for this extension and applied MMA on the NHANES data to identify MCs and quantify the indirect effect of each MC in explaining both racial and ethnic disparities in obesity and the body mass index (BMI) simultaneously. We considered a number of socio-demographic variables, individual factors, and environmental variables as potential MCs and found that some of the ethnic/racial differences in obesity and BMI were explained by the included variables.},
  archive      = {J_JOAS},
  author       = {Qingzhao Yu and Bin Li},
  doi          = {10.1080/02664763.2020.1738359},
  journal      = {Journal of Applied Statistics},
  number       = {4},
  pages        = {750-764},
  shortjournal = {J. Appl. Stat.},
  title        = {A multivariate multiple third-variable effect analysis with an application to explore racial and ethnic disparities in obesity},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Joint model for bivariate zero-inflated recurrent event data
with terminal events. <em>JOAS</em>, <em>48</em>(4), 738–749. (<a
href="https://doi.org/10.1080/02664763.2020.1744539">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bivariate recurrent event data are observed when subjects are at risk of experiencing two different type of recurrent events. In this paper, our interest is to suggest statistical model when there is a substantial portion of subjects not experiencing recurrent events but having a terminal event. In a context of recurrent event data, zero events can be related with either the risk free group or a terminal event. For simultaneously reflecting both a zero inflation and a terminal event in a context of bivariate recurrent event data, a joint model is implemented with bivariate frailty effects. Simulation studies are performed to evaluate the suggested models. Infection data from AML (acute myeloid leukemia) patients are analyzed as an application.},
  archive      = {J_JOAS},
  author       = {Yang-Jin Kim},
  doi          = {10.1080/02664763.2020.1744539},
  journal      = {Journal of Applied Statistics},
  number       = {4},
  pages        = {738-749},
  shortjournal = {J. Appl. Stat.},
  title        = {Joint model for bivariate zero-inflated recurrent event data with terminal events},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A discrete analog of gumbel distribution: Properties,
parameter estimation and applications. <em>JOAS</em>, <em>48</em>(4),
712–737. (<a
href="https://doi.org/10.1080/02664763.2020.1744538">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A discrete version of the Gumbel distribution (Type-I Extreme Value distribution) has been derived by using the general approach of discretization of a continuous distribution. Important distributional and reliability properties have been explored. It has been shown that depending on the choice of parameters the proposed distribution can be positively or negatively skewed; possess long-tail(s). Log-concavity of the distribution and consequent results have been established. Estimation of parameters by method of maximum likelihood, method of moments, and method of proportions has been discussed. A method of checking model adequacy and regression type estimation based on empirical survival function has also been examined. A simulation study has been carried out to compare and check the efficacy of the three methods of estimations. The distribution has been applied to model three real count data sets from diverse application area namely, survival times in number of days, maximum annual floods data from Brazil and goal differences in English premier league, and the results show the relevance of the proposed distribution.},
  archive      = {J_JOAS},
  author       = {Subrata Chakraborty and Dhrubajyoti Chakravarty and Josmar Mazucheli and Wesley Bertoli},
  doi          = {10.1080/02664763.2020.1744538},
  journal      = {Journal of Applied Statistics},
  number       = {4},
  pages        = {712-737},
  shortjournal = {J. Appl. Stat.},
  title        = {A discrete analog of gumbel distribution: Properties, parameter estimation and applications},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust estimation using multivariate t innovations for
vector autoregressive models via ECM algorithm. <em>JOAS</em>,
<em>48</em>(4), 693–711. (<a
href="https://doi.org/10.1080/02664763.2020.1742297">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper considers the vector autoregressive model of order p, VAR( p ), with multivariate t error distributions, the latter being more prevalent in real life than the usual multivariate normal distribution. It is believed that the maximum-likelihood equations for the multivariate t distribution have convergence problem, hence we develop estimation procedures for VAR( p ) model using the normal mean–variance mixture representation of multivariate t distribution. The procedure relies on the computational ease available in Expectation Maximization-based algorithms. The estimators obtained are explicit functions of sample observations and therefore are easy to compute. Extensive simulation experiments show that the estimators have negligible bias and are considerably more efficient than an existing method that uses the least-squares error approach. It is shown that the proposed estimators are robust to plausible deviations from an assumed distribution and hence are more advantageous when compared with the other estimator. One real-life example is given for illustration purposes.},
  archive      = {J_JOAS},
  author       = {Uchenna C. Nduka and Tobias E. Ugah and Chinyeaka H. Izunobi},
  doi          = {10.1080/02664763.2020.1742297},
  journal      = {Journal of Applied Statistics},
  number       = {4},
  pages        = {693-711},
  shortjournal = {J. Appl. Stat.},
  title        = {Robust estimation using multivariate t innovations for vector autoregressive models via ECM algorithm},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Taylor quasi-likelihood for limited generalized linear
models. <em>JOAS</em>, <em>48</em>(4), 669–692. (<a
href="https://doi.org/10.1080/02664763.2020.1743650">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is a major research topic of limited generalized linear models, namely, generalized linear models with limited dependent variables. The models are developed in many research fields. However, quasi-likelihood estimation of the models is an unresolved issue, due to including limited dependent variables. We propose a novel quasi-likelihood, called Taylor quasi-likelihood, to handle with the unified estimation problem of the limited models. It is based on Taylor expansion of distribution function or likelihood function. We also extend the likelihood to a generalized version and an adaptive version and propose a distributed procedure to obtain the likelihood estimator. In low-dimensional setting, we give selection criteria for the proposed method and make arguments for the consistency and asymptotic normality of the estimator. In high-dimensional setting, we discuss feature selection and oracle properties of the proposed method. Simulation results confirm the advantages of the proposed method.},
  archive      = {J_JOAS},
  author       = {Guangbao Guo},
  doi          = {10.1080/02664763.2020.1743650},
  journal      = {Journal of Applied Statistics},
  number       = {4},
  pages        = {669-692},
  shortjournal = {J. Appl. Stat.},
  title        = {Taylor quasi-likelihood for limited generalized linear models},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Heavy or semi-heavy tail, that is the question.
<em>JOAS</em>, <em>48</em>(4), 646–668. (<a
href="https://doi.org/10.1080/02664763.2020.1738360">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While there has been considerable research on the analysis of extreme values and outliers by using heavy-tailed distributions, little is known about the semi-heavy-tailed behaviors of data when there are a few suspicious outliers. To address the situation where data are skewed possessing semi-heavy tails, we introduce two new skewed distribution families of the hyperbolic secant with exciting properties. We extend the semi-heavy-tailedness property of data to a linear regression model. In particular, we investigate the asymptotic properties of the ML estimators of the regression parameters when the error term has a semi-heavy-tailed distribution. We conduct simulation studies comparing the ML estimators of the regression parameters under various assumptions for the distribution of the error term. We also provide three real examples to show the priority of the semi-heavy-tailedness of the error term comparing to heavy-tailedness. Online supplementary materials for this article are available. All the new proposed models in this work are implemented by the shs R package, which can be found on the GitHub webpage.},
  archive      = {J_JOAS},
  author       = {Jamil Ownuk and Hossein Baghishani and Ahmad Nezakati},
  doi          = {10.1080/02664763.2020.1738360},
  journal      = {Journal of Applied Statistics},
  number       = {4},
  pages        = {646-668},
  shortjournal = {J. Appl. Stat.},
  title        = {Heavy or semi-heavy tail, that is the question},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Order restricted classical inference of a weibull multiple
step-stress model. <em>JOAS</em>, <em>48</em>(4), 623–645. (<a
href="https://doi.org/10.1080/02664763.2020.1736526">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a multiple step-stress model is designed and analyzed when the data are Type-I censored. Lifetime distributions of the experimental units at each stress level are assumed to follow a two-parameter Weibull distribution. Further, distributions under each of the stress levels are connected through a tampered failure-rate based model. In a step-stress experiment, as the stress level increases, the load on the experimental units increases and hence the mean lifetime is expected to be shortened. Taking this into account, the aim of this paper is to develop the order restricted inference of the model parameters of a multiple step-stress model based on the frequentist approach. An extensive simulation study has been carried out and two real data sets have been analyzed for illustrative purposes.},
  archive      = {J_JOAS},
  author       = {Ayan Pal and Sharmishtha Mitra and Debasis Kundu},
  doi          = {10.1080/02664763.2020.1736526},
  journal      = {Journal of Applied Statistics},
  number       = {4},
  pages        = {623-645},
  shortjournal = {J. Appl. Stat.},
  title        = {Order restricted classical inference of a weibull multiple step-stress model},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Prediction of tumour pathological subtype from genomic
profile using sparse logistic regression with random effects.
<em>JOAS</em>, <em>48</em>(4), 605–622. (<a
href="https://doi.org/10.1080/02664763.2020.1738358">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The purpose of this study is to highlight the application of sparse logistic regression models in dealing with prediction of tumour pathological subtypes based on lung cancer patients&#39; genomic information. We consider sparse logistic regression models to deal with the high dimensionality and correlation between genomic regions. In a hierarchical likelihood (HL) method, it is assumed that the random effects follow a normal distribution and its variance is assumed to follow a gamma distribution. This formulation considers ridge and lasso penalties as special cases. We extend the HL penalty to include a ridge penalty (called ‘HLnet’) in a similar principle of the elastic net penalty, which is constructed from lasso penalty. The results indicate that the HL penalty creates more sparse estimates than lasso penalty with comparable prediction performance, while HLnet and elastic net penalties have the best prediction performance in real data. We illustrate the methods in a lung cancer study.},
  archive      = {J_JOAS},
  author       = {Özlem Kaymaz and Khaled Alqahtani and Henry M. Wood and Arief Gusnanto},
  doi          = {10.1080/02664763.2020.1738358},
  journal      = {Journal of Applied Statistics},
  number       = {4},
  pages        = {605-622},
  shortjournal = {J. Appl. Stat.},
  title        = {Prediction of tumour pathological subtype from genomic profile using sparse logistic regression with random effects},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bayesian bandwidth estimation and semi-metric selection for
a functional partial linear model with unknown error density.
<em>JOAS</em>, <em>48</em>(4), 583–604. (<a
href="https://doi.org/10.1080/02664763.2020.1736527">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study examines the optimal selections of bandwidth and semi-metric for a functional partial linear model. Our proposed method begins by estimating the unknown error density using a kernel density estimator of residuals, where the regression function, consisting of parametric and nonparametric components, can be estimated by functional principal component and functional Nadayara-Watson estimators. The estimation accuracy of the regression function and error density crucially depends on the optimal estimations of bandwidth and semi-metric. A Bayesian method is utilized to simultaneously estimate the bandwidths in the regression function and kernel error density by minimizing the Kullback-Leibler divergence. For estimating the regression function and error density, a series of simulation studies demonstrate that the functional partial linear model gives improved estimation and forecast accuracies compared with the functional principal component regression and functional nonparametric regression. Using a spectroscopy dataset, the functional partial linear model yields better forecast accuracy than some commonly used functional regression models. As a by-product of the Bayesian method, a pointwise prediction interval can be obtained, and marginal likelihood can be used to select the optimal semi-metric.},
  archive      = {J_JOAS},
  author       = {Han Lin Shang},
  doi          = {10.1080/02664763.2020.1736527},
  journal      = {Journal of Applied Statistics},
  number       = {4},
  pages        = {583-604},
  shortjournal = {J. Appl. Stat.},
  title        = {Bayesian bandwidth estimation and semi-metric selection for a functional partial linear model with unknown error density},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Olympic rankings based on objective weighting schemes.
<em>JOAS</em>, <em>48</em>(3), 573–582. (<a
href="https://doi.org/10.1080/02664763.2020.1736525">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose an objective principal components weighting scheme for all-time Winter Olympic gold, silver and bronze medals based solely on the number of medals won. Our results suggest that the approximately equal weights be assigned (or the total medal counts be used regardless of color) if all of the three medal types are retained for ranking purposes. When the proposed methodology is tested against five alternative weighting schemes that have been suggested in the literature using the results for the 2010 Vancouver Winter Olympics, we find a significant agreement in the country rankings. Furthermore, our implementation of principal components variable reduction strategy results in the identification of silver as the best representative medal count for parsimonious Winter Olympics rankings.},
  archive      = {J_JOAS},
  author       = {Tomson Ogwang and Danny I. Cho},
  doi          = {10.1080/02664763.2020.1736525},
  journal      = {Journal of Applied Statistics},
  number       = {3},
  pages        = {573-582},
  shortjournal = {J. Appl. Stat.},
  title        = {Olympic rankings based on objective weighting schemes},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Generalized symmetrical partial linear model. <em>JOAS</em>,
<em>48</em>(3), 557–572. (<a
href="https://doi.org/10.1080/02664763.2020.1726301">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we propose a new model called generalized symmetrical partial linear model, based on the theory of generalized linear models and symmetrical distributions. In our model the response variable follows a symmetrical distribution such a normal, Student-t, power exponential, among others. Following the context of generalized linear models we consider replacing the traditional linear predictors by the more general predictors in whose case one covariate is related with the response variable in a non-parametric fashion, that we do not specified the parametric function. As an example, we could imagine a regression model in which the intercept term is believed to vary in time or geographical location. The backfitting algorithm is used for estimating the parameters of the proposed model. We perform a simulation study for assessing the behavior of the penalized maximum likelihood estimators. We use the quantile residuals for checking the assumption of the model. Finally, we analyzed real data set related with pH rivers in Ireland.},
  archive      = {J_JOAS},
  author       = {Julio Cezar Souza Vasconcelos and Cristian Villegas},
  doi          = {10.1080/02664763.2020.1726301},
  journal      = {Journal of Applied Statistics},
  number       = {3},
  pages        = {557-572},
  shortjournal = {J. Appl. Stat.},
  title        = {Generalized symmetrical partial linear model},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On the “optimal” density power divergence tuning parameter.
<em>JOAS</em>, <em>48</em>(3), 536–556. (<a
href="https://doi.org/10.1080/02664763.2020.1736524">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The density power divergence, indexed by a single tuning parameter α , has proved to be a very useful tool in minimum distance inference. The family of density power divergences provides a generalized estimation scheme which includes likelihood-based procedures (represented by choice α = 0 α = 0 α=0 for the tuning parameter) as a special case. However, under data contamination, this scheme provides several more stable choices for model fitting and analysis (provided by positive values for the tuning parameter α ). As larger values of α necessarily lead to a drop in model efficiency, determining the optimal value of α to provide the best compromise between model-efficiency and stability against data contamination in any real situation is a major challenge. In this paper, we provide a refinement of an existing technique with the aim of eliminating the dependence of the procedure on an initial pilot estimator. Numerical evidence is provided to demonstrate the very good performance of the method. Our technique has a general flavour, and we expect that similar tuning parameter selection algorithms will work well for other M-estimators, or any robust procedure that depends on the choice of a tuning parameter.},
  archive      = {J_JOAS},
  author       = {Sancharee Basak and Ayanendranath Basu and M. C. Jones},
  doi          = {10.1080/02664763.2020.1736524},
  journal      = {Journal of Applied Statistics},
  number       = {3},
  pages        = {536-556},
  shortjournal = {J. Appl. Stat.},
  title        = {On the ‘optimal’ density power divergence tuning parameter},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Higher order moments of the estimated tangency portfolio
weights. <em>JOAS</em>, <em>48</em>(3), 517–535. (<a
href="https://doi.org/10.1080/02664763.2020.1736523">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we consider the estimated weights of the tangency portfolio. We derive analytical expressions for the higher order non-central and central moments of these weights when the returns are assumed to be independently and multivariate normally distributed. Moreover, the expressions for mean, variance, skewness and kurtosis of the estimated weights are obtained in closed forms. Later, we complement our results with a simulation study where data from the multivariate normal and t -distributions are simulated, and the first four moments of estimated weights are computed by using the Monte Carlo experiment. It is noteworthy to mention that the distributional assumption of returns is found to be important, especially for the first two moments. Finally, through an empirical illustration utilizing returns of four financial indices listed in NASDAQ stock exchange, we observe the presence of time dynamics in higher moments.},
  archive      = {J_JOAS},
  author       = {Farrukh Javed and Stepan Mazur and Edward Ngailo},
  doi          = {10.1080/02664763.2020.1736523},
  journal      = {Journal of Applied Statistics},
  number       = {3},
  pages        = {517-535},
  shortjournal = {J. Appl. Stat.},
  title        = {Higher order moments of the estimated tangency portfolio weights},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Latin hypercube designs based on strong orthogonal arrays
and kriging modelling to improve the payload distribution of trains.
<em>JOAS</em>, <em>48</em>(3), 498–516. (<a
href="https://doi.org/10.1080/02664763.2020.1733943">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, computer experiments are used increasingly more to solve complex engineering and technological issues. Computer experiments are analysed through suitable metamodels acting as statistical interpolators of the simulated input-output data: Kriging is the most appropriate and widely used one. We optimise the braking performance of freight trains through computer experiments and Kriging modelling by focussing on the payload distribution along the train, so as to reduce the effects of in-train forces among wagons during a train emergency braking. One contribution of this manuscript is that to improve the freight train efficiency in terms of braking performance, we consider that the train is composed of several train sections with each one characterised by its own overall payload. A suitable Latin hypercube design is planned for the computer experiment that achieves excellent space-filling properties with a relatively low number of experimental runs. Kriging models with anisotropic covariance function are subsequently applied to assess which is the best payload distribution capable of reducting the in-train forces according to the specific train-set arrangement considered. The results are very satisfactory and confirm that our approach represents a valid method to be successfully applied by interested Railway Undertakings.},
  archive      = {J_JOAS},
  author       = {Nedka Dechkova Nikiforova and Rossella Berni and Gabriele Arcidiacono and Luciano Cantone and Pierpaolo Placidoli},
  doi          = {10.1080/02664763.2020.1733943},
  journal      = {Journal of Applied Statistics},
  number       = {3},
  pages        = {498-516},
  shortjournal = {J. Appl. Stat.},
  title        = {Latin hypercube designs based on strong orthogonal arrays and kriging modelling to improve the payload distribution of trains},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A non-parametric statistic for testing conditional
heteroscedasticity for unobserved component models. <em>JOAS</em>,
<em>48</em>(3), 471–497. (<a
href="https://doi.org/10.1080/02664763.2020.1732885">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When prediction intervals are constructed using unobserved component models (UCM), problems can arise due to the possible existence of components that may or may not be conditionally heteroscedastic. Accurate coverage depends on correctly identifying the source of the heteroscedasticity. Different proposals for testing heteroscedasticity have been applied to UCM; however, in most cases, these procedures are unable to identify the heteroscedastic component correctly. The main issue is that test statistics are affected by the presence of serial correlation, causing the distribution of the statistic under conditional homoscedasticity to remain unknown. We propose a nonparametric statistic for testing heteroscedasticity based on the well-known Wilcoxon&#39;s rank statistic. We study the asymptotic validation of the statistic and examine bootstrap procedures for approximating its finite sample distribution. Simulation results show an improvement in the size of the homoscedasticity tests and a power that is clearly comparable with the best alternative in the literature. We also apply the test on real inflation data. Looking for the presence of a conditionally heteroscedastic effect on the error terms, we arrive at conclusions that almost all cases are different than those given by the alternative test statistics presented in the literature.},
  archive      = {J_JOAS},
  author       = {Alejandro Rodriguez and Gabriel Pino and Rodrigo Herrera},
  doi          = {10.1080/02664763.2020.1732885},
  journal      = {Journal of Applied Statistics},
  number       = {3},
  pages        = {471-497},
  shortjournal = {J. Appl. Stat.},
  title        = {A non-parametric statistic for testing conditional heteroscedasticity for unobserved component models},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Post-randomization for controlling identification risk in
releasing microdata from general surveys. <em>JOAS</em>, <em>48</em>(3),
455–470. (<a
href="https://doi.org/10.1080/02664763.2020.1732310">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Before releasing survey data, statistical agencies usually perturb the original data to keep each survey unit&#39;s information confidential. One significant concern in releasing survey microdata is identity disclosure, which occurs when an intruder correctly identifies the records of a survey unit by matching the values of some key (or pseudo-identifying) variables. We examine a recently developed post-randomization method for a strict control of identification risks in releasing survey microdata. While that procedure well preserves the observed frequencies and hence statistical estimates in case of simple random sampling, we show that in general surveys, it may induce considerable bias in commonly used survey-weighted estimators. We propose a modified procedure that better preserves weighted estimates. The procedure is illustrated and empirically assessed with an application to a publicly available US Census Bureau data set.},
  archive      = {J_JOAS},
  author       = {Cheng Zhang and Tapan K. Nayak},
  doi          = {10.1080/02664763.2020.1732310},
  journal      = {Journal of Applied Statistics},
  number       = {3},
  pages        = {455-470},
  shortjournal = {J. Appl. Stat.},
  title        = {Post-randomization for controlling identification risk in releasing microdata from general surveys},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A distribution-free EWMA control chart for monitoring
time-between-events-and-amplitude data. <em>JOAS</em>, <em>48</em>(3),
434–454. (<a
href="https://doi.org/10.1080/02664763.2020.1729347">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many control charts have been developed for the simultaneous monitoring of the time interval T between successive occurrences of an event E and its magnitude X . All these TBEA (Time Between Events and Amplitude) control charts assume a known distribution for the random variables T and X . But, in practice, as it is rather difficult to know their actual distributions, proposing a distribution free approach could be a way to overcome this ‘distribution choice’ dilemma. For this reason, we propose in this paper a distribution free upper-sided EWMA (Exponentially Weighted Moving Average) type control chart, for simultaneously monitoring the time interval T and the magnitude X of an event. In order to investigate the performance of this control chart and obtain its run length properties, we also develop a specific method called ‘continuousify’ which, coupled with a classical Markov chain technique, allows to obtain reliable and replicable results. A numerical comparison shows that our distribution-free EWMA TBEA chart performs as the parametric Shewhart TBEA chart, but without the need to pre-specify any distribution. An illustrative example obtained from a French forest fire database is also provided to show the implementation of the proposed EWMA TBEA control chart.},
  archive      = {J_JOAS},
  author       = {Shu Wu and Philippe Castagliola and Giovanni Celano},
  doi          = {10.1080/02664763.2020.1729347},
  journal      = {Journal of Applied Statistics},
  number       = {3},
  pages        = {434-454},
  shortjournal = {J. Appl. Stat.},
  title        = {A distribution-free EWMA control chart for monitoring time-between-events-and-amplitude data},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Investigating emergent nested geographic structure in
consumer purchases: A bayesian dynamic multi-scale spatiotemporal
modeling approach. <em>JOAS</em>, <em>48</em>(3), 410–433. (<a
href="https://doi.org/10.1080/02664763.2020.1725810">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatial modeling of consumer response data has gained increased interest recently in the marketing literature. In this paper, we extend the (spatial) multi-scale model by incorporating both spatial and temporal dimensions in the dynamic multi-scale spatiotemporal modeling approach. Our empirical application with a US company’s catalog purchase data for the period 1997–2001 reveals a nested geographic market structure that spans geopolitical boundaries such as state borders. This structure identifies spatial clusters of consumers who exhibit similar spatiotemporal behavior, thus pointing to the importance of emergent geographic structure, emergent nested structure and dynamic patterns in multi-resolution methods. The multi-scale model also has better performance in estimation and prediction compared with several spatial and spatiotemporal models and uses a scalable and computationally efficient Markov chain Monte Carlo method that makes it suitable for analyzing large spatiotemporal consumer purchase datasets.},
  archive      = {J_JOAS},
  author       = {Xia Wang and Joseph Pancras and Dipak K. Dey},
  doi          = {10.1080/02664763.2020.1725810},
  journal      = {Journal of Applied Statistics},
  number       = {3},
  pages        = {410-433},
  shortjournal = {J. Appl. Stat.},
  title        = {Investigating emergent nested geographic structure in consumer purchases: A bayesian dynamic multi-scale spatiotemporal modeling approach},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Variables acceptance reliability sampling plan for items
subject to inverse gaussian degradation process. <em>JOAS</em>,
<em>48</em>(3), 393–409. (<a
href="https://doi.org/10.1080/02664763.2020.1723505">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Until now, in the literature, a variety of acceptance reliability sampling plans have been developed based on different life test plans. In most of the reliability sampling plans, the decision procedures to accept or reject the corresponding lot are developed based on the lifetimes of the items observed on tests, or the number of failures observed during a pre-specified testing time. However, frequently, the items are subject to degradation phenomena and, in these cases, the observed degradation level of the item can be used as a decision statistic. In this paper, we develop a variables acceptance sampling plan based on the information on the degradation process of the items, assuming that the degradation process follows the inverse Gaussian process. It is shown that the developed sampling plan improves the reliability performance of the items conditional on the acceptance in the test and that the lifetimes of items after the reliability sampling test are stochastically larger than those before the test. A study comparing the proposed degradation-based sampling plan with the conventional sampling plan which is based on a life test is also performed.},
  archive      = {J_JOAS},
  author       = {Ji Hwan Cha and F. G. Badía},
  doi          = {10.1080/02664763.2020.1723505},
  journal      = {Journal of Applied Statistics},
  number       = {3},
  pages        = {393-409},
  shortjournal = {J. Appl. Stat.},
  title        = {Variables acceptance reliability sampling plan for items subject to inverse gaussian degradation process},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Improving teeth aesthetics using a spatially
shared-parameters model for independent regular lattices. <em>JOAS</em>,
<em>48</em>(2), 373–392. (<a
href="https://doi.org/10.1080/02664763.2020.1724273">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An important feature in dentistry is teeth gloss. During an intervention, the doctor applies a resin and a polishing to achieve the lowest roughness and the highest gloss possible. This work aims to evaluate the effect of four polishing protocols in teeth surface roughness and gloss when combined with two different resins and eventually indicate the best combination (treatment). An atomic force microscope is used for measuring the in vitro roughness of a dental surface surrogate. We consider a shared parameters approach for linking the information carried by those two correlated variables. The model fitted to the gloss considers some features of the roughness, namely the information conveyed by a set of spatial structured random effects, specific to each treatment, and the within treatment variance, which allows interpreting how the heterogeneity and the variability of the surface roughness impacts a tooth gloss. The statistical model here developed is an alternative to the “traditional” two-way ANOVA used in dentistry journals. The results, using the recent R-NIMBLE package in R, show that variability characteristics of the surface&#39;s roughness are central for explaining differences among the gloss achieved after each treatment and not just the mean roughness of that surface.},
  archive      = {J_JOAS},
  author       = {Rui Martins and Jorge Caldeira and Inês Lopes and José João Mendes},
  doi          = {10.1080/02664763.2020.1724273},
  journal      = {Journal of Applied Statistics},
  number       = {2},
  pages        = {373-392},
  shortjournal = {J. Appl. Stat.},
  title        = {Improving teeth aesthetics using a spatially shared-parameters model for independent regular lattices},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A new regression model for bimodal data and applications in
agriculture. <em>JOAS</em>, <em>48</em>(2), 349–372. (<a
href="https://doi.org/10.1080/02664763.2020.1723503">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We define the odd log-logistic exponential Gaussian regression with two systematic components, which extends the heteroscedastic Gaussian regression and it is suitable for bimodal data quite common in the agriculture area. We estimate the parameters by the method of maximum likelihood. Some simulations indicate that the maximum-likelihood estimators are accurate. The model assumptions are checked through case deletion and quantile residuals. The usefulness of the new regression model is illustrated by means of three real data sets in different areas of agriculture, where the data present bimodality.},
  archive      = {J_JOAS},
  author       = {Julio Cezar Souza Vasconcelos and Gauss Moutinho Cordeiro and Edwin Moises Marcos Ortega and Édila Maria de Rezende},
  doi          = {10.1080/02664763.2020.1723503},
  journal      = {Journal of Applied Statistics},
  number       = {2},
  pages        = {349-372},
  shortjournal = {J. Appl. Stat.},
  title        = {A new regression model for bimodal data and applications in agriculture},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). How much reliable are the integrated “live” data? A
validation strategy proposal for the non-parametric micro statistical
matching. <em>JOAS</em>, <em>48</em>(2), 322–348. (<a
href="https://doi.org/10.1080/02664763.2020.1724272">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The integration of different data sources is a widely discussed topic among both the researchers and the Official Statistics. Integrating data helps to contain costs and time required by new data collections. The non-parametric micro Statistical Matching (SM) allows to integrate ‘live’ data resorting only to the observed information, potentially avoiding the misspecification bias and speeding the computational effort. Despite these pros, the assessment of the integration goodness when we use this method is not robust. Moreover, several applications comply with some commonly accepted practices which recommend e.g. to use the biggest data set as donor. We propose a validation strategy to assess the integration goodness. We apply it to investigate these practices and to explore how different combinations of the SM techniques and distance functions perform in terms of the reliability of the synthetic (complete) data set generated. The validation strategy takes advantage of the relation existing among the variables pre-and-post the integration. The results show that ‘the biggest, the best’ rule must not be considered mandatory anymore. Indeed, the integration goodness increases in relation to the variability of the matching variables rather than with respect to the dimensionality ratio between the recipient and the donor data set.},
  archive      = {J_JOAS},
  author       = {Riccardo D&#39;Alberto and Meri Raggi},
  doi          = {10.1080/02664763.2020.1724272},
  journal      = {Journal of Applied Statistics},
  number       = {2},
  pages        = {322-348},
  shortjournal = {J. Appl. Stat.},
  title        = {How much reliable are the integrated ‘live’ data? a validation strategy proposal for the non-parametric micro statistical matching},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bayesian hierarchical models for the prediction of
volleyball results. <em>JOAS</em>, <em>48</em>(2), 301–321. (<a
href="https://doi.org/10.1080/02664763.2020.1723506">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Statistical modelling of sports data has become more and more popular in the recent years and different types of models have been proposed to achieve a variety of objectives: from identifying the key characteristics which lead a team to win or lose to predicting the outcome of a game or the team rankings in national leagues. Although not as popular as football or basketball, volleyball is a team sport with both national and international level competitions in almost every country. However, there is almost no study investigating the prediction of volleyball game outcomes and team rankings in national leagues. We propose a Bayesian hierarchical model for the prediction of the rankings of volleyball national teams, which also allows to estimate the results of each match in the league. We consider two alternative model specifications of different complexity which are validated using data from the women&#39;s volleyball Italian Serie A1 2017–2018 season.},
  archive      = {J_JOAS},
  author       = {Andrea Gabrio},
  doi          = {10.1080/02664763.2020.1723506},
  journal      = {Journal of Applied Statistics},
  number       = {2},
  pages        = {301-321},
  shortjournal = {J. Appl. Stat.},
  title        = {Bayesian hierarchical models for the prediction of volleyball results},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Reliability of a soccer player based on the bivariate
rayleigh distribution with right censored and ignorable missing data.
<em>JOAS</em>, <em>48</em>(2), 285–300. (<a
href="https://doi.org/10.1080/02664763.2020.1723504">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study the performance of a soccer player based on analysing an incomplete data set. To achieve this aim, we fit the bivariate Rayleigh distribution to the soccer dataset by the maximum likelihood method. In this way, the missing data and right censoring problems, that usually happen in such studies, are considered. Our aim is to inference about the performance of a soccer player by considering the stress and strength components. The first goal of the player of interest in a match is assumed as the stress component and the second goal of the match is assumed as the strength component. We propose some methods to overcome incomplete data problem and we use these methods to inference about the performance of a soccer player.},
  archive      = {J_JOAS},
  author       = {Fayyaz Bahari and Safar Parsi and Mojtaba Ganjali},
  doi          = {10.1080/02664763.2020.1723504},
  journal      = {Journal of Applied Statistics},
  number       = {2},
  pages        = {285-300},
  shortjournal = {J. Appl. Stat.},
  title        = {Reliability of a soccer player based on the bivariate rayleigh distribution with right censored and ignorable missing data},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Monitoring time and magnitude based on the renewal reward
process with a random failure threshold. <em>JOAS</em>, <em>48</em>(2),
247–284. (<a
href="https://doi.org/10.1080/02664763.2020.1723502">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Control charts are effective tools to distinguish between special and natural variations and have applications in medical and business industries besides manufacturing industry. Due to the advancement of modern technology, we often deal with the high-quality products, where the traditional process monitoring techniques have certain drawbacks. This article presents a control chart for jointly monitoring time and magnitude based on the renewal reward process. In particular, the focus of this study is to model magnitudes by threshold exceedance. More specifically, assuming a random failure threshold, two cases for magnitude are considered: (i) magnitude is cumulative over time and, (ii) magnitude is non-cumulative or independent over time. A comparative study to show the effectiveness of the proposal is also a part of this study.},
  archive      = {J_JOAS},
  author       = {Sajid Ali},
  doi          = {10.1080/02664763.2020.1723502},
  journal      = {Journal of Applied Statistics},
  number       = {2},
  pages        = {247-284},
  shortjournal = {J. Appl. Stat.},
  title        = {Monitoring time and magnitude based on the renewal reward process with a random failure threshold},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Outlier detection and robust variable selection via the
penalized weighted LAD-LASSO method. <em>JOAS</em>, <em>48</em>(2),
234–246. (<a
href="https://doi.org/10.1080/02664763.2020.1722079">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies the outlier detection and robust variable selection problem in the linear regression model. The penalized weighted least absolute deviation (PWLAD) regression estimation method and the adaptive least absolute shrinkage and selection operator (LASSO) are combined to simultaneously achieve outlier detection, and robust variable selection. An iterative algorithm is proposed to solve the proposed optimization problem. Monte Carlo studies are evaluated the finite-sample performance of the proposed methods. The results indicate that the finite sample performance of the proposed methods performs better than that of the existing methods when there are leverage points or outliers in the response variable or explanatory variables. Finally, we apply the proposed methodology to analyze two real datasets.},
  archive      = {J_JOAS},
  author       = {Yunlu Jiang and Yan Wang and Jiantao Zhang and Baojian Xie and Jibiao Liao and Wenhui Liao},
  doi          = {10.1080/02664763.2020.1722079},
  journal      = {Journal of Applied Statistics},
  number       = {2},
  pages        = {234-246},
  shortjournal = {J. Appl. Stat.},
  title        = {Outlier detection and robust variable selection via the penalized weighted LAD-LASSO method},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust principal component analysis for compositional
tables. <em>JOAS</em>, <em>48</em>(2), 214–233. (<a
href="https://doi.org/10.1080/02664763.2020.1722078">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A data table arranged according to two factors can often be considered a compositional table. An example is the number of unemployed people, split according to gender and age classes. Analyzed as compositions, the relevant information consists of ratios between different cells of such a table. This is particularly useful when analyzing several compositional tables jointly, where the absolute numbers are in very different ranges, e.g. if unemployment data are considered from different countries. Within the framework of the logratio methodology, compositional tables can be decomposed into independent and interactive parts, and orthonormal coordinates can be assigned to these parts. However, these coordinates usually require some prior knowledge about the data, and they are not easy to handle for exploring the relationships between the given factors. Here we propose a special choice of coordinates with direct relation to centered logratio (clr) coefficients, which are particularly useful for an interpretation in terms of the original cells of the tables. With these coordinates, robust principal component analysis (rPCA) is performed for dimension reduction, allowing to investigate relationships between the factors. The link between orthonormal coordinates and clr coefficients enables to apply rPCA, which would otherwise suffer from the singularity of clr coefficients.},
  archive      = {J_JOAS},
  author       = {J. de Sousa and K. Hron and K. Fačevicová and P. Filzmoser},
  doi          = {10.1080/02664763.2020.1722078},
  journal      = {Journal of Applied Statistics},
  number       = {2},
  pages        = {214-233},
  shortjournal = {J. Appl. Stat.},
  title        = {Robust principal component analysis for compositional tables},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Estimating the turning point of the log-logistic hazard
function in the presence of long-term survivors with an application for
uterine cervical cancer data. <em>JOAS</em>, <em>48</em>(2), 203–213.
(<a href="https://doi.org/10.1080/02664763.2020.1720627">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The hazard function plays an important role in cancer patient survival studies, as it quantifies the instantaneous risk of death of a patient at any given time. Often in cancer clinical trials, unimodal hazard functions are observed, and it is of interest to detect (estimate) the turning point (mode) of hazard function, as this may be an important measure in patient treatment strategies with cancer. Moreover, when patient cure is a possibility, estimating cure rates at different stages of cancer, in addition to their proportions, may provide a better summary of the effects of stages on survival rates. Therefore, the main objective of this paper is to consider the problem of estimating the mode of hazard function of patients at different stages of cervical cancer in the presence of long-term survivors. To this end, a mixture cure rate model is proposed using the log-logistic distribution. The model is conveniently parameterized through the mode of the hazard function, in which cancer stages can affect both the cured fraction and the mode. In addition, we discuss aspects of model inference through the maximum likelihood estimation method. A Monte Carlo simulation study assesses the coverage probability of asymptotic confidence intervals.},
  archive      = {J_JOAS},
  author       = {Patrick Borges},
  doi          = {10.1080/02664763.2020.1720627},
  journal      = {Journal of Applied Statistics},
  number       = {2},
  pages        = {203-213},
  shortjournal = {J. Appl. Stat.},
  title        = {Estimating the turning point of the log-logistic hazard function in the presence of long-term survivors with an application for uterine cervical cancer data},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Is there a causal relationship between oil prices and
tourist arrivals? <em>JOAS</em>, <em>48</em>(1), 191–202. (<a
href="https://doi.org/10.1080/02664763.2020.1720625">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This application note investigates the causal relationship between oil price and tourist arrivals to further explain the impact of oil price volatility on tourism-related economic activities. The analysis itself considers the time domain, frequency domain and information theory domain perspectives. Data relating to US and nine European countries are exploited in this paper with causality tests which include time domain, frequency domain, and Convergent Cross Mapping (CCM). The CCM approach is nonparametric and therefore not restricted by assumptions. We contribute to existing research through the successful and introductory application of an advanced method, and via the uncovering of significant causal links from oil prices to tourist arrivals.},
  archive      = {J_JOAS},
  author       = {Hossein Hassani and Mansi Ghodsi and Xu Huang and Emmanuel Sirimal Silva},
  doi          = {10.1080/02664763.2020.1720625},
  journal      = {Journal of Applied Statistics},
  number       = {1},
  pages        = {191-202},
  shortjournal = {J. Appl. Stat.},
  title        = {Is there a causal relationship between oil prices and tourist arrivals?},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Modelling the reproductive power function. <em>JOAS</em>,
<em>48</em>(1), 176–190. (<a
href="https://doi.org/10.1080/02664763.2020.1716696">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper discusses methods of estimating the reproductive power and the accompanying survival function of communicable events, e.g. infectious disease transmission. The early stage of an outbreak can be described by the infectiousness of the outbreak process, but in later stages of the outbreak, this is complicated by factors such as changing contact patterns and the impact of control measures. It is important to take these factors into account in order to get a good, if approximate, model for an outbreak process. This paper proposes a non-homogeneous birth process and regression model for the reproductive power function, similar to models in discrete survival analysis. A baseline reproductive power function gives a description of the outbreak when covariates are at their baseline values. As an illustration these methods are applied to an avian influenza (H5N1) outbreak among poultry in Thailand.},
  archive      = {J_JOAS},
  author       = {Jan van den Broek},
  doi          = {10.1080/02664763.2020.1716696},
  journal      = {Journal of Applied Statistics},
  number       = {1},
  pages        = {176-190},
  shortjournal = {J. Appl. Stat.},
  title        = {Modelling the reproductive power function},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A boosting inspired personalized threshold method for sepsis
screening. <em>JOAS</em>, <em>48</em>(1), 154–175. (<a
href="https://doi.org/10.1080/02664763.2020.1716695">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sepsis is one of the biggest risks to patient safety, with a natural mortality rate between 25\% and 50\%. It is difficult to diagnose, and no validated standard for diagnosis currently exists. A commonly used scoring criteria is the quick sequential organ failure assessment (qSOFA). It demonstrates very low specificity in ICU populations, however. We develop a method to personalize thresholds in qSOFA that incorporates easily to measure patient baseline characteristics. We compare the personalized threshold method to qSOFA, five previously published methods that obtain an optimal constant threshold for a single biomarker, and to the machine learning algorithms based on logistic regression and AdaBoosting using patient data in the MIMIC-III database. The personalized threshold method achieves higher accuracy than qSOFA and the five published methods and has comparable performance to machine learning methods. Personalized thresholds, however, are much easier to adopt in real-life monitoring than machine learning methods as they are computed once for a patient and used in the same way as qSOFA, whereas the machine learning methods are hard to implement and interpret.},
  archive      = {J_JOAS},
  author       = {Chen Feng and Paul Griffin and Shravan Kethireddy and Yajun Mei},
  doi          = {10.1080/02664763.2020.1716695},
  journal      = {Journal of Applied Statistics},
  number       = {1},
  pages        = {154-175},
  shortjournal = {J. Appl. Stat.},
  title        = {A boosting inspired personalized threshold method for sepsis screening},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A mixed control chart for monitoring failure times under
accelerated hybrid censoring. <em>JOAS</em>, <em>48</em>(1), 138–153.
(<a href="https://doi.org/10.1080/02664763.2020.1713060">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In an accelerated hybrid censoring scheme several stress factors can be accelerated to make the products to respond to fail more quickly than under normal operating conditions. In such situations, the control charts available in the literature cover the attribute characteristics only to monitor the performance of the process over time. This study extends the idea by proposing an optimal mixed attribute-variable control chart for Weibull distribution under an accelerated hybrid censoring scheme keeping the advantages of both attribute and variable control charts. It first monitors the number of defectives under accelerated conditions and switches to the variable control chart to investigate the mean failure times when the process stability is dubious. The performance of the proposed chart is evaluated by using run-length characteristics, and the optimality of the design parameter is achieved by minimizing the out-of-control average run length. The simulation study depicted better performance of the proposed control chart than the traditional charts in detecting shifts in the process. A real-life application is also included.},
  archive      = {J_JOAS},
  author       = {Muhammad Aslam and Muhammad Ali Raza and Rehan Ahmad Khan Sherwani and Muhammad Farooq and Jun Yong Jeong and Chi-Hyuck Jun},
  doi          = {10.1080/02664763.2020.1713060},
  journal      = {Journal of Applied Statistics},
  number       = {1},
  pages        = {138-153},
  shortjournal = {J. Appl. Stat.},
  title        = {A mixed control chart for monitoring failure times under accelerated hybrid censoring},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The cosine geometric distribution with count data modeling.
<em>JOAS</em>, <em>48</em>(1), 124–137. (<a
href="https://doi.org/10.1080/02664763.2019.1711364">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a new two-parameter discrete distribution is introduced. It belongs to the family of the weighted geometric distribution (GD), with the feature of using a particular trigonometric weight. This configuration adds an oscillating property to the former GD which can be helpful in analyzing the data with over-dispersion, as developed in this study. First, we present the basic statistical properties of the new distribution, including the cumulative distribution function, hazard rate function and moment generating function. Estimation of the related model parameters is investigated using the maximum likelihood method. A simulation study is performed to illustrate the convergence of the estimators. Applications to two practical datasets are given to show that the new model performs at least as well as some competitors.},
  archive      = {J_JOAS},
  author       = {Christophe Chesneau and Hassan S. Bakouch and Tassaddaq Hussain and Bilal A. Para},
  doi          = {10.1080/02664763.2019.1711364},
  journal      = {Journal of Applied Statistics},
  number       = {1},
  pages        = {124-137},
  shortjournal = {J. Appl. Stat.},
  title        = {The cosine geometric distribution with count data modeling},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A statistical framework for measuring the temporal stability
of human mobility patterns. <em>JOAS</em>, <em>48</em>(1), 105–123. (<a
href="https://doi.org/10.1080/02664763.2019.1711363">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the growing popularity of human mobility studies that collect GPS location data, the problem of determining the minimum required length of GPS monitoring has not been addressed in the current statistical literature. In this paper, we tackle this problem by laying out a theoretical framework for assessing the temporal stability of human mobility based on GPS location data. We define several measures of the temporal dynamics of human spatiotemporal trajectories based on the average velocity process, and on activity distributions in a spatial observation window. We demonstrate the use of our methods with data that comprise the GPS locations of 185 individuals over the course of 18 months. Our empirical results suggest that GPS monitoring should be performed over periods of time that are significantly longer than what has been previously suggested. Furthermore, we argue that GPS study designs should take into account demographic groups.},
  archive      = {J_JOAS},
  author       = {Zhihang Dong and Yen-Chi Chen and Adrian Dobra},
  doi          = {10.1080/02664763.2019.1711363},
  journal      = {Journal of Applied Statistics},
  number       = {1},
  pages        = {105-123},
  shortjournal = {J. Appl. Stat.},
  title        = {A statistical framework for measuring the temporal stability of human mobility patterns},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An automatic robust bayesian approach to principal component
regression. <em>JOAS</em>, <em>48</em>(1), 84–104. (<a
href="https://doi.org/10.1080/02664763.2019.1710478">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Principal component regression uses principal components (PCs) as regressors. It is particularly useful in prediction settings with high-dimensional covariates. The existing literature treating of Bayesian approaches is relatively sparse. We introduce a Bayesian approach that is robust to outliers in both the dependent variable and the covariates. Outliers can be thought of as observations that are not in line with the general trend. The proposed approach automatically penalises these observations so that their impact on the posterior gradually vanishes as they move further and further away from the general trend, corresponding to a concept in Bayesian statistics called whole robustness . The predictions produced are thus consistent with the bulk of the data. The approach also exploits the geometry of PCs to efficiently identify those that are significant. Individual predictions obtained from the resulting models are consolidated according to model-averaging mechanisms to account for model uncertainty. The approach is evaluated on real data and compared to its nonrobust Bayesian counterpart, the traditional frequentist approach and a commonly employed robust frequentist method. Detailed guidelines to automate the entire statistical procedure are provided. All required code is made available, see ArXiv:1711.06341 .},
  archive      = {J_JOAS},
  author       = {Philippe Gagnon and Mylène Bédard and Alain Desgagné},
  doi          = {10.1080/02664763.2019.1710478},
  journal      = {Journal of Applied Statistics},
  number       = {1},
  pages        = {84-104},
  shortjournal = {J. Appl. Stat.},
  title        = {An automatic robust bayesian approach to principal component regression},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Parameter estimation of cambanis-type bivariate uniform
distribution with ranked set sampling. <em>JOAS</em>, <em>48</em>(1),
61–83. (<a href="https://doi.org/10.1080/02664763.2019.1709808">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The concept of ranked set sampling (RSS) is applicable whenever ranking on a set of sampling units can be done easily using a judgment method or based on an auxiliary variable. In this paper, we consider a study variable Y Y Y correlated with the auxiliary variable X X X and use it to rank the sampling units. Further ( X , Y ) ( X , Y ) (X,Y) is assumed to have Cambanis-type bivariate uniform (CTBU) distribution. We obtain an unbiased estimator of a scale parameter associated with the study variable Y Y Y based on different RSS schemes. We perform the efficiency comparison of the proposed estimators numerically. We present the trends in the efficiency performance of estimators under various RSS schemes with respect to parameters through line and surface plots. Further, we develop a Matlab function to simulate data from CTBU distribution and present the performance of proposed estimators through a simulation study. The results developed are implemented to real-life data also.},
  archive      = {J_JOAS},
  author       = {Rohan D. Koshti and Kirtee K. Kamalja},
  doi          = {10.1080/02664763.2019.1709808},
  journal      = {Journal of Applied Statistics},
  number       = {1},
  pages        = {61-83},
  shortjournal = {J. Appl. Stat.},
  title        = {Parameter estimation of cambanis-type bivariate uniform distribution with ranked set sampling},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A continuous-time markov model for estimating readmission
risk for hospital inpatients. <em>JOAS</em>, <em>48</em>(1), 41–60. (<a
href="https://doi.org/10.1080/02664763.2019.1709810">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Research concerning hospital readmissions has mostly focused on statistical and machine learning models that attempt to predict this unfortunate outcome for individual patients. These models are useful in certain settings, but their performance in many cases is insufficient for implementation in practice, and the dynamics of how readmission risk changes over time is often ignored. Our objective is to develop a model for aggregated readmission risk over time – using a continuous-time Markov chain – beginning at the point of discharge. We derive point and interval estimators for readmission risk, and find the asymptotic distributions for these probabilities. Finally, we validate our derived estimators using simulation, and apply our methods to estimate readmission risk over time using discharge and readmission data for surgical patients.},
  archive      = {J_JOAS},
  author       = {Xu Zhang and Sean Barnes and Bruce Golden and Paul Smith},
  doi          = {10.1080/02664763.2019.1709810},
  journal      = {Journal of Applied Statistics},
  number       = {1},
  pages        = {41-60},
  shortjournal = {J. Appl. Stat.},
  title        = {A continuous-time markov model for estimating readmission risk for hospital inpatients},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). EWMA and DEWMA repetitive control charts under non-normal
processes. <em>JOAS</em>, <em>48</em>(1), 4–40. (<a
href="https://doi.org/10.1080/02664763.2019.1709809">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present a repetitive sampling method to construct control charts using exponentially weighted moving averages (EWMA) and double exponentially weighted moving averages (DEWMA) to monitor shift in the process. For non-normal processes, t -distribution with various degrees of freedom (i.e. df = 4 , 10 , 20 , 40 , 50 df = 4 , 10 , 20 , 40 , 50 df=4,10,20,40,50 ) is used as symmetric distribution, gamma distribution with unit scale parameter and various shape parameters (i.e. 0.5 , 1 , 2 , 3 , 4 0.5 , 1 , 2 , 3 , 4 0.5,1,2,3,4 ) is used as positively skewed distribution and Weibull distribution with unit scale parameter and various shape parameters (i.e. 10 and 20) is used as negatively skewed distribution. We use Monte Carlo simulations to check whether the process is out of control. We use average run length as a tool to find the ability of proposed control charts to identify a shift earlier in a process, as compared to other control charts currently used to monitor the same type of process. The proposed control charts are applied to two real datasets.},
  archive      = {J_JOAS},
  author       = {Muhammad Shujaat Nawaz and Muhammad Azam and Muhammad Aslam},
  doi          = {10.1080/02664763.2019.1709809},
  journal      = {Journal of Applied Statistics},
  number       = {1},
  pages        = {4-40},
  shortjournal = {J. Appl. Stat.},
  title        = {EWMA and DEWMA repetitive control charts under non-normal processes},
  volume       = {48},
  year         = {2021},
}
</textarea>
</details></li>
</ul>

</body>
</html>
