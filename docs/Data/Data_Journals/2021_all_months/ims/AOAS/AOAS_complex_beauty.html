<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>AOAS_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="aoas---99">AOAS - 99</h2>
<ul>
<li><details>
<summary>
(2021). Predicting competitions by combining conditional logistic
regression and subjective bayes: An academy awards case study.
<em>AOAS</em>, <em>15</em>(4), 2083–2100. (<a
href="https://doi.org/10.1214/21-AOAS1464">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting the outcome of elections, sporting events, entertainment awards and other competitions has long captured the human imagination. Such prediction is growing in sophistication in these areas, especially in the rapidly growing field of data-driven journalism intended for a general audience as the availability of historical information rapidly balloons. Providing statistical methodology to probabilistically predict competition outcomes faces two main challenges. First, a suitably general modeling approach is necessary to assign probabilities to competitors. Second, the modeling framework must be able to accommodate expert opinion which is usually available but difficult to fully encapsulate in typical data sets. We overcome these challenges with a combined conditional logistic regression/subjective Bayes approach. To illustrate the method, we reanalyze data from a recent Time.com piece in which the authors attempted to predict the 2019 Best Picture Academy Award winner using standard logistic regression. Toward engaging and educating a broad readership, we discuss strategies to deploy the proposed method via an online application.},
  archive      = {J_AOAS},
  author       = {Christopher T. Franck and Christopher E. Wilson},
  doi          = {10.1214/21-AOAS1464},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {2083-2100},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Predicting competitions by combining conditional logistic regression and subjective bayes: An academy awards case study},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Mediation analysis for associations of categorical
variables: The role of education in social class mobility in britain.
<em>AOAS</em>, <em>15</em>(4), 2061–2082. (<a
href="https://doi.org/10.1214/21-AOAS1467">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We analyse levels and trends of intergenerational social class mobility among three postwar birth cohorts in Britain and examine how much of the observed mobility or immobility in them could be accounted for by existing differences in educational attainment between people from different class backgrounds. We propose for this purpose a method which quantifies associations between categorical variables when we compare groups which differ only in the distribution of a mediating variable, such as education. This is analogous to estimation of indirect effects in causal mediation analysis but is here developed to define and estimate population associations of variables. We propose estimators for these associations which depend only on fitted values from models for the mediator and outcome variables, and propose variance estimators for them. The analysis shows that the part that differences in education play in intergenerational class mobility is by no means so dominant as has been supposed and that, while it varies with gender and with particular mobility transitions, it shows no tendency to change over time.},
  archive      = {J_AOAS},
  author       = {Jouni Kuha and Erzsébet Bukodi and John H. Goldthorpe},
  doi          = {10.1214/21-AOAS1467},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {2061-2082},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Mediation analysis for associations of categorical variables: The role of education in social class mobility in britain},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Inferring food intake from multiple biomarkers using a
latent variable model. <em>AOAS</em>, <em>15</em>(4), 2043–2060. (<a
href="https://doi.org/10.1214/21-AOAS1478">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Metabolomic-based approaches have gained much attention in recent years, due to their promising potential to deliver objective tools for assessment of food intake. In particular, multiple biomarkers have emerged for single foods. However, there is a lack of statistical tools available for combining multiple biomarkers to quantitatively infer food intake. Furthermore, there is a paucity of approaches for estimating the uncertainty around biomarker-based inferred intake. Here, to estimate the relationship between multiple metabolomic biomarkers and food intake in an intervention study conducted under the A-DIET research programme, a latent variable model, multiMarker, is proposed. The multiMarker model integrates factor analytic and mixture of experts models: the observed biomarker values are related to intake which is described as a continuous latent variable which follows a flexible mixture of experts model with Gaussian components. The multiMarker model also facilitates inference on the latent intake when only biomarker data are subsequently observed. A Bayesian hierarchical modelling framework provides flexibility to adapt to different biomarker distributions and facilitates inference of the latent intake along with its associated uncertainty. Simulation studies are conducted to assess the performance of the multiMarker model, prior to its application to the motivating application of quantifying apple intake.},
  archive      = {J_AOAS},
  author       = {Silvia D’Angelo and Lorraine Brennan and Isobel Claire Gormley},
  doi          = {10.1214/21-AOAS1478},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {2043-2060},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Inferring food intake from multiple biomarkers using a latent variable model},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The information in covariate imbalance in studies of hormone
replacement therapy. <em>AOAS</em>, <em>15</em>(4), 2023–2042. (<a
href="https://doi.org/10.1214/21-AOAS1448">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A widely noted failure of causal inference occurred when several observational studies claimed that hormone replacement therapy (HRT) reduced risk of cardiovascular disease; yet, subsequent randomized trials found an increased, not a decreased, cardiovascular risk. We take a close look at covariate imbalances in one of the observational data sets. We use some old, some recent, and some new methods, plus we update an important, simple but largely forgotten suggestion of William Cochran about screening covariates and other variables. In particular, a tapered match shows the impact on all covariates of gradually matching for additional covariates. An exterior match examines the change in the control group as additional covariates are included, and the consequences for outcomes. Because covariates are sometimes continuous, sometimes binary, sometimes ordinal, sometimes missing, we suggest keeping track of magnitudes of aggregate bias in observed covariates using a new estimate of the Kullback–Leibler information between covariate distributions in treated and matched control groups, a flexible measure with several attractive properties. The initial studies ignored some enormous imbalances in socioeconomic covariates that predict the outcomes under study. Our more comprehensive analyses mimic some post-game reanalyses done subsequent to the randomized trials; however, even these omit a large imbalance in a consequential covariate discovered by Cochran’s quick but expansive screening suggestion. Our sense is that a closer examination of covariate imbalance would not have led to a correct conclusion about the effects of HRT, but it would have heightened concerns about the magnitude of the problems in the observational studies, and it would have raised doubts about the ability of a few regression coefficients to eliminate all biases, observed and unobserved, in the comparison. Medical journals need to recognize that certain sources of uncertainty cannot be eliminated from certain necessary types of empirical investigation; moreover, these journals need to learn new ways to describe these sources of uncertainty with objectivity and candor.},
  archive      = {J_AOAS},
  author       = {Ruoqi Yu and Dylan S. Small and Paul R. Rosenbaum},
  doi          = {10.1214/21-AOAS1448},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {2023-2042},
  shortjournal = {Ann. Appl. Stat.},
  title        = {The information in covariate imbalance in studies of hormone replacement therapy},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Analysing the causal effect of london cycle superhighways on
traffic congestion. <em>AOAS</em>, <em>15</em>(4), 1999–2022. (<a
href="https://doi.org/10.1214/21-AOAS1450">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transport operators have a range of intervention options available to improve or enhance their networks. Such interventions are often made in the absence of sound evidence on resulting outcomes. Cycling superhighways were promoted as a sustainable and healthy travel mode, one of the aims of which was to reduce traffic congestion. Estimating the impacts that cycle superhighways have on congestion is complicated due to the nonrandom assignment of such intervention over the transport network. In this paper we analyse the causal effect of cycle superhighways utilising preintervention and postintervention information on traffic and road characteristics along with socioeconomic factors. We propose a modeling framework based on the propensity score and outcome regression model. The method is also extended to the doubly robust set-up. Simulation results show the superiority of the performance of the proposed method over existing competitors. The method is applied to analyse a real dataset on the London transport network. The methodology proposed can assist in effective decision making to improve network performance.},
  archive      = {J_AOAS},
  author       = {Prajamitra Bhuyan and Emma J. McCoy and Haojie Li and Daniel J. Graham},
  doi          = {10.1214/21-AOAS1450},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {1999-2022},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Analysing the causal effect of london cycle superhighways on traffic congestion},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Integrating geostatistical maps and infectious disease
transmission models using adaptive multiple importance sampling.
<em>AOAS</em>, <em>15</em>(4), 1980–1998. (<a
href="https://doi.org/10.1214/21-AOAS1486">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Adaptive Multiple Importance Sampling algorithm (AMIS) is an iterative technique which recycles samples from all previous iterations in order to improve the efficiency of the proposal distribution. We have formulated a new statistical framework, based on AMIS, to take the output from a geostatistical model of infectious disease prevalence, incidence or relative risk, and project it forward in time under a mathematical model for transmission dynamics. We adapted the AMIS algorithm so that it can sample from multiple targets simultaneously by changing the focus of the adaptation at each iteration. By comparing our approach against the standard AMIS algorithm, we showed that these novel adaptations greatly improve the efficiency of the sampling. We tested the performance of our algorithm on four case studies: ascariasis in Ethiopia, onchocerciasis in Togo, human immunodeficiency virus (HIV) in Botswana, and malaria in the Democratic Republic of the Congo.},
  archive      = {J_AOAS},
  author       = {Renata Retkute and Panayiota Touloupou and María-Gloria Basáñez and T. Déirdre Hollingsworth and Simon E. F. Spencer},
  doi          = {10.1214/21-AOAS1486},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {1980-1998},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Integrating geostatistical maps and infectious disease transmission models using adaptive multiple importance sampling},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Space-time smoothing models for subnational measles routine
immunization coverage estimation with complex survey data.
<em>AOAS</em>, <em>15</em>(4), 1959–1979. (<a
href="https://doi.org/10.1214/21-AOAS1474">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite substantial advances in global measles vaccination, measles disease burden remains high in many low- and middle-income countries. A key public health strategy for controlling measles in such high-burden settings is to conduct supplementary immunization activities (SIAs) in the form of mass vaccination campaigns, in addition to delivering scheduled vaccination through routine immunization (RI) programs. To achieve balanced implementations of RI and SIAs, robust measurement of subnational RI-specific coverage is crucial. In this paper we develop a space–time smoothing model for estimating RI-specific coverage of the first dose of measles-containing-vaccines (MCV1) at subnational level using complex survey data. The application that motivated this work is estimation of the RI-specific MCV1 coverage in Nigeria’s 36 states and the Federal Capital Territory. Data come from four demographic and health surveys, three multiple indicator cluster surveys and two national nutrition and health surveys conducted in Nigeria between 2003 and 2018. Our method incorporates information from the SIA calendar published by the World Health Organization and accounts for the impact of SIAs on the overall MCV1 coverage, as measured by cross-sectional surveys. The model can be used to analyze data from multiple surveys with different data collection schemes and construct coverage estimates with uncertainty that reflects the various sampling designs. Implementation of our method can be done efficiently using integrated nested Laplace approximation (INLA).},
  archive      = {J_AOAS},
  author       = {Tracy Qi Dong and Jon Wakefield},
  doi          = {10.1214/21-AOAS1474},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {1959-1979},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Space-time smoothing models for subnational measles routine immunization coverage estimation with complex survey data},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Pan-disease clustering analysis of the trend of period
prevalence. <em>AOAS</em>, <em>15</em>(4), 1945–1958. (<a
href="https://doi.org/10.1214/21-AOAS1470">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prevalence is of essential importance in biomedical and public health research. In the “classic” paradigm it has been studied for each disease individually. Accumulating evidence has shown that diseases can be “correlated.” Joint analysis of prevalence can potentially provide important insights beyond individual-disease analysis but has not been well pursued. In this study we take advantage of the unique Taiwan National Health Insurance Research Database (NHIRD) and conduct the first pan-disease analysis of period prevalence trend. The goal is to identify clusters within which diseases have similar period prevalence trends. A novel penalization pursuit approach is applied which has an intuitive formulation and preferable numerical performance. In data analysis the period prevalence values are computed using the records on close to one million subjects and 14 years of observation. With 405 diseases, 35 clusters with sizes larger than one and 27 clusters with sizes one are identified. The clustering results have sound interpretations and differ significantly from those of the alternatives.},
  archive      = {J_AOAS},
  author       = {Sneha Jadhav and Chenjin Ma and Yefei Jiang and Ben-Chang Shia and Shuangge Ma},
  doi          = {10.1214/21-AOAS1470},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {1945-1958},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Pan-disease clustering analysis of the trend of period prevalence},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Modeling the social media relationships of irish politicians
using a generalized latent space stochastic blockmodel. <em>AOAS</em>,
<em>15</em>(4), 1923–1944. (<a
href="https://doi.org/10.1214/21-AOAS1483">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dáil Éireann is the principal chamber of the Irish parliament. The 31st Dáil was in session from March 11th, 2011 to February 6th, 2016. Many of the members of the Dáil were active on social media, and many were Twitter users who followed other members of the Dáil. The pattern of Twitter following amongst these politicians provides insights into political alignment within the Dáil. We propose a new model, called the generalized latent space stochastic blockmodel, which extends and generalizes both the latent space model and the stochastic blockmodel to study social media connections between members of the Dáil. The probability of an edge between two nodes in a network depends on their respective class labels, as well as sender and receiver effects and latent positions in an unobserved latent space. The proposed model is capable of representing transitivity and clustering, as well as disassortative mixing. A Bayesian method with Markov chain Monte Carlo sampling is proposed for estimation of model parameters. Model selection is performed using the WAIC criterion and models of different number of classes or dimensions of latent space are compared. We use the model to study Twitter following relationships of members of the Dáil and interpret structure found in these relationships. We find that the following relationships amongst politicians is mainly driven by past and present political party membership. We also find that the modeling outputs are informative when studying voting within the Dáil.},
  archive      = {J_AOAS},
  author       = {Tin Lok James Ng and Thomas Brendan Murphy and Ted Westling and Tyler H. McCormick and Bailey Fosdick},
  doi          = {10.1214/21-AOAS1483},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {1923-1944},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Modeling the social media relationships of irish politicians using a generalized latent space stochastic blockmodel},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Spatial voting models in circular spaces: A case study of
the u.s. House of representatives. <em>AOAS</em>, <em>15</em>(4),
1897–1922. (<a href="https://doi.org/10.1214/21-AOAS1454">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The use of spatial models for inferring members’ preferences from voting data has become widespread in the study of deliberative bodies, such as legislatures. Most established spatial voting models assume that ideal points belong to a Euclidean policy space. However, the geometry of Euclidean spaces (even multidimensional ones) cannot fully accommodate situations in which members at the opposite ends of the ideological spectrum reveal similar preferences by voting together against the rest of the legislature. This kind of voting behavior can arise, for example, when extreme conservatives oppose a measure because they see it as being too costly, while extreme liberals oppose it for not going far enough for them. This paper introduces a new class of spatial voting models in which preferences live in a circular policy space. Such geometry for the latent space is motivated by both theoretical (the so-called “horseshoe theory” of political thinking) and empirical (goodness of fit) considerations. Furthermore, the circular model is flexible and can approximate the one-dimensional version of the Euclidean voting model when the data supports it. We apply our circular model to roll-call voting data from the U.S. Congress between 1988 and 2019 and demonstrate that, starting with the 112th House of Representatives, circular policy spaces consistently provide a better explanation of legislators’s behavior than Euclidean ones and that legislators’s rankings, generated through the use of the circular geometry, tend to be more consistent with those implied by their stated policy positions.},
  archive      = {J_AOAS},
  author       = {Xingchen Yu and Abel Rodríguez},
  doi          = {10.1214/21-AOAS1454},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {1897-1922},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Spatial voting models in circular spaces: A case study of the U.S. house of representatives},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Estimating animal utilization distributions from multiple
data types: A joint spatiotemporal point process framework.
<em>AOAS</em>, <em>15</em>(4), 1872–1896. (<a
href="https://doi.org/10.1214/21-AOAS1472">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Models of the spatial distribution of animals provide useful tools to help ecologists quantify species-environment relationships, and they are increasingly being used to help determine the impacts of climate and habitat changes on species. While high-quality survey-style data with known effort are sometimes available, often researchers have multiple datasets of varying quality and type. In particular, collections of sightings made by citizen scientists are becoming increasingly common, with no information typically provided on their observer effort. Many standard modelling approaches ignore observer effort completely which can severely bias estimates of an animal’s distribution. Combining sightings data from observers who followed different protocols is challenging. Any differences in observer skill, spatial effort and the detectability of the animals across space all need to be accounted for. To achieve this, we build upon the recent advancements made in integrative species distribution models and present a novel marked spatiotemporal point process framework for estimating the utilization distribution (UD) of the individuals of a highly mobile species. We show that, in certain settings, we can also use the framework to combine the UDs from the sampled individuals to estimate the species’ distribution. We combine the empirical results from a simulation study with the implications outlined in a causal directed acyclic graph to identify the necessary assumptions required for our framework to control for observer effort when it is unknown. We then apply our framework to combine multiple datasets collected on the endangered Southern Resident Killer Whales to estimate their monthly effort-corrected space-use.},
  archive      = {J_AOAS},
  author       = {Joe Watson and Ruth Joy and Dominic Tollit and Sheila J. Thornton and Marie Auger-Méthé},
  doi          = {10.1214/21-AOAS1472},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {1872-1896},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Estimating animal utilization distributions from multiple data types: A joint spatiotemporal point process framework},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Nonparametric importance sampling for wind turbine
reliability analysis with stochastic computer models. <em>AOAS</em>,
<em>15</em>(4), 1850–1871. (<a
href="https://doi.org/10.1214/21-AOAS1490">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Using aeroelastic stochastic simulations, this study presents an importance sampling method for assessing wind turbine reliability. As the size of modern wind turbines gets larger, structural reliability analysis becomes more important to prevent any catastrophic failures. At the design stage, operational data do not exist or are scarce. Therefore, aeroelastic simulation is often employed for reliability analysis. Importance sampling is one of the powerful variance reduction techniques to mitigate computational burden in stochastic simulations. In the literature, wind turbine reliability assessment with importance sampling has been studied with a single variable, wind speed. However, other atmospheric stability conditions also impose substantial stress on the turbine structure. Moreover, each environmental factor’s effect on the turbine’s load response depends on other factors. This study investigates how multiple environmental factors collectively affect the turbine reliability. Specifically, we devise a new nonparametric importance sampling method that can quantify the contributions of each environmental factor and its interactions with other factors, while avoiding computational problems and data sparsity issue arising in rare event simulation. Our wind turbine case study and numerical examples demonstrate the advantage of the proposed approach.},
  archive      = {J_AOAS},
  author       = {Shuoran Li and Young Myoung Ko and Eunshin Byon},
  doi          = {10.1214/21-AOAS1490},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {1850-1871},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Nonparametric importance sampling for wind turbine reliability analysis with stochastic computer models},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Assessing the reliability of wind power operations under a
changing climate with a non-gaussian bias correction. <em>AOAS</em>,
<em>15</em>(4), 1831–1849. (<a
href="https://doi.org/10.1214/21-AOAS1460">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facing increasing societal and economic pressure, many countries have established strategies to develop renewable energy portfolios whose penetration in the market can alleviate the dependence on fossil fuels. In the case of wind, there is a fundamental question related to the resilience and hence profitability of future wind farms to a changing climate, given that current wind turbines have lifespans of up to 30 years. In this work we develop a new non-Gaussian method to adjust assimilated observational data to simulations and to estimate future wind, predicated on a trans-Gaussian transformation and a clusterwise minimization of the Kullback–Leibler divergence. Future winds abundance will be determined for Saudi Arabia, a country with a recently established plan to develop a portfolio of up to 16 GW of wind energy. Further, we estimate the change in profits over future decades using additional high-resolution simulations, an improved method for vertical wind extrapolation and power curves from a collection of popular wind turbines. We find an overall increase in daily profit of $272,000 for the wind energy market for the optimal locations for wind farming in the country.},
  archive      = {J_AOAS},
  author       = {Jiachen Zhang and Paola Crippa and Marc G. Genton and Stefano Castruccio},
  doi          = {10.1214/21-AOAS1460},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {1831-1849},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Assessing the reliability of wind power operations under a changing climate with a non-gaussian bias correction},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). RADIOHEAD: Radiogenomic analysis incorporating tumor
heterogeneity in imaging through densities. <em>AOAS</em>,
<em>15</em>(4), 1808–1830. (<a
href="https://doi.org/10.1214/21-AOAS1458">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent technological advancements have enabled detailed investigation of associations between the molecular architecture and tumor heterogeneity through multisource integration of radiological imaging and genomic (radiogenomic) data. In this paper we integrate and harness radiogenomic data in patients with lower grade gliomas (LGG), a type of brain cancer, in order to develop a regression framework called RADIOHEAD (RADIOgenomic analysis incorporating tumor HEterogeneity in imAging through Densities) to identify radiogenomic associations. Imaging data is represented through voxel-intensity probability density functions of tumor subregions obtained from multimodal magnetic resonance imaging and genomic data through molecular signatures in the form of pathway enrichment scores corresponding to their gene expression profiles. Employing a Riemannian-geometric framework for principal component analysis on the set of probability density functions, we map each probability density to a vector of principal component scores which are then included as predictors in a Bayesian regression model with the pathway enrichment scores as the response. Variable selection compatible with the grouping structure amongst the predictors induced through the tumor subregions is carried out under a group spike-and-slab prior. A Bayesian false discovery rate mechanism is then used to infer significant associations based on the posterior distribution of the regression coefficients. Our analyses reveal several pathways relevant to LGG etiology (such as synaptic transmission, nerve impulse and neurotransmitter pathways) to have significant associations with the corresponding imaging-based predictors.},
  archive      = {J_AOAS},
  author       = {Shariq Mohammed and Karthik Bharath and Sebastian Kurtek and Arvind Rao and Veerabhadran Baladandayuthapani},
  doi          = {10.1214/21-AOAS1458},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {1808-1830},
  shortjournal = {Ann. Appl. Stat.},
  title        = {RADIOHEAD: Radiogenomic analysis incorporating tumor heterogeneity in imaging through densities},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Information content of high-order associations of the human
gut microbiota network. <em>AOAS</em>, <em>15</em>(4), 1788–1807. (<a
href="https://doi.org/10.1214/21-AOAS1449">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The human gastrointestinal tract is an environment that hosts an ecosystem of microorganisms essential to human health. Vital biological processes emerge from fundamental inter- and intraspecies molecular interactions that influence the assembly and composition of the gut microbiota ecology. Here, we quantify the complexity of the ecological relationships within the human infant gut microbiota ecosystem as a function of the information contained in the nonlinear associations of a sequence of increasingly specified maximum entropy representations of the system. Our paradigm frames the ecological state, in terms of the presence or absence of individual microbial ecological units that are identified by amplicon sequence variants (ASV) in the gut microenvironment, as a function of both the ecological states of its neighboring units and, in a departure from standard graphical model representations, the associations among the units within its neighborhood. We characterize the order of the system based on the relative quantity of statistical information encoded by high-order statistical associations of the infant gut microbiota.},
  archive      = {J_AOAS},
  author       = {Weston D. Viles and Juliette C. Madan and Hongzhe Li and Margaret R. Karagas and Anne G. Hoen},
  doi          = {10.1214/21-AOAS1449},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {1788-1807},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Information content of high-order associations of the human gut microbiota network},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bridging randomized controlled trials and single-arm trials
using commensurate priors in arm-based network meta-analysis.
<em>AOAS</em>, <em>15</em>(4), 1767–1787. (<a
href="https://doi.org/10.1214/21-AOAS1469">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network meta-analysis (NMA) is a powerful tool to compare multiple treatments directly and indirectly by combining and contrasting multiple independent clinical trials. Because many NMAs collect only a few eligible randomized controlled trials (RCTs), there is an urgent need to synthesize different sources of information, for example, from both RCTs and single-arm trials. However, single-arm trials and RCTs may have different populations and quality so that assuming they are exchangeable may be inappropriate. This article presents a novel method using a commensurate prior on variance (CPV) to borrow variance (rather than mean) information from single-arm trials in an arm-based (AB) Bayesian NMA. We illustrate the advantages of this CPV method by reanalyzing an NMA of immune checkpoint inhibitors in cancer patients. Comprehensive simulations investigate the impact on statistical inference of including single-arm trials. The simulation results show that the CPV method provides efficient and robust estimation, even when the two sources of information are moderately inconsistent.},
  archive      = {J_AOAS},
  author       = {Zhenxun Wang and Lifeng Lin and Thomas Murray and James S. Hodges and Haitao Chu},
  doi          = {10.1214/21-AOAS1469},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {1767-1787},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Bridging randomized controlled trials and single-arm trials using commensurate priors in arm-based network meta-analysis},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A bayesian nonparametric approach to super-resolution
single-molecule localization. <em>AOAS</em>, <em>15</em>(4), 1742–1766.
(<a href="https://doi.org/10.1214/21-AOAS1441">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of single-molecule identification in super-resolution microscopy. Super-resolution microscopy overcomes the diffraction limit by localizing individual fluorescing molecules in a field of view. This is particularly difficult since each individual molecule appears and disappears randomly across time and because the total number of molecules in the field of view is unknown. Additionally, data sets acquired with super-resolution microscopes can contain a large number of spurious fluorescent fluctuations caused by background noise. To address these problems, we present a Bayesian nonparametric framework capable of identifying individual emitting molecules in super-resolved time series. We tackle the localization problem in the case in which each individual molecule is already localized in space. First, we collapse observations in time and develop a fast algorithm that builds upon the Dirichlet process. Next, we augment the model to account for the temporal aspect of fluorophore photophysics. Finally, we assess the performance of our methods with ground-truth data sets having known biological structure.},
  archive      = {J_AOAS},
  author       = {Mariano I. Gabitto and Herve Marie-Nelly and Ari Pakman and Andras Pataki and Xavier Darzacq and Michael I. Jordan},
  doi          = {10.1214/21-AOAS1441},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {1742-1766},
  shortjournal = {Ann. Appl. Stat.},
  title        = {A bayesian nonparametric approach to super-resolution single-molecule localization},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bayesian multistudy factor analysis for high-throughput
biological data. <em>AOAS</em>, <em>15</em>(4), 1723–1741. (<a
href="https://doi.org/10.1214/21-AOAS1456">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper analyzes breast cancer gene expression across seven studies to identify genuine and thus replicable gene patterns shared among these studies. Our premise is that genuine biological signal is more likely to be reproducibly present in multiple studies than spurious signal. Our analysis uses a new modeling strategy for the joint analysis of high-throughput biological studies which simultaneously identifies shared as well as study-specific signal. To this end, we generalize the multi-study factor analysis model to handle high-dimensional data and generalize the sparse Bayesian infinite factor model to this context. We provide strategies for the identification of the loading matrices, common and study-specific. Through extensive simulation analysis, we characterize the performance of the proposed approach in various scenarios and show that it outperforms standard factor analysis in identifying replicable signal in all scenarios considered. The analysis of breast cancer gene expression studies identifies clear replicable gene patterns. These patterns are related to well-known biological pathways involved in breast cancer, such as the ER, cell cycle, immune system, collagen, and metabolic pathways. Some of these patterns are also associated with existing breast cancer subtypes, such as LumA, Her2, and basal subtypes, while other patterns identify novel pathways active across subtypes and missed by hierarchical clustering approaches. The R package MSFA implementing the method is available on GitHub.},
  archive      = {J_AOAS},
  author       = {Roberta De Vito and Ruggero Bellio and Lorenzo Trippa and Giovanni Parmigiani},
  doi          = {10.1214/21-AOAS1456},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {1723-1741},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Bayesian multistudy factor analysis for high-throughput biological data},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Joint and individual analysis of breast cancer histologic
images and genomic covariates. <em>AOAS</em>, <em>15</em>(4), 1697–1722.
(<a href="https://doi.org/10.1214/20-AOAS1433">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The two main approaches in the study of breast cancer are histopathology (analyzing visual characteristics of tumors) and genomics. While both histopathology and genomics are fundamental to cancer research, the connections between these fields have been relatively superficial. We bridge this gap by investigating the Carolina Breast Cancer Study through the development of an integrative, exploratory analysis framework. Our analysis gives insights—some known, some novel—that are engaging to both pathologists and geneticists. Our analysis framework is based on angle-based joint and individual variation explained (AJIVE) for statistical data integration and exploits convolutional neural networks (CNNs) as a powerful, automatic method for image feature extraction. CNNs raise interpretability issues that we address by developing novel methods to explore visual modes of variation captured by statistical algorithms (e.g., PCA or AJIVE) applied to CNN features.},
  archive      = {J_AOAS},
  author       = {Iain Carmichael and Benjamin C. Calhoun and Katherine A. Hoadley and Melissa A. Troester and Joseph Geradts and Heather D. Couture and Linnea Olsson and Charles M. Perou and Marc Niethammer and Jan Hannig and J. S. Marron},
  doi          = {10.1214/20-AOAS1433},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {1697-1722},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Joint and individual analysis of breast cancer histologic images and genomic covariates},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Zero-inflated quantile rank-score based test (ZIQRank) with
application to scRNA-seq differential gene expression analysis.
<em>AOAS</em>, <em>15</em>(4), 1673–1696. (<a
href="https://doi.org/10.1214/21-AOAS1442">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Differential gene expression analysis based on scRNA-seq data is challenging due to two unique characteristics of scRNA-seq data. First, multimodality and other heterogeneity of the gene expression among different cell conditions lead to divergences in the tail events or crossings of the expression distributions. Second, scRNA-seq data generally have a considerable fraction of dropout events, causing zero inflation in the expression. To account for the first characteristic, existing parametric approaches targeting the mean difference in gene expression are limited, while quantile regression that examines various locations in the distribution will improve the power. However, the second characteristic, zero inflation, makes the traditional quantile regression invalid and underpowered. We propose a quantile-based test that handles the two characteristics, multimodality and zero inflation, simultaneously. The proposed quantile rank-score based test for differential distribution detection (ZIQRank) is derived under a two-part quantile regression model for zero-inflated outcomes. It comprises a test in logistic modeling for the zero counts and a collection of rank-score tests adjusting for zero inflation at multiple prespecified quantiles of the positive part. The testing decision is based on an aggregate result by combining the marginal p-values by MinP or Cauchy procedure. The proposed test is asymptotically justified and evaluated with simulation studies. It shows a higher precision-recall AUC in detecting true differentially expressed genes (DEGs) than the existing methods. We apply the ZIQRank test to a TPM scRNA-seq data on human glioblastoma tumors and exclusively identify a group of DEGs between neoplastic and nonneoplastic cells, which are heterogeneous and have been proved to be associated with glioma. Application to a UMI count scRNA-seq data on cells from mouse intestinal organoids further demonstrates the capability of ZIQRank to improve and complement the existing approaches.},
  archive      = {J_AOAS},
  author       = {Wodan Ling and Wenfei Zhang and Bin Cheng and Ying Wei},
  doi          = {10.1214/21-AOAS1442},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {1673-1696},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Zero-inflated quantile rank-score based test (ZIQRank) with application to scRNA-seq differential gene expression analysis},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). VCSEL: Prioritizing SNP-set by penalized variance component
selection. <em>AOAS</em>, <em>15</em>(4), 1652–1672. (<a
href="https://doi.org/10.1214/21-AOAS1491">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single nucleotide polymorphism (SNP) set analysis aggregates both common and rare variants and tests for association between phenotype(s) of interest and a set. However, multiple SNP-sets, such as genes, pathways, or sliding windows are usually investigated across the whole genome in which all groups are tested separately, followed by multiple testing adjustments. We propose a novel method to prioritize SNP-sets in a joint multivariate variance component model. Each SNP-set corresponds to a variance component (or kernel), and model selection is achieved by incorporating either convex or nonconvex penalties. The uniqueness of this variance component selection framework, which we call VCSEL, is that it naturally encompasses multivariate traits (VCSEL-M) and SNP-set-treatment or -environment interactions (VCSEL-I). We devise an optimization algorithm scalable to many variance components, based on the majorization-minimization (MM) principle. Simulation studies demonstrate the superiority of our methods in model selection performance, as measured by the area under the precision-recall (PR) curve, compared to the commonly used marginal testing and group penalization methods. Finally, we apply our methods to a real pharmacogenomics study and a real whole exome sequencing study. Some top ranked genes by VCSEL are detected as insignificant by the marginal test methods which emphasizes formal inference of individual genes with a strict significance threshold. This provides alternative insights for biologists to prioritize follow-up studies and develop polygenic risk score models.},
  archive      = {J_AOAS},
  author       = {Juhyun Kim and Judong Shen and Anran Wang and Devan V. Mehrotra and Seyoon Ko and Jin J. Zhou and Hua Zhou},
  doi          = {10.1214/21-AOAS1491},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {1652-1672},
  shortjournal = {Ann. Appl. Stat.},
  title        = {VCSEL: Prioritizing SNP-set by penalized variance component selection},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Markov random field models for vector-based representations
of landscapes. <em>AOAS</em>, <em>15</em>(4), 1628–1651. (<a
href="https://doi.org/10.1214/21-AOAS1447">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In agricultural landscapes the spatial distribution of cultivated and seminatural elements strongly impacts habitat connectivity and species dynamics. To allow for landscape structural analysis and scenario generation, we here develop statistical tools for real landscapes composed of geometric elements, including 2D patches but also 1D linear elements (e.g., hedges). Utilizing the framework of discrete Markov random fields, we design generative stochastic models that combine a multiplex network representation, based on spatial adjacency, with Gibbs energy terms to capture the distribution of landscape descriptors for land-use categories. We implement simulation of agricultural scenarios with parameter-controlled spatial and temporal patterns (e.g., geometry, connectivity, crop rotation), and we demonstrate through simulation that pseudo-likelihood estimation of parameters works well. To study statistical relevance of model components in real landscapes, we discuss model selection and validation, including cross-validated prediction scores. Model validation with a view toward ecologically relevant landscape summaries is achieved by comparing observed and simulated summaries (network metrics but also metrics and appropriately defined variograms using a raster discretization). Models fitted to subregions of the Lower Durance Valley (France) indicate strong deviation from random allocation and realistically capture landscape patterns. In summary, our approach improves the understanding of agroecosystems and enables simulation-based theoretical analysis of how landscape patterns shape biological and ecological processes.},
  archive      = {J_AOAS},
  author       = {Patrizia Zamberletti and Julien Papaïx and Edith Gabriel and Thomas Opitz},
  doi          = {10.1214/21-AOAS1447},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {1628-1651},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Markov random field models for vector-based representations of landscapes},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Uncertainty quantification of a computer model for binary
black hole formation. <em>AOAS</em>, <em>15</em>(4), 1604–1627. (<a
href="https://doi.org/10.1214/21-AOAS1484">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a fast and parallelizable method based on Gaussian processes (GPs) is introduced to emulate computer models that simulate the formation of binary black holes (BBHs) through the evolution of pairs of massive stars. Two obstacles that arise in this application are the a priori unknown conditions of BBH formation and the large scale of the simulation data. We address them by proposing a local emulator which combines a GP classifier and a GP regression model. The resulting emulator can also be utilized in planning future computer simulations through a proposed criterion for sequential design. By propagating uncertainties of simulation input through the emulator, we are able to obtain the distribution of BBH properties under the distribution of physical parameters.},
  archive      = {J_AOAS},
  author       = {Luyao Lin and Derek Bingham and Floor Broekgaarden and Ilya Mandel},
  doi          = {10.1214/21-AOAS1484},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {1604-1627},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Uncertainty quantification of a computer model for binary black hole formation},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Clustering on the torus by conformal prediction.
<em>AOAS</em>, <em>15</em>(4), 1583–1603. (<a
href="https://doi.org/10.1214/21-AOAS1459">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by the analysis of torsion (dihedral) angles in the backbone of proteins, we investigate clustering of bivariate angular data on the torus [−π,π)×[−π,π). We show that naive adaptations of clustering methods, designed for vector-valued data, to the torus are not satisfactory and propose a novel clustering approach based on the conformal prediction framework. We construct several prediction sets for toroidal data with guaranteed finite-sample validity, based on a kernel density estimate and bivariate von Mises mixture models. From a prediction set built from a Gaussian approximation of the bivariate von Mises mixture, we propose a data-driven choice for the number of clusters and present algorithms for an automated cluster identification and cluster membership assignment. The proposed prediction sets and clustering approaches are applied to the torsion angles extracted from three strains of coronavirus spike glycoproteins (including SARS-CoV-2, contagious in humans). The analysis reveals a potential difference in the clusters of the SARS-CoV-2 torsion angles, compared to the clusters found in torsion angles from two different strains of coronavirus, contagious in animals.},
  archive      = {J_AOAS},
  author       = {Sungkyu Jung and Kiho Park and Byungwon Kim},
  doi          = {10.1214/21-AOAS1459},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {1583-1603},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Clustering on the torus by conformal prediction},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Assessing selection bias in regression coefficients
estimated from nonprobability samples with applications to genetics and
demographic surveys. <em>AOAS</em>, <em>15</em>(3), 1556–1581. (<a
href="https://doi.org/10.1214/21-AOAS1453">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Selection bias is a serious potential problem for inference about relationships of scientific interest based on samples without well-defined probability sampling mechanisms. Motivated by the potential for selection bias in: (a) estimated relationships of polygenic scores (PGSs) with phenotypes in genetic studies of volunteers and (b) estimated differences in subgroup means in surveys of smartphone users, we derive novel measures of selection bias for estimates of the coefficients in linear and probit regression models fitted to nonprobability samples, when aggregate-level auxiliary data are available for the selected sample and the target population. The measures arise from normal pattern-mixture models that allow analysts to examine the sensitivity of their inferences to assumptions about nonignorable selection in these samples. We examine the effectiveness of the proposed measures in a simulation study and then use them to quantify the selection bias in: (a) estimated PGS-phenotype relationships in a large study of volunteers recruited via Facebook and (b) estimated subgroup differences in mean past-year employment duration in a nonprobability sample of low-educated smartphone users. We evaluate the performance of the measures in these applications using benchmark estimates from large probability samples.},
  archive      = {J_AOAS},
  author       = {Brady T. West and Roderick J. Little and Rebecca R. Andridge and Philip S. Boonstra and Erin B. Ware and Anita Pandit and Fernanda Alvarado-Leiton},
  doi          = {10.1214/21-AOAS1453},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {1556-1581},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Assessing selection bias in regression coefficients estimated from nonprobability samples with applications to genetics and demographic surveys},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Partial-mastery cognitive diagnosis models. <em>AOAS</em>,
<em>15</em>(3), 1529–1555. (<a
href="https://doi.org/10.1214/21-AOAS1439">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cognitive diagnosis models (CDMs) are a family of discrete latent attribute models that serve as statistical basis in educational and psychological cognitive diagnosis assessments. CDMs aim to achieve fine-grained inference on individuals’ latent attributes, based on their observed responses to a set of designed diagnostic items. In the literature CDMs usually assume that items require mastery of specific latent attributes and that each attribute is either fully mastered or not mastered by a given subject. We propose a new class of models, partial mastery CDMs (PM-CDMs), that generalizes CDMs by allowing for partial mastery levels for each attribute of interest. We demonstrate that PM-CDMs can be represented as restricted latent class models. Relying on the latent class representation, we propose a Bayesian approach for estimation. We present simulation studies to demonstrate parameter recovery, to investigate the impact of model misspecification with respect to partial mastery and to develop diagnostic tools that could be used by practitioners to decide between CDMs and PM-CDMs. We use two examples of real test data—the fraction subtraction and the English tests—to demonstrate that employing PM-CDMs not only improves model fit, compared to CDMs, but also can make substantial difference in conclusions about attribute mastery. We conclude that PM-CDMs can lead to more effective remediation programs by providing detailed individual-level information about skills learned and skills that need to study.},
  archive      = {J_AOAS},
  author       = {Zhuoran Shang and Elena A. Erosheva and Gongjun Xu},
  doi          = {10.1214/21-AOAS1439},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {1529-1555},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Partial-mastery cognitive diagnosis models},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Global estimation and scenario-based projections of sex
ratio at birth and missing female births using a bayesian hierarchical
time series mixture model. <em>AOAS</em>, <em>15</em>(3), 1499–1528. (<a
href="https://doi.org/10.1214/20-AOAS1436">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The sex ratio at birth (SRB) is defined as the ratio of male to female live births. The SRB imbalance in parts of the world over the past several decades is a direct consequence of sex-selective abortion, driven by the coexistence of son preference, readily available technology of prenatal sex determination and fertility decline. Estimation and projection of the degree of SRB imbalance is complicated because of variability in SRB reference levels and because of the uncertainty associated with SRB observations. We develop Bayesian hierarchical time series mixture models for SRB estimation and scenario-based projections for all countries from 1950 to 2100. We model the SRB regional and national reference levels and the fluctuation around national reference levels. We identify countries at risk of SRB imbalances and model both: (i) the absence or presence of sex ratio transitions in such countries and, if present, (ii) the transition process. The transition model of SRB imbalance captures three stages (increase, stagnation and convergence back to SRB baselines). The model identifies countries with statistical evidence of SRB inflation in a fully Bayesian approach. The scenario-based SRB projections are based on the sex ratio transition model with varying assumptions regarding the occurrence of a sex ratio transition in at-risk countries. Projections are used to quantify the future burden of missing female births due to sex-selective abortions under different scenarios.},
  archive      = {J_AOAS},
  author       = {Fengqing Chao and Patrick Gerland and Alex R. Cook and Leontine Alkema},
  doi          = {10.1214/20-AOAS1436},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {1499-1528},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Global estimation and scenario-based projections of sex ratio at birth and missing female births using a bayesian hierarchical time series mixture model},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Diagnosis-group-specific transitional care program
recommendations for 30-day rehospitalization reduction. <em>AOAS</em>,
<em>15</em>(3), 1478–1498. (<a
href="https://doi.org/10.1214/21-AOAS1473">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Thirty-day rehospitalization rate is a well-studied and important measure reflecting the overall performance of health systems. Recently, transitional care (TC) programs have been initiated to reduce avoidable rehospitalizations. These programs typically ask nurses to follow-up with patients after the hospitalization to manage issues and reduce the risk of rehospitalizations during health care transitions. As rehospitalization is a complex process that depends on many factors, it is unlikely that these interventions are effective for all patients across a diverse population. In this paper we consider individualized intervention or treatment recommendation rules (ITRs) aimed at maximizing overall treatment effectiveness. We investigate our approach in a setting where patients are divided into two diagnosis related groups, medically complicated and uncomplicated. As the treatment effects can greatly vary between the two groups, we allow our recommendation rules to be group specific. In particular, our approach can accommodate scale differences in treatment effects and utilize a tuning parameter to drive the similarity of the estimated ITRs between groups. Computation is achieved by transforming our problem into a form solvable by existing software, and a wrapper R package is developed for our proposed treatment recommendation framework. We conduct extensive evaluation through both simulation studies and analysis of a TC program.},
  archive      = {J_AOAS},
  author       = {Menggang Yu and Chensheng Kuang and Jared D. Huling and Maureen Smith},
  doi          = {10.1214/21-AOAS1473},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {1478-1498},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Diagnosis-group-specific transitional care program recommendations for 30-day rehospitalization reduction},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Tensor quantile regression with application to association
between neuroimages and human intelligence. <em>AOAS</em>,
<em>15</em>(3), 1455–1477. (<a
href="https://doi.org/10.1214/21-AOAS1475">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human intelligence is usually measured by well-established psychometric tests through a series of problem solving. The recorded cognitive scores are continuous but usually heavy-tailed with potential outliers and violating the normality assumption. Meanwhile, magnetic resonance imaging (MRI) provides an unparalleled opportunity to study brain structures and cognitive ability. Motivated by association studies between MRI images and human intelligence, we propose a tensor quantile regression model, which is a general and robust alternative to the commonly used scalar-on-image linear regression. Moreover, we take into account rich spatial information of brain structures, incorporating low-rankness and piecewise smoothness of imaging coefficients into a regularized regression framework. We formulate the optimization problem as a sequence of penalized quantile regressions with a generalized Lasso penalty, based on tensor decomposition, and develop a computationally efficient alternating direction method of multipliers algorithm (ADMM) to estimate the model components. Extensive numerical studies are conducted to examine the empirical performance of the proposed method and its competitors. Finally, we apply the proposed method to a large-scale important dataset—the Human Connectome Project. We find that the tensor quantile regression can serve as a prognostic tool to assess future risk of cognitive impairment progression. More importantly, with the proposed method we are able to identify the most activated brain subregions associated with quantiles of human intelligence. The prefrontal and anterior cingulate cortex are found to be mostly associated with lower and upper quantile of fluid intelligence. The insular cortex associated with median of fluid intelligence is a rarely reported region.},
  archive      = {J_AOAS},
  author       = {Cai Li and Heping Zhang},
  doi          = {10.1214/21-AOAS1475},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {1455-1477},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Tensor quantile regression with application to association between neuroimages and human intelligence},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Simultaneous non-gaussian component analysis (SING) for data
integration in neuroimaging. <em>AOAS</em>, <em>15</em>(3), 1431–1454.
(<a href="https://doi.org/10.1214/21-AOAS1466">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As advances in technology allow the acquisition of complementary information, it is increasingly common for scientific studies to collect multiple datasets. Large-scale neuroimaging studies often include multiple modalities (e.g., task functional MRI, resting-state fMRI, diffusion MRI, and/or structural MRI) with the aim to understand the relationships between datasets. In this study, we seek to understand whether regions of the brain activated in a working memory task relate to resting-state correlations. In neuroimaging, a popular approach uses principal component analysis for dimension reduction prior to canonical correlation analysis with joint independent component analysis, but this may discard biological features with low variance and/or spuriously associate structure unique to a dataset with joint structure. We introduce SImultaneous Non-Gaussian component analysis (SING) in which dimension reduction and feature extraction are achieved simultaneously, and shared information is captured via subject scores. We apply our method to a working memory task and resting-state correlations from the Human Connectome Project. We find joint structure as evident from joint scores whose loadings highlight resting-state correlations involving regions associated with working memory. Moreover, some of the subject scores are related to fluid intelligence.},
  archive      = {J_AOAS},
  author       = {Benjamin B. Risk and Irina Gaynanova},
  doi          = {10.1214/21-AOAS1466},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {1431-1454},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Simultaneous non-gaussian component analysis (SING) for data integration in neuroimaging},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bayesian joint modeling of chemical structure and dose
response curves. <em>AOAS</em>, <em>15</em>(3), 1405–1430. (<a
href="https://doi.org/10.1214/21-AOAS1461">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Today there are approximately 85,000 chemicals regulated under the Toxic Substances Control Act, with around 2,000 new chemicals introduced each year. It is impossible to screen all of these chemicals for potential toxic effects, either via full organism in vivo studies or in vitro high-throughput screening (HTS) programs. Toxicologists face the challenge of choosing which chemicals to screen, and predicting the toxicity of as yet unscreened chemicals. Our goal is to describe how variation in chemical structure relates to variation in toxicological response to enable in silico toxicity characterization designed to meet both of these challenges. With our Bayesian partially Supervised Sparse and Smooth Factor Analysis (BS3FA) model, we learn a distance between chemicals targeted to toxicity, rather than one based on molecular structure alone. Our model also enables the prediction of chemical dose-response profiles based on chemical structure (i.e., without in vivo or in vitro testing) by taking advantage of a large database of chemicals that have already been tested for toxicity in HTS programs. We show superior simulation performance in distance learning and modest to large gains in predictive ability compared to existing methods. Results from the high-throughput screening data application elucidate the relationship between chemical structure and a toxicity-relevant high-throughput assay. An R package for BS3FA is available online at https://github.com/kelrenmor/bs3fa.},
  archive      = {J_AOAS},
  author       = {Kelly R. Moran and David Dunson and Matthew W. Wheeler and Amy H. Herring},
  doi          = {10.1214/21-AOAS1461},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {1405-1430},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Bayesian joint modeling of chemical structure and dose response curves},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Perturbed factor analysis: Accounting for group differences
in exposure profiles. <em>AOAS</em>, <em>15</em>(3), 1386–1404. (<a
href="https://doi.org/10.1214/20-AOAS1435">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article we investigate group differences in phthalate exposure profiles using NHANES data. Phthalates are a family of industrial chemicals used in plastics and as solvents. There is increasing evidence of adverse health effects of exposure to phthalates on reproduction and neurodevelopment and concern about racial disparities in exposure. We would like to identify a single set of low-dimensional factors summarizing exposure to different chemicals, while allowing differences across groups. Improving on current multigroup additive factor models, we propose a class of Perturbed Factor Analysis (PFA) models that assume a common factor structure after perturbing the data via multiplication by a group-specific matrix. Bayesian inference algorithms are defined using a matrix normal hierarchical model for the perturbation matrices. The resulting model is just as flexible as current approaches in allowing arbitrarily large differences across groups but has substantial advantages that we illustrate in simulation studies. Applying PFA to NHANES data, we learn common factors summarizing exposures to phthalates, while showing clear differences across groups.},
  archive      = {J_AOAS},
  author       = {Arkaprava Roy and Isaac Lavine and Amy H. Herring and David B. Dunson},
  doi          = {10.1214/20-AOAS1435},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {1386-1404},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Perturbed factor analysis: Accounting for group differences in exposure profiles},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Function-on-function regression for the identification of
epigenetic regions exhibiting windows of susceptibility to environmental
exposures. <em>AOAS</em>, <em>15</em>(3), 1366–1385. (<a
href="https://doi.org/10.1214/20-AOAS1425">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ability to identify time periods when individuals are most susceptible to exposures as well as the biological mechanisms through which these exposures act is of great public health interest. Growing evidence supports an association between prenatal exposure to air pollution and epigenetic marks, such as DNA methylation, but the timing and gene-specific effects of these epigenetic changes are not well understood. Here, we present the first study that aims to identify prenatal windows of susceptibility to air pollution exposures in cord blood DNA methylation. In particular, we propose a function-on-function regression model that leverages data from nearby DNA methylation probes to identify epigenetic regions that exhibit windows of susceptibility to ambient particulate matter less than 2.5 microns (PM2.5). By incorporating the covariance structure among both the multivariate DNA methylation outcome and the time-varying exposure under study, this framework yields greater power to detect windows of susceptibility and greater control of false discoveries than methods that model probes independently. We compare our method to a distributed lag model approach that models DNA methylation in a probe-by-probe manner, both in simulation and by application to motivating data from the Project Viva birth cohort. We identify a window of susceptibility to PM2.5 exposure in the middle of the third trimester of pregnancy in an epigenetic region selected based on prior studies of air pollution effects on epigenome-wide methylation.},
  archive      = {J_AOAS},
  author       = {Michele Zemplenyi and Mark J. Meyer and Andres Cardenas and Marie-France Hivert and Sheryl L. Rifas-Shiman and Heike Gibson and Itai Kloog and Joel Schwartz and Emily Oken and Dawn L. DeMeo and Diane R. Gold and Brent A. Coull},
  doi          = {10.1214/20-AOAS1425},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {1366-1385},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Function-on-function regression for the identification of epigenetic regions exhibiting windows of susceptibility to environmental exposures},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Two-stage circular-circular regression with zero inflation:
Application to medical sciences. <em>AOAS</em>, <em>15</em>(3),
1343–1365. (<a href="https://doi.org/10.1214/20-AOAS1429">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper considers the modeling of zero-inflated circular measurements concerning real case studies from medical sciences. Circular-circular regression models have been discussed in the statistical literature and illustrated with various real-life applications. However, there are no models to deal with zero-inflated response as well as a covariate simultaneously. The Möbius transformation based two-stage circular-circular regression model is proposed, and the Bayesian estimation of the model parameters is suggested using the MCMC algorithm. Simulation results show the superiority of the performance of the proposed method over the existing competitors. The method is applied to analyse real datasets on astigmatism due to cataract surgery and abnormal gait related to orthopaedic impairment. The methodology proposed can assist in efficient decision making during treatment or postoperative care.},
  archive      = {J_AOAS},
  author       = {Jayant Jha and Prajamitra Bhuyan},
  doi          = {10.1214/20-AOAS1429},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {1343-1365},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Two-stage circular-circular regression with zero inflation: Application to medical sciences},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A multivariate spatiotemporal change-point model of opioid
overdose deaths in ohio. <em>AOAS</em>, <em>15</em>(3), 1329–1342. (<a
href="https://doi.org/10.1214/20-AOAS1415">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ohio is one of the states most impacted by the opioid epidemic and experienced the second highest age-adjusted fatal drug overdose rate in 2017. Initially it was believed prescription opioids were driving the opioid crisis in Ohio. However, as the epidemic evolved, opioid overdose deaths due to fentanyl have drastically increased. In this work we develop a Bayesian multivariate spatiotemporal model for Ohio county overdose death rates from 2007 to 2018 due to different types of opioids. The log-odds are assumed to follow a spatially varying change point regression model. By assuming the regression coefficients are a multivariate conditional autoregressive process, we capture spatial dependence within each drug type and also dependence across drug types. The proposed model allows us to not only study spatiotemporal trends in overdose death rates but also to detect county-level shifts in these trends over time for various types of opioids.},
  archive      = {J_AOAS},
  author       = {Staci A. Hepler and Lance A. Waller and David M. Kline},
  doi          = {10.1214/20-AOAS1415},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {1329-1342},
  shortjournal = {Ann. Appl. Stat.},
  title        = {A multivariate spatiotemporal change-point model of opioid overdose deaths in ohio},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Modeling past event feedback through biomarker dynamics in
the multistate event analysis for cardiovascular disease data.
<em>AOAS</em>, <em>15</em>(3), 1308–1328. (<a
href="https://doi.org/10.1214/21-AOAS1445">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In cardiovascular studies we often observe ordered multiple events along disease progression which are, essentially, a series of recurrent events and terminal events with competing risk structure. One of the main interests is to explore the event specific association with the dynamics of longitudinal biomarkers. A new statistical challenge arises when the biomarkers carry information from the past event history, providing feedbacks for the occurrences of future events and, particularly, when these biomarkers are only intermittently observed with measurement errors. In this paper we propose a novel modeling framework where the recurrent events and terminal events are modeled as multistate processes and the longitudinal covariates that account for event feedbacks are described by random effects models. Considering the nature of long-term observation in cardiac studies, flexible models with semiparametric coefficients are adopted. To improve computation efficiency, we develop an one-step estimator of the regression coefficients and derive their asymptotic variances for the computation of the confidence intervals, based on the proposed asymptotically unbiased estimating equation. Simulation studies show that the naive estimators, which either ignore the past event feedbacks or the measurement errors, are biased. Our method achieves better coverage probability, compared to the naive methods. The model is motivated and applied to a dataset from the Atherosclerosis Risk in Communities Study.},
  archive      = {J_AOAS},
  author       = {Chuoxin Ma and Hongsheng Dai and Jianxin Pan},
  doi          = {10.1214/21-AOAS1445},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {1308-1328},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Modeling past event feedback through biomarker dynamics in the multistate event analysis for cardiovascular disease data},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Estrogen receptor expression on breast cancer patients’
survival under shape-restricted cox regression model. <em>AOAS</em>,
<em>15</em>(3), 1291–1307. (<a
href="https://doi.org/10.1214/21-AOAS1446">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For certain subtypes of breast cancer, study findings show that their level of estrogen receptor expression is associated with their risk of cancer death and also suggest a nonlinear effect on the hazard of death. A flexible form of the proportional hazards model, λ(t|x,z)=λ(t)exp(zTβ)q(x), is desirable to facilitate a rich class of covariate effect on a survival outcome to provide meaningful insight, where the functional form of q(x) is not specified except for its shape. Prior biologic knowledge on the shape of the underlying distribution of the covariate effect in regression models can be used to enhance statistical inference. Despite recent progress, major challenges remain for the semiparametric shape-restricted inference due to lack of practical and efficient computational algorithms to accomplish nonconvex optimization. We propose an alternative algorithm to maximize the full log-likelihood with two sets of parameters iteratively under monotone constraints. The first set consists of the nonparametric estimation of the monotone-restricted function q(x), while the second set includes estimating the baseline hazard function and other covariate coefficients. The iterative algorithm, in conjunction with the pool-adjacent-violators algorithm, makes the computation efficient and practical. The jackknife resampling effectively reduces the estimator bias, when sample size is small. Simulations show that the proposed method can accurately capture the underlying shape of q(x) and outperforms the estimators when q(x) in the Cox model is misspecified. We apply the method to model the effect of estrogen receptor on breast cancer patients’ survival.},
  archive      = {J_AOAS},
  author       = {Jing Qin and Geng Deng and Jing Ning and Ao Yuan and Yu Shen},
  doi          = {10.1214/21-AOAS1446},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {1291-1307},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Estrogen receptor expression on breast cancer patients’ survival under shape-restricted cox regression model},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Orthogonal subsampling for big data linear regression.
<em>AOAS</em>, <em>15</em>(3), 1273–1290. (<a
href="https://doi.org/10.1214/21-AOAS1462">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The dramatic growth of big datasets presents a new challenge to data storage and analysis. Data reduction, or subsampling, that extracts useful information from datasets is a crucial step in big-data analysis. We propose an orthogonal subsampling (OSS) approach for big data with a focus on linear regression models. The approach is inspired by the fact that an orthogonal array of two levels provides the best experimental design for linear regression models in the sense that it minimizes the average variance of the estimated parameters and provides the best predictions. The merits of OSS are three-fold: (i) it is easy to implement and fast; (ii) it is suitable for distributed parallel computing and ensures the subsamples selected in different batches have no common data points, and (iii) it outperforms existing methods in minimizing the mean squared errors of the estimated parameters and maximizing the efficiencies of the selected subsamples. Theoretical results and extensive numerical results show that the OSS approach is superior to existing subsampling approaches. It is also more robust to the presence of interactions among covariates, and, when they do exist, OSS provides more precise estimates of the interaction effects than existing methods. The advantages of OSS are also illustrated through analysis of real data.},
  archive      = {J_AOAS},
  author       = {Lin Wang and Jake Elmstedt and Weng Kee Wong and Hongquan Xu},
  doi          = {10.1214/21-AOAS1462},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {1273-1290},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Orthogonal subsampling for big data linear regression},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Qini-based uplift regression. <em>AOAS</em>, <em>15</em>(3),
1247–1272. (<a href="https://doi.org/10.1214/21-AOAS1465">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Uplift models provide a solution to the problem of isolating the marketing effect of a campaign. For customer churn reduction, uplift models are used to identify the customers who are likely to respond positively to a retention activity, only if targeted, and to avoid wasting resources on customers that are very likely to switch to another company. In practice, the uplift models performance is measured by the Qini coefficient. We introduce a Qini-based uplift regression model to analyze a large insurance company’s retention marketing campaign. Our approach is based on logistic regression models. We show that a Qini-optimized uplift model acts as a regularizing factor for uplift, much as a penalized likelihood model does for regression. This results in interpretable models with few relevant explanatory variables. Our results show that Qini-based parameter estimation significantly improves the Qini prediction performance of uplift models.},
  archive      = {J_AOAS},
  author       = {Mouloud Belbahri and Alejandro Murua and Olivier Gandouet and Vahid Partovi Nia},
  doi          = {10.1214/21-AOAS1465},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {1247-1272},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Qini-based uplift regression},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Stabilizing variable selection and regression.
<em>AOAS</em>, <em>15</em>(3), 1220–1246. (<a
href="https://doi.org/10.1214/21-AOAS1487">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider regression in which one predicts a response Y with a set of predictors X across different experiments or environments. This is a common setup in many data-driven scientific fields, and we argue that statistical inference can benefit from an analysis that takes into account the distributional changes across environments. In particular, it is useful to distinguish between stable and unstable predictors, that is, predictors which have a fixed or a changing functional dependence on the response, respectively. We introduce stabilized regression which explicitly enforces stability and thus improves generalization performance to previously unseen environments. Our work is motivated by an application in systems biology. Using multiomic data, we demonstrate how hypothesis generation about gene function can benefit from stabilized regression. We believe that a similar line of arguments for exploiting heterogeneity in data can be powerful for many other applications as well. We draw a theoretical connection between multi-environment regression and causal models which allows to graphically characterize stable vs. unstable functional dependence on the response. Formally, we introduce the notion of a stable blanket which is a subset of the predictors that lies between the direct causal predictors and the Markov blanket. We prove that this set is optimal in the sense that a regression based on these predictors minimizes the mean squared prediction error, given that the resulting regression generalizes to unseen new environments.},
  archive      = {J_AOAS},
  author       = {Niklas Pfister and Evan G. Williams and Jonas Peters and Ruedi Aebersold and Peter Bühlmann},
  doi          = {10.1214/21-AOAS1487},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {1220-1246},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Stabilizing variable selection and regression},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Targeted smooth bayesian causal forests: An analysis of
heterogeneous treatment effects for simultaneous vs. Interval medical
abortion regimens over gestation. <em>AOAS</em>, <em>15</em>(3),
1194–1219. (<a href="https://doi.org/10.1214/20-AOAS1438">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce Targeted Smooth Bayesian Causal Forests (tsBCF), a nonparametric Bayesian approach for estimating heterogeneous treatment effects which vary smoothly over a single covariate in the observational data setting. The tsBCF method induces smoothness by parameterizing terminal tree nodes with smooth functions and allows for separate regularization of treatment effects vs. prognostic effect of control covariates. Smoothing parameters for prognostic and treatment effects can be chosen to reflect prior knowledge or tuned in a data-dependent way. We use tsBCF to analyze a new clinical protocol for early medical abortion. Our aim is to assess the relative effectiveness of simultaneous vs. interval administration of mifepristone and misoprostol over the first nine weeks of gestation. Our analysis yields important clinical insights into how to best counsel patients seeking early medical abortion, where understanding even small differences in relative effectiveness can yield dramatic returns to public health. The model reflects our expectation that the treatment effect varies smoothly over gestation but not necessarily over other covariates. We demonstrate the performance of the tsBCF method on benchmarking experiments. Software for tsBCF is available at https://github.com/jestarling/tsbcf/ and in the Supplementary Material (Starling (2020)).},
  archive      = {J_AOAS},
  author       = {Jennifer E. Starling and Jared S. Murray and Patricia A. Lohr and Abigail R. A. Aiken and Carlos M. Carvalho and James G. Scott},
  doi          = {10.1214/20-AOAS1438},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {1194-1219},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Targeted smooth bayesian causal forests: An analysis of heterogeneous treatment effects for simultaneous vs. interval medical abortion regimens over gestation},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Identifying the recurrence of sleep apnea using a harmonic
hidden markov model. <em>AOAS</em>, <em>15</em>(3), 1171–1193. (<a
href="https://doi.org/10.1214/21-AOAS1455">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose to model time-varying periodic and oscillatory processes by means of a hidden Markov model where the states are defined through the spectral properties of a periodic regime. The number of states is unknown along with the relevant periodicities, the role and number of which may vary across states. We address this inference problem by a Bayesian nonparametric hidden Markov model, assuming a sticky hierarchical Dirichlet process for the switching dynamics between different states while the periodicities characterizing each state are explored by means of a transdimensional Markov chain Monte Carlo sampling step. We develop the full Bayesian inference algorithm and illustrate the use of our proposed methodology for different simulation studies as well as an application related to respiratory research which focuses on the detection of apnea instances in human breathing traces.},
  archive      = {J_AOAS},
  author       = {Beniamino Hadj-Amar and Bärbel Finkenstädt and Mark Fiecas and Robert Huckstepp},
  doi          = {10.1214/21-AOAS1455},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {1171-1193},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Identifying the recurrence of sleep apnea using a harmonic hidden markov model},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Markov-switching state space models for uncovering musical
interpretation. <em>AOAS</em>, <em>15</em>(3), 1147–1170. (<a
href="https://doi.org/10.1214/21-AOAS1457">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For concertgoers, musical interpretation is the most important factor in determining whether or not we enjoy a classical performance. Every performance includes mistakes—intonation issues, a lost note, an unpleasant sound—but these are all easily forgotten (or unnoticed) when a performer engages her audience, imbuing a piece with novel emotional content beyond the vague instructions inscribed on the printed page. In this research we use data from the CHARM Mazurka Project—46 professional recordings of Chopin’s Mazurka Op. 68 No. 3 by consummate artists—with the goal of elucidating musically interpretable performance decisions. We focus specifically on each performer’s use of tempo by examining the interonset intervals of the note attacks in the recording. To explain these tempo decisions, we develop a switching state space model and estimate it by maximum likelihood, combined with prior information gained from music theory and performance practice. We use the estimated parameters to quantitatively describe individual performance decisions and compare recordings. These comparisons suggest methods for informing music instruction, discovering listening preferences and analyzing performances.},
  archive      = {J_AOAS},
  author       = {Daniel J. McDonald and Michael McBride and Yupeng Gu and Christopher Raphael},
  doi          = {10.1214/21-AOAS1457},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {1147-1170},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Markov-switching state space models for uncovering musical interpretation},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Extending models via gradient boosting: An application to
mendelian models. <em>AOAS</em>, <em>15</em>(3), 1126–1146. (<a
href="https://doi.org/10.1214/21-AOAS1482">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Improving existing widely-adopted prediction models is often a more efficient and robust way toward progress than training new models from scratch. Existing models may: (a) incorporate complex mechanistic knowledge, (b) leverage proprietary information, and (c) have surmounted barriers to adoption. Compared to model training, model improvement and modification receive little attention. In this paper we propose a general approach to model improvement: we combine gradient boosting with any previously developed model to improve model performance while retaining important existing characteristics. To exemplify, we consider the context of Mendelian models which estimate the probability of carrying genetic mutations that confer susceptibility to disease by using family pedigrees and health histories of family members. Via simulations, we show that integration of gradient boosting with an existing Mendelian model can produce an improved model that outperforms both that model and the model built using gradient boosting alone. We illustrate the approach on genetic testing data from the USC–Stanford Cancer Genetics Hereditary Cancer Panel (HCP) study.},
  archive      = {J_AOAS},
  author       = {Theodore Huang and Gregory Idos and Christine Hong and Stephen B. Gruber and Giovanni Parmigiani and Danielle Braun},
  doi          = {10.1214/21-AOAS1482},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {1126-1146},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Extending models via gradient boosting: An application to mendelian models},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Unsupervised streaming anomaly detection for instrumented
infrastructure. <em>AOAS</em>, <em>15</em>(3), 1101–1125. (<a
href="https://doi.org/10.1214/20-AOAS1424">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Structural health monitoring (SHM) often involves instrumenting structures with distributed sensor networks. These networks typically provide high frequency data describing the spatiotemporal behaviour of the assets. A main objective of SHM is to reason about changes in structures’ behaviour using sensor data. We construct a streaming anomaly detection method for data from a railway bridge instrumented with a fibre-optic sensor network. The data exhibits trend over time, which may be partially attributable to environmental factors, calling for temporally adaptive estimation. Exploiting a latent structure present in the data motivates a quantity of interest for anomaly detection. This quantity is estimated, sequentially and adaptively, using a new formulation of streaming principal component analysis. Anomaly detection for this quantity is then provided using conformal prediction. Like all streaming methods, the proposed method has free control parameters which are set using simulations based on bridge data. Experiments demonstrate that this method can operate at the sampling frequency of the data while providing accurate tracking of the target quantity. Further, the anomaly detection is able to detect train passage events. Finally, the method reveals a previously unreported cyclic structure present in the data.},
  archive      = {J_AOAS},
  author       = {Henrique Hoeltgebaum and Niall Adams and F. Din-Houn Lau},
  doi          = {10.1214/20-AOAS1424},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {1101-1125},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Unsupervised streaming anomaly detection for instrumented infrastructure},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A deep learning semiparametric regression for adjusting
complex confounding structures. <em>AOAS</em>, <em>15</em>(3),
1086–1100. (<a href="https://doi.org/10.1214/21-AOAS1481">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep Treatment Learning (deepTL), a robust yet efficient deep learning-based semiparametric regression approach, is proposed to adjust the complex confounding structures in comparative effectiveness analysis of observational data, for example, electronic health record (EHR) data in which complex confounding structures are often embedded. Specifically, we develop a deep learning neural network with a score-based ensembling scheme for flexible function approximation. An improved semiparametric procedure is further developed to enhance the performance of the proposed method under finite sample settings. Comprehensive numerical studies have demonstrated the superior performance of the proposed methods, as compared with existing methods, with a remarkably reduced bias and mean squared error in parameter estimates. The proposed research is motivated by a postsurgery pain study, which is also used to illustrate the practical application of deepTL. Finally, an R package, “deepTL,” is developed to implement the proposed method.},
  archive      = {J_AOAS},
  author       = {Xinlei Mi and Patrick Tighe and Fei Zou and Baiming Zou},
  doi          = {10.1214/21-AOAS1481},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {1086-1100},
  shortjournal = {Ann. Appl. Stat.},
  title        = {A deep learning semiparametric regression for adjusting complex confounding structures},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The ASA president’s task force statement on statistical
significance and replicability. <em>AOAS</em>, <em>15</em>(3),
1084–1085. (<a href="https://doi.org/10.1214/21-AOAS1501">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_AOAS},
  author       = {Yoav Benjamini and Richard D. De Veaux and Bradley Efron and Scott Evans and Mark Glickman and Barry I. Graubard and Xuming He and Xiao-Li Meng and Nancy Reid and Stephen M. Stigler and Stephen B. Vardeman and Christopher K. Wikle and Tommy Wright and Linda J. Young and Karen Kafadar},
  doi          = {10.1214/21-AOAS1501},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {1084-1085},
  shortjournal = {Ann. Appl. Stat.},
  title        = {The ASA president’s task force statement on statistical significance and replicability},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). EDITORIAL: Statistical significance, p-values, and
replicability. <em>AOAS</em>, <em>15</em>(3), 1081–1083. (<a
href="https://doi.org/10.1214/21-AOAS1500">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_AOAS},
  author       = {Karen Kafadar},
  doi          = {10.1214/21-AOAS1500},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {1081-1083},
  shortjournal = {Ann. Appl. Stat.},
  title        = {EDITORIAL: Statistical significance, P-values, and replicability},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A bridging model to reconcile statistics based on data from
multiple surveys. <em>AOAS</em>, <em>15</em>(2), 1068–1079. (<a
href="https://doi.org/10.1214/20-AOAS1437">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Surveys designed to collect data on similar variables using samples representing the same population may still result in different estimates due, for example, to differences in sample designs or modes of data collection. Considered in this paper is the case where two surveys were conducted concurrently, with one using the same methodology as used in prior rounds of the survey and the other using an updated methodology, resulting in substantial differences in several key estimates. Due to differences in sample size, only the latter survey was detailed enough for disaggregated-level estimates of publishable quality. We propose a hierarchical model to account for discrepancies in the estimates from the two surveys and a Bayesian approach for producing reliable estimates at various levels of aggregation. The model relies on a common latent structure at the disaggregated level to allow “bridging” between the two surveys. The methodology is applied to the 2016 National Survey of Fishing, Hunting and Wildlife-Associated Recreation and the 2016 50-State Surveys of Fishing, Hunting and Wildlife-Related Recreation. Aligning these two surveys is critical to extend the series of related statistics that have been published since 1955, allowing for meaningful comparisons over time despite the change in survey methodology.},
  archive      = {J_AOAS},
  author       = {Andreea L. Erciulescu and Jean D. Opsomer and F. Jay Breidt},
  doi          = {10.1214/20-AOAS1437},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {1068-1079},
  shortjournal = {Ann. Appl. Stat.},
  title        = {A bridging model to reconcile statistics based on data from multiple surveys},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Length-biased semicompeting risks models for cross-sectional
data: An application to current duration of pregnancy attempt data.
<em>AOAS</em>, <em>15</em>(2), 1054–1067. (<a
href="https://doi.org/10.1214/20-AOAS1428">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-sectional length-biased data arise from questions on the at-risk time for an event of interest from those who are at risk but have yet to experience the event. For example, in the National Survey on Family Growth (NSFG) women, who were currently attempting to become pregnant, were asked how long they had been attempting pregnancy. Cross-sectional survival analysis methods use the observed at-risk times to make inference on the distribution of the unobserved time-to-failure. However, methodological gaps in these methods remain such as how to handle semicompeting risks. For example, if the women attempting pregnancy had undergone fertility treatment during their current pregnancy attempt. In this paper we develop statistical methods that extend cross-sectional survival analysis methods to incorporate semicompeting risks. They can be used to estimate the distribution of the length of natural pregnancy attempts (i.e., without fertility treatment) while correctly accounting for women that sought fertility treatment prior to being sampled using cross-sectional data. We demonstrate our approach based on simulated data and an analysis of data from the NSFG. The proposed method results in separate survival curves for time-to-natural-pregnancy, time-to-fertility treatment and time-to-pregnancy after fertility treatment.},
  archive      = {J_AOAS},
  author       = {Alexander C. McLain and Siyuan Guo and Marie Thoma and Jiajia Zhang},
  doi          = {10.1214/20-AOAS1428},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {1054-1067},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Length-biased semicompeting risks models for cross-sectional data: An application to current duration of pregnancy attempt data},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Inflection points in community-level homeless rates.
<em>AOAS</em>, <em>15</em>(2), 1037–1053. (<a
href="https://doi.org/10.1214/20-AOAS1414">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Statistical models of community-level homeless rates typically assume a linear relationship to covariates. This linear model assumption precludes the possibility of inflection points in homeless rates—thresholds in quantifiable metrics of a community that, once breached, are associated with large increases in homelessness. In this paper we identify points of structural change in the relationship between homeless rates and community-level measures of housing affordability and extreme poverty. We utilize the Ewens–Pitman attraction (EPA) distribution to develop a Bayesian nonparametric regression model in which clusters of communities with similar covariates share common patterns of variation in homeless rates. A main finding of the study is that the expected homeless rate in a community begins to quickly increase once median rental costs exceed 30\% of median income, providing a statistical link between homelessness and the U.S. government’s definition of a housing cost burden. Our analysis also identifies clusters of communities that exhibit distinct geographic patterns and yields insight into the homelessness and housing affordability crisis unfolding on both coasts of the United States.},
  archive      = {J_AOAS},
  author       = {Chris Glynn and Thomas H. Byrne and Dennis P. Culhane},
  doi          = {10.1214/20-AOAS1414},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {1037-1053},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Inflection points in community-level homeless rates},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Tensor-variate finite mixture modeling for the analysis of
university professor remuneration. <em>AOAS</em>, <em>15</em>(2),
1017–1036. (<a href="https://doi.org/10.1214/20-AOAS1420">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There has been a long-standing interest in the analysis of university professor salary data. The vast majority of the publications on the topic employ linear regression models in an attempt to predict individual salaries. Indeed, the administration of any academic institution is interested in adequately compensating the faculty to attract and keep the best specialists available on the market. However, higher administration and legislators are not concerned with the matter of individual compensation and need to have a bigger picture for developing university strategies and policies. This paper is the first attempt to model university compensation data at the institutional level. The analysis of university salary patterns is a challenging problem due to the heterogeneous, skewed, multiway and temporal nature of the data. This paper aims at addressing all the above-mentioned issues by proposing a novel tensor regression mixture model and applying it to the data set obtained from the American Association of University Professors. The utility of the developed model is illustrated on addressing several important questions related to gender equity and peer institution comparison.},
  archive      = {J_AOAS},
  author       = {Shuchismita Sarkar and Volodymyr Melnykov and Xuwen Zhu},
  doi          = {10.1214/20-AOAS1420},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {1017-1036},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Tensor-variate finite mixture modeling for the analysis of university professor remuneration},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A covariance-enhanced approach to multitissue joint eQTL
mapping with application to transcriptome-wide association studies.
<em>AOAS</em>, <em>15</em>(2), 998–1016. (<a
href="https://doi.org/10.1214/20-AOAS1432">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transcriptome-wide association studies based on genetically predicted gene expression have the potential to identify novel regions associated with various complex traits. It has been shown that incorporating expression quantitative trait loci (eQTLs) corresponding to multiple tissue types can improve power for association studies involving complex etiology. In this article we propose a new multivariate response linear regression model and method for predicting gene expression in multiple tissues simultaneously. Unlike existing methods for multitissue joint eQTL mapping, our approach incorporates tissue-tissue expression correlation which allows us to more efficiently handle missing expression measurements and to more accurately predict gene expression using a weighted summation of eQTL genotypes. We show through simulation studies that our approach performs better than the existing methods in many scenarios. We use our method to estimate eQTL weights for 29 tissues collected by GTEx, and show that our approach significantly improves expression prediction accuracy compared to competitors. Using our eQTL weights, we perform a multitissue-based S-MultiXcan (PLoS Genet. 15 (2019) e1007889) transcriptome-wide association study and show that our method leads to more discoveries in novel regions and more discoveries overall than the existing methods. Estimated eQTL weights and code for implementing the method are available for download online at github.com/ajmolstad/MTeQTLResults.},
  archive      = {J_AOAS},
  author       = {Aaron J. Molstad and Wei Sun and Li Hsu},
  doi          = {10.1214/20-AOAS1432},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {998-1016},
  shortjournal = {Ann. Appl. Stat.},
  title        = {A covariance-enhanced approach to multitissue joint eQTL mapping with application to transcriptome-wide association studies},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Efficient bayesian inference of general gaussian models on
large phylogenetic trees. <em>AOAS</em>, <em>15</em>(2), 971–997. (<a
href="https://doi.org/10.1214/20-AOAS1419">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Phylogenetic comparative methods correct for shared evolutionary history among a set of nonindependent organisms by modeling sample traits as arising from a diffusion process along the branches of a possibly unknown history. To incorporate such uncertainty, we present a scalable Bayesian inference framework under a general Gaussian trait evolution model that exploits Hamiltonian Monte Carlo (HMC). HMC enables efficient sampling of the constrained model parameters and takes advantage of the tree structure for fast likelihood and gradient computations, yielding algorithmic complexity linear in the number of observations. This approach encompasses a wide family of stochastic processes, including the general Ornstein–Uhlenbeck (OU) process, with possible missing data and measurement errors. We implement inference tools for a biologically relevant subset of all these models into the BEAST phylogenetic software package and develop model comparison through marginal likelihood estimation. We apply our approach to study the morphological evolution in the superfamily of Musteloidea (including weasels and allies) as well as the heritability of HIV virulence. This second problem furnishes a new measure of evolutionary heritability that demonstrates its utility through a targeted simulation study.},
  archive      = {J_AOAS},
  author       = {Paul Bastide and Lam Si Tung Ho and Guy Baele and Philippe Lemey and Marc A. Suchard},
  doi          = {10.1214/20-AOAS1419},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {971-997},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Efficient bayesian inference of general gaussian models on large phylogenetic trees},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Prediction of the NASH through penalized mixture of logistic
regression models. <em>AOAS</em>, <em>15</em>(2), 952–970. (<a
href="https://doi.org/10.1214/20-AOAS1409">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper an appropriate and interpretable diagnosis statistical model is proposed to predict Nonalcoholic Steatohepatitis (NASH) from near infrared spectrometry data. In this disease, unknown patients’ profiles are expected to lead to a different diagnosis. The model has then to take into account the heterogeneity of the data and the dimension of the spectrometric data. To this end, we propose to fit a mixture model on the joint distribution of the diagnostic binary variable and the covariates selected in the spectra. The penalized maximum likelihood estimator is considered. In practice, a twofold penalty on both regression coefficients and covariance parameters is imposed. Automatic selection criteria, such as the AIC and BIC, are used to select the amount of shrinkage and the number of clusters. The performance of the overall procedure is evaluated by a simulation study, and its application on the NASH data set is analyzed. The model leads to better prediction performance than competitive methods and provides highly interpretable results.},
  archive      = {J_AOAS},
  author       = {Marie Morvan and Emilie Devijver and Madison Giacofci and Valérie Monbet},
  doi          = {10.1214/20-AOAS1409},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {952-970},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Prediction of the NASH through penalized mixture of logistic regression models},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A bayesian nonparametric model for inferring subclonal
populations from structured DNA sequencing data. <em>AOAS</em>,
<em>15</em>(2), 925–951. (<a
href="https://doi.org/10.1214/20-AOAS1434">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There are distinguishing features or “hallmarks” of cancer that are found across tumors, individuals and types of cancer, and these hallmarks can be driven by specific genetic mutations. Yet within a single tumor there is often extensive genetic heterogeneity as evidenced by single-cell and bulk DNA sequencing data. The goal of this work is to jointly infer the underlying genotypes of tumor subpopulations and the distribution of those subpopulations in individual tumors by integrating single-cell and bulk sequencing data. Understanding the genetic composition of the tumor at the time of treatment is important in the personalized design of targeted therapeutic combinations and monitoring for possible recurrence after treatment. We propose a hierarchical Dirichlet process mixture model that incorporates the correlation structure induced by a structured sampling arrangement, and we show that this model improves the quality of inference. We develop a representation of the hierarchical Dirichlet process prior as a Gamma–Poisson hierarchy, and we use this representation to derive a fast Gibbs sampling inference algorithm using the augment-and-marginalize method. Experiments with simulation data show that our model outperforms standard numerical and statistical methods for decomposing admixed count data. Analyses of real acute lymphoblastic leukemia cancer sequencing dataset shows that our model improves upon state-of-the-art bioinformatic methods. An interpretation of the results of our model on this real dataset reveals comutated loci across samples.},
  archive      = {J_AOAS},
  author       = {Shai He and Aaron Schein and Vishal Sarsani and Patrick Flaherty},
  doi          = {10.1214/20-AOAS1434},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {925-951},
  shortjournal = {Ann. Appl. Stat.},
  title        = {A bayesian nonparametric model for inferring subclonal populations from structured DNA sequencing data},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Large-scale multiple inference of collective dependence with
applications to protein function. <em>AOAS</em>, <em>15</em>(2),
902–924. (<a href="https://doi.org/10.1214/20-AOAS1431">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Measuring the dependence of k≥3 random variables and drawing inference from such higher-order dependences are scientifically important yet challenging. Motivated here by protein coevolution with multivariate categorical features, we consider an information theoretic measure of higher-order dependence. The proposed collective dependence is a symmetrization of differential interaction information which generalizes the mutual information of a pair of random variables. We show that the collective dependence can be easily estimated and facilitates a test on the dependence of k≥3 random variables. Upon carefully exploring the null space of collective dependence, we devise a Classification-Assisted Large scaLe inference procedure to DEtect significant k-COllective DEpendence among d≥k random variables, with the false discovery rate controlled. Finite sample performance of our method is examined via simulations. We apply this method to the multiple protein sequence alignment data to study the residue or position coevolution for two protein families, the elongation factor P family and the zinc knuckle family. We identify novel functional triplets of amino acid residues, whose contributions to the protein function are further investigated. These confirm that the collective dependence does yield additional information important for understanding the protein coevolution compared to the pairwise measures.},
  archive      = {J_AOAS},
  author       = {Robert Jernigan and Kejue Jia and Zhao Ren and Wen Zhou},
  doi          = {10.1214/20-AOAS1431},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {902-924},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Large-scale multiple inference of collective dependence with applications to protein function},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A compositional model to assess expression changes from
single-cell RNA-seq data. <em>AOAS</em>, <em>15</em>(2), 880–901. (<a
href="https://doi.org/10.1214/20-AOAS1423">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {On the problem of scoring genes for evidence of changes in the distribution of single-cell expression, we introduce an empirical Bayesian mixture approach and evaluate its operating characteristics in a range of numerical experiments. The proposed approach leverages cell-subtype structure revealed in cluster analysis in order to boost gene-level information on expression changes. Cell clustering informs gene-level analysis through a specially-constructed prior distribution over pairs of multinomial probability vectors; this prior meshes with available model-based tools that score patterns of differential expression over multiple subtypes. We derive an explicit formula for the posterior probability that a gene has the same distribution in two cellular conditions, allowing for a gene-specific mixture over subtypes in each condition. Advantage is gained by the compositional structure of the model not only in which a host of gene-specific mixture components are allowed but also in which the mixing proportions are constrained at the whole cell level. This structure leads to a novel form of information sharing through which the cell-clustering results support gene-level scoring of differential distribution. The result, according to our numerical experiments, is improved sensitivity compared to several standard approaches for detecting distributional expression changes.},
  archive      = {J_AOAS},
  author       = {Xiuyu Ma and Keegan Korthauer and Christina Kendziorski and Michael A. Newton},
  doi          = {10.1214/20-AOAS1423},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {880-901},
  shortjournal = {Ann. Appl. Stat.},
  title        = {A compositional model to assess expression changes from single-cell RNA-seq data},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Two-way sparsity for time-varying networks with applications
in genomics. <em>AOAS</em>, <em>15</em>(2), 856–879. (<a
href="https://doi.org/10.1214/20-AOAS1416">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel way of modelling time-varying networks by inducing two-way sparsity on local models of node connectivity. This two-way sparsity separately promotes sparsity across time and sparsity across variables (within time). Separation of these two types of sparsity is achieved through a novel prior structure which draws on ideas from the Bayesian lasso and from copula modelling. We provide an efficient implementation of the proposed model via a Gibbs sampler, and we apply the model to data from neural development. In doing so, we demonstrate that the proposed model is able to identify changes in genomic network structure that match current biological knowledge. Such changes in genomic network structure can then be used by neurobiologists to identify potential targets for further experimental investigation.},
  archive      = {J_AOAS},
  author       = {Thomas E. Bartlett and Ioannis Kosmidis and Ricardo Silva},
  doi          = {10.1214/20-AOAS1416},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {856-879},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Two-way sparsity for time-varying networks with applications in genomics},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Inference of large modified poisson-type graphical models:
Application to RNA-seq data in childhood atopic asthma studies.
<em>AOAS</em>, <em>15</em>(2), 831–855. (<a
href="https://doi.org/10.1214/20-AOAS1413">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in next-generation sequencing technology have yielded huge amounts of transcriptomic data. The discreteness and the high dimensions of RNA-seq data have posed great challenges in biological network analysis. Although estimation theories for high-dimensional modified Poisson-type graphical models have been proposed for the network analysis of count-valued data, the statistical inference of these models is still largely unknown. We herein propose a two-step procedure in both edgewise and global statistical inference of these modified Poisson-type graphical models using a cutting-edge generalized low-dimensional projection approach for bias correction. Extensive simulations and a real example with ground truth illustrate asymptotic normality of edgewise inference and more accurate inferential results in multiple testing compared to the sole estimation and the inferential method under normal assumption. Furthermore, the application of our method to novel RNA-seq data of childhood atopic asthma in Puerto Ricans demonstrates more biologically meaningful results compared to the sole estimation and the inferential methods based on Gaussian and nonparanormal graphical models.},
  archive      = {J_AOAS},
  author       = {Rong Zhang and Zhao Ren and Juan C. Celedón and Wei Chen},
  doi          = {10.1214/20-AOAS1413},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {831-855},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Inference of large modified poisson-type graphical models: Application to RNA-seq data in childhood atopic asthma studies},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A bayesian semiparametric jolly–seber model with individual
heterogeneity: An application to migratory mallards at stopover.
<em>AOAS</em>, <em>15</em>(2), 813–830. (<a
href="https://doi.org/10.1214/20-AOAS1421">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a Bayesian hierarchical Jolly–Seber model that can accommodate a semiparametric functional relationship between external covariates and capture probabilities, individual heterogeneity in departure due to an internal time-varying covariate and the dependence of arrival time on external covariates. Modelwise, we consider a stochastic process to characterize the evolution of the partially observable internal covariate that is linked to departure probabilities. Computationally, we develop a well-tailored Markov chain Monte Carlo algorithm that is free of tuning through data augmentation. Inferentially, our model allows us to make inference about stopover duration and population sizes, the impacts of various covariates on departure and arrival time and to identify flexible yet data-driven functional relationships between external covariates and capture probabilities. We demonstrate the effectiveness of our model through a motivating dataset collected for studying the migration of mallards (Anas platyrhynchos) in Sweden.},
  archive      = {J_AOAS},
  author       = {Guohui Wu and Scott H. Holan and Alexis Avril and Jonas Waldenström},
  doi          = {10.1214/20-AOAS1421},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {813-830},
  shortjournal = {Ann. Appl. Stat.},
  title        = {A bayesian semiparametric Jolly–Seber model with individual heterogeneity: An application to migratory mallards at stopover},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A continuous-time semi-markov model for animal movement in a
dynamic environment. <em>AOAS</em>, <em>15</em>(2), 797–812. (<a
href="https://doi.org/10.1214/20-AOAS1408">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider an extension to discrete-space, continuous-time models for animal movement that have previously been presented in the literature. The extension from a continuous-time Markov formulation to a continuous-time semi-Markov formulation allows for the inclusion of temporally dynamic habitat conditions as well as temporally changing movement responses by animals to that environment. We show that, with only a little additional consideration, the Poisson likelihood calculation for the Markov version can still be used within the multiple imputation framework commonly employed for analysis of telemetry data. In addition, we consider a Bayesian model selection methodology within the imputation framework. The model selection method uses a Laplace approximation to the posterior model probability to provide a computationally feasible approach. The full methodology is then used to analyze movements of 15 weaned northern fur seal (Callorhinus ursinus) pups with respect to surface winds, geostrophic currents and sea surface temperature. The highest posterior model probabilities belonged to those models containing only winds and current; SST was not a significant factor for modeling their movement.},
  archive      = {J_AOAS},
  author       = {Devin Johnson and Noel Pelland and Jeremy Sterling},
  doi          = {10.1214/20-AOAS1408},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {797-812},
  shortjournal = {Ann. Appl. Stat.},
  title        = {A continuous-time semi-markov model for animal movement in a dynamic environment},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Rapid design of metamaterials via multitarget bayesian
optimization. <em>AOAS</em>, <em>15</em>(2), 768–796. (<a
href="https://doi.org/10.1214/20-AOAS1426">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Composed of a large number of subwavelength unit cells with designable geometries, metamaterials have been widely studied to achieve extraordinary advantageous and unusual optical properties. However, ordinary computer simulator requires a time-consuming fine-tuning to find a proper design of metamaterial for a specific optical property, making the design stage a critical bottleneck in large scale applications of metamaterials. This paper investigates the metamaterial design under the framework of computer experiments, with emphasis on dealing with the challenge of designing numerous unit cells with functional responses, simultaneously, which is not common in traditional computer experiments. We formulate the multiple related design targets as a multitarget design problem. Leveraging on the similarity between different designs, we propose an efficient Bayesian optimization strategy with a parsimonious surrogate model and an integrated acquisition function to design multiple unit cells with very few function evaluations. A wide range of simulations confirm the effectiveness and superiority of the proposed approach compared to the naive strategies where the multiple unit cells are dealt with separately or sequentially. Such a rapid design strategy has the potential to greatly promote large scale applications of metamaterials in practice.},
  archive      = {J_AOAS},
  author       = {Yang Yang and Chunlin Ji and Ke Deng},
  doi          = {10.1214/20-AOAS1426},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {768-796},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Rapid design of metamaterials via multitarget bayesian optimization},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Causal mediation analysis for sparse and irregular
longitudinal data. <em>AOAS</em>, <em>15</em>(2), 747–767. (<a
href="https://doi.org/10.1214/20-AOAS1427">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Causal mediation analysis seeks to investigate how the treatment effect of an exposure on outcomes is mediated through intermediate variables. Although many applications involve longitudinal data, the existing methods are not directly applicable to settings where the mediator and outcome are measured on sparse and irregular time grids. We extend the existing causal mediation framework from a functional data analysis perspective, viewing the sparse and irregular longitudinal data as realizations of underlying smooth stochastic processes. We define causal estimands of direct and indirect effects accordingly and provide corresponding identification assumptions. For estimation and inference, we employ a functional principal component analysis approach for dimension reduction and use the first few functional principal components instead of the whole trajectories in the structural equation models. We adopt the Bayesian paradigm to accurately quantify the uncertainties. The operating characteristics of the proposed methods are examined via simulations. We apply the proposed methods to a longitudinal data set from a wild baboon population in Kenya to investigate the causal relationships between early adversity, strength of social bonds between animals and adult glucocorticoid hormone concentrations. We find that early adversity has a significant direct effect (a 9–14\% increase) on females’ glucocorticoid concentrations across adulthood but find little evidence that these effects were mediated by weak social bonds.},
  archive      = {J_AOAS},
  author       = {Shuxi Zeng and Stacy Rosenbaum and Susan C. Alberts and Elizabeth A. Archie and Fan Li},
  doi          = {10.1214/20-AOAS1427},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {747-767},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Causal mediation analysis for sparse and irregular longitudinal data},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Additive stacking for disaggregate electricity demand
forecasting. <em>AOAS</em>, <em>15</em>(2), 727–746. (<a
href="https://doi.org/10.1214/20-AOAS1417">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Future grid management systems will coordinate distributed production and storage resources to manage, in a cost effective fashion, the increased load and variability brought by the electrification of transportation and by a higher share of weather dependent production. Electricity demand forecasts at a low level of aggregation will be key inputs for such systems. We focus on forecasting demand at the individual household level, which is more challenging than forecasting aggregate demand, due to the lower signal-to-noise ratio and to the heterogeneity of consumption patterns across households. We propose a new ensemble method for probabilistic forecasting which borrows strength across the households while accommodating their individual idiosyncrasies. In particular, we develop a set of models or “experts” which capture different demand dynamics, and we fit each of them to the data from each household. Then, we construct an aggregation of experts where the ensemble weights are estimated on the whole data set, the main innovation being that we let the weights vary with the covariates by adopting an additive model structure. In particular, the proposed aggregation method is an extension of regression stacking where the mixture weights are modelled using linear combinations of parametric, smooth or random effects. The methods for building and fitting additive stacking models are implemented by the gamFactory R package, available at https://github.com/mfasiolo/gamFactory.},
  archive      = {J_AOAS},
  author       = {Christian Capezza and Biagio Palumbo and Yannig Goude and Simon N. Wood and Matteo Fasiolo},
  doi          = {10.1214/20-AOAS1417},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {727-746},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Additive stacking for disaggregate electricity demand forecasting},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Probabilistic forecasting of the arctic sea ice edge with
contour modeling. <em>AOAS</em>, <em>15</em>(2), 711–726. (<a
href="https://doi.org/10.1214/20-AOAS1405">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sea ice, or frozen ocean water, freezes and melts every year in the Arctic. Forecasts of where sea ice will be located weeks to months in advance have become more important as the amount of sea ice declines due to climate change, for maritime planning and other uses. Typical sea ice forecasts are made with ensemble models, physics-based models of sea ice and the surrounding ocean and atmosphere. This paper introduces Mixture Contour Forecasting, a method to forecast sea ice probabilistically using a mixture of two distributions, one based on postprocessed output from ensembles and the other on observed sea ice patterns in recent years. At short lead times, these forecasts are better calibrated than unadjusted dynamic ensemble forecasts and other statistical reference forecasts. To produce these forecasts, a statistical technique is introduced that directly models the sea ice edge contour, the boundary around the region that is ice-covered. Mixture Contour Forecasting and reference methods are evaluated for monthly sea ice forecasts for 2008–2016 at lead times ranging from 0.5–6.5 months using one of the European Centre for Medium-Range Weather Forecasts ensembles.},
  archive      = {J_AOAS},
  author       = {Hannah M. Director and Adrian E. Raftery and Cecilia M. Bitz},
  doi          = {10.1214/20-AOAS1405},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {711-726},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Probabilistic forecasting of the arctic sea ice edge with contour modeling},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Scalable penalized spatiotemporal land-use regression for
ground-level nitrogen dioxide. <em>AOAS</em>, <em>15</em>(2), 688–710.
(<a href="https://doi.org/10.1214/20-AOAS1422">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nitrogen dioxide (NO2) is a primary constituent of traffic-related air pollution and has well-established harmful environmental and human-health impacts. Knowledge of the spatiotemporal distribution of NO2 is critical for exposure and risk assessment. A common approach for assessing air pollution exposure is linear regression involving spatially referenced covariates, known as land-use regression (LUR). We develop a scalable approach for simultaneous variable selection and estimation of LUR models with spatiotemporally correlated errors, by combining a general-Vecchia Gaussian-process approximation with a penalty on the LUR coefficients. In comparison to existing methods using simulated data, our approach resulted in higher model-selection specificity and sensitivity and in better prediction in terms of calibration and sharpness, for a wide range of relevant settings. In our spatiotemporal analysis of daily, US-wide, ground-level NO2 data, our approach was more accurate, and produced a sparser and more interpretable model. Our daily predictions elucidate spatiotemporal patterns of NO2 concentrations across the United States, including significant variations between cities and intra-urban variation. Thus, our predictions will be useful for epidemiological and risk-assessment studies seeking daily, national-scale predictions, and they can be used in acute-outcome health-risk assessments.},
  archive      = {J_AOAS},
  author       = {Kyle P. Messier and Matthias Katzfuss},
  doi          = {10.1214/20-AOAS1422},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {688-710},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Scalable penalized spatiotemporal land-use regression for ground-level nitrogen dioxide},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Simultaneous inference of periods and period-luminosity
relations for mira variable stars. <em>AOAS</em>, <em>15</em>(2),
662–687. (<a href="https://doi.org/10.1214/21-AOAS1440">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The period-luminosity relation (PLR) of Mira variable stars is an important tool to determine astronomical distances. The common approach of estimating the PLR is a two-step procedure that first estimates the Mira periods and then runs a linear regression of magnitude on log period. When the light curves are sparse and noisy, the accuracy of period estimation decreases and can suffer from aliasing effects. Some methods improve accuracy by incorporating complex model structures at the expense of significant computational costs. Another drawback of existing methods is that they only provide point estimation without proper estimation of uncertainty. To overcome these challenges, we develop a hierarchical Bayesian model that simultaneously models the quasi-periodic variations for a collection of Mira light curves while estimating their common PLR. By borrowing strengths through the PLR, our method automatically reduces the aliasing effect, improves the accuracy of period estimation and is capable of characterizing the estimation uncertainty. We develop a scalable stochastic variational inference algorithm for computation that can effectively deal with the multimodal posterior of period. The effectiveness of the proposed method is demonstrated through simulations and an application to observations of Miras in the Local Group galaxy M33. Without using ad hoc period correction tricks, our method achieves a distance estimate of M33 that is consistent with published work. Our method also shows superior robustness to downsampling of the light curves.},
  archive      = {J_AOAS},
  author       = {Shiyuan He and Zhenfeng Lin and Wenlong Yuan and Lucas M. Macri and Jianhua Z. Huang},
  doi          = {10.1214/21-AOAS1440},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {662-687},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Simultaneous inference of periods and period-luminosity relations for mira variable stars},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A statistical pipeline for identifying physical features
that differentiate classes of 3D shapes. <em>AOAS</em>, <em>15</em>(2),
638–661. (<a href="https://doi.org/10.1214/20-AOAS1430">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recent curation of large-scale databases with 3D surface scans of shapes has motivated the development of tools that better detect global patterns in morphological variation. Studies, which focus on identifying differences between shapes, have been limited to simple pairwise comparisons and rely on prespecified landmarks (that are often known). We present SINATRA, the first statistical pipeline for analyzing collections of shapes without requiring any correspondences. Our novel algorithm takes in two classes of shapes and highlights the physical features that best describe the variation between them. We use a rigorous simulation framework to assess our approach. Lastly, as a case study we use SINATRA to analyze mandibular molars from four different suborders of primates and demonstrate its ability recover known morphometric variation across phylogenies.},
  archive      = {J_AOAS},
  author       = {Bruce Wang and Timothy Sudijono and Henry Kirveslahti and Tingran Gao and Douglas M. Boyer and Sayan Mukherjee and Lorin Crawford},
  doi          = {10.1214/20-AOAS1430},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {638-661},
  shortjournal = {Ann. Appl. Stat.},
  title        = {A statistical pipeline for identifying physical features that differentiate classes of 3D shapes},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Aggregated pairwise classification of elastic planar shapes.
<em>AOAS</em>, <em>15</em>(2), 619–637. (<a
href="https://doi.org/10.1214/21-AOAS1452">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The classification of shapes is of great interest in diverse areas ranging from medical imaging to computer vision and beyond. While many statistical frameworks have been developed for the classification problem, most are strongly tied to early formulations of the problem with an object to be classified described as a vector in a relatively low-dimensional Euclidean space. Statistical shape data have two main properties that suggest a need for a novel approach: (i) shapes are inherently infinite-dimensional with strong dependence among the positions of nearby points, and (ii) shape space is not Euclidean but is fundamentally curved. To accommodate these features of the data, we work with the square-root velocity function of the curves to provide a useful formal description of the shape, pass to tangent spaces of the manifold of shapes at projection points (which effectively separate shapes for pairwise classification in the training data) and use principal components within these tangent spaces to reduce dimensionality. We illustrate the impact of the projection point and choice of subspace on the misclassification rate with a novel method of combining pairwise classifiers.},
  archive      = {J_AOAS},
  author       = {Min Ho Cho and Sebastian Kurtek and Steven N. MacEachern},
  doi          = {10.1214/21-AOAS1452},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {619-637},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Aggregated pairwise classification of elastic planar shapes},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning excursion sets of vector-valued gaussian random
fields for autonomous ocean sampling. <em>AOAS</em>, <em>15</em>(2),
597–618. (<a href="https://doi.org/10.1214/21-AOAS1451">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Improving and optimizing oceanographic sampling is a crucial task for marine science and maritime resource management. Faced with limited resources in understanding processes in the water column, the combination of statistics and autonomous systems provides new opportunities for experimental design. In this work we develop efficient spatial sampling methods for characterizing regions, defined by simultaneous exceedances above prescribed thresholds of several responses, with an application focus on mapping coastal ocean phenomena based on temperature and salinity measurements. Specifically, we define a design criterion based on uncertainty in the excursions of vector-valued Gaussian random fields and derive tractable expressions for the expected integrated Bernoulli variance reduction in such a framework. We demonstrate how this criterion can be used to prioritize sampling efforts at locations that are ambiguous, making exploration more effective. We use simulations to study and compare properties of the considered approaches, followed by results from field deployments with an autonomous underwater vehicle as part of a study mapping the boundary of a river plume. The results demonstrate the potential of combining statistical methods and robotic platforms to effectively inform and execute data-driven environmental sampling.},
  archive      = {J_AOAS},
  author       = {Trygve Olav Fossum and Cédric Travelletti and Jo Eidsvik and David Ginsbourger and Kanna Rajan},
  doi          = {10.1214/21-AOAS1451},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {597-618},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Learning excursion sets of vector-valued gaussian random fields for autonomous ocean sampling},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Estimating high-resolution red sea surface temperature
hotspots, using a low-rank semiparametric spatial model. <em>AOAS</em>,
<em>15</em>(2), 572–596. (<a
href="https://doi.org/10.1214/20-AOAS1418">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we estimate extreme sea surface temperature (SST) hotspots, that is, high threshold exceedance regions, for the Red Sea, a vital region of high biodiversity. We analyze high-resolution satellite-derived SST data comprising daily measurements at 16,703 grid cells across the Red Sea over the period 1985–2015. We propose a semiparametric Bayesian spatial mixed-effects linear model with a flexible mean structure to capture spatially-varying trend and seasonality, while the residual spatial variability is modeled through a Dirichlet process mixture (DPM) of low-rank spatial Student’s t processes (LTPs). By specifying cluster-specific parameters for each LTP mixture component, the bulk of the SST residuals influence tail inference and hotspot estimation only moderately. Our proposed model has a nonstationary mean, covariance, and tail dependence, and posterior inference can be drawn efficiently through Gibbs sampling. In our application, we show that the proposed method outperforms some natural parametric and semiparametric alternatives. Moreover, we show how hotspots can be identified, and we estimate extreme SST hotspots for the whole Red Sea, projected until the year 2100, based on the Representative Concentration Pathways 4.5 and 8.5. The estimated 95\% credible region, for joint high threshold exceedances include large areas covering major endangered coral reefs in the southern Red Sea.},
  archive      = {J_AOAS},
  author       = {Arnab Hazra and Raphaël Huser},
  doi          = {10.1214/20-AOAS1418},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {572-596},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Estimating high-resolution red sea surface temperature hotspots, using a low-rank semiparametric spatial model},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hierarchical integrated spatial process modeling of monotone
west antarctic snow density curves. <em>AOAS</em>, <em>15</em>(2),
556–571. (<a href="https://doi.org/10.1214/21-AOAS1443">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Snow density estimates below the surface, used with airplane-acquired ice-penetrating radar measurements, give a site-specific history of snow water accumulation. Because it is infeasible to drill snow cores across all of Antarctica to measure snow density and because it is critical to understand how climatic changes are affecting the world’s largest freshwater reservoir, we develop methods that enable snow density estimation with uncertainty in regions where snow cores have not been drilled. In inland West Antarctica, snow density increases monotonically as a function of depth, except for possible microscale variability or measurement error, and it cannot exceed the density of ice. We present a novel class of integrated spatial process models that allow interpolation of monotone snow density curves. For computational feasibility we construct the space-depth process through kernel convolutions of log-Gaussian spatial processes. We discuss model comparison, model fitting and prediction. Using this model, we extend estimates of snow density beyond the depth of the original core and estimate snow density curves where snow cores have not been drilled. Along flight lines with ice-penetrating radar, we use interpolated snow density curves to estimate recent water accumulation and find predominantly decreasing water accumulation over recent decades.},
  archive      = {J_AOAS},
  author       = {Philip A. White and Durban G. Keeler and Summer Rupper},
  doi          = {10.1214/21-AOAS1443},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {556-571},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Hierarchical integrated spatial process modeling of monotone west antarctic snow density curves},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A hermite–gaussian based exoplanet radial velocity
estimation method. <em>AOAS</em>, <em>15</em>(2), 527–555. (<a
href="https://doi.org/10.1214/20-AOAS1406">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the first successful technique used to detect exoplanets orbiting distant stars, the radial velocity method aims to detect a periodic Doppler shift in a stellar spectrum due to the star’s motion along the line sight. We introduce a new, mathematically rigorous approach to detect such a signal that accounts for the smooth functional relationship of neighboring wavelengths in the spectrum, minimizes the role of wavelength interpolation, accounts for heteroskedastic noise and easily allows for accurate calculation of the estimated radial velocity standard error. Using Hermite–Gaussian functions, we show that the problem of detecting a Doppler shift in the spectrum can be reduced to linear regression in many settings. A simulation study demonstrates that the proposed method is able to accurately estimate an individual spectrum’s radial velocity with precision below 0.3 m s−1, corresponding to a Doppler shift much smaller than the size of a spectral pixel. Furthermore, the new method outperforms the traditional cross-correlation function approach for estimating the radial velocity by reducing the root mean squared error up to 15 cm s−1. The proposed method is also demonstrated on a new set of observations from the EXtreme PREcision Spectrometer (EXPRES) for the host star 51 Pegasi, and successfully recovers estimates of the planetary companion’s parameters that agree well with previous studies. The method is implemented in the R package rvmethod, and supplemental Python code is also available.},
  archive      = {J_AOAS},
  author       = {Parker H. Holzer and Jessi Cisewski-Kehe and Debra Fischer and Lily Zhao},
  doi          = {10.1214/20-AOAS1406},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {527-555},
  shortjournal = {Ann. Appl. Stat.},
  title        = {A Hermite–Gaussian based exoplanet radial velocity estimation method},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An empirical bayes change-point model for transcriptome
time-course data. <em>AOAS</em>, <em>15</em>(1), 509–526. (<a
href="https://doi.org/10.1214/20-AOAS1403">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time-course experiments are commonly conducted to capture temporal changes. It is generally of interest to detect if any changes happen over time, which we define as a detection problem. If there is a change, it is informative to know when the change is, which we define as an identification problem. It is often desired to control Type I error rate at a nominal level while applying a testing procedure to detect or identify these changes. Quite a few analytic methods have been proposed. Most existing methods aim to solve either the detection problem or, more recently, the identification problem. Here, we propose to solve these two problems using a unified multiple-testing framework built upon an empirical Bayes change-point model. Our model provides a flexible framework that can account for sophisticated temporal gene expression patterns. We show that our testing procedure is valid and asymptotically optimal in the sense of rejecting the maximum number of null hypotheses, while the Bayesian false discovery rate (FDR) can be controlled at a predefined nominal level. Simulation studies and application to real transcriptome time-course data illustrate that our proposed model is a flexible and powerful method to capture various temporal patterns in analysis of time-course data.},
  archive      = {J_AOAS},
  author       = {Tian Tian and Ruihua Cheng and Zhi Wei},
  doi          = {10.1214/20-AOAS1403},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {509-526},
  shortjournal = {Ann. Appl. Stat.},
  title        = {An empirical bayes change-point model for transcriptome time-course data},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Model-based feature selection and clustering of RNA-seq data
for unsupervised subtype discovery. <em>AOAS</em>, <em>15</em>(1),
481–508. (<a href="https://doi.org/10.1214/20-AOAS1407">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering is a form of unsupervised learning that aims to uncover latent groups within data based on similarity across a set of features. A common application of this in biomedical research is in delineating novel cancer subtypes from patient gene expression data, given a set of informative genes. However, it is typically unknown a priori what genes may be informative in discriminating between clusters and what the optimal number of clusters are. Few methods exist for performing unsupervised clustering of RNA-seq samples, and none currently adjust for between-sample global normalization factors, select cluster-discriminatory genes or account for potential confounding variables during clustering. To address these issues, we propose the feature selection and clustering of RNA-seq (FSCseq): a model-based clustering algorithm that utilizes a finite mixture of regression (FMR) model and the quadratic penalty method with a smoothly clipped absolute deviation (SCAD) penalty. The maximization is done by a penalized Classification EM algorithm, allowing us to include normalization factors and confounders in our modeling framework. Given the fitted model, our framework allows for subtype prediction in new patients via posterior probabilities of cluster membership, even in the presence of batch effects. Based on simulations and real data analysis, we show the advantages of our method relative to competing approaches.},
  archive      = {J_AOAS},
  author       = {David K. Lim and Naim U. Rashid and Joseph G. Ibrahim},
  doi          = {10.1214/20-AOAS1407},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {481-508},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Model-based feature selection and clustering of RNA-seq data for unsupervised subtype discovery},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). High-fidelity hurricane surge forecasting using emulation
and sequential experiments. <em>AOAS</em>, <em>15</em>(1), 460–480. (<a
href="https://doi.org/10.1214/20-AOAS1398">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Probabilistic hurricane storm surge forecasting using a high-fidelity model has been considered impractical due to the overwhelming computational expense to run thousands of simulations. This article demonstrates that modern statistical tools enable good forecasting performance using a small number of carefully chosen simulations. This article offers algorithms that quickly handle the massive output of a surge model while addressing the missing data at unsubmerged locations. Also included is a new optimal design criterion for selecting simulations that accounts for the log transform required to statistically model surge data. Hurricane Michael (2018) is used as a testbed for this investigation and provides evidence for the approach’s efficacy in comparison to the existing probabilistic surge forecast method.},
  archive      = {J_AOAS},
  author       = {Matthew Plumlee and Taylor G. Asher and Won Chang and Matthew V. Bilskie},
  doi          = {10.1214/20-AOAS1398},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {460-480},
  shortjournal = {Ann. Appl. Stat.},
  title        = {High-fidelity hurricane surge forecasting using emulation and sequential experiments},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Accounting for smoking in forecasting mortality and life
expectancy. <em>AOAS</em>, <em>15</em>(1), 437–459. (<a
href="https://doi.org/10.1214/20-AOAS1381">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Smoking is one of the main risk factors that has affected human mortality and life expectancy over the past century. Smoking accounts for a large part of the nonlinearities in the growth of life expectancy and of the geographic and gender differences in mortality. As Bongaarts (Popul. Dev. Rev. 32 (2006) 605–628) and Janssen (Genus 74 (2018) 21) suggested, accounting for smoking could improve the quality of mortality forecasts due to the predictable nature of the smoking epidemic. We propose a new Bayesian hierarchical model to forecast life expectancy at birth for both genders and for 69 countries/regions with good data on smoking-related mortality. The main idea is to convert the forecast of the nonsmoking life expectancy at birth (i.e., life expectancy at birth removing the smoking effect) into life expectancy forecast through the use of the age-specific smoking attributable fraction (ASSAF). We introduce a new age-cohort model for the ASSAF and a Bayesian hierarchical model for nonsmoking life expectancy at birth. The forecast performance of the proposed method is evaluated by out-of-sample validation compared with four other commonly used methods for life expectancy forecasting. Improvements in forecast accuracy and model calibration based on the new method are observed.},
  archive      = {J_AOAS},
  author       = {Yicheng Li and Adrian E. Raftery},
  doi          = {10.1214/20-AOAS1381},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {437-459},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Accounting for smoking in forecasting mortality and life expectancy},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A multiple imputation procedure for record linkage and
causal inference to estimate the effects of home-delivered meals.
<em>AOAS</em>, <em>15</em>(1), 412–436. (<a
href="https://doi.org/10.1214/20-AOAS1397">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Causal analysis of observational studies requires data that comprise a set of covariates, a treatment assignment indicator and the observed outcomes. However, data confidentiality restrictions or the nature of data collection may distribute these variables across two or more datasets. In the absence of unique identifiers to link records across files, probabilistic record linkage algorithms can be leveraged to merge the datasets. Current applications of record linkage are concerned with estimation of associations between variables that are exclusive to one file and not causal relationships. We propose a Bayesian framework for record linkage and causal inference where one file comprises all the covariate and observed outcome information, and the second file consists of a list of all individuals who receive the active treatment. Under certain ignorability assumptions, the procedure properly propagates the error in the record linkage process, resulting in valid statistical inferences. To estimate the causal effects, we devise a two-stage procedure. The first stage of the procedure performs Bayesian record linkage to multiply-impute the treatment assignment for all individuals in the first file, while adjustments for covariates’ imbalance and imputation of missing potential outcomes are performed in the second stage. This procedure is used to evaluate the effect of Meals on Wheels services on mortality and healthcare utilization among homebound older adults in Rhode Island. In addition, an interpretable sensitivity analysis is developed to assess potential violations of the ignorability assumptions.},
  archive      = {J_AOAS},
  author       = {Mingyang Shan and Kali S. Thomas and Roee Gutman},
  doi          = {10.1214/20-AOAS1397},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {412-436},
  shortjournal = {Ann. Appl. Stat.},
  title        = {A multiple imputation procedure for record linkage and causal inference to estimate the effects of home-delivered meals},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). BayICE: A bayesian hierarchical model for
semireference-based deconvolution of bulk transcriptomic data.
<em>AOAS</em>, <em>15</em>(1), 391–411. (<a
href="https://doi.org/10.1214/20-AOAS1376">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gene expression deconvolution is a powerful tool for exploring the microenvironment of complex tissues comprised of multiple cell groups using transcriptomic data. Characterizing cell activities for a particular condition has been regarded as a primary mission against diseases. For example, cancer immunology aims to clarify the role of the immune system in the progression and development of cancer through analyzing the immune cell components of tumors. To that end, many deconvolution methods have been proposed for inferring cell subpopulations within tissues. Nevertheless, two problems limit the practicality of current approaches. First, most approaches use external purified data to preselect cell type-specific genes that contribute to deconvolution. However, some types of cells cannot be found in purified profiles, and the genes specifically over- or under-expressed in them cannot be identified. This is particularly a problem in cancer studies. Hence, a preselection strategy that is independent from deconvolution is inappropriate. The second problem is that existing approaches do not recover the expression profiles of unknown cells present in bulk tissues when the reference set of purified cell-specific profiles is incomplete which results in biased estimation of unknown cell proportions. Furthermore, it causes the shift-invariant property of deconvolution to fail which then affects the estimation performance. To address these two problems, we propose a novel semireference-based deconvolution approach, BayICE which employs hierarchical Bayesian modeling with stochastic search variable selection. We develop a comprehensive Markov chain Monte Carlo procedure through Gibbs sampling to estimate proportions, expression profiles and signature genes for a set of known reference cell types as well as an unknown cell type. Simulation and validation studies illustrate that BayICE outperforms existing semireference-based deconvolution approaches in estimating cell proportions. We further show that BayICE is applicable to single-cell RNA-seq data. Subsequently, we demonstrate an application of BayICE in the RNA sequencing of patients with nonsmall cell lung cancer. The model is implemented in the R package “BayICE,” and the algorithm is available for download.},
  archive      = {J_AOAS},
  author       = {An-Shun Tai and George C. Tseng and Wen-Ping Hsieh},
  doi          = {10.1214/20-AOAS1376},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {391-411},
  shortjournal = {Ann. Appl. Stat.},
  title        = {BayICE: A bayesian hierarchical model for semireference-based deconvolution of bulk transcriptomic data},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). BETS: The dangers of selection bias in early analyses of the
coronavirus disease (COVID-19) pandemic. <em>AOAS</em>, <em>15</em>(1),
363–390. (<a href="https://doi.org/10.1214/20-AOAS1401">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The coronavirus disease 2019 (COVID-19) has quickly grown from a regional outbreak in Wuhan, China, to a global pandemic. Early estimates of the epidemic growth and incubation period of COVID-19 may have been biased due to sample selection. Using detailed case reports from 14 locations in and outside mainland China, we obtained 378 Wuhan-exported cases who left Wuhan before an abrupt travel quarantine. We developed a generative model we call BETS for four key epidemiological events—Beginning of exposure, End of exposure, time of Transmission, and time of Symptom onset (BETS)—and derived explicit formulas to correct for the sample selection. We gave a detailed illustration of why some early and highly influential analyses of the COVID-19 pandemic were severely biased. All our analyses, regardless of which subsample and model were being used, point to an epidemic doubling time of two to 2.5 days during the early outbreak in Wuhan. A Bayesian nonparametric analysis further suggests that about 5\% of the symptomatic cases may not develop symptoms within 14 days of infection and that men may be much more likely than women to develop symptoms within two days of infection.},
  archive      = {J_AOAS},
  author       = {Qingyuan Zhao and Nianqiao Ju and Sergio Bacallado and Rajen D. Shah},
  doi          = {10.1214/20-AOAS1401},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {363-390},
  shortjournal = {Ann. Appl. Stat.},
  title        = {BETS: The dangers of selection bias in early analyses of the coronavirus disease (COVID-19) pandemic},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Estimation of cell lineage trees by maximum-likelihood
phylogenetics. <em>AOAS</em>, <em>15</em>(1), 343–362. (<a
href="https://doi.org/10.1214/20-AOAS1400">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {CRISPR technology has enabled cell lineage tracing for complex multicellular organisms through insertion-deletion mutations of synthetic genomic barcodes during organismal development. To reconstruct the cell lineage tree from the mutated barcodes, current approaches apply general-purpose computational tools that are agnostic to the mutation process and are unable to take full advantage of the data’s structure. We propose a statistical model for the CRISPR mutation process and develop a procedure to estimate the resulting tree topology, branch lengths and mutation parameters by iteratively applying penalized maximum likelihood estimation. By assuming the barcode evolves according to a molecular clock, our method infers relative ordering across parallel lineages, whereas existing techniques only infer ordering for nodes along the same lineage. When analyzing transgenic zebrafish data from (Science 353 (2016) aaf7907), we find that our method recapitulates known aspects of zebrafish development and the results are consistent across samples.},
  archive      = {J_AOAS},
  author       = {Jean Feng and William S. DeWitt III and Aaron McKenna and Noah Simon and Amy D. Willis and Frederick A. Matsen IV},
  doi          = {10.1214/20-AOAS1400},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {343-362},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Estimation of cell lineage trees by maximum-likelihood phylogenetics},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Spatial distributed lag data fusion for estimating ambient
air pollution. <em>AOAS</em>, <em>15</em>(1), 323–342. (<a
href="https://doi.org/10.1214/20-AOAS1399">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce spatial (DLfuse) and spatiotemporal (DLfuseST) distributed lag data fusion methods for predicting point-level ambient air pollution concentrations, using, as input, gridded average pollution estimates from a deterministic numerical air quality model. The methods incorporate predictive information from grid cells surrounding the prediction location of interest and are shown to collapse to existing downscaling approaches when this information adds no benefit. The spatial lagged parameters are allowed to vary spatially/spatiotemporally to accommodate the setting where surrounding geographic information is useful in one area/time but not in another. We apply the new methods to predict ambient concentrations of eight-hour maximum ozone and 24-hour average PM2.5 at unobserved spatial locations and times, and compare the predictions with those from several state-of-the-art data fusion approaches. Results show that DLfuse and DLfuseST often provide improved model fit and predictive accuracy when the lagged information is shown to be beneficial. Code to apply the methods is available in the R package DLfuse.},
  archive      = {J_AOAS},
  author       = {Joshua L. Warren and Marie Lynn Miranda and Joshua L. Tootoo and Claire E. Osgood and Michelle L. Bell},
  doi          = {10.1214/20-AOAS1399},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {323-342},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Spatial distributed lag data fusion for estimating ambient air pollution},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A regression discontinuity design for ordinal running
variables: Evaluating central bank purchases of corporate bonds.
<em>AOAS</em>, <em>15</em>(1), 304–322. (<a
href="https://doi.org/10.1214/20-AOAS1396">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Regression discontinuity (RD) is a widely used quasi-experimental design for causal inference. In the standard RD the assignment to treatment is determined by a continuous pretreatment variable (i.e., running variable) falling above or below a prefixed threshold. Recent applications increasingly feature ordered categorical or ordinal running variables which pose challenges to RD estimation due to the lack of a meaningful measure of distance. This paper proposes an RD approach for ordinal running variables under the local randomization framework. The proposal first estimates an ordered probit model for the ordinal running variable. The estimated probability of being assigned to treatment is then adopted as a latent continuous running variable and used to identify a covariate-balanced subsample around the threshold. Assuming local unconfoundedness of the treatment in the subsample, an estimate of the effect of the program is obtained by employing a weighted estimator of the average treatment effect. Two weighting estimators—overlap weights and ATT weights—as well as their augmented versions are considered. We apply the method to evaluate the causal effects of the corporate sector purchase programme (CSPP) of the European Central Bank which involves large-scale purchases of securities issued by corporations in the euro area. We find a statistically significant and negative effect of the CSPP on corporate bond spreads at issuance.},
  archive      = {J_AOAS},
  author       = {Fan Li and Andrea Mercatanti and Taneli Mäkinen and Andrea Silvestrini},
  doi          = {10.1214/20-AOAS1396},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {304-322},
  shortjournal = {Ann. Appl. Stat.},
  title        = {A regression discontinuity design for ordinal running variables: Evaluating central bank purchases of corporate bonds},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Two-phase sampling strategies for design-based mapping of
continuous spatial populations in environmental surveys. <em>AOAS</em>,
<em>15</em>(1), 287–303. (<a
href="https://doi.org/10.1214/20-AOAS1392">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The estimation of a surface throughout a continuum of points in a study area is addressed by means of two-phase sampling strategies. To this aim, a family of two-phase inverse distance weighting interpolators is introduced, and their design-based asymptotic properties are derived when the surface remains fixed and the number of sample points approaches infinity. In particular, conditions ensuring asymptotic unbiasedness and consistency are derived and are proven to hold for some of the most widely applied environmental sampling schemes. Furthermore, a computationally simple mean squared error estimator is proposed. Finally, a simulation study is performed to assess the theoretical results. The proposed strategy is adopted to provide the map of basal area in a forested region of Casentino Valley (Central Italy).},
  archive      = {J_AOAS},
  author       = {Lorenzo Fattorini and Sara Franceschi and Marzia Marcheselli and Caterina Pisani and Luca Pratelli},
  doi          = {10.1214/20-AOAS1392},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {287-303},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Two-phase sampling strategies for design-based mapping of continuous spatial populations in environmental surveys},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The role of scale in the estimation of cell-type
proportions. <em>AOAS</em>, <em>15</em>(1), 270–286. (<a
href="https://doi.org/10.1214/20-AOAS1395">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Complex tissues are composed of a large number of different types of cells, each involved in a multitude of biological processes. Consequently, an important component to understanding such processes is understanding the cell-type composition of the tissues. Estimating cell-type composition using high-throughput gene expression data is known as cell-type deconvolution. In this paper we first summarize the extensive deconvolution literature by identifying a common regression-like approach to deconvolution. We call this approach the unified deconvolution-as-regression (UDAR) framework. While methods that fall under this framework all use a similar model, they fit using data on different scales. Two popular scales for gene expression data are logarithmic and linear. Unfortunately, each of these scales has problems in the UDAR framework. Using log-scale gene expressions proposes a biologically implausible model and using linear-scale gene expressions will lead to statistically inefficient estimators. To explore ways to address these issues, in this paper we consider how deconvolution methods may use an adjusted model that is a hybrid of the two scales. In analysis on simulations as well as a collection of eleven real benchmark datasets, we find a prototypical hybrid-scale adjustment to the UDAR framework improves statistical efficiency and robustness. More broadly, we believe these hybrid-scale modeling principles may be incorporated into many existing deconvolution methods.},
  archive      = {J_AOAS},
  author       = {Gregory J. Hunt and Johann A. Gagnon-Bartsch},
  doi          = {10.1214/20-AOAS1395},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {270-286},
  shortjournal = {Ann. Appl. Stat.},
  title        = {The role of scale in the estimation of cell-type proportions},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Monitoring vaccine safety by studying temporal variation of
adverse events using vaccine adverse event reporting system.
<em>AOAS</em>, <em>15</em>(1), 252–269. (<a
href="https://doi.org/10.1214/20-AOAS1393">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Vaccine Adverse Event Reporting System (VAERS) plays a vital role in vaccine safety surveillance. One of the main missions of VAERS is to monitor increases in reporting rate of adverse events, as such signals can indicate safety issues caused by update of vaccines or change in vaccine practices. Existing methods can rarely be used to monitor the temporal variation of reporting adverse events. In this paper we propose a composite likelihood based variance component model to study the temporal variation of reporting adverse events using VAERS data. The proposed method is devised to identify safety signals by testing the heterogeneity of reporting rates of adverse events across years. The proposed method accounts for the well-known underreporting of adverse events and the zero-inflation observations in passive surveillance reporting systems. We applied the proposed method to VAERS reports of trivalent influenza virus vaccine and identified 14 adverse events with significantly heterogeneous reporting rates over years and two of them have increasing trend of reporting rates from 1990 to 2013. Our findings provide early warning signals that can be more rigorously investigated in future studies of the vaccine.},
  archive      = {J_AOAS},
  author       = {Jing Huang and Yi Cai and Jingcheng Du and Ruosha Li and Susan S. Ellenberg and Sean Hennessy and Cui Tao and Yong Chen},
  doi          = {10.1214/20-AOAS1393},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {252-269},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Monitoring vaccine safety by studying temporal variation of adverse events using vaccine adverse event reporting system},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Large-scale inference of correlation among mixed-type
biological traits with phylogenetic multivariate probit models.
<em>AOAS</em>, <em>15</em>(1), 230–251. (<a
href="https://doi.org/10.1214/20-AOAS1394">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inferring concerted changes among biological traits along an evolutionary history remains an important yet challenging problem. Besides adjusting for spurious correlation induced from the shared history, the task also requires sufficient flexibility and computational efficiency to incorporate multiple continuous and discrete traits as data size increases. To accomplish this, we jointly model mixed-type traits by assuming latent parameters for binary outcome dimensions at the tips of an unknown tree informed by molecular sequences. This gives rise to a phylogenetic multivariate probit model. With large sample sizes, posterior computation under this model is problematic, as it requires repeated sampling from a high-dimensional truncated normal distribution. Current best practices employ multiple-try rejection sampling that suffers from slow-mixing and a computational cost that scales quadratically in sample size. We develop a new inference approach that exploits: (1) the bouncy particle sampler (BPS) based on piecewise deterministic Markov processes to simultaneously sample all truncated normal dimensions, and (2) novel dynamic programming that reduces the cost of likelihood and gradient evaluations for BPS to linear in sample size. In an application with 535 HIV viruses and 24 traits that necessitates sampling from a 12,840-dimensional truncated normal, our method makes it possible to estimate the across-trait correlation and detect factors that affect the pathogen’s capacity to cause disease. This inference framework is also applicable to a broader class of covariance structures beyond comparative biology.},
  archive      = {J_AOAS},
  author       = {Zhenyu Zhang and Akihiko Nishimura and Paul Bastide and Xiang Ji and Rebecca P. Payne and Philip Goulder and Philippe Lemey and Marc A. Suchard},
  doi          = {10.1214/20-AOAS1394},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {230-251},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Large-scale inference of correlation among mixed-type biological traits with phylogenetic multivariate probit models},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An evaluation of statistical methods for aggregate patterns
of replication failure. <em>AOAS</em>, <em>15</em>(1), 208–229. (<a
href="https://doi.org/10.1214/20-AOAS1387">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Several programs of research have sought to assess the replicability of scientific findings in different fields, including economics and psychology. These programs attempt to replicate several findings and use the results to say something about large-scale patterns of replicability in a field. However, little work has been done to understand the analytic methods used to do this, including what they are assessing and what their statistical properties are. This article examines several methods that have been used to study patterns of replicability in the social sciences. We describe in concrete terms how each method operationalizes the idea of “replication” and examine various statistical properties, including bias, precision and statistical power. We find that some analytic methods rely on an operational definition of replication that can be misleading. Other methods involve more sound definitions of replication, but most of these have limitations, such as large bias and uncertainty or low power. The findings suggest that we should use caution interpreting the results of such analyses and that work on more accurate methods may be useful to future replication research efforts.},
  archive      = {J_AOAS},
  author       = {Jacob M. Schauer and Kaitlyn G. Fitzgerald and Sarah Peko-Spicer and Mena C. R. Whalen and Rrita Zejnullahi and Larry V. Hedges},
  doi          = {10.1214/20-AOAS1387},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {208-229},
  shortjournal = {Ann. Appl. Stat.},
  title        = {An evaluation of statistical methods for aggregate patterns of replication failure},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Model free estimation of graphical model using gene
expression data. <em>AOAS</em>, <em>15</em>(1), 194–207. (<a
href="https://doi.org/10.1214/20-AOAS1380">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graphical model is a powerful and popular approach to study high-dimensional omic data, such as genome-wide gene expression data. Nonlinear relations between genes are widely documented. However, partly due to sparsity of data points in high-dimensional space (i.e., curse of dimensionality) and computational challenges, most available methods construct graphical models by testing linear relations. We propose to address this challenge by a two-step approach: first, use a model-free approach to prioritize the neighborhood of each gene; then, apply a nonparametric conditional independence testing method to refine such neighborhood estimation. Our method, named as “mofreds” (MOdel FRee Estimation of DAG Skeletons), seeks to estimate the skeleton of a directed acyclic graph (DAG) by this two-step approach. We studied the theoretical properties of mofreds and evaluated its performance in extensive simulation settings. We found mofreds has substantially better performance than the state-of-the art method which is designed to detect linear relations of Gaussian graphical models. We applied mofreds to analyze gene expression data of breast cancer patients from The Cancer Genome Atlas (TCGA). We found that it discovers nonlinear relationships among gene pairs that are missed by the Gaussian graphical model methods.},
  archive      = {J_AOAS},
  author       = {Jenny Yang and Yang Liu and Yufeng Liu and Wei Sun},
  doi          = {10.1214/20-AOAS1380},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {194-207},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Model free estimation of graphical model using gene expression data},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Random-effects meta-analysis of phase i dose-finding studies
using stochastic process priors. <em>AOAS</em>, <em>15</em>(1), 174–193.
(<a href="https://doi.org/10.1214/20-AOAS1390">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Phase I dose-finding studies aim at identifying the maximum tolerated dose (MTD). Often, several dose-finding studies are conducted with some variation in the administration mode or dose panel. For instance, sorafenib (BAY 43-900) was used as monotherapy in 36 phase I trials, according to a recent clinicaltrials.gov search. Since the toxicity may not be directly related to the specific indication, synthesizing the information from several studies might be worthwhile. However, this is rarely done in practice and only a fixed-effect meta-analysis framework was proposed to date. We developed a Bayesian random-effects meta-analysis methodology to pool several phase I trials and suggest the MTD. A curve free hierarchical model on the logistic scale with random effects, accounting for between-trial heterogeneity, is used to model the probability of toxicity across the investigated doses. An Ornstein–Uhlenbeck Gaussian process is adopted for the random effects structure. Prior distributions for the curve-free model are based on a latent Gamma process. An extensive simulation study showed good performance of the proposed method also under model deviations. Sharing information between phase I studies can improve the precision of MTD selection, at least when the number of trials is reasonably large.},
  archive      = {J_AOAS},
  author       = {Moreno Ursino and Christian Röver and Sarah Zohar and Tim Friede},
  doi          = {10.1214/20-AOAS1390},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {174-193},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Random-effects meta-analysis of phase i dose-finding studies using stochastic process priors},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Spike-and-slab lasso biclustering. <em>AOAS</em>,
<em>15</em>(1), 148–173. (<a
href="https://doi.org/10.1214/20-AOAS1385">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Biclustering methods simultaneously group samples and their associated features. In this way, biclustering methods differ from traditional clustering methods, which utilize the entire set of features to distinguish groups of samples. Motivating applications for biclustering include genomics data, where the goal is to cluster patients or samples by their gene expression profiles; and recommender systems, which seek to group customers based on their product preferences. Biclusters of interest often manifest as rank-1 submatrices of the data matrix. This submatrix detection problem can be viewed as a factor analysis problem in which both the factors and loadings are sparse. In this paper, we propose a new biclustering method called Spike-and-Slab Lasso Biclustering (SSLB) which utilizes the Spike-and-Slab Lasso of Ročková and George (J. Amer. Statist. Assoc. 113 (2018) 431–444) to find such a sparse factorization of the data matrix. SSLB also incorporates an Indian Buffet Process prior to automatically choose the number of biclusters. Many biclustering methods make assumptions about the size of the latent biclusters; either assuming that the biclusters are all of the same size, or that the biclusters are very large or very small. In contrast, SSLB can adapt to find biclusters which have a continuum of sizes. SSLB is implemented via a fast EM algorithm with a variational step. In a variety of simulation settings, SSLB outperforms other biclustering methods. We apply SSLB to both a microarray dataset and a single-cell RNA-sequencing dataset and highlight that SSLB can recover biologically meaningful structures in the data. The SSLB software is available as an R/C++ package at https://github.com/gemoran/SSLB.},
  archive      = {J_AOAS},
  author       = {Gemma E. Moran and Veronika Ročková and Edward I. George},
  doi          = {10.1214/20-AOAS1385},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {148-173},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Spike-and-slab lasso biclustering},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust inference when combining inverse-probability
weighting and multiple imputation to address missing data with
application to an electronic health records-based study of bariatric
surgery. <em>AOAS</em>, <em>15</em>(1), 126–147. (<a
href="https://doi.org/10.1214/20-AOAS1386">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While electronic health records present a rich and promising data source for observational research, they are highly susceptible to missing data. For settings like these, Seaman et al. (Biometrics 68 (2012) 129–137) proposed a strategy wherein one handles missingness in some variables using inverse-probability weighting and others using multiple imputation. Seaman et al. (Biometrics 68 (2012) 129–137) show that Rubin’s variance estimator for averaging results across datasets is asymptotically valid when the analysis and imputation models are correctly specified and the weights are either known or correctly specified. Modeled after the approach of Robins and Wang (Biometrika 87 (2000) 113–124), we propose a method for asymptotically valid inference that is robust to violation of these conditions. Following a simulation study in which we demonstrate that a proposed variance estimator can reduce bias due to model misspecification, we illustrate this approach in an electronic health records-based study investigating whether differences in long-term weight loss between bariatric surgery techniques are associated with chronic kidney disease at baseline. We observe that the weight loss advantage after five years of Roux-en-Y gastric bypass surgery, compared to vertical sleeve gastrectomy, is less pronounced among patients with chronic kidney disease at baseline compared to those without.},
  archive      = {J_AOAS},
  author       = {Tanayott Thaweethai and David E. Arterburn and Karen J. Coleman and Sebastien Haneuse},
  doi          = {10.1214/20-AOAS1386},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {126-147},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Robust inference when combining inverse-probability weighting and multiple imputation to address missing data with application to an electronic health records-based study of bariatric surgery},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An approximate best prediction approach to small area
estimation for sheet and rill erosion under informative sampling.
<em>AOAS</em>, <em>15</em>(1), 102–125. (<a
href="https://doi.org/10.1214/20-AOAS1388">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The National Resources Inventory, a longitudinal survey of characteristics related to natural resources and agriculture on nonfederal U.S. land, has increasingly received requests for substate estimates in recent years. We consider estimation of erosion in subdomains of the Boone-Raccoon River Watershed. This region is of interest for its proximity to intensively cropped areas as well as important waterbodies. The NRI application requires a small area prediction approach that can handle nonlinear relationships and appropriately incorporate survey weights that may have nontrivial relationships to the response variable. Because of the informative design, the conditional distribution required to define a standard empirical Bayes predictor is unknown. We develop a prediction approach that utilizes the approximate distribution of survey weighted score equations arising from a specified two-level superpopulation model. We apply the method to construct estimates of mean erosion in small watersheds. We investigate the robustness of the procedure to an assumption of a constant dispersion parameter and validate the properties of the procedure through simulation.},
  archive      = {J_AOAS},
  author       = {Emily Berg and Jae-Kwang Kim},
  doi          = {10.1214/20-AOAS1388},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {102-125},
  shortjournal = {Ann. Appl. Stat.},
  title        = {An approximate best prediction approach to small area estimation for sheet and rill erosion under informative sampling},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Point process modeling of drug overdoses with heterogeneous
and missing data. <em>AOAS</em>, <em>15</em>(1), 88–101. (<a
href="https://doi.org/10.1214/20-AOAS1384">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Opioid overdose rates have increased in the United States over the past decade and reflect a major public health crisis. Modeling and prediction of drug and opioid hotspots, where a high percentage of events fall in a small percentage of space–time, could help better focus limited social and health services. In this work we present a spatial-temporal point process model for drug overdose clustering. The data input into the model comes from two heterogeneous sources: (1) high volume emergency medical calls for service (EMS) records containing location and time but no information on the type of nonfatal overdose, and (2) fatal overdose toxicology reports from the coroner containing location and high-dimensional information from the toxicology screen on the drugs present at the time of death. We first use nonnegative matrix factorization to cluster toxicology reports into drug overdose categories, and we then develop an EM algorithm for integrating the two heterogeneous data sets, where the mark corresponding to overdose category is inferred for the EMS data and the high volume EMS data is used to more accurately predict drug overdose death hotspots. We apply the algorithm to drug overdose data from Indianapolis, showing that the point process defined on the integrated data out-performs point processes that use only coroner data (AUC improvement 0.81 to 0.85). We also investigate the extent to which overdoses are contagious, as a function of the type of overdose, while controlling for exogenous fluctuations in the background rate that might also contribute to clustering. We find that drug and opioid overdose deaths exhibit significant excitation with branching ratio ranging from 0.72 to 0.98.},
  archive      = {J_AOAS},
  author       = {Xueying Liu and Jeremy Carter and Brad Ray and George Mohler},
  doi          = {10.1214/20-AOAS1384},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {88-101},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Point process modeling of drug overdoses with heterogeneous and missing data},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Integrative network learning for multimodality biomarker
data. <em>AOAS</em>, <em>15</em>(1), 64–87. (<a
href="https://doi.org/10.1214/20-AOAS1382">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The biomarker networks measured by different modalities of data (e.g., structural magnetic resonance imaging (sMRI), diffusion tensor imaging (DTI)) may share the same true underlying biological model. In this work we propose a nodewise biomarker graphical model to leverage the shared mechanism between multimodality data to provide a more reliable estimation of the target modality network and account for the heterogeneity in networks due to differences between subjects and networks of external modality. Latent variables are introduced to represent the shared unobserved biological network, and the information from the external modality is incorporated to model the distribution of the underlying biological network. We propose an efficient approximation to the posterior expectation of the latent variables that reduces computational cost by at least 50\%. The performance of the proposed method is demonstrated by extensive simulation studies and an application to construct gray matter brain atrophy network of Huntington’s disease by using sMRI data and DTI data. The identified network connections are more consistent with clinical literature and better improve prediction in follow-up clinical outcomes and separate subjects into clinically meaningful subgroups with different prognosis than alternative methods.},
  archive      = {J_AOAS},
  author       = {Shanghong Xie and Donglin Zeng and Yuanjia Wang},
  doi          = {10.1214/20-AOAS1382},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {64-87},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Integrative network learning for multimodality biomarker data},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Analyzing second order stochasticity of neural spiking under
stimuli-bundle exposure. <em>AOAS</em>, <em>15</em>(1), 41–63. (<a
href="https://doi.org/10.1214/20-AOAS1383">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conventional analysis of neuroscience data involves computing average neural activity over a group of trials and/or a period of time. This approach may be particularly problematic when assessing the response patterns of neurons to more than one simultaneously presented stimulus. In such cases the brain must represent each individual component of the stimuli bundle, but trial-and-time-pooled averaging methods are fundamentally unequipped to address the means by which multiitem representation occurs. We introduce and investigate a novel statistical analysis framework that relates the firing pattern of a single cell, exposed to a stimuli bundle, to the ensemble of its firing patterns under each constituent stimulus. Existing statistical tools focus on what may be called “first order stochasticity” in trial-to-trial variation in the form of unstructured noise around a fixed firing rate curve associated with a given stimulus. Our analysis is based upon the theoretical premise that exposure to a stimuli bundle induces additional stochasticity in the cell’s response pattern in the form of a stochastically varying recombination of its single stimulus firing rate curves. We discuss challenges to statistical estimation of such “second order stochasticity” and address them with a novel dynamic admixture point process (DAPP) model. DAPP is a hierarchical point process model that decomposes second order stochasticity into a Gaussian stochastic process and a random vector of interpretable features and facilitates borrowing of information on the latter across repeated trials through latent clustering. We illustrate the utility and accuracy of the DAPP analysis with synthetic data simulation studies. We present real-world evidence of second order stochastic variation with an analysis of monkey inferior colliculus recordings under auditory stimuli.},
  archive      = {J_AOAS},
  author       = {Chris Glynn and Surya T. Tokdar and Azeem Zaman and Valeria C. Caruso and Jeff T. Mohl and Shawn M. Willett and Jennifer M. Groh},
  doi          = {10.1214/20-AOAS1383},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {41-63},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Analyzing second order stochasticity of neural spiking under stimuli-bundle exposure},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Late 19th century navigational uncertainties and their
influence on sea surface temperature estimates. <em>AOAS</em>,
<em>15</em>(1), 22–40. (<a
href="https://doi.org/10.1214/20-AOAS1367">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate estimates of historical changes in sea surface temperatures (SSTs) and their uncertainties are important for documenting and understanding historical changes in climate. A source of uncertainty that has not previously been quantified in historical SST estimates stems from position errors. A Bayesian inference framework is proposed for quantifying errors in reported positions and their implications on SST estimates. The analysis framework is applied to data from the International Comprehensive Ocean-Atmosphere Data Set (ICOADS3.0) in 1885, a time when astronomical and chronometer estimation of position was common but predated the use of radio signals. Focus is upon a subset of 943 ship tracks from ICOADS3.0 that report their position every two hours to a precision of 0.01∘ longitude and latitude. These data are interpreted as positions determined by dead reckoning that are periodically updated by celestial correction techniques. The posterior medians of uncertainties in celestial correction are 33.1 km (0.30∘ on the equator) in longitude and 24.4 km (0.22∘) in latitude, respectively. Celestial navigation uncertainties being smaller in latitude than longitude is qualitatively consistent with the relative difficulty of obtaining astronomical estimates. The posterior medians for two-hourly dead reckoning uncertainties are 19.2\% for ship speed and 13.2∘ for ship heading, leading to random position uncertainties with median 0.18∘ (20 km on the equator) in longitude and 0.15∘ (17 km) in latitude. Reported ship tracks also contain systematic position uncertainties relating to precursor dead-reckoning positions not being updated after obtaining celestial position estimates, indicating that more accurate positions can be provided for SST observations. Finally, we translate position errors into SST uncertainties by sampling an ensemble of SSTs from the Multiscale Ultrahigh Resolution Sea Surface Temperature (MURSST) data set. Evolving technology for determining ship position, heterogeneous reporting and archiving of position information, and seasonal and spatial changes in navigational uncertainty and SST gradients together imply that accounting for positional error in SST estimates over the span of the instrumental record will require substantial additional effort.},
  archive      = {J_AOAS},
  author       = {Chenguang Dai and Duo Chan and Peter Huybers and Natesh Pillai},
  doi          = {10.1214/20-AOAS1367},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {22-40},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Late 19th century navigational uncertainties and their influence on sea surface temperature estimates},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Discovering interactions using covariate informed random
partition models. <em>AOAS</em>, <em>15</em>(1), 1–21. (<a
href="https://doi.org/10.1214/20-AOAS1372">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Combination chemotherapy treatment regimens created for patients diagnosed with childhood acute lymphoblastic leukemia have had great success in improving cure rates. Unfortunately, patients prescribed these types of treatment regimens have displayed susceptibility to the onset of osteonecrosis. Some have suggested that this is due to pharmacokinetic interaction between two agents in the treatment regimen (asparaginase and dexamethasone) and other physiological variables. Determining which physiological variables to consider when searching for interactions in scenarios like these, minus a priori guidance, has proved to be a challenging problem, particularly if interactions influence the response distribution in ways beyond shifts in expectation or dispersion only. In this paper we propose an exploratory technique that is able to discover associations between covariates and responses in a general way. The procedure connects covariates to responses flexibly through dependent random partition distributions and then employs machine learning techniques to highlight potential associations found in each cluster. We provide a simulation study to show utility and apply the method to data produced from a study dedicated to learning which physiological predictors influence severity of osteonecrosis multiplicatively.},
  archive      = {J_AOAS},
  author       = {Garritt L. Page and Fernando A. Quintana and Gary L. Rosner},
  doi          = {10.1214/20-AOAS1372},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {1-21},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Discovering interactions using covariate informed random partition models},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
</ul>

</body>
</html>
