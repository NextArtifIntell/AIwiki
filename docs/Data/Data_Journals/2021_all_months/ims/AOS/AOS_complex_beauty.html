<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>AOS_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="aos---151">AOS - 151</h2>
<ul>
<li><details>
<summary>
(2021). Adaptive transfer learning. <em>AOS</em>, <em>49</em>(6),
3618–3649. (<a href="https://doi.org/10.1214/21-AOS2102">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In transfer learning, we wish to make inference about a target population when we have access to data both from the distribution itself, and from a different but related source distribution. We introduce a flexible framework for transfer learning in the context of binary classification, allowing for covariate-dependent relationships between the source and target distributions that are not required to preserve the Bayes decision boundary. Our main contributions are to derive the minimax optimal rates of convergence (up to poly-logarithmic factors) in this problem, and show that the optimal rate can be achieved by an algorithm that adapts to key aspects of the unknown transfer relationship, as well as the smoothness and tail parameters of our distributional classes. This optimal rate turns out to have several regimes, depending on the interplay between the relative sample sizes and the strength of the transfer relationship, and our algorithm achieves optimality by careful, decision tree-based calibration of local nearest-neighbour procedures.},
  archive      = {J_AOS},
  author       = {Henry W. J. Reeve and Timothy I. Cannings and Richard J. Samworth},
  doi          = {10.1214/21-AOS2102},
  journal      = {The Annals of Statistics},
  number       = {6},
  pages        = {3618-3649},
  shortjournal = {Ann. Statist.},
  title        = {Adaptive transfer learning},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Are deviations in a gradually varying mean relevant? A
testing approach based on sup-norm estimators. <em>AOS</em>,
<em>49</em>(6), 3583–3617. (<a
href="https://doi.org/10.1214/21-AOS2098">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classical change point analysis aims at (1) detecting abrupt changes in the mean of a possibly nonstationary time series and at (2) identifying regions where the mean exhibits a piecewise constant behavior. In many applications however, it is more reasonable to assume that the mean changes gradually in a smooth way. Those gradual changes may either be nonrelevant (i.e., small), or relevant for a specific problem at hand, and the present paper presents statistical methodology to detect the latter. More precisely, we consider the common nonparametric regression model Xi=μ(i/n)+εi with centered errors and propose a test for the null hypothesis that the maximum absolute deviation of the regression function μ from a functional g(μ) (such as the value μ(0) or the integral ∫01μ(t)dt) is smaller than a given threshold on a given interval [x0,x1]⊆[0,1]. A test for this type of hypotheses is developed using an appropriate estimator, say dˆ∞,n, for the maximum deviation d∞=supt∈[x0,x1]|μ(t)−g(μ)|. We derive the limiting distribution of an appropriately standardized version of dˆ∞,n, where the standardization depends on the Lebesgue measure of the set of extremal points of the function μ(·)−g(μ). A refined procedure based on an estimate of this set is developed and its consistency is proved. The results are illustrated by means of a simulation study and a data example.},
  archive      = {J_AOS},
  author       = {Axel Bücher and Holger Dette and Florian Heinrichs},
  doi          = {10.1214/21-AOS2098},
  journal      = {The Annals of Statistics},
  number       = {6},
  pages        = {3583-3617},
  shortjournal = {Ann. Statist.},
  title        = {Are deviations in a gradually varying mean relevant? a testing approach based on sup-norm estimators},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multiscale bayesian survival analysis. <em>AOS</em>,
<em>49</em>(6), 3559–3582. (<a
href="https://doi.org/10.1214/21-AOS2097">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider Bayesian nonparametric inference in the right-censoring survival model, where modeling is made at the level of the hazard rate. We derive posterior limiting distributions for linear functionals of the hazard, and then for ‘many’ functionals simultaneously in appropriate multiscale spaces. As an application, we derive Bernstein–von Mises theorems for the cumulative hazard and survival functions, which lead to asymptotically efficient confidence bands for these quantities. Further, we show optimal posterior contraction rates for the hazard in terms of the supremum norm. In medical studies, a popular approach is to model hazards a priori as random histograms with possibly dependent heights. This and more general classes of arbitrarily smooth prior distributions are considered as applications of our theory. A sampler is provided for possibly dependent histogram posteriors. Its finite sample properties are investigated on both simulated and real data experiments.},
  archive      = {J_AOS},
  author       = {Ismaël Castillo and Stéphanie van der Pas},
  doi          = {10.1214/21-AOS2097},
  journal      = {The Annals of Statistics},
  number       = {6},
  pages        = {3559-3582},
  shortjournal = {Ann. Statist.},
  title        = {Multiscale bayesian survival analysis},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Posterior analysis of n in the binomial (n,p) problem with
both parameters unknown—with applications to quantitative nanoscopy.
<em>AOS</em>, <em>49</em>(6), 3534–3558. (<a
href="https://doi.org/10.1214/21-AOS2096">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimation of the population size n from k i.i.d. binomial observations with unknown success probability p is relevant to a multitude of applications and has a long history. Without additional prior information this is a notoriously difficult task when p becomes small, and the Bayesian approach becomes particularly useful. For a large class of priors, we establish posterior contraction and a Bernstein-von Mises type theorem in a setting where p→0 and n→∞ as k→∞. Furthermore, we suggest a new class of Bayesian estimators for n and provide a comprehensive simulation study in which we investigate their performance. To showcase the advantages of a Bayesian approach on real data, we also benchmark our estimators in a novel application from super-resolution microscopy.},
  archive      = {J_AOS},
  author       = {Johannes Schmidt-Hieber and Laura Fee Schneider and Thomas Staudt and Andrea Krajina and Timo Aspelmeier and Axel Munk},
  doi          = {10.1214/21-AOS2096},
  journal      = {The Annals of Statistics},
  number       = {6},
  pages        = {3534-3558},
  shortjournal = {Ann. Statist.},
  title        = {Posterior analysis of n in the binomial (n,p) problem with both parameters unknown—with applications to quantitative nanoscopy},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Total variation regularized fréchet regression for
metric-space valued data. <em>AOS</em>, <em>49</em>(6), 3510–3533. (<a
href="https://doi.org/10.1214/21-AOS2095">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Non-Euclidean data that are indexed with a scalar predictor such as time are increasingly encountered in data applications, while statistical methodology and theory for such random objects are not well developed yet. To address the need for new methodology in this area, we develop a total variation regularization technique for nonparametric Fréchet regression, which refers to a regression setting where a response residing in a metric space is paired with a scalar predictor and the target is a conditional Fréchet mean. Specifically, we seek to approximate an unknown metric-space valued function by an estimator that minimizes the Fréchet version of least squares and at the same time has small total variation, appropriately defined for metric-space valued objects. We show that the resulting estimator is representable by a piece-wise constant function and establish the minimax convergence rate of the proposed estimator for metric data objects that reside in Hadamard spaces. We illustrate the numerical performance of the proposed method for both simulated and real data, including metric spaces of symmetric positive-definite matrices with the affine-invariant distance, of probability distributions on the real line with the Wasserstein distance, and of phylogenetic trees with the Billera–Holmes–Vogtmann metric.},
  archive      = {J_AOS},
  author       = {Zhenhua Lin and Hans-Georg Müller},
  doi          = {10.1214/21-AOS2095},
  journal      = {The Annals of Statistics},
  number       = {6},
  pages        = {3510-3533},
  shortjournal = {Ann. Statist.},
  title        = {Total variation regularized fréchet regression for metric-space valued data},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Uncertainty quantification for bayesian CART. <em>AOS</em>,
<em>49</em>(6), 3482–3509. (<a
href="https://doi.org/10.1214/21-AOS2093">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work affords new insights into Bayesian CART in the context of structured wavelet shrinkage. The main thrust is to develop a formal inferential framework for Bayesian tree-based regression. We reframe Bayesian CART as a g-type prior which departs from the typical wavelet product priors by harnessing correlation induced by the tree topology. The practically used Bayesian CART priors are shown to attain adaptive near rate-minimax posterior concentration in the supremum norm in regression models. For the fundamental goal of uncertainty quantification, we construct adaptive confidence bands for the regression function with uniform coverage under self-similarity. In addition, we show that tree-posteriors enable optimal inference in the form of efficient confidence sets for smooth functionals of the regression function.},
  archive      = {J_AOS},
  author       = {Ismaël Castillo and Veronika Ročková},
  doi          = {10.1214/21-AOS2093},
  journal      = {The Annals of Statistics},
  number       = {6},
  pages        = {3482-3509},
  shortjournal = {Ann. Statist.},
  title        = {Uncertainty quantification for bayesian CART},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An asymptotic test for constancy of the variance under
short-range dependence. <em>AOS</em>, <em>49</em>(6), 3460–3481. (<a
href="https://doi.org/10.1214/21-AOS2092">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel approach to test for heteroscedasticity of a nonstationary time series that is based on Gini’s mean difference of logarithmic local sample variances. In order to analyse the large sample behaviour of our test statistic, we establish new limit theorems for U-statistics of dependent triangular arrays. We derive the asymptotic distribution of the test statistic under the null hypothesis of a constant variance and show that the test is consistent against a large class of alternatives, including multiple structural breaks in the variance. Our test is applicable even in the case of nonstationary processes, assuming a locally stationary mean function. The performance of the test and its comparatively low computation time are illustrated in an extensive simulation study. As an application, we analyse Google Trends data, monitoring the relative search interest for the topic “global warming.”},
  archive      = {J_AOS},
  author       = {Sara K. Schmidt and Max Wornowizki and Roland Fried and Herold Dehling},
  doi          = {10.1214/21-AOS2092},
  journal      = {The Annals of Statistics},
  number       = {6},
  pages        = {3460-3481},
  shortjournal = {Ann. Statist.},
  title        = {An asymptotic test for constancy of the variance under short-range dependence},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Analysis of generalized bregman surrogate algorithms for
nonsmooth nonconvex statistical learning. <em>AOS</em>, <em>49</em>(6),
3434–3459. (<a href="https://doi.org/10.1214/21-AOS2090">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern statistical applications often involve minimizing an objective function that may be nonsmooth and/or nonconvex. This paper focuses on a broad Bregman-surrogate algorithm framework including the local linear approximation, mirror descent, iterative thresholding, DC programming and many others as particular instances. The recharacterization via generalized Bregman functions enables us to construct suitable error measures and establish global convergence rates for nonconvex and nonsmooth objectives in possibly high dimensions. For sparse learning problems with a composite objective, under some regularity conditions, the obtained estimators as the surrogate’s fixed points, though not necessarily local minimizers, enjoy provable statistical guarantees, and the sequence of iterates can be shown to approach the statistical truth within the desired accuracy geometrically fast. The paper also studies how to design adaptive momentum based accelerations without assuming convexity or smoothness by carefully controlling stepsize and relaxation parameters.},
  archive      = {J_AOS},
  author       = {Yiyuan She and Zhifeng Wang and Jiuwu Jin},
  doi          = {10.1214/21-AOS2090},
  journal      = {The Annals of Statistics},
  number       = {6},
  pages        = {3434-3459},
  shortjournal = {Ann. Statist.},
  title        = {Analysis of generalized bregman surrogate algorithms for nonsmooth nonconvex statistical learning},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Optimal adaptivity of signed-polygon statistics for network
testing. <em>AOS</em>, <em>49</em>(6), 3408–3433. (<a
href="https://doi.org/10.1214/21-AOS2089">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given a symmetric social network, we are interested in testing whether it has only one community or multiple communities. The desired tests should (a) accommodate severe degree heterogeneity, (b) accommodate mixed memberships, (c) have a tractable null distribution and (d) adapt automatically to different levels of sparsity, and achieve the optimal phase diagram. How to find such a test is a challenging problem. We propose the Signed Polygon as a class of new tests. Fixing m≥3, for each m-gon in the network, define a score using the centered adjacency matrix. The sum of such scores is then the mth order Signed Polygon statistic. The Signed Triangle (SgnT) and the Signed Quadrilateral (SgnQ) are special examples of the Signed Polygon. We show that both the SgnT and SgnQ tests satisfy (a)–(d), and especially, they work well for both very sparse and less sparse networks. Our proposed tests compare favorably with existing tests. For example, the EZ and GC tests behave unsatisfactorily in the less sparse case and do not achieve the optimal phase diagram. Also, many existing tests do not allow for severe heterogeneity or mixed memberships, and they behave unsatisfactorily in our settings. The analysis of the SgnT and SgnQ tests is delicate and extremely tedious, and the main reason is that we need a unified proof that covers a wide range of sparsity levels and a wide range of degree heterogeneity. For lower bound theory, we use a phase transition framework, which includes the standard minimax argument, but is more informative. The proof uses classical theorems on matrix scaling.},
  archive      = {J_AOS},
  author       = {Jiashun Jin and Zheng Tracy Ke and Shengming Luo},
  doi          = {10.1214/21-AOS2089},
  journal      = {The Annals of Statistics},
  number       = {6},
  pages        = {3408-3433},
  shortjournal = {Ann. Statist.},
  title        = {Optimal adaptivity of signed-polygon statistics for network testing},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Asymptotic properties of penalized spline estimators in
concave extended linear models: Rates of convergence. <em>AOS</em>,
<em>49</em>(6), 3383–3407. (<a
href="https://doi.org/10.1214/21-AOS2088">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper develops a general theory on rates of convergence of penalized spline estimators for function estimation when the likelihood functional is concave in candidate functions, where the likelihood is interpreted in a broad sense that includes conditional likelihood, quasi-likelihood and pseudo-likelihood. The theory allows all feasible combinations of the spline degree, the penalty order and the smoothness of the unknown functions. According to this theory, the asymptotic behaviors of the penalized spline estimators depends on interplay between the spline knot number and the penalty parameter. The general theory is applied to obtain results in a variety of contexts, including regression, generalized regression such as logistic regression and Poisson regression, density estimation, conditional hazard function estimation for censored data, quantile regression, diffusion function estimation for a diffusion type process and estimation of spectral density function of a stationary time series. For multidimensional function estimation, the theory (presented in the Supplementary Material) covers both penalized tensor product splines and penalized bivariate splines on triangulations.},
  archive      = {J_AOS},
  author       = {Jianhua Z. Huang and Ya Su},
  doi          = {10.1214/21-AOS2088},
  journal      = {The Annals of Statistics},
  number       = {6},
  pages        = {3383-3407},
  shortjournal = {Ann. Statist.},
  title        = {Asymptotic properties of penalized spline estimators in concave extended linear models: Rates of convergence},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Extreme conditional expectile estimation in heavy-tailed
heteroscedastic regression models. <em>AOS</em>, <em>49</em>(6),
3358–3382. (<a href="https://doi.org/10.1214/21-AOS2087">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Expectiles define a least squares analogue of quantiles. They have been the focus of a substantial quantity of research in the context of actuarial and financial risk assessment over the last decade. The behaviour and estimation of unconditional extreme expectiles using independent and identically distributed heavy-tailed observations have been investigated in a recent series of papers. We build here a general theory for the estimation of extreme conditional expectiles in heteroscedastic regression models with heavy-tailed noise; our approach is supported by general results of independent interest on residual-based extreme value estimators in heavy-tailed regression models, and is intended to cope with covariates having a large but fixed dimension. We demonstrate how our results can be applied to a wide class of important examples, among which are linear models, single-index models as well as ARMA and GARCH time series models. Our estimators are showcased on a numerical simulation study and on real sets of actuarial and financial data.},
  archive      = {J_AOS},
  author       = {Stéphane Girard and Gilles Stupfler and Antoine Usseglio-Carleve},
  doi          = {10.1214/21-AOS2087},
  journal      = {The Annals of Statistics},
  number       = {6},
  pages        = {3358-3382},
  shortjournal = {Ann. Statist.},
  title        = {Extreme conditional expectile estimation in heavy-tailed heteroscedastic regression models},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Optimal linear discriminators for the discrete choice model
in growing dimensions. <em>AOS</em>, <em>49</em>(6), 3324–3357. (<a
href="https://doi.org/10.1214/21-AOS2085">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Manski’s celebrated maximum score estimator for the discrete choice model, which is an optimal linear discriminator, has been the focus of much investigation in both the econometrics and statistics literatures, but its behavior under growing dimension scenarios largely remains unknown. This paper addresses that gap. Two different cases are considered: p grows with n but at a slow rate, that is, p/n→0; and p≫n (fast growth). In the binary response model, we recast Manski’s score estimation as empirical risk minimization for a classification problem, and derive the ℓ2 rate of convergence of the score estimator under a new transition condition in terms of a margin parameter that calibrates the level of difficulty of the estimation problem. We also establish upper and lower bounds for the minimax ℓ2 error in the binary choice model that differ by a logarithmic factor, and construct a minimax-optimal estimator in the slow growth regime. Some extensions to the multinomial choice model are also considered.},
  archive      = {J_AOS},
  author       = {Debarghya Mukherjee and Moulinath Banerjee and Ya’acov Ritov},
  doi          = {10.1214/21-AOS2085},
  journal      = {The Annals of Statistics},
  number       = {6},
  pages        = {3324-3357},
  shortjournal = {Ann. Statist.},
  title        = {Optimal linear discriminators for the discrete choice model in growing dimensions},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Marginal singularity and the benefits of labels in
covariate-shift. <em>AOS</em>, <em>49</em>(6), 3299–3323. (<a
href="https://doi.org/10.1214/21-AOS2084">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transfer Learning addresses common situations in Machine Leaning where little or no labeled data is available for a target prediction problem—corresponding to a distribution Q, but much labeled data is available from some related but different data distribution P. This work is concerned with the fundamental limits of transfer, that is, the limits in target performance in terms of (1) sample sizes from P and Q, and (2) differences in data distributions P, Q. In particular, we aim to address practical questions such as how much target data from Q is sufficient given a certain amount of related data from P, and how to optimally sample such target data for labeling. We present new minimax results for transfer in nonparametric classification (i.e., for situations where little is known about the target classifier), under the common assumption that the marginal distributions of covariates differ between P and Q (often termed covariate-shift). Our results are first to concisely capture the relative benefits of source and target labeled data in these settings through information-theoretic limits. Namely, we show that the benefits of target labels are tightly controlled by a transfer-exponent γ that encodes how singular Q is locally with respect to P, and interestingly paints a more favorable picture of transfer than what might be believed from insights from previous work. In fact, while previous work rely largely on refinements of traditional metrics and divergences between distributions, and often only yield a coarse view of when transfer is possible or not, our analysis—in terms of γ—reveals a continuum of new regimes ranging from easy to hard transfer. We then address the practical question of how to efficiently sample target data to label, by showing that a recently proposed semi-supervised procedure—based on k-NN classification, can be refined to adapt to unknown γ and, therefore, requests target labels only when beneficial, while achieving nearly minimax-optimal transfer rates without knowledge of distributional parameters. Of independent interest, we obtain new minimax-optimality results for vanilla k-NN classification in regimes with nonuniform marginals.},
  archive      = {J_AOS},
  author       = {Samory Kpotufe and Guillaume Martinet},
  doi          = {10.1214/21-AOS2084},
  journal      = {The Annals of Statistics},
  number       = {6},
  pages        = {3299-3323},
  shortjournal = {Ann. Statist.},
  title        = {Marginal singularity and the benefits of labels in covariate-shift},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Statistical guarantees for bayesian uncertainty
quantification in nonlinear inverse problems with gaussian process
priors. <em>AOS</em>, <em>49</em>(6), 3255–3298. (<a
href="https://doi.org/10.1214/21-AOS2082">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian inference and uncertainty quantification in a general class of nonlinear inverse regression models is considered. Analytic conditions on the regression model {G(θ):θ∈Θ} and on Gaussian process priors for θ are provided such that semiparametrically efficient inference is possible for a large class of linear functionals of θ. A general Bernstein–von Mises theorem is proved that shows that the (non-Gaussian) posterior distributions are approximated by certain Gaussian measures centred at the posterior mean. As a consequence, posterior-based credible sets are valid and optimal from a frequentist point of view. The theory is illustrated with two applications with PDEs that arise in nonlinear tomography problems: an elliptic inverse problem for a Schrödinger equation, and inversion of non-Abelian X-ray transforms. New analytical techniques are deployed to show that the relevant Fisher information operators are invertible between suitable function spaces.},
  archive      = {J_AOS},
  author       = {François Monard and Richard Nickl and Gabriel P. Paternain},
  doi          = {10.1214/21-AOS2082},
  journal      = {The Annals of Statistics},
  number       = {6},
  pages        = {3255-3298},
  shortjournal = {Ann. Statist.},
  title        = {Statistical guarantees for bayesian uncertainty quantification in nonlinear inverse problems with gaussian process priors},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Wilks’ theorem for semiparametric regressions with weakly
dependent data. <em>AOS</em>, <em>49</em>(6), 3228–3254. (<a
href="https://doi.org/10.1214/21-AOS2081">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The empirical likelihood inference is extended to a class of semiparametric models for stationary, weakly dependent series. A partially linear single-index regression is used for the conditional mean of the series given its past, and the present and past values of a vector of covariates. A parametric model for the conditional variance of the series is added to capture further nonlinear effects. We propose suitable moment equations which characterize the mean and variance model. We derive an empirical log-likelihood ratio which includes nonparametric estimators of several functions, and we show that this ratio behaves asymptotically as if the functions were given.},
  archive      = {J_AOS},
  author       = {Marie du Roy de Chaumaray and Matthieu Marbac and Valentin Patilea},
  doi          = {10.1214/21-AOS2081},
  journal      = {The Annals of Statistics},
  number       = {6},
  pages        = {3228-3254},
  shortjournal = {Ann. Statist.},
  title        = {Wilks’ theorem for semiparametric regressions with weakly dependent data},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Augmented minimax linear estimation. <em>AOS</em>,
<em>49</em>(6), 3206–3227. (<a
href="https://doi.org/10.1214/21-AOS2080">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many statistical estimands can expressed as continuous linear functionals of a conditional expectation function. This includes the average treatment effect under unconfoundedness and generalizations for continuous-valued and personalized treatments. In this paper, we discuss a general approach to estimating such quantities: we begin with a simple plug-in estimator based on an estimate of the conditional expectation function, and then correct the plug-in estimator by subtracting a minimax linear estimate of its error. We show that our method is semiparametrically efficient under weak conditions and observe promising performance on both real and simulated data.},
  archive      = {J_AOS},
  author       = {David A. Hirshberg and Stefan Wager},
  doi          = {10.1214/21-AOS2080},
  journal      = {The Annals of Statistics},
  number       = {6},
  pages        = {3206-3227},
  shortjournal = {Ann. Statist.},
  title        = {Augmented minimax linear estimation},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Community detection on mixture multilayer networks via
regularized tensor decomposition. <em>AOS</em>, <em>49</em>(6),
3181–3205. (<a href="https://doi.org/10.1214/21-AOS2079">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the problem of community detection in multilayer networks, where pairs of nodes can be related in multiple modalities. We introduce a general framework, that is, mixture multilayer stochastic block model (MMSBM), which includes many earlier models as special cases. We propose a tensor-based algorithm (TWIST) to reveal both global/local memberships of nodes, and memberships of layers. We show that the TWIST procedure can accurately detect the communities with small misclassification error as the number of nodes and/or number of layers increases. Numerical studies confirm our theoretical findings. To our best knowledge, this is the first systematic study on the mixture multilayer networks using tensor decomposition. The method is applied to two real datasets: worldwide trading networks and malaria parasite genes networks, yielding new and interesting findings.},
  archive      = {J_AOS},
  author       = {Bing-Yi Jing and Ting Li and Zhongyuan Lyu and Dong Xia},
  doi          = {10.1214/21-AOS2079},
  journal      = {The Annals of Statistics},
  number       = {6},
  pages        = {3181-3205},
  shortjournal = {Ann. Statist.},
  title        = {Community detection on mixture multilayer networks via regularized tensor decomposition},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptive learning rates for support vector machines working
on data with low intrinsic dimension. <em>AOS</em>, <em>49</em>(6),
3153–3180. (<a href="https://doi.org/10.1214/21-AOS2078">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We derive improved regression and classification rates for support vector machines using Gaussian kernels under the assumption that the data has some low-dimensional intrinsic structure that is described by the box-counting dimension. Under some standard regularity assumptions for regression and classification, we prove learning rates, in which the dimension of the ambient space is replaced by the box-counting dimension of the support of the data generating distribution. In the regression case, our rates are in some cases minimax optimal up to logarithmic factors, whereas in the classification case our rates are minimax optimal up to logarithmic factors in a certain range of our assumptions and otherwise of the form of the best known rates. Furthermore, we show that a training validation approach for choosing the hyperparameters of a SVM in a data dependent way achieves the same rates adaptively, that is, without any knowledge on the data generating distribution.},
  archive      = {J_AOS},
  author       = {Thomas Hamm and Ingo Steinwart},
  doi          = {10.1214/21-AOS2078},
  journal      = {The Annals of Statistics},
  number       = {6},
  pages        = {3153-3180},
  shortjournal = {Ann. Statist.},
  title        = {Adaptive learning rates for support vector machines working on data with low intrinsic dimension},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On fixed-domain asymptotics, parameter estimation and
isotropic gaussian random fields with matérn covariance functions.
<em>AOS</em>, <em>49</em>(6), 3127–3152. (<a
href="https://doi.org/10.1214/21-AOS2077">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A method is proposed for estimating the microergodic parameters (including the smoothness parameter) of stationary Gaussian random fields on Rd with isotropic Matérn covariance functions using irregularly spaced data. This approach uses higher-order quadratic variations and is applied to three designs, namely stratified sampling design, randomized sampling design and deformed lattice design. Microergodic parameter estimators are constructed for each of the designs. Under mild conditions, these estimators are shown to be consistent with respect to fixed-domain asymptotics. Upper bounds to the convergence rate of the estimators are also established. A simulation study is conducted to gauge the accuracy of the proposed estimators.},
  archive      = {J_AOS},
  author       = {Wei-Liem Loh and Saifei Sun and Jun Wen},
  doi          = {10.1214/21-AOS2077},
  journal      = {The Annals of Statistics},
  number       = {6},
  pages        = {3127-3152},
  shortjournal = {Ann. Statist.},
  title        = {On fixed-domain asymptotics, parameter estimation and isotropic gaussian random fields with matérn covariance functions},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Online inference with multi-modal likelihood functions.
<em>AOS</em>, <em>49</em>(6), 3103–3126. (<a
href="https://doi.org/10.1214/21-AOS2076">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Let (Yt)t≥1 be a sequence of i.i.d. observations and {fθ,θ∈Rd} be a parametric model. We introduce a new online algorithm for computing a sequence (θˆt)t≥1, which is shown to converge almost surely to argmaxθ∈RdE[logfθ(Y1)] at rate O(log(t)(1+ε)/2t−1/2), with ε&gt;0 a user specified parameter. This convergence result is obtained under standard conditions on the statistical model and, most notably, we allow the mapping θ↦E[logfθ(Y1)] to be multi-modal. However, the computational cost to process each observation grows exponentially with the dimension of θ, which makes the proposed approach applicable to low or moderate dimensional problems only. We also derive a version of the estimator θˆt, which is well suited to student-t linear regression models that are popular tools for robust linear regression. As shown by experiments on simulated and real data, the corresponding estimator of the regression coefficients is, as expected, robust to the presence of outliers and thus, as a by-product, we obtain a new adaptive and robust online estimation procedure for linear regression models.},
  archive      = {J_AOS},
  author       = {Mathieu Gerber and Kari Heine},
  doi          = {10.1214/21-AOS2076},
  journal      = {The Annals of Statistics},
  number       = {6},
  pages        = {3103-3126},
  shortjournal = {Ann. Statist.},
  title        = {Online inference with multi-modal likelihood functions},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A simple measure of conditional dependence. <em>AOS</em>,
<em>49</em>(6), 3070–3102. (<a
href="https://doi.org/10.1214/21-AOS2073">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a coefficient of conditional dependence between two random variables Y and Z given a set of other variables X1,…,Xp, based on an i.i.d. sample. The coefficient has a long list of desirable properties, the most important of which is that under absolutely no distributional assumptions, it converges to a limit in [0,1], where the limit is 0 if and only if Y and Z are conditionally independent given X1,…,Xp, and is 1 if and only if Y is equal to a measurable function of Z given X1,…,Xp. Moreover, it has a natural interpretation as a nonlinear generalization of the familiar partial R2 statistic for measuring conditional dependence by regression. Using this statistic, we devise a new variable selection algorithm, called Feature Ordering by Conditional Independence (FOCI), which is model-free, has no tuning parameters, and is provably consistent under sparsity assumptions. A number of applications to synthetic and real data sets are worked out.},
  archive      = {J_AOS},
  author       = {Mona Azadkia and Sourav Chatterjee},
  doi          = {10.1214/21-AOS2073},
  journal      = {The Annals of Statistics},
  number       = {6},
  pages        = {3070-3102},
  shortjournal = {Ann. Statist.},
  title        = {A simple measure of conditional dependence},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Estimating the number of components in finite mixture models
via the group-sort-fuse procedure. <em>AOS</em>, <em>49</em>(6),
3043–3069. (<a href="https://doi.org/10.1214/21-AOS2072">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimation of the number of components (or order) of a finite mixture model is a long standing and challenging problem in statistics. We propose the Group-Sort-Fuse (GSF) procedure—a new penalized likelihood approach for simultaneous estimation of the order and mixing measure in multidimensional finite mixture models. Unlike methods which fit and compare mixtures with varying orders using criteria involving model complexity, our approach directly penalizes a continuous function of the model parameters. More specifically, given a conservative upper bound on the order, the GSF groups and sorts mixture component parameters to fuse those which are redundant. For a wide range of finite mixture models, we show that the GSF is consistent in estimating the true mixture order and achieves the n−1/2 convergence rate for parameter estimation up to polylogarithmic factors. The GSF is implemented for several univariate and multivariate mixture models in the R package GroupSortFuse. Its finite sample performance is supported by a thorough simulation study, and its application is illustrated on two real data examples.},
  archive      = {J_AOS},
  author       = {Tudor Manole and Abbas Khalili},
  doi          = {10.1214/21-AOS2072},
  journal      = {The Annals of Statistics},
  number       = {6},
  pages        = {3043-3069},
  shortjournal = {Ann. Statist.},
  title        = {Estimating the number of components in finite mixture models via the group-sort-fuse procedure},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Two-level parallel flats designs. <em>AOS</em>,
<em>49</em>(5), 3015–3042. (<a
href="https://doi.org/10.1214/21-AOS2071">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Regular 2n−p designs are also known as single flat designs. Parallel flats designs (PFDs) consisting of three parallel flats (3-PFDs) are the most frequently utilized PFDs, due to their simple structure. Generalizing to f-PFD with f&gt;3 is more challenging. This paper aims to study the general theory for the f-PFD for any f≥3. We propose a method for obtaining the confounding frequency vectors for all nonequivalent f-PFDs, and to find the least G-aberration (or highest D-efficiency) f-PFD constructed from any single flat. PFDs are particularly useful for constructing nonregular fraction, split-plot or randomized block designs. We also characterize the quaternary code design series as PFDs. Finally, we show how designs constructed by concatenating regular fractions from different families may also have a parallel flats structure. Examples are given throughout to illustrate the results.},
  archive      = {J_AOS},
  author       = {Chunyan Wang and Robert W. Mee},
  doi          = {10.1214/21-AOS2071},
  journal      = {The Annals of Statistics},
  number       = {5},
  pages        = {3015-3042},
  shortjournal = {Ann. Statist.},
  title        = {Two-level parallel flats designs},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Semiparametric optimal estimation with nonignorable
nonresponse data. <em>AOS</em>, <em>49</em>(5), 2991–3014. (<a
href="https://doi.org/10.1214/21-AOS2070">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When the response mechanism is believed to be not missing at random (NMAR), a valid analysis requires stronger assumptions on the response mechanism than standard statistical methods would otherwise require. Semiparametric estimators have been developed under the parametric model assumptions on the response mechanism. In this paper, a new statistical test is proposed to guarantee model identifiability without using instrumental variable assumption. Furthermore, we develop optimal semiparametric estimation for parameters such as the population mean. Specifically, we propose two semiparametric optimal estimators that do not require any model assumptions other than the response mechanism. Asymptotic properties of the proposed estimators are discussed. An extensive simulation study is presented to compare with some existing methods. We present an application of our method using Korean labor and income panel survey data.},
  archive      = {J_AOS},
  author       = {Kosuke Morikawa and Jae Kwang Kim},
  doi          = {10.1214/21-AOS2070},
  journal      = {The Annals of Statistics},
  number       = {5},
  pages        = {2991-3014},
  shortjournal = {Ann. Statist.},
  title        = {Semiparametric optimal estimation with nonignorable nonresponse data},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Efficiency of delayed-acceptance random walk metropolis
algorithms. <em>AOS</em>, <em>49</em>(5), 2972–2990. (<a
href="https://doi.org/10.1214/21-AOS2068">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Delayed-acceptance Metropolis–Hastings and delayed-acceptance pseudo-marginal Metropolis–Hastings algorithms can be applied when it is computationally expensive to calculate the true posterior or an unbiased stochastic approximation thereof, but a computationally cheap deterministic approximation is available. An initial accept–reject stage uses the cheap approximation for computing the Metropolis–Hastings ratio; proposals which are accepted at this stage are subjected to a further accept–reject step, which corrects for the error in the approximation. Since the expensive posterior, or the approximation thereof, is only evaluated for proposals which are accepted at the first stage, the cost of the algorithm is reduced and larger scalings may be used. We focus on the random walk Metropolis (RWM) and consider the delayed-acceptance RWM and the delayed-acceptance pseudo-marginal RWM. We provide a framework for incorporating relatively general deterministic approximations into the theoretical analysis of high-dimensional targets. Justified by diffusion-approximation arguments, we derive expressions for the limiting efficiency and acceptance rates in high-dimensional settings. Finally, these theoretical insights are leveraged to formulate practical guidelines for the efficient tuning of the algorithms. The robustness of these guidelines and predicted properties are verified against simulation studies, all of which are strictly outside of the domain of validity of our limit results.},
  archive      = {J_AOS},
  author       = {Chris Sherlock and Alexandre H. Thiery and Andrew Golightly},
  doi          = {10.1214/21-AOS2068},
  journal      = {The Annals of Statistics},
  number       = {5},
  pages        = {2972-2990},
  shortjournal = {Ann. Statist.},
  title        = {Efficiency of delayed-acceptance random walk metropolis algorithms},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bridging convex and nonconvex optimization in robust PCA:
Noise, outliers and missing data. <em>AOS</em>, <em>49</em>(5),
2948–2971. (<a href="https://doi.org/10.1214/21-AOS2066">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper delivers improved theoretical guarantees for the convex programming approach in low-rank matrix estimation, in the presence of (1) random noise, (2) gross sparse outliers and (3) missing data. This problem, often dubbed as robust principal component analysis (robust PCA), finds applications in various domains. Despite the wide applicability of convex relaxation, the available statistical support (particularly the stability analysis in the presence of random noise) remains highly suboptimal, which we strengthen in this paper. When the unknown matrix is well conditioned, incoherent and of constant rank, we demonstrate that a principled convex program achieves near-optimal statistical accuracy, in terms of both the Euclidean loss and the ℓ∞ loss. All of this happens even when nearly a constant fraction of observations are corrupted by outliers with arbitrary magnitudes. The key analysis idea lies in bridging the convex program in use and an auxiliary nonconvex optimization algorithm, and hence the title of this paper.},
  archive      = {J_AOS},
  author       = {Yuxin Chen and Jianqing Fan and Cong Ma and Yuling Yan},
  doi          = {10.1214/21-AOS2066},
  journal      = {The Annals of Statistics},
  number       = {5},
  pages        = {2948-2971},
  shortjournal = {Ann. Statist.},
  title        = {Bridging convex and nonconvex optimization in robust PCA: Noise, outliers and missing data},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Measuring dependence in the wasserstein distance for
bayesian nonparametric models. <em>AOS</em>, <em>49</em>(5), 2916–2947.
(<a href="https://doi.org/10.1214/21-AOS2065">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The proposal and study of dependent Bayesian nonparametric models has been one of the most active research lines in the last two decades, with random vectors of measures representing a natural and popular tool to define them. Nonetheless, a principled approach to understand and quantify the associated dependence structure is still missing. We devise a general, and not model-specific, framework to achieve this task for random measure based models, which consists in: (a) quantify dependence of a random vector of probabilities in terms of closeness to exchangeability, which corresponds to the maximally dependent coupling with the same marginal distributions, that is, the comonotonic vector; (b) recast the problem in terms of the underlying random measures (in the same Fréchet class) and quantify the closeness to comonotonicity; (c) define a distance based on the Wasserstein metric, which is ideally suited for spaces of measures, to measure the dependence in a principled way. Several results, which represent the very first in the area, are obtained. In particular, useful bounds in terms of the underlying Lévy intensities are derived relying on compound Poisson approximations. These are then specialized to popular models in the Bayesian literature leading to interesting insights.},
  archive      = {J_AOS},
  author       = {Marta Catalano and Antonio Lijoi and Igor Prünster},
  doi          = {10.1214/21-AOS2065},
  journal      = {The Annals of Statistics},
  number       = {5},
  pages        = {2916-2947},
  shortjournal = {Ann. Statist.},
  title        = {Measuring dependence in the wasserstein distance for bayesian nonparametric models},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Foundations of structural causal models with cycles and
latent variables. <em>AOS</em>, <em>49</em>(5), 2885–2915. (<a
href="https://doi.org/10.1214/21-AOS2064">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Structural causal models (SCMs), also known as (nonparametric) structural equation models (SEMs), are widely used for causal modeling purposes. In particular, acyclic SCMs, also known as recursive SEMs, form a well-studied subclass of SCMs that generalize causal Bayesian networks to allow for latent confounders. In this paper, we investigate SCMs in a more general setting, allowing for the presence of both latent confounders and cycles. We show that in the presence of cycles, many of the convenient properties of acyclic SCMs do not hold in general: they do not always have a solution; they do not always induce unique observational, interventional and counterfactual distributions; a marginalization does not always exist, and if it exists the marginal model does not always respect the latent projection; they do not always satisfy a Markov property; and their graphs are not always consistent with their causal semantics. We prove that for SCMs in general each of these properties does hold under certain solvability conditions. Our work generalizes results for SCMs with cycles that were only known for certain special cases so far. We introduce the class of simple SCMs that extends the class of acyclic SCMs to the cyclic setting, while preserving many of the convenient properties of acyclic SCMs. With this paper, we aim to provide the foundations for a general theory of statistical causal modeling with SCMs.},
  archive      = {J_AOS},
  author       = {Stephan Bongers and Patrick Forré and Jonas Peters and Joris M. Mooij},
  doi          = {10.1214/21-AOS2064},
  journal      = {The Annals of Statistics},
  number       = {5},
  pages        = {2885-2915},
  shortjournal = {Ann. Statist.},
  title        = {Foundations of structural causal models with cycles and latent variables},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Construction of mixed orthogonal arrays with high strength.
<em>AOS</em>, <em>49</em>(5), 2870–2884. (<a
href="https://doi.org/10.1214/21-AOS2063">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A considerable portion of the work on mixed orthogonal arrays applies specifically to arrays of strength 2. Although strength t=2 is arguably the most important case for statistical applications, there is an urgent need for better methods for t≥3. However, the knowledge on the existence of arrays for t≥3 is rather limited. In this paper, new construction methods for symmetric and asymmetric orthogonal arrays (OAs) with high strength are proposed by using lower strength orthogonal partitions of spaces and OAs. A positive answer is provided to the open problem in Hedayat, Sloane and Stufken (Orthogonal Arrays: Theory and Applications (1999) Springer) on developing better methods and tools for the construction of mixed orthogonal arrays with strength t≥3. Not only are the methods straightforward, but also they are useful for constructing symmetric or asymmetric OAs of arbitrary strengths, numbers of levels and various sizes. The constructed OAs can be utilized to generate more OAs. The resulting OAs have a high degree of flexibility and many other desirable properties. Some selective OAs are tabulated for practical uses.},
  archive      = {J_AOS},
  author       = {Shanqi Pang and Jing Wang and Dennis K. J. Lin and Min-Qian Liu},
  doi          = {10.1214/21-AOS2063},
  journal      = {The Annals of Statistics},
  number       = {5},
  pages        = {2870-2884},
  shortjournal = {Ann. Statist.},
  title        = {Construction of mixed orthogonal arrays with high strength},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Distributed statistical inference for massive data.
<em>AOS</em>, <em>49</em>(5), 2851–2869. (<a
href="https://doi.org/10.1214/21-AOS2062">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper considers distributed statistical inference for general symmetric statistics in the context of massive data with efficient computation. Estimation efficiency and asymptotic distributions of the distributed statistics are provided, which reveal different results between the nondegenerate and degenerate cases, and show the number of the data subsets plays an important role. Two distributed bootstrap methods are proposed and analyzed to approximation the underlying distribution of the distributed statistics with improved computation efficiency over existing methods. The accuracy of the distributional approximation by the bootstrap are studied theoretically. One of the methods, the pseudo-distributed bootstrap, is particularly attractive if the number of datasets is large as it directly resamples the subset-based statistics, assumes less stringent conditions and its performance can be improved by studentization.},
  archive      = {J_AOS},
  author       = {Song Xi Chen and Liuhua Peng},
  doi          = {10.1214/21-AOS2062},
  journal      = {The Annals of Statistics},
  number       = {5},
  pages        = {2851-2869},
  shortjournal = {Ann. Statist.},
  title        = {Distributed statistical inference for massive data},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The cost of privacy: Optimal rates of convergence for
parameter estimation with differential privacy. <em>AOS</em>,
<em>49</em>(5), 2825–2850. (<a
href="https://doi.org/10.1214/21-AOS2058">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Privacy-preserving data analysis is a rising challenge in contemporary statistics, as the privacy guarantees of statistical methods are often achieved at the expense of accuracy. In this paper, we investigate the tradeoff between statistical accuracy and privacy in mean estimation and linear regression, under both the classical low-dimensional and modern high-dimensional settings. A primary focus is to establish minimax optimality for statistical estimation with the (ε,δ)-differential privacy constraint. By refining the “tracing adversary” technique for lower bounds in the theoretical computer science literature, we improve existing minimax lower bound for low-dimensional mean estimation and establish new lower bounds for high-dimensional mean estimation and linear regression problems. We also design differentially private algorithms that attain the minimax lower bounds up to logarithmic factors. In particular, for high-dimensional linear regression, a novel private iterative hard thresholding algorithm is proposed. The numerical performance of differentially private algorithms is demonstrated by simulation studies and applications to real data sets.},
  archive      = {J_AOS},
  author       = {T. Tony Cai and Yichen Wang and Linjun Zhang},
  doi          = {10.1214/21-AOS2058},
  journal      = {The Annals of Statistics},
  number       = {5},
  pages        = {2825-2850},
  shortjournal = {Ann. Statist.},
  title        = {The cost of privacy: Optimal rates of convergence for parameter estimation with differential privacy},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Integrative methods for post-selection inference under
convex constraints. <em>AOS</em>, <em>49</em>(5), 2803–2824. (<a
href="https://doi.org/10.1214/21-AOS2057">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inference after model selection has been an active research topic in the past few years, with numerous works offering different approaches to addressing the perils of the reuse of data. In particular, major progress has been made recently on large and useful classes of problems by harnessing general theory of hypothesis testing in exponential families, but these methods have their limitations. Perhaps most immediate is the gap between theory and practice: implementing the exact theoretical prescription in realistic situations—for example, when new data arrives and inference needs to be adjusted accordingly—may be a prohibitive task. In this paper, we propose a Bayesian framework for carrying out inference after variable selection. Our framework is very flexible in the sense that it naturally accommodates different models for the data instead of requiring a case-by-case treatment. This flexibility is achieved by considering the full selective likelihood function where, crucially, we propose a novel and nontrivial approximation to the exact but intractable expression. The advantages of our methods in practical data analysis are demonstrated in an application to HIV drug-resistance data.},
  archive      = {J_AOS},
  author       = {Snigdha Panigrahi and Jonathan Taylor and Asaf Weinstein},
  doi          = {10.1214/21-AOS2057},
  journal      = {The Annals of Statistics},
  number       = {5},
  pages        = {2803-2824},
  shortjournal = {Ann. Statist.},
  title        = {Integrative methods for post-selection inference under convex constraints},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Reconciling the gaussian and whittle likelihood with an
application to estimation in the frequency domain. <em>AOS</em>,
<em>49</em>(5), 2774–2802. (<a
href="https://doi.org/10.1214/21-AOS2055">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In time series analysis there is an apparent dichotomy between time and frequency domain methods. The aim of this paper is to draw connections between frequency and time domain methods. Our focus will be on reconciling the Gaussian likelihood and the Whittle likelihood. We derive an exact, interpretable, bound between the Gaussian and Whittle likelihood of a second order stationary time series. The derivation is based on obtaining the transformation which is biorthogonal to the discrete Fourier transform of the time series. Such a transformation yields a new decomposition for the inverse of a Toeplitz matrix and enables the representation of the Gaussian likelihood within the frequency domain. We show that the difference between the Gaussian and Whittle likelihood is due to the omission of the best linear predictions outside the domain of observation in the periodogram associated with the Whittle likelihood. Based on this result, we obtain an approximation for the difference between the Gaussian and Whittle likelihoods in terms of the best fitting, finite order autoregressive parameters. These approximations are used to define two new frequency domain quasi-likelihood criteria. We show that these new criteria can yield a better approximation of the spectral divergence criterion, as compared to both the Gaussian and Whittle likelihoods. In simulations, we show that the proposed estimators have satisfactory finite sample properties.},
  archive      = {J_AOS},
  author       = {Suhasini Subba Rao and Junho Yang},
  doi          = {10.1214/21-AOS2055},
  journal      = {The Annals of Statistics},
  number       = {5},
  pages        = {2774-2802},
  shortjournal = {Ann. Statist.},
  title        = {Reconciling the gaussian and whittle likelihood with an application to estimation in the frequency domain},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Prediction bounds for higher order total variation
regularized least squares. <em>AOS</em>, <em>49</em>(5), 2755–2773. (<a
href="https://doi.org/10.1214/21-AOS2054">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We establish adaptive results for trend filtering: least squares estimation with a penalty on the total variation of (k−1)th order differences. Our approach is based on combining a general oracle inequality for the ℓ1-penalized least squares estimator with “interpolating vectors” to upper bound the “effective sparsity.” This allows one to show that the ℓ1-penalty on the kth order differences leads to an estimator that can adapt to the number of jumps in the (k−1)th order differences of the underlying signal or an approximation thereof. We show the result for k∈{1,2,3,4} and indicate how it could be derived for general k∈N.},
  archive      = {J_AOS},
  author       = {Francesco Ortelli and Sara van de Geer},
  doi          = {10.1214/21-AOS2054},
  journal      = {The Annals of Statistics},
  number       = {5},
  pages        = {2755-2773},
  shortjournal = {Ann. Statist.},
  title        = {Prediction bounds for higher order total variation regularized least squares},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Existence and uniqueness of the kronecker covariance MLE.
<em>AOS</em>, <em>49</em>(5), 2721–2754. (<a
href="https://doi.org/10.1214/21-AOS2052">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In matrix-valued datasets the sampled matrices often exhibit correlations among both their rows and their columns. A useful and parsimonious model of such dependence is the matrix normal model, in which the covariances among the elements of a random matrix are parameterized in terms of the Kronecker product of two covariance matrices, one representing row covariances and one representing column covariance. An appealing feature of such a matrix normal model is that the Kronecker covariance structure allows for standard likelihood inference even when only a very small number of data matrices is available. For instance, in some cases a likelihood ratio test of dependence may be performed with a sample size of one. However, more generally the sample size required to ensure boundedness of the matrix normal likelihood or the existence of a unique maximizer depends in a complicated way on the matrix dimensions. This motivates the study of how large a sample size is needed to ensure that maximum likelihood estimators exist, and exist uniquely with probability one. Our main result gives precise sample size thresholds in the paradigm where the number of rows and the number of columns of the data matrices differ by at most a factor of two. Our proof uses invariance properties that allow us to consider data matrices in canonical form, as obtained from the Kronecker canonical form for matrix pencils.},
  archive      = {J_AOS},
  author       = {Mathias Drton and Satoshi Kuriki and Peter Hoff},
  doi          = {10.1214/21-AOS2052},
  journal      = {The Annals of Statistics},
  number       = {5},
  pages        = {2721-2754},
  shortjournal = {Ann. Statist.},
  title        = {Existence and uniqueness of the kronecker covariance MLE},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Inference for a two-stage enrichment design. <em>AOS</em>,
<em>49</em>(5), 2697–2720. (<a
href="https://doi.org/10.1214/21-AOS2051">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Two-stage enrichment designs can be used to target the benefiting population in clinical trials based on patients’ biomarkers. In the case of continuous biomarkers, we show that using a bivariate model that treats biomarkers as random variables more accurately identifies a treatment-benefiting enriched population than assuming biomarkers are fixed. Additionally, we show that under the bivariate model, the maximum likelihood estimators (MLEs) follow a randomly scaled mixture of normal distributions. Using random normings, we obtain asymptotically standard normal MLEs and construct hypothesis tests. Finally, in a simulation study, we demonstrate that our proposed design is more powerful than a single stage design when outcomes and biomarkers are correlated; the model-based estimators have smaller bias and mean square error (MSE) than weighted average estimators.},
  archive      = {J_AOS},
  author       = {Zhantao Lin and Nancy Flournoy and William F. Rosenberger},
  doi          = {10.1214/21-AOS2051},
  journal      = {The Annals of Statistics},
  number       = {5},
  pages        = {2697-2720},
  shortjournal = {Ann. Statist.},
  title        = {Inference for a two-stage enrichment design},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Empirical tail copulas for functional data. <em>AOS</em>,
<em>49</em>(5), 2672–2696. (<a
href="https://doi.org/10.1214/21-AOS2050">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For multivariate distributions in the domain of attraction of a max-stable distribution, the tail copula and the stable tail dependence function are equivalent ways to capture the dependence in the upper tail. The empirical versions of these functions are rank-based estimators whose inflated estimation errors are known to converge weakly to a Gaussian process that is similar in structure to the weak limit of the empirical copula process. We extend this multivariate result to continuous functional data by establishing the asymptotic normality of the estimators of the tail copula, uniformly over all finite subsets of at most D points (D fixed). An application for testing tail copula stationarity is presented. The main tool for deriving the result is the uniform asymptotic normality of all the D-variate tail empirical processes. The proof of the main result is nonstandard.},
  archive      = {J_AOS},
  author       = {John H. J. Einmahl and Johan Segers},
  doi          = {10.1214/21-AOS2050},
  journal      = {The Annals of Statistics},
  number       = {5},
  pages        = {2672-2696},
  shortjournal = {Ann. Statist.},
  title        = {Empirical tail copulas for functional data},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Set structured global empirical risk minimizers are rate
optimal in general dimensions. <em>AOS</em>, <em>49</em>(5), 2642–2671.
(<a href="https://doi.org/10.1214/21-AOS2049">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Entropy integrals are widely used as a powerful empirical process tool to obtain upper bounds for the rates of convergence of global empirical risk minimizers (ERMs), in standard settings such as density estimation and regression. The upper bound for the convergence rates thus obtained typically matches the minimax lower bound when the entropy integral converges, but admits a strict gap compared to the lower bound when it diverges. Birgé and Massart (Probab. Theory Related Fields 97 (1993) 113–150) provided a striking example showing that such a gap is real with the entropy structure alone: for a variant of the natural Hölder class with low regularity, the global ERM actually converges at the rate predicted by the entropy integral that substantially deviates from the lower bound. The counter-example has spawned a long-standing negative position on the use of global ERMs in the regime where the entropy integral diverges, as they are heuristically believed to converge at a suboptimal rate in a variety of models. The present paper demonstrates that this gap can be closed if the models admit certain degree of “set structures” in addition to the entropy structure. In other words, the global ERMs in such set structured models will indeed be rate-optimal, matching the lower bound even when the entropy integral diverges. The models with set structures we investigate include (i) image and edge estimation, (ii) binary classification, (iii) multiple isotonic regression, (iv) s-concave density estimation, all in general dimensions when the entropy integral diverges. Here, set structures are interpreted broadly in the sense that the complexity of the underlying models can be essentially captured by the size of the empirical process over certain class of measurable sets, for which matching upper and lower bounds are obtained to facilitate the derivation of sharp convergence rates for the associated global ERMs.},
  archive      = {J_AOS},
  author       = {Qiyang Han},
  doi          = {10.1214/21-AOS2049},
  journal      = {The Annals of Statistics},
  number       = {5},
  pages        = {2642-2671},
  shortjournal = {Ann. Statist.},
  title        = {Set structured global empirical risk minimizers are rate optimal in general dimensions},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Additive regression for non-euclidean responses and
predictors. <em>AOS</em>, <em>49</em>(5), 2611–2641. (<a
href="https://doi.org/10.1214/21-AOS2048">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Additive regression is studied in a very general setting where both the response and predictors are allowed to be non-Euclidean. The response takes values in a general separable Hilbert space, whereas the predictors take values in general semimetric spaces, which covers a very wide range of nonstandard response variables and predictors. A general framework of estimating additive models is presented for semimetric space-valued predictors. In particular, full details of implementation and the corresponding theory are given for predictors taking values in Hilbert spaces and/or Riemannian manifolds. The existence of the estimators, convergence of a backfitting algorithm, rates of convergence and asymptotic distributions of the estimators are discussed. The finite sample performance of the estimators is investigated by means of two simulation studies. Finally, three data sets covering several types of non-Euclidean data are analyzed to illustrate the usefulness of the proposed general approach.},
  archive      = {J_AOS},
  author       = {Jeong Min Jeon and Byeong U. Park and Ingrid Van Keilegom},
  doi          = {10.1214/21-AOS2048},
  journal      = {The Annals of Statistics},
  number       = {5},
  pages        = {2611-2641},
  shortjournal = {Ann. Statist.},
  title        = {Additive regression for non-euclidean responses and predictors},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Estimation of smooth functionals in normal models: Bias
reduction and asymptotic efficiency. <em>AOS</em>, <em>49</em>(5),
2577–2610. (<a href="https://doi.org/10.1214/20-AOS2047">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Let X1,…,Xn be i.i.d. random variables sampled from a normal distribution N(μ,Σ) in Rd with unknown parameter θ=(μ,Σ)∈Θ:=Rd×C+d, where C+d is the cone of positively definite covariance operators in Rd. Given a smooth functional f:Θ↦R1, the goal is to estimate f(θ) based on X1,…,Xn. Let Θ(a;d):=Rd×{Σ∈C+d:σ(Σ)⊂[1/a,a]},a≥1, where σ(Σ) is the spectrum of covariance Σ. Let θˆ:=(μˆ,Σˆ), where μˆ is the sample mean and Σˆ is the sample covariance, based on the observations X1,…,Xn. For an arbitrary functional f∈Cs(Θ), s=k+1+ρ,k≥0,ρ∈(0,1], we define a functional fk:Θ↦R such that supθ∈Θ(a;d)‖fk(θˆ)−f(θ)‖L2(Pθ)≲s,β‖f‖Cs(Θ)[(a n∨aβs( d n)s)∧1], where β=1 for k=0 and β&gt;s−1 is arbitrary for k≥1. This error rate is minimax optimal and similar bounds hold for more general loss functions. If d=dn≤nα for some α∈(0,1) and s≥11−α, the rate becomes O(n−1/2). Moreover, for s&gt;11−α, the estimator fk(θˆ) is shown to be asymptotically efficient. The crucial part of the construction of estimator fk(θˆ) is a bias reduction method studied in the paper for more general statistical models than normal.},
  archive      = {J_AOS},
  author       = {Vladimir Koltchinskii and Mayya Zhilova},
  doi          = {10.1214/20-AOS2047},
  journal      = {The Annals of Statistics},
  number       = {5},
  pages        = {2577-2610},
  shortjournal = {Ann. Statist.},
  title        = {Estimation of smooth functionals in normal models: Bias reduction and asymptotic efficiency},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Rank-based estimation under asymptotic dependence and
independence, with applications to spatial extremes. <em>AOS</em>,
<em>49</em>(5), 2552–2576. (<a
href="https://doi.org/10.1214/20-AOS2046">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multivariate extreme value theory is concerned with modeling the joint tail behavior of several random variables. Existing work mostly focuses on asymptotic dependence, where the probability of observing a large value in one of the variables is of the same order as observing a large value in all variables simultaneously. However, there is growing evidence that asymptotic independence is equally important in real world applications. Available statistical methodology in the latter setting is scarce and not well understood theoretically. We revisit nonparametric estimation and introduce rank-based M-estimators for parametric models that simultaneously work under asymptotic dependence and asymptotic independence, without requiring prior knowledge on which of the two regimes applies. Asymptotic normality of the proposed estimators is established under weak regularity conditions. We further show how bivariate estimators can be leveraged to obtain parametric estimators in spatial tail models, and again provide a thorough theoretical justification for our approach.},
  archive      = {J_AOS},
  author       = {Michaël Lalancette and Sebastian Engelke and Stanislav Volgushev},
  doi          = {10.1214/20-AOS2046},
  journal      = {The Annals of Statistics},
  number       = {5},
  pages        = {2552-2576},
  shortjournal = {Ann. Statist.},
  title        = {Rank-based estimation under asymptotic dependence and independence, with applications to spatial extremes},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptive estimation of multivariate piecewise polynomials
and bounded variation functions by optimal decision trees. <em>AOS</em>,
<em>49</em>(5), 2531–2551. (<a
href="https://doi.org/10.1214/20-AOS2045">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Proposed by Donoho (Ann. Statist. 25 (1997) 1870–1911), Dyadic CART is a nonparametric regression method which computes a globally optimal dyadic decision tree and fits piecewise constant functions in two dimensions. In this article, we define and study Dyadic CART and a closely related estimator, namely Optimal Regression Tree (ORT), in the context of estimating piecewise smooth functions in general dimensions in the fixed design setup. More precisely, these optimal decision tree estimators fit piecewise polynomials of any given degree. Like Dyadic CART in two dimensions, we reason that these estimators can also be computed in polynomial time in the sample size N via dynamic programming. We prove oracle inequalities for the finite sample risk of Dyadic CART and ORT, which imply tight risk bounds for several function classes of interest. First, they imply that the finite sample risk of ORT of order r≥0 is always bounded by CklogN N whenever the regression function is piecewise polynomial of degree r on some reasonably regular axis aligned rectangular partition of the domain with at most k rectangles. Beyond the univariate case, such guarantees are scarcely available in the literature for computationally efficient estimators. Second, our oracle inequalities uncover minimax rate optimality and adaptivity of the Dyadic CART estimator for function spaces with bounded variation. We consider two function spaces of recent interest where multivariate total variation denoising and univariate trend filtering are the state of the art methods. We show that Dyadic CART enjoys certain advantages over these estimators while still maintaining all their known guarantees.},
  archive      = {J_AOS},
  author       = {Sabyasachi Chatterjee and Subhajit Goswami},
  doi          = {10.1214/20-AOS2045},
  journal      = {The Annals of Statistics},
  number       = {5},
  pages        = {2531-2551},
  shortjournal = {Ann. Statist.},
  title        = {Adaptive estimation of multivariate piecewise polynomials and bounded variation functions by optimal decision trees},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Optimality of spectral clustering in the gaussian mixture
model. <em>AOS</em>, <em>49</em>(5), 2506–2530. (<a
href="https://doi.org/10.1214/20-AOS2044">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spectral clustering is one of the most popular algorithms to group high- dimensional data. It is easy to implement and computationally efficient. Despite its popularity and successful applications, its theoretical properties have not been fully understood. In this paper, we show that spectral clustering is minimax optimal in the Gaussian mixture model with isotropic covariance matrix, when the number of clusters is fixed and the signal-to-noise ratio is large enough. Spectral gap conditions are widely assumed in the literature to analyze spectral clustering. On the contrary, these conditions are not needed to establish optimality of spectral clustering in this paper.},
  archive      = {J_AOS},
  author       = {Matthias Löffler and Anderson Y. Zhang and Harrison H. Zhou},
  doi          = {10.1214/20-AOS2044},
  journal      = {The Annals of Statistics},
  number       = {5},
  pages        = {2506-2530},
  shortjournal = {Ann. Statist.},
  title        = {Optimality of spectral clustering in the gaussian mixture model},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Variable selection consistency of gaussian process
regression. <em>AOS</em>, <em>49</em>(5), 2491–2505. (<a
href="https://doi.org/10.1214/20-AOS2043">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian nonparametric regression under a rescaled Gaussian process prior offers smoothness-adaptive function estimation with near minimax-optimal error rates. Hierarchical extensions of this approach, equipped with stochastic variable selection, are known to also adapt to the unknown intrinsic dimension of a sparse true regression function. But it remains unclear if such extensions offer variable selection consistency, that is, if the true subset of important variables could be consistently learned from the data. It is shown here that variable consistency may indeed be achieved with such models at least when the true regression function has finite smoothness to induce a polynomially larger penalty on inclusion of false positive predictors. Our result covers the high-dimensional asymptotic setting where the predictor dimension is allowed to grow with the sample size. The proof utilizes Schwartz theory to establish that the posterior probability of wrong selection vanishes asymptotically. A necessary and challenging technical development involves providing sharp upper and lower bounds to small ball probabilities at all rescaling levels of the Gaussian process prior, a result that could be of independent interest.},
  archive      = {J_AOS},
  author       = {Sheng Jiang and Surya T. Tokdar},
  doi          = {10.1214/20-AOS2043},
  journal      = {The Annals of Statistics},
  number       = {5},
  pages        = {2491-2505},
  shortjournal = {Ann. Statist.},
  title        = {Variable selection consistency of gaussian process regression},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Optimal rates for independence testing via u-statistic
permutation tests. <em>AOS</em>, <em>49</em>(5), 2457–2490. (<a
href="https://doi.org/10.1214/20-AOS2041">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the problem of independence testing given independent and identically distributed pairs taking values in a σ-finite, separable measure space. Defining a natural measure of dependence D(f) as the squared L2-distance between a joint density f and the product of its marginals, we first show that there is no valid test of independence that is uniformly consistent against alternatives of the form {f:D(f)≥ρ2}. We therefore restrict attention to alternatives that impose additional Sobolev-type smoothness constraints, and define a permutation test based on a basis expansion and a U-statistic estimator of D(f) that we prove is minimax optimal in terms of its separation rates in many instances. Finally, for the case of a Fourier basis on [0,1]2, we provide an approximation to the power function that offers several additional insights. Our methodology is implemented in the R package USP.},
  archive      = {J_AOS},
  author       = {Thomas B. Berrett and Ioannis Kontoyiannis and Richard J. Samworth},
  doi          = {10.1214/20-AOS2041},
  journal      = {The Annals of Statistics},
  number       = {5},
  pages        = {2457-2490},
  shortjournal = {Ann. Statist.},
  title        = {Optimal rates for independence testing via U-statistic permutation tests},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Willem van zwet, teacher and thesis advisor. <em>AOS</em>,
<em>49</em>(5), 2448–2456. (<a
href="https://doi.org/10.1214/21-AOS2069">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Willem van Zwet was supervisor of sixteen PhD students. All of them pursued academic careers and most of them became full professor. Below are some stories of PhD students Wim Albers, Cees Diks, Ronald Does, Marta Fiocco, Sara van de Geer, Mathisca de Gunst, Chris Klaassen, Hein Putter, Aad van der Vaart, Marten Wegkamp and Martien van Zuijlen with in addition a contribution by Nelly Litvak who was guided by Willem after her PhD.},
  archive      = {J_AOS},
  author       = {Sara van de Geer and Chris A. J. Klaassen},
  doi          = {10.1214/21-AOS2069},
  journal      = {The Annals of Statistics},
  number       = {5},
  pages        = {2448-2456},
  shortjournal = {Ann. Statist.},
  title        = {Willem van zwet, teacher and thesis advisor},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Willem van zwet’s research. <em>AOS</em>, <em>49</em>(5),
2439–2447. (<a href="https://doi.org/10.1214/21-AOS2060">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Willem van Zwet made deep and influential contributions to probability and statistics, which we review in this paper. Bickel and Götze collaborated with him on his major contributions to higher order asymptotics of nonlinear statistics and on resampling and the bootstrap. We relate this work to his remarkable development of the properties of the Hoeffding expansion for symmetric statistics as well as Fourier analytic tools. Fiocco and De Gunst were his students. We describe how in their theses and subsequent papers with him they developed statistical inference in two subtle stochastic models, the contact process and cell population development under a plausible regime. We also touch on his solutions of intriguing problems not related directly to his main interests.},
  archive      = {J_AOS},
  author       = {Peter Bickel and Marta Fiocco and Mathisca de Gunst and Friedrich Götze},
  doi          = {10.1214/21-AOS2060},
  journal      = {The Annals of Statistics},
  number       = {5},
  pages        = {2439-2447},
  shortjournal = {Ann. Statist.},
  title        = {Willem van zwet’s research},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Willem van zwet’s contributions to the profession.
<em>AOS</em>, <em>49</em>(5), 2432–2438. (<a
href="https://doi.org/10.1214/21-AOS2053">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bill van Zwet’s contributions to his profession are, in their own way, comparable to his very substantial research, research supervision and teaching contributions. He had several leadership roles in professional societies, edited leading theoretical journals, started up or revived important conference series, founded a research center and played a huge role in providing colleagues in Central and Eastern Europe access to the western world of probability and statistics. And to each of these several activities, he brought farsightedness, energy and wit, qualities that shone through when he was interviewed about what he’d done (Statist. Sci. 24 (2009) 87–115). In fact, because Bill provided a wealth of detail in “the interview,” this article focuses more on the recollections of some of the people with whom he interacted.},
  archive      = {J_AOS},
  author       = {Nicholas I. Fisher and Adrian F. M. Smith},
  doi          = {10.1214/21-AOS2053},
  journal      = {The Annals of Statistics},
  number       = {5},
  pages        = {2432-2438},
  shortjournal = {Ann. Statist.},
  title        = {Willem van zwet’s contributions to the profession},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Preface: Section of memorial articles for willem van zwet.
<em>AOS</em>, <em>49</em>(5), 2431. (<a
href="https://doi.org/10.1214/21-AOS2112">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_AOS},
  author       = {Richard J. Samworth and Ming Yuan},
  doi          = {10.1214/21-AOS2112},
  journal      = {The Annals of Statistics},
  number       = {5},
  pages        = {2431},
  shortjournal = {Ann. Statist.},
  title        = {Preface: Section of memorial articles for willem van zwet},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Correction note: “Statistical inference for the mean outcome
under a possibly nonunique optimal treatment rule.” <em>AOS</em>,
<em>49</em>(4), 2429–2430. (<a
href="https://doi.org/10.1214/20-AOS2031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_AOS},
  author       = {Alex Luedtke and Aurélien Bibaut and Mark van der Laan},
  doi          = {10.1214/20-AOS2031},
  journal      = {The Annals of Statistics},
  number       = {4},
  pages        = {2429-2430},
  shortjournal = {Ann. Statist.},
  title        = {Correction note: “Statistical inference for the mean outcome under a possibly nonunique optimal treatment rule”},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Community detection with dependent connectivity.
<em>AOS</em>, <em>49</em>(4), 2378–2428. (<a
href="https://doi.org/10.1214/20-AOS2042">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In network analysis, within-community members are more likely to be connected than between-community members, which is reflected in that the edges within a community are intercorrelated. However, existing probabilistic models for community detection such as the stochastic block model (SBM) are not designed to capture the dependence among edges. In this paper, we propose a new community detection approach to incorporate intra-community dependence of connectivities through the Bahadur representation. The proposed method does not require specifying the likelihood function, which could be intractable for correlated binary connectivities. In addition, the proposed method allows for heterogeneity among edges between different communities. In theory, we show that incorporating correlation information can achieve a faster convergence rate compared to the independent SBM, and the proposed algorithm has a lower estimation bias and accelerated convergence compared to the variational EM. Our simulation studies show that the proposed algorithm outperforms the existing multinetwork community detection methods assuming conditional independence among edges. We also demonstrate the application of the proposed method to agricultural product trading networks from different countries and to brain fMRI imaging networks.},
  archive      = {J_AOS},
  author       = {Yubai Yuan and Annie Qu},
  doi          = {10.1214/20-AOS2042},
  journal      = {The Annals of Statistics},
  number       = {4},
  pages        = {2378-2428},
  shortjournal = {Ann. Statist.},
  title        = {Community detection with dependent connectivity},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Propriety of the reference posterior distribution in
gaussian process modeling. <em>AOS</em>, <em>49</em>(4), 2356–2377. (<a
href="https://doi.org/10.1214/20-AOS2040">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a seminal article, Berger, De Oliveira and Sansó [J. Amer. Statist. Assoc. 96 (2001) 1361–1374] compare several objective prior distributions for the parameters of Gaussian process models with isotropic correlation kernel. The reference prior distribution stands out among them insofar as it always leads to a proper posterior. They prove this result for rough correlation kernels: Spherical, Exponential with power ρ&lt;2, Matérn with smoothness ν&lt;1. This paper provides a proof for smooth correlation kernels: Exponential with power ρ=2, Matérn with smoothness ν≥1, Rational Quadratic, along with tail rates of the reference prior for these kernels.},
  archive      = {J_AOS},
  author       = {Joseph Muré},
  doi          = {10.1214/20-AOS2040},
  journal      = {The Annals of Statistics},
  number       = {4},
  pages        = {2356-2377},
  shortjournal = {Ann. Statist.},
  title        = {Propriety of the reference posterior distribution in gaussian process modeling},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Optimal change-point estimation in time series.
<em>AOS</em>, <em>49</em>(4), 2336–2355. (<a
href="https://doi.org/10.1214/20-AOS2039">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper establishes asymptotic theory for optimal estimation of change points in general time series models under α-mixing conditions. We show that the Bayes-type estimator is asymptotically minimax for change-point estimation under squared error loss. Two bootstrap procedures are developed to construct confidence intervals for the change points. An approximate limiting distribution of the change-point estimator under small change is also derived. Simulations and real data applications are presented to investigate the finite sample performance of the Bayes-type estimator and the bootstrap procedures.},
  archive      = {J_AOS},
  author       = {Ngai Hang Chan and Wai Leong Ng and Chun Yip Yau and Haihan Yu},
  doi          = {10.1214/20-AOS2039},
  journal      = {The Annals of Statistics},
  number       = {4},
  pages        = {2336-2355},
  shortjournal = {Ann. Statist.},
  title        = {Optimal change-point estimation in time series},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The distribution of the lasso: Uniform control over sparse
balls and adaptive parameter tuning. <em>AOS</em>, <em>49</em>(4),
2313–2335. (<a href="https://doi.org/10.1214/20-AOS2038">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Lasso is a popular regression method for high-dimensional problems in which the number of parameters θ1,…,θN, is larger than the number n of samples: N&gt;n. A useful heuristics relates the statistical properties of the Lasso estimator to that of a simple soft-thresholding denoiser, in a denoising problem in which the parameters (θi)i≤N are observed in Gaussian noise, with a carefully tuned variance. Earlier work confirmed this picture in the limit n,N→∞, pointwise in the parameters θ and in the value of the regularization parameter. Here, we consider a standard random design model and prove exponential concentration of its empirical distribution around the prediction provided by the Gaussian denoising model. Crucially, our results are uniform with respect to θ belonging to ℓq balls, q∈[0,1], and with respect to the regularization parameter. This allows us to derive sharp results for the performances of various data-driven procedures to tune the regularization. Our proofs make use of Gaussian comparison inequalities, and in particular of a version of Gordon’s minimax theorem developed by Thrampoulidis, Oymak and Hassibi, which controls the optimum value of the Lasso optimization problem. Crucially, we prove a stability property of the minimizer in Wasserstein distance that allows one to characterize properties of the minimizer itself.},
  archive      = {J_AOS},
  author       = {Léo Miolane and Andrea Montanari},
  doi          = {10.1214/20-AOS2038},
  journal      = {The Annals of Statistics},
  number       = {4},
  pages        = {2313-2335},
  shortjournal = {Ann. Statist.},
  title        = {The distribution of the lasso: Uniform control over sparse balls and adaptive parameter tuning},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). What is resolution? A statistical minimax testing
perspective on superresolution microscopy. <em>AOS</em>, <em>49</em>(4),
2292–2312. (<a href="https://doi.org/10.1214/20-AOS2037">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a general rule of thumb the resolution of a light microscope (i.e., the ability to discern objects) is predominantly described by the full width at half maximum (FWHM) of its point spread function (psf)—the diameter of the blurring density at half of its maximum. Classical wave optics suggests a linear relationship between FWHM and resolution also manifested in the well-known Abbe and Rayleigh criteria, dating back to the end of the 19th century. However, during the last two decades conventional light microscopy has undergone a shift from microscopic scales to nanoscales. This increase in resolution comes with the need to incorporate the random nature of observations (light photons) and challenges the classical view of discernability, as we argue in this paper. Instead, we suggest a statistical description of resolution obtained from such random data. Our notion of discernability is based on statistical testing whether one or two objects with the same total intensity are present. For Poisson measurements, we get linear dependence of the (minimax) detection boundary on the FWHM, whereas for a homogeneous Gaussian model the dependence of resolution is nonlinear. Hence, at small physical scales modeling by homogeneous gaussians is inadequate, although often implicitly assumed in many reconstruction algorithms. In contrast, the Poisson model and its variance stabilized Gaussian approximation seem to provide a statistically sound description of resolution at the nanoscale. Our theory is also applicable to other imaging setups, such as telescopes.},
  archive      = {J_AOS},
  author       = {Gytis Kulaitis and Axel Munk and Frank Werner},
  doi          = {10.1214/20-AOS2037},
  journal      = {The Annals of Statistics},
  number       = {4},
  pages        = {2292-2312},
  shortjournal = {Ann. Statist.},
  title        = {What is resolution? a statistical minimax testing perspective on superresolution microscopy},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Monitoring for a change point in a sequence of
distributions. <em>AOS</em>, <em>49</em>(4), 2271–2291. (<a
href="https://doi.org/10.1214/20-AOS2036">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a method for the detection of a change point in a sequence {Fi} of distributions, which are available through a large number of observations at each i≥1. Under the null hypothesis, the distributions Fi are equal. Under the alternative hypothesis, there is a change point i∗&gt;1, such that Fi=G for i≥i∗ and some unknown distribution G, which is not equal to F1. The change point, if it exists, is unknown, and the distributions before and after the potential change point are unknown. The decision about the existence of a change point is made sequentially, as new data arrive. At each time i, the count of observations, N, can increase to infinity. The detection procedure is based on a weighted version of the Wasserstein distance. Its asymptotic and finite sample validity is established. Its performance is illustrated by an application to returns on stocks in the S&amp;P 500 index.},
  archive      = {J_AOS},
  author       = {Lajos Horváth and Piotr Kokoszka and Shixuan Wang},
  doi          = {10.1214/20-AOS2036},
  journal      = {The Annals of Statistics},
  number       = {4},
  pages        = {2271-2291},
  shortjournal = {Ann. Statist.},
  title        = {Monitoring for a change point in a sequence of distributions},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Infinite-dimensional gradient-based descent for
alpha-divergence minimisation. <em>AOS</em>, <em>49</em>(4), 2250–2270.
(<a href="https://doi.org/10.1214/20-AOS2035">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces the (α,Γ)-descent, an iterative algorithm which operates on measures and performs α-divergence minimisation in a Bayesian framework. This gradient-based procedure extends the commonly-used variational approximation by adding a prior on the variational parameters in the form of a measure. We prove that for a rich family of functions Γ, this algorithm leads at each step to a systematic decrease in the α-divergence and derive convergence results. Our framework recovers the Entropic Mirror Descent algorithm and provides an alternative algorithm that we call the Power Descent. Moreover, in its stochastic formulation, the (α,Γ)-descent allows to optimise the mixture weights of any given mixture model without any information on the underlying distribution of the variational parameters. This renders our method compatible with many choices of parameters updates and applicable to a wide range of Machine Learning tasks. We demonstrate empirically on both toy and real-world examples the benefit of using the Power Descent and going beyond the Entropic Mirror Descent framework, which fails as the dimension grows.},
  archive      = {J_AOS},
  author       = {Kamélia Daudel and Randal Douc and François Portier},
  doi          = {10.1214/20-AOS2035},
  journal      = {The Annals of Statistics},
  number       = {4},
  pages        = {2250-2270},
  shortjournal = {Ann. Statist.},
  title        = {Infinite-dimensional gradient-based descent for alpha-divergence minimisation},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On the rate of convergence of fully connected deep neural
network regression estimates. <em>AOS</em>, <em>49</em>(4), 2231–2249.
(<a href="https://doi.org/10.1214/20-AOS2034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent results in nonparametric regression show that deep learning, that is, neural network estimates with many hidden layers, are able to circumvent the so-called curse of dimensionality in case that suitable restrictions on the structure of the regression function hold. One key feature of the neural networks used in these results is that their network architecture has a further constraint, namely the network sparsity. In this paper, we show that we can get similar results also for least squares estimates based on simple fully connected neural networks with ReLU activation functions. Here, either the number of neurons per hidden layer is fixed and the number of hidden layers tends to infinity suitably fast for sample size tending to infinity, or the number of hidden layers is bounded by some logarithmic factor in the sample size and the number of neurons per hidden layer tends to infinity suitably fast for sample size tending to infinity. The proof is based on new approximation results concerning deep neural networks.},
  archive      = {J_AOS},
  author       = {Michael Kohler and Sophie Langer},
  doi          = {10.1214/20-AOS2034},
  journal      = {The Annals of Statistics},
  number       = {4},
  pages        = {2231-2249},
  shortjournal = {Ann. Statist.},
  title        = {On the rate of convergence of fully connected deep neural network regression estimates},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust k-means clustering for distributions with two
moments. <em>AOS</em>, <em>49</em>(4), 2206–2230. (<a
href="https://doi.org/10.1214/20-AOS2033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the robust algorithms for the k-means clustering problem where a quantizer is constructed based on N independent observations. Our main results are median of means based nonasymptotic excess distortion bounds that hold under the two bounded moments assumption in a general separable Hilbert space. In particular, our results extend the renowned asymptotic result of (Ann. Statist. 9 (1981) 135–140) who showed that the existence of two moments is sufficient for strong consistency of an empirically optimal quantizer in Rd. In a special case of clustering in Rd, under two bounded moments, we prove matching (up to constant factors) nonasymptotic upper and lower bounds on the excess distortion, which depend on the probability mass of the lightest cluster of an optimal quantizer. Our bounds have the sub-Gaussian form, and the proofs are based on the versions of uniform bounds for robust mean estimators.},
  archive      = {J_AOS},
  author       = {Yegor Klochkov and Alexey Kroshnin and Nikita Zhivotovskiy},
  doi          = {10.1214/20-AOS2033},
  journal      = {The Annals of Statistics},
  number       = {4},
  pages        = {2206-2230},
  shortjournal = {Ann. Statist.},
  title        = {Robust k-means clustering for distributions with two moments},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Estimation of the number of components of nonparametric
multivariate finite mixture models. <em>AOS</em>, <em>49</em>(4),
2178–2205. (<a href="https://doi.org/10.1214/20-AOS2032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel estimator for the number of mixture components (denoted by M) in a nonparametric finite mixture model. The setting that we consider is one where the analyst has repeated observations of K≥2 variables that are conditionally independent given a finitely supported latent variable with M support points. Under a mild assumption on the joint distribution of the observed and latent variables, we show that an integral operator T that is identified from the data has rank equal to M. We use this observation, in conjunction with the fact that singular values of operators are stable under perturbations, to propose an estimator of M, which essentially consists of a thresholding rule that counts the number of singular values of a consistent estimator of T that are greater than a data-driven threshold. We prove that our estimator of M is consistent, and establish nonasymptotic results, which provide finite sample performance guarantees for our estimator. We present a Monte Carlo study, which shows that our estimator performs well for samples of moderate size.},
  archive      = {J_AOS},
  author       = {Caleb Kwon and Eric Mbakop},
  doi          = {10.1214/20-AOS2032},
  journal      = {The Annals of Statistics},
  number       = {4},
  pages        = {2178-2205},
  shortjournal = {Ann. Statist.},
  title        = {Estimation of the number of components of nonparametric multivariate finite mixture models},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Minimax optimal conditional independence testing.
<em>AOS</em>, <em>49</em>(4), 2151–2177. (<a
href="https://doi.org/10.1214/20-AOS2030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of conditional independence testing of X and Y given Z where X,Y and Z are three real random variables and Z is continuous. We focus on two main cases—when X and Y are both discrete, and when X and Y are both continuous. In view of recent results on conditional independence testing [Ann. Statist. 48 (2020) 1514–1538], one cannot hope to design nontrivial tests, which control the type I error for all absolutely continuous conditionally independent distributions, while still ensuring power against interesting alternatives. Consequently, we identify various, natural smoothness assumptions on the conditional distributions of X,Y|Z=z as z varies in the support of Z, and study the hardness of conditional independence testing under these smoothness assumptions. We derive matching lower and upper bounds on the critical radius of separation between the null and alternative hypotheses in the total variation metric. The tests we consider are easily implementable and rely on binning the support of the continuous variable Z. To complement these results, we provide a new proof of the hardness result of Shah and Peters [Ann. Statist. 48 (2020) 1514–1538].},
  archive      = {J_AOS},
  author       = {Matey Neykov and Sivaraman Balakrishnan and Larry Wasserman},
  doi          = {10.1214/20-AOS2030},
  journal      = {The Annals of Statistics},
  number       = {4},
  pages        = {2151-2177},
  shortjournal = {Ann. Statist.},
  title        = {Minimax optimal conditional independence testing},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Universal bayes consistency in metric spaces. <em>AOS</em>,
<em>49</em>(4), 2129–2150. (<a
href="https://doi.org/10.1214/20-AOS2029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We extend a recently proposed 1-nearest-neighbor based multiclass learning algorithm and prove that our modification is universally strongly Bayes consistent in all metric spaces admitting any such learner, making it an “optimistically universal” Bayes-consistent learner. This is the first learning algorithm known to enjoy this property; by comparison, the k-NN classifier and its variants are not generally universally Bayes consistent, except under additional structural assumptions, such as an inner product, a norm, finite dimension or a Besicovitch-type property. The metric spaces in which universal Bayes consistency is possible are the “essentially separable” ones—a notion that we define, which is more general than standard separability. The existence of metric spaces that are not essentially separable is widely believed to be independent of the ZFC axioms of set theory. We prove that essential separability exactly characterizes the existence of a universal Bayes-consistent learner for the given metric space. In particular, this yields the first impossibility result for universal Bayes consistency. Taken together, our results completely characterize strong and weak universal Bayes consistency in metric spaces.},
  archive      = {J_AOS},
  author       = {Steve Hanneke and Aryeh Kontorovich and Sivan Sabato and Roi Weiss},
  doi          = {10.1214/20-AOS2029},
  journal      = {The Annals of Statistics},
  number       = {4},
  pages        = {2129-2150},
  shortjournal = {Ann. Statist.},
  title        = {Universal bayes consistency in metric spaces},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Boosted nonparametric hazards with time-dependent
covariates. <em>AOS</em>, <em>49</em>(4), 2101–2128. (<a
href="https://doi.org/10.1214/20-AOS2028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given functional data from a survival process with time-dependent covariates, we derive a smooth convex representation for its nonparametric log-likelihood functional and obtain its functional gradient. From this, we devise a generic gradient boosting procedure for estimating the hazard function nonparametrically. An illustrative implementation of the procedure using regression trees is described to show how to recover the unknown hazard. The generic estimator is consistent if the model is correctly specified; alternatively, an oracle inequality can be demonstrated for tree-based models. To avoid overfitting, boosting employs several regularization devices. One of them is stepsize restriction, but the rationale for this is somewhat mysterious from the viewpoint of consistency. Our work brings some clarity to this issue by revealing that stepsize restriction is a mechanism for preventing the curvature of the risk from derailing convergence.},
  archive      = {J_AOS},
  author       = {Donald K. K. Lee and Ningyuan Chen and Hemant Ishwaran},
  doi          = {10.1214/20-AOS2028},
  journal      = {The Annals of Statistics},
  number       = {4},
  pages        = {2101-2128},
  shortjournal = {Ann. Statist.},
  title        = {Boosted nonparametric hazards with time-dependent covariates},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Debiased inverse-variance weighted estimator in two-sample
summary-data mendelian randomization. <em>AOS</em>, <em>49</em>(4),
2079–2100. (<a href="https://doi.org/10.1214/20-AOS2027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mendelian randomization (MR) has become a popular approach to study the effect of a modifiable exposure on an outcome by using genetic variants as instrumental variables. A challenge in MR is that each genetic variant explains a relatively small proportion of variance in the exposure and there are many such variants, a setting known as many weak instruments. To this end, we provide a theoretical characterization of the statistical properties of two popular estimators in MR: the inverse-variance weighted (IVW) estimator and the IVW estimator with screened instruments using an independent selection dataset, under many weak instruments. We then propose a debiased IVW estimator, a simple modification of the IVW estimator, that is robust to many weak instruments and does not require screening. Additionally, we present two instrument selection methods to improve the efficiency of the new estimator when a selection dataset is available. An extension of the debiased IVW estimator to handle balanced horizontal pleiotropy is also discussed. We conclude by demonstrating our results in simulated and real datasets.},
  archive      = {J_AOS},
  author       = {Ting Ye and Jun Shao and Hyunseung Kang},
  doi          = {10.1214/20-AOS2027},
  journal      = {The Annals of Statistics},
  number       = {4},
  pages        = {2079-2100},
  shortjournal = {Ann. Statist.},
  title        = {Debiased inverse-variance weighted estimator in two-sample summary-data mendelian randomization},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On extended admissible procedures and their nonstandard
bayes risk. <em>AOS</em>, <em>49</em>(4), 2053–2078. (<a
href="https://doi.org/10.1214/20-AOS2026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For finite parameter spaces, among decision procedures with finite risk functions, a decision procedure is extended admissible if and only if it is Bayes. Various relaxations of this classical equivalence have been established for infinite parameter spaces, but these extensions are each subject to technical conditions that limit their applicability, especially to modern (semi and nonparametric) statistical problems. Using results in mathematical logic and nonstandard analysis, we extend this equivalence to arbitrary statistical decision problems: informally, we show that, among decision procedures with finite risk functions, a decision procedure is extended admissible if and only if it has infinitesimal excess Bayes risk. In contrast to existing results, our equivalence holds in complete generality, that is, without regularity conditions or restrictions on the model or loss function. We also derive a nonstandard analogue of Blyth’s method that yields sufficient conditions for admissibility, and apply the nonstandard theory to derive a purely standard theorem: when risk functions are continuous on a compact Hausdorff parameter space, a procedure is extended admissible if and only if it is Bayes.},
  archive      = {J_AOS},
  author       = {Haosui Duanmu and Daniel M. Roy},
  doi          = {10.1214/20-AOS2026},
  journal      = {The Annals of Statistics},
  number       = {4},
  pages        = {2053-2078},
  shortjournal = {Ann. Statist.},
  title        = {On extended admissible procedures and their nonstandard bayes risk},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Confidence intervals for multiple isotonic regression and
other monotone models. <em>AOS</em>, <em>49</em>(4), 2021–2052. (<a
href="https://doi.org/10.1214/20-AOS2025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of constructing pointwise confidence intervals in the multiple isotonic regression model. Recently, Han and Zhang (2020) obtained a pointwise limit distribution theory for the so-called block max–min and min–max estimators (Fokianos, Leucht and Neumann (2020); Deng and Zhang (2020)) in this model, but inference remains a difficult problem due to the nuisance parameter in the limit distribution that involves multiple unknown partial derivatives of the true regression function. In this paper, we show that this difficult nuisance parameter can be effectively eliminated by taking advantage of information beyond point estimates in the block max–min and min–max estimators. Formally, let uˆ(x0) (resp. vˆ(x0)) be the maximizing lower-left (resp. minimizing upper-right) vertex in the block max–min (resp. min–max) estimator, and fˆn be the average of the block max–min and min–max estimators. If all (first-order) partial derivatives of f0 are nonvanishing at x0, then the following pivotal limit distribution theory holds: nuˆ,vˆ(x0)(fˆn(x0)−f0(x0))⇝σ·L1d. Here nuˆ,vˆ(x0) is the number of design points in the block [uˆ(x0),vˆ(x0)], σ is the standard deviation of the errors, and L1d is a universal limit distribution free of nuisance parameters. This immediately yields confidence intervals for f0(x0) with asymptotically exact confidence level and oracle length. Notably, the construction of the confidence intervals, even new in the univariate setting, requires no more efforts than performing an isotonic regression once using the block max–min and min–max estimators, and can be easily adapted to other common monotone models including, for example, (i) monotone density estimation, (ii) interval censoring model with current status data, (iii) counting process model with panel count data, and (iv) generalized linear models. Extensive simulations are carried out to support our theory.},
  archive      = {J_AOS},
  author       = {Hang Deng and Qiyang Han and Cun-Hui Zhang},
  doi          = {10.1214/20-AOS2025},
  journal      = {The Annals of Statistics},
  number       = {4},
  pages        = {2021-2052},
  shortjournal = {Ann. Statist.},
  title        = {Confidence intervals for multiple isotonic regression and other monotone models},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Asymptotic distributions of high-dimensional distance
correlation inference. <em>AOS</em>, <em>49</em>(4), 1999–2020. (<a
href="https://doi.org/10.1214/20-AOS2024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distance correlation has become an increasingly popular tool for detecting the nonlinear dependence between a pair of potentially high-dimensional random vectors. Most existing works have explored its asymptotic distributions under the null hypothesis of independence between the two random vectors when only the sample size or the dimensionality diverges. Yet its asymptotic null distribution for the more realistic setting when both sample size and dimensionality diverge in the full range remains largely underdeveloped. In this paper, we fill such a gap and develop central limit theorems and associated rates of convergence for a rescaled test statistic based on the bias-corrected distance correlation in high dimensions under some mild regularity conditions and the null hypothesis. Our new theoretical results reveal an interesting phenomenon of blessing of dimensionality for high-dimensional distance correlation inference in the sense that the accuracy of normal approximation can increase with dimensionality. Moreover, we provide a general theory on the power analysis under the alternative hypothesis of dependence, and further justify the capability of the rescaled distance correlation in capturing the pure nonlinear dependency under moderately high dimensionality for a certain type of alternative hypothesis. The theoretical results and finite-sample performance of the rescaled statistic are illustrated with several simulation examples and a blockchain application.},
  archive      = {J_AOS},
  author       = {Lan Gao and Yingying Fan and Jinchi Lv and Qi-Man Shao},
  doi          = {10.1214/20-AOS2024},
  journal      = {The Annals of Statistics},
  number       = {4},
  pages        = {1999-2020},
  shortjournal = {Ann. Statist.},
  title        = {Asymptotic distributions of high-dimensional distance correlation inference},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Volatility coupling. <em>AOS</em>, <em>49</em>(4),
1982–1998. (<a href="https://doi.org/10.1214/20-AOS2023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper provides a strong approximation, or coupling, theory for spot volatility estimators formed using high-frequency data. We show that the t-statistic process associated with the nonparametric spot volatility estimator can be strongly approximated by a growing-dimensional vector of independent variables defined as functions of Brownian increments. We use this coupling theory to study the uniform inference for the volatility process in an infill asymptotic setting. Specifically, we propose uniform confidence bands for spot volatility, beta, idiosyncratic variance processes, and their nonlinear transforms. The theory is also applied to address an open question concerning the inference of monotone nonsmooth integrated volatility functionals such as the occupation time and its quantiles.},
  archive      = {J_AOS},
  author       = {Jean Jacod and Jia Li and Zhipeng Liao},
  doi          = {10.1214/20-AOS2023},
  journal      = {The Annals of Statistics},
  number       = {4},
  pages        = {1982-1998},
  shortjournal = {Ann. Statist.},
  title        = {Volatility coupling},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Peskun–tierney ordering for markovian monte carlo: Beyond
the reversible scenario. <em>AOS</em>, <em>49</em>(4), 1958–1981. (<a
href="https://doi.org/10.1214/20-AOS2008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Historically time-reversibility of the transitions or processes underpinning Markov chain Monte Carlo methods (MCMC) has played a key role in their development, while the self-adjointness of associated operators together with the use of classical functional analysis techniques on Hilbert spaces have led to powerful and practically successful tools to characterise and compare their performance. Similar results for algorithms relying on nonreversible Markov processes are scarce. We show that for a type of nonreversible Monte Carlo Markov chains and processes, of current or renewed interest in the physics and statistical literatures, it is possible to develop comparison results which closely mirror those available in the reversible scenario. We show that these results shed light on earlier literature, proving some conjectures and strengthening some earlier results.},
  archive      = {J_AOS},
  author       = {Christophe Andrieu and Samuel Livingstone},
  doi          = {10.1214/20-AOS2008},
  journal      = {The Annals of Statistics},
  number       = {4},
  pages        = {1958-1981},
  shortjournal = {Ann. Statist.},
  title        = {Peskun–Tierney ordering for markovian monte carlo: Beyond the reversible scenario},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Nonexchangeable random partition models for microclustering.
<em>AOS</em>, <em>49</em>(4), 1931–1957. (<a
href="https://doi.org/10.1214/20-AOS2003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many popular random partition models, such as the Chinese restaurant process and its two-parameter extension, fall in the class of exchangeable random partitions, and have found wide applicability in various fields. While the exchangeability assumption is sensible in many cases, it implies that the size of the clusters necessarily grows linearly with the sample size, and such feature may be undesirable for some applications. We present here a flexible class of nonexchangeable random partition models, which are able to generate partitions whose cluster sizes grow sublinearly with the sample size, and where the growth rate is controlled by one parameter. Along with this result, we provide the asymptotic behaviour of the number of clusters of a given size, and show that the model can exhibit a power-law behaviour, controlled by another parameter. The construction is based on completely random measures and a Poisson embedding of the random partition, and inference is performed using a Sequential Monte Carlo algorithm. Experiments on real data sets emphasise the usefulness of the approach compared to a two-parameter Chinese restaurant process.},
  archive      = {J_AOS},
  author       = {Giuseppe Di Benedetto and François Caron and Yee Whye Teh},
  doi          = {10.1214/20-AOS2003},
  journal      = {The Annals of Statistics},
  number       = {4},
  pages        = {1931-1957},
  shortjournal = {Ann. Statist.},
  title        = {Nonexchangeable random partition models for microclustering},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Consistent nonparametric estimation for heavy-tailed sparse
graphs. <em>AOS</em>, <em>49</em>(4), 1904–1930. (<a
href="https://doi.org/10.1214/20-AOS1985">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study graphons as a nonparametric generalization of stochastic block models, and show how to obtain compactly represented estimators for sparse networks in this framework. In contrast to previous work, we relax the usual boundedness assumption for the generating graphon and instead assume only integrability, so that we can handle networks that have long tails in their degree distributions. We also relax the usual assumption that the graphon is defined on the unit interval, to allow latent position graphs based on more general spaces. We analyze three algorithms. The first is a least squares algorithm, which gives a consistent estimator for all square-integrable graphons, with errors expressed in terms of the best possible stochastic block model approximation. Next, we analyze an algorithm based on the cut norm, which works for all integrable graphons. Finally, we show that clustering based on degrees works whenever the underlying degree distribution is atomless.},
  archive      = {J_AOS},
  author       = {Christian Borgs and Jennifer T. Chayes and Henry Cohn and Shirshendu Ganguly},
  doi          = {10.1214/20-AOS1985},
  journal      = {The Annals of Statistics},
  number       = {4},
  pages        = {1904-1930},
  shortjournal = {Ann. Statist.},
  title        = {Consistent nonparametric estimation for heavy-tailed sparse graphs},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Second-order stein: SURE for SURE and other applications in
high-dimensional inference. <em>AOS</em>, <em>49</em>(4), 1864–1903. (<a
href="https://doi.org/10.1214/20-AOS2005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stein’s formula states that a random variable of the form z⊤f(z)−divf(z) is mean-zero for all functions f with integrable gradient. Here, divf is the divergence of the function f and z is a standard normal vector. This paper aims to propose a second-order Stein formula to characterize the variance of such random variables for all functions f(z) with square integrable gradient, and to demonstrate the usefulness of this second-order Stein formula in various applications. In the Gaussian sequence model, a remarkable consequence of Stein’s formula is Stein’s Unbiased Risk Estimate (SURE), an unbiased estimate of the mean squared risk for almost any given estimator μˆ of the unknown mean vector. A first application of the second-order Stein formula is an Unbiased Risk Estimate for SURE itself (SURE for SURE): an unbiased estimate providing information about the squared distance between SURE and the squared estimation error of μˆ. SURE for SURE has a simple form as a function of the data and is applicable to all μˆ with square integrable gradient, for example, the Lasso and the Elastic Net. In addition to SURE for SURE, the following statistical applications are developed: (1) upper bounds on the risk of SURE when the estimation target is the mean squared error; (2) confidence regions based on SURE and using the second-order Stein formula; (3) oracle inequalities satisfied by SURE-tuned estimates under a mild Lipschtiz assumption; (4) an upper bound on the variance of the size of the model selected by the Lasso, and more generally an upper bound on the variance of the empirical degrees-of-freedom of convex penalized estimators; (5) explicit expressions of SURE for SURE for the Lasso and the Elastic Net; (6) in the linear model, a general semiparametric scheme to de-bias a differentiable initial estimator for the statistical inference of a low-dimensional projection of the unknown regression coefficient vector, with a characterization of the variance after debiasing; and (7) an accuracy analysis of a Gaussian Monte Carlo scheme to approximate the divergence of functions f:Rn→Rn.},
  archive      = {J_AOS},
  author       = {Pierre C. Bellec and Cun-Hui Zhang},
  doi          = {10.1214/20-AOS2005},
  journal      = {The Annals of Statistics},
  number       = {4},
  pages        = {1864-1903},
  shortjournal = {Ann. Statist.},
  title        = {Second-order stein: SURE for SURE and other applications in high-dimensional inference},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Stein’s method of normal approximation: Some recollections
and reflections. <em>AOS</em>, <em>49</em>(4), 1850–1863. (<a
href="https://doi.org/10.1214/21-AOS2083">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper is a short exposition of Stein’s method of normal approximation from my personal perspective. It focuses mainly on the characterization of the normal distribution and the construction of Stein identities. Through examples, it provides glimpses into the many approaches to constructing Stein identities and the diverse applications of Stein’s method to mathematical problems. It also includes anecdotes of historical interest, including how Stein discovered his method and how I found an unpublished proof of his of the Berry–Esseen theorem.},
  archive      = {J_AOS},
  author       = {Louis H. Y. Chen},
  doi          = {10.1214/21-AOS2083},
  journal      = {The Annals of Statistics},
  number       = {4},
  pages        = {1850-1863},
  shortjournal = {Ann. Statist.},
  title        = {Stein’s method of normal approximation: Some recollections and reflections},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Stein 1956: Efficient nonparametric testing and estimation.
<em>AOS</em>, <em>49</em>(4), 1836–1849. (<a
href="https://doi.org/10.1214/21-AOS2056">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We revisit a paper by Charles Stein, and discuss its follow-up.},
  archive      = {J_AOS},
  author       = {A. W. van der Vaart and J. A. Wellner},
  doi          = {10.1214/21-AOS2056},
  journal      = {The Annals of Statistics},
  number       = {4},
  pages        = {1836-1849},
  shortjournal = {Ann. Statist.},
  title        = {Stein 1956: Efficient nonparametric testing and estimation},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On charles stein’s contributions to (in)admissibility.
<em>AOS</em>, <em>49</em>(4), 1823–1835. (<a
href="https://doi.org/10.1214/21-AOS2108">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Charles Stein made fundamental contributions to admissibility and inadmissibility in estimation and testing. This paper surveys some of the more important ones. Particular attention will be paid to his monumentally important, and at the time, incredibly surprising discovery of the inadmissibility of the usual estimator of the mean in three and higher dimensions. His result on admissibility of Pitman’s estimator of a mean in one and two dimensions, and his results on estimation of a mean matrix and a covariance matrix are also discussed. His work on testing is briefly covered.},
  archive      = {J_AOS},
  author       = {William E. Strawderman},
  doi          = {10.1214/21-AOS2108},
  journal      = {The Annals of Statistics},
  number       = {4},
  pages        = {1823-1835},
  shortjournal = {Ann. Statist.},
  title        = {On charles stein’s contributions to (in)admissibility},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Charles stein and invariance: Beginning with the hunt–stein
theorem. <em>AOS</em>, <em>49</em>(4), 1815–1822. (<a
href="https://doi.org/10.1214/21-AOS2075">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When statistical decision theory was emerging as a promising new paradigm, Charles Stein was to play a major role in the development of minimax theory for invariant statistical problems. In some of his earliest work with Gil Hunt, he set out to prove that, in problems where invariant procedures have constant risk, any best invariant test would be minimax among all tests. Although finding it not quite true in general, this led to the legendary Hunt–Stein theorem, which established the result under restrictive conditions on the underlying group of transformations. In decision problems invariant under such suitable groups, an overall minimax test was guaranteed to reside within the class of invariant procedures where it would typically be much easier to find. But when it did not seem possible to establish this result for invariance under the full linear group, he instead turned to prove its impossibility with counterexamples such as the nonminimaxity of the usual sample covariance estimator where the full linear group was just too big for the Hunt–Stein theorem to apply. Further explorations of invariance such as the sometimes problematic inference under a fiducial distribution, or the characterization of a best invariant procedure as a formal Bayes procedure under a right Haar prior, are further examples of the far reaching influence of Stein’s contributions to invariance theory.},
  archive      = {J_AOS},
  author       = {Morris L. Eaton and Edward I. George},
  doi          = {10.1214/21-AOS2075},
  journal      = {The Annals of Statistics},
  number       = {4},
  pages        = {1815-1822},
  shortjournal = {Ann. Statist.},
  title        = {Charles stein and invariance: Beginning with the Hunt–Stein theorem},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Editorial: Memorial issue for charles stein. <em>AOS</em>,
<em>49</em>(4), 1811–1814. (<a
href="https://doi.org/10.1214/21-AOS2110">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_AOS},
  author       = {Richard J. Samworth and Ming Yuan},
  doi          = {10.1214/21-AOS2110},
  journal      = {The Annals of Statistics},
  number       = {4},
  pages        = {1811-1814},
  shortjournal = {Ann. Statist.},
  title        = {Editorial: Memorial issue for charles stein},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SuperMix: Sparse regularization for mixtures. <em>AOS</em>,
<em>49</em>(3), 1779–1809. (<a
href="https://doi.org/10.1214/20-AOS2022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the statistical estimation of a discrete mixing measure μ0 involved in a kernel mixture model. Using some recent advances in ℓ1-regularization over the space of measures, we introduce a “data fitting and regularization” convex program for estimating μ0 in a grid-less manner from a sample of mixture law, this method is referred to as Beurling-LASSO. Our contribution is two-fold: we derive a lower bound on the bandwidth of our data fitting term depending only on the support of μ0 and its so-called “minimum separation” to ensure quantitative support localization error bounds; and under a so-called “nondegenerate source condition” we derive a nonasymptotic support stability property. This latter shows that for a sufficiently large sample size n, our estimator has exactly as many weighted Dirac masses as the target μ0, converging in amplitude and localization towards the true ones. Finally, we also introduce some tractable algorithms for solving this convex program based on “Sliding Frank–Wolfe” or “Conic Particle Gradient Descent”. Statistical performances of this estimator are investigated designing a so-called “dual certificate”, which is appropriate to our setting. Some classical situations as, for example, mixtures of super-smooth distributions (see, e.g., Gaussian distributions) or ordinary-smooth distributions (see, e.g., Laplace distributions), are discussed at the end of the paper.},
  archive      = {J_AOS},
  author       = {Y. De Castro and S. Gadat and C. Marteau and C. Maugis-Rabusseau},
  doi          = {10.1214/20-AOS2022},
  journal      = {The Annals of Statistics},
  number       = {3},
  pages        = {1779-1809},
  shortjournal = {Ann. Statist.},
  title        = {SuperMix: Sparse regularization for mixtures},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Causal discovery in heavy-tailed models. <em>AOS</em>,
<em>49</em>(3), 1755–1778. (<a
href="https://doi.org/10.1214/20-AOS2021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Causal questions are omnipresent in many scientific problems. While much progress has been made in the analysis of causal relationships between random variables, these methods are not well suited if the causal mechanisms only manifest themselves in extremes. This work aims to connect the two fields of causal inference and extreme value theory. We define the causal tail coefficient that captures asymmetries in the extremal dependence of two random variables. In the population case, the causal tail coefficient is shown to reveal the causal structure if the distribution follows a linear structural causal model. This holds even in the presence of latent common causes that have the same tail index as the observed variables. Based on a consistent estimator of the causal tail coefficient, we propose a computationally highly efficient algorithm that estimates the causal structure. We prove that our method consistently recovers the causal order and we compare it to other well-established and nonextremal approaches in causal discovery on synthetic and real data. The code is available as an open-access R package.},
  archive      = {J_AOS},
  author       = {Nicola Gnecco and Nicolai Meinshausen and Jonas Peters and Sebastian Engelke},
  doi          = {10.1214/20-AOS2021},
  journal      = {The Annals of Statistics},
  number       = {3},
  pages        = {1755-1778},
  shortjournal = {Ann. Statist.},
  title        = {Causal discovery in heavy-tailed models},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). E-values: Calibration, combination and applications.
<em>AOS</em>, <em>49</em>(3), 1736–1754. (<a
href="https://doi.org/10.1214/20-AOS2020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple testing of a single hypothesis and testing multiple hypotheses are usually done in terms of p-values. In this paper, we replace p-values with their natural competitor, e-values, which are closely related to betting, Bayes factors and likelihood ratios. We demonstrate that e-values are often mathematically more tractable; in particular, in multiple testing of a single hypothesis, e-values can be merged simply by averaging them. This allows us to develop efficient procedures using e-values for testing multiple hypotheses.},
  archive      = {J_AOS},
  author       = {Vladimir Vovk and Ruodu Wang},
  doi          = {10.1214/20-AOS2020},
  journal      = {The Annals of Statistics},
  number       = {3},
  pages        = {1736-1754},
  shortjournal = {Ann. Statist.},
  title        = {E-values: Calibration, combination and applications},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). LASSO-driven inference in time and space. <em>AOS</em>,
<em>49</em>(3), 1702–1735. (<a
href="https://doi.org/10.1214/20-AOS2019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the estimation and inference in a system of high-dimensional regression equations allowing for temporal and cross-sectional dependency in covariates and error processes, covering rather general forms of weak temporal dependence. A sequence of regressions with many regressors using LASSO (Least Absolute Shrinkage and Selection Operator) is applied for variable selection purpose, and an overall penalty level is carefully chosen by a block multiplier bootstrap procedure to account for multiplicity of the equations and dependencies in the data. Correspondingly, oracle properties with a jointly selected tuning parameter are derived. We further provide high-quality de-biased simultaneous inference on the many target parameters of the system. We provide bootstrap consistency results of the test procedure, which are based on a general Bahadur representation for the Z-estimators with dependent data. Simulations demonstrate good performance of the proposed inference procedure. Finally, we apply the method to quantify spillover effects of textual sentiment indices in a financial market and to test the connectedness among sectors.},
  archive      = {J_AOS},
  author       = {Victor Chernozhukov and Wolfgang Karl Härdle and Chen Huang and Weining Wang},
  doi          = {10.1214/20-AOS2019},
  journal      = {The Annals of Statistics},
  number       = {3},
  pages        = {1702-1735},
  shortjournal = {Ann. Statist.},
  title        = {LASSO-driven inference in time and space},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust bregman clustering. <em>AOS</em>, <em>49</em>(3),
1679–1701. (<a href="https://doi.org/10.1214/20-AOS2018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering with Bregman divergences encompasses a wide family of clustering procedures that are well suited to mixtures of distributions from exponential families (J. Mach. Learn. Res. 6 (2005) 1705–1749). However, these techniques are highly sensitive to noise. To address the issue of clustering data with possibly adversarial noise, we introduce a robustified version of Bregman clustering based on a trimming approach. We investigate its theoretical properties, showing for instance that our estimator converges at a sub-Gaussian rate 1/n, where n denotes the sample size, under mild tail assumptions. We also show that it is robust to a certain amount of noise, stated in terms of breakdown point. We also derive a Lloyd-type algorithm with a trimming parameter, along with a heuristic to select this parameter and the number of clusters from sample. Some numerical experiments assess the performance of our method on simulated and real datasets.},
  archive      = {J_AOS},
  author       = {Claire Brécheteau and Aurélie Fischer and Clément Levrard},
  doi          = {10.1214/20-AOS2018},
  journal      = {The Annals of Statistics},
  number       = {3},
  pages        = {1679-1701},
  shortjournal = {Ann. Statist.},
  title        = {Robust bregman clustering},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Factor-driven two-regime regression. <em>AOS</em>,
<em>49</em>(3), 1656–1678. (<a
href="https://doi.org/10.1214/20-AOS2017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel two-regime regression model where regime switching is driven by a vector of possibly unobservable factors. When the factors are latent, we estimate them by the principal component analysis of a panel data set. We show that the optimization problem can be reformulated as mixed integer optimization, and we present two alternative computational algorithms. We derive the asymptotic distribution of the resulting estimator under the scheme that the threshold effect shrinks to zero. In particular, we establish a phase transition that describes the effect of first-stage factor estimation as the cross-sectional dimension of panel data increases relative to the time-series dimension. Moreover, we develop bootstrap inference and illustrate our methods via numerical studies.},
  archive      = {J_AOS},
  author       = {Sokbae Lee and Yuan Liao and Myung Hwan Seo and Youngki Shin},
  doi          = {10.1214/20-AOS2017},
  journal      = {The Annals of Statistics},
  number       = {3},
  pages        = {1656-1678},
  shortjournal = {Ann. Statist.},
  title        = {Factor-driven two-regime regression},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On statistical learning of simplices: Unmixing problem
revisited. <em>AOS</em>, <em>49</em>(3), 1626–1655. (<a
href="https://doi.org/10.1214/20-AOS2016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the sample complexity of learning a high-dimensional simplex from a set of points uniformly sampled from its interior. Learning of simplices is a long studied problem in computer science and has applications in computational biology and remote sensing, mostly under the name of “spectral unmixing.” We theoretically show that a sufficient sample complexity for reliable learning of a K-dimensional simplex up to a total-variation error of ϵ is O(K2ϵlogKϵ), which yields a substantial improvement over existing bounds. Based on our new theoretical framework, we also propose a heuristic approach for the inference of simplices. Experimental results on synthetic and real-world datasets demonstrate a comparable performance for our method on noiseless samples, while we outperform the state-of-the-art in noisy cases.},
  archive      = {J_AOS},
  author       = {Amir Najafi and Saeed Ilchi and Amir Hossein Saberi and Seyed Abolfazl Motahari and Babak H. Khalaj and Hamid R. Rabiee},
  doi          = {10.1214/20-AOS2016},
  journal      = {The Annals of Statistics},
  number       = {3},
  pages        = {1626-1655},
  shortjournal = {Ann. Statist.},
  title        = {On statistical learning of simplices: Unmixing problem revisited},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Approximate and exact designs for total effects.
<em>AOS</em>, <em>49</em>(3), 1594–1625. (<a
href="https://doi.org/10.1214/20-AOS2015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper considers both approximate and exact designs for estimating the total effects under one crossover and two interference models. They are different from the traditional block designs in the sense that the assigned treatments also affect their neighboring plots, hence a design is understood as a collection of sequences of treatments. A notable result in literature is that the circular neighbor balanced design (CNBD) is optimal among designs, which do not allow treatments to be neighbors of themselves. However, we find it necessary to allow self-neighboring, and further show that it is the best to allocate each treatment in a subblock of adjacent plots with equal or almost equal numbers of replications. This explains why the efficiency of CNBD drops down to 50\% as the sequence length, say k, increases. Unlike CNBD or the designs for direct effects, our proposed designs do not try to put as many treatments in a sequence as possible. The optimal number of distinct treatments in a sequence is around 2k for crossover designs and k/0.96 for interference models, whenever they are smaller than the total number of treatments under consideration. We systematically study necessary and sufficient conditions for any design to be universally optimal under the approximate design framework, based on which algorithms for deriving optimal or efficient exact designs are proposed. This hybrid nature of cohesively combining theories with algorithms makes our method more flexible than existing ones in the following aspects. (i) Not only symmetric designs are studied, general procedures for producing asymmetric designs are also provided. (ii) Our method applies to any form of within-block covariance matrix instead of specific forms. (iii) We cover all configurations of the numbers of treatments and sequence lengths, especially for large values of them when purely computational methods are not applicable. (iv) On top of the latter, we cover a continuous spectrum of the number of sequences instead of special numbers decided by combinatorial constraints.},
  archive      = {J_AOS},
  author       = {Xiangshun Kong and Mingao Yuan and Wei Zheng},
  doi          = {10.1214/20-AOS2015},
  journal      = {The Annals of Statistics},
  number       = {3},
  pages        = {1594-1625},
  shortjournal = {Ann. Statist.},
  title        = {Approximate and exact designs for total effects},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Central limit theorem for linear spectral statistics of
large dimensional kendall’s rank correlation matrices and its
applications. <em>AOS</em>, <em>49</em>(3), 1569–1593. (<a
href="https://doi.org/10.1214/20-AOS2013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper is concerned with the limiting spectral behaviors of large dimensional Kendall’s rank correlation matrices generated by samples with independent and continuous components. The statistical setting in this paper covers a wide range of highly skewed and heavy-tailed distributions since we do not require the components to be identically distributed, and do not need any moment conditions. We establish the central limit theorem (CLT) for the linear spectral statistics (LSS) of the Kendall’s rank correlation matrices under the Marchenko–Pastur asymptotic regime, in which the dimension diverges to infinity proportionally with the sample size. We further propose three nonparametric procedures for high dimensional independent test and their limiting null distributions are derived by implementing this CLT. Our numerical comparisons demonstrate the robustness and superiority of our proposed test statistics under various mixed and heavy-tailed cases.},
  archive      = {J_AOS},
  author       = {Zeng Li and Qinwen Wang and Runze Li},
  doi          = {10.1214/20-AOS2013},
  journal      = {The Annals of Statistics},
  number       = {3},
  pages        = {1569-1593},
  shortjournal = {Ann. Statist.},
  title        = {Central limit theorem for linear spectral statistics of large dimensional kendall’s rank correlation matrices and its applications},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A convex optimization approach to high-dimensional sparse
quadratic discriminant analysis. <em>AOS</em>, <em>49</em>(3),
1537–1568. (<a href="https://doi.org/10.1214/20-AOS2012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study high-dimensional sparse Quadratic Discriminant Analysis (QDA) and aim to establish the optimal convergence rates for the classification error. Minimax lower bounds are established to demonstrate the necessity of structural assumptions such as sparsity conditions on the discriminating direction and differential graph for the possible construction of consistent high-dimensional QDA rules. We then propose a classification algorithm called SDAR using constrained convex optimization under the sparsity assumptions. Both minimax upper and lower bounds are obtained and this classification rule is shown to be simultaneously rate optimal over a collection of parameter spaces, up to a logarithmic factor. Simulation studies demonstrate that SDAR performs well numerically. The algorithm is also illustrated through an analysis of prostate cancer data and colon tissue data. The methodology and theory developed for high-dimensional QDA for two groups in the Gaussian setting are also extended to multigroup classification and to classification under the Gaussian copula model.},
  archive      = {J_AOS},
  author       = {T. Tony Cai and Linjun Zhang},
  doi          = {10.1214/20-AOS2012},
  journal      = {The Annals of Statistics},
  number       = {3},
  pages        = {1537-1568},
  shortjournal = {Ann. Statist.},
  title        = {A convex optimization approach to high-dimensional sparse quadratic discriminant analysis},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Statistical inference in sparse high-dimensional additive
models. <em>AOS</em>, <em>49</em>(3), 1514–1536. (<a
href="https://doi.org/10.1214/20-AOS2011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we discuss the estimation of a nonparametric component f1 of a nonparametric additive model Y=f1(X1)+⋯+fq(Xq)+ϵ. We allow the number q of additive components to grow to infinity and we make sparsity assumptions about the number of nonzero additive components. We compare this estimation problem with that of estimating f1 in the oracle model Z=f1(X1)+ϵ, for which the additive components f2,…,fq are known. We construct a two-step presmoothing-and-resmoothing estimator of f1 and state finite-sample bounds for the difference between our estimator and a corresponding smoothing estimator fˆ1(oracle) in the oracle model. In an asymptotic setting, these bounds can be used to show asymptotic equivalence of our estimator and the oracle estimator; the paper thus shows that, asymptotically, under strong enough sparsity conditions, knowledge of f2,…,fq has no effect on estimation accuracy. Our first step is to estimate f1 with an undersmoothed estimator based on near-orthogonal projections with a group Lasso bias correction. In the second step, we construct pseudo responses Yˆ by evaluating this undersmoothed estimator of f1 at the design points and then apply the smoothing method of the oracle estimator fˆ1(oracle) to the nonparametric regression problem with “responses” Yˆ and covariates X1. Our mathematical exposition centers primarily on establishing properties of the presmoothing estimator. We present simulation results demonstrating close-to-oracle performance of our estimator in practical applications.},
  archive      = {J_AOS},
  author       = {Karl Gregory and Enno Mammen and Martin Wahl},
  doi          = {10.1214/20-AOS2011},
  journal      = {The Annals of Statistics},
  number       = {3},
  pages        = {1514-1536},
  shortjournal = {Ann. Statist.},
  title        = {Statistical inference in sparse high-dimensional additive models},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Principal components in linear mixed models with general
bulk. <em>AOS</em>, <em>49</em>(3), 1489–1513. (<a
href="https://doi.org/10.1214/20-AOS2010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the principal components of covariance estimators in multivariate mixed-effects linear models. We show that, in high dimensions, the principal eigenvalues and eigenvectors may exhibit bias and aliasing effects that are not present in low-dimensional settings. We derive the first-order limits of the principal eigenvalue locations and eigenvector projections in a high-dimensional asymptotic framework, allowing for general population spectral distributions for the random effects and extending previous results from a more restrictive spiked model. Our analysis uses free probability techniques, and we develop two general tools of independent interest—strong asymptotic freeness of GOE and deterministic matrices and a free deterministic equivalent approximation for bilinear forms of resolvents.},
  archive      = {J_AOS},
  author       = {Zhou Fan and Yi Sun and Zhichao Wang},
  doi          = {10.1214/20-AOS2010},
  journal      = {The Annals of Statistics},
  number       = {3},
  pages        = {1489-1513},
  shortjournal = {Ann. Statist.},
  title        = {Principal components in linear mixed models with general bulk},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A causal bootstrap. <em>AOS</em>, <em>49</em>(3), 1460–1488.
(<a href="https://doi.org/10.1214/20-AOS2009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The bootstrap, introduced by The Jackknife, the Bootstrap and Other Resampling Plans ((1982), SIAM), has become a very popular method for estimating variances and constructing confidence intervals. A key insight is that one can approximate the properties of estimators by using the empirical distribution function of the sample as an approximation for the true distribution function. This approach views the uncertainty in the estimator as coming exclusively from sampling uncertainty. We argue that for causal estimands the uncertainty arises entirely, or partially, from a different source, corresponding to the stochastic nature of the treatment received. We develop a bootstrap procedure for inference regarding the average treatment effect that accounts for this uncertainty, and compare its properties to that of the classical bootstrap. We consider completely randomized and observational designs as well as designs with imperfect compliance.},
  archive      = {J_AOS},
  author       = {Guido Imbens and Konrad Menzel},
  doi          = {10.1214/20-AOS2009},
  journal      = {The Annals of Statistics},
  number       = {3},
  pages        = {1460-1488},
  shortjournal = {Ann. Statist.},
  title        = {A causal bootstrap},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Total positivity in exponential families with application to
binary variables. <em>AOS</em>, <em>49</em>(3), 1436–1459. (<a
href="https://doi.org/10.1214/20-AOS2007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study exponential families of distributions that are multivariate totally positive of order 2 (MTP2), show that these are convex exponential families and derive conditions for existence of the MLE. Quadratic exponential familes of MTP2 distributions contain attractive Gaussian graphical models and ferromagnetic Ising models as special examples. We show that these are defined by intersecting the space of canonical parameters with a polyhedral cone whose faces correspond to conditional independence relations. Hence MTP2 serves as an implicit regularizer for quadratic exponential families and leads to sparsity in the estimated graphical model. We prove that the maximum likelihood estimator (MLE) in an MTP2 binary exponential family exists if and only if both of the sign patterns (1,−1) and (−1,1) are represented in the sample for every pair of variables; in particular, this implies that the MLE may exist with n=d observations, in stark contrast to unrestricted binary exponential families where 2d observations are required. Finally, we provide a novel and globally convergent algorithm for computing the MLE for MTP2 Ising models similar to iterative proportional scaling and apply it to the analysis of data from two psychological disorders.},
  archive      = {J_AOS},
  author       = {Steffen Lauritzen and Caroline Uhler and Piotr Zwiernik},
  doi          = {10.1214/20-AOS2007},
  journal      = {The Annals of Statistics},
  number       = {3},
  pages        = {1436-1459},
  shortjournal = {Ann. Statist.},
  title        = {Total positivity in exponential families with application to binary variables},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bootstrap long memory processes in the frequency domain.
<em>AOS</em>, <em>49</em>(3), 1407–1435. (<a
href="https://doi.org/10.1214/20-AOS2006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The aim of the paper is to describe a bootstrap, contrary to the sieve bootstrap, valid under either long memory (LM) or short memory (SM) dependence. One of the reasons of the failure of the sieve bootstrap in our context is that under LM dependence, the sieve bootstrap may not be able to capture the true covariance structure of the original data. We also describe and examine the validity of the bootstrap scheme for the least squares estimator of the parameter in a regression model and for model specification. The motivation for the latter example comes from the observation that the asymptotic distribution of the test is intractable.},
  archive      = {J_AOS},
  author       = {Javier Hidalgo},
  doi          = {10.1214/20-AOS2006},
  journal      = {The Annals of Statistics},
  number       = {3},
  pages        = {1407-1435},
  shortjournal = {Ann. Statist.},
  title        = {Bootstrap long memory processes in the frequency domain},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning models with uniform performance via
distributionally robust optimization. <em>AOS</em>, <em>49</em>(3),
1378–1406. (<a href="https://doi.org/10.1214/20-AOS2004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A common goal in statistics and machine learning is to learn models that can perform well against distributional shifts, such as latent heterogeneous subpopulations, unknown covariate shifts or unmodeled temporal effects. We develop and analyze a distributionally robust stochastic optimization (DRO) framework that learns a model providing good performance against perturbations to the data-generating distribution. We give a convex formulation for the problem, providing several convergence guarantees. We prove finite-sample minimax upper and lower bounds, showing that distributional robustness sometimes comes at a cost in convergence rates. We give limit theorems for the learned parameters, where we fully specify the limiting distribution so that confidence intervals can be computed. On real tasks including generalizing to unknown subpopulations, fine-grained recognition and providing good tail performance, the distributionally robust approach often exhibits improved performance.},
  archive      = {J_AOS},
  author       = {John C. Duchi and Hongseok Namkoong},
  doi          = {10.1214/20-AOS2004},
  journal      = {The Annals of Statistics},
  number       = {3},
  pages        = {1378-1406},
  shortjournal = {Ann. Statist.},
  title        = {Learning models with uniform performance via distributionally robust optimization},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptive robust estimation in sparse vector model.
<em>AOS</em>, <em>49</em>(3), 1347–1377. (<a
href="https://doi.org/10.1214/20-AOS2002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For the sparse vector model, we consider estimation of the target vector, of its ℓ2-norm and of the noise variance. We construct adaptive estimators and establish the optimal rates of adaptive estimation when adaptation is considered with respect to the triplet “noise level—noise distribution—sparsity.” We consider classes of noise distributions with polynomially and exponentially decreasing tails as well as the case of Gaussian noise. The obtained rates turn out to be different from the minimax nonadaptive rates when the triplet is known. A crucial issue is the ignorance of the noise variance. Moreover, knowing or not knowing the noise distribution can also influence the rate. For example, the rates of estimation of the noise variance can differ depending on whether the noise is Gaussian or sub-Gaussian without a precise knowledge of the distribution. Estimation of noise variance in our setting can be viewed as an adaptive variant of robust estimation of scale in the contamination model, where instead of fixing the “nominal” distribution in advance we assume that it belongs to some class of distributions.},
  archive      = {J_AOS},
  author       = {L. Comminges and O. Collier and M. Ndaoud and A. B. Tsybakov},
  doi          = {10.1214/20-AOS2002},
  journal      = {The Annals of Statistics},
  number       = {3},
  pages        = {1347-1377},
  shortjournal = {Ann. Statist.},
  title        = {Adaptive robust estimation in sparse vector model},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Frame-constrained total variation regularization for white
noise regression. <em>AOS</em>, <em>49</em>(3), 1318–1346. (<a
href="https://doi.org/10.1214/20-AOS2001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the popularity and practical success of total variation (TV) regularization for function estimation, surprisingly little is known about its theoretical performance in a statistical setting. While TV regularization has been known for quite some time to be minimax optimal for denoising one-dimensional signals, for higher dimensions this remains elusive until today. In this paper, we consider frame-constrained TV estimators including many well-known (overcomplete) frames in a white noise regression model, and prove their minimax optimality w.r.t. Lq-risk (1≤q&lt;∞) up to a logarithmic factor in any dimension d≥1. Overcomplete frames are an established tool in mathematical imaging and signal recovery, and their combination with TV regularization has been shown to give excellent results in practice, which our theory now confirms. Our results rely on a novel connection between frame-constraints and certain Besov norms, and on an interpolation inequality to relate them to the risk functional. Additionally, our results explain a phase transition in the minimax risk for BV functions.},
  archive      = {J_AOS},
  author       = {Miguel del Álamo and Housen Li and Axel Munk},
  doi          = {10.1214/20-AOS2001},
  journal      = {The Annals of Statistics},
  number       = {3},
  pages        = {1318-1346},
  shortjournal = {Ann. Statist.},
  title        = {Frame-constrained total variation regularization for white noise regression},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On cross-validated lasso in high dimensions. <em>AOS</em>,
<em>49</em>(3), 1300–1317. (<a
href="https://doi.org/10.1214/20-AOS2000">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we derive nonasymptotic error bounds for the Lasso estimator when the penalty parameter for the estimator is chosen using K-fold cross-validation. Our bounds imply that the cross-validated Lasso estimator has nearly optimal rates of convergence in the prediction, L2, and L1 norms. For example, we show that in the model with the Gaussian noise and under fairly general assumptions on the candidate set of values of the penalty parameter, the estimation error of the cross-validated Lasso estimator converges to zero in the prediction norm with the slogp/n×log(pn) rate, where n is the sample size of available data, p is the number of covariates and s is the number of nonzero coefficients in the model. Thus, the cross-validated Lasso estimator achieves the fastest possible rate of convergence in the prediction norm up to a small logarithmic factor log(pn), and similar conclusions apply for the convergence rate both in L2 and in L1 norms. Importantly, our results cover the case when p is (potentially much) larger than n and also allow for the case of non-Gaussian noise. Our paper therefore serves as a justification for the widely spread practice of using cross-validation as a method to choose the penalty parameter for the Lasso estimator.},
  archive      = {J_AOS},
  author       = {Denis Chetverikov and Zhipeng Liao and Victor Chernozhukov},
  doi          = {10.1214/20-AOS2000},
  journal      = {The Annals of Statistics},
  number       = {3},
  pages        = {1300-1317},
  shortjournal = {Ann. Statist.},
  title        = {On cross-validated lasso in high dimensions},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Strong selection consistency of bayesian vector
autoregressive models based on a pseudo-likelihood approach.
<em>AOS</em>, <em>49</em>(3), 1267–1299. (<a
href="https://doi.org/10.1214/20-AOS1992">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vector autoregressive (VAR) models aim to capture linear temporal interdependencies among multiple time series. They have been widely used in macroeconomics and financial econometrics and more recently have found novel applications in functional genomics and neuroscience. These applications have also accentuated the need to investigate the behavior of the VAR model in a high-dimensional regime, which will provide novel insights into the role of temporal dependence for regularized estimates of the models parameters. However, hardly anything is known regarding posterior model selection consistency for Bayesian VAR models in such regimes. In this work, we develop a pseudo-likelihood based Bayesian approach for consistent variable selection in high-dimensional VAR models by considering hierarchical normal priors on the autoregressive coefficients, as well as on the model space. We establish strong selection consistency of the proposed method, namely that the posterior probability assigned to the true underlying VAR model converges to one under high-dimensional scaling where the dimension p of the VAR system grows nearly exponentially with the sample size n. Further, the result is established under mild regularity conditions on the problem parameters. Finally, as a by-product of these results, we also establish strong selection consistency for the sparse high-dimensional linear regression model with serially correlated regressors and errors.},
  archive      = {J_AOS},
  author       = {Satyajit Ghosh and Kshitij Khare and George Michailidis},
  doi          = {10.1214/20-AOS1992},
  journal      = {The Annals of Statistics},
  number       = {3},
  pages        = {1267-1299},
  shortjournal = {Ann. Statist.},
  title        = {Strong selection consistency of bayesian vector autoregressive models based on a pseudo-likelihood approach},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A shrinkage principle for heavy-tailed data:
High-dimensional robust low-rank matrix recovery. <em>AOS</em>,
<em>49</em>(3), 1239–1266. (<a
href="https://doi.org/10.1214/20-AOS1980">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a simple principle for robust statistical inference via appropriate shrinkage on the data. This widens the scope of high-dimensional techniques, reducing the distributional conditions from subexponential or sub-Gaussian to more relaxed bounded second or fourth moment. As an illustration of this principle, we focus on robust estimation of the low-rank matrix Θ∗ from the trace regression model Y=Tr(Θ∗⊤X)+ε. It encompasses four popular problems: sparse linear model, compressed sensing, matrix completion and multitask learning. We propose to apply the penalized least-squares approach to the appropriately truncated or shrunk data. Under only bounded 2+δ moment condition on the response, the proposed robust methodology yields an estimator that possesses the same statistical error rates as previous literature with sub-Gaussian errors. For sparse linear model and multitask regression, we further allow the design to have only bounded fourth moment and obtain the same statistical rates. As a byproduct, we give a robust covariance estimator with concentration inequality and optimal rate of convergence in terms of the spectral norm, when the samples only bear bounded fourth moment. This result is of its own interest and importance. We reveal that under high dimensions, the sample covariance matrix is not optimal whereas our proposed robust covariance can achieve optimality. Extensive simulations are carried out to support the theories.},
  archive      = {J_AOS},
  author       = {Jianqing Fan and Weichen Wang and Ziwei Zhu},
  doi          = {10.1214/20-AOS1980},
  journal      = {The Annals of Statistics},
  number       = {3},
  pages        = {1239-1266},
  shortjournal = {Ann. Statist.},
  title        = {A shrinkage principle for heavy-tailed data: High-dimensional robust low-rank matrix recovery},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Only closed testing procedures are admissible for
controlling false discovery proportions. <em>AOS</em>, <em>49</em>(2),
1218–1238. (<a href="https://doi.org/10.1214/20-AOS1999">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the class of all multiple testing methods controlling tail probabilities of the false discovery proportion, either for one random set or simultaneously for many such sets. This class encompasses methods controlling familywise error rate, generalized familywise error rate, false discovery exceedance, joint error rate, simultaneous control of all false discovery proportions, and others, as well as gene set testing in genomics and cluster inference in neuroimaging. We show that all such methods are either equivalent to a closed testing procedure, or are uniformly improved by one. Moreover, we show that a closed testing method is admissible if and only if all its local tests are admissible. This implies that, when designing methods, it is sufficient to restrict attention to closed testing. We demonstrate the practical usefulness of this design principle by obtaining more informative inferences from the method of higher criticism, and by constructing a uniform improvement of a recently proposed method.},
  archive      = {J_AOS},
  author       = {Jelle J. Goeman and Jesse Hemerik and Aldo Solari},
  doi          = {10.1214/20-AOS1999},
  journal      = {The Annals of Statistics},
  number       = {2},
  pages        = {1218-1238},
  shortjournal = {Ann. Statist.},
  title        = {Only closed testing procedures are admissible for controlling false discovery proportions},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Estimation and inference in the presence of fractional d=1/2
and weakly nonstationary processes. <em>AOS</em>, <em>49</em>(2),
1195–1217. (<a href="https://doi.org/10.1214/20-AOS1998">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We provide new limit theory for functionals of a general class of processes lying at the boundary between stationarity and nonstationarity—what we term weakly nonstationary processes (WNPs). This includes, as leading examples, fractional processes with d=1/2, and arrays of autoregressive processes with roots drifting slowly towards unity. We first apply the theory to study inference in parametric and nonparametric regression models involving WNPs as covariates. We then use these results to develop a new specification test for parametric regression models. By construction, our specification test statistic has a χ2 limiting distribution regardless of the form and extent of persistence of the regressor, implying that a practitioner can validly perform the test using a fixed critical value, while remaining agnostic about the mechanism generating the regressor. Simulation exercises confirm that the test controls size across a wide range of data generating processes, and outperforms a comparable test due to Wang and Phillips (Ann. Statist. 40 (2012) 727–758) against many alternatives.},
  archive      = {J_AOS},
  author       = {James A. Duffy and Ioannis Kasparis},
  doi          = {10.1214/20-AOS1998},
  journal      = {The Annals of Statistics},
  number       = {2},
  pages        = {1195-1217},
  shortjournal = {Ann. Statist.},
  title        = {Estimation and inference in the presence of fractional d=1/2 and weakly nonstationary processes},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Minimax estimation of smooth optimal transport maps.
<em>AOS</em>, <em>49</em>(2), 1166–1194. (<a
href="https://doi.org/10.1214/20-AOS1997">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Brenier’s theorem is a cornerstone of optimal transport that guarantees the existence of an optimal transport map T between two probability distributions P and Q over Rd under certain regularity conditions. The main goal of this work is to establish the minimax estimation rates for such a transport map from data sampled from P and Q under additional smoothness assumptions on T. To achieve this goal, we develop an estimator based on the minimization of an empirical version of the semidual optimal transport problem, restricted to truncated wavelet expansions. This estimator is shown to achieve near minimax optimality using new stability arguments for the semidual and a complementary minimax lower bound. Furthermore, we provide numerical experiments on synthetic data supporting our theoretical findings and highlighting the practical benefits of smoothness regularization. These are the first minimax estimation rates for transport maps in general dimension.},
  archive      = {J_AOS},
  author       = {Jan-Christian Hütter and Philippe Rigollet},
  doi          = {10.1214/20-AOS1997},
  journal      = {The Annals of Statistics},
  number       = {2},
  pages        = {1166-1194},
  shortjournal = {Ann. Statist.},
  title        = {Minimax estimation of smooth optimal transport maps},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Distribution and quantile functions, ranks and signs in
dimension d: A measure transportation approach. <em>AOS</em>,
<em>49</em>(2), 1139–1165. (<a
href="https://doi.org/10.1214/20-AOS1996">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unlike the real line, the real space Rd, for d≥2, is not canonically ordered. As a consequence, such fundamental univariate concepts as quantile and distribution functions and their empirical counterparts, involving ranks and signs, do not canonically extend to the multivariate context. Palliating that lack of a canonical ordering has been an open problem for more than half a century, generating an abundant literature and motivating, among others, the development of statistical depth and copula-based methods. We show that, unlike the many definitions proposed in the literature, the measure transportation-based ranks and signs introduced in Chernozhukov, Galichon, Hallin and Henry (Ann. Statist. 45 (2017) 223–256) enjoy all the properties that make univariate ranks a successful tool for semiparametric inference. Related with those ranks, we propose a new center-outward definition of multivariate distribution and quantile functions, along with their empirical counterparts, for which we establish a Glivenko–Cantelli result. Our approach is based on McCann (Duke Math. J. 80 (1995) 309–323) and our results do not require any moment assumptions. The resulting ranks and signs are shown to be strictly distribution-free and essentially maximal ancillary in the sense of Basu (Sankhyā 21 (1959) 247–256) which, in semiparametric models involving noise with unspecified density, can be interpreted as a finite-sample form of semiparametric efficiency. Although constituting a sufficient summary of the sample, empirical center-outward distribution functions are defined at observed values only. A continuous extension to the entire d-dimensional space, yielding smooth empirical quantile contours and sign curves while preserving the essential monotonicity and Glivenko–Cantelli features of the concept, is provided. A numerical study of the resulting empirical quantile contours is conducted.},
  archive      = {J_AOS},
  author       = {Marc Hallin and Eustasio del Barrio and Juan Cuesta-Albertos and Carlos Matrán},
  doi          = {10.1214/20-AOS1996},
  journal      = {The Annals of Statistics},
  number       = {2},
  pages        = {1139-1165},
  shortjournal = {Ann. Statist.},
  title        = {Distribution and quantile functions, ranks and signs in dimension d: A measure transportation approach},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Spiked separable covariance matrices and principal
components. <em>AOS</em>, <em>49</em>(2), 1113–1138. (<a
href="https://doi.org/10.1214/20-AOS1995">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study a class of separable sample covariance matrices of the form Q˜1:=A˜1/2XB˜X∗A˜1/2. Here, A˜ and B˜ are positive definite matrices whose spectrums consist of bulk spectrums plus several spikes, that is, larger eigenvalues that are separated from the bulks. Conceptually, we call Q˜1 a spiked separable covariance matrix model. On the one hand, this model includes the spiked covariance matrix as a special case with B˜=I. On the other hand, it allows for more general correlations of datasets. In particular, for spatio-temporal dataset, A˜ and B˜ represent the spatial and temporal correlations, respectively. In this paper, we study the outlier eigenvalues and eigenvectors, that is, the principal components, of the spiked separable covariance model Q˜1. We prove the convergence of the outlier eigenvalues λ˜i and the generalized components (i.e., ⟨v,ξ˜i⟩ for any deterministic vector v) of the outlier eigenvectors ξ˜i with optimal convergence rates. Moreover, we also prove the delocalization of the nonoutlier eigenvectors. We state our results in full generality, in the sense that they also hold near the so-called BBP transition and for degenerate outliers. Our results highlight both the similarity and difference between the spiked separable covariance matrix model and the spiked covariance matrix model in (Probab. Theory Related Fields 164 (2016) 459–552). In particular, we show that the spikes of both A˜ and B˜ will cause outliers of the eigenvalue spectrum, and the eigenvectors can help to select the outliers that correspond to the spikes of A˜ (or B˜).},
  archive      = {J_AOS},
  author       = {Xiucai Ding and Fan Yang},
  doi          = {10.1214/20-AOS1995},
  journal      = {The Annals of Statistics},
  number       = {2},
  pages        = {1113-1138},
  shortjournal = {Ann. Statist.},
  title        = {Spiked separable covariance matrices and principal components},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Minimax rates in sparse, high-dimensional change point
detection. <em>AOS</em>, <em>49</em>(2), 1081–1112. (<a
href="https://doi.org/10.1214/20-AOS1994">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the detection of a sparse change in a high-dimensional mean vector as a minimax testing problem. Our first main contribution is to derive the exact minimax testing rate across all parameter regimes for n independent, p-variate Gaussian observations. This rate exhibits a phase transition when the sparsity level is of order ploglog(8n) and has a very delicate dependence on the sample size: in a certain sparsity regime, it involves a triple iterated logarithmic factor in n. Further, in a dense asymptotic regime, we identify the sharp leading constant, while in the corresponding sparse asymptotic regime, this constant is determined to within a factor of 2. Extensions that cover spatial and temporal dependence, primarily in the dense case, are also provided.},
  archive      = {J_AOS},
  author       = {Haoyang Liu and Chao Gao and Richard J. Samworth},
  doi          = {10.1214/20-AOS1994},
  journal      = {The Annals of Statistics},
  number       = {2},
  pages        = {1081-1112},
  shortjournal = {Ann. Statist.},
  title        = {Minimax rates in sparse, high-dimensional change point detection},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Time-uniform, nonparametric, nonasymptotic confidence
sequences. <em>AOS</em>, <em>49</em>(2), 1055–1080. (<a
href="https://doi.org/10.1214/20-AOS1991">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A confidence sequence is a sequence of confidence intervals that is uniformly valid over an unbounded time horizon. Our work develops confidence sequences whose widths go to zero, with nonasymptotic coverage guarantees under nonparametric conditions. We draw connections between the Cramér–Chernoff method for exponential concentration, the law of the iterated logarithm (LIL) and the sequential probability ratio test—our confidence sequences are time-uniform extensions of the first; provide tight, nonasymptotic characterizations of the second; and generalize the third to nonparametric settings, including sub-Gaussian and Bernstein conditions, self-normalized processes and matrix martingales. We illustrate the generality of our proof techniques by deriving an empirical-Bernstein bound growing at a LIL rate, as well as a novel upper LIL for the maximum eigenvalue of a sum of random matrices. Finally, we apply our methods to covariance matrix estimation and to estimation of sample average treatment effect under the Neyman–Rubin potential outcomes model.},
  archive      = {J_AOS},
  author       = {Steven R. Howard and Aaditya Ramdas and Jon McAuliffe and Jasjeet Sekhon},
  doi          = {10.1214/20-AOS1991},
  journal      = {The Annals of Statistics},
  number       = {2},
  pages        = {1055-1080},
  shortjournal = {Ann. Statist.},
  title        = {Time-uniform, nonparametric, nonasymptotic confidence sequences},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Linearized two-layers neural networks in high dimension.
<em>AOS</em>, <em>49</em>(2), 1029–1054. (<a
href="https://doi.org/10.1214/20-AOS1990">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of learning an unknown function f⋆ on the d-dimensional sphere with respect to the square loss, given i.i.d. samples {(yi,xi)}i≤n where xi is a feature vector uniformly distributed on the sphere and yi=f⋆(xi)+εi. We study two popular classes of models that can be regarded as linearizations of two-layers neural networks around a random initialization: the random features model of Rahimi–Recht (RF); the neural tangent model of Jacot–Gabriel–Hongler (NT). Both these models can also be regarded as randomized approximations of kernel ridge regression (with respect to different kernels), and enjoy universal approximation properties when the number of neurons N diverges, for a fixed dimension d. We consider two specific regimes: the infinite-sample finite-width regime, in which n=∞ while d and N are large but finite, and the infinite-width finite-sample regime in which N=∞ while d and n are large but finite. In the first regime, we prove that if dℓ+δ≤N≤dℓ+1−δ for small δ&gt;0, then RF effectively fits a degree-ℓ polynomial in the raw features, and NT fits a degree-(ℓ+1) polynomial. In the second regime, both RF and NT reduce to kernel methods with rotationally invariant kernels. We prove that, if the sample size satisfies dℓ+δ≤n≤dℓ+1−δ, then kernel methods can fit at most a degree-ℓ polynomial in the raw features. This lower bound is achieved by kernel ridge regression, and near-optimal prediction error is achieved for vanishing ridge regularization.},
  archive      = {J_AOS},
  author       = {Behrooz Ghorbani and Song Mei and Theodor Misiakiewicz and Andrea Montanari},
  doi          = {10.1214/20-AOS1990},
  journal      = {The Annals of Statistics},
  number       = {2},
  pages        = {1029-1054},
  shortjournal = {Ann. Statist.},
  title        = {Linearized two-layers neural networks in high dimension},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Coverage of credible intervals in nonparametric monotone
regression. <em>AOS</em>, <em>49</em>(2), 1011–1028. (<a
href="https://doi.org/10.1214/20-AOS1989">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For nonparametric univariate regression under a monotonicity constraint on the regression function f, we study the coverage of a Bayesian credible interval for f(x0), where x0 is an interior point. Analysis of the posterior becomes a lot more tractable by considering a “projection-posterior” distribution based on a finite random series of step functions with normal basis coefficients as a prior for f. A sample f from the resulting conjugate posterior distribution is projected on the space of monotone increasing functions to obtain a monotone function f∗ closest to f, inducing the “projection-posterior.” We use projection-posterior samples to obtain credible intervals for f(x0). We obtain the asymptotic coverage of the credible interval thus constructed and observe that it is free of nuisance parameters involving the true function. We observe a very interesting phenomenon that the coverage is typically higher than the nominal credibility level, the opposite of a phenomenon observed by Cox (Ann. Statist. 21 (1993) 903–923) in the Gaussian sequence model. We further show that a recalibration gives the right asymptotic coverage by starting from a lower credibility level that can be explicitly calculated.},
  archive      = {J_AOS},
  author       = {Moumita Chakraborty and Subhashis Ghosal},
  doi          = {10.1214/20-AOS1989},
  journal      = {The Annals of Statistics},
  number       = {2},
  pages        = {1011-1028},
  shortjournal = {Ann. Statist.},
  title        = {Coverage of credible intervals in nonparametric monotone regression},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multiscale geometric feature extraction for high-dimensional
and non-euclidean data with applications. <em>AOS</em>, <em>49</em>(2),
988–1010. (<a href="https://doi.org/10.1214/20-AOS1988">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A method for extracting multiscale geometric features from a data cloud is proposed and analyzed. Based on geometric considerations, we map each pair of data points into a real-valued feature function defined on the unit interval. Further statistical analysis is then based on the collection of feature functions. The potential of the method is illustrated by different applications, including classification and anomaly detection. Connections to other concepts, such as random set theory, localized depth measures and nonlinear dimension reduction, are also explored.},
  archive      = {J_AOS},
  author       = {Gabriel Chandler and Wolfgang Polonik},
  doi          = {10.1214/20-AOS1988},
  journal      = {The Annals of Statistics},
  number       = {2},
  pages        = {988-1010},
  shortjournal = {Ann. Statist.},
  title        = {Multiscale geometric feature extraction for high-dimensional and non-euclidean data with applications},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Asymptotic distribution and convergence rates of stochastic
algorithms for entropic optimal transportation between probability
measures. <em>AOS</em>, <em>49</em>(2), 968–987. (<a
href="https://doi.org/10.1214/20-AOS1987">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper is devoted to the stochastic approximation of entropically regularized Wasserstein distances between two probability measures, also known as Sinkhorn divergences. The semi-dual formulation of such regularized optimal transportation problems can be rewritten as a nonstrongly concave optimisation problem. It allows to implement a Robbins–Monro stochastic algorithm to estimate the Sinkhorn divergence using a sequence of data sampled from one of the two distributions. Our main contribution is to establish the almost sure convergence and the asymptotic normality of a new recursive estimator of the Sinkhorn divergence between two probability measures in the discrete and semi-discrete settings. We also study the rate of convergence of the expected excess risk of this estimator in the absence of strong concavity of the objective function. Numerical experiments on synthetic and real datasets are also provided to illustrate the usefulness of our approach for data analysis.},
  archive      = {J_AOS},
  author       = {Bernard Bercu and Jérémie Bigot},
  doi          = {10.1214/20-AOS1987},
  journal      = {The Annals of Statistics},
  number       = {2},
  pages        = {968-987},
  shortjournal = {Ann. Statist.},
  title        = {Asymptotic distribution and convergence rates of stochastic algorithms for entropic optimal transportation between probability measures},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Subspace estimation from unbalanced and incomplete data
matrices: ℓ2,∞ statistical guarantees. <em>AOS</em>, <em>49</em>(2),
944–967. (<a href="https://doi.org/10.1214/20-AOS1986">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper is concerned with estimating the column space of an unknown low-rank matrix A⋆∈Rd1×d2, given noisy and partial observations of its entries. There is no shortage of scenarios where the observations—while being too noisy to support faithful recovery of the entire matrix—still convey sufficient information to enable reliable estimation of the column space of interest. This is particularly evident and crucial for the highly unbalanced case where the column dimension d2 far exceeds the row dimension d1, which is the focal point of the current paper. We investigate an efficient spectral method, which operates upon the sample Gram matrix with diagonal deletion. While this algorithmic idea has been studied before, we establish new statistical guarantees for this method in terms of both ℓ2 and ℓ2,∞ estimation accuracy, which improve upon prior results if d2 is substantially larger than d1. To illustrate the effectiveness of our findings, we derive matching minimax lower bounds with respect to the noise levels, and develop consequences of our general theory for three applications of practical importance: (1) tensor completion from noisy data, (2) covariance estimation/principal component analysis with missing data and (3) community recovery in bipartite graphs. Our theory leads to improved performance guarantees for all three cases.},
  archive      = {J_AOS},
  author       = {Changxiao Cai and Gen Li and Yuejie Chi and H. Vincent Poor and Yuxin Chen},
  doi          = {10.1214/20-AOS1986},
  journal      = {The Annals of Statistics},
  number       = {2},
  pages        = {944-967},
  shortjournal = {Ann. Statist.},
  title        = {Subspace estimation from unbalanced and incomplete data matrices: ℓ2,∞ statistical guarantees},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Distributed linear regression by averaging. <em>AOS</em>,
<em>49</em>(2), 918–943. (<a
href="https://doi.org/10.1214/20-AOS1984">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributed statistical learning problems arise commonly when dealing with large datasets. In this setup, datasets are partitioned over machines, which compute locally, and communicate short messages. Communication is often the bottleneck. In this paper, we study one-step and iterative weighted parameter averaging in statistical linear models under data parallelism. We do linear regression on each machine, send the results to a central server and take a weighted average of the parameters. Optionally, we iterate, sending back the weighted average and doing local ridge regressions centered at it. How does this work compared to doing linear regression on the full data? Here, we study the performance loss in estimation and test error, and confidence interval length in high dimensions, where the number of parameters is comparable to the training data size. We find the performance loss in one-step weighted averaging, and also give results for iterative averaging. We also find that different problems are affected differently by the distributed framework. Estimation error and confidence interval length increases a lot, while prediction error increases much less. We rely on recent results from random matrix theory, where we develop a new calculus of deterministic equivalents as a tool of broader interest.},
  archive      = {J_AOS},
  author       = {Edgar Dobriban and Yue Sheng},
  doi          = {10.1214/20-AOS1984},
  journal      = {The Annals of Statistics},
  number       = {2},
  pages        = {918-943},
  shortjournal = {Ann. Statist.},
  title        = {Distributed linear regression by averaging},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Safe adaptive importance sampling: A mixture approach.
<em>AOS</em>, <em>49</em>(2), 885–917. (<a
href="https://doi.org/10.1214/20-AOS1983">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates adaptive importance sampling algorithms for which the policy, the sequence of distributions used to generate the particles, is a mixture distribution between a flexible kernel density estimate (based on the previous particles), and a “safe” heavy-tailed density. When the share of samples generated according to the safe density goes to zero but not too quickly, two results are established: (i) uniform convergence rates are derived for the policy toward the target density; (ii) a central limit theorem is obtained for the resulting integral estimates. The fact that the asymptotic variance is the same as the variance of an “oracle” procedure with variance-optimal policy, illustrates the benefits of the approach. In addition, a subsampling step (among the particles) can be conducted before constructing the kernel estimate in order to decrease the computational effort without altering the performance of the method. The practical behavior of the algorithms is illustrated in a simulation study.},
  archive      = {J_AOS},
  author       = {Bernard Delyon and François Portier},
  doi          = {10.1214/20-AOS1983},
  journal      = {The Annals of Statistics},
  number       = {2},
  pages        = {885-917},
  shortjournal = {Ann. Statist.},
  title        = {Safe adaptive importance sampling: A mixture approach},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Survival analysis via hierarchically dependent mixture
hazards. <em>AOS</em>, <em>49</em>(2), 863–884. (<a
href="https://doi.org/10.1214/20-AOS1982">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hierarchical nonparametric processes are popular tools for defining priors on collections of probability distributions, which induce dependence across multiple samples. In survival analysis problems, one is typically interested in modeling the hazard rates, rather than the probability distributions themselves, and the currently available methodologies are not applicable. Here, we fill this gap by introducing a novel, and analytically tractable, class of multivariate mixtures whose distribution acts as a prior for the vector of sample-specific baseline hazard rates. The dependence is induced through a hierarchical specification of the mixing random measures that ultimately corresponds to a composition of random discrete combinatorial structures. Our theoretical results allow to develop a full Bayesian analysis for this class of models, which can also account for right-censored survival data and covariates, and we also show posterior consistency. In particular, we emphasize that the posterior characterization we achieve is the key for devising both marginal and conditional algorithms for evaluating Bayesian inferences of interest. The effectiveness of our proposal is illustrated through some synthetic and real data examples.},
  archive      = {J_AOS},
  author       = {Federico Camerlenghi and Antonio Lijoi and Igor Prünster},
  doi          = {10.1214/20-AOS1982},
  journal      = {The Annals of Statistics},
  number       = {2},
  pages        = {863-884},
  shortjournal = {Ann. Statist.},
  title        = {Survival analysis via hierarchically dependent mixture hazards},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Empirical process results for exchangeable arrays.
<em>AOS</em>, <em>49</em>(2), 845–862. (<a
href="https://doi.org/10.1214/20-AOS1981">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Exchangeable arrays are natural tools to model common forms of dependence between units of a sample. Jointly exchangeable arrays are well suited to dyadic data, where observed random variables are indexed by two units from the same population. Examples include trade flows between countries or relationships in a network. Separately exchangeable arrays are well suited to multiway clustering, where units sharing the same cluster (e.g., geographical areas or sectors of activity when considering individual wages) may be dependent in an unrestricted way. We prove uniform laws of large numbers and central limit theorems for such exchangeable arrays. We obtain these results under the same moment restrictions and conditions on the class of functions as those typically assumed with i.i.d. data. We also show the convergence of bootstrap processes adapted to such arrays.},
  archive      = {J_AOS},
  author       = {Laurent Davezies and Xavier D’Haultfœuille and Yannick Guyonvarch},
  doi          = {10.1214/20-AOS1981},
  journal      = {The Annals of Statistics},
  number       = {2},
  pages        = {845-862},
  shortjournal = {Ann. Statist.},
  title        = {Empirical process results for exchangeable arrays},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Necessary and sufficient conditions for variable selection
consistency of the LASSO in high dimensions. <em>AOS</em>,
<em>49</em>(2), 820–844. (<a
href="https://doi.org/10.1214/20-AOS1979">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates conditions for variable selection consistency of the LASSO in high dimensional regression models and gives necessary and sufficient conditions for the same, potentially allowing the model dimension p to grow arbitrarily fast as a function of the sample size n. These conditions require both upper and lower bounds on the growth rate of the penalty parameter. It turns out that a variant of the irrepresentable Condition (IRC) of (J. Mach. Learn. Res. 7 (2006) 2541–2563), herein called the lower irrepresentable Condition (or LIRC), is determined by the lower bound considerations while the upper bound considerations lead to a new condition, called the upper irrepresentable Condition (or UIRC) in this paper. It is shown that the LIRC together with the UIRC is necessary and sufficient for the variable selection consistency of the LASSO, thereby settling a conjecture of (J. Mach. Learn. Res. 7 (2006) 2541–2563). Further, it is shown that under some mild regularity conditions, the penalty parameter must necessarily tend to infinity at a certain minimal rate to ensure variable selection consistency of the LASSO and that the corresponding LASSO estimators of the nonzero regression parameters cannot be n-consistent (even for individual parameters). Thus, under fairly general conditions, the LASSO with a single choice of the penalty parameter cannot achieve both variable selection consistency and n-consistency simultaneously.},
  archive      = {J_AOS},
  author       = {Soumendra N. Lahiri},
  doi          = {10.1214/20-AOS1979},
  journal      = {The Annals of Statistics},
  number       = {2},
  pages        = {820-844},
  shortjournal = {Ann. Statist.},
  title        = {Necessary and sufficient conditions for variable selection consistency of the LASSO in high dimensions},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Analysis of “learn-as-you-go” (LAGO) studies. <em>AOS</em>,
<em>49</em>(2), 793–819. (<a
href="https://doi.org/10.1214/20-AOS1978">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Learn-As-you-GO (LAGO) adaptive studies, the intervention is a complex multicomponent package, and is adapted in stages during the study based on past outcome data. This design formalizes standard practice in public health intervention studies. An effective intervention package is sought, while minimizing intervention package cost. In LAGO study data, the interventions in later stages depend upon the outcomes in the previous stages, violating standard statistical theory. We develop an estimator for the intervention effects, and prove consistency and asymptotic normality using a novel coupling argument, ensuring the validity of the test for the hypothesis of no overall intervention effect. We develop a confidence set for the optimal intervention package and confidence bands for the success probabilities under alternative package compositions. We illustrate our methods in the BetterBirth Study, which aimed to improve maternal and neonatal outcomes among 157,689 births in Uttar Pradesh, India through a multicomponent intervention package.},
  archive      = {J_AOS},
  author       = {Daniel Nevo and Judith J. Lok and Donna Spiegelman},
  doi          = {10.1214/20-AOS1978},
  journal      = {The Annals of Statistics},
  number       = {2},
  pages        = {793-819},
  shortjournal = {Ann. Statist.},
  title        = {Analysis of “learn-as-you-go” (LAGO) studies},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multivariate extensions of isotonic regression and total
variation denoising via entire monotonicity and hardy–krause variation.
<em>AOS</em>, <em>49</em>(2), 769–792. (<a
href="https://doi.org/10.1214/20-AOS1977">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of nonparametric regression when the covariate is d dimensional, where d≥1. In this paper, we introduce and study two nonparametric least squares estimators (LSEs) in this setting—the entirely monotonic LSE and the constrained Hardy–Krause variation LSE. We show that these two LSEs are natural generalizations of univariate isotonic regression and univariate total variation denoising, respectively, to multiple dimensions. We discuss the characterization and computation of these two LSEs obtained from n data points. We provide a detailed study of their risk properties under the squared error loss and fixed uniform lattice design. We show that the finite sample risk of these LSEs is always bounded from above by n−2/3 modulo logarithmic factors depending on d; thus these nonparametric LSEs avoid the curse of dimensionality to some extent. We also prove nearly matching minimax lower bounds. Further, we illustrate that these LSEs are particularly useful in fitting rectangular piecewise constant functions. Specifically, we show that the risk of the entirely monotonic LSE is almost parametric (at most 1/n up to logarithmic factors) when the true function is well approximable by a rectangular piecewise constant entirely monotone function with not too many constant pieces. A similar result is also shown to hold for the constrained Hardy–Krause variation LSE for a simple subclass of rectangular piecewise constant functions. We believe that the proposed LSEs yield a novel approach to estimating multivariate functions using convex optimization that avoid the curse of dimensionality to some extent.},
  archive      = {J_AOS},
  author       = {Billy Fang and Adityanand Guntuboyina and Bodhisattva Sen},
  doi          = {10.1214/20-AOS1977},
  journal      = {The Annals of Statistics},
  number       = {2},
  pages        = {769-792},
  shortjournal = {Ann. Statist.},
  title        = {Multivariate extensions of isotonic regression and total variation denoising via entire monotonicity and Hardy–Krause variation},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Network representation using graph root distributions.
<em>AOS</em>, <em>49</em>(2), 745–768. (<a
href="https://doi.org/10.1214/20-AOS1976">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Exchangeable random graphs serve as an important probabilistic framework for the statistical analysis of network data. In this work, we develop an alternative parameterization for a large class of exchangeable random graphs, where the nodes are independent random vectors in a linear space equipped with an indefinite inner product, and the edge probability between two nodes equals the inner product of the corresponding node vectors. Therefore, the distribution of exchangeable random graphs in this subclass can be represented by a node sampling distribution on this linear space, which we call the graph root distribution. We study existence and identifiability of such representations, the topological relationship between the graph root distribution and the exchangeable random graph sampling distribution and estimation of graph root distributions.},
  archive      = {J_AOS},
  author       = {Jing Lei},
  doi          = {10.1214/20-AOS1976},
  journal      = {The Annals of Statistics},
  number       = {2},
  pages        = {745-768},
  shortjournal = {Ann. Statist.},
  title        = {Network representation using graph root distributions},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Optimal disclosure risk assessment. <em>AOS</em>,
<em>49</em>(2), 723–744. (<a
href="https://doi.org/10.1214/20-AOS1975">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Protection against disclosure is a legal and ethical obligation for agencies releasing microdata files for public use. Consider a microdata sample of size n from a finite population of size n¯=n+λn, with λ&gt;0, such that each sample record contains two disjoint types of information: identifying categorical information and sensitive information. Any decision about releasing data is supported by the estimation of measures of disclosure risk, which are defined as discrete functionals of the number of sample records with a unique combination of values of identifying variables. The most common measure is arguably the number τ1 of sample unique records that are population uniques. In this paper, we first study nonparametric estimation of τ1 under the Poisson abundance model for sample records. We introduce a class of linear estimators of τ1 that are simple, computationally efficient and scalable to massive datasets, and we give uniform theoretical guarantees for them. In particular, we show that they provably estimate τ1 all of the way up to the sampling fraction (λ+1)−1∝(logn)−1, with vanishing normalized mean-square error (NMSE) for large n. We then establish a lower bound for the minimax NMSE for the estimation of τ1, which allows us to show that: (i) (λ+1)−1∝(logn)−1 is the smallest possible sampling fraction for consistently estimating τ1; (ii) estimators’ NMSE is near optimal, in the sense of matching the minimax lower bound, for large n. This is the main result of our paper, and it provides a rigorous answer to an open question about the feasibility of nonparametric estimation of τ1 under the Poisson abundance model and for a sampling fraction (λ+1)−1&lt;1/2.},
  archive      = {J_AOS},
  author       = {Federico Camerlenghi and Stefano Favaro and Zacharie Naulet and Francesca Panero},
  doi          = {10.1214/20-AOS1975},
  journal      = {The Annals of Statistics},
  number       = {2},
  pages        = {723-744},
  shortjournal = {Ann. Statist.},
  title        = {Optimal disclosure risk assessment},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The adaptive wynn algorithm in generalized linear models
with univariate response. <em>AOS</em>, <em>49</em>(2), 702–722. (<a
href="https://doi.org/10.1214/20-AOS1974">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For a nonlinear regression model, the information matrices of designs depend on the parameter of the model. The adaptive Wynn algorithm for D-optimal design estimates the parameter at each step on the basis of the observed responses and employed design points so far, and selects the next design point as in the classical Wynn algorithm for D-optimal design. The name “Wynn algorithm” is in honor of Henry P. Wynn who established the latter “classical” algorithm in his 1970 paper (Ann. Math. Stat. 41 (1970) 1655–1664). The asymptotics of the sequences of designs and maximum likelihood estimates generated by the adaptive algorithm is studied for an important class of nonlinear regression models: generalized linear models whose (univariate) response variables follow a distribution from a one-parameter exponential family. Under the assumptions of compactness of the experimental region and of the parameter space together with some natural continuity assumptions, it is shown that the adaptive ML-estimators are strongly consistent and the design sequence is asymptotically locally D-optimal at the true parameter point. If the true parameter point is an interior point of the parameter space, then under some smoothness assumptions the asymptotic normality of the adaptive ML-estimators is obtained.},
  archive      = {J_AOS},
  author       = {Fritjof Freise and Norbert Gaffke and Rainer Schwabe},
  doi          = {10.1214/20-AOS1974},
  journal      = {The Annals of Statistics},
  number       = {2},
  pages        = {702-722},
  shortjournal = {Ann. Statist.},
  title        = {The adaptive wynn algorithm in generalized linear models with univariate response},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Average treatment effects in the presence of unknown
interference. <em>AOS</em>, <em>49</em>(2), 673–701. (<a
href="https://doi.org/10.1214/20-AOS1973">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate large-sample properties of treatment effect estimators under unknown interference in randomized experiments. The inferential target is a generalization of the average treatment effect estimand that marginalizes over potential spillover effects. We show that estimators commonly used to estimate treatment effects under no interference are consistent for the generalized estimand for several common experimental designs under limited but otherwise arbitrary and unknown interference. The rates of convergence depend on the growth rate of the unit-average amount of interference and the degree to which the interference aligns with dependencies in treatment assignment. Importantly for practitioners, the results imply that even if one erroneously assumes that units do not interfere in a setting with moderate interference, standard estimators are nevertheless likely to be close to an average treatment effect if the sample is sufficiently large. Conventional confidence statements may, however, not be accurate.},
  archive      = {J_AOS},
  author       = {Fredrik Sävje and Peter M. Aronow and Michael G. Hudgens},
  doi          = {10.1214/20-AOS1973},
  journal      = {The Annals of Statistics},
  number       = {2},
  pages        = {673-701},
  shortjournal = {Ann. Statist.},
  title        = {Average treatment effects in the presence of unknown interference},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). High-dimensional nonparametric density estimation via
symmetry and shape constraints. <em>AOS</em>, <em>49</em>(2), 650–672.
(<a href="https://doi.org/10.1214/20-AOS1972">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We tackle the problem of high-dimensional nonparametric density estimation by taking the class of log-concave densities on Rp and incorporating within it symmetry assumptions, which facilitate scalable estimation algorithms and can mitigate the curse of dimensionality. Our main symmetry assumption is that the super-level sets of the density are K-homothetic (i.e., scalar multiples of a convex body K⊆Rp). When K is known, we prove that the K-homothetic log-concave maximum likelihood estimator based on n independent observations from such a density achieves the minimax optimal rate of convergence with respect to, for example, squared Hellinger loss, of order n−4/5, independent of p. Moreover, we show that the estimator is adaptive in the sense that if the data generating density admits a special form, then a nearly parametric rate may be attained. We also provide worst case and adaptive risk bounds in cases where K is only known up to a positive definite transformation, and where it is completely unknown and must be estimated nonparametrically. Our estimation algorithms are fast even when n and p are on the order of hundreds of thousands, and we illustrate the strong finite-sample performance of our methods on simulated data.},
  archive      = {J_AOS},
  author       = {Min Xu and Richard J. Samworth},
  doi          = {10.1214/20-AOS1972},
  journal      = {The Annals of Statistics},
  number       = {2},
  pages        = {650-672},
  shortjournal = {Ann. Statist.},
  title        = {High-dimensional nonparametric density estimation via symmetry and shape constraints},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Density deconvolution under general assumptions on the
distribution of measurement errors. <em>AOS</em>, <em>49</em>(2),
615–649. (<a href="https://doi.org/10.1214/20-AOS1969">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study the problem of density deconvolution under general assumptions on the measurement error distribution. Typically, deconvolution estimators are constructed using Fourier transform techniques, and it is assumed that the characteristic function of the measurement errors does not have zeros on the real line. This assumption is rather strong and is not fulfilled in many cases of interest. In this paper, we develop a methodology for constructing optimal density deconvolution estimators in the general setting that covers vanishing and nonvanishing characteristic functions of the measurement errors. We derive upper bounds on the risk of the proposed estimators and provide sufficient conditions under which zeros of the corresponding characteristic function have no effect on estimation accuracy. Moreover, we show that the derived conditions are also necessary in some specific problem instances.},
  archive      = {J_AOS},
  author       = {Denis Belomestny and Alexander Goldenshluger},
  doi          = {10.1214/20-AOS1969},
  journal      = {The Annals of Statistics},
  number       = {2},
  pages        = {615-649},
  shortjournal = {Ann. Statist.},
  title        = {Density deconvolution under general assumptions on the distribution of measurement errors},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Correction note: Higher order elicitability and osband’s
principle. <em>AOS</em>, <em>49</em>(1), 614. (<a
href="https://doi.org/10.1214/20-AOS2014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_AOS},
  author       = {Tobias Fissler and Johanna F. Ziegel},
  doi          = {10.1214/20-AOS2014},
  journal      = {The Annals of Statistics},
  number       = {1},
  pages        = {614},
  shortjournal = {Ann. Statist.},
  title        = {Correction note: Higher order elicitability and osband’s principle},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Correction note: “Optimal two-stage procedures for
estimating location and size of the maximum of a multivariate regression
function” ann. Statist. 40 (2012) 2850–2876. <em>AOS</em>,
<em>49</em>(1), 612–613. (<a
href="https://doi.org/10.1214/20-AOS1993">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We rectify a wrongly stated fact in the paper of Belitser, Ghosal and van Zanten (Ann. Statist. 40 (2012) 2850–2876).},
  archive      = {J_AOS},
  author       = {Eduard Belitser and Subhashis Ghosal and Harry van Zanten},
  doi          = {10.1214/20-AOS1993},
  journal      = {The Annals of Statistics},
  number       = {1},
  pages        = {612-613},
  shortjournal = {Ann. Statist.},
  title        = {Correction note: “Optimal two-stage procedures for estimating location and size of the maximum of a multivariate regression function” ann. statist. 40 (2012) 2850–2876},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Wasserstein <span
class="math inline"><em>F</em></span>-tests and confidence bands for the
fréchet regression of density response curves. <em>AOS</em>,
<em>49</em>(1), 590–611. (<a
href="https://doi.org/10.1214/20-AOS1971">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data consisting of samples of probability density functions are increasingly prevalent, necessitating the development of methodologies for their analysis that respect the inherent nonlinearities associated with densities. In many applications, density curves appear as functional response objects in a regression model with vector predictors. For such models, inference is key to understand the importance of density-predictor relationships, and the uncertainty associated with the estimated conditional mean densities, defined as conditional Fréchet means under a suitable metric. Using the Wasserstein geometry of optimal transport, we consider the Fréchet regression of density curve responses and develop tests for global and partial effects, as well as simultaneous confidence bands for estimated conditional mean densities. The asymptotic behavior of these objects is based on underlying functional central limit theorems within Wasserstein space, and we demonstrate that they are asymptotically of the correct size and coverage, with uniformly strong consistency of the proposed tests under sequences of contiguous alternatives. The accuracy of these methods, including nominal size, power and coverage, is assessed through simulations, and their utility is illustrated through a regression analysis of post-intracerebral hemorrhage hematoma densities and their associations with a set of clinical and radiological covariates.},
  archive      = {J_AOS},
  author       = {Alexander Petersen and Xi Liu and Afshin A. Divani},
  doi          = {10.1214/20-AOS1971},
  journal      = {The Annals of Statistics},
  number       = {1},
  pages        = {590-611},
  shortjournal = {Ann. Statist.},
  title        = {Wasserstein $F$-tests and confidence bands for the fréchet regression of density response curves},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sharp minimax distribution estimation for current status
censoring with or without missing. <em>AOS</em>, <em>49</em>(1),
568–589. (<a href="https://doi.org/10.1214/20-AOS1970">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nonparametric estimation of the cumulative distribution function and the probability density of a lifetime X modified by a current status censoring (CSC), including cases of right and left missing data, is a classical ill-posed problem with biased data. The biased nature of CSC data may preclude us from consistent estimation unless the biasing function is known or may be estimated, and its ill-posed nature slows down rates of convergence. Under a traditionally studied CSC, we observe a sample from $(Z,\Delta )$ where a continuous monitoring time $Z$ is independent of $X$, $\Delta :=I(X\leq Z)$ is the status, and the bias of observations is created by the density of $Z$ which is estimable. In presence of right or left missing, we observe corresponding samples from $(\Delta Z,\Delta )$ or $((1-\Delta )Z,\Delta )$; the data are again biased but now the density of $Z$ cannot be estimated from the data. As a result, to solve the estimation problem, either the density of $Z$ must be known (like in a controlled study) or an extra cross-sectional sampling of $Z$, which is typically simpler than an underlying CSC study, be conducted. The main aim of the paper is to develop for this biased and ill-posed problem the theory of efficient (sharp-minimax) estimation which is inspired by known results for the case of directly observed $X$. Among interesting aspects of the developed theory: (i) While sharp-minimax analysis of missing CSC may follow the classical Pinsker’s methodology, analysis of CSC requires a more complicated estimation procedure based on a special smoothing in both frequency and time domains; (ii) Efficient estimation requires solving an old-standing problem of approximating aperiodic Sobolev functions; (iii) If smoothness of the cdf of $X$ is known, then its rate-minimax estimation is possible even if the density of $Z$ is rougher. Real and simulated examples, as well as extensions of the core models to dependent $X$ and Z and case-control CSC, are presented.},
  archive      = {J_AOS},
  author       = {Sam Efromovich},
  doi          = {10.1214/20-AOS1970},
  journal      = {The Annals of Statistics},
  number       = {1},
  pages        = {568-589},
  shortjournal = {Ann. Statist.},
  title        = {Sharp minimax distribution estimation for current status censoring with or without missing},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A rule of thumb: Run lengths to false alarm of many types of
control charts run in parallel on dependent streams are asymptotically
independent. <em>AOS</em>, <em>49</em>(1), 557–567. (<a
href="https://doi.org/10.1214/20-AOS1968">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Consider a process that produces a series of independent identically distributed vectors. A change in an underlying state may become manifest in a modification of one or more of the marginal distributions. Often, the dependence structure between coordinates is unknown, impeding surveillance based on the joint distribution. A popular approach is to construct control charts for each coordinate separately and raise an alarm the first time any (or some) of the control charts signals. The difficulty is obtaining an expression for the overall average run length to false alarm (ARL2FA). We argue that despite the dependence structure, when the process is in control, for large ARLs to false alarm, run lengths of many types of control charts run in parallel are asymptotically independent. Furthermore, often, in-control run lengths are asymptotically exponentially distributed, enabling uncomplicated asymptotic expressions for the ARL2FA. We prove this assertion for certain Cusum and Shiryaev–Roberts-type control charts and illustrate it by simulations.},
  archive      = {J_AOS},
  author       = {Moshe Pollak},
  doi          = {10.1214/20-AOS1968},
  journal      = {The Annals of Statistics},
  number       = {1},
  pages        = {557-567},
  shortjournal = {Ann. Statist.},
  title        = {A rule of thumb: Run lengths to false alarm of many types of control charts run in parallel on dependent streams are asymptotically independent},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Concentration of kernel matrices with application to kernel
spectral clustering. <em>AOS</em>, <em>49</em>(1), 531–556. (<a
href="https://doi.org/10.1214/20-AOS1967">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the concentration of random kernel matrices around their mean. We derive nonasymptotic exponential concentration inequalities for Lipschitz kernels assuming that the data points are independent draws from a class of multivariate distributions on $\mathbb{R}^{d}$, including the strongly log-concave distributions under affine transformations. A feature of our result is that the data points need not have identical distributions or zero mean, which is key in certain applications such as clustering. Our bound for the Lipschitz kernels is dimension-free and sharp up to constants. For comparison, we also derive the companion result for the Euclidean (inner product) kernel for a class of sub-Gaussian distributions. A notable difference between the two cases is that, in contrast to the Euclidean kernel, in the Lipschitz case, the concentration inequality does not depend on the mean of the underlying vectors. As an application of these inequalities, we derive a bound on the misclassification rate of a kernel spectral clustering (KSC) algorithm, under a perturbed nonparametric mixture model. We show an example where this bound establishes the high-dimensional consistency (as $d\to \infty $) of the KSC, when applied with a Gaussian kernel, to a noisy model of nested nonlinear manifolds.},
  archive      = {J_AOS},
  author       = {Arash A. Amini and Zahra S. Razaee},
  doi          = {10.1214/20-AOS1967},
  journal      = {The Annals of Statistics},
  number       = {1},
  pages        = {531-556},
  shortjournal = {Ann. Statist.},
  title        = {Concentration of kernel matrices with application to kernel spectral clustering},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust estimation of superhedging prices. <em>AOS</em>,
<em>49</em>(1), 508–530. (<a
href="https://doi.org/10.1214/20-AOS1966">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider statistical estimation of superhedging prices using historical stock returns in a frictionless market with $d$ traded assets. We introduce a plug-in estimator based on empirical measures and show it is consistent but lacks suitable robustness. To address this, we propose novel estimators which use a larger set of martingale measures defined through a tradeoff between the radius of Wasserstein balls around the empirical measure and the allowed norm of martingale densities. We then extend our study, in part, to estimation of risk measures, to the case of markets with traded options, to a multi-period setting and to settings with model uncertainty. We also study convergence rates of estimators and convergence of super-hedging strategies.},
  archive      = {J_AOS},
  author       = {Jan Obłój and Johannes Wiesel},
  doi          = {10.1214/20-AOS1966},
  journal      = {The Annals of Statistics},
  number       = {1},
  pages        = {508-530},
  shortjournal = {Ann. Statist.},
  title        = {Robust estimation of superhedging prices},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Predictive inference with the jackknife+. <em>AOS</em>,
<em>49</em>(1), 486–507. (<a
href="https://doi.org/10.1214/20-AOS1965">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces the jackknife+, which is a novel method for constructing predictive confidence intervals. Whereas the jackknife outputs an interval centered at the predicted response of a test point, with the width of the interval determined by the quantiles of leave-one-out residuals, the jackknife+ also uses the leave-one-out predictions at the test point to account for the variability in the fitted regression function. Assuming exchangeable training samples, we prove that this crucial modification permits rigorous coverage guarantees regardless of the distribution of the data points, for any algorithm that treats the training points symmetrically. Such guarantees are not possible for the original jackknife and we demonstrate examples where the coverage rate may actually vanish. Our theoretical and empirical analysis reveals that the jackknife and the jackknife+ intervals achieve nearly exact coverage and have similar lengths whenever the fitting algorithm obeys some form of stability. Further, we extend the jackknife+ to $K$-fold cross validation and similarly establish rigorous coverage properties. Our methods are related to cross-conformal prediction proposed by Vovk (Ann. Math. Artif. Intell. 74 (2015) 9–28) and we discuss connections.},
  archive      = {J_AOS},
  author       = {Rina Foygel Barber and Emmanuel J. Candès and Aaditya Ramdas and Ryan J. Tibshirani},
  doi          = {10.1214/20-AOS1965},
  journal      = {The Annals of Statistics},
  number       = {1},
  pages        = {486-507},
  shortjournal = {Ann. Statist.},
  title        = {Predictive inference with the jackknife+},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Complex sampling designs: Uniform limit theorems and
applications. <em>AOS</em>, <em>49</em>(1), 459–485. (<a
href="https://doi.org/10.1214/20-AOS1964">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we develop a general approach to proving global and local uniform limit theorems for the Horvitz–Thompson empirical process arising from complex sampling designs. Global theorems such as Glivenko–Cantelli and Donsker theorems, and local theorems such as local asymptotic modulus and related ratio-type limit theorems are proved for both the Horvitz–Thompson empirical process, and its calibrated version. Limit theorems of other variants and their conditional versions are also established. Our approach reveals an interesting feature: the problem of deriving uniform limit theorems for the Horvitz–Thompson empirical process is essentially no harder than the problem of establishing the corresponding finite-dimensional limit theorems, once the usual complexity conditions on the function class are satisfied. These global and local uniform limit theorems are then applied to important statistical problems including (i) $M$-estimation, (ii) $Z$-estimation and (iii) frequentist theory of pseudo-Bayes procedures, all with weighted likelihood, to illustrate their wide applicability.},
  archive      = {J_AOS},
  author       = {Qiyang Han and Jon A. Wellner},
  doi          = {10.1214/20-AOS1964},
  journal      = {The Annals of Statistics},
  number       = {1},
  pages        = {459-485},
  shortjournal = {Ann. Statist.},
  title        = {Complex sampling designs: Uniform limit theorems and applications},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Asymmetry helps: Eigenvalue and eigenvector analyses of
asymmetrically perturbed low-rank matrices. <em>AOS</em>,
<em>49</em>(1), 435–458. (<a
href="https://doi.org/10.1214/20-AOS1963">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper is concerned with the interplay between statistical asymmetry and spectral methods. Suppose we are interested in estimating a rank-1 and symmetric matrix $\boldsymbol{M}^{\star }\in \mathbb{R}^{n\times n}$, yet only a randomly perturbed version $\boldsymbol{M}$ is observed. The noise matrix $\boldsymbol{M}-\boldsymbol{M}^{\star }$ is composed of independent (but not necessarily homoscedastic) entries and is, therefore, not symmetric in general. This might arise if, for example, when we have two independent samples for each entry of $\boldsymbol{M}^{\star }$ and arrange them in an asymmetric fashion. The aim is to estimate the leading eigenvalue and the leading eigenvector of $\boldsymbol{M}^{\star }$. We demonstrate that the leading eigenvalue of the data matrix $\boldsymbol{M}$ can be $O(\sqrt{n})$ times more accurate (up to some log factor) than its (unadjusted) leading singular value of $\boldsymbol{M}$ in eigenvalue estimation. Moreover, the eigen-decomposition approach is fully adaptive to heteroscedasticity of noise, without the need of any prior knowledge about the noise distributions. In a nutshell, this curious phenomenon arises since the statistical asymmetry automatically mitigates the bias of the eigenvalue approach, thus eliminating the need of careful bias correction. Additionally, we develop appealing nonasymptotic eigenvector perturbation bounds; in particular, we are able to bound the perturbation of any linear function of the leading eigenvector of $\boldsymbol{M}$ (e.g., entrywise eigenvector perturbation). We also provide partial theory for the more general rank-$r$ case. The takeaway message is this: arranging the data samples in an asymmetric manner and performing eigendecomposition could sometimes be quite beneficial.},
  archive      = {J_AOS},
  author       = {Yuxin Chen and Chen Cheng and Jianqing Fan},
  doi          = {10.1214/20-AOS1963},
  journal      = {The Annals of Statistics},
  number       = {1},
  pages        = {435-458},
  shortjournal = {Ann. Statist.},
  title        = {Asymmetry helps: Eigenvalue and eigenvector analyses of asymmetrically perturbed low-rank matrices},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Classification accuracy as a proxy for two-sample testing.
<em>AOS</em>, <em>49</em>(1), 411–434. (<a
href="https://doi.org/10.1214/20-AOS1962">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When data analysts train a classifier and check if its accuracy is significantly different from chance, they are implicitly performing a two-sample test. We investigate the statistical properties of this flexible approach in the high-dimensional setting. We prove two results that hold for all classifiers in any dimensions: if its true error remains $\epsilon $-better than chance for some $\epsilon &gt;0$ as $d,n\to \infty $, then (a) the permutation-based test is consistent (has power approaching to one), (b) a computationally efficient test based on a Gaussian approximation of the null distribution is also consistent. To get a finer understanding of the rates of consistency, we study a specialized setting of distinguishing Gaussians with mean-difference $\delta $ and common (known or unknown) covariance $\Sigma $, when $d/n\to c\in (0,\infty )$. We study variants of Fisher’s linear discriminant analysis (LDA) such as “naive Bayes” in a nontrivial regime when $\epsilon \to 0$ (the Bayes classifier has true accuracy approaching 1/2), and contrast their power with corresponding variants of Hotelling’s test. Surprisingly, the expressions for their power match exactly in terms of $n$, $d$, $\delta $, $\Sigma $, and the LDA approach is only worse by a constant factor, achieving an asymptotic relative efficiency (ARE) of $1/\sqrt{\pi }$ for balanced samples. We also extend our results to high-dimensional elliptical distributions with finite kurtosis. Other results of independent interest include minimax lower bounds, and the optimality of Hotelling’s test when $d=o(n)$. Simulation results validate our theory, and we present practical takeaway messages along with natural open problems.},
  archive      = {J_AOS},
  author       = {Ilmun Kim and Aaditya Ramdas and Aarti Singh and Larry Wasserman},
  doi          = {10.1214/20-AOS1962},
  journal      = {The Annals of Statistics},
  number       = {1},
  pages        = {411-434},
  shortjournal = {Ann. Statist.},
  title        = {Classification accuracy as a proxy for two-sample testing},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust multivariate mean estimation: The optimality of
trimmed mean. <em>AOS</em>, <em>49</em>(1), 393–410. (<a
href="https://doi.org/10.1214/20-AOS1961">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of estimating the mean of a random vector based on i.i.d. observations and adversarial contamination. We introduce a multivariate extension of the trimmed-mean estimator and show its optimal performance under minimal conditions.},
  archive      = {J_AOS},
  author       = {Gábor Lugosi and Shahar Mendelson},
  doi          = {10.1214/20-AOS1961},
  journal      = {The Annals of Statistics},
  number       = {1},
  pages        = {393-410},
  shortjournal = {Ann. Statist.},
  title        = {Robust multivariate mean estimation: The optimality of trimmed mean},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Singular vector and singular subspace distribution for the
matrix denoising model. <em>AOS</em>, <em>49</em>(1), 370–392. (<a
href="https://doi.org/10.1214/20-AOS1960">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study the matrix denoising model $Y=S+X$, where $S$ is a low rank deterministic signal matrix and $X$ is a random noise matrix, and both are $M\times n$. In the scenario that $M$ and $n$ are comparably large and the signals are supercritical, we study the fluctuation of the outlier singular vectors of $Y$, under fully general assumptions on the structure of $S$ and the distribution of $X$. More specifically, we derive the limiting distribution of angles between the principal singular vectors of $Y$ and their deterministic counterparts, the singular vectors of $S$. Further, we also derive the distribution of the distance between the subspace spanned by the principal singular vectors of $Y$ and that spanned by the singular vectors of $S$. It turns out that the limiting distributions depend on the structure of the singular vectors of $S$ and the distribution of $X$, and thus they are nonuniversal. Statistical applications of our results to singular vector and singular subspace inferences are also discussed.},
  archive      = {J_AOS},
  author       = {Zhigang Bao and Xiucai Ding and and Ke Wang},
  doi          = {10.1214/20-AOS1960},
  journal      = {The Annals of Statistics},
  number       = {1},
  pages        = {370-392},
  shortjournal = {Ann. Statist.},
  title        = {Singular vector and singular subspace distribution for the matrix denoising model},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Asymptotics for spherical functional autoregressions.
<em>AOS</em>, <em>49</em>(1), 346–369. (<a
href="https://doi.org/10.1214/20-AOS1959">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we investigate a class of spherical functional autoregressive processes, and we discuss the estimation of the corresponding autoregressive kernels. In particular, we first establish a consistency result (in mean-square and sup norm), then a quantitative central limit theorem (in Wasserstein distance), and finally a weak convergence result, under more restrictive regularity conditions. Our results are validated by a small numerical investigation.},
  archive      = {J_AOS},
  author       = {Alessia Caponera and Domenico Marinucci},
  doi          = {10.1214/20-AOS1959},
  journal      = {The Annals of Statistics},
  number       = {1},
  pages        = {346-369},
  shortjournal = {Ann. Statist.},
  title        = {Asymptotics for spherical functional autoregressions},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Estimation of low-rank matrices via approximate message
passing. <em>AOS</em>, <em>49</em>(1), 321–345. (<a
href="https://doi.org/10.1214/20-AOS1958">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Consider the problem of estimating a low-rank matrix when its entries are perturbed by Gaussian noise, a setting that is also known as “spiked model” or “deformed random matrix.” If the empirical distribution of the entries of the spikes is known, optimal estimators that exploit this knowledge can substantially outperform simple spectral approaches. Recent work characterizes the asymptotic accuracy of Bayes-optimal estimators in the high-dimensional limit. In this paper, we present a practical algorithm that can achieve Bayes-optimal accuracy above the spectral threshold. A bold conjecture from statistical physics posits that no polynomial-time algorithm achieves optimal error below the same threshold (unless the best estimator is trivial). Our approach uses Approximate Message Passing (AMP) in conjunction with a spectral initialization. AMP algorithms have proved successful in a variety of statistical estimation tasks, and are amenable to exact asymptotic analysis via state evolution. Unfortunately, state evolution is uninformative when the algorithm is initialized near an unstable fixed point, as often happens in low-rank matrix estimation problems. We develop a new analysis of AMP that allows for spectral initializations, and builds on a decoupling between the outlier eigenvectors and the bulk in the spiked random matrix model. Our main theorem is general and applies beyond matrix estimation. However, we use it to derive detailed predictions for the problem of estimating a rank-one matrix in noise. Special cases of this problem are closely related—via universality arguments—to the network community detection problem for two asymmetric communities. For general rank-one models, we show that AMP can be used to construct confidence intervals and control false discovery rate. We provide illustrations of the general methodology by considering the cases of sparse low-rank matrices and of block-constant low-rank matrices with symmetric blocks (we refer to the latter as to the “Gaussian block model”).},
  archive      = {J_AOS},
  author       = {Andrea Montanari and Ramji Venkataramanan},
  doi          = {10.1214/20-AOS1958},
  journal      = {The Annals of Statistics},
  number       = {1},
  pages        = {321-345},
  shortjournal = {Ann. Statist.},
  title        = {Estimation of low-rank matrices via approximate message passing},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multiple block sizes and overlapping blocks for multivariate
time series extremes. <em>AOS</em>, <em>49</em>(1), 295–320. (<a
href="https://doi.org/10.1214/20-AOS1957">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Block maxima methods constitute a fundamental part of the statistical toolbox in extreme value analysis. However, most of the corresponding theory is derived under the simplifying assumption that block maxima are independent observations from a genuine extreme value distribution. In practice, however, block sizes are finite and observations from different blocks are dependent. Theory respecting the latter complications is not well developed, and, in the multivariate case, has only recently been established for disjoint blocks of a single block size. We show that using overlapping blocks instead of disjoint blocks leads to a uniform improvement in the asymptotic variance of the multivariate empirical distribution function of rescaled block maxima and any smooth functionals thereof (such as the empirical copula), without any sacrifice in the asymptotic bias. We further derive functional central limit theorems for multivariate empirical distribution functions and empirical copulas that are uniform in the block size parameter, which seems to be the first result of this kind for estimators based on block maxima in general. The theory allows for various aggregation schemes over multiple block sizes, leading to substantial improvements over the single block length case and opens the door to further methodology developments. In particular, we consider bias correction procedures that can improve the convergence rates of extreme-value estimators and shed some new light on estimation of the second-order parameter when the main purpose is bias correction.},
  archive      = {J_AOS},
  author       = {Nan Zou and Stanislav Volgushev and Axel Bücher},
  doi          = {10.1214/20-AOS1957},
  journal      = {The Annals of Statistics},
  number       = {1},
  pages        = {295-320},
  shortjournal = {Ann. Statist.},
  title        = {Multiple block sizes and overlapping blocks for multivariate time series extremes},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Estimating minimum effect with outlier selection.
<em>AOS</em>, <em>49</em>(1), 272–294. (<a
href="https://doi.org/10.1214/20-AOS1956">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce one-sided versions of Huber’s contamination model, in which corrupted samples tend to take larger values than uncorrupted ones. Two intertwined problems are addressed: estimation of the mean of the uncorrupted samples (minimum effect) and selection of the corrupted samples (outliers). Regarding estimation of the minimum effect, we derive the minimax risks and introduce estimators that are adaptive with respect to the unknown number of contaminations. The optimal convergence rates differ from the ones in the classical Huber contamination model. This fact uncovers the effect of the one-sided structural assumption of the contaminations. As for the problem of selecting the outliers, we formulate the problem in a multiple testing framework for which the location and scaling of the null hypotheses are unknown. We rigorously prove that estimating the null hypothesis while maintaining a theoretical guarantee on the amount of the falsely selected outliers is possible, both through false discovery rate (FDR) and through post hoc bounds. As a by-product, we address a long-standing open issue on FDR control under equi-correlation, which reinforces the interest of removing dependency in such a setting.},
  archive      = {J_AOS},
  author       = {Alexandra Carpentier and Sylvain Delattre and Etienne Roquain and Nicolas Verzelen},
  doi          = {10.1214/20-AOS1956},
  journal      = {The Annals of Statistics},
  number       = {1},
  pages        = {272-294},
  shortjournal = {Ann. Statist.},
  title        = {Estimating minimum effect with outlier selection},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Wordlength enumerator for fractional factorial designs.
<em>AOS</em>, <em>49</em>(1), 255–271. (<a
href="https://doi.org/10.1214/20-AOS1955">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While the minimum aberration criterion is popular for selecting good designs with qualitative factors under an ANOVA model, the minimum $\beta$-aberration criterion is more suitable for selecting designs with quantitative factors under a polynomial model. In this paper, we propose the concept of wordlength enumerator to unify these two criteria. The wordlength enumerator is defined as an average similarity of contrasts among all possible pairs of runs. The wordlength enumerator is easy and fast to compute, and can be used to compare and rank designs efficiently. Based on the wordlength enumerator, we develop simple and fast methods for calculating both the generalized wordlength pattern and the $\beta$-wordlength pattern. We further obtain a lower bound of the wordlength enumerator for three-level designs and characterize the combinatorial structure of designs achieving the lower bound. Finally, we propose two methods for constructing supersaturated designs that have both generalized minimum aberration and minimum $\beta$-aberration.},
  archive      = {J_AOS},
  author       = {Yu Tang and Hongquan Xu},
  doi          = {10.1214/20-AOS1955},
  journal      = {The Annals of Statistics},
  number       = {1},
  pages        = {255-271},
  shortjournal = {Ann. Statist.},
  title        = {Wordlength enumerator for fractional factorial designs},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Convergence of covariance and spectral density estimates for
high-dimensional locally stationary processes. <em>AOS</em>,
<em>49</em>(1), 233–254. (<a
href="https://doi.org/10.1214/20-AOS1954">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Covariances and spectral density functions play a fundamental role in the theory of time series. There is a well-developed asymptotic theory for their estimates for low-dimensional stationary processes. For high-dimensional nonstationary processes, however, many important problems on their asymptotic behaviors are still unanswered. This paper presents a systematic asymptotic theory for the estimates of time-varying second-order statistics for a general class of high-dimensional locally stationary processes. Using the framework of functional dependence measure, we derive convergence rates of the estimates which depend on the sample size $T$, the dimension $p$, the moment condition and the dependence of the underlying processes.},
  archive      = {J_AOS},
  author       = {Danna Zhang and Wei Biao Wu},
  doi          = {10.1214/20-AOS1954},
  journal      = {The Annals of Statistics},
  number       = {1},
  pages        = {233-254},
  shortjournal = {Ann. Statist.},
  title        = {Convergence of covariance and spectral density estimates for high-dimensional locally stationary processes},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Optimal change point detection and localization in sparse
dynamic networks. <em>AOS</em>, <em>49</em>(1), 203–232. (<a
href="https://doi.org/10.1214/20-AOS1953">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the problem of change point localization in dynamic networks models. We assume that we observe a sequence of independent adjacency matrices of the same size, each corresponding to a realization of an unknown inhomogeneous Bernoulli model. The underlying distribution of the adjacency matrices are piecewise constant, and may change over a subset of the time points, called change points. We are concerned with recovering the unknown number and positions of the change points. In our model setting, we allow for all the model parameters to change with the total number of time points, including the network size, the minimal spacing between consecutive change points, the magnitude of the smallest change and the degree of sparsity of the networks. We first identify a region of impossibility in the space of the model parameters such that no change point estimator is provably consistent if the data are generated according to parameters falling in that region. We propose a computationally-simple algorithm for network change point localization, called network binary segmentation, that relies on weighted averages of the adjacency matrices. We show that network binary segmentation is consistent over a range of the model parameters that nearly cover the complement of the impossibility region, thus demonstrating the existence of a phase transition for the problem at hand. Next, we devise a more sophisticated algorithm based on singular value thresholding, called local refinement, that delivers more accurate estimates of the change point locations. Under appropriate conditions, local refinement guarantees a minimax optimal rate for network change point localization while remaining computationally feasible.},
  archive      = {J_AOS},
  author       = {Daren Wang and Yi Yu and Alessandro Rinaldo},
  doi          = {10.1214/20-AOS1953},
  journal      = {The Annals of Statistics},
  number       = {1},
  pages        = {203-232},
  shortjournal = {Ann. Statist.},
  title        = {Optimal change point detection and localization in sparse dynamic networks},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Frequentist validity of bayesian limits. <em>AOS</em>,
<em>49</em>(1), 182–202. (<a
href="https://doi.org/10.1214/20-AOS1952">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To the frequentist who computes posteriors, not all priors are useful asymptotically: in this paper, a Bayesian perspective on test sequences is proposed and Schwartz’s Kullback–Leibler condition is generalised to widen the range of frequentist applications of posterior convergence. With Bayesian tests and a weakened form of contiguity termed remote contiguity, we prove simple and fully general frequentist theorems, for posterior consistency and rates of convergence, for consistency of posterior odds in model selection, and for conversion of sequences of credible sets into sequences of confidence sets with asymptotic coverage one. For frequentist uncertainty quantification, this means that a prior inducing remote contiguity allows one to enlarge credible sets of calculated, simulated or approximated posteriors to obtain asymptotically consistent confidence sets.},
  archive      = {J_AOS},
  author       = {B. J. K. Kleijn},
  doi          = {10.1214/20-AOS1952},
  journal      = {The Annals of Statistics},
  number       = {1},
  pages        = {182-202},
  shortjournal = {Ann. Statist.},
  title        = {Frequentist validity of bayesian limits},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Asymptotically independent u-statistics in high-dimensional
testing. <em>AOS</em>, <em>49</em>(1), 154–181. (<a
href="https://doi.org/10.1214/20-AOS1951">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many high-dimensional hypothesis tests aim to globally examine marginal or low-dimensional features of a high-dimensional joint distribution, such as testing of mean vectors, covariance matrices and regression coefficients. This paper constructs a family of U-statistics as unbiased estimators of the $\ell_{p}$-norms of those features. We show that under the null hypothesis, the U-statistics of different finite orders are asymptotically independent and normally distributed. Moreover, they are also asymptotically independent with the maximum-type test statistic, whose limiting distribution is an extreme value distribution. Based on the asymptotic independence property, we propose an adaptive testing procedure which combines $p$-values computed from the U-statistics of different orders. We further establish power analysis results and show that the proposed adaptive procedure maintains high power against various alternatives.},
  archive      = {J_AOS},
  author       = {Yinqiu He and Gongjun Xu and Chong Wu and Wei Pan},
  doi          = {10.1214/20-AOS1951},
  journal      = {The Annals of Statistics},
  number       = {1},
  pages        = {154-181},
  shortjournal = {Ann. Statist.},
  title        = {Asymptotically independent U-statistics in high-dimensional testing},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptation in multivariate log-concave density estimation.
<em>AOS</em>, <em>49</em>(1), 129–153. (<a
href="https://doi.org/10.1214/20-AOS1950">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the adaptation properties of the multivariate log-concave maximum likelihood estimator over three subclasses of log-concave densities. The first consists of densities with polyhedral support whose logarithms are piecewise affine. The complexity of such densities $f$ can be measured in terms of the sum $\Gamma(f)$ of the numbers of facets of the subdomains in the polyhedral subdivision of the support induced by $f$. Given $n$ independent observations from a $d$-dimensional log-concave density with $d\in{2,3}$, we prove a sharp oracle inequality, which in particular implies that the Kullback–Leibler risk of the log-concave maximum likelihood estimator for such densities is bounded above by $\Gamma(f)/n$, up to a polylogarithmic factor. Thus, the rate can be essentially parametric, even in this multivariate setting. For the second type of adaptation, we consider densities that are bounded away from zero on a polytopal support; we show that up to polylogarithmic factors, the log-concave maximum likelihood estimator attains the rate $n^{-4/7}$ when $d=3$, which is faster than the worst-case rate of $n^{-1/2}$. Finally, our third type of subclass consists of densities whose contours are well separated; these new classes are constructed to be affine invariant and turn out to contain a wide variety of densities, including those that satisfy Hölder regularity conditions. Here, we prove another sharp oracle inequality, which reveals in particular that the log-concave maximum likelihood estimator attains a risk bound of order $n^{-\min (\frac{\beta+3}{\beta+7},\frac{4}{7})}$ when $d=3$ over the class of $\beta$-Hölder log-concave densities with $\beta &gt;1$, again up to a polylogarithmic factor.},
  archive      = {J_AOS},
  author       = {Oliver Y. Feng and Adityanand Guntuboyina and Arlene K. H. Kim and Richard J. Samworth},
  doi          = {10.1214/20-AOS1950},
  journal      = {The Annals of Statistics},
  number       = {1},
  pages        = {129-153},
  shortjournal = {Ann. Statist.},
  title        = {Adaptation in multivariate log-concave density estimation},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Transfer learning for nonparametric classification: Minimax
rate and adaptive classifier. <em>AOS</em>, <em>49</em>(1), 100–128. (<a
href="https://doi.org/10.1214/20-AOS1949">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human learners have the natural ability to use knowledge gained in one setting for learning in a different but related setting. This ability to transfer knowledge from one task to another is essential for effective learning. In this paper, we study transfer learning in the context of nonparametric classification based on observations from different distributions under the posterior drift model, which is a general framework and arises in many practical problems. We first establish the minimax rate of convergence and construct a rate-optimal two-sample weighted $K$-NN classifier. The results characterize precisely the contribution of the observations from the source distribution to the classification task under the target distribution. A data-driven adaptive classifier is then proposed and is shown to simultaneously attain within a logarithmic factor of the optimal rate over a large collection of parameter spaces. Simulation studies and real data applications are carried out where the numerical results further illustrate the theoretical analysis. Extensions to the case of multiple source distributions are also considered.},
  archive      = {J_AOS},
  author       = {T. Tony Cai and Hongji Wei},
  doi          = {10.1214/20-AOS1949},
  journal      = {The Annals of Statistics},
  number       = {1},
  pages        = {100-128},
  shortjournal = {Ann. Statist.},
  title        = {Transfer learning for nonparametric classification: Minimax rate and adaptive classifier},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Statistically optimal and computationally efficient low rank
tensor completion from noisy entries. <em>AOS</em>, <em>49</em>(1),
76–99. (<a href="https://doi.org/10.1214/20-AOS1942">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we develop methods for estimating a low rank tensor from noisy observations on a subset of its entries to achieve both statistical and computational efficiencies. There have been a lot of recent interests in this problem of noisy tensor completion. Much of the attention has been focused on the fundamental computational challenges often associated with problems involving higher order tensors, yet very little is known about their statistical performance. To fill in this void, in this article, we characterize the fundamental statistical limits of noisy tensor completion by establishing minimax optimal rates of convergence for estimating a $k$th order low rank tensor under the general $\ell _{p}$ ($1\le p\le 2$) norm which suggest significant room for improvement over the existing approaches. Furthermore, we propose a polynomial-time computable estimating procedure based upon power iteration and a second-order spectral initialization that achieves the optimal rates of convergence. Our method is fairly easy to implement and numerical experiments are presented to further demonstrate the practical merits of our estimator.},
  archive      = {J_AOS},
  author       = {Dong Xia and Ming Yuan and Cun-Hui Zhang},
  doi          = {10.1214/20-AOS1942},
  journal      = {The Annals of Statistics},
  number       = {1},
  pages        = {76-99},
  shortjournal = {Ann. Statist.},
  title        = {Statistically optimal and computationally efficient low rank tensor completion from noisy entries},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Concordance and value information criteria for optimal
treatment decision. <em>AOS</em>, <em>49</em>(1), 49–75. (<a
href="https://doi.org/10.1214/19-AOS1908">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Personalized medicine is a medical procedure that receives considerable scientific and commercial attention. The goal of personalized medicine is to assign the optimal treatment regime for each individual patient, according to his/her personal prognostic information. When there are a large number of pretreatment variables, it is crucial to identify those important variables that are necessary for treatment decision making. In this paper, we study two information criteria: the concordance and value information criteria, for variable selection in optimal treatment decision making. We consider both fixed-$p$ and high dimensional settings, and show our information criteria are consistent in model/tuning parameter selection. We further apply our information criteria to four estimation approaches, including robust learning, concordance-assisted learning, penalized A-learning and sparse concordance-assisted learning, and demonstrate the empirical performance of our methods by simulations.},
  archive      = {J_AOS},
  author       = {Chengchun Shi and Rui Song and Wenbin Lu},
  doi          = {10.1214/19-AOS1908},
  journal      = {The Annals of Statistics},
  number       = {1},
  pages        = {49-75},
  shortjournal = {Ann. Statist.},
  title        = {Concordance and value information criteria for optimal treatment decision},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Asymptotic optimality in stochastic optimization.
<em>AOS</em>, <em>49</em>(1), 21–48. (<a
href="https://doi.org/10.1214/19-AOS1831">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study local complexity measures for stochastic convex optimization problems, providing a local minimax theory analogous to that of Hájek and Le Cam for classical statistical problems. We give complementary optimality results, developing fully online methods that adaptively achieve optimal convergence guarantees. Our results provide function-specific lower bounds and convergence results that make precise a correspondence between statistical difficulty and the geometric notion of tilt-stability from optimization. As part of this development, we show how variants of Nesterov’s dual averaging—a stochastic gradient-based procedure—guarantee finite time identification of constraints in optimization problems, while stochastic gradient procedures fail. Additionally, we highlight a gap between problems with linear and nonlinear constraints: standard stochastic-gradient-based procedures are suboptimal even for the simplest nonlinear constraints, necessitating the development of asymptotically optimal Riemannian stochastic gradient methods.},
  archive      = {J_AOS},
  author       = {John C. Duchi and Feng Ruan},
  doi          = {10.1214/19-AOS1831},
  journal      = {The Annals of Statistics},
  number       = {1},
  pages        = {21-48},
  shortjournal = {Ann. Statist.},
  title        = {Asymptotic optimality in stochastic optimization},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On the optimality of sliced inverse regression in high
dimensions. <em>AOS</em>, <em>49</em>(1), 1–20. (<a
href="https://doi.org/10.1214/19-AOS1813">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The central subspace of a pair of random variables $(y,\boldsymbol{x})\in \mathbb{R}^{p+1}$ is the minimal subspace $\mathcal{S}$ such that $y\perp\!\!\!\!\!\perp \boldsymbol{x}|P_{\mathcal{S}}\boldsymbol{x}$. In this paper, we consider the minimax rate of estimating the central space under the multiple index model $y=f(\boldsymbol{\beta }_{1}^{\tau }\boldsymbol{x},\boldsymbol{\beta }_{2}^{\tau }\boldsymbol{x},\ldots,\boldsymbol{\beta }_{d}^{\tau }\boldsymbol{x},\epsilon )$ with at most $s$ active predictors, where $\boldsymbol{x}\sim N(0,\boldsymbol{\Sigma })$ for some class of $\boldsymbol{\Sigma }$. We first introduce a large class of models depending on the smallest nonzero eigenvalue $\lambda $ of $\operatorname{var}(\mathbb{E}[\boldsymbol{x}|y])$, over which we show that an aggregated estimator based on the SIR procedure converges at rate $d\wedge ((sd+s\log (ep/s))/(n\lambda ))$. We then show that this rate is optimal in two scenarios, the single index models and the multiple index models with fixed central dimension $d$ and fixed $\lambda $. By assuming a technical conjecture, we can show that this rate is also optimal for multiple index models with bounded dimension of the central space.},
  archive      = {J_AOS},
  author       = {Qian Lin and Xinran Li and Dongming Huang and Jun S. Liu},
  doi          = {10.1214/19-AOS1813},
  journal      = {The Annals of Statistics},
  number       = {1},
  pages        = {1-20},
  shortjournal = {Ann. Statist.},
  title        = {On the optimality of sliced inverse regression in high dimensions},
  volume       = {49},
  year         = {2021},
}
</textarea>
</details></li>
</ul>

</body>
</html>
