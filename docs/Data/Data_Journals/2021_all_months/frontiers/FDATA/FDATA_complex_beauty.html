<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>FDATA_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="fdata---124">FDATA - 124</h2>
<ul>
<li><details>
<summary>
(2021). Editorial: Data-driven solutions for smart grids.
<em>FDATA</em>, <em>4</em>, 815686. (<a
href="https://doi.org/10.3389/fdata.2021.815686">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_FDATA},
  author       = {Milano, F. and Vaccaro, A. and Manana, M.},
  doi          = {10.3389/fdata.2021.815686},
  journal      = {Frontiers in Big Data},
  month        = {12},
  pages        = {815686},
  shortjournal = {Front. Big Data},
  title        = {Editorial: Data-driven solutions for smart grids},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Social security number holders in the united states,
1909-2019. <em>FDATA</em>, <em>4</em>, 802256. (<a
href="https://doi.org/10.3389/fdata.2021.802256">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, a social security number (SSN) is held by almost every legal resident of the United States and works as an important numbering system. However, this was not the case in the early years of the Social Security program and historical changes in SSN holder rates had not been examined sufficiently. It is important to understand the changes in health policies and situations. Thus, the present article examined historical changes in the rates of SSN holders in the United States between 1909 and 2019. Analyses demonstrated that the rates clearly increased. Specifically, in Phase 1 (1909-1919), the rates were low in the early period, but they increased markedly. In Phase 2 (1919-1952), the rates continued to increase gradually. In Phase 3 (1952-2019), the rates were almost 100% and reached saturation. This basic information leads to a better understanding of the health policies and situations, contributing to medical and social science research.},
  archive      = {J_FDATA},
  author       = {Ogihara, Yuji},
  doi          = {10.3389/fdata.2021.802256},
  journal      = {Frontiers in Big Data},
  month        = {12},
  pages        = {802256},
  shortjournal = {Front. Big Data},
  title        = {Social security number holders in the united states, 1909-2019},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DroughtCast: A machine learning forecast of the united
states drought monitor. <em>FDATA</em>, <em>4</em>, 773478. (<a
href="https://doi.org/10.3389/fdata.2021.773478">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Drought is one of the most ecologically and economically devastating natural phenomena affecting the United States, causing the U.S. economy billions of dollars in damage, and driving widespread degradation of ecosystem health. Many drought indices are implemented to monitor the current extent and status of drought so stakeholders such as farmers and local governments can appropriately respond. Methods to forecast drought conditions weeks to months in advance are less common but would provide a more effective early warning system to enhance drought response, mitigation, and adaptation planning. To resolve this issue, we introduce DroughtCast, a machine learning framework for forecasting the United States Drought Monitor (USDM). DroughtCast operates on the knowledge that recent anomalies in hydrology and meteorology drive future changes in drought conditions. We use simulated meteorology and satellite observed soil moisture as inputs into a recurrent neural network to accurately forecast the USDM between 1 and 12 weeks into the future. Our analysis shows that precipitation, soil moisture, and temperature are the most important input variables when forecasting future drought conditions. Additionally, a case study of the 2017 Northern Plains Flash Drought shows that DroughtCast was able to forecast a very extreme drought event up to 12 weeks before its onset. Given the favorable forecasting skill of the model, DroughtCast may provide a promising tool for land managers and local governments in preparing for and mitigating the effects of drought.},
  archive      = {J_FDATA},
  author       = {Brust, Colin and Kimball, John S. and Maneta, Marco P. and Jencso, Kelsey and Reichle, Rolf H.},
  doi          = {10.3389/fdata.2021.773478},
  journal      = {Frontiers in Big Data},
  month        = {12},
  pages        = {773478},
  shortjournal = {Front. Big Data},
  title        = {DroughtCast: A machine learning forecast of the united states drought monitor},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Are you willing to self-disclose for science? Effects of
privacy awareness and trust in privacy on self-disclosure of personal
and health data in online scientific studies—an experimental study.
<em>FDATA</em>, <em>4</em>, 763196. (<a
href="https://doi.org/10.3389/fdata.2021.763196">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Digital interactions via the internet have become the norm rather than the exception in our global society. Concerns have been raised about human-centered privacy and the often unreflected self-disclosure behavior of internet users. This study on human-centered privacy follows two major aims: first, investigate the willingness of university students (as digital natives) to disclose private data and information about their person, social and academic life, their mental health as well as their health behavior habits, when taking part as a volunteer in a scientific online survey. Second, examine to what extent the participants’ self-disclosure behavior can be modulated by experimental induction of privacy awareness (PA) or trust in privacy (TIP) or a combination of both (PA and TIP). In addition, the role of human factors such as personality traits, gender or mental health (e.g., self-reported depressive symptoms) on self-disclosure behavior was explored. Participants were randomly assigned to four experimental groups. In group A (n = 50, 7 males), privacy awareness (PA) was induced implicitly by the inclusion of privacy concern items. In group B (n = 43, 6 males), trust in privacy (TIP) was experimentally induced by buzzwords and by visual TIP primes promising safe data storage. Group C (n = 79, 12 males) received both, PA and TIP induction, while group D (n = 55, 9 males) served as control group. Participants had the choice to answer the survey items by agreeing to one of a number of possible answers including the options to refrain from self-disclosure by choosing the response options “don’t know” or “no answer.” Self-disclosure among participants was high irrespective of experimental group and irrespective of psychological domains of the information provided. The results of this study suggest that willingness of volunteers to self-disclose private data in a scientific online study cannot simply be overruled or changed by any of the chosen experimental privacy manipulations. The present results extend the previous literature on human-centered privacy and despite limitations can give important insights into self-disclosure behavior of young people and the privacy paradox.},
  archive      = {J_FDATA},
  author       = {Herbert, Cornelia and Marschin, Verena and Erb, Benjamin and Meißner, Dominik and Aufheimer, Maria and Bösch, Christoph},
  doi          = {10.3389/fdata.2021.763196},
  journal      = {Frontiers in Big Data},
  month        = {12},
  pages        = {763196},
  shortjournal = {Front. Big Data},
  title        = {Are you willing to self-disclose for science? effects of privacy awareness and trust in privacy on self-disclosure of personal and health data in online scientific Studies—An experimental study},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). High precision mammography lesion identification from
imprecise medical annotations. <em>FDATA</em>, <em>4</em>, 742779. (<a
href="https://doi.org/10.3389/fdata.2021.742779">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Breast cancer screening using Mammography serves as the earliest defense against breast cancer, revealing anomalous tissue years before it can be detected through physical screening. Despite the use of high resolution radiography, the presence of densely overlapping patterns challenges the consistency of human-driven diagnosis and drives interest in leveraging state-of-art localization ability of deep convolutional neural networks (DCNN). The growing availability of digitized clinical archives enables the training of deep segmentation models, but training using the most widely available form of coarse hand-drawn annotations works against learning the precise boundary of cancerous tissue in evaluation, while producing results that are more aligned with the annotations rather than the underlying lesions. The expense of collecting high quality pixel-level data in the field of medical science makes this even more difficult. To surmount this fundamental challenge, we propose LatentCADx, a deep learning segmentation model capable of precisely annotating cancer lesions underlying hand-drawn annotations, which we procedurally obtain using joint classification training and a strict segmentation penalty. We demonstrate the capability of LatentCADx on a publicly available dataset of 2,620 Mammogram case files, where LatentCADx obtains classification ROC of 0.97, AP of 0.87, and segmentation AP of 0.75 (IOU = 0.5), giving comparable or better performance than other models. Qualitative and precision evaluation of LatentCADx annotations on validation samples reveals that LatentCADx increases the specificity of segmentations beyond that of existing models trained on hand-drawn annotations, with pixel level specificity reaching a staggering value of 0.90. It also obtains sharp boundary around lesions unlike other methods, reducing the confused pixels in the output by more than 60%.},
  archive      = {J_FDATA},
  author       = {An, Ulzee and Bhardwaj, Ankit and Shameer, Khader and Subramanian, Lakshminarayanan},
  doi          = {10.3389/fdata.2021.742779},
  journal      = {Frontiers in Big Data},
  month        = {12},
  pages        = {742779},
  shortjournal = {Front. Big Data},
  title        = {High precision mammography lesion identification from imprecise medical annotations},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Computational modeling of hierarchically polarized groups by
structured matrix factorization. <em>FDATA</em>, <em>4</em>, 729881. (<a
href="https://doi.org/10.3389/fdata.2021.729881">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper extends earlier work on modeling hierarchically polarized groups on social media. An algorithm is described that 1) detects points of agreement and disagreement between groups, and 2) divides them hierarchically to represent nested patterns of agreement and disagreement given a structural guide. For example, two opposing parties might disagree on core issues. Moreover, within a party, despite agreement on fundamentals, disagreement might occur on further details. We call such scenarios hierarchically polarized groups. An (enhanced) unsupervised Non-negative Matrix Factorization (NMF) algorithm is described for computational modeling of hierarchically polarized groups. It is enhanced with a language model, and with a proof of orthogonality of factorized components. We evaluate it on both synthetic and real-world datasets, demonstrating ability to hierarchically decompose overlapping beliefs. In the case where polarization is flat, we compare it to prior art and show that it outperforms state of the art approaches for polarization detection and stance separation. An ablation study further illustrates the value of individual components, including new enhancements.},
  archive      = {J_FDATA},
  author       = {Sun, Dachun and Yang, Chaoqi and Li, Jinyang and Wang, Ruijie and Yao, Shuochao and Shao, Huajie and Liu, Dongxin and Liu, Shengzhong and Wang, Tianshi and Abdelzaher, Tarek F.},
  doi          = {10.3389/fdata.2021.729881},
  journal      = {Frontiers in Big Data},
  month        = {12},
  pages        = {729881},
  shortjournal = {Front. Big Data},
  title        = {Computational modeling of hierarchically polarized groups by structured matrix factorization},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Mapping international geopolitical agenda. Continuing
national conceptions of the emerging european crisis. <em>FDATA</em>,
<em>4</em>, 718809. (<a
href="https://doi.org/10.3389/fdata.2021.718809">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study proposes a geopolitical analysis of opinion dynamics based on a statistical exploration of a press dataset covering 2014–2019. This exploration questions three case studies of geopolitical and international interest: international migration, political borders, and pandemics. Through the framework of geopolitical agenda, the aim of this study is to question the “crisis” status of changes in the media coverage of the three topics in a cross-analysis and multilingual analysis of 20 western European newspapers. It concludes that there is a prevalence of national agendas.},
  archive      = {J_FDATA},
  author       = {Grasland, Claude and Toureille, Etienne and Leconte, Romain and Severo, Marta},
  doi          = {10.3389/fdata.2021.718809},
  journal      = {Frontiers in Big Data},
  month        = {12},
  pages        = {718809},
  shortjournal = {Front. Big Data},
  title        = {Mapping international geopolitical agenda. continuing national conceptions of the emerging european crisis},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Weighting methods for rare event identification from
imbalanced datasets. <em>FDATA</em>, <em>4</em>, 715320. (<a
href="https://doi.org/10.3389/fdata.2021.715320">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In machine learning, we often face the situation where the event we are interested in has very few data points buried in a massive amount of data. This is typical in network monitoring, where data are streamed from sensing or measuring units continuously but most data are not for events. With imbalanced datasets, the classifiers tend to be biased in favor of the main class. Rare event detection has received much attention in machine learning, and yet it is still a challenging problem. In this paper, we propose a remedy for the standing problem. Weighting and sampling are two fundamental approaches to address the problem. We focus on the weighting method in this paper. We first propose a boosting-style algorithm to compute class weights, which is proved to have excellent theoretical property. Then we propose an adaptive algorithm, which is suitable for real-time applications. The adaptive nature of the two algorithms allows a controlled tradeoff between true positive rate and false positive rate and avoids excessive weight on the rare class, which leads to poor performance on the main class. Experiments on power grid data and some public datasets show that the proposed algorithms outperform the existing weighting and boosting methods, and that their superiority is more noticeable with noisy data.},
  archive      = {J_FDATA},
  author       = {He, Jia and Cheng, Maggie X.},
  doi          = {10.3389/fdata.2021.715320},
  journal      = {Frontiers in Big Data},
  month        = {12},
  pages        = {715320},
  shortjournal = {Front. Big Data},
  title        = {Weighting methods for rare event identification from imbalanced datasets},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Solutions to big data privacy and security challenges
associated with COVID-19 surveillance systems. <em>FDATA</em>,
<em>4</em>, 645204. (<a
href="https://doi.org/10.3389/fdata.2021.645204">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The growing dependency on digital technologies is becoming a way of life, and at the same time, the collection of data using them for surveillance operations has raised concerns. Notably, some countries use digital surveillance technologies for tracking and monitoring individuals and populations to prevent the transmission of the new coronavirus. The technology has the capacity to contribute towards tackling the pandemic effectively, but the success also comes at the expense of privacy rights. The crucial point to make is regardless of who uses and which mechanism, in one way another will infringe personal privacy. Therefore, when considering the use of technologies to combat the pandemic, the focus should also be on the impact of facial recognition cameras, police surveillance drones, and other digital surveillance devices on the privacy rights of those under surveillance. The GDPR was established to ensure that information could be shared without causing any infringement on personal data and businesses; therefore, in generating Big Data, it is important to ensure that the information is securely collected, processed, transmitted, stored, and accessed in accordance with established rules. This paper focuses on Big Data challenges associated with surveillance methods used within the COVID-19 parameters. The aim of this research is to propose practical solutions to Big Data challenges associated with COVID-19 pandemic surveillance approaches. To that end, the researcher will identify the surveillance measures being used by countries in different regions, the sensitivity of generated data, and the issues associated with the collection of large volumes of data and finally propose feasible solutions to protect the privacy rights of the people, during the post-COVID-19 era.},
  archive      = {J_FDATA},
  author       = {Bentotahewa, Vibhushinie and Hewage, Chaminda and Williams, Jason},
  doi          = {10.3389/fdata.2021.645204},
  journal      = {Frontiers in Big Data},
  month        = {12},
  pages        = {645204},
  shortjournal = {Front. Big Data},
  title        = {Solutions to big data privacy and security challenges associated with COVID-19 surveillance systems},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Corrigendum: Statistical enrichment analysis of samples: A
general-purpose tool to annotate metadata neighborhoods of biological
samples. <em>FDATA</em>, <em>4</em>, 804141. (<a
href="https://doi.org/10.3389/fdata.2021.804141">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_FDATA},
  author       = {Nguyen, Thanh M. and Bharti, Samuel and Yue, Zongliang and Willey, Christopher D. and Chen, Jake Y.},
  doi          = {10.3389/fdata.2021.804141},
  journal      = {Frontiers in Big Data},
  month        = {11},
  pages        = {804141},
  shortjournal = {Front. Big Data},
  title        = {Corrigendum: statistical enrichment analysis of samples: a general-purpose tool to annotate metadata neighborhoods of biological samples},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Editorial: AI-enabled data science for COVID-19.
<em>FDATA</em>, <em>4</em>, 802452. (<a
href="https://doi.org/10.3389/fdata.2021.802452">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_FDATA},
  author       = {Yan, Da and Qin, Hong and Wu, Hsiang-Yun and Chen, Jake Y.},
  doi          = {10.3389/fdata.2021.802452},
  journal      = {Frontiers in Big Data},
  month        = {11},
  pages        = {802452},
  shortjournal = {Front. Big Data},
  title        = {Editorial: AI-enabled data science for COVID-19},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Management of COVID-19 pandemic data in india: Challenges
faced and lessons learnt. <em>FDATA</em>, <em>4</em>, 790158. (<a
href="https://doi.org/10.3389/fdata.2021.790158">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {COVID-19 is an ongoing pandemic, which has already claimed millions of lives worldwide. In the absence of prior information on the pandemic, the governments can use generated testing data to drive policy decisions. Thus, a one-stop repository is essential to ensure sharing of clean, de-duplicated, and updated records to all the stakeholders. In India, the government initiated the testing through a network of VRDLs headed by the Indian Council of Medical Research (ICMR). Initially, the generated data were captured and shared in Excel sheets. As the number of cases increased, there was a need for a data management system to ensure reliable and up-to-date data to drive policy decisions. Thus, the data management team at ICMR initiated the development of a national COVID-19 testing data management tool that is currently maintaining all the data in a central hub. The first version of the tool was released in March 2020 and was subsequently modified with the changing testing guidelines and strategies. On completing one and a half years of managing the data and collecting approximately 550 million records, the team analyzed the challenges faced and the strategies used to ensure a seamless flow of data to the system and its real-time analysis. In this study, the entire duration of the pandemic has been divided into four different phases based on the resourcefulness of the country. Since the pandemic is currently ongoing, this study can be useful for countries in a different phase of pandemic facing similar challenges.},
  archive      = {J_FDATA},
  author       = {Kaur, Jasmine and Kaur, Jasleen and Dhama, Ajay Singh and Kumar, Vinit and Singh, Harpreet},
  doi          = {10.3389/fdata.2021.790158},
  journal      = {Frontiers in Big Data},
  month        = {11},
  pages        = {790158},
  shortjournal = {Front. Big Data},
  title        = {Management of COVID-19 pandemic data in india: Challenges faced and lessons learnt},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Editorial: Big data management in industry 4.0.
<em>FDATA</em>, <em>4</em>, 788491. (<a
href="https://doi.org/10.3389/fdata.2021.788491">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_FDATA},
  author       = {Firmani, Donatella and Leotta, Francesco and Mandreoli, Federica and Mecella, Massimo},
  doi          = {10.3389/fdata.2021.788491},
  journal      = {Frontiers in Big Data},
  month        = {11},
  pages        = {788491},
  shortjournal = {Front. Big Data},
  title        = {Editorial: Big data management in industry 4.0},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fair and effective policing for neighborhood safety:
Understanding and overcoming selection biases. <em>FDATA</em>,
<em>4</em>, 787459. (<a
href="https://doi.org/10.3389/fdata.2021.787459">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An accurate crime prediction and risk estimation can help improve the efficiency and effectiveness of policing activities. However, reports have revealed that biases like racial prejudice could exist in policing enforcement, and trained predictors may inherit them. In this work, we study the possible reasons and countermeasures to this problem, using records from the New York frisk and search program (NYCSF) as the dataset. Concretely, we provide analysis on the possible origin of this phenomenon from the perspective of risk discrepancy, and study it with the scope of selection bias. Motivated by theories in causal inference, we propose a re-weighting approach based on propensity score to balance the data distribution, with respect to the identified treatment: search action. Naively applying existing re-weighting approaches in causal inference is not suitable as the weight is passively estimated from observational data. Inspired by adversarial learning techniques, we formulate the predictor training and re-weighting as a min-max game, so that the re-weighting scale can be automatically learned. Specifically, the proposed approach aims to train a model that: 1) able to balance the data distribution in the searched and un-searched groups; 2) remain discriminative between treatment interventions. Extensive evaluations on real-world dataset are conducted, and results validate the effectiveness of the proposed framework.},
  archive      = {J_FDATA},
  author       = {Ren, Weijeiying and Liu, Kunpeng and Zhao , Tianxiang and Fu , Yanjie},
  doi          = {10.3389/fdata.2021.787459},
  journal      = {Frontiers in Big Data},
  month        = {11},
  pages        = {787459},
  shortjournal = {Front. Big Data},
  title        = {Fair and effective policing for neighborhood safety: Understanding and overcoming selection biases},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Link prediction between structured geopolitical events:
Models and experiments. <em>FDATA</em>, <em>4</em>, 779792. (<a
href="https://doi.org/10.3389/fdata.2021.779792">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Often thought of as higher-order entities, events have recently become important subjects of research in the computational sciences, including within complex systems and natural language processing (NLP). One such application is event link prediction. Given an input event, event link prediction is the problem of retrieving a relevant set of events, similar to the problem of retrieving relevant documents on the Web in response to keyword queries. Since geopolitical events have complex semantics, it is an open question as to how to best model and represent events within the framework of event link prediction. In this paper, we formalize the problem and discuss how established representation learning algorithms from the machine learning community could potentially be applied to it. We then conduct a detailed empirical study on the Global Terrorism Database (GTD) using a set of metrics inspired by the information retrieval community. Our results show that, while there is considerable signal in both network-theoretic and text-centric models of the problem, classic text-only models such as bag-of-words prove surprisingly difficult to outperform. Our results establish both a baseline for event link prediction on GTD, and currently outstanding challenges for the research community to tackle in this space.},
  archive      = {J_FDATA},
  author       = {Kejriwal, Mayank},
  doi          = {10.3389/fdata.2021.779792},
  journal      = {Frontiers in Big Data},
  month        = {11},
  pages        = {779792},
  shortjournal = {Front. Big Data},
  title        = {Link prediction between structured geopolitical events: Models and experiments},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Knowledge-infused learning for entity prediction in driving
scenes. <em>FDATA</em>, <em>4</em>, 759110. (<a
href="https://doi.org/10.3389/fdata.2021.759110">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scene understanding is a key technical challenge within the autonomous driving domain. It requires a deep semantic understanding of the entities and relations found within complex physical and social environments that is both accurate and complete. In practice, this can be accomplished by representing entities in a scene and their relations as a knowledge graph (KG). This scene knowledge graph may then be utilized for the task of entity prediction, leading to improved scene understanding. In this paper, we will define and formalize this problem as Knowledge-based Entity Prediction (KEP). KEP aims to improve scene understanding by predicting potentially unrecognized entities by leveraging heterogeneous, high-level semantic knowledge of driving scenes. An innovative neuro-symbolic solution for KEP is presented, based on knowledge-infused learning, which 1) introduces a dataset agnostic ontology to describe driving scenes, 2) uses an expressive, holistic representation of scenes with knowledge graphs, and 3) proposes an effective, non-standard mapping of the KEP problem to the problem of link prediction (LP) using knowledge-graph embeddings (KGE). Using real, complex and high-quality data from urban driving scenes, we demonstrate its effectiveness by showing that the missing entities may be predicted with high precision (0.87 Hits@1) while significantly outperforming the non-semantic/rule-based baselines.},
  archive      = {J_FDATA},
  author       = {Wickramarachchi, Ruwan and Henson , Cory and Sheth , Amit},
  doi          = {10.3389/fdata.2021.759110},
  journal      = {Frontiers in Big Data},
  month        = {11},
  pages        = {759110},
  shortjournal = {Front. Big Data},
  title        = {Knowledge-infused learning for entity prediction in driving scenes},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fast GPU-based generation of large graph networks from
degree distributions. <em>FDATA</em>, <em>4</em>, 737963. (<a
href="https://doi.org/10.3389/fdata.2021.737963">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Synthetically generated, large graph networks serve as useful proxies to real-world networks for many graph-based applications. The ability to generate such networks helps overcome several limitations of real-world networks regarding their number, availability, and access. Here, we present the design, implementation, and performance study of a novel network generator that can produce very large graph networks conforming to any desired degree distribution. The generator is designed and implemented for efficient execution on modern graphics processing units (GPUs). Given an array of desired vertex degrees and number of vertices for each desired degree, our algorithm generates the edges of a random graph that satisfies the input degree distribution. Multiple runtime variants are implemented and tested: 1) a uniform static work assignment using a fixed thread launch scheme, 2) a load-balanced static work assignment also with fixed thread launch but with cost-aware task-to-thread mapping, and 3) a dynamic scheme with multiple GPU kernels asynchronously launched from the CPU. The generation is tested on a range of popular networks such as Twitter and Facebook, representing different scales and skews in degree distributions. Results show that, using our algorithm on a single modern GPU (NVIDIA Volta V100), it is possible to generate large-scale graph networks at rates exceeding 50 billion edges per second for a 69 billion-edge network. GPU profiling confirms high utilization and low branching divergence of our implementation from small to large network sizes. For networks with scattered distributions, we provide a coarsening method that further increases the GPU-based generation speed by up to a factor of 4 on tested input networks with over 45 billion edges.},
  archive      = {J_FDATA},
  author       = {Alam, Maksudul and Perumalla, Kalyan},
  doi          = {10.3389/fdata.2021.737963},
  journal      = {Frontiers in Big Data},
  month        = {11},
  pages        = {737963},
  shortjournal = {Front. Big Data},
  title        = {Fast GPU-based generation of large graph networks from degree distributions},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Teeport: Break the wall between the optimization algorithms
and problems. <em>FDATA</em>, <em>4</em>, 734650. (<a
href="https://doi.org/10.3389/fdata.2021.734650">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimization algorithms/techniques such as genetic algorithm, particle swarm optimization, and Gaussian process have been widely used in the accelerator field to tackle complex design/online optimization problems. However, connecting the algorithm with the optimization problem can be difficult, as the algorithms and the problems may be implemented in different languages, or they may require specific resources. We introduce an optimization platform named Teeport that is developed to address the above issues. This real-time communication-based platform is designed to minimize the effort of integrating the algorithms and problems. Once integrated, the users are granted a rich feature set, such as monitoring, controlling, and benchmarking. Some real-life applications of the platform are also discussed.},
  archive      = {J_FDATA},
  author       = {Zhang, Zhe and Huang, Xiaobiao and Song, Minghao},
  doi          = {10.3389/fdata.2021.734650},
  journal      = {Frontiers in Big Data},
  month        = {11},
  pages        = {734650},
  shortjournal = {Front. Big Data},
  title        = {Teeport: Break the wall between the optimization algorithms and problems},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A systematic review on model watermarking for neural
networks. <em>FDATA</em>, <em>4</em>, 729663. (<a
href="https://doi.org/10.3389/fdata.2021.729663">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning (ML) models are applied in an increasing variety of domains. The availability of large amounts of data and computational resources encourages the development of ever more complex and valuable models. These models are considered the intellectual property of the legitimate parties who have trained them, which makes their protection against stealing, illegitimate redistribution, and unauthorized application an urgent need. Digital watermarking presents a strong mechanism for marking model ownership and, thereby, offers protection against those threats. This work presents a taxonomy identifying and analyzing different classes of watermarking schemes for ML models. It introduces a unified threat model to allow structured reasoning on and comparison of the effectiveness of watermarking methods in different scenarios. Furthermore, it systematizes desired security requirements and attacks against ML model watermarking. Based on that framework, representative literature from the field is surveyed to illustrate the taxonomy. Finally, shortcomings and general limitations of existing approaches are discussed, and an outlook on future research directions is given.},
  archive      = {J_FDATA},
  author       = {Boenisch, Franziska},
  doi          = {10.3389/fdata.2021.729663},
  journal      = {Frontiers in Big Data},
  month        = {11},
  pages        = {729663},
  shortjournal = {Front. Big Data},
  title        = {A systematic review on model watermarking for neural networks},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Systematic exploration in tissue-pathway associations of
complex traits using comprehensive eQTLs catalog. <em>FDATA</em>,
<em>4</em>, 719737. (<a
href="https://doi.org/10.3389/fdata.2021.719737">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The collection of expression quantitative trait loci (eQTLs) is an important resource to study complex traits through understanding where and how transcriptional regulations are controlled by genetic variations in the non-coding regions of the genome. Previous studies have focused on associating eQTLs with traits to identify the roles of trait-related eQTLs and their corresponding target genes involved in trait determination. Since most genes function as a part of pathways in a systematic manner, it is crucial to explore the pathways’ involvements in complex traits to test potentially novel hypotheses and to reveal underlying mechanisms of disease pathogenesis. In this study, we expanded and applied loci2path software to perform large-scale eQTLs enrichment [i.e., eQTLs’ target genes (eGenes) enrichment] analysis at pathway level to identify the tissue-specific enriched pathways within trait-related genomic intervals. By utilizing 13,791,909 eQTLs cataloged in the Genotype-Tissue Expression (GTEx) V8 data for 49 tissue types, 2,893 pathway sets reported from MSigDB, and query regions derived from the Phenotype-Genotype Integrator (PheGenI) catalog, we identified intriguing biological pathways that are likely to be involved in ten traits [Alzheimer’s disease (AD), body mass index, Parkinson’s disease (PD), schizophrenia, amyotrophic lateral sclerosis, non-small cell lung cancer (NSCLC), stroke, blood pressure, autism spectrum disorder, and myocardial infarction]. Furthermore, we extracted the most significant pathways for AD, such as BioCarta D4-GDI pathway and WikiPathways sulfation biotransformation reaction and viral acute myocarditis pathways, to study specific genes within pathways. Our data presented new hypotheses in AD pathogenesis supported by previous studies, like the increased level of caspase-3 in the amygdala that cleaves GDP dissociation inhibitor and binds to beta-amyloid, leading to increased apoptosis and neuronal loss. Our findings also revealed potential pathogenesis mechanisms for PD, schizophrenia, NSCLC, blood pressure, autism spectrum disorder, and myocardial infarction, which were consistent with past studies. Our results indicated that loci2path′s eQTLs enrichment test was valuable in unveiling novel biological mechanisms of complex traits. The discovered mechanisms of disease pathogenesis and traits require further in-depth analysis and experimental validation.},
  archive      = {J_FDATA},
  author       = {Wang, Boqi and Yang, James and Qiu, Steven and Bai, Yongsheng and Qin, Zhaohui S.},
  doi          = {10.3389/fdata.2021.719737},
  journal      = {Frontiers in Big Data},
  month        = {11},
  pages        = {719737},
  shortjournal = {Front. Big Data},
  title        = {Systematic exploration in tissue-pathway associations of complex traits using comprehensive eQTLs catalog},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Anomaly detection for the centralised elasticsearch service
at CERN. <em>FDATA</em>, <em>4</em>, 718879. (<a
href="https://doi.org/10.3389/fdata.2021.718879">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For several years CERN has been offering a centralised service for Elasticsearch, a popular distributed system for search and analytics of user provided data. The service offered by CERN IT is better described as a service of services, delivering centrally managed and maintained Elasticsearch instances to CERN users who have a justified need for it. This dynamic infrastructure currently consists of about 30 distinct and independent Elasticsearch installations, in the following referred to as Elasticsearch clusters, some of which are shared between different user communities. The service is used by several hundred users mainly for logs and service analytics. Due to its size and complexity, the installation produces a huge amount of internal monitoring data which can be difficult to process in real time with limited available person power. Early on, an idea was therefore born to process this data automatically, aiming to extract anomalies and possible issues building up in real time, allowing the experts to address them before they start to cause an issue for the users of the service. Both deep learning and traditional methods have been applied to analyse the data in order to achieve this goal. This resulted in the current deployment of an anomaly detection system based on a one layer multi dimensional LSTM neural network, coupled with applying a simple moving average to the data to validate the results. This paper will describe which methods were investigated and give an overview of the current system, including data retrieval, data pre-processing and analysis. In addition, reports on experiences gained when applying the system to actual data will be provided. Finally, weaknesses of the current system will be briefly discussed, and ideas for future system improvements will be sketched out.},
  archive      = {J_FDATA},
  author       = {Andersson, Jennifer R. and Moya, Jose Alonso and Schwickerath, Ulrich},
  doi          = {10.3389/fdata.2021.718879},
  journal      = {Frontiers in Big Data},
  month        = {11},
  pages        = {718879},
  shortjournal = {Front. Big Data},
  title        = {Anomaly detection for the centralised elasticsearch service at CERN},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fitting and cross-validating cox models to censored big data
with missing values using extensions of partial least squares regression
models. <em>FDATA</em>, <em>4</em>, 684794. (<a
href="https://doi.org/10.3389/fdata.2021.684794">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fitting Cox models in a big data context -on a massive scale in terms of volume, intensity, and complexity exceeding the capacity of usual analytic tools-is often challenging. If some data are missing, it is even more difficult. We proposed algorithms that were able to fit Cox models in high dimensional settings using extensions of partial least squares regression to the Cox models. Some of them were able to cope with missing data. We were recently able to extend our most recent algorithms to big data, thus allowing to fit Cox model for big data with missing values. When cross-validating standard or extended Cox models, the commonly used criterion is the cross-validated partial loglikelihood using a naive or a van Houwelingen scheme —to make efficient use of the death times of the left out data in relation to the death times of all the data. Quite astonishingly, we will show, using a strong simulation study involving three different data simulation algorithms, that these two cross-validation methods fail with the extensions, either straightforward or more involved ones, of partial least squares regression to the Cox model. This is quite an interesting result for at least two reasons. Firstly, several nice features of PLS based models, including regularization, interpretability of the components, missing data support, data visualization thanks to biplots of individuals and variables —and even parsimony or group parsimony for Sparse partial least squares or sparse group SPLS based models, account for a common use of these extensions by statisticians who usually select their hyperparameters using cross-validation. Secondly, they are almost always featured in benchmarking studies to assess the performance of a new estimation technique used in a high dimensional or big data context and often show poor statistical properties. We carried out a vast simulation study to evaluate more than a dozen of potential cross-validation criteria, either AUC or prediction error based. Several of them lead to the selection of a reasonable number of components. Using these newly found cross-validation criteria to fit extensions of partial least squares regression to the Cox model, we performed a benchmark reanalysis that showed enhanced performances of these techniques. In addition, we proposed sparse group extensions of our algorithms and defined a new robust measure based on the Schmid score and the R coefficient of determination for least absolute deviation: the integrated R Schmid Score weighted. The R-package used in this article is available on the CRAN, http://cran.r-project.org/web/packages/plsRcox/index.html. The R package bigPLS will soon be available on the CRAN and, until then, is available on Github https://github.com/fbertran/bigPLS.},
  archive      = {J_FDATA},
  author       = {Bertrand , Frédéric and Maumy-Bertrand , Myriam},
  doi          = {10.3389/fdata.2021.684794},
  journal      = {Frontiers in Big Data},
  month        = {11},
  pages        = {684794},
  shortjournal = {Front. Big Data},
  title        = {Fitting and cross-validating cox models to censored big data with missing values using extensions of partial least squares regression models},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The old and the new: Can physics-informed deep-learning
replace traditional linear solvers? <em>FDATA</em>, <em>4</em>, 669097.
(<a href="https://doi.org/10.3389/fdata.2021.669097">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Physics-Informed Neural Networks (PINN) are neural networks encoding the problem governing equations, such as Partial Differential Equations (PDE), as a part of the neural network. PINNs have emerged as a new essential tool to solve various challenging problems, including computing linear systems arising from PDEs, a task for which several traditional methods exist. In this work, we focus first on evaluating the potential of PINNs as linear solvers in the case of the Poisson equation, an omnipresent equation in scientific computing. We characterize PINN linear solvers in terms of accuracy and performance under different network configurations (depth, activation functions, input data set distribution). We highlight the critical role of transfer learning. Our results show that low-frequency components of the solution converge quickly as an effect of the F-principle. In contrast, an accurate solution of the high frequencies requires an exceedingly long time. To address this limitation, we propose integrating PINNs into traditional linear solvers. We show that this integration leads to the development of new solvers whose performance is on par with other high-performance solvers, such as PETSc conjugate gradient linear solvers, in terms of performance and accuracy. Overall, while the accuracy and computational performance are still a limiting factor for the direct use of PINN linear solvers, hybrid strategies combining old traditional linear solver approaches with new emerging deep-learning techniques are among the most promising methods for developing a new class of linear solvers.},
  archive      = {J_FDATA},
  author       = {Markidis , Stefano},
  doi          = {10.3389/fdata.2021.669097},
  journal      = {Frontiers in Big Data},
  month        = {11},
  pages        = {669097},
  shortjournal = {Front. Big Data},
  title        = {The old and the new: Can physics-informed deep-learning replace traditional linear solvers?},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptive on-the-fly changes in distributed processing
pipelines. <em>FDATA</em>, <em>4</em>, 666174. (<a
href="https://doi.org/10.3389/fdata.2021.666174">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributed data processing systems have become the standard means for big data analytics. These systems are based on processing pipelines where operations on data are performed in a chain of consecutive steps. Normally, the operations performed by these pipelines are set at design time, and any changes to their functionality require the applications to be restarted. This is not always acceptable, for example, when we cannot afford downtime or when a long-running calculation would lose significant progress. The introduction of variation points to distributed processing pipelines allows for on-the-fly updating of individual analysis steps. In this paper, we extend such basic variation point functionality to provide fully automated reconfiguration of the processing steps within a running pipeline through an automated planner. We have enabled pipeline modeling through constraints. Based on these constraints, we not only ensure that configurations are compatible with type but also verify that expected pipeline functionality is achieved. Furthermore, automating the reconfiguration process simplifies its use, in turn allowing users with less development experience to make changes. The system can automatically generate and validate pipeline configurations that achieve a specified goal, selecting from operation definitions available at planning time. It then automatically integrates these configurations into the running pipeline. We verify the system through the testing of a proof-of-concept implementation. The proof of concept also shows promising results when reconfiguration is performed frequently.},
  archive      = {J_FDATA},
  author       = {Albers, Toon and Lazovik, Elena and Hadadian Nejad Yousefi, Mostafa and Lazovik, Alexander},
  doi          = {10.3389/fdata.2021.666174},
  journal      = {Frontiers in Big Data},
  month        = {11},
  pages        = {666174},
  shortjournal = {Front. Big Data},
  title        = {Adaptive on-the-fly changes in distributed processing pipelines},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Autotuning of exascale applications with anomalies
detection. <em>FDATA</em>, <em>4</em>, 657218. (<a
href="https://doi.org/10.3389/fdata.2021.657218">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The execution of complex distributed applications in exascale systems faces many challenges, as it involves empirical evaluation of countless code variations and application runtime parameters over a heterogeneous set of resources. To mitigate these challenges, the research field of autotuning has gained momentum. The autotuning automates identifying the most desirable application implementation in terms of code variations and runtime parameters. However, the complexity and size of the exascale systems make the autotuning process very difficult, especially considering the number of parameter variations that have to be identified. Therefore, we introduce a novel approach for autotuning exascale applications based on a genetic multi-objective optimization algorithm integrated within the ASPIDE exascale computing framework. The approach considers multi-dimensional search space with support for pluggable objective functions, including execution time and energy requirements. Furthermore, the autotuner employs a machine learning-based event detection approach to detect events and anomalies during application execution, such as hardware failures or communication bottlenecks.},
  archive      = {J_FDATA},
  author       = {Kimovski, Dragi and Mathá, Roland and Iuhasz, Gabriel and Marozzo, Fabrizio and Petcu, Dana and Prodan, Radu},
  doi          = {10.3389/fdata.2021.657218},
  journal      = {Frontiers in Big Data},
  month        = {11},
  pages        = {657218},
  shortjournal = {Front. Big Data},
  title        = {Autotuning of exascale applications with anomalies detection},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Towards semantically-rich spatial network representation
learning via automated feature topic pairing. <em>FDATA</em>,
<em>4</em>, 762899. (<a
href="https://doi.org/10.3389/fdata.2021.762899">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automated characterization of spatial data is a kind of critical geographical intelligence. As an emerging technique for characterization, spatial Representation Learning (SRL) uses deep neural networks (DNNs) to learn non-linear embedded features of spatial data for characterization. However, SRL extracts features by internal layers of DNNs, and thus suffers from lacking semantic labels. Texts of spatial entities, on the other hand, provide semantic understanding of latent feature labels, but is insensible to deep SRL models. How can we teach a SRL model to discover appropriate topic labels in texts and pair learned features with the labels? This paper formulates a new problem: feature-topic pairing, and proposes a novel Particle Swarm Optimization (PSO) based deep learning framework. Specifically, we formulate the feature-topic pairing problem into an automated alignment task between 1) a latent embedding feature space and 2) a textual semantic topic space. We decompose the alignment of the two spaces into: 1) point-wise alignment, denoting the correlation between a topic distribution and an embedding vector; 2) pair-wise alignment, denoting the consistency between a feature-feature similarity matrix and a topic-topic similarity matrix. We design a PSO based solver to simultaneously select an optimal set of topics and learn corresponding features based on the selected topics. We develop a closed loop algorithm to iterate between 1) minimizing losses of representation reconstruction and feature-topic alignment and 2) searching the best topics. Finally, we present extensive experiments to demonstrate the enhanced performance of our method.},
  archive      = {J_FDATA},
  author       = {Wang, Dongjie and Liu, Kunpeng and Mohaisen, David and Wang, Pengyang and Lu, Chang-Tien and Fu, Yanjie},
  doi          = {10.3389/fdata.2021.762899},
  journal      = {Frontiers in Big Data},
  month        = {10},
  pages        = {762899},
  shortjournal = {Front. Big Data},
  title        = {Towards semantically-rich spatial network representation learning via automated feature topic pairing},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Automated analysis of the US drought monitor maps with
machine learning and multiple drought indicators. <em>FDATA</em>,
<em>4</em>, 750536. (<a
href="https://doi.org/10.3389/fdata.2021.750536">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The US Drought Monitor (USDM) is a hallmark in real time drought monitoring and assessment as it was developed by multiple agencies to provide an accurate and timely assessment of drought conditions in the US on a weekly basis. The map is built based on multiple physical indicators as well as reported observations from local contributors before human analysts combine the information and produce the drought map using their best judgement. Since human subjectivity is included in the production of the USDM maps, it is not an entirely clear quantitative procedure for other entities to reproduce the maps. In this study, we developed a framework to automatically generate the maps through a machine learning approach by predicting the drought categories across the domain of study. A persistence model served as the baseline model for comparison in the framework. Three machine learning algorithms, logistic regression, random forests, and support vector machines, with four different groups of input data, which formed an overall of 12 different configurations, were used for the prediction of drought categories. Finally, all the configurations were evaluated against the baseline model to select the best performing option. The results showed that our proposed framework could reproduce the drought maps to a near-perfect level with the support vector machines algorithm and the group 4 data. The rest of the findings of this study can be highlighted as: 1) employing the past week drought data as a predictor in the models played an important role in achieving high prediction scores, 2) the nonlinear models, random forest, and support vector machines had a better overall performance compared to the logistic regression models, and 3) with borrowing the neighboring grid cells information, we could compensate the lack of training data in the grid cells with insufficient historical USDM data particularly for extreme and exceptional drought conditions.},
  archive      = {J_FDATA},
  author       = {Hatami Bahman Beiglou, Pouyan and Luo, Lifeng and Tan, Pang-Ning and Pei, Lisi},
  doi          = {10.3389/fdata.2021.750536},
  journal      = {Frontiers in Big Data},
  month        = {10},
  pages        = {750536},
  shortjournal = {Front. Big Data},
  title        = {Automated analysis of the US drought monitor maps with machine learning and multiple drought indicators},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Testing a generalizable machine learning workflow for
aquatic invasive species on rainbow trout (oncorhynchus mykiss) in
northwest montana. <em>FDATA</em>, <em>4</em>, 734990. (<a
href="https://doi.org/10.3389/fdata.2021.734990">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Biological invasions are accelerating worldwide, causing major ecological and economic impacts in aquatic ecosystems. The urgent decision-making needs of invasive species managers can be better met by the integration of biodiversity big data with large-domain models and data-driven products. Remotely sensed data products can be combined with existing invasive species occurrence data via machine learning models to provide the proactive spatial risk analysis necessary for implementing coordinated and agile management paradigms across large scales. We present a workflow that generates rapid spatial risk assessments on aquatic invasive species using occurrence data, spatially explicit environmental data, and an ensemble approach to species distribution modeling using five machine learning algorithms. For proof of concept and validation, we tested this workflow using extensive spatial and temporal hybridization and occurrence data from a well-studied, ongoing, and climate-driven species invasion in the upper Flathead River system in northwestern Montana, USA. Rainbow Trout (RBT; Oncorhynchus mykiss), an introduced species in the Flathead River basin, compete and readily hybridize with native Westslope Cutthroat Trout (WCT; O. clarkii lewisii), and the spread of RBT individuals and their alleles has been tracked for decades. We used remotely sensed and other geospatial data as key environmental predictors for projecting resultant habitat suitability to geographic space. The ensemble modeling technique yielded high accuracy predictions relative to 30-fold cross-validated datasets (87% 30-fold cross-validated accuracy score). Both top predictors and model performance relative to these predictors matched current understanding of the drivers of RBT invasion and habitat suitability, indicating that temperature is a major factor influencing the spread of invasive RBT and hybridization with native WCT. The congruence between more time-consuming modeling approaches and our rapid machine-learning approach suggest that this workflow could be applied more broadly to provide data-driven management information for early detection of potential invaders.},
  archive      = {J_FDATA},
  author       = {Carter, S. and van Rees, C. B. and Hand, B. K. and Muhlfeld, C. C. and Luikart, G. and Kimball, J. S.},
  doi          = {10.3389/fdata.2021.734990},
  journal      = {Frontiers in Big Data},
  month        = {10},
  pages        = {734990},
  shortjournal = {Front. Big Data},
  title        = {Testing a generalizable machine learning workflow for aquatic invasive species on rainbow trout (Oncorhynchus mykiss) in northwest montana},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Modelling representative population mobility for COVID-19
spatial transmission in south africa. <em>FDATA</em>, <em>4</em>,
718351. (<a href="https://doi.org/10.3389/fdata.2021.718351">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The COVID-19 pandemic starting in the first half of 2020 has changed the lives of everyone across the world. Reduced mobility was essential due to it being the largest impact possible against the spread of the little understood SARS-CoV-2 virus. To understand the spread, a comprehension of human mobility patterns is needed. The use of mobility data in modelling is thus essential to capture the intrinsic spread through the population. It is necessary to determine to what extent mobility data sources convey the same message of mobility within a region. This paper compares different mobility data sources by constructing spatial weight matrices at a variety of spatial resolutions and further compares the results through hierarchical clustering. We consider four methods for constructing spatial weight matrices representing mobility between spatial units, taking into account distance between spatial units as well as spatial covariates. This provides insight for the user into which data provides what type of information and in what situations a particular data source is most useful.},
  archive      = {J_FDATA},
  author       = {Potgieter, A. and Fabris-Rotelli, I. N. and Kimmie, Z. and Dudeni-Tlhone, N. and Holloway, J. P. and Janse van Rensburg, C. and Thiede, R. N. and Debba, P. and Manjoo-Docrat, R. and Abdelatif, N. and Khuluse-Makhanya, S.},
  doi          = {10.3389/fdata.2021.718351},
  journal      = {Frontiers in Big Data},
  month        = {10},
  pages        = {718351},
  shortjournal = {Front. Big Data},
  title        = {Modelling representative population mobility for COVID-19 spatial transmission in south africa},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A data-driven personalized lighting recommender system.
<em>FDATA</em>, <em>4</em>, 706117. (<a
href="https://doi.org/10.3389/fdata.2021.706117">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommender systems attempt to identify and recommend the most preferable item (product-service) to individual users. These systems predict user interest in items based on related items, users, and the interactions between items and users. We aim to build an auto-routine and color scheme recommender system for home-based smart lighting that leverages a wealth of historical data and machine learning methods. We utilize an unsupervised method to recommend a routine for smart lighting. Moreover, by analyzing users’ daily logs, geographical location, temporal and usage information, we understand user preferences and predict their preferred light colors. To do so, users are clustered based on their geographical information and usage distribution. We then build and train a predictive model within each cluster and aggregate the results. Results indicate that models based on similar users increases the prediction accuracy, with and without prior knowledge about user preferences.},
  archive      = {J_FDATA},
  author       = {Zarindast , Atousa and Wood , Jonathan},
  doi          = {10.3389/fdata.2021.706117},
  journal      = {Frontiers in Big Data},
  month        = {10},
  pages        = {706117},
  shortjournal = {Front. Big Data},
  title        = {A data-driven personalized lighting recommender system},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Prefix imputation of orphan events in event stream
processing. <em>FDATA</em>, <em>4</em>, 705243. (<a
href="https://doi.org/10.3389/fdata.2021.705243">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the context of process mining, event logs consist of process instances called cases. Conformance checking is a process mining task that inspects whether a log file is conformant with an existing process model. This inspection is additionally quantifying the conformance in an explainable manner. Online conformance checking processes streaming event logs by having precise insights into the running cases and timely mitigating non-conformance, if any. State-of-the-art online conformance checking approaches bound the memory by either delimiting storage of the events per case or limiting the number of cases to a specific window width. The former technique still requires unbounded memory as the number of cases to store is unlimited, while the latter technique forgets running, not yet concluded, cases to conform to the limited window width. Consequently, the processing system may later encounter events that represent some intermediate activity as per the process model and for which the relevant case has been forgotten, to be referred to as orphan events. The naïve approach to cope with an orphan event is to either neglect its relevant case for conformance checking or treat it as an altogether new case. However, this might result in misleading process insights, for instance, overestimated non-conformance. In order to bound memory yet effectively incorporate the orphan events into processing, we propose an imputation of missing-prefix approach for such orphan events. Our approach utilizes the existing process model for imputing the missing prefix. Furthermore, we leverage the case storage management to increase the accuracy of the prefix prediction. We propose a systematic forgetting mechanism that distinguishes and forgets the cases that can be reliably regenerated as prefix upon receipt of their future orphan event. We evaluate the efficacy of our proposed approach through multiple experiments with synthetic and three real event logs while simulating a streaming setting. Our approach achieves considerably higher realistic conformance statistics than the state of the art while requiring the same storage.},
  archive      = {J_FDATA},
  author       = {Zaman, Rashid and Hassani, Marwan and Van Dongen, Boudewijn F.},
  doi          = {10.3389/fdata.2021.705243},
  journal      = {Frontiers in Big Data},
  month        = {10},
  pages        = {705243},
  shortjournal = {Front. Big Data},
  title        = {Prefix imputation of orphan events in event stream processing},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Data-driven modeling of breast cancer tumors using boolean
networks. <em>FDATA</em>, <em>4</em>, 656395. (<a
href="https://doi.org/10.3389/fdata.2021.656395">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cancer is a genomic disease involving various intertwined pathways with complex cross-communication links. Conceptually, this complex interconnected system forms a network, which allows one to model the dynamic behavior of the elements that characterize it to describe the entire system’s development in its various evolutionary stages of carcinogenesis. Knowing the activation or inhibition status of the genes that make up the network during its temporal evolution is necessary for the rational intervention on the critical factors for controlling the system’s dynamic evolution. In this report, we proposed a methodology for building data-driven boolean networks that model breast cancer tumors. We defined the network components and topology based on gene expression data from RNA-seq of breast cancer cell lines. We used a Boolean logic formalism to describe the network dynamics. The combination of single-cell RNA-seq and interactome data enabled us to study the dynamics of malignant subnetworks of up-regulated genes. First, we used the same Boolean function construction scheme for each network node, based on canalyzing functions. Using single-cell breast cancer datasets from The Cancer Genome Atlas, we applied a binarization algorithm. The binarized version of scRNA-seq data allowed identifying attractors specific to patients and critical genes related to each breast cancer subtype. The model proposed in this report may serve as a basis for a methodology to detect critical genes involved in malignant attractor stability, whose inhibition could have potential applications in cancer theranostics.},
  archive      = {J_FDATA},
  author       = {Sgariglia, Domenico and Conforte, Alessandra Jordano and Pedreira, Carlos Eduardo and Vidal de Carvalho, Luis Alfredo and Carneiro, Flavia Raquel Gonçalves and Carels, Nicolas and Silva, Fabricio Alves Barbosa da},
  doi          = {10.3389/fdata.2021.656395},
  journal      = {Frontiers in Big Data},
  month        = {10},
  pages        = {656395},
  shortjournal = {Front. Big Data},
  title        = {Data-driven modeling of breast cancer tumors using boolean networks},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep learning exploration of agent-based social network
model parameters. <em>FDATA</em>, <em>4</em>, 739081. (<a
href="https://doi.org/10.3389/fdata.2021.739081">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interactions between humans give rise to complex social networks that are characterized by heterogeneous degree distribution, weight-topology relation, overlapping community structure, and dynamics of links. Understanding these characteristics of social networks is the primary goal of their research as they constitute scaffolds for various emergent social phenomena from disease spreading to political movements. An appropriate tool for studying them is agent-based modeling, in which nodes, representing individuals, make decisions about creating and deleting links, thus yielding various macroscopic behavioral patterns. Here we focus on studying a generalization of the weighted social network model, being one of the most fundamental agent-based models for describing the formation of social ties and social networks. This generalized weighted social network (GWSN) model incorporates triadic closure, homophilic interactions, and various link termination mechanisms, which have been studied separately in the previous works. Accordingly, the GWSN model has an increased number of input parameters and the model behavior gets excessively complex, making it challenging to clarify the model behavior. We have executed massive simulations with a supercomputer and used the results as the training data for deep neural networks to conduct regression analysis for predicting the properties of the generated networks from the input parameters. The obtained regression model was also used for global sensitivity analysis to identify which parameters are influential or insignificant. We believe that this methodology is applicable for a large class of complex network models, thus opening the way for more realistic quantitative agent-based modeling.},
  archive      = {J_FDATA},
  author       = {Murase, Yohsuke and Jo, Hang-Hyun and Török, János and Kertész, János and Kaski, Kimmo},
  doi          = {10.3389/fdata.2021.739081},
  journal      = {Frontiers in Big Data},
  month        = {9},
  pages        = {739081},
  shortjournal = {Front. Big Data},
  title        = {Deep learning exploration of agent-based social network model parameters},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Digital support for renal patients before and during the
COVID-19 pandemic: Examining the efforts of singapore social service
agencies in facebook. <em>FDATA</em>, <em>4</em>, 737507. (<a
href="https://doi.org/10.3389/fdata.2021.737507">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {During the coronavirus disease 2019 (COVID-19) pandemic, social service agencies (SSAs) play a crucial role in supporting renal patients, who are particularly vulnerable to infections. Social media platforms such as Facebook, serves as an effective medium for these SSAs to disseminate information. Content analysis of the SSAs’ Facebook posts can provide insights on whether Facebook has been adequately utilized during the COVID-19 pandemic and enable SSAs to improve their social media use in future pandemics. This study aimed to compare renal-related SSAs’ Facebook post content before and during the COVID-19 pandemic. Facebook posts of three SSAs National Kidney Foundation (NKF), Kidney Dialysis Foundation (KDF), and Muslim Kidney Action Association (MKAC), posted during the pre-COVID-19 period (January 23, 2019 to June 2, 2019) and the peri-COVID-19 period (January 23, 2020 to June 1, 2020) were extracted. A classification scheme was developed by two coders with themes derived inductively and deductively. Each Facebook post was assigned with a theme. Quantitative analyses indicate that the number of Facebook posts increased from 115 in the pre-COVID-19 period to 293 in the peri-COVID-19 period. During peri-COVID-19, posts regarding lifestyle changes, donations and infectious disease surfaced. While the proportion of posts about encouraging kindness increased from one to 77 posts, the proportion of posts about community-based events and psychosocial support decreased from 44 to 15 posts and 17 to 10 posts respectively during the two periods. Facebook was found to be well-utilized by two of the three renal SSAs in engaging their beneficiaries during the pandemic. During future pandemics, renal SSAs should place emphasis on posts related to psychosocial support and encouraging kindness. Further studies are required to ascertain the impact of COVID-19 from the perspective of renal patients and also to validate the classification scheme which was developed in this study. The study’s methodology and classification scheme can be used to guide future studies for evaluating the social media outreach performance of renal health support groups.},
  archive      = {J_FDATA},
  author       = {Tan, Junjie and Sesagiri Raamkumar, Aravind and Wee, Hwee Lin},
  doi          = {10.3389/fdata.2021.737507},
  journal      = {Frontiers in Big Data},
  month        = {9},
  pages        = {737507},
  shortjournal = {Front. Big Data},
  title        = {Digital support for renal patients before and during the COVID-19 pandemic: Examining the efforts of singapore social service agencies in facebook},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The rise of populism and the reconfiguration of the german
political space. <em>FDATA</em>, <em>4</em>, 731349. (<a
href="https://doi.org/10.3389/fdata.2021.731349">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper explores the notion of a reconfiguration of political space in the context of the rise of populism and its effects on the political system. We focus on Germany and the appearance of the new right wing party “Alternative for Germany” (AfD). The idea of a political space is closely connected to the ubiquitous use of spatial metaphors in political talk. In particular the idea of a “distance” between “political positions” would suggest that political actors are situated in a metric space. Using the electoral manifestos from the Manifesto project database we investigate to which extent the spatial metaphors so common in political talk can be brought to mathematical rigor. Many scholars of politics discuss the rise of the new populism in Western Europe and the United States with respect to a new political cleavage related to globalization, which is assumed to mainly affect the cultural dimension of the political space. As such, it might replace the older economic cleavage based on class divisions in defining the dominant dimension of political conflict. An explanation along these lines suggests a reconfiguration of the political space in the sense that 1) the main cleavage within the political space changes its direction from the economic axis towards the cultural axis, but 2) also the semantics of the cultural axis itself is changing towards globalization related topics. In this paper, we empirically address this reconfiguration of the political space by comparing political spaces for Germany built using topic modeling with the spaces based on the content analysis of the Manifesto project and the corresponding categories of political goals. We find that both spaces have a similar structure and that the AfD appears on a new dimension. In order to characterize this new dimension we employ a novel technique, inter-issue consistency networks (IICN) that allow to analyze the evolution of the correlations between the political positions on different issues over several elections. We find that the new dimension introduced by the AfD can be related to the split off of a new “cultural right” issue bundle from the previously existing center-right bundle.},
  archive      = {J_FDATA},
  author       = {Olbrich, Eckehard and Banisch, Sven},
  doi          = {10.3389/fdata.2021.731349},
  journal      = {Frontiers in Big Data},
  month        = {9},
  pages        = {731349},
  shortjournal = {Front. Big Data},
  title        = {The rise of populism and the reconfiguration of the german political space},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Statistical enrichment analysis of samples: A
general-purpose tool to annotate metadata neighborhoods of biological
samples. <em>FDATA</em>, <em>4</em>, 725276. (<a
href="https://doi.org/10.3389/fdata.2021.725276">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised learning techniques, such as clustering and embedding, have been increasingly popular to cluster biomedical samples from high-dimensional biomedical data. Extracting clinical data or sample meta-data shared in common among biomedical samples of a given biological condition remains a major challenge. Here, we describe a powerful analytical method called Statistical Enrichment Analysis of Samples (SEAS) for interpreting clustered or embedded sample data from omics studies. The method derives its power by focusing on sample sets, i.e., groups of biological samples that were constructed for various purposes, e.g., manual curation of samples sharing specific characteristics or automated clusters generated by embedding sample omic profiles from multi-dimensional omics space. The samples in the sample set share common clinical measurements, which we refer to as “clinotypes,” such as age group, gender, treatment status, or survival days. We demonstrate how SEAS yields insights into biological data sets using glioblastoma (GBM) samples. Notably, when analyzing the combined The Cancer Genome Atlas (TCGA)—patient-derived xenograft (PDX) data, SEAS allows approximating the different clinical outcomes of radiotherapy-treated PDX samples, which has not been solved by other tools. The result shows that SEAS may support the clinical decision. The SEAS tool is publicly available as a freely available software package at https://aimed-lab.shinyapps.io/SEAS/.},
  archive      = {J_FDATA},
  author       = {Nguyen, Thanh M. and Bharti, Samuel and Yue, Zongliang and Willey, Christopher D. and Chen, Jake Y.},
  doi          = {10.3389/fdata.2021.725276},
  journal      = {Frontiers in Big Data},
  month        = {9},
  pages        = {725276},
  shortjournal = {Front. Big Data},
  title        = {Statistical enrichment analysis of samples: A general-purpose tool to annotate metadata neighborhoods of biological samples},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). NPARS—a novel approach to address accuracy and
reproducibility in genomic data science. <em>FDATA</em>, <em>4</em>,
725095. (<a href="https://doi.org/10.3389/fdata.2021.725095">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background: Accuracy and reproducibility are vital in science and presents a significant challenge in the emerging discipline of data science, especially when the data are scientifically complex and massive in size. Further complicating matters, in the field of genomic-based science high-throughput sequencing technologies generate considerable amounts of data that needs to be stored, manipulated, and analyzed using a plethora of software tools. Researchers are rarely able to reproduce published genomic studies.Results: Presented is a novel approach which facilitates accuracy and reproducibility for large genomic research data sets. All data needed is loaded into a portable local database, which serves as an interface for well-known software frameworks. These include python-based Jupyter Notebooks and the use of RStudio projects and R markdown. All software is encapsulated using Docker containers and managed by Git, simplifying software configuration management.Conclusion: Accuracy and reproducibility in science is of a paramount importance. For the biomedical sciences, advances in high throughput technologies, molecular biology and quantitative methods are providing unprecedented insights into disease mechanisms. With these insights come the associated challenge of scientific data that is complex and massive in size. This makes collaboration, verification, validation, and reproducibility of findings difficult. To address these challenges the NGS post-pipeline accuracy and reproducibility system (NPARS) was developed. NPARS is a robust software infrastructure and methodology that can encapsulate data, code, and reporting for large genomic studies. This paper demonstrates the successful use of NPARS on large and complex genomic data sets across different computational platforms.},
  archive      = {J_FDATA},
  author       = {Ma, Li and Peterson, Erich A. and Shin, Ik Jae and Muesse, Jason and Marino, Katy and Steliga, Matthew A. and Johann, Donald J.},
  doi          = {10.3389/fdata.2021.725095},
  journal      = {Frontiers in Big Data},
  month        = {9},
  pages        = {725095},
  shortjournal = {Front. Big Data},
  title        = {NPARS—A novel approach to address accuracy and reproducibility in genomic data science},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). TSI-GNN: Extending graph neural networks to handle missing
data in temporal settings. <em>FDATA</em>, <em>4</em>, 693869. (<a
href="https://doi.org/10.3389/fdata.2021.693869">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel approach for imputing missing data that incorporates temporal information into bipartite graphs through an extension of graph representation learning. Missing data is abundant in several domains, particularly when observations are made over time. Most imputation methods make strong assumptions about the distribution of the data. While novel methods may relax some assumptions, they may not consider temporality. Moreover, when such methods are extended to handle time, they may not generalize without retraining. We propose using a joint bipartite graph approach to incorporate temporal sequence information. Specifically, the observation nodes and edges with temporal information are used in message passing to learn node and edge embeddings and to inform the imputation task. Our proposed method, temporal setting imputation using graph neural networks (TSI-GNN), captures sequence information that can then be used within an aggregation function of a graph neural network. To the best of our knowledge, this is the first effort to use a joint bipartite graph approach that captures sequence information to handle missing data. We use several benchmark datasets to test the performance of our method against a variety of conditions, comparing to both classic and contemporary methods. We further provide insight to manage the size of the generated TSI-GNN model. Through our analysis we show that incorporating temporal information into a bipartite graph improves the representation at the 30% and 60% missing rate, specifically when using a nonlinear model for downstream prediction tasks in regularly sampled datasets and is competitive with existing temporal methods under different scenarios.},
  archive      = {J_FDATA},
  author       = {Gordon, David and Petousis, Panayiotis and Zheng, Henry and Zamanzadeh, Davina and Bui, Alex A.T.},
  doi          = {10.3389/fdata.2021.693869},
  journal      = {Frontiers in Big Data},
  month        = {9},
  pages        = {693869},
  shortjournal = {Front. Big Data},
  title        = {TSI-GNN: Extending graph neural networks to handle missing data in temporal settings},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Factor-based framework for multivariate and multi-step-ahead
forecasting of large scale time series. <em>FDATA</em>, <em>4</em>,
690267. (<a href="https://doi.org/10.3389/fdata.2021.690267">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {State-of-the-art multivariate forecasting methods are restricted to low dimensional tasks, linear dependencies and short horizons. The technological advances (notably the Big data revolution) are instead shifting the focus to problems characterized by a large number of variables, non-linear dependencies and long forecasting horizons. In the last few years, the majority of the best performing techniques for multivariate forecasting have been based on deep-learning models. However, such models are characterized by high requirements in terms of data availability and computational resources and suffer from a lack of interpretability. To cope with the limitations of these methods, we propose an extension to the DFML framework, a hybrid forecasting technique inspired by the Dynamic Factor Model (DFM) approach, a successful forecasting methodology in econometrics. This extension improves the capabilities of the DFM approach, by implementing and assessing both linear and non-linear factor estimation techniques as well as model-driven and data-driven factor forecasting techniques. We assess several method integrations within the DFML, and we show that the proposed technique provides competitive results both in terms of forecasting accuracy and computational efficiency on multiple very large-scale (&amp;gt;102 variables and &amp;gt; 103 samples) real forecasting tasks.},
  archive      = {J_FDATA},
  author       = {De Stefani, Jacopo and Bontempi, Gianluca},
  doi          = {10.3389/fdata.2021.690267},
  journal      = {Frontiers in Big Data},
  month        = {9},
  pages        = {690267},
  shortjournal = {Front. Big Data},
  title        = {Factor-based framework for multivariate and multi-step-ahead forecasting of large scale time series},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Supporting regulatory measures in the context of big data
applications for smart grids. <em>FDATA</em>, <em>4</em>, 675461. (<a
href="https://doi.org/10.3389/fdata.2021.675461">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_FDATA},
  author       = {Mladin, Mihai A.},
  doi          = {10.3389/fdata.2021.675461},
  journal      = {Frontiers in Big Data},
  month        = {9},
  pages        = {675461},
  shortjournal = {Front. Big Data},
  title        = {Supporting regulatory measures in the context of big data applications for smart grids},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Data ecosystems for scientific experiments: Managing
combustion experiments and simulation analyses in chemical engineering.
<em>FDATA</em>, <em>4</em>, 663410. (<a
href="https://doi.org/10.3389/fdata.2021.663410">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development of scientific predictive models has been of great interest over the decades. A scientific model is capable of forecasting domain outcomes without the necessity of performing expensive experiments. In particular, in combustion kinetics, the model can help improving the combustion facilities and the fuel efficiency reducing the pollutants. At the same time, the amount of available scientific data has increased and helped speeding up the continuous cycle of model improvement and validation. This has also opened new opportunities for leveraging a large amount of data to support knowledge extraction. However, experiments are affected by several data quality problems since they are a collection of information over several decades of research, each characterized by different representation formats and reasons of uncertainty. In this context, it is necessary to develop an automatic data ecosystem capable of integrating heterogeneous information sources while maintaining a quality repository. We present an innovative approach to data quality management from the chemical engineering domain, based on an available prototype of a scientific framework, SciExpeM, which has been significantly extended. We identified a new methodology from the model development research process that systematically extracts knowledge from the experimental data and the predictive model. In the paper, we show how our general framework could support the model development process, and save precious research time also in other experimental domains with similar characteristics, i.e., managing numerical data from experiments.},
  archive      = {J_FDATA},
  author       = {Ramalli, Edoardo and Scalia, Gabriele and Pernici, Barbara and Stagni, Alessandro and Cuoci, Alberto and Faravelli, Tiziano},
  doi          = {10.3389/fdata.2021.663410},
  journal      = {Frontiers in Big Data},
  month        = {9},
  pages        = {663410},
  shortjournal = {Front. Big Data},
  title        = {Data ecosystems for scientific experiments: Managing combustion experiments and simulation analyses in chemical engineering},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Prognostic factors of survival in pancreatic cancer
metastasis to liver at different ages of diagnosis: A SEER
population-based cohort study. <em>FDATA</em>, <em>4</em>, 654972. (<a
href="https://doi.org/10.3389/fdata.2021.654972">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background: Liver is a common metastatic organ for most malignancies, especially the pancreas. However, evidence for prognostic factors of pancreatic cancer metastasis to the liver at different ages is lacking. Thus, we aimed to evaluate the predictors of patients with pancreatic cancer metastasis to liver grouped by age of diagnosis.Methods: We chose the patients diagnosed between 2004 and 2015 from the SEER database. The primary lesions of metastatic liver cancer between sexes were compared using the Pearson’s chi-square test for categorical variables. The overall survival (OS) and cancer-specific survival (CSS) were the endpoint of the study. The prognostic factors were analyzed with the Kaplan-Meier method and log-rank test, and Cox proportional-hazards regression model.Results: The main primary sites of metastatic liver cancer for our patients are lung and brunchu, sigmoid colon, pancreas, which in males are lung and bronchu, sigmoid colon and pancreas, while breast, lung and bronchu, sigmoid colon in females. Furthermore, we explored the prognostic factors of pancreatic cancer metastasis to liver grouped by age at diagnosis. Tumor grade, histology and treatment are valid prognostic factors in all age groups. Additionally, gender and AJCC N stage in age&amp;lt;52 years old, while race and AJCC N stage in age &amp;gt;69 years old were predictors. Surgery alone was the optimal treatment in group age&amp;gt;69 years old, whereas surgery combined with chemotherapy was the best option in the other groups.Conclusion: Our study evaluated the predictors of patients with pancreatic cancer metastasis to liver at various ages of diagnosis.},
  archive      = {J_FDATA},
  author       = {Liu, Meiqi and Wang, Moran and Li, Sheng},
  doi          = {10.3389/fdata.2021.654972},
  journal      = {Frontiers in Big Data},
  month        = {9},
  pages        = {654972},
  shortjournal = {Front. Big Data},
  title        = {Prognostic factors of survival in pancreatic cancer metastasis to liver at different ages of diagnosis: A SEER population-based cohort study},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Editorial: Ethical machine learning and artificial
intelligence. <em>FDATA</em>, <em>4</em>, 742589. (<a
href="https://doi.org/10.3389/fdata.2021.742589">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_FDATA},
  author       = {Quadrianto, Novi and Schuller, Björn W. and Lattimore, Finnian Rachel},
  doi          = {10.3389/fdata.2021.742589},
  journal      = {Frontiers in Big Data},
  month        = {8},
  pages        = {742589},
  shortjournal = {Front. Big Data},
  title        = {Editorial: Ethical machine learning and artificial intelligence},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Editorial: Innovative analysis ecosystems for HEP data.
<em>FDATA</em>, <em>4</em>, 736105. (<a
href="https://doi.org/10.3389/fdata.2021.736105">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This editorial summarizes the contributions to the Frontiers Research topic “Innovative Analysis Ecosystems for HEP Data”, established under the Big Data and AI in High Energy Physics section and appearing under the Frontiers in Big Data and Frontiers in Artificial Intelligence journals.},
  archive      = {J_FDATA},
  author       = {Sekmen, Sezen and Innocenti, Gian Michele and Jayatilaka, Bo},
  doi          = {10.3389/fdata.2021.736105},
  journal      = {Frontiers in Big Data},
  month        = {8},
  pages        = {736105},
  shortjournal = {Front. Big Data},
  title        = {Editorial: Innovative analysis ecosystems for HEP data},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Editorial: Safe and trustworthy machine learning.
<em>FDATA</em>, <em>4</em>, 731605. (<a
href="https://doi.org/10.3389/fdata.2021.731605">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_FDATA},
  author       = {Kailkhura, Bhavya and Chen, Pin-Yu and Lin, Xue and Li, Bo},
  doi          = {10.3389/fdata.2021.731605},
  journal      = {Frontiers in Big Data},
  month        = {8},
  pages        = {731605},
  shortjournal = {Front. Big Data},
  title        = {Editorial: Safe and trustworthy machine learning},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On the vernacular language games of an antagonistic online
subculture. <em>FDATA</em>, <em>4</em>, 718368. (<a
href="https://doi.org/10.3389/fdata.2021.718368">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we develop an empirical, big data approach to analyze how alt-right vernacular concepts (such as kek and beta) were used on the notorious anonymous and ephemeral imageboard 4chan/pol/and the fan wiki Encyclopedia Dramatica. While 4chan/pol/is broadly regarded as an influential source of many of the web’s most successful memes such as Pepe the Frog, Encyclopedia Dramatica functions as a kind of satirical Wikipedia for this meme subculture, written in high concept and highly offensive vernacular style. While the site’s affordances make them distinct, they are connected by a subcultural style and politics that has recently become increasingly connected with violent right-wing activism, forming a loose subcultural language community. Contrary to “memetic” theories of cultural evolution in media studies, our analysis draws on theoretical frameworks from poststructuralist and pragmatist philosophies of language and deploys empirical techniques from corpus linguistics to consider the role of online platforms in shaping these vernacular modes of expression. This approach helps us to identify instances of vernacular innovation within these corpora from 2012-2020—a period during which the white supremacist “alt-right” movement arose online. Through these analyses we contribute both to ongoing interdisciplinary attempts to bridge the gap between cultural-theoretical and computational-linguistic approaches to studying online subcultures, and to the empirical study of the vernacular roots of the “toxic memes” that appear to be an increasingly common feature on social media.},
  archive      = {J_FDATA},
  author       = {Peeters, Stijn and Tuters, Marc and Willaert, Tom and de Zeeuw, Daniël},
  doi          = {10.3389/fdata.2021.718368},
  journal      = {Frontiers in Big Data},
  month        = {8},
  pages        = {718368},
  shortjournal = {Front. Big Data},
  title        = {On the vernacular language games of an antagonistic online subculture},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Structural compression of convolutional neural networks with
applications in interpretability. <em>FDATA</em>, <em>4</em>, 704182.
(<a href="https://doi.org/10.3389/fdata.2021.704182">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep convolutional neural networks (CNNs) have been successful in many tasks in machine vision, however, millions of weights in the form of thousands of convolutional filters in CNNs make them difficult for human interpretation or understanding in science. In this article, we introduce a greedy structural compression scheme to obtain smaller and more interpretable CNNs, while achieving close to original accuracy. The compression is based on pruning filters with the least contribution to the classification accuracy or the lowest Classification Accuracy Reduction (CAR) importance index. We demonstrate the interpretability of CAR-compressed CNNs by showing that our algorithm prunes filters with visually redundant functionalities such as color filters. These compressed networks are easier to interpret because they retain the filter diversity of uncompressed networks with an order of magnitude fewer filters. Finally, a variant of CAR is introduced to quantify the importance of each image category to each CNN filter. Specifically, the most and the least important class labels are shown to be meaningful interpretations of each filter.},
  archive      = {J_FDATA},
  author       = {Abbasi-Asl, Reza and Yu, Bin},
  doi          = {10.3389/fdata.2021.704182},
  journal      = {Frontiers in Big Data},
  month        = {8},
  pages        = {704182},
  shortjournal = {Front. Big Data},
  title        = {Structural compression of convolutional neural networks with applications in interpretability},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A policy-driven approach to secure extraction of COVID-19
data from research papers. <em>FDATA</em>, <em>4</em>, 701966. (<a
href="https://doi.org/10.3389/fdata.2021.701966">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The entire scientific and academic community has been mobilized to gain a better understanding of the COVID-19 disease and its impact on humanity. Most research related to COVID-19 needs to analyze large amounts of data in very little time. This urgency has made Big Data Analysis, and related questions around the privacy and security of the data, an extremely important part of research in the COVID-19 era. The White House OSTP has, for example, released a large dataset of papers related to COVID research from which the research community can extract knowledge and information. We show an example system with a machine learning-based knowledge extractor which draws out key medical information from COVID-19 related academic research papers. We represent this knowledge in a Knowledge Graph that uses the Unified Medical Language System (UMLS). However, publicly available studies rely on dataset that might have sensitive data. Extracting information from academic papers can potentially leak sensitive data, and protecting the security and privacy of this data is equally important. In this paper, we address the key challenges around the privacy and security of such information extraction and analysis systems. Policy regulations like HIPAA have updated the guidelines to access data, specifically, data related to COVID-19, securely. In the US, healthcare providers must also comply with the Office of Civil Rights (OCR) rules to protect data integrity in matters like plasma donation, media access to health care data, telehealth communications, etc. Privacy policies are typically short and unstructured HTML or PDF documents. We have created a framework to extract relevant knowledge from the health centers’ policy documents and also represent these as a knowledge graph. Our framework helps to understand the extent to which individual provider policies comply with regulations and define access control policies that enforce the regulation rules on data in the knowledge graph extracted from COVID-related papers. Along with being compliant, privacy policies must also be transparent and easily understood by the clients. We analyze the relative readability of healthcare privacy policies and discuss the impact. In this paper, we develop a framework for access control decisions that uses policy compliance information to securely retrieve COVID data. We show how policy compliance information can be used to restrict access to COVID-19 data and information extracted from research papers.},
  archive      = {J_FDATA},
  author       = {Elluri, Lavanya and Piplai, Aritran and Kotal, Anantaa and Joshi, Anupam and Joshi, Karuna Pande},
  doi          = {10.3389/fdata.2021.701966},
  journal      = {Frontiers in Big Data},
  month        = {8},
  pages        = {701966},
  shortjournal = {Front. Big Data},
  title        = {A policy-driven approach to secure extraction of COVID-19 data from research papers},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Unified representation of twitter and online news using
graph and entities. <em>FDATA</em>, <em>4</em>, 699070. (<a
href="https://doi.org/10.3389/fdata.2021.699070">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To improve consumer engagement and satisfaction, online news services employ strategies for personalizing and recommending articles to their users based on their interests. In addition to news agencies’ own digital platforms, they also leverage social media to reach out to a broad user base. These engagement efforts are often disconnected with each other, but present a compelling opportunity to incorporate engagement data from social media to inform their digital news platform and vice-versa, leading to a more personalized experience for users. While this idea seems intuitive, there are several challenges due to the disparate nature of the two sources. In this paper, we propose a model to build a generalized graph of news articles and tweets that can be used for different downstream tasks such as identifying sentiment, trending topics, and misinformation, as well as sharing relevant articles on social media in a timely fashion. We evaluate our framework on a downstream task of identifying related pairs of news articles and tweets with promising results. The content unification problem addressed by our model is not unique to the domain of news, and thus can be applicable to other problems linking different content platforms.},
  archive      = {J_FDATA},
  author       = {Syed, Munira and Wang, Daheng and Jiang, Meng and Conway, Oliver and Juneja, Vishal and Subramanian, Sriram and Chawla, Nitesh V.},
  doi          = {10.3389/fdata.2021.699070},
  journal      = {Frontiers in Big Data},
  month        = {8},
  pages        = {699070},
  shortjournal = {Front. Big Data},
  title        = {Unified representation of twitter and online news using graph and entities},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Which neural network to choose for post-fault localization,
dynamic state estimation, and optimal measurement placement in power
systems? <em>FDATA</em>, <em>4</em>, 692493. (<a
href="https://doi.org/10.3389/fdata.2021.692493">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a power transmission system monitored using phasor measurement units (PMUs) placed at significant, but not all, nodes of the system. Assuming that a sufficient number of distinct single-line faults, specifically the pre-fault state and the (not cleared) post-fault state, are recorded by the PMUs and are available for training, we first design a comprehensive sequence of neural networks (NNs) locating the faulty line. Performance of different NNs in the sequence, including linear regression, feed-forward NNs, AlexNet, graph convolutional NNs, neural linear ordinary differential equations (ODEs) and neural graph-based ODEs, ordered according to the type and amount of the power flow physics involved, are compared for different levels of observability. Second, we build a sequence of advanced power system dynamics–informed and neural ODE–based machine learning schemes that are trained, given the pre-fault state, to predict the post-fault state and also, in parallel, to estimate system parameters. Finally, third and continuing to work with the first (fault localization) setting, we design an (NN-based) algorithm which discovers optimal PMU placement.},
  archive      = {J_FDATA},
  author       = {Afonin, Andrei and Chertkov, Michael},
  doi          = {10.3389/fdata.2021.692493},
  journal      = {Frontiers in Big Data},
  month        = {8},
  pages        = {692493},
  shortjournal = {Front. Big Data},
  title        = {Which neural network to choose for post-fault localization, dynamic state estimation, and optimal measurement placement in power systems?},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Computer vision for continuous bedside pharmacological data
extraction: A novel application of artificial intelligence for clinical
data recording and biomedical research. <em>FDATA</em>, <em>4</em>,
689358. (<a href="https://doi.org/10.3389/fdata.2021.689358">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Introduction: As real time data processing is integrated with medical care for traumatic brain injury (TBI) patients, there is a requirement for devices to have digital output. However, there are still many devices that fail to have the required hardware to export real time data into an acceptable digital format or in a continuously updating manner. This is particularly the case for many intravenous pumps and older technological systems. Such accurate and digital real time data integration within TBI care and other fields is critical as we move towards digitizing healthcare information and integrating clinical data streams to improve bedside care. We propose to address this gap in technology by building a system that employs Optical Character Recognition through computer vision, using real time images from a pump monitor to extract the desired real time information.Methods: Using freely available software and readily available technology, we built a script that extracts real time images from a medication pump and then processes them using Optical Character Recognition to create digital text from the image. This text was then transferred to an ICM + real-time monitoring software in parallel with other retrieved physiological data.Results: The prototype that was built works effectively for our device, with source code openly available to interested end-users. However, future work is required for a more universal application of such a system.Conclusion: Advances here can improve medical information collection in the clinical environment, eliminating human error with bedside charting, and aid in data integration for biomedical research where many complex data sets can be seamlessly integrated digitally. Our design demonstrates a simple adaptation of current technology to help with this integration.},
  archive      = {J_FDATA},
  author       = {Froese, Logan and Dian, Joshua and Batson, Carleen and Gomez, Alwyn and Sainbhi, Amanjyot Singh and Unger, Bertram and Zeiler, Frederick A.},
  doi          = {10.3389/fdata.2021.689358},
  journal      = {Frontiers in Big Data},
  month        = {8},
  pages        = {689358},
  shortjournal = {Front. Big Data},
  title        = {Computer vision for continuous bedside pharmacological data extraction: A novel application of artificial intelligence for clinical data recording and biomedical research},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Disease modelling of cognitive outcomes and biomarkers in
the european prevention of alzheimer’s dementia longitudinal cohort.
<em>FDATA</em>, <em>4</em>, 676168. (<a
href="https://doi.org/10.3389/fdata.2021.676168">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A key challenge for the secondary prevention of Alzheimer’s dementia is the need to identify individuals early on in the disease process through sensitive cognitive tests and biomarkers. The European Prevention of Alzheimer’s Dementia (EPAD) consortium recruited participants into a longitudinal cohort study with the aim of building a readiness cohort for a proof-of-concept clinical trial and also to generate a rich longitudinal data-set for disease modelling. Data have been collected on a wide range of measurements including cognitive outcomes, neuroimaging, cerebrospinal fluid biomarkers, genetics and other clinical and environmental risk factors, and are available for 1,828 eligible participants at baseline, 1,567 at 6 months, 1,188 at one-year follow-up, 383 at 2 years, and 89 participants at three-year follow-up visit. We novelly apply state-of-the-art longitudinal modelling and risk stratification approaches to these data in order to characterise disease progression and biological heterogeneity within the cohort. Specifically, we use longitudinal class-specific mixed effects models to characterise the different clinical disease trajectories and a semi-supervised Bayesian clustering approach to explore whether participants can be stratified into homogeneous subgroups that have different patterns of cognitive functioning evolution, while also having subgroup-specific profiles in terms of baseline biomarkers and longitudinal rate of change in biomarkers.},
  archive      = {J_FDATA},
  author       = {Howlett, James and Hill, Steven M. and Ritchie, Craig W. and Tom, Brian D. M.},
  doi          = {10.3389/fdata.2021.676168},
  journal      = {Frontiers in Big Data},
  month        = {8},
  pages        = {676168},
  shortjournal = {Front. Big Data},
  title        = {Disease modelling of cognitive outcomes and biomarkers in the european prevention of alzheimer’s dementia longitudinal cohort},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A predictive maintenance model for flexible manufacturing in
the context of industry 4.0. <em>FDATA</em>, <em>4</em>, 663466. (<a
href="https://doi.org/10.3389/fdata.2021.663466">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Industry 4.0 paradigm is the focus of modern manufacturing system design. The integration of cutting-edge technologies such as the Internet of things, cyber–physical systems, big data analytics, and cloud computing requires a flexible platform supporting the effective optimization of manufacturing-related processes, e.g., predictive maintenance. Existing predictive maintenance studies generally focus on either a predictive model without considering the maintenance decisions or maintenance optimizations based on the degradation models of the known system. To address this, we propose PMMI 4.0, a Predictive Maintenance Model for Industry 4.0, which utilizes a newly proposed solution PMS4MMC for supporting an optimized maintenance schedule plan for multiple machine components driven by a data-driven LSTM model for RUL (remaining useful life) estimation. The effectiveness of the proposed solution is demonstrated using a real-world industrial case with related data. The results showed the validity and applicability of this work.},
  archive      = {J_FDATA},
  author       = {Sang, Go Muan and Xu, Lai and de Vrieze, Paul},
  doi          = {10.3389/fdata.2021.663466},
  journal      = {Frontiers in Big Data},
  month        = {8},
  pages        = {663466},
  shortjournal = {Front. Big Data},
  title        = {A predictive maintenance model for flexible manufacturing in the context of industry 4.0},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A multi-study model-based evaluation of the sequence of
imaging and clinical biomarker changes in huntington’s disease.
<em>FDATA</em>, <em>4</em>, 662200. (<a
href="https://doi.org/10.3389/fdata.2021.662200">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding the order and progression of change in biomarkers of neurodegeneration is essential to detect the effects of pharmacological interventions on these biomarkers. In Huntington’s disease (HD), motor, cognitive and MRI biomarkers are currently used in clinical trials of drug efficacy. Here for the first time we use directly compare data from three large observational studies of HD (total N = 532) using a probabilistic event-based model (EBM) to characterise the order in which motor, cognitive and MRI biomarkers become abnormal. We also investigate the impact of the genetic cause of HD, cytosine-adenine-guanine (CAG) repeat length, on progression through these stages. We find that EBM uncovers a broadly consistent order of events across all three studies; that EBM stage reflects clinical stage; and that EBM stage is related to age and genetic burden. Our findings indicate that measures of subcortical and white matter volume become abnormal prior to clinical and cognitive biomarkers. Importantly, CAG repeat length has a large impact on the timing of onset of each stage and progression through the stages, with a longer repeat length resulting in earlier onset and faster progression. Our results can be used to help design clinical trials of treatments for Huntington’s disease, influencing the choice of biomarkers and the recruitment of participants.},
  archive      = {J_FDATA},
  author       = {Wijeratne, Peter A. and Johnson, Eileanoir B. and Gregory, Sarah and Georgiou-Karistianis, Nellie and Paulsen, Jane S. and Scahill, Rachael I. and Tabrizi, Sarah J. and Alexander, Daniel C.},
  doi          = {10.3389/fdata.2021.662200},
  journal      = {Frontiers in Big Data},
  month        = {8},
  pages        = {662200},
  shortjournal = {Front. Big Data},
  title        = {A multi-study model-based evaluation of the sequence of imaging and clinical biomarker changes in huntington’s disease},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Towards machine-readable (meta) data and the FAIR value for
artificial intelligence exploration of COVID-19 and cancer research
data. <em>FDATA</em>, <em>4</em>, 656553. (<a
href="https://doi.org/10.3389/fdata.2021.656553">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_FDATA},
  author       = {Campos, Maria Luiza. M. and Silva, Eugênio and Cerceau, Renato and Cruz, Sérgio Manuel Serra da and Silva, Fabricio A. B. and Gouveia, Fábio. C. and Jardim, Rodrigo and Kotowski, Nelson and Lopes, Giseli Rabello and Dávila, Alberto. M. R.},
  doi          = {10.3389/fdata.2021.656553},
  journal      = {Frontiers in Big Data},
  month        = {8},
  pages        = {656553},
  shortjournal = {Front. Big Data},
  title        = {Towards machine-readable (Meta) data and the FAIR value for artificial intelligence exploration of COVID-19 and cancer research data},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Benchmarking of data-driven causality discovery approaches
in the interactions of arctic sea ice and atmosphere. <em>FDATA</em>,
<em>4</em>, 642182. (<a
href="https://doi.org/10.3389/fdata.2021.642182">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Arctic sea ice has retreated rapidly in the past few decades, which is believed to be driven by various dynamic and thermodynamic processes in the atmosphere. The newly open water resulted from sea ice decline in turn exerts large influence on the atmosphere. Therefore, this study aims to investigate the causality between multiple atmospheric processes and sea ice variations using three distinct data-driven causality approaches that have been proposed recently: Temporal Causality Discovery Framework Non-combinatorial Optimization via Trace Exponential and Augmented lagrangian for Structure learning (NOTEARS) and Directed Acyclic Graph-Graph Neural Networks (DAG-GNN). We apply these three algorithms to 39 years of historical time-series data sets, which include 11 atmospheric variables from ERA-5 reanalysis product and passive microwave satellite retrieved sea ice extent. By comparing the causality graph results of these approaches with what we summarized from the literature, it shows that the static graphs produced by NOTEARS and DAG-GNN are relatively reasonable. The results from NOTEARS indicate that relative humidity and precipitation dominate sea ice changes among all variables, while the results from DAG-GNN suggest that the horizontal and meridional wind are more important for driving sea ice variations. However, both approaches produce some unrealistic cause-effect relationships. Additionally, these three methods cannot well detect the delayed impact of one variable on another in the Arctic. It also turns out that the results are rather sensitive to the choice of hyperparameters of the three methods. As a pioneer study, this work paves the way to disentangle the complex causal relationships in the Earth system, by taking the advantage of cutting-edge Artificial Intelligence technologies.},
  archive      = {J_FDATA},
  author       = {Huang, Yiyi and Kleindessner, Matthäus and Munishkin, Alexey and Varshney, Debvrat and Guo, Pei and Wang, Jianwu},
  doi          = {10.3389/fdata.2021.642182},
  journal      = {Frontiers in Big Data},
  month        = {8},
  pages        = {642182},
  shortjournal = {Front. Big Data},
  title        = {Benchmarking of data-driven causality discovery approaches in the interactions of arctic sea ice and atmosphere},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Editorial: ML and AI safety, effectiveness and
explainability in healthcare. <em>FDATA</em>, <em>4</em>, 727856. (<a
href="https://doi.org/10.3389/fdata.2021.727856">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_FDATA},
  author       = {Benrimoh, David and Israel, Sonia and Fratila, Robert and Armstrong, Caitrin and Perlman, Kelly and Rosenfeld, Ariel and Kapelner, Adam},
  doi          = {10.3389/fdata.2021.727856},
  journal      = {Frontiers in Big Data},
  month        = {7},
  pages        = {727856},
  shortjournal = {Front. Big Data},
  title        = {Editorial: ML and AI safety, effectiveness and explainability in healthcare},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Flood inundation mapping with limited observations based on
physics-aware topography constraint. <em>FDATA</em>, <em>4</em>, 707951.
(<a href="https://doi.org/10.3389/fdata.2021.707951">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatial classification with limited observations is important in geographical applications where only a subset of sensors are deployed at certain spots or partial responses are collected in field surveys. For example, in observation-based flood inundation mapping, there is a need to map the full flood extent on geographic terrains based on earth imagery that partially covers a region. Existing research mostly focuses on addressing incomplete or missing data through data cleaning and imputation or modeling missing values as hidden variables in the EM algorithm. These methods, however, assume that missing feature observations are rare and thus are ineffective in problems whereby the vast majority of feature observations are missing. To address this issue, we recently proposed a new approach that incorporates physics-aware structural constraint into the model representation. We design efficient learning and inference algorithms. This paper extends our recent approach by allowing feature values of samples in each class to follow a multi-modal distribution. Evaluations on real-world flood mapping applications show that our approach significantly outperforms baseline methods in classification accuracy, and the multi-modal extension is more robust than our early single-modal version. Computational experiments show that the proposed solution is computationally efficient on large datasets.},
  archive      = {J_FDATA},
  author       = {Sainju, Arpan Man and He, Wenchong and Jiang, Zhe and Yan, Da and Chen, Haiquan},
  doi          = {10.3389/fdata.2021.707951},
  journal      = {Frontiers in Big Data},
  month        = {7},
  pages        = {707951},
  shortjournal = {Front. Big Data},
  title        = {Flood inundation mapping with limited observations based on physics-aware topography constraint},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Moral expressions in 280 characters or less: An analysis of
politician tweets following the 2016 brexit referendum vote.
<em>FDATA</em>, <em>4</em>, 699653. (<a
href="https://doi.org/10.3389/fdata.2021.699653">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ideas about morality are deeply entrenched into political opinions. This article examines the online communication of British parliamentarians from May 2017-December 2019, following the 2016 referendum that resulted in Britain&#39;s exit (Brexit) from the European Union. It aims to uncover how British parliamentarians use moral foundations to discuss the Brexit withdrawal agreement on Twitter, using Moral Foundations Theory as a classification basis for their tweets. It is found that the majority of Brexit related tweets contain elements of moral reasoning, especially relating to the foundations of Authority and Loyalty. There are common underlying foundations between parties, but parties express opposing viewpoints within a single foundation. The study provides useful insights into Twitter’s use as an arena for moral argumentation, as well as uncovers the politician’s uses of moral arguments during Brexit agreement negotiations on Twitter. It contributes to the limited body of work focusing on the moral arguments made by politicians through Twitter.},
  archive      = {J_FDATA},
  author       = {van Vliet, Livia},
  doi          = {10.3389/fdata.2021.699653},
  journal      = {Frontiers in Big Data},
  month        = {7},
  pages        = {699653},
  shortjournal = {Front. Big Data},
  title        = {Moral expressions in 280 characters or less: An analysis of politician tweets following the 2016 brexit referendum vote},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An opinion facilitator for online news media.
<em>FDATA</em>, <em>4</em>, 695667. (<a
href="https://doi.org/10.3389/fdata.2021.695667">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With more and more voices and opinions entering the public domain, a key challenge facing journalists and editors is maximizing the context of the information that is presented on news websites. In this paper, we argue that systems for exposing readers to the many aspects of societal debates should be grounded in methods and tools that can provide a fine-grained understanding of these debates. The present article thereby explores the conceptual transition from opinion observation to opinion facilitation by introducing and discussing the Penelope opinion facilitator: a proof-of-concept reading instrument for online news media that operationalizes emerging methods for the computational analysis of cultural conflict developed in the context of the H2020 ODYCCEUS project. It will be demonstrated how these methods can be combined into an instrument that complements the reading experience of the news website The Guardian by automatically interlinking news articles on the level of semantic frames. In linguistic theory, semantic frames are defined as coherent structures of related concepts. We thereby zoom in on instances of the “causation” frame, such as “climate change causes global warming,” and illustrate how a reading instrument that links articles based on such frames might reconfigure our readings of climate news coverage, with specific attention to the case of global warming controversies. Finally, we relate our findings to the context of the development of computational social science, and discuss pathways for the evaluation of the instrument, as well as for the future upscaling of qualitative analyses and close readings.},
  archive      = {J_FDATA},
  author       = {Willaert, Tom and Van Eecke, Paul and Van Soest, Jeroen and Beuls, Katrien},
  doi          = {10.3389/fdata.2021.695667},
  journal      = {Frontiers in Big Data},
  month        = {7},
  pages        = {695667},
  shortjournal = {Front. Big Data},
  title        = {An opinion facilitator for online news media},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Comparative analysis of energy use and greenhouse gas
emission of diesel and electric trucks for food distribution in gowanus
district of new york city. <em>FDATA</em>, <em>4</em>, 693820. (<a
href="https://doi.org/10.3389/fdata.2021.693820">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {New York City’s food distribution system is among the largest in the United States. Food is transported by trucks from twelve major distribution centers to the city’s point-of-sale locations. Trucks consume large amounts of energy and contribute to large amounts of greenhouse gas emissions. Therefore, there is interest to increase the efficiency of New York City’s food distribution system. The Gowanus district in New York City is undergoing rezoning from an industrial zone to a mix residential and industrial zone. It serves as a living lab to test new initiatives, policies, and new infrastructure for electric vehicles. We analyze the impact of electrification of food-distribution trucks on greenhouse gas emissions and electricity demand in this paper. However, such analysis faces the challenges of accessing available and granular data, modeling of demands and deliveries that incorporate logistics and inventory management of different types of food retail stores, delivery route selection, and delivery schedule to optimize food distribution. We propose a framework to estimate truck routes for food delivery at a district level. We model the schedule of food delivery from a distribution center to retail stores as a vehicle routing problem using an optimization solver. Our case study shows that diesel trucks consume 300% more energy than electric trucks and generate 40% more greenhouse gases than diesel trucks for food distribution in the Gowanus district.},
  archive      = {J_FDATA},
  author       = {Elangovan, Raghul and Kanwhen, Ondrea and Dong, Ziqian and Mohamed, Ahmed and Rojas-Cessa, Roberto},
  doi          = {10.3389/fdata.2021.693820},
  journal      = {Frontiers in Big Data},
  month        = {7},
  pages        = {693820},
  shortjournal = {Front. Big Data},
  title        = {Comparative analysis of energy use and greenhouse gas emission of diesel and electric trucks for food distribution in gowanus district of new york city},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A benchmark for data imputation methods. <em>FDATA</em>,
<em>4</em>, 693674. (<a
href="https://doi.org/10.3389/fdata.2021.693674">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing importance and complexity of data pipelines, data quality became one of the key challenges in modern software applications. The importance of data quality has been recognized beyond the field of data engineering and database management systems (DBMSs). Also, for machine learning (ML) applications, high data quality standards are crucial to ensure robust predictive performance and responsible usage of automated decision making. One of the most frequent data quality problems is missing values. Incomplete datasets can break data pipelines and can have a devastating impact on downstream ML applications when not detected. While statisticians and, more recently, ML researchers have introduced a variety of approaches to impute missing values, comprehensive benchmarks comparing classical and modern imputation approaches under fair and realistic conditions are underrepresented. Here, we aim to fill this gap. We conduct a comprehensive suite of experiments on a large number of datasets with heterogeneous data and realistic missingness conditions, comparing both novel deep learning approaches and classical ML imputation methods when either only test or train and test data are affected by missing data. Each imputation method is evaluated regarding the imputation quality and the impact imputation has on a downstream ML task. Our results provide valuable insights into the performance of a variety of imputation methods under realistic conditions. We hope that our results help researchers and engineers to guide their data preprocessing method selection for automated data quality improvement.},
  archive      = {J_FDATA},
  author       = {Jäger, Sebastian and Allhorn, Arndt and Bießmann, Felix},
  doi          = {10.3389/fdata.2021.693674},
  journal      = {Frontiers in Big Data},
  month        = {7},
  pages        = {693674},
  shortjournal = {Front. Big Data},
  title        = {A benchmark for data imputation methods},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sequence-based explainable hybrid song recommendation.
<em>FDATA</em>, <em>4</em>, 693494. (<a
href="https://doi.org/10.3389/fdata.2021.693494">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite advances in deep learning methods for song recommendation, most existing methods do not take advantage of the sequential nature of song content. In addition, there is a lack of methods that can explain their predictions using the content of recommended songs and only a few approaches can handle the item cold start problem. In this work, we propose a hybrid deep learning model that uses collaborative filtering (CF) and deep learning sequence models on the Musical Instrument Digital Interface (MIDI) content of songs to provide accurate recommendations, while also being able to generate a relevant, personalized explanation for each recommended song. Compared to state-of-the-art methods, our validation experiments showed that in addition to generating explainable recommendations, our model stood out among the top performers in terms of recommendation accuracy and the ability to handle the item cold start problem. Moreover, validation shows that our personalized explanations capture properties that are in accordance with the user’s preferences.},
  archive      = {J_FDATA},
  author       = {Damak, Khalil and Nasraoui, Olfa and Sanders, William Scott},
  doi          = {10.3389/fdata.2021.693494},
  journal      = {Frontiers in Big Data},
  month        = {7},
  pages        = {693494},
  shortjournal = {Front. Big Data},
  title        = {Sequence-based explainable hybrid song recommendation},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Structure of the region-technology network as a driver for
technological innovation. <em>FDATA</em>, <em>4</em>, 689310. (<a
href="https://doi.org/10.3389/fdata.2021.689310">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Agglomeration and spillovers are key phenomena of technological innovation, driving regional economic growth. Here, we investigate these phenomena through technological outputs of over 4,000 regions spanning 42 countries, by analyzing more than 30 years of patent data (approximately 2.7 million patents) from the European Patent Office. We construct a bipartite network—based on revealed comparative advantage—linking geographic regions with areas of technology and compare its properties to those of artificial networks using a series of randomization strategies, to uncover the patterns of regional diversity and technological ubiquity. Our results show that the technological outputs of regions create nested patterns similar to those of ecological networks. These patterns suggest that regions need to dominate various technologies first (those allegedly less sophisticated), creating a diverse knowledge base, before subsequently developing less ubiquitous (and perhaps more sophisticated) technologies as a consequence of complementary knowledge that facilitates innovation. Finally, we create a map—the Patent Space Network—showing the interactions between technologies according to their regional presence. This network reveals how technology across industries co-appear to form several explicit clusters, which may aid future works on predicting technological innovation due to agglomeration and spillovers.},
  archive      = {J_FDATA},
  author       = {O’Neale, Dion R. J. and Hendy, Shaun C. and Vasques Filho, Demival},
  doi          = {10.3389/fdata.2021.689310},
  journal      = {Frontiers in Big Data},
  month        = {7},
  pages        = {689310},
  shortjournal = {Front. Big Data},
  title        = {Structure of the region-technology network as a driver for technological innovation},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Marginalizing the mainstream: How social media privilege
political information. <em>FDATA</em>, <em>4</em>, 689036. (<a
href="https://doi.org/10.3389/fdata.2021.689036">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The following reports on research undertaken concerning the “misinformation problem” on social media during the run-up to the U.S. presidential elections in 2020. Employing techniques borrowed from data journalism, it develops a form of cross-platform analysis that is attuned to both commensurability as well as platform specificity. It analyses the most engaged-with or top-ranked political content on seven online platforms: TikTok, 4chan, Reddit, Twitter, Facebook, Instagram and Google Web Search. Discussing the extent to which social media platforms marginalize mainstream media and mainstream the fringe, the analyses found that TikTok parodies mainstream media, 4chan and Reddit dismiss it and direct users to alternative influencer networks and extreme YouTube content. Twitter prefers the hyperpartisan over it. Facebook’s “fake news” problem also concerns declining amounts of mainstream media referenced. Instagram has influencers (rather than, say, experts) dominating user engagement. By comparison, Google Web Search buoys the liberal mainstream (and sinks conservative sites), but generally gives special interest sources, as they were termed in the study, the privilege to provide information rather than official sources. The piece concludes with a discussion of source and “platform criticism”, concerning how online platforms are seeking to filter the content that is posted or found there through increasing editorial intervention. These “editorial epistemologies”, applied especially around COVID-19 keywords, are part of an expansion of so-called content moderation to what I call “serious queries”, or keywords that return official information. Other epistemological strategies for editorially moderating the misinformation problem are also treated.},
  archive      = {J_FDATA},
  author       = {Rogers, Richard},
  doi          = {10.3389/fdata.2021.689036},
  journal      = {Frontiers in Big Data},
  month        = {7},
  pages        = {689036},
  shortjournal = {Front. Big Data},
  title        = {Marginalizing the mainstream: How social media privilege political information},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Principles and practice of explainable machine learning.
<em>FDATA</em>, <em>4</em>, 688969. (<a
href="https://doi.org/10.3389/fdata.2021.688969">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial intelligence (AI) provides many opportunities to improve private and public life. Discovering patterns and structures in large troves of data in an automated manner is a core component of data science, and currently drives applications in diverse areas such as computational biology, law and finance. However, such a highly positive impact is coupled with a significant challenge: how do we understand the decisions suggested by these systems in order that we can trust them? In this report, we focus specifically on data-driven methods—machine learning (ML) and pattern recognition models in particular—so as to survey and distill the results and observations from the literature. The purpose of this report can be especially appreciated by noting that ML models are increasingly deployed in a wide range of businesses. However, with the increasing prevalence and complexity of methods, business stakeholders in the very least have a growing number of concerns about the drawbacks of models, data-specific biases, and so on. Analogously, data science practitioners are often not aware about approaches emerging from the academic literature or may struggle to appreciate the differences between different methods, so end up using industry standards such as SHAP. Here, we have undertaken a survey to help industry practitioners (but also data scientists more broadly) understand the field of explainable machine learning better and apply the right tools. Our latter sections build a narrative around a putative data scientist, and discuss how she might go about explaining her models by asking the right questions. From an organization viewpoint, after motivating the area broadly, we discuss the main developments, including the principles that allow us to study transparent models vs. opaque models, as well as model-specific or model-agnostic post-hoc explainability approaches. We also briefly reflect on deep learning models, and conclude with a discussion about future research directions.},
  archive      = {J_FDATA},
  author       = {Belle, Vaishak and Papantonis, Ioannis},
  doi          = {10.3389/fdata.2021.688969},
  journal      = {Frontiers in Big Data},
  month        = {7},
  pages        = {688969},
  shortjournal = {Front. Big Data},
  title        = {Principles and practice of explainable machine learning},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Conditional-GAN based data augmentation for deep learning
task classifier improvement using fNIRS data. <em>FDATA</em>,
<em>4</em>, 659146. (<a
href="https://doi.org/10.3389/fdata.2021.659146">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Functional near-infrared spectroscopy (fNIRS) is a neuroimaging technique used for mapping the functioning human cortex. fNIRS can be widely used in population studies due to the technology’s economic, non-invasive, and portable nature. fNIRS can be used for task classification, a crucial part of functioning with Brain-Computer Interfaces (BCIs). fNIRS data are multidimensional and complex, making them ideal for deep learning algorithms for classification. Deep Learning classifiers typically need a large amount of data to be appropriately trained without over-fitting. Generative networks can be used in such cases where a substantial amount of data is required. Still, the collection is complex due to various constraints. Conditional Generative Adversarial Networks (CGAN) can generate artificial samples of a specific category to improve the accuracy of the deep learning classifier when the sample size is insufficient. The proposed system uses a CGAN with a CNN classifier to enhance the accuracy through data augmentation. The system can determine whether the subject’s task is a Left Finger Tap, Right Finger Tap, or Foot Tap based on the fNIRS data patterns. The authors obtained a task classification accuracy of 96.67% for the CGAN-CNN combination.},
  archive      = {J_FDATA},
  author       = {Wickramaratne, Sajila D. and Mahmud, Md.Shaad},
  doi          = {10.3389/fdata.2021.659146},
  journal      = {Frontiers in Big Data},
  month        = {7},
  pages        = {659146},
  shortjournal = {Front. Big Data},
  title        = {Conditional-GAN based data augmentation for deep learning task classifier improvement using fNIRS data},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Editorial: Heterogeneous computing for AI and big data in
high energy physics. <em>FDATA</em>, <em>4</em>, 652881. (<a
href="https://doi.org/10.3389/fdata.2021.652881">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_FDATA},
  author       = {D’Agostino, Daniele and Cesini, Daniele},
  doi          = {10.3389/fdata.2021.652881},
  journal      = {Frontiers in Big Data},
  month        = {7},
  pages        = {652881},
  shortjournal = {Front. Big Data},
  title        = {Editorial: Heterogeneous computing for AI and big data in high energy physics},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). COVID-19 as a research dynamic transformer: Emerging
cross-disciplinary and national characteristics. <em>FDATA</em>,
<em>4</em>, 631073. (<a
href="https://doi.org/10.3389/fdata.2021.631073">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The outbreak of the COVID-19 pandemic has had an unprecedented impact on humanity as well as research activities in life sciences and medicine. Between January and August 2020, the number of coronavirus-related scientific articles was roughly 50 times more than that of articles published in the entire year of 2019 in PubMed. It is necessary to better understand the dynamics of research on COVID-19, an emerging topic, and suggest ways to understand and improve the quality of research. We analyze the dynamics of coronavirus research before and after the outbreaks of SARS, MERS, and COVID-19 by examining all the published articles from the past 25 years in PubMed. We delineate research networks on coronaviruses as we identify experts’ background in terms of topics of previous research, affiliations, and international co-authorships. Two distinct dynamics of coronavirus research were found: 1) in the cases of regional pandemics, SARS and MERS, the scope of cross-disciplinary research remained between neighboring research areas; 2) in the case of the global pandemic, COVID-19, research activities have spread beyond neighboring disciplines with little transnational collaboration. Thus, COVID-19 has transformed the structure of research on coronaviruses as an emerging issue. Knowledge on COVID-19 is distributed across the widest range of disciplines, transforming research networks well beyond the field of medicine but within national boundaries. Given the unprecedented scale of COVID-19 and the nationalization of responses, the most likely way forward is to accumulate local knowledge with the awareness of transdisciplinary research dynamics.},
  archive      = {J_FDATA},
  author       = {Ohniwa, Ryosuke L. and Kijima, Joji and Fukushige, Mizuho and Ohneda, Osamu},
  doi          = {10.3389/fdata.2021.631073},
  journal      = {Frontiers in Big Data},
  month        = {7},
  pages        = {631073},
  shortjournal = {Front. Big Data},
  title        = {COVID-19 as a research dynamic transformer: Emerging cross-disciplinary and national characteristics},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Differences between MR brain region segmentation methods:
Impact on single-subject analysis. <em>FDATA</em>, <em>4</em>, 577164.
(<a href="https://doi.org/10.3389/fdata.2021.577164">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For the segmentation of magnetic resonance brain images into anatomical regions, numerous fully automated methods have been proposed and compared to reference segmentations obtained manually. However, systematic differences might exist between the resulting segmentations, depending on the segmentation method and underlying brain atlas. This potentially results in sensitivity differences to disease and can further complicate the comparison of individual patients to normative data. In this study, we aim to answer two research questions: 1) to what extent are methods interchangeable, as long as the same method is being used for computing normative volume distributions and patient-specific volumes? and 2) can different methods be used for computing normative volume distributions and assessing patient-specific volumes? To answer these questions, we compared volumes of six brain regions calculated by five state-of-the-art segmentation methods: Erasmus MC (EMC), FreeSurfer (FS), geodesic information flows (GIF), multi-atlas label propagation with expectation–maximization (MALP-EM), and model-based brain segmentation (MBS). We applied the methods on 988 non-demented (ND) subjects and computed the correlation (PCC-v) and absolute agreement (ICC-v) on the volumes. For most regions, the PCC-v was good (&lt;mml:math id=&quot;m1&quot; xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;&amp;gt;&lt;/mml:mo&gt;&lt;mml:mn&gt;0.75&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;), indicating that volume differences between methods in ND subjects are mainly due to systematic differences. The ICC-v was generally lower, especially for the smaller regions, indicating that it is essential that the same method is used to generate normative and patient data. To evaluate the impact on single-subject analysis, we also applied the methods to 42 patients with Alzheimer’s disease (AD). In the case where the normative distributions and the patient-specific volumes were calculated by the same method, the patient’s distance to the normative distribution was assessed with the z-score. We determined the diagnostic value of this z-score, which showed to be consistent across methods. The absolute agreement on the AD patients’ z-scores was high for regions of thalamus and putamen. This is encouraging as it indicates that the studied methods are interchangeable for these regions. For regions such as the hippocampus, amygdala, caudate nucleus and accumbens, and globus pallidus, not all method combinations showed a high ICC-z. Whether two methods are indeed interchangeable should be confirmed for the specific application and dataset of interest.},
  archive      = {J_FDATA},
  author       = {Huizinga, W. and Poot, D. H. J. and Vinke, E. J. and Wenzel, F. and Bron, E. E. and Toussaint, N. and Ledig, C. and Vrooman, H. and Ikram, M. A. and Niessen, W. J. and Vernooij, M. W. and Klein, S.},
  doi          = {10.3389/fdata.2021.577164},
  journal      = {Frontiers in Big Data},
  month        = {7},
  pages        = {577164},
  shortjournal = {Front. Big Data},
  title        = {Differences between MR brain region segmentation methods: Impact on single-subject analysis},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An integrated deep network for cancer survival prediction
using omics data. <em>FDATA</em>, <em>4</em>, 568352. (<a
href="https://doi.org/10.3389/fdata.2021.568352">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a highly sophisticated disease that humanity faces, cancer is known to be associated with dysregulation of cellular mechanisms in different levels, which demands novel paradigms to capture informative features from different omics modalities in an integrated way. Successful stratification of patients with respect to their molecular profiles is a key step in precision medicine and in tailoring personalized treatment for critically ill patients. In this article, we use an integrated deep belief network to differentiate high-risk cancer patients from the low-risk ones in terms of the overall survival. Our study analyzes RNA, miRNA, and methylation molecular data modalities from both labeled and unlabeled samples to predict cancer survival and subsequently to provide risk stratification. To assess the robustness of our novel integrative analytics, we utilize datasets of three cancer types with 836 patients and show that our approach outperforms the most successful supervised and semi-supervised classification techniques applied to the same cancer prediction problems. In addition, despite the preconception that deep learning techniques require large size datasets for proper training, we have illustrated that our model can achieve better results for moderately sized cancer datasets.},
  archive      = {J_FDATA},
  author       = {Hassanzadeh, Hamid Reza and Wang, May D.},
  doi          = {10.3389/fdata.2021.568352},
  journal      = {Frontiers in Big Data},
  month        = {7},
  pages        = {568352},
  shortjournal = {Front. Big Data},
  title        = {An integrated deep network for cancer survival prediction using omics data},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Usability of end-to-end encryption in e-mail communication.
<em>FDATA</em>, <em>4</em>, 568284. (<a
href="https://doi.org/10.3389/fdata.2021.568284">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents the results of a usability study focused on three end-to-end encryption technologies for securing e-mail traffic, namely PGP, S/MIME, and Pretty Easy Privacy (pEp). The findings of this study show that, despite of existing technology, users seldom apply them for securing e-mail communication. Moreover, this study helps to explain why users hesitate to employ encryption technology in their e-mail communication. For this usability study, we have combined two methods: 1) an online survey, 2) and user testing with 12 participants who were enrolled in tasks requiring e-mail encryption. We found that more than 60% of our study participants (in both methods) are unaware of the existence of encryption technologies and thus never tried to use one. We observed that above all, users 1) are overwhelmed with the management of public keys and 2) struggle with the setup of encryption technology in their e-mail software. Nonetheless, 66% of the participants consider secure e-mail communication as important or very important. Particularly, we found an even stronger concern about identity theft among e-mail users, as 78% of the participants want to make sure that no other person is able to write e-mail on their behalf.},
  archive      = {J_FDATA},
  author       = {Reuter, Adrian and Abdelmaksoud, Ahmed and Boudaoud, Karima and Winckler, Marco},
  doi          = {10.3389/fdata.2021.568284},
  journal      = {Frontiers in Big Data},
  month        = {7},
  pages        = {568284},
  shortjournal = {Front. Big Data},
  title        = {Usability of end-to-end encryption in E-mail communication},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Measuring urban vibrancy of residential communities using
big crowdsourced geotagged data. <em>FDATA</em>, <em>4</em>, 690970. (<a
href="https://doi.org/10.3389/fdata.2021.690970">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The pervasiveness of mobile and sensing technologies today has facilitated the creation of Big Crowdsourced Geotagged Data (BCGD) from individual users in real time and at different locations in the city. Such ubiquitous user-generated data allow us to infer various patterns of human behavior, which helps us understand the interactions between humans and cities. In this article, we aim to analyze BCGD, including mobile consumption check-ins, urban geography data, and human mobility data, to learn a model that can unveil the impact of urban geography and human mobility on the vibrancy of residential communities. Vibrant communities are defined as places that show diverse and frequent consumer activities. To effectively identify such vibrant communities, we propose a supervised data mining system to learn and mimic the unique spatial configuration patterns and social interaction patterns of vibrant communities using urban geography and human mobility data. Specifically, to prepare the benchmark vibrancy scores of communities for training, we first propose a fused scoring method by fusing the frequency and the diversity of consumer activities using mobile check-in data. Besides, we define and extract the features of spatial configuration and social interaction for each community by mining urban geography and human mobility data. In addition, we strategically combine a pairwise ranking objective with a sparsity regularization to learn a predictor of community vibrancy. And we develop an effective solution for the optimization problem. Finally, our experiment is instantiated on BCGD including real estate, point of interests, taxi and bus GPS trajectories, and mobile check-ins in Beijing. The experimental results demonstrate the competitive performances of both the extracted features and the proposed model. Our results suggest that a structurally diverse community usually shows higher social interaction and better business performance, and incompatible land uses may decrease the vibrancy of a community. Our studies demonstrate the potential of how to best make use of BCGD to create local economic matrices and sustain urban vibrancy in a fast, cheap, and meaningful way.},
  archive      = {J_FDATA},
  author       = {Wang, Pengyang and Liu, Kunpeng and Wang, Dongjie and Fu, Yanjie},
  doi          = {10.3389/fdata.2021.690970},
  journal      = {Frontiers in Big Data},
  month        = {6},
  pages        = {690970},
  shortjournal = {Front. Big Data},
  title        = {Measuring urban vibrancy of residential communities using big crowdsourced geotagged data},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep graph mapper: Seeing graphs through the neural lens.
<em>FDATA</em>, <em>4</em>, 680535. (<a
href="https://doi.org/10.3389/fdata.2021.680535">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph summarization has received much attention lately, with various works tackling the challenge of defining pooling operators on data regions with arbitrary structures. These contrast the grid-like ones encountered in image inputs, where techniques such as max-pooling have been enough to show empirical success. In this work, we merge the Mapper algorithm with the expressive power of graph neural networks to produce topologically grounded graph summaries. We demonstrate the suitability of Mapper as a topological framework for graph pooling by proving that Mapper is a generalization of pooling methods based on soft cluster assignments. Building upon this, we show how easy it is to design novel pooling algorithms that obtain competitive results with other state-of-the-art methods. Additionally, we use our method to produce GNN-aided visualisations of attributed complex networks.},
  archive      = {J_FDATA},
  author       = {Bodnar, Cristian and Cangea, Cătălina and Liò, Pietro},
  doi          = {10.3389/fdata.2021.680535},
  journal      = {Frontiers in Big Data},
  month        = {6},
  pages        = {680535},
  shortjournal = {Front. Big Data},
  title        = {Deep graph mapper: Seeing graphs through the neural lens},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Holdout-based empirical assessment of mixed-type synthetic
data. <em>FDATA</em>, <em>4</em>, 679939. (<a
href="https://doi.org/10.3389/fdata.2021.679939">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {AI-based data synthesis has seen rapid progress over the last several years and is increasingly recognized for its promise to enable privacy-respecting high-fidelity data sharing. This is reflected by the growing availability of both commercial and open-sourced software solutions for synthesizing private data. However, despite these recent advances, adequately evaluating the quality of generated synthetic datasets is still an open challenge. We aim to close this gap and introduce a novel holdout-based empirical assessment framework for quantifying the fidelity as well as the privacy risk of synthetic data solutions for mixed-type tabular data. Measuring fidelity is based on statistical distances of lower-dimensional marginal distributions, which provide a model-free and easy-to-communicate empirical metric for the representativeness of a synthetic dataset. Privacy risk is assessed by calculating the individual-level distances to closest record with respect to the training data. By showing that the synthetic samples are just as close to the training as to the holdout data, we yield strong evidence that the synthesizer indeed learned to generalize patterns and is independent of individual training records. We empirically demonstrate the presented framework for seven distinct synthetic data solutions across four mixed-type datasets and compare these then to traditional data perturbation techniques. Both a Python-based implementation of the proposed metrics and the demonstration study setup is made available open-source. The results highlight the need to systematically assess the fidelity just as well as the privacy of these emerging class of synthetic data generators.},
  archive      = {J_FDATA},
  author       = {Platzer, Michael and Reutterer, Thomas},
  doi          = {10.3389/fdata.2021.679939},
  journal      = {Frontiers in Big Data},
  month        = {6},
  pages        = {679939},
  shortjournal = {Front. Big Data},
  title        = {Holdout-based empirical assessment of mixed-type synthetic data},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Development of an individualized risk prediction model for
COVID-19 using electronic health record data. <em>FDATA</em>,
<em>4</em>, 675882. (<a
href="https://doi.org/10.3389/fdata.2021.675882">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Developing an accurate and interpretable model to predict an individual’s risk for Coronavirus Disease 2019 (COVID-19) is a critical step to efficiently triage testing and other scarce preventative resources. To aid in this effort, we have developed an interpretable risk calculator that utilized de-identified electronic health records (EHR) from the University of Alabama at Birmingham Informatics for Integrating Biology and the Bedside (UAB-i2b2) COVID-19 repository under the U-BRITE framework. The generated risk scores are analogous to commonly used credit scores where higher scores indicate higher risks for COVID-19 infection. By design, these risk scores can easily be calculated in spreadsheets or even with pen and paper. To predict risk, we implemented a Credit Scorecard modeling approach on longitudinal EHR data from 7,262 patients enrolled in the UAB Health System who were evaluated and/or tested for COVID-19 between January and June 2020. In this cohort, 912 patients were positive for COVID-19. Our workflow considered the timing of symptoms and medical conditions and tested the effects by applying different variable selection techniques such as LASSO and Elastic-Net. Within the two weeks before a COVID-19 diagnosis, the most predictive features were respiratory symptoms such as cough, abnormalities of breathing, pain in the throat and chest as well as other chronic conditions including nicotine dependence and major depressive disorder. When extending the timeframe to include all medical conditions across all time, our models also uncovered several chronic conditions impacting the respiratory, cardiovascular, central nervous and urinary organ systems. The whole pipeline of data processing, risk modeling and web-based risk calculator can be applied to any EHR data following the OMOP common data format. The results can be employed to generate questionnaires to estimate COVID-19 risk for screening in building entries or to optimize hospital resources.},
  archive      = {J_FDATA},
  author       = {Mamidi, Tarun Karthik Kumar and Tran-Nguyen, Thi K. and Melvin, Ryan L. and Worthey, Elizabeth A.},
  doi          = {10.3389/fdata.2021.675882},
  journal      = {Frontiers in Big Data},
  month        = {6},
  pages        = {675882},
  shortjournal = {Front. Big Data},
  title        = {Development of an individualized risk prediction model for COVID-19 using electronic health record data},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Comprehensive data integration approach to assess immune
responses and correlates of RTS,s/AS01-mediated protection from malaria
infection in controlled human malaria infection trials. <em>FDATA</em>,
<em>4</em>, 672460. (<a
href="https://doi.org/10.3389/fdata.2021.672460">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {RTS,S/AS01 (GSK) is the world’s first malaria vaccine. However, despite initial efficacy of almost 70% over the first 6 months of follow-up, efficacy waned over time. A deeper understanding of the immune features that contribute to RTS,S/AS01-mediated protection could be beneficial for further vaccine development. In two recent controlled human malaria infection (CHMI) trials of the RTS,S/AS01 vaccine in malaria-naïve adults, MAL068 and MAL071, vaccine efficacy against patent parasitemia ranged from 44% to 87% across studies and arms (each study included a standard RTS,S/AS01 arm with three vaccine doses delivered in four-week-intervals, as well as an alternative arm with a modified version of this regimen). In each trial, RTS,S/AS01 immunogenicity was interrogated using a broad range of immunological assays, assessing cellular and humoral immune parameters as well as gene expression. Here, we used a predictive modeling framework to identify immune biomarkers measured at day-of-challenge that could predict sterile protection against malaria infection. Using cross-validation on MAL068 data (either the standard RTS,S/AS01 arm alone, or across both the standard RTS,S/AS01 arm and the alternative arm), top-performing univariate models identified variables related to Fc effector functions and titer of antibodies that bind to the central repeat region (NANP6) of CSP as the most predictive variables; all NANP6-related variables consistently associated with protection. In cross-study prediction analyses of MAL071 outcomes (the standard RTS,S/AS01 arm), top-performing univariate models again identified variables related to Fc effector functions of NANP6-targeting antibodies as highly predictive. We found little benefit–with this dataset–in terms of improved prediction accuracy in bivariate models vs. univariate models. These findings await validation in children living in malaria-endemic regions, and in vaccinees administered a fourth RTS,S/AS01 dose. Our findings support a “quality as well as quantity” hypothesis for RTS,S/AS01-elicited antibodies against NANP6, implying that malaria vaccine clinical trials should assess both titer and Fc effector functions of anti-NANP6 antibodies.},
  archive      = {J_FDATA},
  author       = {Young, William Chad and Carpp, Lindsay N. and Chaudhury, Sidhartha and Regules, Jason A. and Bergmann-Leitner, Elke S. and Ockenhouse, Christian and Wille-Reece, Ulrike and deCamp, Allan C. and Hughes, Ellis and Mahoney, Celia and Pallikkuth, Suresh and Pahwa, Savita and Dennison, S. Moses and Mudrak, Sarah V. and Alam, S. Munir and Seaton, Kelly E. and Spreng, Rachel L. and Fallon, Jon and Michell, Ashlin and Ulloa-Montoya, Fernando and Coccia, Margherita and Jongert, Erik and Alter, Galit and Tomaras, Georgia D. and Gottardo, Raphael},
  doi          = {10.3389/fdata.2021.672460},
  journal      = {Frontiers in Big Data},
  month        = {6},
  pages        = {672460},
  shortjournal = {Front. Big Data},
  title        = {Comprehensive data integration approach to assess immune responses and correlates of RTS,S/AS01-mediated protection from malaria infection in controlled human malaria infection trials},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Porting HEP parameterized calorimeter simulation code to
GPUs. <em>FDATA</em>, <em>4</em>, 665783. (<a
href="https://doi.org/10.3389/fdata.2021.665783">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The High Energy Physics (HEP) experiments, such as those at the Large Hadron Collider (LHC), traditionally consume large amounts of CPU cycles for detector simulations and data analysis, but rarely use compute accelerators such as GPUs. As the LHC is upgraded to allow for higher luminosity, resulting in much higher data rates, purely relying on CPUs may not provide enough computing power to support the simulation and data analysis needs. As a proof of concept, we investigate the feasibility of porting a HEP parameterized calorimeter simulation code to GPUs. We have chosen to use FastCaloSim, the ATLAS fast parametrized calorimeter simulation. While FastCaloSim is sufficiently fast such that it does not impose a bottleneck in detector simulations overall, significant speed-ups in the processing of large samples can be achieved from GPU parallelization at both the particle (intra-event) and event levels; this is especially beneficial in conditions expected at the high-luminosity LHC, where extremely high per-event particle multiplicities will result from the many simultaneous proton-proton collisions. We report our experience with porting FastCaloSim to NVIDIA GPUs using CUDA. A preliminary Kokkos implementation of FastCaloSim for portability to other parallel architectures is also described.},
  archive      = {J_FDATA},
  author       = {Dong, Zhihua and Gray, Heather and Leggett, Charles and Lin, Meifeng and Pascuzzi, Vincent R. and Yu, Kwangmin},
  doi          = {10.3389/fdata.2021.665783},
  journal      = {Frontiers in Big Data},
  month        = {6},
  pages        = {665783},
  shortjournal = {Front. Big Data},
  title        = {Porting HEP parameterized calorimeter simulation code to GPUs},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Using 3D CityGML for the modeling of the food waste and
wastewater generation—a case study for the city of montréal.
<em>FDATA</em>, <em>4</em>, 662011. (<a
href="https://doi.org/10.3389/fdata.2021.662011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper explains a workflow to simulate the food energy water (FEW) nexus for an urban district combining various data sources like 3D city models, particularly the City Geography Markup Language (CityGML) data model from the Open Geospatial Consortium, Open StreetMap and Census data. A long term vision is to extend the CityGML data model by developing a FEW Application Domain Extension (FEW ADE) to support future FEW simulation workflows such as the one explained in this paper. Together with the mentioned simulation workflow, this paper also identifies some necessary FEW related parameters for the future development of a FEW ADE. Furthermore, relevant key performance indicators are investigated, and the relevant datasets necessary to calculate these indicators are studied. Finally, different calculations are performed for the downtown borough Ville-Marie in the city of Montréal (Canada) for the domains of food waste (FW) and wastewater (WW) generation. For this study, a workflow is developed to calculate the energy generation from anaerobic digestion of FW and WW. In the first step, the data collection and preparation was done. Here relevant data for georeferencing, data for model set-up, and data for creating the required usage libraries, like food waste and wastewater generation per person, were collected. The next step was the data integration and calculation of the relevant parameters, and lastly, the results were visualized for analysis purposes. As a use case to support such calculations, the CityGML level of detail two model of Montréal is enriched with information such as building functions and building usages from OpenStreetMap. The calculation of the total residents based on the CityGML model as the main input for Ville-Marie results in a population of 72,606. The statistical value for 2016 was 89,170, which corresponds to a deviation of 15.3%. The energy recovery potential of FW is about 24,024 GJ/year, and that of wastewater is about 1,629 GJ/year, adding up to 25,653 GJ/year. Relating values to the calculated number of inhabitants in Ville-Marie results in 330.9 kWh/year for FW and 22.4 kWh/year for wastewater, respectively.},
  archive      = {J_FDATA},
  author       = {Braun, Reiner and Padsala, Rushikesh and Malmir, Tahereh and Mohammadi, Soufia and Eicker, Ursula},
  doi          = {10.3389/fdata.2021.662011},
  journal      = {Frontiers in Big Data},
  month        = {6},
  pages        = {662011},
  shortjournal = {Front. Big Data},
  title        = {Using 3D CityGML for the modeling of the food waste and wastewater Generation—A case study for the city of montréal},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). CutLang v2: Advances in a runtime-interpreted analysis
description language for HEP data. <em>FDATA</em>, <em>4</em>, 659986.
(<a href="https://doi.org/10.3389/fdata.2021.659986">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We will present the latest developments in CutLang, the runtime interpreter of a recently-developed analysis description language (ADL) for collider data analysis. ADL is a domain-specific, declarative language that describes the contents of an analysis in a standard and unambiguous way, independent of any computing framework. In ADL, analyses are written in human-readable plain text files, separating object, variable and event selection definitions in blocks, with a syntax that includes mathematical and logical operations, comparison and optimisation operators, reducers, four-vector algebra and commonly used functions. Adopting ADLs would bring numerous benefits to the LHC experimental and phenomenological communities, ranging from analysis preservation beyond the lifetimes of experiments or analysis software to facilitating the abstraction, design, visualization, validation, combination, reproduction, interpretation and overall communication of the analysis contents. Since their initial release, ADL and CutLang have been used for implementing and running numerous LHC analyses. In this process, the original syntax from CutLang v1 has been modified for better ADL compatibility, and the interpreter has been adapted to work with that syntax, resulting in the current release v2. Furthermore, CutLang has been enhanced to handle object combinatorics, to include tables and weights, to save events at any analysis stage, to benefit from multi-core/multi-CPU hardware among other improvements. In this contribution, these and other enhancements are discussed in details. In addition, real life examples from LHC analyses are presented together with a user manual.},
  archive      = {J_FDATA},
  author       = {Unel, G. and Sekmen, S. and Toon, A. M. and Gokturk, B. and Orgen, B. and Paul, A. and Ravel, N. and Setpal, J.},
  doi          = {10.3389/fdata.2021.659986},
  journal      = {Frontiers in Big Data},
  month        = {6},
  pages        = {659986},
  shortjournal = {Front. Big Data},
  title        = {CutLang v2: Advances in a runtime-interpreted analysis description language for HEP data},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Quantifying the importance of firms by means of reputation
and network control. <em>FDATA</em>, <em>4</em>, 652913. (<a
href="https://doi.org/10.3389/fdata.2021.652913">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As recently argued in the literature, the reputation of firms can be channeled through their ownership structure. We use this relation to model reputation spillovers between transnational companies and their participated companies in an ownership network core of 1,318 firms. We then apply concepts of network controllability to identify minimum sets of driver nodes (MDSs) of 314 firms in this network. The importance of these driver nodes is classified according to their control contribution, their operating revenue, and their reputation. The latter two are also taken as proxies for the access costs when utilizing firms as driver nodes. Using an enrichment analysis, we find that firms with high reputation maintain the controllability of the network but rarely become top drivers, whereas firms with medium reputation most likely become top driver nodes. We further show that MDSs with lower access costs can be used to control the reputation dynamics in the whole network.},
  archive      = {J_FDATA},
  author       = {Zhang, Yan and Schweitzer, Frank},
  doi          = {10.3389/fdata.2021.652913},
  journal      = {Frontiers in Big Data},
  month        = {6},
  pages        = {652913},
  shortjournal = {Front. Big Data},
  title        = {Quantifying the importance of firms by means of reputation and network control},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Enterprise integration and interoperability for big
data-driven processes in the frame of industry 4.0. <em>FDATA</em>,
<em>4</em>, 644651. (<a
href="https://doi.org/10.3389/fdata.2021.644651">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional manufacturing businesses lack the standards, skills, processes, and technologies to meet today&#39;s challenges of Industry 4.0 driven by an interconnected world. Enterprise Integration and Interoperability can ensure efficient communication among various services driven by big data. However, the data management challenges affect not only the technical implementation of software solutions but the function of the whole organization. In this paper, we bring together Enterprise Integration and Interoperability, Big Data Processing, and Industry 4.0 in order to identify synergies that have the potential to enable the so-called “Fourth Industrial Revolution.” On this basis, we propose an architectural framework for designing and modeling Industry 4.0 solutions for big data-driven manufacturing operations. We demonstrate the applicability of the proposed framework through its instantiation to predictive maintenance, a manufacturing function that increasingly concerns manufacturers due to the high costs, safety issues, and complexity of its application.},
  archive      = {J_FDATA},
  author       = {Bousdekis, Alexandros and Mentzas, Gregoris},
  doi          = {10.3389/fdata.2021.644651},
  journal      = {Frontiers in Big Data},
  month        = {6},
  pages        = {644651},
  shortjournal = {Front. Big Data},
  title        = {Enterprise integration and interoperability for big data-driven processes in the frame of industry 4.0},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Public perception of the fifth generation of cellular
networks (5G) on social media. <em>FDATA</em>, <em>4</em>, 640868. (<a
href="https://doi.org/10.3389/fdata.2021.640868">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advancement of social media networks, there are lots of unlabeled reviews available online, therefore it is necessarily to develop automatic tools to classify these types of reviews. To utilize these reviews for user perception, there is a need for automated tools that can process online user data. In this paper, a sentiment analysis framework has been proposed to identify people’s perception towards mobile networks. The proposed framework consists of three basic steps: preprocessing, feature selection, and applying different machine learning algorithms. The performance of the framework has taken into account different feature combinations. The simulation results show that the best performance is by integrating unigram, bigram, and trigram features.},
  archive      = {J_FDATA},
  author       = {Dashtipour, Kia and Taylor, William and Ansari, Shuja and Gogate, Mandar and Zahid, Adnan and Sambo, Yusuf and Hussain, Amir and Abbasi, Qammer H. and Imran, Muhammad Ali},
  doi          = {10.3389/fdata.2021.640868},
  journal      = {Frontiers in Big Data},
  month        = {6},
  pages        = {640868},
  shortjournal = {Front. Big Data},
  title        = {Public perception of the fifth generation of cellular networks (5G) on social media},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Interactive web-based visualization of multidimensional
physical and astronomical data. <em>FDATA</em>, <em>4</em>, 626998. (<a
href="https://doi.org/10.3389/fdata.2021.626998">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose expanding the use of scientific repositories such as Zenodo and HEP data, in particular, to better study multiparametric solutions of physical models. The implementation of interactive web-based visualizations enables quick and convenient reanalysis and comparisons of phenomenological data. To illustrate our point of view, we present some examples and demos for dark matter models, supersymmetry exclusions, and LHC simulations.},
  archive      = {J_FDATA},
  author       = {Diblen, Faruk and Hendriks, Luc and Stienen, Bob and Caron, Sascha and Bakhshi, Rena and Attema, Jisk},
  doi          = {10.3389/fdata.2021.626998},
  journal      = {Frontiers in Big Data},
  month        = {6},
  pages        = {626998},
  shortjournal = {Front. Big Data},
  title        = {Interactive web-based visualization of multidimensional physical and astronomical data},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A world full of stereotypes? Further investigation on origin
and gender bias in multi-lingual word embeddings. <em>FDATA</em>,
<em>4</em>, 625290. (<a
href="https://doi.org/10.3389/fdata.2021.625290">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Publicly available off-the-shelf word embeddings that are often used in productive applications for natural language processing have been proven to be biased. We have previously shown that this bias can come in different forms, depending on the language and the cultural context. In this work, we extend our previous work and further investigate how bias varies in different languages. We examine Italian and Swedish word embeddings for gender and origin bias, and demonstrate how an origin bias concerning local migration groups in Switzerland is included in German word embeddings. We propose BiasWords, a method to automatically detect new forms of bias. Finally, we discuss how cultural and language aspects are relevant to the impact of bias on the application and to potential mitigation measures.},
  archive      = {J_FDATA},
  author       = {Kurpicz-Briki, Mascha and Leoni, Tomaso},
  doi          = {10.3389/fdata.2021.625290},
  journal      = {Frontiers in Big Data},
  month        = {6},
  pages        = {625290},
  shortjournal = {Front. Big Data},
  title        = {A world full of stereotypes? further investigation on origin and gender bias in multi-lingual word embeddings},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Social media big data: The good, the bad, and the ugly
(un)truths. <em>FDATA</em>, <em>4</em>, 623794. (<a
href="https://doi.org/10.3389/fdata.2021.623794">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_FDATA},
  author       = {Chew, Alton M. K. and Gunasekeran, Dinesh Visva},
  doi          = {10.3389/fdata.2021.623794},
  journal      = {Frontiers in Big Data},
  month        = {6},
  pages        = {623794},
  shortjournal = {Front. Big Data},
  title        = {Social media big data: The good, the bad, and the ugly (Un)truths},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Understanding cybersecurity threat trends through dynamic
topic modeling. <em>FDATA</em>, <em>4</em>, 601529. (<a
href="https://doi.org/10.3389/fdata.2021.601529">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cybersecurity threats continue to increase and are impacting almost all aspects of modern life. Being aware of how vulnerabilities and their exploits are changing gives helpful insights into combating new threats. Applying dynamic topic modeling to a time-stamped cybersecurity document collection shows how the significance and details of concepts found in them are evolving. We correlate two different temporal corpora, one with reports about specific exploits and the other with research-oriented papers on cybersecurity vulnerabilities and threats. We represent the documents, concepts, and dynamic topic modeling data in a semantic knowledge graph to support integration, inference, and discovery. A critical insight into discovering knowledge through topic modeling is seeding the knowledge graph with domain concepts to guide the modeling process. We use Wikipedia concepts to provide a basis for performing concept phrase extraction and show how using those phrases improves the quality of the topic models. Researchers can query the resulting knowledge graph to reveal important relations and trends. This work is novel because it uses topics as a bridge to relate documents across corpora over time.},
  archive      = {J_FDATA},
  author       = {Sleeman, Jennifer and Finin, Tim and Halem, Milton},
  doi          = {10.3389/fdata.2021.601529},
  journal      = {Frontiers in Big Data},
  month        = {6},
  pages        = {601529},
  shortjournal = {Front. Big Data},
  title        = {Understanding cybersecurity threat trends through dynamic topic modeling},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Data consistency for data-driven smart energy assessment.
<em>FDATA</em>, <em>4</em>, 683682. (<a
href="https://doi.org/10.3389/fdata.2021.683682">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the smart grid era, the number of data available for different applications has increased considerably. However, data could not perfectly represent the phenomenon or process under analysis, so their usability requires a preliminary validation carried out by experts of the specific domain. The process of data gathering and transmission over the communication channels has to be verified to ensure that data are provided in a useful format, and that no external effect has impacted on the correct data to be received. Consistency of the data coming from different sources (in terms of timings and data resolution) has to be ensured and managed appropriately. Suitable procedures are needed for transforming data into knowledge in an effective way. This contribution addresses the previous aspects by highlighting a number of potential issues and the solutions in place in different power and energy system, including the generation, grid and user sides. Recent references, as well as selected historical references, are listed to support the illustration of the conceptual aspects.},
  archive      = {J_FDATA},
  author       = {Chicco, Gianfranco},
  doi          = {10.3389/fdata.2021.683682},
  journal      = {Frontiers in Big Data},
  month        = {5},
  pages        = {683682},
  shortjournal = {Front. Big Data},
  title        = {Data consistency for data-driven smart energy assessment},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Increasing the execution speed of containerized analysis
workflows using an image snapshotter in combination with CVMFS.
<em>FDATA</em>, <em>4</em>, 673163. (<a
href="https://doi.org/10.3389/fdata.2021.673163">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The past years have shown a revolution in the way scientific workloads are being executed thanks to the wide adoption of software containers. These containers run largely isolated from the host system, ensuring that the development and execution environments are the same everywhere. This enables full reproducibility of the workloads and therefore also the associated scientific analyses performed. However, as the research software used becomes increasingly complex, the software images grow easily to sizes of multiple gigabytes. Downloading the full image onto every single compute node on which the containers are executed becomes unpractical. In this paper, we describe a novel way of distributing software images on the Kubernetes platform, with which the container can start before the entire image contents become available locally (so-called “lazy pulling”). Each file required for the execution is fetched individually and subsequently cached on-demand using the CernVM file system (CVMFS), enabling the execution of very large software images on potentially thousands of Kubernetes nodes with very little overhead. We present several performance benchmarks making use of typical high-energy physics analysis workloads.},
  archive      = {J_FDATA},
  author       = {Mosciatti, Simone and Lange, Clemens and Blomer, Jakob},
  doi          = {10.3389/fdata.2021.673163},
  journal      = {Frontiers in Big Data},
  month        = {5},
  pages        = {673163},
  shortjournal = {Front. Big Data},
  title        = {Increasing the execution speed of containerized analysis workflows using an image snapshotter in combination with CVMFS},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Functional structure in production networks. <em>FDATA</em>,
<em>4</em>, 666712. (<a
href="https://doi.org/10.3389/fdata.2021.666712">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Production networks are integral to economic dynamics, yet dis-aggregated network data on inter-firm trade is rarely collected and often proprietary. Here we situate company-level production networks within a wider space of networks that are different in nature, but similar in local connectivity structure. Through this lens, we study a regional and a national network of inferred trade relationships reconstructed from Dutch national economic statistics and re-interpret prior empirical findings. We find that company-level production networks have so-called functional structure, as previously identified in protein-protein interaction (PPI) networks. Functional networks are distinctive in their over-representation of closed squares, which we quantify using an existing measure called spectral bipartivity. Shared local connectivity structure lets us ferry insights between domains. PPI networks are shaped by complementarity, rather than homophily, and we use multi-layer directed configuration models to show that this principle explains the emergence of functional structure in production networks. Companies are especially similar to their close competitors, not to their trading partners. Our findings have practical implications for the analysis of production networks and give us precise terms for the local structural features that may be key to understanding their routine function, failure, and growth.},
  archive      = {J_FDATA},
  author       = {Mattsson, Carolina E. S. and Takes, Frank W. and Heemskerk, Eelke M. and Diks, Cees and Buiten, Gert and Faber, Albert and Sloot, Peter M. A.},
  doi          = {10.3389/fdata.2021.666712},
  journal      = {Frontiers in Big Data},
  month        = {5},
  pages        = {666712},
  shortjournal = {Front. Big Data},
  title        = {Functional structure in production networks},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Iowa urban FEWS: Integrating social and biophysical models
for exploration of urban food, energy, and water systems.
<em>FDATA</em>, <em>4</em>, 662186. (<a
href="https://doi.org/10.3389/fdata.2021.662186">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most people in the world live in urban areas, and their high population densities, heavy reliance on external sources of food, energy, and water, and disproportionately large waste production result in severe and cumulative negative environmental effects. Integrated study of urban areas requires a system-of-systems analytical framework that includes modeling with social and biophysical data. We describe preliminary work toward an integrated urban food-energy-water systems (FEWS) analysis using co-simulation for assessment of current and future conditions, with an emphasis on local (urban and urban-adjacent) food production. We create a framework to enable simultaneous analyses of climate dynamics, changes in land cover, built forms, energy use, and environmental outcomes associated with a set of drivers of system change related to policy, crop management, technology, social interaction, and market forces affecting food production. The ultimate goal of our research program is to enhance understanding of the urban FEWS nexus so as to improve system function and management, increase resilience, and enhance sustainability. Our approach involves data-driven co-simulation to enable coupling of disparate food, energy and water simulation models across a range of spatial and temporal scales. When complete, these models will quantify energy use and water quality outcomes for current systems, and determine if undesirable environmental effects are decreased and local food supply is increased with different configurations of socioeconomic and biophysical factors in urban and urban-adjacent areas. The effort emphasizes use of open-source simulation models and expert knowledge to guide modeling for individual and combined systems in the urban FEWS nexus.},
  archive      = {J_FDATA},
  author       = {Thompson, Jan and Ganapathysubramanian, Baskar and Chen, Wei and Dorneich, Michael and Gassman, Philip and Krejci, Caroline and Liebman, Matthew and Nair, Ajay and Passe, Ulrike and Schwab, Nicholas and Rosentrater, Kurt and Stone, Tiffanie and Wang, Yiming and Zhou, Yuyu},
  doi          = {10.3389/fdata.2021.662186},
  journal      = {Frontiers in Big Data},
  month        = {5},
  pages        = {662186},
  shortjournal = {Front. Big Data},
  title        = {Iowa urban FEWS: Integrating social and biophysical models for exploration of urban food, energy, and water systems},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Scalable declarative HEP analysis workflows for
containerised compute clouds. <em>FDATA</em>, <em>4</em>, 661501. (<a
href="https://doi.org/10.3389/fdata.2021.661501">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We describe a novel approach for experimental High-Energy Physics (HEP) data analyses that is centred around the declarative rather than imperative paradigm when describing analysis computational tasks. The analysis process can be structured in the form of a Directed Acyclic Graph (DAG), where each graph vertex represents a unit of computation with its inputs and outputs, and the graph edges describe the interconnection of various computational steps. We have developed REANA, a platform for reproducible data analyses, that supports several such DAG workflow specifications. The REANA platform parses the analysis workflow and dispatches its computational steps to various supported computing backends (Kubernetes, HTCondor, Slurm). The focus on declarative rather than imperative programming enables researchers to concentrate on the problem domain at hand without having to think about implementation details such as scalable job orchestration. The declarative programming approach is further exemplified by a multi-level job cascading paradigm that was implemented in the Yadage workflow specification language. We present two recent LHC particle physics analyses, ATLAS searches for dark matter and CMS jet energy correction pipelines, where the declarative approach was successfully applied. We argue that the declarative approach to data analyses, combined with recent advancements in container technology, facilitates the portability of computational data analyses to various compute backends, enhancing the reproducibility and the knowledge preservation behind particle physics data analyses.},
  archive      = {J_FDATA},
  author       = {Šimko, Tibor and Heinrich, Lukas Alexander and Lange, Clemens and Lintuluoto, Adelina Eleonora and MacDonell, Danika Marina and Mečionis, Audrius and Rodríguez Rodríguez, Diego and Shandilya, Parth and Vidal García, Marco},
  doi          = {10.3389/fdata.2021.661501},
  journal      = {Frontiers in Big Data},
  month        = {5},
  pages        = {661501},
  shortjournal = {Front. Big Data},
  title        = {Scalable declarative HEP analysis workflows for containerised compute clouds},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Inter-cohort validation of SuStaIn model for alzheimer’s
disease. <em>FDATA</em>, <em>4</em>, 661110. (<a
href="https://doi.org/10.3389/fdata.2021.661110">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Alzheimer’s disease (AD) is a neurodegenerative disorder which spans several years from preclinical manifestations to dementia. In recent years, interest in the application of machine learning (ML) algorithms to personalized medicine has grown considerably, and a major challenge that such models face is the transferability from the research settings to clinical practice. The objective of this work was to demonstrate the transferability of the Subtype and Stage Inference (SuStaIn) model from well-characterized research data set, employed as training set, to independent less-structured and heterogeneous test sets representative of the clinical setting. The training set was composed of MRI data of 1043 subjects from the Alzheimer’s disease Neuroimaging Initiative (ADNI), and the test set was composed of data from 767 subjects from OASIS, Pharma-Cog, and ViTA clinical datasets. Both sets included subjects covering the entire spectrum of AD, and for both sets volumes of relevant brain regions were derived from T1-3D MRI scans processed with Freesurfer v5.3 cross-sectional stream. In order to assess the predictive value of the model, subpopulations of subjects with stable mild cognitive impairment (MCI) and MCIs that progressed to AD dementia (pMCI) were identified in both sets. SuStaIn identified three disease subtypes, of which the most prevalent corresponded to the typical atrophy pattern of AD. The other SuStaIn subtypes exhibited similarities with the previously defined hippocampal sparing and limbic predominant atrophy patterns of AD. Subject subtyping proved to be consistent in time for all cohorts and the staging provided by the model was correlated with cognitive performance. Classification of subjects on the basis of a combination of SuStaIn subtype and stage, mini mental state examination and amyloid-β1-42 cerebrospinal fluid concentration was proven to predict conversion from MCI to AD dementia on par with other novel statistical algorithms, with ROC curves that were not statistically different for the training and test sets and with area under curve respectively equal to 0.77 and 0.76. This study proves the transferability of a SuStaIn model for AD from research data to less-structured clinical cohorts, and indicates transferability to the clinical setting.},
  archive      = {J_FDATA},
  author       = {Archetti, Damiano and Young, Alexandra L. and Oxtoby, Neil P. and Ferreira, Daniel and Mårtensson, Gustav and Westman, Eric and Alexander, Daniel C. and Frisoni, Giovanni B. and Redolfi, Alberto and , for Alzheimer’s Disease Neuroimaging Initiative and EuroPOND Consortium},
  doi          = {10.3389/fdata.2021.661110},
  journal      = {Frontiers in Big Data},
  month        = {5},
  pages        = {661110},
  shortjournal = {Front. Big Data},
  title        = {Inter-cohort validation of SuStaIn model for alzheimer’s disease},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Interpretability of machine learning solutions in public
healthcare: The CRISP-ML approach. <em>FDATA</em>, <em>4</em>, 660206.
(<a href="https://doi.org/10.3389/fdata.2021.660206">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Public healthcare has a history of cautious adoption for artificial intelligence (AI) systems. The rapid growth of data collection and linking capabilities combined with the increasing diversity of the data-driven AI techniques, including machine learning (ML), has brought both ubiquitous opportunities for data analytics projects and increased demands for the regulation and accountability of the outcomes of these projects. As a result, the area of interpretability and explainability of ML is gaining significant research momentum. While there has been some progress in the development of ML methods, the methodological side has shown limited progress. This limits the practicality of using ML in the health domain: the issues with explaining the outcomes of ML algorithms to medical practitioners and policy makers in public health has been a recognized obstacle to the broader adoption of data science approaches in this domain. This study builds on the earlier work which introduced CRISP-ML, a methodology that determines the interpretability level required by stakeholders for a successful real-world solution and then helps in achieving it. CRISP-ML was built on the strengths of CRISP-DM, addressing the gaps in handling interpretability. Its application in the Public Healthcare sector follows its successful deployment in a number of recent real-world projects across several industries and fields, including credit risk, insurance, utilities, and sport. This study elaborates on the CRISP-ML methodology on the determination, measurement, and achievement of the necessary level of interpretability of ML solutions in the Public Healthcare sector. It demonstrates how CRISP-ML addressed the problems with data diversity, the unstructured nature of data, and relatively low linkage between diverse data sets in the healthcare domain. The characteristics of the case study, used in the study, are typical for healthcare data, and CRISP-ML managed to deliver on these issues, ensuring the required level of interpretability of the ML solutions discussed in the project. The approach used ensured that interpretability requirements were met, taking into account public healthcare specifics, regulatory requirements, project stakeholders, project objectives, and data characteristics. The study concludes with the three main directions for the development of the presented cross-industry standard process.},
  archive      = {J_FDATA},
  author       = {Kolyshkina, Inna and Simoff, Simeon},
  doi          = {10.3389/fdata.2021.660206},
  journal      = {Frontiers in Big Data},
  month        = {5},
  pages        = {660206},
  shortjournal = {Front. Big Data},
  title        = {Interpretability of machine learning solutions in public healthcare: The CRISP-ML approach},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Proving the correctness of knowledge graph update: A
scenario from surveillance of adverse childhood experiences.
<em>FDATA</em>, <em>4</em>, 660101. (<a
href="https://doi.org/10.3389/fdata.2021.660101">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge graphs are a modern way to store information. However, the knowledge they contain is not static. Instances of various classes may be added or deleted and the semantic relationship between elements might evolve as well. When such changes take place, a knowledge graph might become inconsistent and the knowledge it conveys meaningless. In order to ensure the consistency and coherency of dynamic knowledge graphs, we propose a method to model the transformations that a knowledge graph goes through and to prove that the new transformations do not yield inconsistencies. To do so, we express the knowledge graphs as logically decorated graphs, then we describe the transformations as algorithmic graph transformations and we use a Hoare-like verification process to prove correctness. To demonstrate the proposed method in action, we use examples from Adverse Childhood Experiences (ACEs), which is a public health crisis.},
  archive      = {J_FDATA},
  author       = {Brenas, Jon Haël and Shaban-Nejad, Arash},
  doi          = {10.3389/fdata.2021.660101},
  journal      = {Frontiers in Big Data},
  month        = {5},
  pages        = {660101},
  shortjournal = {Front. Big Data},
  title        = {Proving the correctness of knowledge graph update: A scenario from surveillance of adverse childhood experiences},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Requirements for a dashboard to support quality improvement
teams in pain management. <em>FDATA</em>, <em>4</em>, 654914. (<a
href="https://doi.org/10.3389/fdata.2021.654914">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pain management is often considered lower priority than many other aspects of health management in hospitals. However, there is potential for Quality Improvement (QI) teams to improve pain management by visualising and exploring pain data sets. Although dashboards are already used by QI teams in hospitals, there is limited evidence of teams accessing visualisations to support their decision making. This study aims to identify the needs of the QI team in a UK Critical Care Unit (CCU) and develop dashboards that visualise longitudinal data on the efficacy of patient pain management to assist the team in making informed decisions to improve pain management within the CCU. This research is based on an analysis of transcripts of interviews with healthcare professionals with a variety of roles in the CCU and their evaluation of probes. We identified two key uses of pain data: direct patient care (focusing on individual patient data) and QI (aggregating data across the CCU and over time); in this paper, we focus on the QI role. We have identified how CCU staff currently interpret information and determine what supplementary information can better inform their decision making and support sensemaking. From these, a set of data visualisations has been proposed, for integration with the hospital electronic health record. These visualisations are being iteratively refined in collaboration with CCU staff and technical staff responsible for maintaining the electronic health record. The paper presents user requirements for QI in pain management and a set of visualisations, including the design rationale behind the various methods proposed for visualising and exploring pain data using dashboards.},
  archive      = {J_FDATA},
  author       = {Opie, Jeremy and Bellio, Maura and Williams, Rachel and Sussman, Maya and Voegele, Petra and Welch, John and Blandford, Ann},
  doi          = {10.3389/fdata.2021.654914},
  journal      = {Frontiers in Big Data},
  month        = {5},
  pages        = {654914},
  shortjournal = {Front. Big Data},
  title        = {Requirements for a dashboard to support quality improvement teams in pain management},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Housing prices and the skills composition of neighborhoods.
<em>FDATA</em>, <em>4</em>, 652153. (<a
href="https://doi.org/10.3389/fdata.2021.652153">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the United States (US), low-income workers are being pushed away from city centers where the cost of living is high. The effects of such changes on labor mobility and housing price have been explored in the literature. However, few studies have focused on the occupations and specific skills that identify the most susceptible workers. For example, it has become increasingly challenging to fill the service sector jobs in the San Francisco (SF) Bay Area because appropriately skilled workers cannot afford the growing cost of living within commuting distance. With this example in mind, how does a neighborhood&#39;s skill composition change as a result of higher housing prices? Are there certain skill sets that are being pushed to the geographical periphery of a city despite their essentialness to the city&#39;s economy? Our study focuses on the impact of housing prices with a granular view of skills compositions to answer the following question: Has the density of cognitive skill workers been increasing in a gentrified area? We hypothesize that, over time, low-skilled workers are pushed away from downtown or areas where high-skill establishments thrive. Our preliminary results show that high-level cognitive skills are getting closer to the city center indicating adaptation to the increase of median housing prices as opposed to low-level physical skills that got further away. We examined tracts that the literature indicates as gentrified areas and found a pattern in which there is a temporal increase in median housing prices and the number of business establishments coupled with an increase in the percentage of skilled cognitive workers.},
  archive      = {J_FDATA},
  author       = {Althobaiti, Shahad and Alghumayjan, Saud and Frank, Morgan R. and Moro, Esteban and Alabdulkareem, Ahmad and Pentland, Alex},
  doi          = {10.3389/fdata.2021.652153},
  journal      = {Frontiers in Big Data},
  month        = {5},
  pages        = {652153},
  shortjournal = {Front. Big Data},
  title        = {Housing prices and the skills composition of neighborhoods},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Association vs. Prediction: The impact of cortical surface
smoothing and parcellation on brain age. <em>FDATA</em>, <em>4</em>,
637724. (<a href="https://doi.org/10.3389/fdata.2021.637724">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Association and prediction studies of the brain target the biological consequences of aging and their impact on brain function. Such studies are conducted using different smoothing levels and parcellations at the preprocessing stage, on which their results are dependent. However, the impact of these parameters on the relationship between association values and prediction accuracy is not established. In this study, we used cortical thickness and its relationship with age to investigate how different smoothing and parcellation levels affect the detection of age-related brain correlates as well as brain age prediction accuracy. Our main measures were resel numbers—resolution elements—and age-related variance explained. Using these common measures enabled us to directly compare parcellation and smoothing effects in both association and prediction studies. In our sample of N = 608 participants with age range 18–88, we evaluated age-related cortical thickness changes as well as brain age prediction. We found a negative relationship between prediction performance and correlation values for both parameters. Our results also quantify the relationship between delta age estimates obtained based on different processing parameters. Furthermore, with the direct comparison of the two approaches, we highlight the importance of correct choice of smoothing and parcellation parameters in each task, and how they can affect the results of the analysis in opposite directions.},
  archive      = {J_FDATA},
  author       = {Zeighami, Yashar and Evans, Alan C.},
  doi          = {10.3389/fdata.2021.637724},
  journal      = {Frontiers in Big Data},
  month        = {5},
  pages        = {637724},
  shortjournal = {Front. Big Data},
  title        = {Association vs. prediction: The impact of cortical surface smoothing and parcellation on brain age},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Up-to-the-minute privacy policies via gossips in
participatory epidemiological studies. <em>FDATA</em>, <em>4</em>,
624424. (<a href="https://doi.org/10.3389/fdata.2021.624424">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Researchers and researched populations are actively involved in participatory epidemiology. Such studies collect many details about an individual. Recent developments in statistical inferences can lead to sensitive information leaks from seemingly insensitive data about individuals. Typical safeguarding mechanisms are vetted by ethics committees; however, the attack models are constantly evolving. Newly discovered threats, change in applicable laws or an individual&#39;s perception can raise concerns that affect the study. Addressing these concerns is imperative to maintain trust with the researched population. We are implementing Lohpi: an infrastructure for building accountability in data processing for participatory epidemiology. We address the challenge of data-ownership by allowing institutions to host data on their managed servers while being part of Lohpi. We update data access policies using gossips. We present Lohpi as a novel architecture for research data processing and evaluate the dissemination, overhead, and fault-tolerance.},
  archive      = {J_FDATA},
  author       = {Sharma, Aakash and Nilsen, Thomas Bye and Czerwinska, Katja Pauline and Onitiu, Daria and Brenna, Lars and Johansen, Dag and Johansen, Håvard D.},
  doi          = {10.3389/fdata.2021.624424},
  journal      = {Frontiers in Big Data},
  month        = {5},
  pages        = {624424},
  shortjournal = {Front. Big Data},
  title        = {Up-to-the-minute privacy policies via gossips in participatory epidemiological studies},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Search engine gender bias. <em>FDATA</em>, <em>4</em>,
622106. (<a href="https://doi.org/10.3389/fdata.2021.622106">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article discusses possible search engine page rank biases as a consequence of search engine profile information. After describing search engine biases, their causes, and their ethical implications, we present data about the Google search engine (GSE) and DuckDuckGo (DDG) for which only the first uses profile data for the production of page ranks. We analyze 408 search engine screen prints of 102 volunteers (53 male and 49 female) on queries for job search and political participation. For job searches via GSE, we find a bias toward stereotypically “female” jobs for women but also for men, although the bias is significantly stronger for women. For political participation, the bias of GSE is toward more powerful positions. Contrary to our hypothesis, this bias is even stronger for women than for men. Our analysis of DDG does not give statistically significant page rank differences for male and female users. We, therefore, conclude that GSE’s personal profiling is not reinforcing a gender stereotype. Although no gender differences in page ranks was found for DDG, DDG usage in general gave a bias toward “male-dominant” vacancies for both men and women. We, therefore, believe that search engine page ranks are not biased by profile ranking algorithms, but that page rank biases may be caused by many other factors in the search engine’s value chain. We propose ten search engine bias factors with virtue ethical implications for further research.},
  archive      = {J_FDATA},
  author       = {Wijnhoven, Fons and van Haren, Jeanna},
  doi          = {10.3389/fdata.2021.622106},
  journal      = {Frontiers in Big Data},
  month        = {5},
  pages        = {622106},
  shortjournal = {Front. Big Data},
  title        = {Search engine gender bias},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Developing an explainable machine learning-based
personalised dementia risk prediction model: A transfer learning
approach with ensemble learning algorithms. <em>FDATA</em>, <em>4</em>,
613047. (<a href="https://doi.org/10.3389/fdata.2021.613047">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Alzheimer&#39;s disease (AD) has its onset many decades before dementia develops, and work is ongoing to characterise individuals at risk of decline on the basis of early detection through biomarker and cognitive testing as well as the presence/absence of identified risk factors. Risk prediction models for AD based on various computational approaches, including machine learning, are being developed with promising results. However, these approaches have been criticised as they are unable to generalise due to over-reliance on one data source, poor internal and external validations, and lack of understanding of prediction models, thereby limiting the clinical utility of these prediction models. We propose a framework that employs a transfer-learning paradigm with ensemble learning algorithms to develop explainable personalised risk prediction models for dementia. Our prediction models, known as source models, are initially trained and tested using a publicly available dataset (n = 84,856, mean age = 69 years) with 14 years of follow-up samples to predict the individual risk of developing dementia. The decision boundaries of the best source model are further updated by using an alternative dataset from a different and much younger population (n = 473, mean age = 52 years) to obtain an additional prediction model known as the target model. We further apply the SHapely Additive exPlanation (SHAP) algorithm to visualise the risk factors responsible for the prediction at both population and individual levels. The best source model achieves a geometric accuracy of 87%, specificity of 99%, and sensitivity of 76%. In comparison to a baseline model, our target model achieves better performance across several performance metrics, within an increase in geometric accuracy of 16.9%, specificity of 2.7%, and sensitivity of 19.1%, an area under the receiver operating curve (AUROC) of 11% and a transfer learning efficacy rate of 20.6%. The strength of our approach is the large sample size used in training the source model, transferring and applying the “knowledge” to another dataset from a different and undiagnosed population for the early detection and prediction of dementia risk, and the ability to visualise the interaction of the risk factors that drive the prediction. This approach has direct clinical utility.},
  archive      = {J_FDATA},
  author       = {Danso, Samuel O. and Zeng, Zhanhang and Muniz-Terrera, Graciela and Ritchie, Craig W.},
  doi          = {10.3389/fdata.2021.613047},
  journal      = {Frontiers in Big Data},
  month        = {5},
  pages        = {613047},
  shortjournal = {Front. Big Data},
  title        = {Developing an explainable machine learning-based personalised dementia risk prediction model: A transfer learning approach with ensemble learning algorithms},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep graph learning for circuit deobfuscation.
<em>FDATA</em>, <em>4</em>, 608286. (<a
href="https://doi.org/10.3389/fdata.2021.608286">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Circuit obfuscation is a recently proposed defense mechanism to protect the intellectual property (IP) of digital integrated circuits (ICs) from reverse engineering. There have been effective schemes, such as satisfiability (SAT)-checking based attacks that can potentially decrypt obfuscated circuits, which is called deobfuscation. Deobfuscation runtime could be days or years, depending on the layouts of the obfuscated ICs. Hence, accurately pre-estimating the deobfuscation runtime within a reasonable amount of time is crucial for IC designers to optimize their defense. However, it is challenging due to (1) the complexity of graph-structured circuit; (2) the varying-size topology of obfuscated circuits; (3) requirement on efficiency for deobfuscation method. This study proposes a framework that predicts the deobfuscation runtime based on graph deep learning techniques to address the challenges mentioned above. A conjunctive normal form (CNF) bipartite graph is utilized to characterize the complexity of this SAT problem by analyzing the SAT attack method. Multi-order information of the graph matrix is designed to identify the essential features and reduce the computational cost. To overcome the difficulty in capturing the dynamic size of the CNF graph, an energy-based kernel is proposed to aggregate dynamic features into an identical vector space. Then, we designed a framework, Deep Survival Analysis with Graph (DSAG), which integrates energy-based layers and predicts runtime inspired by censored regression in survival analysis. Integrating uncensored data with censored data, the proposed model improves the standard regression significantly. DSAG is an end-to-end framework that can automatically extract the determinant features for deobfuscation runtime. Extensive experiments on benchmarks demonstrate its effectiveness and efficiency.},
  archive      = {J_FDATA},
  author       = {Chen, Zhiqian and Zhang, Lei and Kolhe, Gaurav and Kamali, Hadi Mardani and Rafatirad, Setareh and Pudukotai Dinakarrao, Sai Manoj and Homayoun, Houman and Lu, Chang-Tien and Zhao, Liang},
  doi          = {10.3389/fdata.2021.608286},
  journal      = {Frontiers in Big Data},
  month        = {5},
  pages        = {608286},
  shortjournal = {Front. Big Data},
  title        = {Deep graph learning for circuit deobfuscation},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MARGIN: Uncovering deep neural networks using graph signal
analysis. <em>FDATA</em>, <em>4</em>, 589417. (<a
href="https://doi.org/10.3389/fdata.2021.589417">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interpretability has emerged as a crucial aspect of building trust in machine learning systems, aimed at providing insights into the working of complex neural networks that are otherwise opaque to a user. There are a plethora of existing solutions addressing various aspects of interpretability ranging from identifying prototypical samples in a dataset to explaining image predictions or explaining mis-classifications. While all of these diverse techniques address seemingly different aspects of interpretability, we hypothesize that a large family of interepretability tasks are variants of the same central problem which is identifying relative change in a model’s prediction. This paper introduces MARGIN, a simple yet general approach to address a large set of interpretability tasks MARGIN exploits ideas rooted in graph signal analysis to determine influential nodes in a graph, which are defined as those nodes that maximally describe a function defined on the graph. By carefully defining task-specific graphs and functions, we demonstrate that MARGIN outperforms existing approaches in a number of disparate interpretability challenges.},
  archive      = {J_FDATA},
  author       = {Anirudh, Rushil and Thiagarajan, Jayaraman J. and Sridhar, Rahul and Bremer, Peer-Timo},
  doi          = {10.3389/fdata.2021.589417},
  journal      = {Frontiers in Big Data},
  month        = {5},
  pages        = {589417},
  shortjournal = {Front. Big Data},
  title        = {MARGIN: Uncovering deep neural networks using graph signal analysis},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Routing of electric vehicles with intermediary charging
stations: A reinforcement learning approach. <em>FDATA</em>, <em>4</em>,
586481. (<a href="https://doi.org/10.3389/fdata.2021.586481">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the past few years, the importance of electric mobility has increased in response to growing concerns about climate change. However, limited cruising range and sparse charging infrastructure could restrain a massive deployment of electric vehicles (EVs). To mitigate the problem, the need for optimal route planning algorithms emerged. In this paper, we propose a mathematical formulation of the EV-specific routing problem in a graph-theoretical context, which incorporates the ability of EVs to recuperate energy. Furthermore, we consider a possibility to recharge on the way using intermediary charging stations. As a possible solution method, we present an off-policy model-free reinforcement learning approach that aims to generate energy feasible paths for EV from source to target. The algorithm was implemented and tested on a case study of a road network in Switzerland. The training procedure requires low computing and memory demands and is suitable for online applications. The results achieved demonstrate the algorithm’s capability to take recharging decisions and produce desired energy feasible paths.},
  archive      = {J_FDATA},
  author       = {Dorokhova, Marina and Ballif, Christophe and Wyrsch, Nicolas},
  doi          = {10.3389/fdata.2021.586481},
  journal      = {Frontiers in Big Data},
  month        = {5},
  pages        = {586481},
  shortjournal = {Front. Big Data},
  title        = {Routing of electric vehicles with intermediary charging stations: A reinforcement learning approach},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Evaluating the effectiveness of personalized medicine with
software. <em>FDATA</em>, <em>4</em>, 572532. (<a
href="https://doi.org/10.3389/fdata.2021.572532">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present methodological advances in understanding the effectiveness of personalized medicine models and supply easy-to-use open-source software. Personalized medicine involves the systematic use of individual patient characteristics to determine which treatment option is most likely to result in a better average outcome for the patient. Why is personalized medicine not done more in practice? One of many reasons is because practitioners do not have any easy way to holistically evaluate whether their personalization procedure does better than the standard of care, termed improvement. Our software, “Personalized Treatment Evaluator” (the R package PTE), provides inference for improvement out-of-sample in many clinical scenarios. We also extend current methodology by allowing evaluation of improvement in the case where the endpoint is binary or survival. In the software, the practitioner inputs 1) data from a single-stage randomized trial with one continuous, incidence or survival endpoint and 2) an educated guess of a functional form of a model for the endpoint constructed from domain knowledge. The bootstrap is then employed on data unseen during model fitting to provide confidence intervals for the improvement for the average future patient (assuming future patients are similar to the patients in the trial). One may also test against a null scenario where the hypothesized personalization are not more useful than a standard of care. We demonstrate our method’s promise on simulated data as well as on data from a randomized comparative trial investigating two treatments for depression.},
  archive      = {J_FDATA},
  author       = {Kapelner, Adam and Bleich, Justin and Levine, Alina and Cohen, Zachary D. and DeRubeis, Robert J. and Berk, Richard},
  doi          = {10.3389/fdata.2021.572532},
  journal      = {Frontiers in Big Data},
  month        = {5},
  pages        = {572532},
  shortjournal = {Front. Big Data},
  title        = {Evaluating the effectiveness of personalized medicine with software},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Predicting admissions from a paediatric emergency department
– protocol for developing and validating a low-dimensional machine
learning prediction model. <em>FDATA</em>, <em>4</em>, 643558. (<a
href="https://doi.org/10.3389/fdata.2021.643558">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Introduction: Patients boarding in the Emergency Department can contribute to overcrowding, leading to longer waiting times and patients leaving without being seen or completing their treatment. The early identification of potential admissions could act as an additional decision support tool to alert clinicians that a patient needs to be reviewed for admission and would also be of benefit to bed managers in advance bed planning for the patient. We aim to create a low-dimensional model predicting admissions early from the paediatric Emergency Department.Methods and Analysis: The methodology Cross Industry Standard Process for Data Mining (CRISP-DM) will be followed. The dataset will comprise of 2 years of data, ~76,000 records. Potential predictors were identified from previous research, comprising of demographics, registration details, triage assessment, hospital usage and past medical history. Fifteen models will be developed comprised of 3 machine learning algorithms (Logistic regression, naïve Bayes and gradient boosting machine) and 5 sampling methods, 4 of which are aimed at addressing class imbalance (undersampling, oversampling, and synthetic oversampling techniques). The variables of importance will then be identified from the optimal model (selected based on the highest Area under the curve) and used to develop an additional low-dimensional model for deployment.Discussion: A low-dimensional model comprised of routinely collected data, captured up to post triage assessment would benefit many hospitals without data rich platforms for the development of models with a high number of predictors. Novel to the planned study is the use of data from the Republic of Ireland and the application of sampling techniques aimed at improving model performance impacted by an imbalance between admissions and discharges in the outcome variable.},
  archive      = {J_FDATA},
  author       = {Leonard, Fiona and Gilligan, John and Barrett, Michael J.},
  doi          = {10.3389/fdata.2021.643558},
  journal      = {Frontiers in Big Data},
  month        = {4},
  pages        = {643558},
  shortjournal = {Front. Big Data},
  title        = {Predicting admissions from a paediatric emergency department – protocol for developing and validating a low-dimensional machine learning prediction model},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Data-driven computational social network science: Predictive
and inferential models for web-enabled scientific discoveries.
<em>FDATA</em>, <em>4</em>, 591749. (<a
href="https://doi.org/10.3389/fdata.2021.591749">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ultimate goal of the social sciences is to find a general social theory encompassing all aspects of social and collective phenomena. The traditional approach to this is very stringent by trying to find causal explanations and models. However, this approach has been recently criticized for preventing progress due to neglecting prediction abilities of models that support more problem-oriented approaches. The latter models would be enabled by the surge of big Web-data currently available. Interestingly, this problem cannot be overcome with methods from computational social science (CSS) alone because this field is dominated by simulation-based approaches and descriptive models. In this article, we address this issue and argue that the combination of big social data with social networks is needed for creating prediction models. We will argue that this alliance has the potential for gradually establishing a causal social theory. In order to emphasize the importance of integrating big social data with social networks, we call this approach data-driven computational social network science (DD-CSNS).},
  archive      = {J_FDATA},
  author       = {Emmert-Streib, Frank and Dehmer, Matthias},
  doi          = {10.3389/fdata.2021.591749},
  journal      = {Frontiers in Big Data},
  month        = {4},
  pages        = {591749},
  shortjournal = {Front. Big Data},
  title        = {Data-driven computational social network science: Predictive and inferential models for web-enabled scientific discoveries},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sonographic diagnosis of COVID-19: A review of image
processing for lung ultrasound. <em>FDATA</em>, <em>4</em>, 612561. (<a
href="https://doi.org/10.3389/fdata.2021.612561">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The sustained increase in new cases of COVID-19 across the world and potential for subsequent outbreaks call for new tools to assist health professionals with early diagnosis and patient monitoring. Growing evidence around the world is showing that lung ultrasound examination can detect manifestations of COVID-19 infection. Ultrasound imaging has several characteristics that make it ideally suited for routine use: small hand-held systems can be contained inside a protective sheath, making it easier to disinfect than X-ray or computed tomography equipment; lung ultrasound allows triage of patients in long term care homes, tents or other areas outside of the hospital where other imaging modalities are not available; and it can determine lung involvement during the early phases of the disease and monitor affected patients at bedside on a daily basis. However, some challenges still remain with routine use of lung ultrasound. Namely, current examination practices and image interpretation are quite challenging, especially for unspecialized personnel. This paper reviews how lung ultrasound (LUS) imaging can be used for COVID-19 diagnosis and explores different image processing methods that have the potential to detect manifestations of COVID-19 in LUS images. Then, the paper reviews how general lung ultrasound examinations are performed before addressing how COVID-19 manifests itself in the images. This will provide the basis to study contemporary methods for both segmentation and classification of lung ultrasound images. The paper concludes with a discussion regarding practical considerations of lung ultrasound image processing use and draws parallels between different methods to allow researchers to decide which particular method may be best considering their needs. With the deficit of trained sonographers who are working to diagnose the thousands of people afflicted by COVID-19, a partially or totally automated lung ultrasound detection and diagnosis tool would be a major asset to fight the pandemic at the front lines.},
  archive      = {J_FDATA},
  author       = {McDermott, Conor and Łącki, Maciej and Sainsbury, Ben and Henry, Jessica and Filippov, Mihail and Rossa, Carlos},
  doi          = {10.3389/fdata.2021.612561},
  journal      = {Frontiers in Big Data},
  month        = {3},
  pages        = {612561},
  shortjournal = {Front. Big Data},
  title        = {Sonographic diagnosis of COVID-19: A review of image processing for lung ultrasound},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A framework for contractual graphs. <em>FDATA</em>,
<em>4</em>, 603282. (<a
href="https://doi.org/10.3389/fdata.2021.603282">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies contractual graphs, where the formation of edges between nodes result in dyadic exchanges. Each dyadic exchange is analyzed as a contractual agreement that is implemented upon fulfilment of underlying conditions. As these dyadic exchanges proliferate, the resulting population of these exchanges creates a contractual graph. A contractual framework for graphs is especially useful in applications where AI-enabled software is employed to create or automate smart contracts between nodes. While some smart contracts may be easily created and executed, others may contain a higher level of ambiguity which may prevent their efficient implementation. Ambiguity in contractual elements is especially difficult to implement, since nodes have to efficiently sense the ambiguity and allocate appropriate amounts of computational resources to the ambiguous contractual task. This paper develops a two-node contractual model of graphs, with varying levels of ambiguity in the contracts and examines its consequences for a market where tasks of differing ambiguity are available to be completed by nodes. The central theme of this paper is that as ambiguity increases, it is difficult for nodes to efficiently commit to the contract since there is an uncertainty in the amount of resources that they have to allocate for completion of the tasks specified in the contract. Thus, while linguistic ambiguity or situational ambiguity might not be cognitively burdensome for humans, it might become expensive for nodes involved in the smart contract. The paper also shows that timing matters—the order in which nodes enter the contract is important as they proceed to sense the ambiguity in a task and then allocate appropriate resources. We propose a game-theoretic formulation to scrutinize how nodes that move first to complete a task are differently impacted than those that move second. We discuss the applications of such a contractual framework for graphs and obtain conditions under which two-node contracts can achieve a successful coalition.},
  archive      = {J_FDATA},
  author       = {Murimi, Renita M.},
  doi          = {10.3389/fdata.2021.603282},
  journal      = {Frontiers in Big Data},
  month        = {3},
  pages        = {603282},
  shortjournal = {Front. Big Data},
  title        = {A framework for contractual graphs},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Knowledge transfer via pre-training for recommendation: A
review and prospect. <em>FDATA</em>, <em>4</em>, 602071. (<a
href="https://doi.org/10.3389/fdata.2021.602071">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommender systems aim to provide item recommendations for users and are usually faced with data sparsity problems (e.g., cold start) in real-world scenarios. Recently pre-trained models have shown their effectiveness in knowledge transfer between domains and tasks, which can potentially alleviate the data sparsity problem in recommender systems. In this survey, we first provide a review of recommender systems with pre-training. In addition, we show the benefits of pre-training to recommender systems through experiments. Finally, we discuss several promising directions for future research of recommender systems with pre-training. The source code of our experiments will be available to facilitate future research.},
  archive      = {J_FDATA},
  author       = {Zeng, Zheni and Xiao, Chaojun and Yao, Yuan and Xie, Ruobing and Liu, Zhiyuan and Lin, Fen and Lin, Leyu and Sun, Maosong},
  doi          = {10.3389/fdata.2021.602071},
  journal      = {Frontiers in Big Data},
  month        = {3},
  pages        = {602071},
  shortjournal = {Front. Big Data},
  title        = {Knowledge transfer via pre-training for recommendation: A review and prospect},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). User, usage and usability: Redefining human centric cyber
security. <em>FDATA</em>, <em>4</em>, 583723. (<a
href="https://doi.org/10.3389/fdata.2021.583723">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The effectiveness of cyber security measures are often questioned in the wake of hard hitting security events. Despite much work being done in the field of cyber security, most of the focus seems to be concentrated on system usage. In this paper, we survey advancements made in the development and design of the human centric cyber security domain. We explore the increasing complexity of cyber security with a wider perspective, defining user, usage and usability (3U’s) as three essential components for cyber security consideration, and classify developmental efforts through existing research works based on the human centric security design, implementation and deployment of these components. Particularly, the focus is on studies that specifically illustrate the shift in paradigm from functional and usage centred cyber security, to user centred cyber security by considering the human aspects of users. The aim of this survey is to provide both users and system designers with insights into the workings and applications of human centric cyber security.},
  archive      = {J_FDATA},
  author       = {Grobler, Marthie and Gaire, Raj and Nepal, Surya},
  doi          = {10.3389/fdata.2021.583723},
  journal      = {Frontiers in Big Data},
  month        = {3},
  pages        = {583723},
  shortjournal = {Front. Big Data},
  title        = {User, usage and usability: Redefining human centric cyber security},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Named data networking for genomics data management and
integrated workflows. <em>FDATA</em>, <em>4</em>, 582468. (<a
href="https://doi.org/10.3389/fdata.2021.582468">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advanced imaging and DNA sequencing technologies now enable the diverse biology community to routinely generate and analyze terabytes of high resolution biological data. The community is rapidly heading toward the petascale in single investigator laboratory settings. As evidence, the single NCBI SRA central DNA sequence repository contains over 45 petabytes of biological data. Given the geometric growth of this and other genomics repositories, an exabyte of mineable biological data is imminent. The challenges of effectively utilizing these datasets are enormous as they are not only large in the size but also stored in geographically distributed repositories in various repositories such as National Center for Biotechnology Information (NCBI), DNA Data Bank of Japan (DDBJ), European Bioinformatics Institute (EBI), and NASA’s GeneLab. In this work, we first systematically point out the data-management challenges of the genomics community. We then introduce Named Data Networking (NDN), a novel but well-researched Internet architecture, is capable of solving these challenges at the network layer. NDN performs all operations such as forwarding requests to data sources, content discovery, access, and retrieval using content names (that are similar to traditional filenames or filepaths) and eliminates the need for a location layer (the IP address) for data management. Utilizing NDN for genomics workflows simplifies data discovery, speeds up data retrieval using in-network caching of popular datasets, and allows the community to create infrastructure that supports operations such as creating federation of content repositories, retrieval from multiple sources, remote data subsetting, and others. Named based operations also streamlines deployment and integration of workflows with various cloud platforms. Our contributions in this work are as follows 1) we enumerate the cyberinfrastructure challenges of the genomics community that NDN can alleviate, and 2) we describe our efforts in applying NDN for a contemporary genomics workflow (GEMmaker) and quantify the improvements. The preliminary evaluation shows a sixfold speed up in data insertion into the workflow. 3) As a pilot, we have used an NDN naming scheme (agreed upon by the community and discussed in Section 4) to publish data from broadly used data repositories including the NCBI SRA. We have loaded the NDN testbed with these pre-processed genomes that can be accessed over NDN and used by anyone interested in those datasets. Finally, we discuss our continued effort in integrating NDN with cloud computing platforms, such as the Pacific Research Platform (PRP). The reader should note that the goal of this paper is to introduce NDN to the genomics community and discuss NDN’s properties that can benefit the genomics community. We do not present an extensive performance evaluation of NDN—we are working on extending and evaluating our pilot deployment and will present systematic results in a future work.},
  archive      = {J_FDATA},
  author       = {Ogle, Cameron and Reddick, David and McKnight, Coleman and Biggs, Tyler and Pauly, Rini and Ficklin, Stephen P. and Feltus, F. Alex and Shannigrahi, Susmit},
  doi          = {10.3389/fdata.2021.582468},
  journal      = {Frontiers in Big Data},
  month        = {2},
  pages        = {582468},
  shortjournal = {Front. Big Data},
  title        = {Named data networking for genomics data management and integrated workflows},
  volume       = {4},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Detecting group anomalies in tera-scale multi-aspect data
via dense-subtensor mining. <em>FDATA</em>, <em>3</em>, 594302. (<a
href="https://doi.org/10.3389/fdata.2020.594302">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How can we detect fraudulent lockstep behavior in large-scale multi-aspect data (i.e., tensors)? Can we detect it when data are too large to fit in memory or even on a disk? Past studies have shown that dense subtensors in real-world tensors (e.g., social media, Wikipedia, TCP dumps, etc.) signal anomalous or fraudulent behavior such as retweet boosting, bot activities, and network attacks. Thus, various approaches, including tensor decomposition and search, have been proposed for detecting dense subtensors rapidly and accurately. However, existing methods suffer from low accuracy, or they assume that tensors are small enough to fit in main memory, which is unrealistic in many real-world applications such as social media and web. To overcome these limitations, we propose D-Cube, a disk-based dense-subtensor detection method, which also can run in a distributed manner across multiple machines. Compared to state-of-the-art methods, D-Cube is (1) Memory Efficient: requires up to 1,561× less memory and handles 1,000× larger data (2.6TB), (2) Fast: up to 7× faster due to its near-linear scalability, (3) Provably Accurate: gives a guarantee on the densities of the detected subtensors, and (4) Effective: spotted network attacks from TCP dumps and synchronized behavior in rating data most accurately.},
  archive      = {J_FDATA},
  author       = {Shin, Kijung and Hooi, Bryan and Kim, Jisu and Faloutsos, Christos},
  doi          = {10.3389/fdata.2020.594302},
  journal      = {Frontiers in Big Data},
  month        = {4},
  pages        = {594302},
  shortjournal = {Front. Big Data},
  title        = {Detecting group anomalies in tera-scale multi-aspect data via dense-subtensor mining},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A policy and practice review of consumer protections and
their application to hospital-sourced data aggregation and analytics by
third-party companies. <em>FDATA</em>, <em>3</em>, 603044. (<a
href="https://doi.org/10.3389/fdata.2020.603044">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Office of the National Coordinator for Health Information Technology estimates that 96% of all U.S. hospitals use a basic electronic health record, but only 62% are able to exchange health information with outside providers. Barriers to information exchange across EHR systems challenge data aggregation and analysis that hospitals need to evaluate healthcare quality and safety. A growing number of hospital systems are partnering with third-party companies to provide these services. In exchange, companies reserve the rights to sell the aggregated data and analyses produced therefrom, often without the knowledge of patients from whom the data were sourced. Such partnerships fall in a regulatory grey area and raise new ethical questions about whether health, consumer, or health and consumer privacy protections apply. The current opinion probes this question in the context of consumer privacy reform in California. It analyzes protections for health information recently expanded under the California Consumer Privacy Act (“CA Privacy Act”) in 2020 and compares them to protections outlined in the Health Information Portability and Accountability Act (“Federal Privacy Rule”). Four perspectives are considered in this ethical analysis: 1) standards of data deidentification; 2) rights of patients and consumers in relation to their health information; 3) entities covered by the CA Privacy Act; 4) scope and complementarity of federal and state regulations. The opinion concludes that the CCPA is limited in its application when health information is processed by a third-party data aggregation company that is contractually designated as a business associate; when health information is deidentified; and when hospital data are sourced from publicly owned and operated hospitals. Lastly, the opinion offers practical recommendations for facilitating parity between state and federal health data privacy laws and for how a more equitable distribution of informational risks and benefits from the sale of aggregated hospital data could be fostered and presents ways both for-profit and nonprofit hospitals can sustain patient trust when negotiating partnerships with third-party data aggregation companies.},
  archive      = {J_FDATA},
  author       = {Rahimzadeh, Vasiliki},
  doi          = {10.3389/fdata.2020.603044},
  journal      = {Frontiers in Big Data},
  month        = {2},
  pages        = {603044},
  shortjournal = {Front. Big Data},
  title        = {A policy and practice review of consumer protections and their application to hospital-sourced data aggregation and analytics by third-party companies},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Subgroup invariant perturbation for unbiased pre-trained
model prediction. <em>FDATA</em>, <em>3</em>, 590296. (<a
href="https://doi.org/10.3389/fdata.2020.590296">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern deep learning systems have achieved unparalleled success and several applications have significantly benefited due to these technological advancements. However, these systems have also shown vulnerabilities with strong implications on the fairness and trustability of such systems. Among these vulnerabilities, bias has been an Achilles’ heel problem. Many applications such as face recognition and language translation have shown high levels of bias in the systems towards particular demographic sub-groups. Unbalanced representation of these sub-groups in the training data is one of the primary reasons of biased behavior. To address this important challenge, we propose a two-fold contribution: a bias estimation metric termed as Precise Subgroup Equivalence to jointly measure the bias in model prediction and the overall model performance. Secondly, we propose a novel bias mitigation algorithm which is inspired from adversarial perturbation and uses the PSE metric. The mitigation algorithm learns a single uniform perturbation termed as Subgroup Invariant Perturbation which is added to the input dataset to generate a transformed dataset. The transformed dataset, when given as input to the pre-trained model reduces the bias in model prediction. Multiple experiments performed on four publicly available face datasets showcase the effectiveness of the proposed algorithm for race and gender prediction.},
  archive      = {J_FDATA},
  author       = {Majumdar, Puspita and Chhabra, Saheb and Singh, Richa and Vatsa, Mayank},
  doi          = {10.3389/fdata.2020.590296},
  journal      = {Frontiers in Big Data},
  month        = {2},
  pages        = {590296},
  shortjournal = {Front. Big Data},
  title        = {Subgroup invariant perturbation for unbiased pre-trained model prediction},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Editorial: Innovations and perspectives in data mining and
knowledge discovery. <em>FDATA</em>, <em>3</em>, 637906. (<a
href="https://doi.org/10.3389/fdata.2020.637906">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_FDATA},
  author       = {Abe, Naoki and Liu, Huan and Wang, Kuansan},
  doi          = {10.3389/fdata.2020.637906},
  journal      = {Frontiers in Big Data},
  month        = {1},
  pages        = {637906},
  shortjournal = {Front. Big Data},
  title        = {Editorial: Innovations and perspectives in data mining and knowledge discovery},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Proximity-based compression for network embedding.
<em>FDATA</em>, <em>3</em>, 608043. (<a
href="https://doi.org/10.3389/fdata.2020.608043">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network embedding that encodes structural information of graphs into a low-dimensional vector space has been proven to be essential for network analysis applications, including node classification and community detection. Although recent methods show promising performance for various applications, graph embedding still has some challenges; either the huge size of graphs may hinder a direct application of the existing network embedding method to them, or they suffer compromises in accuracy from locality and noise. In this paper, we propose a novel Network Embedding method, NECL, to generate embedding more efficiently or effectively. Our goal is to answer the following two questions: 1) Does the network Compression significantly boost Learning? 2) Does network compression improve the quality of the representation? For these goals, first, we propose a novel graph compression method based on the neighborhood similarity that compresses the input graph to a smaller graph with incorporating local proximity of its vertices into super-nodes; second, we employ the compressed graph for network embedding instead of the original large graph to bring down the embedding cost and also to capture the global structure of the original graph; third, we refine the embeddings from the compressed graph to the original graph. NECL is a general meta-strategy that improves the efficiency and effectiveness of many state-of-the-art graph embedding algorithms based on node proximity, including DeepWalk, Node2vec, and LINE. Extensive experiments validate the efficiency and effectiveness of our method, which decreases embedding time and improves classification accuracy as evaluated on single and multi-label classification tasks with large real-world graphs.},
  archive      = {J_FDATA},
  author       = {Islam, Muhammad Ifte and Tanvir, Farhan and Johnson, Ginger and Akbas, Esra and Aktas, Mehmet Emin},
  doi          = {10.3389/fdata.2020.608043},
  journal      = {Frontiers in Big Data},
  month        = {1},
  pages        = {608043},
  shortjournal = {Front. Big Data},
  title        = {Proximity-based compression for network embedding},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). GPU-accelerated machine learning inference as a service for
computing in neutrino experiments. <em>FDATA</em>, <em>3</em>, 604083.
(<a href="https://doi.org/10.3389/fdata.2020.604083">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning algorithms are becoming increasingly prevalent and performant in the reconstruction of events in accelerator-based neutrino experiments. These sophisticated algorithms can be computationally expensive. At the same time, the data volumes of such experiments are rapidly increasing. The demand to process billions of neutrino events with many machine learning algorithm inferences creates a computing challenge. We explore a computing model in which heterogeneous computing with GPU coprocessors is made available as a web service. The coprocessors can be efficiently and elastically deployed to provide the right amount of computing for a given processing task. With our approach, Services for Optimized Network Inference on Coprocessors (SONIC), we integrate GPU acceleration specifically for the ProtoDUNE-SP reconstruction chain without disrupting the native computing workflow. With our integrated framework, we accelerate the most time-consuming task, track and particle shower hit identification, by a factor of 17. This results in a factor of 2.7 reduction in the total processing time when compared with CPU-only production. For this particular task, only 1 GPU is required for every 68 CPU threads, providing a cost-effective solution.},
  archive      = {J_FDATA},
  author       = {Wang, Michael and Yang, Tingjun and Flechas, Maria Acosta and Harris, Philip and Hawks, Benjamin and Holzman, Burt and Knoepfel, Kyle and Krupa, Jeffrey and Pedro, Kevin and Tran, Nhan},
  doi          = {10.3389/fdata.2020.604083},
  journal      = {Frontiers in Big Data},
  month        = {1},
  pages        = {604083},
  shortjournal = {Front. Big Data},
  title        = {GPU-accelerated machine learning inference as a service for computing in neutrino experiments},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Entropy of co-enrolment networks reveal disparities in high
school STEM participation. <em>FDATA</em>, <em>3</em>, 599016. (<a
href="https://doi.org/10.3389/fdata.2020.599016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The current study uses a network analysis approach to explore the STEM pathways that students take through their final year of high school in Aotearoa New Zealand. By accessing individual-level microdata from New Zealand’s Integrated Data Infrastructure, we are able to create a co-enrolment network comprised of all STEM assessment standards taken by students in New Zealand between 2010 and 2016. We explore the structure of this co-enrolment network though use of community detection and a novel measure of entropy. We then investigate how network structure differs across sub-populations based on students’ sex, ethnicity, and the socio-economic-status (SES) of the high school they attended. Results show the structure of the STEM co-enrolment network differs across these sub-populations, and also changes over time. We find that, while female students were more likely to have been enrolled in life science standards, they were less well represented in physics, calculus, and vocational (e.g., agriculture, practical technology) standards. Our results also show that the enrollment patterns of Asian students had lower entropy, an observation that may be explained by increased enrolments in key science and mathematics standards. Through further investigation of differences in entropy across ethnic group and high school SES, we find that ethnic group differences in entropy are moderated by high school SES, such that sub-populations at higher SES schools had lower entropy. We also discuss these findings in the context of the New Zealand education system and policy changes that occurred between 2010 and 2016.},
  archive      = {J_FDATA},
  author       = {Turnbull, Steven Martin and O’Neale, Dion R. J.},
  doi          = {10.3389/fdata.2020.599016},
  journal      = {Frontiers in Big Data},
  month        = {1},
  pages        = {599016},
  shortjournal = {Front. Big Data},
  title        = {Entropy of co-enrolment networks reveal disparities in high school STEM participation},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Distance-weighted graph neural networks on FPGAs for
real-time particle reconstruction in high energy physics.
<em>FDATA</em>, <em>3</em>, 598927. (<a
href="https://doi.org/10.3389/fdata.2020.598927">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural networks have been shown to achieve excellent performance for several crucial tasks in particle physics, such as charged particle tracking, jet tagging, and clustering. An important domain for the application of these networks is the FGPA-based first layer of real-time data filtering at the CERN Large Hadron Collider, which has strict latency and resource constraints. We discuss how to design distance-weighted graph networks that can be executed with a latency of less than one μs on an FPGA. To do so, we consider a representative task associated to particle reconstruction and identification in a next-generation calorimeter operating at a particle collider. We use a graph network architecture developed for such purposes, and apply additional simplifications to match the computing constraints of Level-1 trigger systems, including weight quantization. Using the hls4ml library, we convert the compressed models into firmware to be implemented on an FPGA. Performance of the synthesized models is presented both in terms of inference accuracy and resource usage.},
  archive      = {J_FDATA},
  author       = {Iiyama, Yutaro and Cerminara, Gianluca and Gupta, Abhijay and Kieseler, Jan and Loncar, Vladimir and Pierini, Maurizio and Qasim, Shah Rukh and Rieger, Marcel and Summers, Sioni and Van Onsem, Gerrit and Wozniak, Kinga Anna and Ngadiuba, Jennifer and Di Guglielmo, Giuseppe and Duarte, Javier and Harris, Philip and Rankin, Dylan and Jindariani, Sergo and Liu, Mia and Pedro, Kevin and Tran, Nhan and Kreinar, Edward and Wu, Zhenbin},
  doi          = {10.3389/fdata.2020.598927},
  journal      = {Frontiers in Big Data},
  month        = {1},
  pages        = {598927},
  shortjournal = {Front. Big Data},
  title        = {Distance-weighted graph neural networks on FPGAs for real-time particle reconstruction in high energy physics},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Monitoring crop status in the continental united states
using the SMAP level-4 carbon product. <em>FDATA</em>, <em>3</em>,
597720. (<a href="https://doi.org/10.3389/fdata.2020.597720">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate monitoring of crop condition is critical to detect anomalies that may threaten the economic viability of agriculture and to understand how crops respond to climatic variability. Retrievals of soil moisture and vegetation information from satellite-based remote-sensing products offer an opportunity for continuous and affordable crop condition monitoring. This study compared weekly anomalies in accumulated gross primary production (GPP) from the SMAP Level-4 Carbon (L4C) product to anomalies calculated from a state-scale weekly crop condition index (CCI) and also to crop yield anomalies calculated from county-level yield data reported at the end of the season. We focused on barley, spring wheat, corn, and soybeans cultivated in the continental United States from 2000 to 2018. We found that consistencies between SMAP L4C GPP anomalies and both crop condition and yield anomalies increased as crops developed from the emergence stage (r: 0.4–0.7) and matured (r: 0.6–0.9) and that the agreement was better in drier regions (r: 0.4–0.9) than in wetter regions (r: −0.8–0.4). The L4C provides weekly GPP estimates at a 1-km scale, permitting the evaluation and tracking of anomalies in crop status at higher spatial detail than metrics based on the state-level CCI or county-level crop yields. We demonstrate that the L4C GPP product can be used operationally to monitor crop condition with the potential to become an important tool to inform decision-making and research.},
  archive      = {J_FDATA},
  author       = {Wurster, Patrick M. and Maneta, Marco and Kimball, John S. and Endsley, K. Arthur and Beguería, Santiago},
  doi          = {10.3389/fdata.2020.597720},
  journal      = {Frontiers in Big Data},
  month        = {1},
  pages        = {597720},
  shortjournal = {Front. Big Data},
  title        = {Monitoring crop status in the continental united states using the SMAP level-4 carbon product},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Identifying clinical and genomic features associated with
chronic kidney disease. <em>FDATA</em>, <em>3</em>, 528828. (<a
href="https://doi.org/10.3389/fdata.2020.528828">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We apply a pattern-based classification method to identify clinical and genomic features associated with the progression of Chronic Kidney disease (CKD). We analyze the African-American Study of Chronic Kidney disease with Hypertension dataset and construct a decision-tree classification model, consisting 15 combinatorial patterns of clinical features and single nucleotide polymorphisms (SNPs), seven of which are associated with slow progression and eight with rapid progression of renal disease among African-American Study of Chronic Kidney patients. We identify four clinical features and two SNPs that can accurately predict CKD progression. Clinical and genomic features identified in our experiments may be used in a future study to develop new therapeutic interventions for CKD patients.},
  archive      = {J_FDATA},
  author       = {Moreno, M. Megan and Bain, Travaughn C. and Moreno, Melissa S. and Carroll, Katherine C. and Cunningham, Emily R. and Ashton, Zoe and Poteau, Roby and Subasi, Ersoy and Lipkowitz, Michael and Subasi, Munevver Mine},
  doi          = {10.3389/fdata.2020.528828},
  journal      = {Frontiers in Big Data},
  month        = {1},
  pages        = {528828},
  shortjournal = {Front. Big Data},
  title        = {Identifying clinical and genomic features associated with chronic kidney disease},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Merging datasets of CyberSecurity incidents for fun and
insight. <em>FDATA</em>, <em>3</em>, 521132. (<a
href="https://doi.org/10.3389/fdata.2020.521132">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Providing an adequate assessment of their cyber-security posture requires companies and organisations to collect information about threats from a wide range of sources. One of such sources is history, intended as the knowledge about past cyber-security incidents, their size, type of attacks, industry sector and so on. Ideally, having a large enough dataset of past security incidents, it would be possible to analyze it with automated tools and draw conclusions that may help in preventing future incidents. Unfortunately, it seems that there are only a few publicly available datasets of this kind that are of good quality. The paper reports our initial efforts in collecting all publicly available security incidents datasets, and building a single, large dataset that can be used to draw statistically significant observations. In order to argue about its statistical quality, we analyze the resulting combined dataset against the original ones. Additionally, we perform an analysis of the combined dataset and compare our results with the existing literature. Finally, we present our findings, discuss the limitations of the proposed approach, and point out interesting research directions.},
  archive      = {J_FDATA},
  author       = {Abbiati, Giovanni and Ranise, Silvio and Schizzerotto, Antonio and Siena, Alberto},
  doi          = {10.3389/fdata.2020.521132},
  journal      = {Frontiers in Big Data},
  month        = {1},
  pages        = {521132},
  shortjournal = {Front. Big Data},
  title        = {Merging datasets of CyberSecurity incidents for fun and insight},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Algorithmic accountability in context. Socio-technical
perspectives on structural causal models. <em>FDATA</em>, <em>3</em>,
519957. (<a href="https://doi.org/10.3389/fdata.2020.519957">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing use of automated decision making (ADM) and machine learning sparked an ongoing discussion about algorithmic accountability. Within computer science, a new form of producing accountability has been discussed recently: causality as an expression of algorithmic accountability, formalized using structural causal models (SCMs). However, causality itself is a concept that needs further exploration. Therefore, in this contribution we confront ideas of SCMs with insights from social theory, more explicitly pragmatism, and argue that formal expressions of causality must always be seen in the context of the social system in which they are applied. This results in the formulation of further research questions and directions.},
  archive      = {J_FDATA},
  author       = {Poechhacker, Nikolaus and Kacianka, Severin},
  doi          = {10.3389/fdata.2020.519957},
  journal      = {Frontiers in Big Data},
  month        = {1},
  pages        = {519957},
  shortjournal = {Front. Big Data},
  title        = {Algorithmic accountability in context. socio-technical perspectives on structural causal models},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
</ul>

</body>
</html>
