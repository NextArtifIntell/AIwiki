<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>OR_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="or---103">OR - 103</h2>
<ul>
<li><details>
<summary>
(2021). Technical note–a simple heuristic policy for stochastic
distribution inventory systems with fixed shipment costs. <em>OR</em>,
<em>69</em>(6), ii–iv. (<a
href="https://doi.org/10.1287/opre.2021.2101">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study a continuous-review, two-echelon inventory system with one central warehouse, multiple local facilities, and each facility facing random demand. Local facilities replenish their stock from the central warehouse (or distribution center), which in turn places orders at an outside supplier with ample supply. Inventory replenishment at each location incurs a fixed-plus-variable cost for each shipment. The optimal policy remains unknown, and even if it exists, such a policy must be extremely complicated. Instead, we evaluate a class of easy-to-implement heuristics, called modified echelon ( r, Q ) policies. The parameters for such a heuristic are obtained by solving a set of independent single-stage systems. We show that the proposed policy is asymptotically optimal, as pairs of system primitives, such as the ratios of the fixed cost of the central facility to those of the local facilities, are scaled up. We also show that as the number of retailers grows, the performance bound of the heuristic converges to a primitive-dependent constant.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2101},
  journal      = {Operations Research},
  number       = {6},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {Technical Note–A simple heuristic policy for stochastic distribution inventory systems with fixed shipment costs},
  volume       = {69},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Compressing over-the-counter markets. <em>OR</em>,
<em>69</em>(6), ii–iv. (<a
href="https://doi.org/10.1287/opre.2021.2107">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over-the-counter markets are at the center of the global reform of the financial system. We show how the size and structure of these markets can undergo rapid and extensive changes when participants engage in portfolio compression, which is an optimization technology that exploits multilateral netting opportunities. We find that tightly knit and concentrated trading structures, as featured by many large over-the-counter markets, are especially susceptible to reductions of notional amounts and network reconfigurations resulting from compression activities. Using a unique transaction-level data set on credit-default-swaps markets, we estimate reduction levels, suggesting that the adoption of this technology can account for a large share of the historical development observed in these markets since the global financial crisis. Finally, we test the effect of a mandate to centrally clear over the counter markets in terms of size and structure. When participants engage in both central clearing and portfolio compression with the clearinghouse, we find large netting failures if clearinghouses proliferate. Allowing for compression across clearinghouses by and large offsets this adverse effect.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2107},
  journal      = {Operations Research},
  number       = {6},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {Compressing over-the-counter markets},
  volume       = {69},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Atomic dynamic flow games: Adaptive vs. Nonadaptive agents.
<em>OR</em>, <em>69</em>(6), ii–iv. (<a
href="https://doi.org/10.1287/opre.2021.2105">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a game model for selfish routing of atomic agents, who compete for use of a network to travel from their origins to a common destination as quickly as possible. We follow a frequently used rule that the latency an agent experiences on each edge is a constant transit time plus a variable waiting time in a queue. A key feature that differentiates our model from related ones is an edge-based tie-breaking rule for prioritizing agents in queueing when they reach an edge at the same time. We study both nonadaptive agents (each choosing a one-off origin–destination path simultaneously at the very beginning) and adaptive ones (each making an online decision at every nonterminal vertex they reach as to which next edge to take). On the one hand, we constructively prove that a (pure) Nash equilibrium (NE) always exists for nonadaptive agents and show that every NE is weakly Pareto optimal and globally first-in first-out. We present efficient algorithms for finding an NE and best responses of nonadaptive agents. On the other hand, we are among the first to consider adaptive atomic agents, for which we show that a subgame perfect equilibrium (SPE) always exists and that each NE outcome for nonadaptive agents is an SPE outcome for adaptive agents but not vice versa.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2105},
  journal      = {Operations Research},
  number       = {6},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {Atomic dynamic flow games: Adaptive vs. nonadaptive agents},
  volume       = {69},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A mathematical model of humanitarian aid agencies in
attritional conflict environments. <em>OR</em>, <em>69</em>(6), ii–iv.
(<a href="https://doi.org/10.1287/opre.2021.2130">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional combat models, such as Lanchester’s equations, are typically limited to two competing populations and exhibit solutions characterized by exponential decay—and growth if logistics are included. We enrich such models to account for modern and future complexities, particularly around the role of interagency engagement in operations as often displayed in counterinsurgency operations. To address this, we explore incorporation of nontrophic effects from ecological modeling. This provides a global representation of asymmetrical combat between two forces in the modern setting in which noncombatant populations are present. As an example, we set the noncombatant population in our model to be a neutral agency supporting the native population to the extent that they are noncombatants. Correspondingly, the opposing intervention force is under obligations to enable an environment in which the neutral agency may undertake its work. In contrast to the typical behavior seen in the classic Lanchester system, our model gives rise to limit cycles and bifurcations that we interpret through a warfighting application. Finally, through a case study, we highlight the importance of the agility of a force in achieving victory when noncombatant populations are present.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2130},
  journal      = {Operations Research},
  number       = {6},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {A mathematical model of humanitarian aid agencies in attritional conflict environments},
  volume       = {69},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). “Now or later?” When to deploy qualification screening in
open-bid auction for re-sourcing. <em>OR</em>, <em>69</em>(6), ii–iv.
(<a href="https://doi.org/10.1287/opre.2021.2111">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper considers a resourcing setting in which a qualified supplier (the incumbent) and multiple suppliers that have not yet been qualified (the entrants) compete in an open-bid descending auction for a single-supplier contract. Because of the risk of supplier nonperformance, the buyer only awards the contract to a qualified supplier; meanwhile, the buyer can conduct supplier qualification screening at a cost to verify whether the entrant suppliers can perform the contract. Conventionally, the buyer would screen entrants before running an auction, that is, the prequalification strategy (PRE). We explore an alternative approach called postqualification strategy (POST), in which the buyer first runs an auction and then conducts qualification screenings based on the suppliers’ auction bids. Our characterization of the dynamic structure of the suppliers’ equilibrium bidding strategy enables the calculation of the buyer’s expected cost under POST, which is computationally intractable without this characterization. We show analytically that POST is cheaper than PRE when the cost of conducting qualification screening is high, the number of entrant suppliers is large, or the entrants’ chance of passing qualification screening is high. To quantify the benefit of POST, we conduct a comprehensive numerical study and find that using the cheaper option between PRE and POST provides significant cost savings over the conventional PRE-only approach. Furthermore, we leverage a revelation principle for multistage games to derive the optimal mechanism as a stronger benchmark for performance comparison. Although the optimal mechanism is theoretically optimal, we find that its complexity renders it difficult to implement in practice; but quite strikingly, the simple and practical approach of choosing the cheaper option between PRE and POST captures the majority of the benefit that the optimal mechanism can offer over PRE, highlighting the practical benefit of POST.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2111},
  journal      = {Operations Research},
  number       = {6},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {“Now or later?” when to deploy qualification screening in open-bid auction for re-sourcing},
  volume       = {69},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Optimal auction duration: A price formation viewpoint.
<em>OR</em>, <em>69</em>(6), ii–iv. (<a
href="https://doi.org/10.1287/opre.2021.2113">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider an auction market in which market makers fill the order book during a given time period while some other investors send market orders. We define the clearing price of the auction as the price maximizing the exchanged volume at the clearing time according to the supply and demand of each market participant. Then we derive in a semiexplicit form the error made between this clearing price and the efficient price as a function of the auction duration. We study the impact of the behavior of market takers on this error. To do so, we consider the case of naive market takers and that of rational market takers playing a Nash equilibrium to minimize their transaction costs. We compute the optimal duration of the auctions for 77 stocks traded on Euronext and compare the quality of the price formation process under this optimal value to the case of a continuous limit order book. Continuous limit order books are found to be usually suboptimal. However, in terms of our metric, they only moderately impair the quality of the price formation process. The order of magnitude of optimal auction durations is from 2–10 minutes.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2113},
  journal      = {Operations Research},
  number       = {6},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {Optimal auction duration: A price formation viewpoint},
  volume       = {69},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dynamic learning and market making in spread betting markets
with informed bettors. <em>OR</em>, <em>69</em>(6), ii–iv. (<a
href="https://doi.org/10.1287/opre.2021.2109">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the profit-maximization problem of a market maker in a spread betting market. In this market, the market maker quotes cutoff lines for the outcome of a certain future event as “prices,” and bettors bet on whether the event outcome exceeds the cutoff lines. Anonymous bettors with heterogeneous strategic behavior and information levels participate in the market. The market maker has limited information on the event outcome distribution, aiming to extract information from the market (i.e., “learning”) while guarding against an informed bettor’s strategic manipulation (i.e., “bluff-proofing”). We show that Bayesian policies that ignore bluffing are typically vulnerable to the informed bettor’s strategic manipulation, resulting in exceedingly large profit losses for the market maker as well as market inefficiency. We develop and analyze a novel family of policies, called inertial policies , that balance the trade-off between learning and bluff-proofing. We construct a simple instance of this family that (i) enables the market maker to achieve a near-optimal profit loss and (ii) eventually yields market efficiency.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2109},
  journal      = {Operations Research},
  number       = {6},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {Dynamic learning and market making in spread betting markets with informed bettors},
  volume       = {69},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On the futility of dynamics in robust mechanism design.
<em>OR</em>, <em>69</em>(6), ii–iv. (<a
href="https://doi.org/10.1287/opre.2021.2122">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a principal who repeatedly interacts with a strategic agent holding private information. In each round, the agent observes an idiosyncratic shock drawn independently and identically from a distribution known to the agent but not to the principal. The utilities of the principal and the agent are determined by the values of the shock and outcomes that are chosen by the principal based on reports made by the agent. When the principal commits to a dynamic mechanism, the agent best-responds to maximize his aggregate utility over the whole time horizon. The principal’s goal is to design a dynamic mechanism to minimize his worst-case regret, that is, the largest difference possible between the aggregate utility he could obtain if he knew the agent’s distribution and the actual aggregate utility he obtains. We identify a broad class of games in which the principal’s optimal mechanism is static without any meaningful dynamics. The optimal dynamic mechanism, if it exists, simply repeats an optimal mechanism for a single-round problem in each round. The minimax regret is the number of rounds times the minimax regret in the single-round problem. The class of games includes repeated selling of identical copies of a single good or multiple goods, repeated principal-agent relationships with hidden information, and repeated allocation of a resource without money. Outside this class of games, we construct examples in which a dynamic mechanism provably outperforms any static mechanism.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2122},
  journal      = {Operations Research},
  number       = {6},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {On the futility of dynamics in robust mechanism design},
  volume       = {69},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Information inundation on platforms and implications.
<em>OR</em>, <em>69</em>(6), ii–iv. (<a
href="https://doi.org/10.1287/opre.2021.2119">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study a model of information consumption in which consumers sequentially interact with a platform that offers a menu of signals (posts) about an underlying state of the world (fact). At each time, incapable of consuming all posts, consumers screen the posts and only select (and consume) one from the offered menu. We show that, in the presence of uncertainty about the accuracy of these posts and as the number of posts increases, adverse effects, such as slow learning and polarization, arise. Specifically, we establish that, in this setting, bias emerges as a consequence of the consumer’s screening process. Namely, consumers, in their quest to choose the post that reduces their uncertainty about the state of the world, choose to consume the post that is closest to their own beliefs. We study the evolution of beliefs, and we show that such a screening bias slows down the learning process and that the speed of learning decreases with the menu size. Further, we show that the society becomes polarized during the prolonged learning process even in situations in which the society’s belief distribution was not a priori polarized.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2119},
  journal      = {Operations Research},
  number       = {6},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {Information inundation on platforms and implications},
  volume       = {69},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Improved revenue bounds for posted-price and second-price
mechanisms. <em>OR</em>, <em>69</em>(6), ii–iv. (<a
href="https://doi.org/10.1287/opre.2021.2121">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study revenue maximization through sequential posted-price (SPP) mechanisms in single-dimensional settings with n buyers and independent but not necessarily identical value distributions. We construct the SPP mechanisms by considering the best of two simple pricing rules: one that imitates the revenue optimal mechanism, namely, the Myersonian mechanism, via the taxation principle and the other that posts a uniform price. Our pricing rules are rather generalizable and yield the first improvement over long established approximation factors in several settings. We design factor-revealing mathematical programs that crisply capture the approximation factor of our SPP mechanism. In the single-unit setting, our SPP mechanism yields a better approximation factor than the state of the art prior to our work. In the multiunit setting, our SPP mechanism yields the first improved approximation factor over the state of the art after over nine years. Our results on SPP mechanisms immediately imply improved performance guarantees for the equivalent free-order prophet inequality problem. In the position auction setting, our SPP mechanism yields the first higher-than ( 1 − 1 / e ) approximation factor. In eager second-price auctions, our two simple pricing rules lead to the first improved approximation factor that is strictly greater than what is obtained by the SPP mechanism in the single-unit setting.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2121},
  journal      = {Operations Research},
  number       = {6},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {Improved revenue bounds for posted-price and second-price mechanisms},
  volume       = {69},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An analysis of “buy x, get one free” reward programs.
<em>OR</em>, <em>69</em>(6), ii–iv. (<a
href="https://doi.org/10.1287/opre.2021.2128">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the effects of redemption hurdles on reward program members’ decision making and firm profitability. We focus on the popular “Buy X , Get One Free” (BXGO) programs, which set a redemption threshold ( X ) and possibly, an expiration term for the reward. In our model, forward-looking consumers interact with a monopolistic firm and strategically make purchase and redemption decisions over an infinite time horizon. Our analysis leads to the following results. First, a consumer’s purchase utility and purchase probability increase as her reward point inventory approaches the redemption threshold or expiration. These patterns are consistent with the “point pressure” phenomenon documented in the empirical literature. Second, a redemption threshold alone cannot improve the firm’s profit, unless it is coupled with a finite expiration term or a positive transaction utility that consumers may derive from reward redemption. Third, setting the optimal redemption threshold requires the program to strike a balance between the effective price paid by consumers and their purchase probabilities. These results have rich managerial implications for effectively designing reward programs.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2128},
  journal      = {Operations Research},
  number       = {6},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {An analysis of “Buy x, get one free” reward programs},
  volume       = {69},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Timing it right: Balancing inpatient congestion
vs. Readmission risk at discharge. <em>OR</em>, <em>69</em>(6), ii–iv.
(<a href="https://doi.org/10.1287/opre.2020.2044">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When to discharge a patient plays an important role in hospital patient flow management and the quality of care and patient outcomes. In this work, we develop and implement a data-integrated decision support framework to aid hospitals in managing the delicate balance between readmission risk at discharge and ward congestion. We formulate a large-scale Markov decision process (MDP) that integrates a personalized readmission prediction model to dynamically prescribe both how many and which patients to discharge on each day. Because of patient heterogeneity and the fact that length of stay is not memoryless, the MDP has the curse of dimensionality. We leverage structural properties and an analytical solution for a special cost setting to transform the MDP into a univariate optimization; this leads to a novel, efficient dynamic heuristic. Furthermore, for our decision framework to be implementable in practice, we build a unified prediction model that integrates several statistical methods and provides key inputs to the decision framework; existing off-the-shelf readmission prediction models alone could not adequately parametrize our decision support. Through extensive counterfactual analyses, we demonstrate the value of our discharge decision tool over our partner hospital’s historical discharge behavior. We also obtain generalizable insights by applying the tool to a broad range of hospital types through a high-fidelity simulation. Last, we showcase an implementation of our tool at our partner hospital to demonstrate broader applicability through our framework’s plug-and-play design for integration with general hospital data systems and workflows.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.2044},
  journal      = {Operations Research},
  number       = {6},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {Timing it right: Balancing inpatient congestion vs. readmission risk at discharge},
  volume       = {69},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). To pool or not to pool: Queueing design for large-scale
service systems. <em>OR</em>, <em>69</em>(6), ii–iv. (<a
href="https://doi.org/10.1287/opre.2019.1976">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There are two basic queue structures commonly adopted in service systems: the pooled structure, where waiting customers are organized into a single queue served by a group of servers, and the dedicated structure, where each server has her own queue. Although the pooled structure, known to minimize the servers’ idle time, is widely used in large-scale service systems, this study reveals that the dedicated structure, along with the join-the-shortest-queue routing policy, could be more advantageous for improving certain performance measures, such as the probability of a customer’s waiting time being within a delay target. The servers’ additional idleness resulting from the dedicated structure will be negligible when the system scale is large. Using a fluid model substantiated by asymptotic analysis, we provide a performance comparison between the two structures for a moderately overloaded queueing system with customer abandonment. We intend to help service system designers answer the following question: To reach a specified service-level target, which queue structure will be more cost effective? Aside from structure design, our results are of practical value for performance analysis and staffing deployment.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1976},
  journal      = {Operations Research},
  number       = {6},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {To pool or not to pool: Queueing design for large-scale service systems},
  volume       = {69},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Nonadditive multiattribute utility functions for portfolio
decision analysis. <em>OR</em>, <em>69</em>(6), ii–iv. (<a
href="https://doi.org/10.1287/opre.2020.2046">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Portfolio decision analysis models support selecting a portfolio of projects in view of multiple objectives and limited resources. In applications, portfolio utility is commonly modeled as the sum of the projects’ multiattribute utilities, although such approaches lack rigorous decision-theoretic justification. This paper establishes the axiomatic foundations of a more general class of multilinear portfolio utility functions, which includes additive and multiplicative portfolio utility functions as special cases. Furthermore, we develop preference elicitation techniques to assess these portfolio utility functions as well as optimization models to identify the most preferred portfolio in view of resource and other constraints. We also examine how the functional form of the portfolio utility function affects decision recommendations by using randomly generated and real problem instances.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.2046},
  journal      = {Operations Research},
  number       = {6},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {Nonadditive multiattribute utility functions for portfolio decision analysis},
  volume       = {69},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Performance of the smallest-variance-first rule in
appointment sequencing. <em>OR</em>, <em>69</em>(6), ii–iv. (<a
href="https://doi.org/10.1287/opre.2020.2025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A classic problem in appointment scheduling with applications in healthcare concerns the determination of the patients’ arrival times that minimize a cost function that is a weighted sum of mean waiting times and mean idle times. One aspect of this problem is the sequencing problem , which focuses on ordering the patients. We assess the performance of the smallest-variance-first (SVF) rule, which sequences patients in order of increasing variance of their service durations. Although it is known that SVF is not always optimal, it has been widely observed that it performs well in practice and simulation. We provide a theoretical justification for this observation by proving, in various settings, quantitative worst-case bounds on the ratio between the cost incurred by the SVF rule and the minimum attainable cost. We also show that, in great generality, SVF is asymptotically optimal, that is, the ratio approaches one as the number of patients grows large. Although evaluating policies by considering an approximation ratio is a standard approach in many algorithmic settings, our results appear to be the first of this type in the appointment-scheduling literature.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.2025},
  journal      = {Operations Research},
  number       = {6},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {Performance of the smallest-variance-first rule in appointment sequencing},
  volume       = {69},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dynamic server assignment in multiclass queues with shifts,
with applications to nurse staffing in emergency departments.
<em>OR</em>, <em>69</em>(6), ii–iv. (<a
href="https://doi.org/10.1287/opre.2020.2050">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many service systems are staffed by workers who work in shifts . In this article, we study the dynamic assignment of servers to different areas of a service system at the beginning of discrete time intervals, namely, shifts. The ability to reassign servers at discrete intervals, rather than continuously, introduces a partial flexibility that provides an opportunity for reducing the expected waiting time of customers. The problem is primarily motivated by an application to nurse staffing in emergency departments (EDs), where nurses can work in different areas of the ED, but their assignment can only be changed at the beginning of their shifts (typically 8–12 hours). To investigate the reassignment decision and its potential benefits, we consider a multiclass queueing system, where customers of each class differ in terms of their average service requirements and the holding cost incurred as they wait in the queues. We study a discrete-time fluid control problem to minimize transient holding costs over a finite horizon and show that an appropriate “translation” of the solution to the fluid control problem is asymptotically optimal for the original stochastic system. Through analysis of the fluid control problem we further obtain insights on the structure of “good” policies in the presence of the shift constraint. Leveraging these insights, we develop heuristic policies and use simulation to demonstrate their effectiveness in systems with dynamics often observed in EDs. We find that, in a parameter regime relevant to our motivating application, the partial flexibility introduced by reassigning servers at the beginning of shifts can substantially reduce the expected cost of the system—by 10\%–50\% in some parameter regimes—compared with the status quo, dedicated staffing.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.2050},
  journal      = {Operations Research},
  number       = {6},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {Dynamic server assignment in multiclass queues with shifts, with applications to nurse staffing in emergency departments},
  volume       = {69},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Procurement with cost and noncost attributes: Cost-sharing
mechanisms. <em>OR</em>, <em>69</em>(5), ii–iv. (<a
href="https://doi.org/10.1287/opre.2020.2060">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A buyer faces a two-dimensional mechanism design problem for awarding a project to one among a set of contractors, each of whom is privately informed about the contractor’s cost and the contractor’s estimate of an a priori random noncost attribute. The winning contractor realizes the noncost attribute upon the project’s completion and may “manipulate” it in a costless manner (if such a manipulation is beneficial to the contractor). The noncost attribute inflicts a disutility cost on the buyer. This procurement problem arises in situations such as highway construction projects, in which completion times are a major concern. We establish the significance of incorporating the possibility of manipulation in two ways: (1) Using an optimal mechanism obtained by ignoring the possibility of manipulation can generate perverse incentives for the winning contractor to engage in manipulation. (2) The privacy of the noncost estimates can generate information rent only because of the possibility of contractors’ manipulation. We further study the family of cost-sharing mechanisms as a nonmanipulable, easy-to-implement, and near-optimal solution to the buyer’s procurement problem. In a cost-sharing mechanism, the winning contractor is selected via a second-price auction and needs to reimburse a prespecified fraction—referred to as the cost-sharing fraction—of the buyer’s disutility cost upon completion of the project. We show that the cost-sharing fraction plays an unequivocal role in capturing the essential trade-off between allocative inefficiency and information rent. We also characterize the optimal cost-sharing fraction and offer prescriptive guidelines on the choice of this fraction based on the second-moment information of the buyer’s belief distribution. Finally, we establish the theoretical performance guarantees for the optimal cost-sharing mechanism.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.2060},
  journal      = {Operations Research},
  number       = {5},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {Procurement with cost and noncost attributes: Cost-sharing mechanisms},
  volume       = {69},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The effect of social preferences on sales and operations
planning. <em>OR</em>, <em>69</em>(5), ii–iv. (<a
href="https://doi.org/10.1287/opre.2020.2068">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sales and operations planning processes are used to align production quantities and customer demand. Two key activities of these processes are demand planning and production planning, which are often assigned to individuals in different departments. Production planning requires accurate demand forecasts from demand planning to be able to choose proper production quantities, but demand planners have to invest effort to create accurate demand forecasts. We study the role of social preferences (altruism, inequality aversion, and competitive pressure) in incentivizing demand planners to invest effort, and we analyze how social preferences interact with monetary incentives. We use a game-theoretic model and laboratory experiments. Our results indicate that social preferences can be used to incentivize demand planners to invest effort and that this effect is anticipated by production planners. The resulting more accurate demand forecasts and adapted production quantities result in higher company profit. We also provide an optimization model for optimally allocating investments to financial incentives and social preference building.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.2068},
  journal      = {Operations Research},
  number       = {5},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {The effect of social preferences on sales and operations planning},
  volume       = {69},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). M♮-convexity and its applications in operations.
<em>OR</em>, <em>69</em>(5), ii–iv. (<a
href="https://doi.org/10.1287/opre.2020.2070">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {M ♮ -convexity, one of the main concepts in discrete convex analysis, possesses many salient structural properties and allows for the design of efficient algorithms. In this paper, we establish several new fundamental properties of M ♮ -convexity and its variant SSQM ♮ -convexity (semistrictly quasi M ♮ -convexity). We show that in a parametric maximization model, the optimal solution is nonincreasing in the parameters when the objective function is SSQM ♮ -concave and the constraint is a box and illustrate when SSQM ♮ -convexity and M ♮ -convexity are preserved. A sufficient and necessary characterization of twice continuously differentiable M ♮ -convex functions is provided. We then use them to analyze two important operations models: a classical multiproduct dynamic stochastic inventory model and a portfolio contract model where a buyer reserves capacities in blocks from multiple competing suppliers. We illustrate that looking from the lens of M ♮ -convexity allows to simplify the complicated analysis in the literature for each model and extend the results to more general settings.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.2070},
  journal      = {Operations Research},
  number       = {5},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {M♮-convexity and its applications in operations},
  volume       = {69},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Integrated ad delivery planning for targeted display
advertising. <em>OR</em>, <em>69</em>(5), ii–iv. (<a
href="https://doi.org/10.1287/opre.2021.2136">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Consider a publisher of online display advertising that sells its ad resources in both an up-front market and a spot market. When planning its ad delivery, the publisher needs to make a trade-off between earning a greater short-term profit from the spot market and improving advertising effectiveness in the up-front market. To address this challenge, we propose an integrated planning model that is robust to the uncertainties associated with the supply of advertising resources. Specifically, we model the problem as a distributionally robust chance-constrained program. We first approximate the program by using a robust optimization model, which is then transformed into a linear program. We provide a theoretical bound on the performance loss due to this transformation. A clustering algorithm is proposed to solve large-scale cases in practice. We implement ad serving of our planning model on two real data sets, and we demonstrate how to incorporate realistic constraints such as exclusivity and frequency caps. Our numerical experiments demonstrate that our approach is very effective: it generates more revenue while fulfilling the guaranteed contracts and ensuring advertising effectiveness.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2136},
  journal      = {Operations Research},
  number       = {5},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {Integrated ad delivery planning for targeted display advertising},
  volume       = {69},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Competitive investment with bayesian learning: Choice of
business size and timing. <em>OR</em>, <em>69</em>(5), ii–iv. (<a
href="https://doi.org/10.1287/opre.2020.2080">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by the challenges faced by firms entering an unknown market, we study a strategic investment problem in a duopoly setting. The favorableness of the market is unknown to both firms, but firms have prior information about it. A leader invests first by choosing its investment size. Then, in a continuous-time Bayesian setting, a competitive follower dynamically learns about the favorableness of the market by observing the leader’s earnings and chooses its investment size and timing. In this setting, we characterize equilibrium strategies of firms. A distinctive feature of our model is that firms choose their investment sizes, and thus the follower’s observations about the favorableness of the market can be censored due to the leader’s investment size choice. It is generally accepted that if there is an increase in the likelihood of a favorable market, then the firm’s expected discounted profit and its investment size increase. Our paper shows that, contrary to this common understanding, the leader’s equilibrium expected discounted profit and equilibrium investment size can strictly decrease when there is an increase in the likelihood of a favorable market. This is due to a nontrivial interplay between the leader’s investment size decision and the follower’s investment strategy.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.2080},
  journal      = {Operations Research},
  number       = {5},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {Competitive investment with bayesian learning: Choice of business size and timing},
  volume       = {69},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deals or no deals: Contract design for online advertising.
<em>OR</em>, <em>69</em>(5), ii–iv. (<a
href="https://doi.org/10.1287/opre.2020.2087">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a formal study of first-look and preferred deals that are a recently introduced generation of contracts for selling online advertisements, which generalize traditional reservation contracts and are suitable for advertisers with advanced targeting capabilities. Under these deals, one or more advertisers gain priority access to an inventory of impressions before others and can choose to purchase in real time. In particular, we propose constant-factor approximation algorithms for maximizing the revenue that can be obtained from these deals when offered to all or a subset of the advertisers, whose valuation distributions can be independent or correlated through a common value component. We evaluate our algorithms using data from Google’s ad exchange platform and show they perform better than the approximation guarantees and obtain significantly higher revenue than auctions; in certain cases, the observed revenue is 85\%–96\% of the optimal revenue achievable. We also prove the NP-hardness of designing deals when advertisers’ valuations are arbitrarily correlated and the optimality of menus of deals among a certain class of selling mechanisms in an incomplete distributional information setting.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.2087},
  journal      = {Operations Research},
  number       = {5},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {Deals or no deals: Contract design for online advertising},
  volume       = {69},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Placement optimization in refugee resettlement. <em>OR</em>,
<em>69</em>(5), ii–iv. (<a
href="https://doi.org/10.1287/opre.2020.2093">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Every year, tens of thousands of refugees are resettled to dozens of host countries. Although there is growing evidence that the initial placement of refugee families profoundly affects their lifetime outcomes, there have been few attempts to optimize resettlement decisions. We integrate machine learning and integer optimization into an innovative software tool, Annie ™ Matching and Outcome Optimization for Refugee Empowerment ( Annie ™ Moore) , that assists a U.S. resettlement agency with matching refugees to their initial placements. Our software suggests optimal placements while giving substantial autonomy to the resettlement staff to fine-tune recommended matches, thereby streamlining their resettlement operations. Initial back testing indicates that Annie ™ can improve short-run employment outcomes by 22\%–38\%. We conclude by discussing several directions for future work.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.2093},
  journal      = {Operations Research},
  number       = {5},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {Placement optimization in refugee resettlement},
  volume       = {69},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Operations research: Topics, impact, and trends from
1952–2019. <em>OR</em>, <em>69</em>(5), ii–iv. (<a
href="https://doi.org/10.1287/opre.2021.2139">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper is a retrospective look at 68 years of publication output of Operations Research , revealing changes in its publications, its authors, and their impact over time and how these changes might affect researchers and practitioners in the present. A total of 5,440 journal articles from its inception in 1952 to 2019 are used. The analysis initially focuses on the most studied topics and then continues with the top research methods and research problems investigated. The top contributing countries and authors to the most investigated research problems are also studied. The results indicate that mathematical programming is the most common research method, whereas inventory is the most investigated problem. However, investigations related to pricing are growing significantly. The United States, Canada, and the United Kingdom publish the most papers, with the United States and Canada having similar publication profiles per capita. Inventory is the most popular research problem studied by North American, Asian, and Middle Eastern countries, whereas European countries focus on scheduling problems. In order to understand the latest research trend, we visualize the networks of the last 10 years of Operations Research that show dynamic programming as the most used method and pricing as the most studied problem. We further visualize the coauthor networks on both dynamic programming and pricing to identify the most significant clusters of researchers and the topics these research clusters collaborate on. Finally, we provide researchers information about where Operations Research is and where it is likely heading.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2139},
  journal      = {Operations Research},
  number       = {5},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {Operations research: Topics, impact, and trends from 1952–2019},
  volume       = {69},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Assortment optimization and pricing under the multinomial
logit model with impatient customers: Sequential recommendation and
selection. <em>OR</em>, <em>69</em>(5), ii–iv. (<a
href="https://doi.org/10.1287/opre.2021.2127">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a variant of the multinomial logit model with impatient customers and study assortment optimization and pricing problems under this choice model. In our choice model, a customer incrementally views the assortment of available products in multiple stages. The patience level of a customer determines the maximum number of stages in which the customer is willing to view the assortments of products. In each stage, if the product with the largest utility provides larger utility than a minimum acceptable utility, which we refer to as the utility of the outside option, then the customer purchases that product right away. Otherwise, the customer views the assortment of products in the next stage as long as the customer’s patience level allows the customer to do so. Under the assumption that the utilities have the Gumbel distribution and are independent, we give a closed-form expression for the choice probabilities. For the assortment-optimization problem, we develop a polynomial-time algorithm to find the revenue-maximizing sequence of assortments to offer. For the pricing problem, we show that, if the sequence of offered assortments is fixed, then we can solve a convex program to find the revenue-maximizing prices, with which the decision variables are the probabilities that a customer reaches different stages. We build on this result to give a 0.878-approximation algorithm when both the sequence of assortments and the prices are decision variables. We consider the assortment-optimization problem when each product occupies some space and there is a constraint on the total space consumption of the offered products. We give a fully polynomial-time approximation scheme for this constrained problem. We use a data set from Expedia to demonstrate that incorporating patience levels, as in our model, can improve purchase predictions. We also check the practical performance of our approximation schemes in terms of both the quality of solutions and the computation times.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2127},
  journal      = {Operations Research},
  number       = {5},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {Assortment optimization and pricing under the multinomial logit model with impatient customers: Sequential recommendation and selection},
  volume       = {69},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Technical note—on nested partitions method for global
optimization. <em>OR</em>, <em>69</em>(5), ii–iv. (<a
href="https://doi.org/10.1287/opre.2020.2026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Shi and Ólafsson [(2000) Nested Partitions Method for Global Optimization. Operations Research . 48(3):390–407] proposed the Nested Partitions (NP) method with two different NP backtracking rules—namely, NP I and NP II—for solving global optimization problems. Two of their main results are the properties of the global convergence of the NP method stated in theorems 3 and 4 on pages 398 and 399, respectively. In particular, theorem 3 provides a hitting-probability-based formula to represent the bound of the expected number of convergence iterations for both NP I and II, and theorem 4 evaluates its expected numeric bound of convergence iterations under a particular case for NP II. We prove that both theorems are fundamentally incorrect and rectify them in this study. Our computational results show that the expected convergence bounds presented by Shi and Ólafsson can be deviated from the actual convergence bounds of NP II approximately by 50\% for some tested scenarios.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.2026},
  journal      = {Operations Research},
  number       = {5},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {Technical Note—On nested partitions method for global optimization},
  volume       = {69},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On the stability of redundancy models. <em>OR</em>,
<em>69</em>(5), ii–iv. (<a
href="https://doi.org/10.1287/opre.2020.2030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate the stability condition of redundancy-d multiserver systems. Each server has its own queue and implements popular scheduling disciplines such as first-come-first-serve (FCFS), processor sharing (PS), and random order of service (ROS). New jobs arrive according to a Poisson process, and copies of each job are sent to d servers chosen uniformly at random. The service times of jobs are assumed to be exponentially distributed. A job departs as soon as one of its copies finishes service. Under the assumption that all d copies are independent and identically distributed (i.i.d.), we show that for PS and ROS (for FCFS it is already known), sending redundant copies does not reduce the stability region. Under the assumption that the d copies are identical, we show that (i) ROS does not reduce the stability region; (ii) FCFS reduces the stability region, which can be characterized through an associated saturated system; and (iii) PS severely reduces the stability region, which coincides with the system where all copies have to be fully served. The proofs are based on careful characterizations of scaling limits of the underlying stochastic process. Through simulations, we obtain interesting insights on the system’s performance for nonexponential service time distributions and heterogeneous server speeds.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.2030},
  journal      = {Operations Research},
  number       = {5},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {On the stability of redundancy models},
  volume       = {69},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Technical note—approximating systems fed by poisson
processes with rapidly changing arrival rates. <em>OR</em>,
<em>69</em>(5), ii–iv. (<a
href="https://doi.org/10.1287/opre.2020.2031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a new asymptotic regime for simplifying stochastic models having nonstationary effects, such as those that arise in the presence of time-of-day effects. This regime describes an operating environment within which the arrival process to a service system has an arrival intensity that is fluctuating rapidly. We show that such a service system is well approximated by the corresponding model in which the arrival process is Poisson with a constant arrival rate. In addition to the basic weak convergence theorem, we also establish a first order correction for the distribution of the cumulative number of arrivals over [ 0 , t ] , as well as the number-in-system process for an infinite-server queue fed by an arrival process having a rapidly changing arrival rate. This new asymptotic regime provides a second regime within which nonstationary stochastic models can be reasonably approximated by a process with stationary dynamics, thereby complementing the previously studied setting within which rates vary slowly in time.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.2031},
  journal      = {Operations Research},
  number       = {5},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {Technical Note—Approximating systems fed by poisson processes with rapidly changing arrival rates},
  volume       = {69},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Private sequential learning. <em>OR</em>, <em>69</em>(5),
ii–iv. (<a href="https://doi.org/10.1287/opre.2020.2021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We formulate a private learning model to study an intrinsic tradeoff between privacy and query complexity in sequential learning. Our model involves a learner who aims to determine a scalar value v ∗ by sequentially querying an external database and receiving binary responses. In the meantime, an adversary observes the learner’s queries, although not the responses, and tries to infer from them the value of v ∗ . The objective of the learner is to obtain an accurate estimate of v ∗ using only a small number of queries while simultaneously protecting his or her privacy by making v ∗ provably difficult to learn for the adversary. Our main results provide tight upper and lower bounds on the learner’s query complexity as a function of desired levels of privacy and estimation accuracy. We also construct explicit query strategies whose complexity is optimal up to an additive constant.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.2021},
  journal      = {Operations Research},
  number       = {5},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {Private sequential learning},
  volume       = {69},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dynamic programming deconstructed: Transformations of the
bellman equation and computational efficiency. <em>OR</em>,
<em>69</em>(5), ii–iv. (<a
href="https://doi.org/10.1287/opre.2020.2006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Some approaches to solving challenging dynamic programming problems, such as Q-learning, begin by transforming the Bellman equation into an alternative functional equation to open up a new line of attack. Our paper studies this idea systematically with a focus on boosting computational efficiency. We provide a characterization of the set of valid transformations of the Bellman equation, for which validity means that the transformed Bellman equation maintains the link to optimality held by the original Bellman equation. We then examine the solutions of the transformed Bellman equations and analyze correspondingly transformed versions of the algorithms used to solve for optimal policies. These investigations yield new approaches to a variety of discrete time dynamic programming problems, including those with features such as recursive preferences or desire for robustness. Increased computational efficiency is demonstrated via time complexity arguments and numerical experiments.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.2006},
  journal      = {Operations Research},
  number       = {5},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {Dynamic programming deconstructed: Transformations of the bellman equation and computational efficiency},
  volume       = {69},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Envelope theorems for multistage linear stochastic
optimization. <em>OR</em>, <em>69</em>(5), ii–iv. (<a
href="https://doi.org/10.1287/opre.2020.2038">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a method to compute derivatives of multistage linear stochastic optimization problems with respect to parameters that influence the problem’s data. Our results are based on classical envelope theorems and can be used in problems directly solved via their deterministic equivalents as well as in stochastic dual dynamic programming for which the derivatives of the optimal value are sampled. We derive smoothness properties for optimal values of linear optimization problems, which we use to show that the computed derivatives are valid almost everywhere under mild assumptions. We discuss two numerical case studies, demonstrating that our approach is superior, both in terms of accuracy and computationally, to naïve methods of computing derivatives that are based on difference quotients.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.2038},
  journal      = {Operations Research},
  number       = {5},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {Envelope theorems for multistage linear stochastic optimization},
  volume       = {69},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Calibration of distributionally robust empirical
optimization models. <em>OR</em>, <em>69</em>(5), ii–iv. (<a
href="https://doi.org/10.1287/opre.2020.2041">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the out-of-sample properties of robust empirical optimization problems with smooth φ -divergence penalties and smooth concave objective functions, and we develop a theory for data-driven calibration of the nonnegative “robustness parameter” δ that controls the size of the deviations from the nominal model. Building on the intuition that robust optimization reduces the sensitivity of the expected reward to errors in the model by controlling the spread of the reward distribution, we show that the first-order benefit of “little bit of robustness” (i.e., δ small, positive) is a significant reduction in the variance of the out-of-sample reward, whereas the corresponding impact on the mean is almost an order of magnitude smaller. One implication is that substantial variance (sensitivity) reduction is possible at little cost if the robustness parameter is properly calibrated. To this end, we introduce the notion of a robust mean-variance frontier to select the robustness parameter and show that it can be approximated using resampling methods such as the bootstrap. Our examples show that robust solutions resulting from “open-loop” calibration methods (e.g., selecting a 90\% confidence level regardless of the data and objective function) can be very conservative out of sample, whereas those corresponding to the robustness parameter that optimizes an estimate of the out-of-sample expected reward (e.g., via the bootstrap) with no regard for the variance are often insufficiently robust.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.2041},
  journal      = {Operations Research},
  number       = {5},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {Calibration of distributionally robust empirical optimization models},
  volume       = {69},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Technical note—preservation of additive convexity and its
applications in stochastic optimization problems. <em>OR</em>,
<em>69</em>(4), ii–iv. (<a
href="https://doi.org/10.1287/opre.2020.2064">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we establish two preservation results of additive convexity for a class of optimal transformation problems and a class of optimal disposal problems. For both classes of problems, there are multiple resources; our results show that if these resources have different priorities to be transformed/disposed under the optimal policy, then the additive convexity and bounded monotonicity of the objective function are preserved to the value function after optimization. A key observation is that an optimal transformation problem with prioritized optimal decisions is equivalent to a serial inventory problem with zero lead times. We demonstrate the applications of our results to several stochastic optimization problems in operations management.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.2064},
  journal      = {Operations Research},
  number       = {4},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {Technical Note—Preservation of additive convexity and its applications in stochastic optimization problems},
  volume       = {69},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Inventory integration with rational consumers. <em>OR</em>,
<em>69</em>(4), ii–iv. (<a
href="https://doi.org/10.1287/opre.2020.2084">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the value of inventory integration (or pooling) for a firm selling a seasonal good over two periods: in the first period the firm charges a high price, and in the second period the firm charges a low price to clear remaining inventory. Consumers are rational and decide when to visit the firm based on the price of the product and its anticipated availability. We show that integration—which combines inventory from distinct selling channels or geographic regions, for example, online and offline channels or two physical locations, into a single virtual stock from which any consumer demand may be fulfilled—possesses both operational and behavioral value in this setting. The operational value, which results from better matching of supply and demand, is always positive; however, the behavioral value, which results from the way integration influences inventory availability and hence consumer purchasing incentives, can be positive or negative. Negative behavioral value occurs when integration increases inventory availability in the second period and encourages more consumers to delay a purchase to obtain a lower price; this may significantly reduce, and even make negative, the total value of integration. We show that negative behavioral value of integration is most likely to occur when demand variability is high, the underlying markets are sufficiently correlated, and the salvage value-to-cost ratio is low. We also consider how three additional factors influence the value of integration, and find that the behavioral value of integration is more likely to be negative when the clearance price is endogenously determined as opposed to fixed ex ante or when the component market demands are more asymmetric; however, behavioral value is more likely to be positive when consumers incur visit costs. We conclude that rational consumer behavior can play a substantial role in determining the value of inventory integration.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.2084},
  journal      = {Operations Research},
  number       = {4},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {Inventory integration with rational consumers},
  volume       = {69},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Technical note—consumer choice and market expansion:
Modeling, optimization, and estimation. <em>OR</em>, <em>69</em>(4),
ii–iv. (<a href="https://doi.org/10.1287/opre.2020.2059">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Market size, measured by the number of people who are interested in products from the same category, may be largely influenced by assortment planning and pricing decisions. This effect is referred to as market expansion. In this paper, I incorporate the market expansion effects into consumer choice models and investigate various operations problems. In particular, I take the widely used multinomial logit model as a showcase to examine the market expansion effects on assortment planning and pricing, and propose an alternating-optimization expectation-maximization method that separates the estimation of consumer choice behavior and the market expansion effects to calibrate the new model. An empirical study on a real data set demonstrates the efficiency of the proposed estimation method and the importance of incorporating market expansion effects into consumer choice models. Failure to account for the market expansion effects may lead to substantial losses in demand estimation and operations management.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.2059},
  journal      = {Operations Research},
  number       = {4},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {Technical Note—Consumer choice and market expansion: Modeling, optimization, and estimation},
  volume       = {69},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On a deterministic approximation of inventory systems with
sequential service-level constraints. <em>OR</em>, <em>69</em>(4),
ii–iv. (<a href="https://doi.org/10.1287/opre.2020.2083">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Service-level constraint is often used as a metric to directly control the quality of service (e.g., managing the probability of stockout) in practice. Many inventory problems with service-level constraints are often difficult to solve and are typically approximated by deterministic formulations. This raises an important question regarding the quality of such an approach. To shed light on this question, in this paper, we consider two simplified yet fundamental inventory models (with backorder and lost sales) with independent demands, positive lead times, and sequential probabilistic service-level constraints, and study the performance of a natural order-up-to policy whose parameters can be calculated using the optimal solution of a deterministic approximation of the backorder inventory system. We show that it is asymptotically optimal for both the backorder and lost-sales systems in the setting with a high service-level requirement with a stronger performance bound for the backorder system. Our analysis for the lost-sales system involves construction of an alternative backorder system whose expected total cost can be related to that of the analogous lost-sales system. Overall, our result contributes to the growing body of inventory literature that suggests the near optimality of simple heuristic policies. Moreover, it also gives credence to the use of deterministic approximation for solving complex inventory problems in practice, at least for applications in which the targeted service level is sufficiently high.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.2083},
  journal      = {Operations Research},
  number       = {4},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {On a deterministic approximation of inventory systems with sequential service-level constraints},
  volume       = {69},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On the heavy-tail behavior of the distributionally robust
newsvendor. <em>OR</em>, <em>69</em>(4), ii–iv. (<a
href="https://doi.org/10.1287/opre.2020.2091">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since the seminal work of Scarf (A min-max solution of an inventory problem) in 1958 on the newsvendor problem with ambiguity in the demand distribution, there has been a growing interest in the study of the distributionally robust newsvendor problem. The model is criticized at times for being conservative because the worst-case distribution is discrete with a few support points. However, it is the order quantity prescribed by the model that is of practical relevance. Interestingly, the order quantity from Scarf’s model is optimal for a heavy-tailed distribution. In this paper, we generalize this observation by showing a heavy-tail optimality property of the distributionally robust order quantity for an ambiguity set where information on the first and the αth moment is known, for any real α &gt; 1. We show that the optimal order quantity for the distributionally robust newsvendor is also optimal for a regularly varying distribution with parameter α. In the high service level regime, when the original demand distribution is given by an exponential or a lognormal distribution and contaminated with a regularly varying distribution, the distributionally robust order quantity is shown to outperform the optimal order quantity of the original distribution, even with a small amount of contamination.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.2091},
  journal      = {Operations Research},
  number       = {4},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {On the heavy-tail behavior of the distributionally robust newsvendor},
  volume       = {69},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Time consistency of the mean-risk problem. <em>OR</em>,
<em>69</em>(4), ii–iv. (<a
href="https://doi.org/10.1287/opre.2020.2002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Choosing a portfolio of risky assets over time that maximizes the expected return at the same time as it minimizes portfolio risk is a classical problem in mathematical finance and is referred to as the dynamic Markowitz problem (when the risk is measured by variance) or, more generally, the dynamic mean-risk problem. In most of the literature, the mean-risk problem is scalarized, and it is well known that this scalarized problem does not satisfy the (scalar) Bellman’s principle. Thus, the classical dynamic programming methods are not applicable. For the purpose of this paper we focus on the discrete time setup, and we will use a time-consistent dynamic convex risk measure to evaluate the risk of a portfolio. We will show that, when we do not scalarize the problem but leave it in its original form as a vector optimization problem, the upper images, whose boundaries contain the efficient frontiers, recurse backward in time under very mild assumptions. Thus, the dynamic mean-risk problem does satisfy a Bellman’s principle, but a more general one, that seems more appropriate for a vector optimization problem: a set-valued Bellman’s principle. We will present conditions under which this recursion can be exploited directly to compute a solution in the spirit of dynamic programming. Numerical examples illustrate the proposed method. The obtained results open the door for a new branch in mathematics: dynamic multivariate programming.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.2002},
  journal      = {Operations Research},
  number       = {4},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {Time consistency of the mean-risk problem},
  volume       = {69},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Data-driven transit network design at scale. <em>OR</em>,
<em>69</em>(4), ii–iv. (<a
href="https://doi.org/10.1287/opre.2020.2057">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mass transit remains the most efficient way to service a densely packed commuter population. However, reliability issues and increasing competition in the transportation space have led to declining ridership across the United States, and transit agencies must also operate under tight budget constraints. Recent attempts at using bus network redesign to improve ridership have attracted attention from various transit authorities. However, the analysis seems to rely on ad hoc methods, for example, considering each line in isolation and using manual incremental adjustments with backtracking. We provide a holistic approach to designing a transit network using column generation. Our approach scales to hundreds of stops, and we demonstrate its usefulness on a case study with real data from Boston.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.2057},
  journal      = {Operations Research},
  number       = {4},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {Data-driven transit network design at scale},
  volume       = {69},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A large-scale optimization model for replicating portfolios
in the life insurance industry. <em>OR</em>, <em>69</em>(4), ii–iv. (<a
href="https://doi.org/10.1287/opre.2020.2098">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Replicating portfolios have emerged as an important tool in the life insurance industry, used for the valuation of companies’ liabilities. This paper describes the replicating portfolio (RP) model used to approximate life insurance liabilities in a large global insurance company. We describe the challenges presented by the latest solvency regimes in Europe and how the RP model enables this company to comply with the Swiss Solvency Test. The model minimizes the L 1 error between the discounted life insurance liability cash flows and the discounted RP cash flows over a multiperiod time horizon for a broad range of different future economic scenarios. A numerical application of the RP model to empirical data sets demonstrates that the model delivers RPs that match the liabilities and perform well for economic capital calculations.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.2098},
  journal      = {Operations Research},
  number       = {4},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {A large-scale optimization model for replicating portfolios in the life insurance industry},
  volume       = {69},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Large-scale bundle-size pricing: A theoretical analysis.
<em>OR</em>, <em>69</em>(4), ii–iv. (<a
href="https://doi.org/10.1287/opre.2020.2097">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bundle-size pricing (BSP) is a multidimensional selling mechanism where the firm prices the size of the bundle rather than the different possible combinations of bundles. In BSP, the firm offers the customer a menu of different sizes and prices. The customer then chooses the size that maximizes her surplus and customizes her bundle given her chosen size. Although BSP is commonly used across several industries, little is known about the optimal BSP policy in terms of sizes and prices, along with the theoretical properties of its profit. In this paper, we provide a simple and tractable theoretical framework to analyze the large-scale BSP problem where a multiproduct firm is selling a large number of products. The BSP problem is in general hard as it involves optimizing over order statistics; however, we show that for large numbers of products, the BSP problem transforms from a hard multidimensional problem to a simple multiunit pricing problem with concave and increasing utilities. Our framework allows us to identify the main source of inefficiency of BSP: the heterogeneity of marginal costs across products. For this reason, we propose two new BSP policies, “clustered BSP” and “assorted BSP,” which significantly reduce the inefficiency of regular BSP. We then utilize our framework to study richer models of BSP, such as when customers have budgets.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.2097},
  journal      = {Operations Research},
  number       = {4},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {Large-scale bundle-size pricing: A theoretical analysis},
  volume       = {69},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Improving the chilean college admissions system.
<em>OR</em>, <em>69</em>(4), ii–iv. (<a
href="https://doi.org/10.1287/opre.2021.2116">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we present the design and implementation of a new system to solve the Chilean college admissions problem. We develop an algorithm that obtains all applicant/program pairs that can be part of a stable allocation when preferences are not strict and when all students tied in the last seat of a program (if any) must be allocated. We use this algorithm to identify which mechanism was used in the past to perform the allocation, and we propose a new method to incorporate the affirmative action that is part of the system to correct the inefficiencies that arise from having double-assigned students. By unifying the regular admission with the affirmative action, we have improved the allocation of approximately 2.5\% of students assigned every year since 2016. From a theoretical standpoint, we show that some desired properties, such as strategy-proofness and monotonicity, cannot be guaranteed under flexible quotas.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2116},
  journal      = {Operations Research},
  number       = {4},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {Improving the chilean college admissions system},
  volume       = {69},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Mallows-smoothed distribution over rankings approach for
modeling choice. <em>OR</em>, <em>69</em>(4), ii–iv. (<a
href="https://doi.org/10.1287/opre.2020.2085">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Assortment optimization is an important problem arising in many applications, including retailing and online advertising. The goal in such problems is to determine a revenue-/profit-maximizing subset of products to offer from a large universe of products when customers exhibit stochastic substitution behavior. We consider a mixture of Mallows model for demand, which can be viewed as a “smoothed” generalization of sparse, rank-based choice models, designed to overcome some of their key limitations. In spite of these advantages, the Mallows distribution has an exponential support size and does not admit a closed-form expression for choice probabilities. We first conduct a case study using a publicly available data set involving real-world preferences on sushi types to show that Mallows-based smoothing significantly improves both the prediction accuracy and the decision quality on this data set. We then present an efficient procedure to compute the choice probabilities for any assortment under the mixture of Mallows model. Surprisingly, this finding allows us to formulate a compact mixed integer program (MIP) that leads to a practical approach for solving the assortment-optimization problem under a mixture of Mallows model. To complement this MIP formulation, we exploit additional structural properties of the underlying distribution to propose several polynomial-time approximation schemes (PTAS), taking the form of a quasi-PTAS in the most general setting, which can be strengthened to a PTAS or a fully PTAS under stronger assumptions. These are the first algorithmic approaches with provably near-optimal performance guarantees for the assortment-optimization problem under the Mallows or the mixture of Mallows model in such generality.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.2085},
  journal      = {Operations Research},
  number       = {4},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {Mallows-smoothed distribution over rankings approach for modeling choice},
  volume       = {69},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Technical note—dynamic data-driven estimation of
nonparametric choice models. <em>OR</em>, <em>69</em>(4), ii–iv. (<a
href="https://doi.org/10.1287/opre.2020.2077">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study nonparametric estimation of choice models, which was introduced to alleviate unreasonable assumptions in traditional parametric models and is prevalent in several application areas. Existing literature focuses only on the static observational setting where all of the observations are given up front and lacks algorithms that provide explicit convergence rate guarantees or an a priori analysis for the model accuracy versus sparsity trade-off on the actual estimated model returned. As opposed to this, we focus on estimating a nonparametric choice model from observational data in a dynamic setting, where observations are obtained over time. We show that choice model estimation can be cast as a convex-concave saddle point joint estimation and optimization problem, and we provide an online convex optimization-based primal-dual framework for deriving algorithms to solve this problem. By tailoring our framework carefully to the choice model estimation problem, we obtain tractable algorithms with provable convergence guarantees and explicit bounds on the sparsity of the estimated model. Our numerical experiments confirm the effectiveness of the algorithms derived from our framework.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.2077},
  journal      = {Operations Research},
  number       = {4},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {Technical Note—Dynamic data-driven estimation of nonparametric choice models},
  volume       = {69},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Transmission capacity allocation in zonal electricity
markets. <em>OR</em>, <em>69</em>(4), ii–iv. (<a
href="https://doi.org/10.1287/opre.2020.2082">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel framework for modeling zonal electricity markets, based on projecting the constraints of the nodal network onto the space of the zonal aggregation of the network. The framework avoids circular definitions and discretionary parameters, which are recurrent in the implementation and study of zonal markets. Using this framework, we model and analyze two zonal market designs currently present in Europe: flow-based market coupling (FBMC) and available-transfer-capacity market coupling (ATCMC). We develop cutting-plane algorithms for simulating FBMC and ATCMC while accounting for the robustness of imports/exports to single element failures, and we conduct numerical simulations of FBMC and ATCMC on a realistic instance of the Central Western European system under the equivalent of 100 years of operating conditions. We find that FBMC and ATCMC are unable to anticipate the congestion of branches interconnecting zones and branches within zones and that both zonal designs achieve similar overall cost efficiencies (0.01\% difference), whereas a nodal market design largely outperforms both of them (5.09\% better than FBMC). These findings raise the question of whether it is worth it for more European countries to switch from ATCMC to FBMC, instead of advancing directly toward a nodal market design.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.2082},
  journal      = {Operations Research},
  number       = {4},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {Transmission capacity allocation in zonal electricity markets},
  volume       = {69},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A fluid model for one-sided bipartite matching queues with
match-dependent rewards. <em>OR</em>, <em>69</em>(4), ii–iv. (<a
href="https://doi.org/10.1287/opre.2020.2015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a one-sided bipartite matching queueing system (OBMQ) with customers and resources of multiple types, where different customer-resource combinations can generate different rewards. Each resource is allocated on arrival to the customer with the highest score (or index), which is the sum of the customer’s waiting score and matching score, so we call it an M+W index. We assume that the waiting score is an increasing function of a customer’s waiting time and that the matching score is a function of both the customer’s and the resource’s types. Unmatched customers wait in the queue until being matched or will abandon the waitlist after a random duration. We study the fluid sample path in such a system, which provides an approximate to the system dynamics. We show that a fluid sample path can be computed over any finite horizon. We propose an efficient algorithm to check whether a steady state of the fluid sample path exists or not. When a steady state exists, the algorithm also computes one efficiently. We prove that there can be at most one steady state and that the fluid sample path will be attracted to the unique steady state under mild conditions. These results enable a policy designer to predict the behavior of an OBMQ when using an M+W index and to choose an indexing formula that optimizes a given set of performance metrics. We derive a closed-form index that optimizes the steady-state performance according to some well-known efficiency and fairness metrics.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.2015},
  journal      = {Operations Research},
  number       = {4},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {A fluid model for one-sided bipartite matching queues with match-dependent rewards},
  volume       = {69},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Optimal control of partially observable semi-markovian
failing systems: An analysis using a phase methodology. <em>OR</em>,
<em>69</em>(4), ii–iv. (<a
href="https://doi.org/10.1287/opre.2020.2086">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We formulate a maintenance control model as an optimal stopping problem under partial observations. The key challenge in our formulation is that the underlying state process is not restricted to be Markovian but rather is allowed to follow a semi-Markov process, which is more realistic in practice. Consequently, the stopping problem is not representable as a partially observable Markov decision process (POMDP) with finite state space, a commonly adopted modeling framework in the maintenance optimization literature; it constitutes a partially observable semi-Markov decision process (POSMDP), a problem class that in general is both computationally intractable and not amenable to structural analysis. In this paper, we develop a new analysis approach based on a phase methodology where the idea is to view the intractable POSMDP as the limiting problem of a sequence of tractable POMDPs. We show how this approach allows us to (i) characterize the structural form of the optimal policy and (ii) efficiently compute the optimal policy and associated optimal value via successive approximation. In particular, we show that the optimal control policy can be represented as a control limit policy that monitors the estimated conditional reliability at each decision epoch, and, by exploiting this structure, we develop an efficient computational approach to solve for the optimal control limit and corresponding optimal value. Numerical comparisons are provided that show substantial improvement over existing policies.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.2086},
  journal      = {Operations Research},
  number       = {4},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {Optimal control of partially observable semi-markovian failing systems: An analysis using a phase methodology},
  volume       = {69},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Asset selling under debt obligations. <em>OR</em>,
<em>69</em>(4), ii–iv. (<a
href="https://doi.org/10.1287/opre.2019.1961">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We extend the classical asset-selling problem to include debt repayment obligation, selling capacity constraint, and Markov price evolution. Specifically, we consider the problem of selling a divisible asset that is acquired through debt financing. The amount of asset that can be sold per period may be limited by physical constraints. The seller uses part of the sales revenue to repay the debt. If unable to pay off the debt, the seller must go bankrupt and liquidate the remaining asset. Our analysis reveals that in the presence of debt, the optimal asset-selling policy must take into account two opposing forces: an incentive to sell part of the asset early to secure debt payment and an incentive to delay selling the asset to capture revenue potential under limited liability. We analyze how these two forces, originating from debt financing, will distort the seller’s optimal policy.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1961},
  journal      = {Operations Research},
  number       = {4},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {Asset selling under debt obligations},
  volume       = {69},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Queueing dynamics and state space collapse in fragmented
limit order book markets. <em>OR</em>, <em>69</em>(4), ii–iv. (<a
href="https://doi.org/10.1287/opre.2020.1989">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In modern equity markets, participants have a choice of many exchanges at which to trade. Exchanges typically operate as electronic limit order books under a price-time priority rule and, in turn, can be modeled as multiclass first-in-first-out queueing systems. A market with multiple exchanges can be thought as a decentralized, parallel queueing system. Heterogeneous traders that submit limit orders select the exchange (i.e., the queue), in which to place their orders by trading off financial considerations against anticipated delays until their orders may fill. These limit orders can be thought of as jobs waiting for service. Simultaneously, traders that submit market orders select which exchange (i.e., queue) to direct their order. These market orders trigger instantaneous service completions of queued limit orders. In this way, the server is the aggregation of self-interested, atomistic traders submitting market orders. Taking into account the effect of investors’ order-routing decisions across exchanges, we find that the equilibrium of this decentralized market exhibits a state space collapse property whereby (a) the queue lengths at different exchanges are coupled in an intuitive manner; (b) the behavior of the market is captured through a one-dimensional process that can be viewed as a weighted aggregate queue length across all exchanges; and (c) the behavior at each exchange can be inferred via a mapping of the aggregated market depth process that takes into account the heterogeneous trader characteristics. The key driver of this coupling phenomenon is anticipated delay as opposed to the queue lengths themselves. Analyzing a trade and quote data set for a sample of stocks over a one-month period, we find empirical support for the predicted state space collapse. Separately, using the data before and after NASDAQ’s natural fee-change experiment from 2015, we again find agreement between the observed market behavior and the model’s predictions around the fee change.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.1989},
  journal      = {Operations Research},
  number       = {4},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {Queueing dynamics and state space collapse in fragmented limit order book markets},
  volume       = {69},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). How service quality variability hurts revenue when customers
learn: Implications for dynamic personalized pricing. <em>OR</em>,
<em>69</em>(3), ii–iv. (<a
href="https://doi.org/10.1287/opre.2020.2058">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We formalize an understudied mechanism through which quality variability hurts firm revenues, analyze when and why this mechanism is important, and generate new insight into how dynamic personalized pricing strategies can mitigate the negative effects of quality variability. To do so, we model a firm that sells a service repeatedly with variable but stationary quality. Customers update their quality beliefs based on their experiences (exponential smoothing), and their purchase probability in each period increases with their respective beliefs about mean quality (logit choice). For any fixed price, we show that quality variability reduces the firm’s revenue and leads to a downward bias in customer beliefs about quality. These effects arise even when customers are risk neutral and update their beliefs symmetrically after good and bad experiences. We then investigate whether the firm can improve revenues through dynamic personalized pricing. We find that a fixed perceived surplus pricing policy—charging a lower price when a customer believes the quality is lower to induce a constant purchase probability—is not only optimal but can also match the optimal revenue when quality is not variable. The revenue gain from implementing this pricing policy (compared with fixed pricing) is greatest when quality variability is large, customers react strongly to recent experiences, and/or mean service quality is high. Our numerical results further show that firms can achieve significant revenue gains through dynamic pricing even in information-poor or low-price-flexibility environments. Finally, we extend our model to consider social learning and competition between two firms.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.2058},
  journal      = {Operations Research},
  number       = {3},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {How service quality variability hurts revenue when customers learn: Implications for dynamic personalized pricing},
  volume       = {69},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The constrained reliable shortest path problem in stochastic
time-dependent networks. <em>OR</em>, <em>69</em>(3), ii–iv. (<a
href="https://doi.org/10.1287/opre.2020.2089">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The constrained reliable shortest path problem in stochastic time-dependent networks (CRSP-STD) extends the reliable, the time-dependent, and the constrained shortest path problem. In the CRSP-STD, shortest paths need to ensure on-time arrival with a given probability and additionally satisfy constraints on time-dependent weights. Examples of such time-dependent weights in transport networks include time-varying congestion charges or dynamic fees for shared vehicles. If weights decrease over time, waiting at nodes can be beneficial and is, therefore, allowed in our problem formulation. Travel times are modeled as time-dependent random variables and assumed to satisfy stochastic first in, first out (FIFO). We introduce a weak form of this condition to extend applicability to networks with scheduled connections, for example, public transport. To solve the CRSP-STD, we define essential paths, a subset of optimal paths. Essential paths have two main properties: first, they cover all nondominated combinations of worst-case weights that occur in the set of optimal paths, and second, their subpaths are nondominated, which can be used for pruning. Multiple properties of essential paths are exploited in our exact solution method, which extends multiobjective A* search. Runtime complexity is analyzed in theory and in numerical experiments, which show that key elements of our solution method effectively improve runtime performance.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.2089},
  journal      = {Operations Research},
  number       = {3},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {The constrained reliable shortest path problem in stochastic time-dependent networks},
  volume       = {69},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Soft OR and practice: The contribution of the founders of
operations research. <em>OR</em>, <em>69</em>(3), ii–iv. (<a
href="https://doi.org/10.1287/opre.2020.2051">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper explores the work of some 43 founders of operations research. In particular, it considers the links between soft operations research (OR) and these founders. Several of the founders were direct influencers of the soft OR proponents, whereas others related to the context, process, and content of soft OR. Coupled with the deductive and inductive reasoning approaches of soft OR, it is argued that soft OR is a legitimate branch of OR. The paper also focuses on the embeddedness of the founders, and the soft OR proponents, in practice and argues that, for academics, engagement with practice has been and will continue to be an important driver for the health and development of operations research.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.2051},
  journal      = {Operations Research},
  number       = {3},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {Soft OR and practice: The contribution of the founders of operations research},
  volume       = {69},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Branch-cut-and-price for the robust capacitated vehicle
routing problem with knapsack uncertainty. <em>OR</em>, <em>69</em>(3),
ii–iv. (<a href="https://doi.org/10.1287/opre.2020.2035">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We examine the robust counterpart of the classical capacitated vehicle routing problem (CVRP). We consider two types of uncertainty sets for the customer demands: the classical budget polytope and a partitioned budget polytope. We show that using the set-partitioning formulation it is possible to reformulate our problem as a deterministic heterogeneous vehicle routing problem. Thus, many state-of-the-art techniques for exactly solving deterministic VRPs can be applied to the robust counterpart, and a modern branch-cut-and-price algorithm can be adapted to our setting by keeping the number of pricing subproblems strictly polynomial. More importantly, we introduce new techniques to significantly improve the efficiency of the algorithm. We present analytical conditions under which a pricing subproblem is infeasible. This result is general and can be applied to other combinatorial optimization problems with knapsack uncertainty. We also introduce robust capacity cuts that are provably stronger than the ones known in the literature. Finally, a fast-iterated local search algorithm is proposed to obtain heuristic solutions for the problem. Using our branch-cut-and-price algorithm incorporating existing and new techniques, we solve to optimality all but one of the open instances from the literature.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.2035},
  journal      = {Operations Research},
  number       = {3},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {Branch-cut-and-price for the robust capacitated vehicle routing problem with knapsack uncertainty},
  volume       = {69},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Risk management for sustainable sovereign debt financing.
<em>OR</em>, <em>69</em>(3), ii–iv. (<a
href="https://doi.org/10.1287/opre.2020.2055">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We model sovereign debt sustainability with optimal financing decisions under macroeconomic, financial, and fiscal uncertainty, with endogenous risk and term premia. Using a coherent risk measure we trade off debt stock and flow risks subject to sustainability constraints. We optimize static and dynamic financing strategies and demonstrate economically significant savings from optimal financing compared with simple rules and consols, and find that optimizing the trade-offs can be critical for sustainability. The model quantifies minimum refinancing risk and maximum rate of debt reduction that a sovereign can achieve given its economic fundamentals, and an extension identifies optimal timing of flow adjustments that allow the sovereign to go beyond these limits. We put the model to the data on a eurozone crisis country, a low-debt country (Netherlands), and a high-debt country (Italy) and document the significance of the stock-flow trade-off for debt sustainability, identify potential improvements of Dutch Treasury practices, and identify unsustainability risks in the 2019 Italian budget. The model informs diverse policy decisions on sustainable public finance and is an essential building block of the European Stability Mechanism methodological framework to assess debt sustainability and repayment capacity of member states, especially in the context of financial assistance.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.2055},
  journal      = {Operations Research},
  number       = {3},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {Risk management for sustainable sovereign debt financing},
  volume       = {69},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). OR practice–data analytics for optimal detection of
metastatic prostate cancer. <em>OR</em>, <em>69</em>(3), ii–iv. (<a
href="https://doi.org/10.1287/opre.2020.2020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We used data-analytics approaches to develop, calibrate, and validate predictive models, to help urologists in a large statewide collaborative make prostate cancer staging decisions on the basis of individual patient risk factors. The models were validated using statistical methods based on bootstrapping and evaluation on out-of-sample data. These models were used to design guidelines that optimally weigh the benefits and harms of radiological imaging for the detection of metastatic prostate cancer. The Michigan Urological Surgery Improvement Collaborative, a statewide medical collaborative, implemented these guidelines, which were predicted to reduce unnecessary imaging by more than 40\% and limit the percentage of patients with missed metastatic disease to be less than 1\%. The effects of the guidelines were measured after implementation to confirm their impact on reducing unnecessary imaging across the state of Michigan.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.2020},
  journal      = {Operations Research},
  number       = {3},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {OR Practice–Data analytics for optimal detection of metastatic prostate cancer},
  volume       = {69},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Procurement mechanisms for assortments of differentiated
products. <em>OR</em>, <em>69</em>(3), ii–iv. (<a
href="https://doi.org/10.1287/opre.2020.2053">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem faced by a procurement agency that runs a mechanism for constructing an assortment of differentiated products with posted prices, from which heterogeneous consumers buy their most preferred alternative. Procurement mechanisms used by large organizations, including framework agreements (FAs), which are widely used in the public sector, often take this form. When choosing the assortment, the procurement agency must optimize the tradeoff between offering a richer menu of products for consumers and offering less variety, hoping to engage the suppliers in more aggressive price competition. We formulate the problem faced by the procurement agency as a mechanism design problem, and we progressively incorporate more complex and often more realistic implementation constraints, including that the allocations should be decentralized (that is, consumers choose what to buy) and that payments must be implemented through linear pricing (in particular, no up-front payments are allowed). We characterize the optimal buying mechanisms that highlight the importance of restricting the entry of close-substitute products to the assortment as a way to increase price competition without much damage to variety. Motivated by the implementation of the Chilean FAs, which are being used to acquire around US$3 billion in goods and services per year, we leverage our characterization of the optimal mechanism to study the design of first-price-auction-type mechanisms that are commonly used in public settings. Our results shed light on simple ways to improve their performance.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.2053},
  journal      = {Operations Research},
  number       = {3},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {Procurement mechanisms for assortments of differentiated products},
  volume       = {69},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Online allocation and pricing: Constant regret via bellman
inequalities. <em>OR</em>, <em>69</em>(3), ii–iv. (<a
href="https://doi.org/10.1287/opre.2020.2061">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a framework for designing simple and efficient policies for a family of online allocation and pricing problems that includes online packing, budget-constrained probing, dynamic pricing, and online contextual bandits with knapsacks. In each case, we evaluate the performance of our policies in terms of their regret (i.e., additive gap) relative to an offline controller that is endowed with more information than the online controller. Our framework is based on Bellman inequalities, which decompose the loss of an algorithm into two distinct sources of error: (1) arising from computational tractability issues, and (2) arising from estimation/prediction of random trajectories. Balancing these errors guides the choice of benchmarks, and leads to policies that are both tractable and have strong performance guarantees. In particular, in all our examples, we demonstrate constant-regret policies that only require resolving a linear program in each period, followed by a simple greedy action-selection rule; thus, our policies are practical as well as provably near optimal.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.2061},
  journal      = {Operations Research},
  number       = {3},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {Online allocation and pricing: Constant regret via bellman inequalities},
  volume       = {69},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Heteroscedastic exponomial choice. <em>OR</em>,
<em>69</em>(3), ii–iv. (<a
href="https://doi.org/10.1287/opre.2020.2074">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate analytical and empirical properties of the Heteroscedastic Exponomial Choice (HEC) model to lay the groundwork for its use in theoretical and empirical studies that build demand models on a discrete choice foundation. The HEC model generalizes the Exponomial Choice (EC) model by including choice-specific variances for the random components of utility (the error terms). We show that the HEC model inherits some of the properties found in the EC model: closed-form choice probabilities, demand elasticities, and consumer surplus; optimal monopoly prices that are increasing with ideal utilities in a hockey-stick pattern; and unique equilibrium oligopoly prices that are easily computed using a series of single-variable equations. However, the HEC model has several key differences with the EC model, which show that variances matter: the choice probabilities (market shares) as well as equilibrium oligopoly prices are not necessarily increasing with ideal utilities; and the new model can include choices with deterministic utility or choices with zero probability. However, because the HEC model uses more parameters, it is harder to estimate. To justify its use, we apply HEC to grocery purchase data for 30 product categories and find that it significantly improves model fit and generally improves out-of-sample prediction compared with EC. We go on to investigate the more nuanced impact of the variance parameters on oligopoly pricing. We find that the individual and collective incentives differ in equilibrium: firms individually want lower error variability for their own product but collectively prefer higher error variability for all products—including their own—because higher error variability softens the price competition.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.2074},
  journal      = {Operations Research},
  number       = {3},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {Heteroscedastic exponomial choice},
  volume       = {69},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Budget-management strategies in repeated auctions.
<em>OR</em>, <em>69</em>(3), ii–iv. (<a
href="https://doi.org/10.1287/opre.2020.2073">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In online advertising, advertisers purchase ad placements by participating in a long sequence of repeated auctions. One of the most important features that advertising platforms often provide and advertisers often use is budget management, which allows advertisers to control their cumulative expenditures. Advertisers typically declare the maximum daily amount they are willing to pay, and the platform adjusts allocations and payments to guarantee that cumulative expenditures do not exceed budgets. There are multiple ways to achieve this goal, and each one, when applied to all budget-constrained advertisers simultaneously, drives the system toward a different equilibrium. Our goal is to compare the “system equilibria” of a range of budget-management strategies. In particular, we consider six different budget-management strategies, including probabilistic throttling, thresholding, bid shading, reserve pricing, and two versions of multiplicative boosting. We show that these methods admit a system equilibrium, study their incentive properties, prove dominance relations among them in a simplified setting, and confirm our theoretical findings using real ad auction data from a sponsored search. Our study sheds light on the impact of budget-management strategies on the trade-off between the seller’s profit and buyers’ utility and may be of practical relevance for advertising platforms.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.2073},
  journal      = {Operations Research},
  number       = {3},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {Budget-management strategies in repeated auctions},
  volume       = {69},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Points gained in football: Using markov process-based value
functions to assess team performance. <em>OR</em>, <em>69</em>(3),
ii–iv. (<a href="https://doi.org/10.1287/opre.2020.2034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To develop a novel approach for performance assessment, this paper considers the problem of computing value functions in professional American football. We provide a theoretical justification for using a dynamic programming approach to estimating value functions in sports by formulating the problem as a Markov chain for two asymmetric teams. We show that the Bellman equation has a unique solution equal to the bias of the underlying infinite horizon Markov reward process. This result provides, for the first time in the sports analytics literature, a precise interpretation of the value function as the expected number of points gained or lost over and above the steady state points gained or lost. We derive a martingale representation for the value function that provides justification, in addition to the analysis of our empirical transition probabilities, for using an infinite horizon approximation of a finite horizon game. Using more than 160,000 plays from the 2013–2016 National Football League seasons, we derive an empirical value function that forms our points gained performance assessment metric, which quantifies the value created or destroyed on any play relative to expected performance. We show how this metric provides new insight into factors that distinguish performance. For example, passing plays generate the most points gained, whereas running plays tend to generate negative value. Short passes account for the majority of the top teams’ success and the worst teams’ poor performance. Other insights include how teams differ by down, quarter, and field position. The paper concludes with a case study of the 2019 Super Bowl and suggests how the key concepts might apply outside of sports.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.2034},
  journal      = {Operations Research},
  number       = {3},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {Points gained in football: Using markov process-based value functions to assess team performance},
  volume       = {69},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Online resource allocation under partially predictable
demand. <em>OR</em>, <em>69</em>(3), ii–iv. (<a
href="https://doi.org/10.1287/opre.2020.2017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For online resource allocation problems, we propose a new demand arrival model where the sequence of arrivals contains both an adversarial component and a stochastic one. Our model requires no demand forecasting; however, because of the presence of the stochastic component, we can partially predict future demand as the sequence of arrivals unfolds. Under the proposed model, we study the problem of the online allocation of a single resource to two types of customers and design online algorithms that outperform existing ones. Our algorithms are adjustable to the relative size of the stochastic component; our analysis reveals that as the portion of the stochastic component grows, the loss due to making online decisions decreases. This highlights the value of (even partial) predictability in online resource allocation. We impose no conditions on how the resource capacity scales with the maximum number of customers. However, we show that using an adaptive algorithm—which makes online decisions based on observed data—is particularly beneficial when capacity scales linearly with the number of customers. Our work serves as a first step in bridging the long-standing gap between the two well-studied approaches to the design and analysis of online algorithms based on (1) adversarial models and (2) stochastic ones. Using novel algorithm design, we demonstrate that even if the arrival sequence contains an adversarial component, we can take advantage of the limited information that the data reveal to improve allocation decisions. We also study the classical secretary problem under our proposed arrival model, and we show that randomizing over multiple stopping rules may increase the probability of success.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.2017},
  journal      = {Operations Research},
  number       = {3},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {Online resource allocation under partially predictable demand},
  volume       = {69},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Optimal contract for machine repair and maintenance.
<em>OR</em>, <em>69</em>(3), ii–iv. (<a
href="https://doi.org/10.1287/opre.2020.2018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A principal hires an agent to repair a machine when it is down and maintain it when it is up and earns a revenue flow when the machine is up. Both the up- and downtimes follow exponential distributions. If the agent exerts effort, the downtime is shortened, and uptime is prolonged. Effort, however, is costly to the agent and unobservable to the principal. We study optimal dynamic contracts that always induce the agent to exert effort while maximizing the principal’s profits. We formulate the contract design problem as a stochastic optimal control model with incentive constraints in continuous time over an infinite horizon. Although we consider the contract space that allows payments and potential contract termination time to take general forms, the optimal contracts demonstrate simple and intuitive structures, making them easy to describe and implement in practice.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.2018},
  journal      = {Operations Research},
  number       = {3},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {Optimal contract for machine repair and maintenance},
  volume       = {69},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A finite time analysis of temporal difference learning with
linear function approximation. <em>OR</em>, <em>69</em>(3), ii–iv. (<a
href="https://doi.org/10.1287/opre.2020.2024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal difference learning (TD) is a simple iterative algorithm used to estimate the value function corresponding to a given policy in a Markov decision process. Although TD is one of the most widely used algorithms in reinforcement learning, its theoretical analysis has proved challenging and few guarantees on its statistical efficiency are available. In this work, we provide a simple and explicit finite time analysis of temporal difference learning with linear function approximation. Except for a few key insights, our analysis mirrors standard techniques for analyzing stochastic gradient descent algorithms and therefore inherits the simplicity and elegance of that literature. Final sections of the paper show how all of our main results extend to the study of TD learning with eligibility traces, known as TD( λ ), and to Q-learning applied in high-dimensional optimal stopping problems.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.2024},
  journal      = {Operations Research},
  number       = {3},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {A finite time analysis of temporal difference learning with linear function approximation},
  volume       = {69},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Nonparametric pricing analytics with customer covariates.
<em>OR</em>, <em>69</em>(3), ii–iv. (<a
href="https://doi.org/10.1287/opre.2020.2016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Personalized pricing analytics is becoming an essential tool in retailing. Upon observing the personalized information of each arriving customer, the firm needs to set a price accordingly based on the covariates, such as income, education background, and past purchasing history, to extract more revenue. For new entrants of the business, the lack of historical data may severely limit the power and profitability of personalized pricing. We propose a nonparametric pricing policy to simultaneously learn the preference of customers based on the covariates and maximize the expected revenue over a finite horizon. The policy does not depend on any prior assumptions on how the personalized information affects consumers’ preferences (such as linear models). It adaptively splits the covariate space into smaller bins (hyper-rectangles) and clusters customers based on their covariates and preferences, offering similar prices for customers who belong to the same cluster trading off granularity and accuracy. We show that the algorithm achieves a regret of order O ( log ( T ) 2 T ( 2 + d ) / ( 4 + d ) ) , where T is the length of the horizon and d is the dimension of the covariate. It improves the current regret in the literature ( Slivkins 2014 ) under mild technical conditions in the pricing context (smoothness and local concavity). We also prove that no policy can achieve a regret less than O ( T ( 2 + d ) / ( 4 + d ) ) for a particular instance and, thus, demonstrate the near-optimality of the proposed policy.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.2016},
  journal      = {Operations Research},
  number       = {3},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {Nonparametric pricing analytics with customer covariates},
  volume       = {69},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sample out-of-sample inference based on wasserstein
distance. <em>OR</em>, <em>69</em>(3), ii–iv. (<a
href="https://doi.org/10.1287/opre.2020.2028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel inference approach that we call sample out-of-sample inference. The approach can be used widely, ranging from semisupervised learning to stress testing, and it is fundamental in the application of data-driven distributionally robust optimization. Our method enables measuring the impact of plausible out-of-sample scenarios in a given performance measure of interest, such as a financial loss. The methodology is inspired by empirical likelihood (EL), but we optimize the empirical Wasserstein distance (instead of the empirical likelihood) induced by observations. From a methodological standpoint, our analysis of the asymptotic behavior of the induced Wasserstein-distance profile function shows dramatic qualitative differences relative to EL. For instance, in contrast to EL, which typically yields chi-squared weak convergence limits, our asymptotic distributions are often not chi-squared. Also, the rates of convergence that we obtain have some dependence on the dimension in a nontrivial way but remain controlled as the dimension increases.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.2028},
  journal      = {Operations Research},
  number       = {3},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {Sample out-of-sample inference based on wasserstein distance},
  volume       = {69},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Credit risk: Simple closed-form approximate maximum
likelihood estimator. <em>OR</em>, <em>69</em>(2), ii–iv. (<a
href="https://doi.org/10.1287/opre.2020.2029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider discrete default intensity-based and logit-type reduced-form models for conditional default probabilities for corporate loans where we develop simple closed-form approximations to the maximum likelihood estimator (MLE) when the underlying covariates follow a stationary Gaussian process. In a practical asymptotic regime where the default probabilities are small, say less than 3\% annually, and the number of firms and the time period of data available are reasonably large, we rigorously show that the proposed estimator behaves similarly or slightly worse than the MLE when the underlying model is correctly specified. These approximations and conclusions are extended to the regularized MLE. For a more realistic case of model misspecification, both the estimators are seen to be equally good or equally bad. Further, in the presence of zero mean additive corruption in the data, the proposed estimator is somewhat better than the MLE. These conclusions are validated on empirical and simulated data.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.2029},
  journal      = {Operations Research},
  number       = {2},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {Credit risk: Simple closed-form approximate maximum likelihood estimator},
  volume       = {69},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Asymmetric multidepot vehicle routing problems: Valid
inequalities and a branch-and-cut algorithm. <em>OR</em>,
<em>69</em>(2), ii–iv. (<a
href="https://doi.org/10.1287/opre.2020.2033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a generic branch-and-cut framework for solving routing problems with multiple depots and asymmetric cost structures, which determines a set of cost-minimizing (capacitated) vehicle tours that fulfill a set of customer demands. The backbone of the framework is a series of valid inequalities with corresponding separation algorithms that exploit the asymmetric cost structure in directed graphs. We derive three new classes of so-called D k inequalities that eliminate subtours, enforce tours to be linked to a single depot, and impose bounds on the number of customers in a tour. In addition, other well-known valid inequalities for solving vehicle routing problems are generalized and adapted to be valid for routing problems with multiple depots and asymmetric cost structures. The framework is tested on four specific problem variants, for which we develop a new set of large-scale benchmark instances. The results show that the new inequalities can reduce root-node optimality gaps by up to 67.2\% compared with existing approaches. Moreover, the complete framework is very effective and can solve instances of considerably larger size than reported in the literature. For instance, it solves asymmetric multidepot traveling-salesman-problem instances containing up to 400 customers and 50 depots, whereas to date only solutions of instances containing up to 300 customer nodes and 60 depots have been reported.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.2033},
  journal      = {Operations Research},
  number       = {2},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {Asymmetric multidepot vehicle routing problems: Valid inequalities and a branch-and-cut algorithm},
  volume       = {69},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Selfishness need not be bad. <em>OR</em>, <em>69</em>(2),
ii–iv. (<a href="https://doi.org/10.1287/opre.2020.2036">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate the price of anarchy (PoA) in nonatomic congestion games when the total demand T gets very large. First results in this direction have recently been obtained by Colini-Baldeschi et al. (2016, 2017, 2020) for routing games and show that the PoA converges to one when the growth of the total demand T satisfies certain regularity conditions. We extend their results by developing a new framework for the limit analysis of the PoA that offers strong techniques such as the limit of games and applies to arbitrary growth patterns of T . We show that the PoA converges to one in the limit game regardless of the type of growth of T for a large class of cost functions that contains all polynomials and all regularly varying functions. For routing games with Bureau of Public Road (BPR) cost functions, we show in addition that socially optimal strategy profiles converge to equilibria in the limit game and that the PoA converges to one at a power law with exponent β , where β &gt; 0 is the degree of the BPR functions. However, the precise convergence rate depends crucially on the the growth of T , which shows that a conjecture proposed by O’Hare et al. (2016) need not hold.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.2036},
  journal      = {Operations Research},
  number       = {2},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {Selfishness need not be bad},
  volume       = {69},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Branch-price-and-cut algorithms for the vehicle routing
problem with stochastic and correlated travel times. <em>OR</em>,
<em>69</em>(2), ii–iv. (<a
href="https://doi.org/10.1287/opre.2020.2037">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we consider a version of the capacitated vehicle routing problem (CVRP) where travel times are assumed to be uncertain and statistically correlated (CVRP-SCT). In particular, we suppose that travel times follow a multivariate probability distribution whose first and second moments are known. The main purpose of the CVRP-CST is to plan vehicle routes whose travel times are reliable, in the sense that observed travel times are not excessively dispersed with respect to their expected value. To this scope we adopt a mean-variance approach, where routes with high travel time variability are penalized. This leads to a parametric binary quadratic program for which we propose two alternative set partitioning reformulations and show how to exploit the structure of the correlation matrix when there is correlation only between adjacent links. For each model, we develop an exact branch-price-and-cut algorithm, where the quadratic component is dealt with either in the column generation master problem or in its subproblem. We tested our algorithms on a rich collection of instances derived from well-known data sets. Computational results show that our algorithms can efficiently solve problem instances with up to 75 customers. Furthermore, the obtained solutions significantly reduce the time variability when compared with standard CVRP solutions.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.2037},
  journal      = {Operations Research},
  number       = {2},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {Branch-price-and-cut algorithms for the vehicle routing problem with stochastic and correlated travel times},
  volume       = {69},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deferred acceptance with compensation chains. <em>OR</em>,
<em>69</em>(2), ii–iv. (<a
href="https://doi.org/10.1287/opre.2020.2042">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {I introduce a class of algorithms called deferred acceptance with compensation chains (DACC). DACC algorithms generalize the Gale–Shapley algorithm by allowing both sides of the market to make offers. The main result is a characterization of the set of stable matchings: a matching is stable if and only if it is the outcome of a DACC algorithm. The proof of convergence of DACC algorithms uses a novel technique based on a construction of a potential function.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.2042},
  journal      = {Operations Research},
  number       = {2},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {Deferred acceptance with compensation chains},
  volume       = {69},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust data-driven vehicle routing with time windows.
<em>OR</em>, <em>69</em>(2), ii–iv. (<a
href="https://doi.org/10.1287/opre.2020.2043">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimal routing solutions based on deterministic models usually fail to deliver promised on-time services in an uncertain real world, which can lead to the loss of customers and revenue. We study a vehicle routing problem with time windows ( vrptw ) toward the end of mitigating the risk of late customer arrivals as much as possible when travel times are based on empirical distributions. To prevent overfitting, we propose a distributionally robust optimization model that uses a Wasserstein distance–based ambiguity set to characterize ambiguous distributions that are close to the empirical distribution. Our model minimizes the decision criterion regarding delays, termed the service fulfillment risk index ( sri ), while limiting budgeted travel costs. The sri accounts for both the late arrival probability and its magnitude, captures the risk and ambiguity in travel times, and can be evaluated efficiently in closed form. Under the Wasserstein distance–based ambiguity, the closed-form solution reduces the vrptw of interest to the problem under empirical travel times where all deadlines are advanced by some Wasserstein distance–related durations. To solve the problem, we develop an exact branch-and-cut approach and a variable neighborhood search meta-heuristic algorithm and explore their speedup strategies. The effectiveness of these algorithms is established by extensive computational studies. In particular, our solution greatly improves on-time arrival performance with only modest increases in expenditures compared with the deterministic solution. Finally, our sri also performs better during out-of-sample simulations than do the canonical decision criteria of lateness probability and expected lateness duration.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.2043},
  journal      = {Operations Research},
  number       = {2},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {Robust data-driven vehicle routing with time windows},
  volume       = {69},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multilevel approaches for the critical node problem.
<em>OR</em>, <em>69</em>(2), ii–iv. (<a
href="https://doi.org/10.1287/opre.2020.2014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, a lot of effort has been dedicated to develop strategies to defend networks against possible cascade failures or malicious viral attacks. On the one hand, network safety is investigated from a preventive perspective. On the other hand, blocking models have been proposed for scenarios in which the attack has already taken place causing a harmful spreading throughout the network. In this work, we combine these two perspectives. More precisely, following the framework defender–attacker–defender, we consider a model of prevention, attack, and damage containment using a three-stage, zero-sum game. The defender is not only able to adopt preventive strategies, but also to defend the network after an attack takes place. Assuming that the attacker acts optimally, we compute a defensive strategy for the first stage that minimizes the total damage to the network in the end of the third stage. Our contribution consists of considering this problem as a trilevel mixed-integer program and designing an exact algorithm for it based on tools developed for multilevel programming.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.2014},
  journal      = {Operations Research},
  number       = {2},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {Multilevel approaches for the critical node problem},
  volume       = {69},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Incentive-compatible learning of reserve prices for repeated
auctions. <em>OR</em>, <em>69</em>(2), ii–iv. (<a
href="https://doi.org/10.1287/opre.2020.2007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large fractions of online advertisements are sold via repeated second-price auctions. In these auctions, the reserve price is the main tool for the auctioneer to boost revenues. In this work, we investigate the following question: how can the auctioneer optimize reserve prices by learning from the previous bids while accounting for the long-term incentives and strategic behavior of the bidders? To this end, we consider a seller who repeatedly sells ex ante identical items via a second-price auction. Buyers’ valuations for each item are drawn independently and identically from a distribution F that is unknown to the seller. We find that if the seller attempts to dynamically update a common reserve price based on the bidding history, this creates an incentive for buyers to shade their bids, which can hurt revenue. When there is more than one buyer, incentive compatibility can be restored by using personalized reserve prices, where the personal reserve price for each buyer is set using the historical bids of other buyers. Such a mechanism asymptotically achieves the expected revenue obtained under the static Myerson optimal auction for F . Further, if valuation distributions differ across bidders, the loss relative to the Myerson benchmark is only quadratic in the size of such differences. We extend our results to a contextual setting where the valuations of the buyers depend on observed features of the items. When up-front fees are permitted, we show how the seller can determine such payments based on the bids of others to obtain an approximately incentive-compatible mechanism that extracts nearly all the surplus.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.2007},
  journal      = {Operations Research},
  number       = {2},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {Incentive-compatible learning of reserve prices for repeated auctions},
  volume       = {69},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Demand shaping through bundling and product configuration: A
dynamic multiproduct inventory-pricing model. <em>OR</em>,
<em>69</em>(2), ii–iv. (<a
href="https://doi.org/10.1287/opre.2020.2062">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In today’s digital age, with the aid of the internet and data mining, many firms use vertically differentiated product bundling to influence demand to match up with inventory status, especially in industries with short product life cycles. Despite this practice, there is little understanding on how exactly the inventory dynamics impact the bundling strategy and, in turn, how the bundling strategy affects the firm’s inventory decisions. To fill this gap, we present a dynamic model to analyze the optimal joint replenishment, pricing, and bundling decisions over time. A key enabler of our analysis is a novel demand model that transfers the discrete bundling decision and the corresponding pricing decision into a continuous market share decision. We show that the optimal policy is dictated by a no-order set in each period. For items in this set, we do not place replenishment orders, because these items are overstocked. The rest of the policy parameters—the order-up-to-levels for the items that we do order, the bundling and pricing decisions, and the bundle assembly quantity—all depend on the overstock levels. We also characterize how the optimal bundling decision depends on item complementarity, cost structure, inventory levels, demand uncertainty, and supply responsiveness.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.2062},
  journal      = {Operations Research},
  number       = {2},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {Demand shaping through bundling and product configuration: A dynamic multiproduct inventory-pricing model},
  volume       = {69},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Duopoly competition with network effects in discrete choice
models. <em>OR</em>, <em>69</em>(2), ii–iv. (<a
href="https://doi.org/10.1287/opre.2020.2079">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider two firms selling products to a market of network-connected customers. Each firm is selling one product, and the two products are substitutable. The customers make purchases based on the multinomial logit model, and the firms compete for their purchasing probabilities. We characterize possible Nash equilibria for homogeneous network interactions and identical firms: When the network effects are weak, there is a symmetric equilibrium that the two firms evenly split the market; when the network effects are strong, there exist two asymmetric equilibria additionally, in which one firm dominates the market; interestingly, when the product quality is low and the network effects are neither too weak nor too strong, the resulting market equilibrium is never symmetric, although the firms are ex ante symmetric. We extend these results along multiple directions. First, when the products have heterogeneous qualities, the firm selling inferior product can still retain market dominance in equilibrium due to the strong network effects. Second, when the network effects are heterogeneous, customers with higher social influences or larger price sensitivities are more likely to purchase either product in the symmetric equilibrium. Third, when the network consists of two communities, market segmentation may arise. Fourth, we extend to the dynamic game when the network effects build up over time to explain the first-mover advantage.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.2079},
  journal      = {Operations Research},
  number       = {2},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {Duopoly competition with network effects in discrete choice models},
  volume       = {69},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Technical note—joint learning and optimization of
multi-product pricing with finite resource capacity and unknown demand
parameters. <em>OR</em>, <em>69</em>(2), ii–iv. (<a
href="https://doi.org/10.1287/opre.2020.2078">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider joint learning and pricing in network revenue management (NRM) with multiple products, multiple resources with finite capacity, parametric demand model, and a continuum set of feasible price vectors. We study the setting with a general parametric demand model and the setting with a well-separated demand model. For the general parametric demand model, we propose a heuristic that is rate-optimal (i.e., its regret bound exactly matches the known theoretical lower bound under any feasible pricing control for our setting). This heuristic is the first rate-optimal heuristic for an NRM with a general parametric demand model and a continuum of feasible price vectors. For the well-separated demand model, we propose a heuristic that is close to rate-optimal (up to a multiplicative logarithmic term). Our second heuristic is the first in the literature that deals with the setting of an NRM with a well-separated parametric demand model and a continuum set of feasible price vectors.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.2078},
  journal      = {Operations Research},
  number       = {2},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {Technical Note—Joint learning and optimization of multi-product pricing with finite resource capacity and unknown demand parameters},
  volume       = {69},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Patient-type bayes-adaptive treatment plans. <em>OR</em>,
<em>69</em>(2), ii–iv. (<a
href="https://doi.org/10.1287/opre.2020.2011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Patient heterogeneity in disease progression is prevalent in many settings. Treatment decisions that explicitly consider this heterogeneity can lower the cost of care and improve outcomes by providing the right care for the right patient at the right time. In this paper, we analyze the problem of designing ongoing treatment plans for a population with heterogeneity in disease progression and response to medical interventions. We create a model that learns the patient type by monitoring the patient health over time and updates a patient’s treatment plan according to the gathered information. We formulate the problem as a multivariate state-space partially observable Markov decision process (POMDP) and provide structural properties of the value function, as well as the optimal policy. We extend this modeling framework to a general class of treatment initiation problems where there is a stochastic lead time before a treatment becomes available or effective. As a case study, we develop a data-driven decision-analytic model to study the optimal timing of vascular access surgery for patients with progressive chronic kidney disease, and we establish policies that consider a patient’s rate of disease progression in addition to the kidney health state. To circumvent the curse of dimensionality of the POMDP, we develop several approximate policies, as well as simpler heuristics, and evaluate them against a high-quality lower bound. Through a numerical study and several sensitivity analyses, we establish the high quality and robustness of an approximate policy that we develop. We provide further policy insights that sharpen existing guidelines for the case-study problem.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.2011},
  journal      = {Operations Research},
  number       = {2},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {Patient-type bayes-adaptive treatment plans},
  volume       = {69},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Measuring beliefs under ambiguity. <em>OR</em>,
<em>69</em>(2), ii–iv. (<a
href="https://doi.org/10.1287/opre.2020.1980">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a simple method to measure the beliefs of a decision-maker with nonneutral ambiguity attitudes. Our method requires three simple choices, is incentive compatible, and allows for risk aversion and deviations from expected utility, including probability weighting and ambiguity aversion. An experiment using two natural sources of uncertainty (the temperature in Rotterdam and in New York City) shows that the model’s estimated beliefs are well calibrated, sensitive to the source of uncertainty, and similar to beliefs estimated by more sophisticated but time-consuming methods.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.1980},
  journal      = {Operations Research},
  number       = {2},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {Measuring beliefs under ambiguity},
  volume       = {69},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fast algorithms for rank-1 bimatrix games. <em>OR</em>,
<em>69</em>(2), ii–iv. (<a
href="https://doi.org/10.1287/opre.2020.1981">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rank of a bimatrix game is the matrix rank of the sum of the two payoff matrices. This paper comprehensively analyzes games of rank one and shows the following: (1) For a game of rank r , the set of its Nash equilibria is the intersection of a generically one-dimensional set of equilibria of parameterized games of rank r − 1 with a hyperplane. (2) One equilibrium of a rank-1 game can be found in polynomial time. (3) All equilibria of a rank-1 game can be found by following a piecewise linear path. In contrast, such a path-following method finds only one equilibrium of a bimatrix game. (4) The number of equilibria of a rank-1 game may be exponential. (5) There is a homeomorphism between the space of bimatrix games and their equilibrium correspondence that preserves rank. It is a variation of the homeomorphism used for the concept of strategic stability of an equilibrium component.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.1981},
  journal      = {Operations Research},
  number       = {2},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {Fast algorithms for rank-1 bimatrix games},
  volume       = {69},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bayesian decision making in groups is hard. <em>OR</em>,
<em>69</em>(2), ii–iv. (<a
href="https://doi.org/10.1287/opre.2020.2000">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the computations that Bayesian agents undertake when exchanging opinions over a network. The agents act repeatedly on their private information and take myopic actions that maximize their expected utility according to a fully rational posterior belief. We show that such computations are NP-hard for two natural utility functions: one with binary actions and another where agents reveal their posterior beliefs. In fact, we show that distinguishing between posteriors that are concentrated on different states of the world is NP-hard. Therefore, even approximating the Bayesian posterior beliefs is hard. We also describe a natural search algorithm to compute agents’ actions, which we call elimination of impossible signals , and show that if the network is transitive, the algorithm can be modified to run in polynomial time.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.2000},
  journal      = {Operations Research},
  number       = {2},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {Bayesian decision making in groups is hard},
  volume       = {69},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Matching while learning. <em>OR</em>, <em>69</em>(2), ii–iv.
(<a href="https://doi.org/10.1287/opre.2020.2013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem faced by a service platform that needs to match limited supply with demand while learning the attributes of new users to match them better in the future. We introduce a benchmark model with heterogeneous workers (demand) and a limited supply of jobs that arrive over time. Job types are known to the platform, but worker types are unknown and must be learned by observing match outcomes. Workers depart after performing a certain number of jobs. The expected payoff from a match depends on the pair of types, and the goal is to maximize the steady-state rate of accumulation of payoff. Although we use terminology inspired by labor markets, our framework applies more broadly to platforms where a limited supply of heterogeneous products is matched to users over time. Our main contribution is a complete characterization of the structure of the optimal policy in the limit that each worker performs many jobs. The platform faces a tradeoff for each worker between myopically maximizing payoffs ( exploitation ) and learning the type of the worker ( exploration ). This creates a multitude of multiarmed bandit problems, one for each worker, coupled together by the constraint on availability of jobs of different types ( capacity constraints ). We find that the platform should estimate a shadow price for each job type and use the payoffs adjusted by these prices first to determine its learning goals and then for each worker (i) to balance learning with payoffs during the exploration phase and (ii) to myopically match after it has achieved its learning goals during the exploitation phase.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.2013},
  journal      = {Operations Research},
  number       = {2},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {Matching while learning},
  volume       = {69},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Preservation of supermodularity in parametric optimization:
Necessary and sufficient conditions on constraint structures.
<em>OR</em>, <em>69</em>(1), iii–vi. (<a
href="https://doi.org/10.1287/opre.2020.1992">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a systematic study of the preservation of supermodularity under parametric optimization, allowing us to derive complementarity among parameters and monotonic structural properties for optimal policies in many operational models. We introduce the new concepts of mostly sublattice and additive mostly sublattice, which generalize the commonly imposed sublattice condition significantly, and use them to establish the necessary and sufficient conditions for the feasible set so that supermodularity can be preserved under various assumptions about the objective functions. Furthermore, we identify some classes of polyhedral sets that satisfy these concepts. Finally, we illustrate the use of our results in assemble-to-order systems.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.1992},
  journal      = {Operations Research},
  number       = {1},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Preservation of supermodularity in parametric optimization: Necessary and sufficient conditions on constraint structures},
  volume       = {69},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Pooling queues with strategic servers: The effects of
customer ownership. <em>OR</em>, <em>69</em>(1), iii–vi. (<a
href="https://doi.org/10.1287/opre.2020.2004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although pooling queues offer in principle many operational benefits, these may not always be achieved in practice. One reason, observed in, prior empirical studies, relates to customer ownership. In this paper, we formalize these empirical observations by developing a game-theoretic model to assess the performance of pooling when servers choose their capacities strategically and exhibit varying scopes of customer ownership, captured by two cost components, respectively, associated with the processing time and the waiting time of a server’s customers. We show that the core benefits of pooling are mostly annihilated in this setting. In fact, for any given scope of customer ownership, the queue configuration has almost no impact on the customers’ average throughput time unless (i) servers’ degree of customer ownership is so low that they choose to operate at high utilization and (ii) they care much more about their customers’ processing times than about their waiting times. Under these conditions, adopting a dedicated queue configuration can yield significantly lower throughput times. This prescription becomes even more pertinent if the switch to a dedicated queue configuration is associated with an expansion in the scope of customer ownership, as may happen in practice.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.2004},
  journal      = {Operations Research},
  number       = {1},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Pooling queues with strategic servers: The effects of customer ownership},
  volume       = {69},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust multiperiod vehicle routing under customer order
uncertainty. <em>OR</em>, <em>69</em>(1), iii–vi. (<a
href="https://doi.org/10.1287/opre.2020.2009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study multiperiod vehicle routing problems where the aim is to determine a minimum cost visit schedule and associated routing plan for each period using capacity-constrained vehicles. In our setting, we allow for customer service requests that are received dynamically over the planning horizon. In order to guarantee the generation of routing plans that can flexibly accommodate potential service requests that have not yet been placed, we model future potential service requests as binary random variables, and we seek to determine a visit schedule that remains feasible for all anticipated realizations of service requests. To that end, the decision-making process can be viewed as a multistage robust optimization problem with binary recourse decisions. We approximate the multistage problem via a nonanticipative two-stage model for which we propose a novel integer programming formulation and a branch-and-cut solution approach. In order to investigate the quality of the solutions we obtain, we also derive a valid lower bound on the multistage problem and present numerical schemes for its computation. Computational experiments on benchmark data sets show that our approach is practically tractable and generates high-quality robust plans that significantly outperform existing approaches in terms of both operational costs and fleet utilization.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.2009},
  journal      = {Operations Research},
  number       = {1},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Robust multiperiod vehicle routing under customer order uncertainty},
  volume       = {69},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Technical note—understanding the performance of capped
base-stock policies in lost-sales inventory models. <em>OR</em>,
<em>69</em>(1), iii–vi. (<a
href="https://doi.org/10.1287/opre.2020.2019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single-sourcing lost-sales inventory systems with lead times are notoriously difficult to optimize. In this paper, we propose a new family of capped base-stock policies and provide a new perspective on constructing a practical hybrid policy combining two well-known heuristics: base-stock and constant-order policies. Each capped base-stock policy is associated with two parameters: a base-stock level and an order cap. We prove that for any fixed order cap, the capped base-stock policy converges exponentially fast in the base-stock level to a constant-order policy, providing a theoretical foundation for a phenomenon by which a capped dual-index policy converges numerically to a tailored base-surge policy recently observed in other work in a different but related dual-sourcing inventory model. As a consequence, there exists a sequence of capped base-stock policies that are asymptotically optimal as the lead time grows. We also numerically demonstrate its superior performance in general (including small lead times) by comparing it with otherwell-known heuristics.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.2019},
  journal      = {Operations Research},
  number       = {1},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Technical Note—Understanding the performance of capped base-stock policies in lost-sales inventory models},
  volume       = {69},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Targeting, deployment, and loss-tolerance in lanchester
engagements. <em>OR</em>, <em>69</em>(1), iii–vi. (<a
href="https://doi.org/10.1287/opre.2020.2022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing Lanchester combat models focus on two force parameters: numbers (force size) and per-capita effectiveness (attrition rate). Whereas these two parameters are central in projecting a battle’s outcome, there are other important factors that affect the battlefield: (1) targeting capability, that is, the capacity to identify live enemy units and not dissipate fire on nontargets; (2) tactical restrictions preventing full deployment of forces; and (3) morale and tolerance of losses, that is, the capacity to endure casualties. In the spirit of Lanchester theory, we derive, for the first time, force-parity equations for various combinations of these effects and obtain general implications and trade-offs. We show that more units and better weapons (higher attrition rate) are preferred over improved targeting capability and relaxed deployment restrictions unless these are poor. However, when facing aimed fire and unable to deploy more than half of one’s force, it is better to be able to deploy more existing units than to have either additional reserve units or the same increase in attrition effectiveness. Likewise, more relaxed deployment constraints are preferred over enhanced loss-tolerance when initial reserves are greater than the force level at which withdrawal occurs.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.2022},
  journal      = {Operations Research},
  number       = {1},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Targeting, deployment, and loss-tolerance in lanchester engagements},
  volume       = {69},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The shortest path interdiction problem with randomized
interdiction strategies: Complexity and algorithms. <em>OR</em>,
<em>69</em>(1), iii–vi. (<a
href="https://doi.org/10.1287/opre.2020.2023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Shortest-path interdiction problems involve a leader and a follower playing a zero-sum game over a directed network. The leader interdicts a set of arcs, and arc costs increase as a function of the number of times they are interdicted. The follower observes the leader’s actions and selects a shortest path in response. The leader’s optimal interdiction strategy maximizes the follower’s minimum-cost path. In classic formulations of these problems, the leader’s interdiction actions are deterministic. In this paper the leader selects a policy of randomized interdiction actions, and the follower only knows the probability of where interdictions are deployed on the network. The follower identifies a path having the minimum expected cost, whereas the leader seeks to maximize the follower’s objective. When the arc costs are affine functions of the number of times they are interdicted, this problem is solvable by linear programming. However, when the cost functions are convex or when they are concave, we show that the expected costs are Schur concave or convex, respectively. This property allows us to prove that the problem becomes strongly NP-hard in the nonaffine case. We also propose several algorithms for this problem. Some are for special cases, and one is a general algorithm. We examine the efficacy of our algorithms on a test bed of randomly generated instances by comparing our algorithms to a standard solver.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.2023},
  journal      = {Operations Research},
  number       = {1},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {The shortest path interdiction problem with randomized interdiction strategies: Complexity and algorithms},
  volume       = {69},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Risk arbitrage opportunities for stock index options.
<em>OR</em>, <em>69</em>(1), iii–vi. (<a
href="https://doi.org/10.1287/opre.2020.2012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To analyze the economic significance of pricing errors of stock index options, a system of linear inequalities is developed that completely characterizes all risk arbitrage opportunities that arise if a well-behaved pricing kernel does not exist. The stochastic arbitrage system can account for market imperfections in the form of transactions costs and general portfolio restrictions. An active trading strategy based on the stochastic arbitrage system for front-month S&amp;P500 stock index options yields significant abnormal returns out of sample for small-scale portfolios. However, outperformance seems elusive if the strategy is scaled up and market depth is taken into account.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.2012},
  journal      = {Operations Research},
  number       = {1},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Risk arbitrage opportunities for stock index options},
  volume       = {69},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Competitive equilibrium and trading networks: A network flow
approach. <em>OR</em>, <em>69</em>(1), iii–vi. (<a
href="https://doi.org/10.1287/opre.2020.1997">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Under full substitutability of preferences, it is known that a competitive equilibrium exists in trading networks and is equivalent to (chain) stable outcomes. In this paper, we formulate the problem of finding an efficient set of trades as a generalized submodular flow problem in a suitable network. Existence of a competitive equilibrium and its equivalence with the seemingly weaker notion of stability follow directly from the optimality conditions of the flow problem. Our formulation enables us to perform comparative statics with respect to the number of buyers, sellers, and trades. For instance, we establish that if a new buyer is added to the economy, at an equilibrium the prices of all existing trades increase. In addition, we give a polynomial time algorithm for finding competitive equilibria in trading networks and testing (chain) stability. Funding : O. Candogan gratefully acknowledges financial support from the University of Chicago Booth School of Business. M. Epitropou gratefully acknowledges financial support from the Department of Electrical and Systems Engineering, University of Pennsylvania. The research of R.V. Vohra was supported in part by the National Science Foundation [Grant AST-1343381].},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.1997},
  journal      = {Operations Research},
  number       = {1},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Competitive equilibrium and trading networks: A network flow approach},
  volume       = {69},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Value of information in bayesian routing games. <em>OR</em>,
<em>69</em>(1), iii–vi. (<a
href="https://doi.org/10.1287/opre.2020.1999">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study a routing game in an environment with multiple heterogeneous information systems and an uncertain state that affects edge costs of a congested network. Each information system sends a noisy signal about the state to its subscribed traveler population. Travelers make route choices based on their private beliefs about the state and other populations’ signals. The question then arises, “How does the presence of asymmetric and incomplete information affect the travelers’ equilibrium route choices and costs?” We develop a systematic approach to characterize the equilibrium structure and determine the effect of population sizes on the relative value of information (i.e., difference in expected traveler costs) between any two populations. This effect can be evaluated using a population-specific size threshold. One population enjoys a strictly positive value of information in comparison with the other if and only if its size is below the corresponding threshold. We also consider the situation when travelers may choose an information system based on its value and characterize the set of equilibrium adoption rates delineating the sizes of subscribed traveler populations. The resulting routing strategies are such that all travelers face an identical expected cost and no traveler has the incentive to change subscriptions.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.1999},
  journal      = {Operations Research},
  number       = {1},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Value of information in bayesian routing games},
  volume       = {69},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Technical note—global robust stability in a general price
and assortment competition model. <em>OR</em>, <em>69</em>(1), iii–vi.
(<a href="https://doi.org/10.1287/opre.2020.2001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We analyze a general but parsimonious price competition model for an oligopoly in which each firm offers any number of products. The demand volumes are general piecewise affine functions of the full price vector, generated as the “regular” extension of a base set of affine functions. The model specifies a product assortment , along with their prices and demand volumes, in contrast to most commonly used demand models, such as the multinomial logit model or any of its variants. We show that a special equilibrium in this model has global robust stability . This means that, from any starting point, the market converges to this equilibrium when firms use a particular response mapping to dynamically adjust their own prices in response to their competitors’ prices. The mapping involves each firm optimizing its own prices over a limited subset of possible prices and requires each firm to only know the demand function and cost structure for its own products (but not for other firms’ products).},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.2001},
  journal      = {Operations Research},
  number       = {1},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Technical Note—Global robust stability in a general price and assortment competition model},
  volume       = {69},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Technical note—on revenue management with strategic
customers choosing when and what to buy. <em>OR</em>, <em>69</em>(1),
iii–vi. (<a href="https://doi.org/10.1287/opre.2020.2008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Consider a network revenue management model in which a seller offers multiple products, which consume capacitated resources. The seller uses an anonymous posted-price policy, and arriving customers strategize on (a) when and (b) which product to purchase to maximize their utility, based on heterogeneous product valuations. Such models, whereby customers are both forward looking and choose what to buy, have not yet been amenable to analysis, mainly because their associated dynamic mechanism design counterparts are multidimensional; that is, they involve constraints with multivariate private information (the product valuations). Within the context of the aforementioned model, we present a novel decomposition approach that enables us to deal with the underlying multidimensional mechanism design problem. Using this approach, we derive for all nonanticipating dynamic pricing policies an upper bound to expected revenues. We use our bound to conduct theoretical and numerical performance analyses of static pricing policies. In our theoretical analysis, we derive guarantees for the performance of static pricing, for the classical fluid-type regime where inventory and demand grow large. Our numerical analysis shows static pricing to be able capture at least 75\%–90\% of maximum possible expected revenue under a wide range of realistic problem parameters.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.2008},
  journal      = {Operations Research},
  number       = {1},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Technical Note—On revenue management with strategic customers choosing when and what to buy},
  volume       = {69},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A simple and approximately optimal mechanism for a buyer
with complements. <em>OR</em>, <em>69</em>(1), iii–vi. (<a
href="https://doi.org/10.1287/opre.2020.2039">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a revenue-maximizing seller with m heterogeneous items and a single buyer whose valuation for the items may exhibit both substitutes and complements. We show that the better of selling the items separately and bundling them together— guarantees a Θ ( d ) -fraction of the optimal revenue, where d is a measure of the degree of complementarity; it extends prior work showing that the same simple mechanism achieves a constant-factor approximation when buyer valuations are subadditive (the most general class of complement-free valuations). Our proof is enabled by a recent duality framework, which we use to obtain a bound on the optimal revenue in the generalized setting. Our technical contributions are domain specific to handle the intricacies of settings with complements. One key modeling contribution is a tractable notion of “degree of complementarity” that admits meaningful results and insights—we demonstrate that previous definitions fall short in this regard.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.2039},
  journal      = {Operations Research},
  number       = {1},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {A simple and approximately optimal mechanism for a buyer with complements},
  volume       = {69},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On policies for single-leg revenue management with limited
demand information. <em>OR</em>, <em>69</em>(1), iii–vi. (<a
href="https://doi.org/10.1287/opre.2020.2048">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study the single-item revenue management problem, with no information given about the demand trajectory over time. When the item is sold through accepting/rejecting different fare classes, the tight competitive ratio for this problem has been established by Ball and Queyranne through booking limit policies, which raise the acceptance threshold as the remaining inventory dwindles. However, when the item is sold through dynamic pricing instead, there is the additional challenge that offering a low price may entice high-paying customers to substitute down. We show that despite this challenge, the same competitive ratio can still be achieved using a randomized dynamic pricing policy. Our policy incorporates the price-skimming technique originated by Eren and Maglaras, but importantly we show how the randomized price distribution should be stochastically increased as the remaining inventory dwindles. A key technical ingredient in our policy is a new “Valuation Tracking” subroutine, which tracks the possible values for the optimum, and follows the most “inventory-conservative” control, which maintains the desired competitive ratio. Finally, we demonstrate the empirical effectiveness of our policy in simulations, where its average-case performance surpasses all naive modifications of the existing policies.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.2048},
  journal      = {Operations Research},
  number       = {1},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {On policies for single-leg revenue management with limited demand information},
  volume       = {69},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Tractable equilibria in sponsored search with endogenous
budgets. <em>OR</em>, <em>69</em>(1), iii–vi. (<a
href="https://doi.org/10.1287/opre.2020.2052">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider an ad network’s problem of allocating the auction for each individual impression to an optimal subset of advertisers with the goal of revenue maximization. This is a variant of bipartite matching except that advertisers may strategize by choosing their bidding profiles and their total budget. Because the ad network’s allocation rule affects the bidders’ strategies, equilibrium analysis is challenging. We show that this analysis is tractable when advertisers face a linear budget cost r j . In particular, we show that the strategy in which advertisers bid their valuations shaded by a factor of 1 + r j is an approximate equilibrium with the error decreasing with market size. This equilibrium can be interpreted as one in which a bidder facing an opportunity cost r j is guaranteed a return on investment of at least r j per dollar spent. Furthermore, in this equilibrium, the optimal allocation for the ad network, as determined from a linear program (LP), is greedy with high probability. This is in contrast with the exogenous budgets case, in which the LP optimization is challenging at practical scales. These results are evidence that, although in general such bipartite matching problems may be challenging to solve because of their high dimensionality, the optimal solution is remarkably simple at equilibrium.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.2052},
  journal      = {Operations Research},
  number       = {1},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Tractable equilibria in sponsored search with endogenous budgets},
  volume       = {69},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Intertemporal price discrimination with time-varying
valuations. <em>OR</em>, <em>69</em>(1), iii–vi. (<a
href="https://doi.org/10.1287/opre.2020.1984">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A firm that sells a nonperishable product considers intertemporal price discrimination in the objective of maximizing its long-run average revenue. We consider a general model of patient customers with changing valuations. Arriving customers wait for a random but bounded length of time and purchase the product when its price falls below their valuation, which varies following a stochastic process. We show the optimality of a class of cyclic strategies and obtain an algorithm that yields them. When the pace of intertemporal pricing is constrained to be comparable to the upper bound on customers’ patience levels, we have a good control on the cycle length and on the structure of the optimizing cyclic policies. The results also hold for forward-looking strategic customers as well as under an alternative objective of maximizing infinite horizon discounted revenue. We cast our results in a general framework of optimizing the long-run average revenue for a class of revenue functions that we denote weakly coupled , in which the revenue per period depends on a finite number of neighboring prices. We analyze in detail the case of Markovian valuations where we can obtain closed form formulations of the revenue function and some refinements of our main results. We also extend our results to the case of patience levels that are bounded only in expectation.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.1984},
  journal      = {Operations Research},
  number       = {1},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Intertemporal price discrimination with time-varying valuations},
  volume       = {69},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Technical note—a stochastic assignment problem with unknown
eligibility probabilities. <em>OR</em>, <em>69</em>(1), iii–vi. (<a
href="https://doi.org/10.1287/opre.2020.1988">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Consider n initially empty boxes, numbered 1 through n . Balls arrive sequentially. Each ball has a binary n -vector attached to it, with the interpretation that the ball is eligible to be put in box i if component i of its vector is equal to 1. An arriving ball can be put in any empty box for which it is eligible. Assuming that components of the vector are independent Bernoulli random variables with initially unknown probabilities, our primary interest is to compare several policies to determine which leads to a stochastically smaller number of observed balls until all boxes are filled.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.1988},
  journal      = {Operations Research},
  number       = {1},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Technical Note—A stochastic assignment problem with unknown eligibility probabilities},
  volume       = {69},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Technical note—a note on the equivalence of upper confidence
bounds and gittins indices for patient agents. <em>OR</em>,
<em>69</em>(1), iii–vi. (<a
href="https://doi.org/10.1287/opre.2020.1987">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This note gives a short, self-contained proof of a sharp connection between Gittins indices and Bayesian upper confidence bound algorithms. I consider a Gaussian multiarmed bandit problem with discount factor γ. The Gittins index of an arm is shown to equal the γ-quantile of the posterior distribution of the arm’s mean plus an error term that vanishes as γ → 1. In this sense, for sufficiently patient agents, a Gittins index measures the highest plausible mean-reward of an arm in a manner equivalent to an upper confidence bound.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.1987},
  journal      = {Operations Research},
  number       = {1},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Technical Note—A note on the equivalence of upper confidence bounds and gittins indices for patient agents},
  volume       = {69},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The discrete moment problem with nonconvex shape
constraints. <em>OR</em>, <em>69</em>(1), iii–vi. (<a
href="https://doi.org/10.1287/opre.2020.1990">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The discrete moment problem is a foundational problem in distribution-free robust optimization, where the goal is to find a worst-case distribution that satisfies a given set of moments. This paper studies the discrete moment problems with additional shape constraints that guarantee the worst-case distribution is either log-concave (LC), has an increasing failure rate (IFR), or increasing generalized failure rate (IGFR). These classes of shape constraints have not previously been studied in the literature, in part due to their inherent nonconvexities. Nonetheless, these classes are useful in practice, with applications in revenue management, reliability, and inventory control. We characterize the structure of optimal extreme point distributions under these constraints. We show, for example, that an optimal extreme point solution to a moment problem with m moments and LC shape constraints is piecewise geometric with at most m pieces. This optimality structure allows us to design an exact algorithm for computing optimal solutions in a low-dimensional space of parameters. We also leverage this structure to study a robust newsvendor problem with shape constraints and compute optimal solutions.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.1990},
  journal      = {Operations Research},
  number       = {1},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {The discrete moment problem with nonconvex shape constraints},
  volume       = {69},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dynamic incentive-aware learning: Robust pricing in
contextual auctions. <em>OR</em>, <em>69</em>(1), iii–vi. (<a
href="https://doi.org/10.1287/opre.2020.1991">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by pricing in ad exchange markets, we consider the problem of robust learning of reserve prices against strategic buyers in repeated contextual second-price auctions. Buyers’ valuations for an item depend on the context that describes the item. However, the seller is not aware of the relationship between the context and buyers’ valuations (i.e., buyers’ preferences). The seller’s goal is to design a learning policy to set reserve prices via observing the past sales data, and her objective is to minimize her regret for revenue, where the regret is computed against a clairvoyant policy that knows buyers’ heterogeneous preferences. Given the seller’s goal, utility-maximizing buyers have the incentive to bid untruthfully in order to manipulate the seller’s learning policy. We propose learning policies that are robust to such strategic behavior. These policies use the outcomes of the auctions, rather than the submitted bids, to estimate the preferences while controlling the long-term effect of the outcome of each auction on the future reserve prices. When the market noise distribution is known to the seller, we propose a policy called contextual robust pricing that achieves a T -period regret of O ( d log( T d ) log( T )), where d is the dimension of the contextual information. When the market noise distribution is unknown to the seller, we propose two policies whose regrets are sublinear in T .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.1991},
  journal      = {Operations Research},
  number       = {1},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Dynamic incentive-aware learning: Robust pricing in contextual auctions},
  volume       = {69},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning unknown service rates in queues: A multiarmed
bandit approach. <em>OR</em>, <em>69</em>(1), iii–vi. (<a
href="https://doi.org/10.1287/opre.2020.1995">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Consider a queueing system consisting of multiple servers. Jobs arrive over time and enter a queue for service; the goal is to minimize the size of this queue. At each opportunity for service, at most one server can be chosen, and at most one job can be served. Service is successful with a probability (the service probability ) that is a priori unknown for each server. An algorithm that knows the service probabilities (the “genie”) can always choose the server of highest service probability. We study algorithms that learn the unknown service probabilities. Our goal is to minimize queue regret : the (expected) difference between the queue lengths obtained by the algorithm and those obtained by the “genie.” Because queue regret cannot be larger than classical regret, results for the standard multiarmed bandit problem give algorithms for which queue regret increases no more than logarithmically in time. Our paper shows surprisingly more complex behavior. In particular, as long as the bandit algorithm’s queues have relatively long regenerative cycles, queue regret is similar to cumulative regret and scales (essentially) logarithmically. However, we show that this “early stage” of the queueing bandit eventually gives way to a “late stage,” where the optimal queue-regret scaling is O (1/ t ). We demonstrate an algorithm that (orderwise) achieves this asymptotic queue regret in the late stage. Our results are developed in a more general model that allows for multiple job classes as well.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.1995},
  journal      = {Operations Research},
  number       = {1},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Learning unknown service rates in queues: A multiarmed bandit approach},
  volume       = {69},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust power management via learning and game design.
<em>OR</em>, <em>69</em>(1), iii–vi. (<a
href="https://doi.org/10.1287/opre.2020.1996">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the target-rate power management problem for wireless networks; and we propose two simple, distributed power management schemes that regulate power in a provably robust manner by efficiently leveraging past information. Both schemes are obtained via a combined approach of learning and “game design” where we (1) design a game with suitable payoff functions such that the optimal joint power profile in the original power management problem is the unique Nash equilibrium of the designed game; (2) derive distributed power management algorithms by directing the networks’ users to employ a no-regret learning algorithm to maximize their individual utility over time. To establish convergence, we focus on the well-known online eager gradient descent learning algorithm in the class of weighted strongly monotone games. In this class of games, we show that when players only have access to imperfect stochastic feedback, multiagent online eager gradient descent converges to the unique Nash equilibrium in mean square at a O ( 1 T ) rate. In the context of power management in static networks, we show that the designed games are weighted strongly monotone if the network is feasible (i.e., when all users can concurrently attain their target rates). This allows us to derive a geometric convergence rate to the joint optimal transmission power. More importantly, in stochastic networks where channel quality fluctuates over time, the designed games are also weighted strongly monotone and the proposed algorithms converge in mean square to the joint optimal transmission power at a O ( 1 T ) rate, even when the network is only feasible on average (i.e., users may be unable to meet their requirements with positive probability). This comes in stark contrast to existing algorithms (like the seminal Foschini–Miljanic algorithm and its variants) that may fail to converge altogether.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.1996},
  journal      = {Operations Research},
  number       = {1},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Robust power management via learning and game design},
  volume       = {69},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Nonsparse learning with latent variables. <em>OR</em>,
<em>69</em>(1), iii–vi. (<a
href="https://doi.org/10.1287/opre.2020.2005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a popular tool for producing meaningful and interpretable models, large-scale sparse learning works efficiently in many optimization applications when the underlying structures are indeed or close to sparse. However, naively applying the existing regularization methods can result in misleading outcomes because of model misspecification. In this paper, we consider nonsparse learning under the factors plus sparsity structure, which yields a joint modeling of sparse individual effects and common latent factors. A new methodology of nonsparse learning with latent variables (NSL) is proposed for joint estimation of the effects of two groups of features, one for individual effects and the other associated with the latent substructures, when the nonsparse effects are captured by the leading population principal component score vectors. We derive the convergence rates of both sample principal components and their score vectors that hold for a wide class of distributions. With the properly estimated latent variables, properties including model selection consistency and oracle inequalities under various prediction and estimation losses are established. Our new methodology and results are evidenced by simulation and real-data examples.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.2005},
  journal      = {Operations Research},
  number       = {1},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Nonsparse learning with latent variables},
  volume       = {69},
  year         = {2021},
}
</textarea>
</details></li>
</ul>

</body>
</html>
