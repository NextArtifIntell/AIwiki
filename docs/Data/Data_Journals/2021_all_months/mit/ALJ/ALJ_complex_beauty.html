<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>ALJ_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="alj---21">ALJ - 21</h2>
<ul>
<li><details>
<summary>
(2021). IMPROBED: Multiple problem-solving brain via evolved
developmental programs. <em>ALJ</em>, <em>27</em>(3–4), 300–335. (<a
href="https://doi.org/10.1162/artl_a_00346">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial neural networks (ANNs) were originally inspired by the brain; however, very few models use evolution and development, both of which are fundamental to the construction of the brain. We describe a simple neural model, called IMPROBED, in which two neural programs construct an artificial brain that can simultaneously solve multiple computational problems. One program represents the neuron soma and the other the dendrite. The soma program decides whether neurons move, change, die, or replicate. The dendrite program decides whether dendrites extend, change, die, or replicate. Since developmental programs build networks that change over time, it is necessary to define new problem classes that are suitable to evaluate such approaches. We show that the pair of evolved programs can build a single network from which multiple conventional ANNs can be extracted, each of which can solve a different computational problem. Our approach is quite general and it could be applied to a much wider variety of problems.},
  archive      = {J_ALJ},
  author       = {Miller, Julian Francis},
  doi          = {10.1162/artl_a_00346},
  journal      = {Artificial Life},
  number       = {3–4},
  pages        = {300-335},
  shortjournal = {Artif. Life},
  title        = {IMPROBED: Multiple problem-solving brain via evolved developmental programs},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Emergence of self-reproducing metabolisms as recursive
algorithms in an artificial chemistry. <em>ALJ</em>, <em>27</em>(3–4),
277–299. (<a href="https://doi.org/10.1162/artl_a_00355">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the main goals of Artificial Life is to research the conditions for the emergence of life, not necessarily as it is, but as it could be. Artificial chemistries are one of the most important tools for this purpose because they provide us with a basic framework to investigate under which conditions metabolisms capable of reproducing themselves, and ultimately, of evolving, can emerge. While there have been successful attempts at producing examples of emergent self-reproducing metabolisms, the set of rules involved remain too complex to shed much light on the underlying principles at work. In this article, we hypothesize that the key property needed for self-reproducing metabolisms to emerge is the existence of an autocatalyzed subset of Turing-complete reactions. We validate this hypothesis with a minimalistic artificial chemistry with conservation laws, which is based on a Turing-complete rewriting system called combinatory logic. Our experiments show that a single run of this chemistry, starting from a tabula rasa state, discovers—with no external intervention—a wide range of emergent structures including ones that self-reproduce in each cycle. All of these structures take the form of recursive algorithms that acquire basic constituents from the environment and decompose them in a process that is remarkably similar to biological metabolisms.},
  archive      = {J_ALJ},
  author       = {Kruszewski, Germán and Mikolov, Tomáš},
  doi          = {10.1162/artl_a_00355},
  journal      = {Artificial Life},
  number       = {3–4},
  pages        = {277-299},
  shortjournal = {Artif. Life},
  title        = {Emergence of self-reproducing metabolisms as recursive algorithms in an artificial chemistry},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Pleasing enhances indirect reciprocity-based cooperation
under private assessment. <em>ALJ</em>, <em>27</em>(3–4), 246–276. (<a
href="https://doi.org/10.1162/artl_a_00344">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Indirect reciprocity is an important mechanism for promoting cooperation among self-interested agents. Simplified, it means “you help me; therefore somebody else will help you” (in contrast to direct reciprocity: “you help me; therefore I will help you”). Indirect reciprocity can be achieved via reputation and norms. Strategies, such as the so-called leading eight , relying on these principles can maintain high levels of cooperation and remain stable against invasion, even in the presence of errors. However, this is only the case if the reputation of an agent is modeled as a shared public opinion. If agents have private opinions and hence can disagree as to whether somebody is good or bad, even rare errors can cause cooperation to break apart. We show that most strategies can overcome the private assessment problem by applying pleasing . A pleasing agent acts in accordance with others&#39; expectations of their behaviour (i.e., pleasing them) instead of being guided by their own, private assessment. As such, a pleasing agent can achieve a better reputation than previously considered strategies when there is disagreement in the population. Pleasing is effective even if the opinions of only a few other individuals are considered and when it bears additional costs. Finally, through a more exhaustive analysis of the parameter space than previous studies, we show that some of the leading eight still function under private assessment, i.e., that cooperation rates are well above an objective baseline. Yet, pleasing strategies supersede formerly described ones and enhance cooperation.},
  archive      = {J_ALJ},
  author       = {Krellner, Marcus and Han, The Anh},
  doi          = {10.1162/artl_a_00344},
  journal      = {Artificial Life},
  number       = {3–4},
  pages        = {246-276},
  shortjournal = {Artif. Life},
  title        = {Pleasing enhances indirect reciprocity-based cooperation under private assessment},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Classification of discrete dynamical systems based on
transients. <em>ALJ</em>, <em>27</em>(3–4), 220–245. (<a
href="https://doi.org/10.1162/artl_a_00342">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to develop systems capable of artificial evolution, we need to identify which systems can produce complex behavior. We present a novel classification method applicable to any class of deterministic discrete space and time dynamical systems. The method is based on classifying the asymptotic behavior of the average computation time in a given system before entering a loop. We were able to identify a critical region of behavior that corresponds to a phase transition from ordered behavior to chaos across various classes of dynamical systems. To show that our approach can be applied to many different computational systems, we demonstrate the results of classifying cellular automata, Turing machines, and random Boolean networks. Further, we use this method to classify 2D cellular automata to automatically find those with interesting, complex dynamics. We believe that our work can be used to design systems in which complex structures emerge. Also, it can be used to compare various versions of existing attempts to model open-ended evolution (Channon, 2006 ; Ofria &amp; Wilke, 2004 ; Ray, 1991 ).},
  archive      = {J_ALJ},
  author       = {Hudcová, Barbora and Mikolov, Tomáš},
  doi          = {10.1162/artl_a_00342},
  journal      = {Artificial Life},
  number       = {3–4},
  pages        = {220-245},
  shortjournal = {Artif. Life},
  title        = {Classification of discrete dynamical systems based on transients},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Morphological sensitivity and falling behavior of paper
v-shapes. <em>ALJ</em>, <em>27</em>(3–4), 204–219. (<a
href="https://doi.org/10.1162/artl_a_00340">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Behavioral diversity seen in biological systems is, at the most basic level, driven by interactions between physical materials and their environment. In this context we are interested in falling paper systems, specifically the V-shaped falling paper (VSFP) system that exhibits a set of discrete falling behaviors across the morphological parameter space. Our previous work has investigated how morphology influences dominant falling behaviors in the VSFP system. In this article we build on this analysis to investigate the nature of behavioral transitions in the same system. First, we investigate stochastic behavior transitions. We demonstrate how morphology influences the likelihood of different transitions, with certain morphologies leading to a wide range of possible paths through the behavior-space. Second, we investigate deterministic transitions. To investigate behaviors over longer time periods than available in falling experiments we introduce a new experimental platform. We demonstrate how we can induce behavior transitions by modulating the energy input to the system. Certain behavior transitions are found to be irreversible, exhibiting a form of hysteresis, while others are fully reversible. Certain morphologies are shown to behave like simplistic sequential logic circuits, indicating that the system has a form of memory encoded into the morphology–environment interactions. Investigating the limits of how morphology–environment interactions induce non-trivial behaviors is a key step for the design of embodied artificial life-forms.},
  archive      = {J_ALJ},
  author       = {Howison, Toby and Hughes, Josie and Iida, Fumiya},
  doi          = {10.1162/artl_a_00340},
  journal      = {Artificial Life},
  number       = {3–4},
  pages        = {204-219},
  shortjournal = {Artif. Life},
  title        = {Morphological sensitivity and falling behavior of paper V-shapes},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Problem-solving benefits of down-sampled lexicase selection.
<em>ALJ</em>, <em>27</em>(3–4), 183–203. (<a
href="https://doi.org/10.1162/artl_a_00341">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In genetic programming, an evolutionary method for producing computer programs that solve specified computational problems, parent selection is ordinarily based on aggregate measures of performance across an entire training set. Lexicase selection, by contrast, selects on the basis of performance on random sequences of training cases; this has been shown to enhance problem-solving power in many circumstances. Lexicase selection can also be seen as better reflecting biological evolution, by modeling sequences of challenges that organisms face over their lifetimes. Recent work has demonstrated that the advantages of lexicase selection can be amplified by down-sampling, meaning that only a random subsample of the training cases is used each generation. This can be seen as modeling the fact that individual organisms encounter only subsets of the possible environments and that environments change over time. Here we provide the most extensive benchmarking of down-sampled lexicase selection to date, showing that its benefits hold up to increased scrutiny. The reasons that down-sampling helps, however, are not yet fully understood. Hypotheses include that down-sampling allows for more generations to be processed with the same budget of program evaluations; that the variation of training data across generations acts as a changing environment, encouraging adaptation; or that it reduces overfitting, leading to more general solutions. We systematically evaluate these hypotheses, finding evidence against all three, and instead draw the conclusion that down-sampled lexicase selection&#39;s main benefit stems from the fact that it allows the evolutionary process to examine more individuals within the same computational budget, even though each individual is examined less completely.},
  archive      = {J_ALJ},
  author       = {Helmuth, Thomas and Spector, Lee},
  doi          = {10.1162/artl_a_00341},
  journal      = {Artificial Life},
  number       = {3–4},
  pages        = {183-203},
  shortjournal = {Artif. Life},
  title        = {Problem-solving benefits of down-sampled lexicase selection},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The importance of noise colour in simulations of
evolutionary systems. <em>ALJ</em>, <em>27</em>(3–4), 164–182. (<a
href="https://doi.org/10.1162/artl_a_00354">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Simulations of evolutionary dynamics often employ white noise as a model of stochastic environmental variation. Whilst white noise has the advantages of being simply generated and analytically tractable, empirical analyses demonstrate that most real environmental time series have power spectral densities consistent with pink or red noise, in which lower frequencies contribute proportionally greater amplitudes than higher frequencies. Simulated white noise environments may therefore fail to capture key components of real environmental time series, leading to erroneous results. To explore the effects of different noise colours on evolving populations, a simple evolutionary model of the interaction between life-history and the specialism-generalism axis was developed. Simulations were conducted using a range of noise colours as the environments to which agents adapted. Results demonstrate complex interactions between noise colour, reproductive rate, and the degree of evolved generalism; importantly, contradictory conclusions arise from simulations using white as opposed to red noise, suggesting that noise colour plays a fundamental role in generating adaptive responses. These results are discussed in the context of previous research on evolutionary responses to fluctuating environments, and it is suggested that Artificial Life as a field should embrace a wider spectrum of coloured noise models to ensure that results are truly representative of environmental and evolutionary dynamics.},
  archive      = {J_ALJ},
  author       = {Grove, Matt and Timbrell, Lucy and Jolley, Ben and Polack, Fiona and Borg, James M.},
  doi          = {10.1162/artl_a_00354},
  journal      = {Artificial Life},
  number       = {3–4},
  pages        = {164-182},
  shortjournal = {Artif. Life},
  title        = {The importance of noise colour in simulations of evolutionary systems},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Explaining evolutionary agent-based models via principled
simplification. <em>ALJ</em>, <em>27</em>(3–4), 143–163. (<a
href="https://doi.org/10.1162/artl_a_00339">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding how evolutionary agents behave in complex environments is a challenging problem. Agents can be faced with complex fitness landscapes derived from multi-stage tasks, interaction with others, and limited environmental feedback. Agents that evolve to overcome these can sometimes access greater fitness, as a result of factors such as cooperation and tool use. However, it is often difficult to explain why evolutionary agents behave in certain ways, and what specific elements of the environment or task may influence the ability of evolution to find goal-achieving behaviours; even seemingly simple environments or tasks may contain features that affect agent evolution in unexpected ways. We explore principled simplification of evolutionary agent-based models, as a possible route to aiding their explainability. Using the River Crossing Task (RCT) as a case study, we draw on analysis in the Minimal River Crossing (RC-) Task testbed, which was designed to simplify the original task while keeping its key features. Using this method, we present new analysis concerning when agents evolve to successfully complete the RCT. We demonstrate that the RC- environment can be used to understand the effect that a cost to movement has on agent evolution, and that these findings can be generalised back to the original RCT. Then, we present new insight into the use of principled simplification in understanding evolutionary agents. We find evidence that behaviour dependent on features that survive simplification, such as problem structure, are amenable to prediction; while predicting behaviour dependent on features that are typically reduced in simplification, such as scale, can be invalid.},
  archive      = {J_ALJ},
  author       = {Barnes, Chloe M. and Ghouri, Abida and Lewis, Peter R.},
  doi          = {10.1162/artl_a_00339},
  journal      = {Artificial Life},
  number       = {3–4},
  pages        = {143-163},
  shortjournal = {Artif. Life},
  title        = {Explaining evolutionary agent-based models via principled simplification},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Editorial: Introduction to the 2020 conference on artificial
life special issue. <em>ALJ</em>, <em>27</em>(3–4), 141–142. (<a
href="https://doi.org/10.1162/artl_e_00356">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This special issue highlights key selections from the 2020 Conference on Artificial Life, which is the primary international meeting organized yearly by the International Society for Artificial Life (www.alife.org). The conference themes broadly address the synthesis and simulation of living systems, welcoming scientific research that either deepens our understanding of life as we know it or broadens our conception of life as it could be (Langton, 1989).The 2020 conference, hosted by the University of Vermont and the Vermont Complex Systems Center, was originally intended to be held in Montréal, Québec, Canada. However, the global COVID-19 pandemic forced this event, like many others, to be held online. In truth, this challenge afforded a unique opportunity to hold a truly global conference, with 390 registered attendees from around the world.Of 183 submissions, 75 articles (41\%) were accepted for full presentations at the conference and published in the proceedings (Bongard et al., 2020). An additional 11 submissions were presented as lighting talks and 24 as posters and were also included in the proceedings.Reflecting the highly interdisciplinary nature of the field, the topics covered in this special issue include evolutionary dynamics, artificial chemistry, agent-based modelling, game theory, genetic programming, neuroevolution, embodiment, and complex systems research: Ghouri, Barnes, and Lewis present a minimal version of the classic river crossing task, isolating the core task of building a bridge in a grid world for the purpose of increasing explainability of the original problem. Results with the minimal environment are consistent with results from the original version, highlighting the utility of the minimal environment for experiments on explainable evolutionary intelligence.Grove, Timbrell, Jolley, Polack, and Borg demonstrate that the mathematical color of noise in an environment has a significant impact on dynamics in evolving populations. In particular, their results call into question whether commonly employed Gaussian or white noise models should be the default.Lexicase selection in genetic programming is an alternative to traditional parent selection. Helmuth and Spector conduct an extensive benchmarking of a variant called down-sampled lexicase selection, showing that it outperforms standard selection, and investigate hypotheses about why it performs so well.Howison, Hugues, and Iida explore how morphology can be used to control and program interactions with the environment in their study of V-shaped falling papers. They also show how Bayesian optimization can be used to design functional constructs of nonliving materials in the real world.Hudcová and Mikolov provide a framework for classifying cellular automata complexity based on transients, which are parts of automata trajectories observed before entering into a loop. In particular, the presented classification is based on the asymptotic growth of the average transient length with increasing grid size. This framework is intended to aid the identification of interesting phenomena in evolving systems.Krellner and Han study the evolution of cooperation among agents playing a donation game. In particular, this work introduces a novel approach to information sharing to solve the problem of private information.Kruszewski and Mikolov develop an artificial chemistry based on combinatory logic, showing that complex structures emerge over time from a simple dynamical system. This work explicitly addresses the origins of open-ended evolution, which is a longstanding pursuit for the field of Artificial Life (and science in general).Miller re-envisions the artificial neuron model to better reflect natural evolution and development processes. Using this model, evolved programs can construct artificial neural networks that can be broken down into smaller networks that each solve distinct tasks.The 2020 conference theme “New frontiers in AI: What can ALife offer AI?” asked the community to consider how the unbridled and sometimes unconventional creativity of Artificial Life research might inspire innovation in mainstream Artificial Intelligence. In fact, the two fields share a deeply intertwined history, as some of the greatest pioneers in early Artificial Intelligence work also (or first) pursued what would now be called Artificial Life. As an example, Shannon (1940), often referred to as the father of information theory, wrote his doctoral dissertation An Algebra for Theoretical Genetics 16 years before he helped found the field of Artificial Intelligence at the Dartmouth Conference.At the same time, the Artificial Life community continues in its own myriad pursuits, recapitulating and reinventing nature often with computational tools, as evidenced by the works contained in this volume. Evolution on Earth gave rise to natural intelligence, and so evolution in silico (a mainstay of Artificial Life research) similarly bears the potential for creating Artificial Intelligence open-endedly; such is the foundational assumption of research on open-ended evolution. What ALife can offer AI is, among other things, an invitation to question what about life and intelligence might transcend substrates and, in doing so, to discover how what is might inspire what will be.},
  archive      = {J_ALJ},
  author       = {Bongard, Joshua and Lovato, Juniper and Hébert-Dufresne, Laurent and Dasari, Radhakrishna and Soros, Lisa},
  doi          = {10.1162/artl_e_00356},
  journal      = {Artificial Life},
  number       = {3–4},
  pages        = {141-142},
  shortjournal = {Artif. Life},
  title        = {Editorial: Introduction to the 2020 conference on artificial life special issue},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Rise of the self-replicators: Early visions of machines, AI
and robots that can reproduce and evolve by tim taylor and alan dorin.
<em>ALJ</em>, <em>27</em>(2), 138–140. (<a
href="https://doi.org/10.1162/artl_r_00345">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Imagine a factory where robots are constructing other robots. We know such scenes from the movie I, Robot (Proyas, 2004), where the company U. S. Robotics produces humanoid robots, or an island factory from the theatre play R.U.R. (Rossum&#39;s Universal Robots; Čapek, 1920). However, these examples are fictional; Such factories have never existed in the real world and still belong to science fiction. But what is the history and the state of the art of self-replicating machines from a scientific and technological point of view? The answer to this question can be found in a recent book, Rise of the Self-Replicators, subtitled as Early Visions of Machines, AI and Robots That Can Reproduce and Evolve, by Tim Taylor and Alan Dorin.In the book the journey through the history of self-reproducing and self-evolving objects starts in Chapter 2 in the seventeenth century. The very early works on self-replicators are related to the philosophical ideas that animals can be viewed as machines. Such self-replicators, which could reproduce by building a copy of themselves, are categorized as standard-replicators. As one of the first visions of standard-replicators, a story of René Descartes is discussed. Descartes suggested to Queen Christina of Sweden that the human body could be regarded as a machine, and she responded that “she had never seen her watch give birth to baby watches” (p. 12). Taylor and Dorin searched extensively for the original source of this anecdote to prove its truth; finally they concluded that it is apocryphal. However, in the course of their search, they uncovered a similar discussion by the seventeenth century French academic Bernard Le Bovier de Fontenelle, for which they were able to verify the original source. They are therefore able to confirm that ideas of machines that can beget machines were already in the air four centuries ago. To determine the original sources and specify where and how such very early works were published is not trivial. Taylor and Dorin have done an excellent job; they have performed a deep search in many libraries and archives, have contacted historians and experts from various fields and various countries, and have checked the original sources written in non-English languages. As a result, their book brings to light many historical and scientific facts that have never been published in the modern literature before.Since the authors offer a survey on self-replicators in a chronological way, in Chapter 3 they focus on the nineteenth century and the key people of this period and their works one by one: Samuel Butler, Alfred Marshall, and George Eliot. These people were influenced by both the industrial revolution and Darwin&#39;s On the Origin of Species, published in 1859, and they discuss machines that are able not only to reproduce, but also to evolve by natural selection just like their living counterparts. These kinds of self-replicators are categorized as evolvable self-replicators (evo-replicators).The third main era of self-replicators is dated to the mid-twentieth century and focuses on so-called manufacturing self-replicators (maker-replicators). Such machines have the ability not only to self-replicate, but also to create specific goods and materials as by-products when they self-replicate. This idea is attributed to John von Neumann, who took the inspiration for his universal constructing machines from Alan Turing&#39;s universal computing machines. This is described in Chapter 5, together with the implementation of physical self-reproducing systems by Lionel Penrose and Homer Jacobson.This chapter also introduces Nils Aall Barricelli, who is still relatively unknown in the artificial life community, although he deserves more attention. Barricelli was a visiting researcher in von Neumann&#39;s group in 1953, where he used one of the very first electronic computers to run programs that mapped the foundations of self-reproducing entities simulated entirely by numbers. He conducted his experiments as one-dimensional cellular automata with the aim of studying the simplest possible system that could display evolutionary processes. Barricelli was a pioneer in computational biology and evolutionary algorithms (maybe even their originator) and it is very commendable that Taylor and Dorin highlight his merits in their book.Chapter 5 covers the period up to the 1960s, and Chapter 6 summarizes the works from the last few decades. As there are easily-accessible sources and several comprehensive reviews focusing on works from the 1960s to the present, the authors do not go into as much detail as in earlier chapters, but rather, they outline the crucial works in the field and recommend the appropriate literature. The book focuses on both physical machines (e.g., clockwork automata and electromechanical devices) and logical machines (e.g., software programs and abstract automata), but not on bio-mechanical hybrids where the replication is based on biological principles. Since the book mainly focuses on the early history of the development of the topic, the more recent area of nano-machines and their self-replication on a molecular level are mentioned only in passing.Chapter 7 reflects on what has been achieved in the area of self-replicators and suggests what could be further directions for this research, what remains as unresolved, and what the major challenges are for future progress. Among other topics, the creation of artificial general intelligence and the building of self-replicating spaceships are discussed. The authors ask themselves what the implications would be for the machines, for ourselves, for our environment, and for the future of life on Earth and elsewhere, if the construction of self-replicating and self-evolving artificial objects were possible. Together with further philosophical, ethical, and religious questions, the authors show that each question has two sides and that we should look at progress in the design and creation of self-replicators from various perspectives, including ecological, economical, and societal aspects.Besides the scientific works, the authors also include a description of early fictional stories and their impact on scientific developments. Chapter 4 focuses on pop culture and futurology in the early twentieth century and considers the fate of humanity in the context of robot evolution. I personally appreciate that. Besides other famous novels and literary works, the Czech theatre play R.U.R. (Rossum&#39;s Universal Robots) by Karel Čapek is also presented. In this play the word robot was used for the first time. Because this play was premiered in 1921 in Prague, all robots (regardless of whether they can reproduce or not) are celebrating the centenary of their name this year.As I mentioned above, Taylor and Dorin have performed a fascinating survey on self-replicators, and also have enriched the book with engaging stories and details about how the key people in this field are interconnected, for example, how one scientist became a member of another family full of scientists through marriage, how the grandsons followed the footsteps of their grandfathers, and so forth. It would be interesting to create family trees in combination with professional networks for researchers, and thus to see the connections between, for example, Turing and Darwin, or Ada Lovelace and Mary Shelley. From this perspective, the book could be attractive for those who like the reading of scientific literature with a breath of leisure reading.To mention one negative aspect of the book for me, it makes extensive use of footnotes throughout. I personally prefer a continuous read and would have liked this content to be incorporated into the main text.In conclusion, Taylor and Dorin have written their book Rise of the Self-Replicators in such a way that I fully recommend it to anyone interested in learning about the origins of the idea of artificial objects that can reproduce and evolve. The target audience of this book is a long list of representatives of various disciplines—not only researchers interested in artificial life and artificial intelligence, but also philosophers, biologists, engineers, historians of science, computer scientists, roboticists, and many others who may find the book at least partially related to their field.To start this review, I used the vision of robots constructing other robots as a hook, but you should be aware that a recipe for fabricating Terminators does not appear in this book! However, this book does comprehensively present all the important details that led to the early visions of self-replicating machines. I believe that all open-minded readers could benefit from the knowledge that they get from this excellent book.},
  archive      = {J_ALJ},
  author       = {Čejková, Jitka},
  doi          = {10.1162/artl_r_00345},
  journal      = {Artificial Life},
  number       = {2},
  pages        = {138-140},
  shortjournal = {Artif. Life},
  title        = {Rise of the self-replicators: Early visions of machines, AI and robots that can reproduce and evolve by tim taylor and alan dorin},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A brief history of artificial intelligence research.
<em>ALJ</em>, <em>27</em>(2), 131–137. (<a
href="https://doi.org/10.1162/artl_a_00349">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Day by day, however, the machines are gaining ground upon us; day by day we are becoming more subservient to them; more men are daily bound down as slaves to tend them, more men are daily devoting the energies of their whole lives to the development of mechanical life.—Samuel Butler, “Darwin Among the Machines,” 1863Can machines ever be sentient? Could they perceive and feel things; be conscious of their surroundings? From a materialistic point of view the answer must surely be “Yes,” in particular if we accept that we ourselves are mere machines, albeit made from flesh and bones and neurons. But a point of view is not a proof: Only if we succeed in creating sentient machines can this question be answered definitively. But what are the prospects of achieving such a feat? Is it ethical to embark on such a path to begin with? And what are the dangers associated with such an endeavor? In the series of articles for this Column, I discuss one possible path towards “general intelligence” in machines: to use the process of Darwinian evolution to produce artificial brains that can be grafted onto mobile robotic platforms, with the goal of achieving fully-embodied sentient machines.The idea to evolve rather than design artificial brains has emerged independently several times. Today, this research is generally known as Neuroevolution, and it is a well-established branch of Artificial Life research (reviewed expertly in Stanley et al., 2019). I will discuss the promises and perils of neuroevolution, and compare and contrast this perspective to the more standard approaches, in particular the deep learning paradigm. But this column is not a review: Rather, I will discuss what I consider to be crucial “design decisions” for neuroevolution from my personal point of view, formed from over 20 years of research in the field. What should these “digital brains” be made of? How capable should a single neuron be? What kind of networks should they form? How complex a “digital world” should we simulate within a computer, so that the artificial brains that control artificial agents within these worlds evolve to become ever more complex? These are difficult questions, and I will not be able to offer definite answers to any of them. Instead, I want to point out how important it is that we ask these questions to begin with, even as we struggle to answer them.In June of 1863, the British writer Samuel Butler wrote a letter to the editor of The Press (a newspaper in Christchurch, New Zealand) in which he asked “what sort of creature man&#39;s next successor in the supremacy of the earth is likely to be” (Butler, 1863, para. 5). As the epigraph to the present article implies, Butler thought that machines were already a kind of “mechanical life” that one day might evolve, and ultimately supplant us all. Needless to say, this is not the first (nor the last) time that such doomsday scenarios were conjured. Today, fears of machines endowed with artificial intelligence abound, but the fear doesn&#39;t stem from the specter of evolving machines, but rather from an imagined existential threat posed by designed ones: the super-intelligence that will eliminate us all. Ironically, this fear seems to grow in sync with the realization that, with all the machine learning prowess being unleashed on problem after problem in disparate fields of study, we are still at least a generation away from designing even a semblance of general artificial intelligence that might give us pause.In a way, we know exactly why this goal remains far away, rather than getting closer with every passing generation. We know that at the heart of our sentience—and by extension the sentience of any animal—lies the brain: arguably the most complex organ on Earth. Given that attempts to circumvent building artificial brains by designing rule-based systems have utterly failed, the hope is that reverse-engineering the brain, or at least its central cognitive mechanisms, is the path that will ultimately lead to the sentient machines that we have been promised (and learned to fear) in countless movies. The Defense Advanced Research Projects Agency (DARPA) in the United States, for example, funded the SyNAPSE program to the tune of 42.1 million US dollars (SyNAPSE stood for systems of neuromorphic adaptive plastic scalable electronics) from 2008 to 2012, billed as “the quest to engineer the mind by reverse-engineering the brain” (Lohr, 2011, para. 10) only for the principal investigator of the project to later backtrack and proclaim: “We&#39;re not trying to replicate the brain. That&#39;s impossible. We don&#39;t know how the brain works, really” (Lohr, 2011, para. 12). In the end, the project produced a prototype chip with 256 neuron-like nodes, surrounded by over 262,000 synaptic memory modules. In the Almaden research lab at IBM, a computer running the chip has ultimately learned to play the video game Pong. Even more outrageous was the Human Brain Project led by Henry Markram, which (funded in 2013 by a one billion Euro grant from the European Commission) had the stated goal of simulating the entire human brain within 10 years. The project is considered by some to be an extraordinary failure (Yong, 2019).Do we then have to wait until we understand the brain to finally build one? Experience gained from Artificial Life suggests otherwise. While the field of Artificial Life is broad and interdisciplinary, one branch of Artificial Life has shown us that complex designs, sometimes beyond what can be imagined by human engineers, can emerge from evolution within the computer: the field of evolutionary computation (Miikkulainen &amp;amp; Forrest, 2021). This approach to the “automatic discovery” of design solutions to complex problems leverages Darwinian evolution as a sophisticated “search algorithm” that, over time, can lead to complex solutions by applying mutation, recombination, and selection, to a population of candidate solutions. Indeed, applying the Darwinian algorithm to the task of finding complex molecules that nature has yet to discover is now one of the most prominent approaches in molecule design, leading to the 2018 Nobel Prize awarded to Frances Arnold. Applying this principle to computing machinery, instead, can potentially remove the design problem that has vexed artificial intelligence researchers for so long. With evolutionary computation and Artificial Life we can evolve solutions to problems without first understanding the nature of the solution.But as opposed to the burgeoning fields of deep learning (Goodfellow et al., 2016; LeCun et al., 2015) and deep reinforcement learning (Mnih et al., 2015) that rely on a set of well-established substrates and techniques (such as artificial neural networks and backpropagation), neuroevolution is far more diverse, with implementations and substrates (i.e., realizations of computer architectures) that differ wildly. Furthermore, how the process of evolution interacts with the substrate (i.e., how information is encoded, how mutations act on the information, and how information is stored and transmitted within the artificial brain) depends on a number of design decisions that either directly or indirectly affect the performance of the brain. For neuroevolution to be the path to sentient machines, we must make progress in understanding all these elements. I will discuss some of these issues in nontechnical language, with the hope that it stimulates the research community to walk this path. To set the stage, I&#39;d like to give a brief (and highly conceptual) overview of how I perceive the history of research in artificial intelligence, in part because that history will allow us to understand the path that was taken and why some other paths were abandoned. Ultimately, that history should illuminate the path that is before us.The modern era of research (Figure 1) with the goal of creating sentient machines can be traced back to the Cybernetics movement founded by the mathematician Norbert Wiener, which began around the time of the publication of Wiener&#39;s book entitled Cybernetics: Or Control and Communication in the Animal and the Machine (Wiener, 1948). The Dartmouth conference that would launch the term artificial intelligence (AI) followed eight years later (Moor, 2006). As we can imagine, there was never agreement among the protagonists (neither among the Cybernetics disciples, nor among the key group that met at the Dartmouth conference) about the path that would lead to AI. The Cybernetics movement was heavily influenced by Wiener, who thought that intelligence could be found by understanding signal processing (e.g., filtering, Fourier analysis, and feedback control). In hindsight, I feel that the 1948 version of the book did little to advance the field, and reads more like a hodgepodge of unconnected observations. The chapter on self-reproduction in terms of nonlinear transducers added in 1961 is, in my opinion, particularly naive. Gradually, the focus of the Cybernetics group turned towards the neural network described earlier by McCulloch and Pitts (1943; McCulloch was one of the founding members of the Cybernetics movement) and away from the goal of creating sentient machines.The AI Revolution started in earnest after the Dartmouth conference in 1956. A significant amount of hype followed, with some of the central figures such as Marvin Minsky announcing in 1967 that “Within a generation … the problems of creating ‘artificial intelligence’ will be substantially solved” (Minsky, 1967. p. 2). When these predictions did not materialize, funding for AI research dried up, leading to a period known as the AI Winter. At this point in time, both algorithmic and symbolic approaches to AI as well as artificial neural networks (ANNs) were being pursued. The algorithmic/symbolic approach sought to use rule-based systems as well as search and natural language processing as the engine of intelligence, while the connectionist approach jettisoned the manipulation of symbols and rules for a black box that can approximate any function: the neural network. The neural approach had its roots in the McCulloch-Pitts automaton that gave rise to the perceptron: a single-layer neural network whose inputs are summed up with appropriate weights to form the output via a step function (Rosenblatt, 1958). While perceptrons were popular for a while, their importance waned when it was discovered that some simple functions such as the “exclusive OR” (XOR) logical operation could not be approximated by a single perceptron (even though XOR can be computed by combining three perceptrons) because XOR is not linearly separable (Minsky &amp;amp; Papert, 1969). Meanwhile, natural language processing approaches flourished, in part due to the success of Joseph Weizenbaum&#39;s chatbot ELIZA (Weizenbaum, 1966).The connectionist approach to AI was resurrected, however, with the rise of multi-layered networks favored by the Parallel Distributed Processing (PDP) group headed by James McClelland and David Rumelhart (Rumelhart et al., 1986), a rise that also intensified the debate between the two “schools” of AI. The adherents to the algorithmic/symbolic route argued that in order to have mental models and syntactical rules for data processing, the internal manipulation of symbols was necessary, while a connectionist would argue that symbolic manipulation is a “higher-order” effect that will ultimately emerge from simulating the low-level computations performed by neurons. The algorithmic/symbolic approach was, however, severely wounded when MIT Professor of Computer Science Rodney Brooks announced that creating internal models of the world within the “brains” of machines by programming them is the wrong approach, as these models should be automatically generated instead (Brooks, 1991). Brooks argued instead for bootstrapping intelligence by creating machines that are entirely reactive, that is, where action is connected to perception directly. In a sense, Brooks advocated for a return to perceptrons, only implemented on moving robots using his “subsumption architecture.” History tells us that the result was the Roomba, the vacuum cleaning robot sold by the company iRobot, founded by Brooks and two colleagues from MIT in 1990.Two very different approaches to AI emerged shortly after Brooks asserted that “explicit representations and models of the world simply get in the way” of AI (Brooks, 1991, p. 140). Ackley and Littman (1992) showed that neural networks can be trained via evolution, while Gruau (1994) developed an evolvable graph grammar that allowed him to generate neural networks via a developmental process. Furthermore, a different kind of neural network appeared on the scene: The long short-term memory (LSTM) networks were able to automatically create memories with ease (see the review Hochreiter &amp;amp; Schmidhuber, 1997). Clearly, the announcement of the demise of representations was premature.The world&#39;s attention became focused once again on AI after the spectacular win of IBM&#39;s Deep Blue chess computer against the reigning world champion Garry Kasparov in 1997 (Kasparov, 2017), a year after an earlier version of Deep Blue had lost against him. Deep Blue was a chess program squarely in the symbolic realm, exploring advances in search methods and taking advantage of the power of parallel computing. Beyond these, there was little innovation in Deep Blue. The logical follow-up to Deep Blue was IBM&#39;s Watson, which went on to win the popular question-and-answer game Jeopardy! on live television in 2011. Around this time, however, a Deep Learning Revolution (Sejnowski, 2018) was taking place, focusing on “deep” (multi-layer) feedforward neural networks trained via back-propagation, running on graphical processing units (GPUs). Of course, multi-layer feedforward networks had been around for decades, but hardware advances as well as algorithmic insights led to a re-evaluation of the power of neural networks, specifically in the realm of image classification (Ciresan et al., 2012; Krizhevsky et al., 2012), driven to a large extent by the CIFAR (Canadian Institute for Advanced Research) collaboration. The rise of these connectionist approaches coincided with the fall of the symbolic giants: the Watson architecture, billed by IBM as the future of automated medical diagnostics, failed to make inroads at hospitals (Wachter, 2015). In a story reminiscent of the failure of IBM&#39;s neuromorphic chip, IBM&#39;s vice president of health care and life sciences research finally admitted that “Diagnosis is not the place to go. That&#39;s something the experts do pretty well. It&#39;s a hard task, and no matter how well you do it with AI, it&#39;s not going to displace the expert practitioner” (Strickland, 2019, p. 29).As of this writing, it appears as if the connectionists have the upper hand. The spectacular defeat of Go grandmaster Lee Sedol at the hands (or should we say, the artificial brain) of AlphaGo cemented the ascendance of the Deep Q Networks. AlphaGo was developed by DeepMind Technologies led by Demis Hassabis, and later acquired by Google. In November of 2020, DeepMind announced that its program AlphaFold had won the 13th Critical Assessment of Techniques for Protein Structure Prediction (CASP) competition, by correctly predicting the structure of 25 out of 43 proteins using the amino acid sequence only. Predicting protein structure from sequence data alone has long been hailed as the holy grail of biology (Callaway, 2020). However, there are significant doubts that deep networks are the path to artificial general intelligence, that is, the path to sentient machines. Indeed, it has now become clear that the stupendous image classification ability of deep networks is vulnerable to deception. For example, a team led by Jeff Clune showed that it was possible to evolve images that would fool deep networks to misclassify images while claiming high confidence in their judgment (Nguyen et al., 2015). In many cases, the misclassified images look nothing like what the AI claims it could see, looking instead to the human eye like static noise. Jason Jo and Yoshua Bengio1 concluded that “The sensitivity of high performance CNNs [convolutional neural networks] to adversarial examples casts serious doubt that these networks are actually learning high level abstract concepts” (Jo &amp;amp; Bengio, 2017, p. 1). In other words, it looks like these networks have no idea what it is they are classifying.Where does this leave the quest for thinking machines? It is now becoming clear that deep networks do not actually “know” anything about the objects they are so successfully classifying, and that symbolic approaches won&#39;t work because it is not possible to simply render knowledge about the world in terms of a list of rules. It is also becoming clear that the crucial element that is missing, both from the symbolic as well as the connectionist approaches, is the ability to automatically generate, within the brain, accurate representations of the worlds within which those brains are to function. Algorithmic approaches to programming these worlds are doomed to fail because (as Brooks, 1991, correctly diagnosed) in those approaches the task of abstraction is left to humans attempting to capture the world in code, when the brain should be doing that itself. Deep networks, on the contrary, do not even attempt to build those models, and instead simply “fit” the statistical features that appear in the retina. How can this dilemma be resolved?A look at human brain evolution and anatomy might give us a clue. Figure 2 gives us a schematic view of the neocortex, the largest, and from an evolutionary perspective most recent, part of the mammalian cerebral cortex. In the very back of your head is the visual cortex, which processes information sent to it from the eyes&#39; retinas. According to the “two streams” hypothesis (Goodale &amp;amp; Milner, 1992, 2004), this information is processed (and then relayed to the motor cortex to initiate behavior) via two different pathways: the dorsal “where” pathway and the ventral “what” pathway. The dorsal (on the “back” of the brain) stream is evolutionarily ancient, and processes spatial information and movement. It is fast and allows us to quickly react to changes. For example, if you stand under an overhang and a rock falls, you might catch it from the corner of your eye and, using the dorsal pathway, move away quickly. But you won&#39;t know what it was that fell. Instead, an analysis of the rock that landed next to you using the ventral (on the brain&#39;s “belly”) pathway that is evolutionarily much more recent (but also slower due to recurrent connections), would tell you that this was a rock—and allow you to infer that in this context there might be other rocks loose above you, leading to the decision to move to a different, safer location.The ventral pathway is slow because of pervasive back-connections through the layers of the cortex. It accesses long-term memory: stored representations of the world that allow you to interpret visual (and auditory) information contextually. In that way, the ventral pathway is akin to the algorithmic/symbolic approaches to AI: They attempt to make sense of the world using rules and models, at the expense of speed. The ancient dorsal pathway is instead reminiscent of the fast, feedforward connectionist architectures that do not rely on stored representations.How did these stored representations come to exist in the ventral pathway&#39;s neurons? We know the answer to this question: They arrived there by way of evolution (the instinctive models), as well as reinforcement learning during a lifetime. If we posit that these internal models cannot be programmed, then it seems as if the only alternative is to use Darwinian evolution and lifetime learning to allow these representations to slowly emerge over time.As I indicated in the prologue, I believe that the tool to achieve this is neuroevolution. In the next installments of this Column, we will look at what is necessary to successfully navigate this path towards sentient machines. First up, we will discuss the structure of networks (standard ones compared to those used in neuroevolution), as well as the methods to optimize them.},
  archive      = {J_ALJ},
  author       = {Adami, Christoph},
  doi          = {10.1162/artl_a_00349},
  journal      = {Artificial Life},
  number       = {2},
  pages        = {131-137},
  shortjournal = {Artif. Life},
  title        = {A brief history of artificial intelligence research},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Network-based phase space analysis of the el farol bar
problem. <em>ALJ</em>, <em>27</em>(2), 113–130. (<a
href="https://doi.org/10.1162/artl_a_00347">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The El Farol Bar problem highlights the issue of bounded rationality through a coordination problem where agents must decide individually whether or not to attend a bar without prior communication. Each agent is provided a set of attendance predictors (or decision-making strategies) and uses the previous bar attendances to guess bar attendance for a given week to determine if the bar is worth attending. We previously showed how the distribution of used strategies among the population settles into an attractor by using a spatial phase space. However, this approach was limited as it required N − 1 dimensions to fully visualize the phase space of the problem, where N is the number of strategies available. Here we propose a new approach to phase space visualization and analysis by converting the strategy dynamics into a state transition network centered on strategy distributions. The resulting weighted, directed network gives a clearer representation of the strategy dynamics once we define an attractor of the strategy phase space as a sink-strongly connected component. This enables us to study the resulting network to draw conclusions about the performance of the different strategies. We find that this approach not only is applicable to the El Farol Bar problem, but also addresses the dimensionality issue and is theoretically applicable to a wide variety of discretized complex systems.},
  archive      = {J_ALJ},
  author       = {St. Luce, Shane and Sayama, Hiroki},
  doi          = {10.1162/artl_a_00347},
  journal      = {Artificial Life},
  number       = {2},
  pages        = {113-130},
  shortjournal = {Artif. Life},
  title        = {Network-based phase space analysis of the el farol bar problem},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Life worth mentioning: Complexity in life-like cellular
automata. <em>ALJ</em>, <em>27</em>(2), 105–112. (<a
href="https://doi.org/10.1162/artl_a_00348">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cellular automata (CA) have been lauded for their ability to generate complex global patterns from simple local rules. The late English mathematician, John Horton Conway, developed his illustrious Game of Life (Life) CA in 1970, which has since remained one of the most quintessential CA constructions—capable of producing a myriad of complex dynamic patterns and computational universality. Life and several other Life-like rules have been classified in the same group of aesthetically and dynamically interesting CA rules characterized by their complex behaviors. However, a rigorous quantitative comparison among similarly classified Life-like rules has not yet been fully established. Here we show that Life is capable of maintaining as much complexity as similar rules while remaining the most parsimonious. In other words, Life contains a consistent amount of complexity throughout its evolution, with the least number of rule conditions compared to other Life-like rules. We also found that the complexity of higher density Life-like rules, which themselves contain the Life rule as a subset, form a distinct concave density-complexity relationship whereby an optimal complexity candidate is proposed. Our results also support the notion that Life functions as the basic ingredient for cultivating the balance between structure and randomness to maintain complexity in 2D CA for low- and high-density regimes, especially over many iterations. This work highlights the genius of John Horton Conway and serves as a testament to his timeless marvel, which is referred to simply as: Life.},
  archive      = {J_ALJ},
  author       = {Peña, Eric and Sayama, Hiroki},
  doi          = {10.1162/artl_a_00348},
  journal      = {Artificial Life},
  number       = {2},
  pages        = {105-112},
  shortjournal = {Artif. Life},
  title        = {Life worth mentioning: Complexity in life-like cellular automata},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A comprehensive conceptual and computational dynamics
framework for autonomous regeneration systems. <em>ALJ</em>,
<em>27</em>(2), 80–104. (<a
href="https://doi.org/10.1162/artl_a_00343">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many biological organisms regenerate structure and function after damage. Despite the long history of research on molecular mechanisms, many questions remain about algorithms by which cells can cooperate towards the same invariant morphogenetic outcomes. Therefore, conceptual frameworks are needed not only for motivating hypotheses for advancing the understanding of regeneration processes in living organisms, but also for regenerative medicine and synthetic biology. Inspired by planarian regeneration, this study offers a novel generic conceptual framework that hypothesizes mechanisms and algorithms by which cell collectives may internally represent an anatomical target morphology towards which they build after damage. Further, the framework contributes a novel nature-inspired computing method for self-repair in engineering and robotics. Our framework, based on past in vivo and in silico studies on planaria, hypothesizes efficient novel mechanisms and algorithms to achieve complete and accurate regeneration of a simple in silico flatwormlike organism from any damage, much like the body-wide immortality of planaria, with minimal information and algorithmic complexity. This framework that extends our previous circular tissue repair model integrates two levels of organization: tissue and organism. In Level 1, three individual in silico tissues (head, body, and tail—each with a large number of tissue cells and a single stem cell at the centre) repair themselves through efficient local communications. Here, the contribution extends our circular tissue model to other shapes and invests them with tissue-wide immortality through an information field holding the minimum body plan. In Level 2, individual tissues combine to form a simple organism. Specifically, the three stem cells form a network that coordinates organism-wide regeneration with the help of Level 1. Here we contribute novel concepts for collective decision-making by stem cells for stem cell regeneration and large-scale recovery. Both levels (tissue cells and stem cells) represent networks that perform simple neural computations and form a feedback control system. With simple and limited cellular computations, our framework minimises computation and algorithmic complexity to achieve complete recovery. We report results from computer simulations of the framework to demonstrate its robustness in recovering the organism after any injury. This comprehensive hypothetical framework that significantly extends the existing biological regeneration models offers a new way to conceptualise the information-processing aspects of regeneration, which may also help design living and non-living self-repairing agents.},
  archive      = {J_ALJ},
  author       = {Minh-Thai, Tran Nguyen and Samarasinghe, Sandhya and Levin, Michael},
  doi          = {10.1162/artl_a_00343},
  journal      = {Artificial Life},
  number       = {2},
  pages        = {80-104},
  shortjournal = {Artif. Life},
  title        = {A comprehensive conceptual and computational dynamics framework for autonomous regeneration systems},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Are artificial dendrites useful in neuro-evolution?
<em>ALJ</em>, <em>27</em>(2), 75–79. (<a
href="https://doi.org/10.1162/artl_a_00338">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The significant role of dendritic processing within neuronal networks has become increasingly clear. This letter explores the effects of including a simple dendrite-inspired mechanism into neuro-evolution. The phenomenon of separate dendrite activation thresholds on connections is allowed to emerge under an evolutionary process. It is shown how such processing can be positively selected for, particularly for connections between the hidden and output layers, and increases performance.},
  archive      = {J_ALJ},
  author       = {Bull, Larry},
  doi          = {10.1162/artl_a_00338},
  journal      = {Artificial Life},
  number       = {2},
  pages        = {75-79},
  shortjournal = {Artif. Life},
  title        = {Are artificial dendrites useful in neuro-evolution?},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Editorial: News from the new co-editors in chief.
<em>ALJ</em>, <em>27</em>(2), 73–74. (<a
href="https://doi.org/10.1162/artl_e_00350">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It has been almost 20 years, and 20 volumes, since Mark Bedau took on the role of Editor in Chief of Artificial Life. After diligently maintaining the journal day-to-day and steering it strategically over the long term, Mark has decided to retire from this role. He was instrumental in cementing the Artificial Life journal as a dependable and valuable publication venue for research in our field. But also, during his lengthy term, Mark was willing to trust would-be guest editors with the independence to manage their own special issues. He remained consistently keen to innovate and to explore new directions for the journal that capitalised on our field’s inherent multidisciplinarity, curiosity, and diversity.Mark also led ISAL (International Society for Artificial Life) during the years 2001–2015. In the mid-2000s, he was also Chief Operating Officer of ProtoLife SRL, a Venice-based company investigating the possibilities of wet artificial life, and he co-founded the European Center for Living Technology. During this period, he continued to publish actively himself on themes of emergence, open-ended evolution, and the philosophy of Artificial (and natural) Life, all the while continuing to edit the journal. And he has been part of the organising committee of multiple ALIFE conferences, too. We, the incoming Co-Editors in Chief, Susan Stepney and Alan Dorin, and the journal’s Editorial Assistant, Lin Reedijk, are sure you will join us in thanking Mark for such dedication, and for the valuable contributions he has made not only to the journal but more widely to our field and community. We wish him all the best in his “pseudo-retirement”—no sooner had Mark conducted a smooth handover of the journal’s reins to us, and seen us sign contracts with MIT Press for its custodianship, than he was back on Zoom proposing to guest-edit a special issue! It is terrific to see that Mark’s commitment and support for Artificial Life research hasn’t waned. We look forward to working with him on many exciting projects in the future, to reading his articles in the journal, and to hearing about his activities at future ALIFE conferences.So we, Susan Stepney and Alan Dorin, are taking over as incoming Co-Editors in Chief. Two editors are needed to replace Mark, and to provide the breadth that our field now covers. Our aim is for the journal to continue to provide the ISAL community and all Artificial Life researchers with a natural outlet for their research, and for it to continue to keep pace with developments in publishing and science communication. To this end, we have already been working with Mark and the Associate Editors to trial some new management processes behind the scenes, to reach out and expand our audience via social media, to engage with the ALIFE conferences directly by providing speaking slots for journal article authors, to recognise and thank the reviewers for their contributions by giving an annual award for the best journal article review, and to offer “online early” access to articles before their final electronic publication. Also, in this issue we introduce a new multi-article column, “The Evolutionary Path to Sentient Machines”, authored by long-time Artificial Life contributor, Chris Adami. As these changes bed in, we will continue Mark’s tradition of innovation and further exploit the potential of web-based publication.Thank you, Mark, for all your hard work and dedication to the field, and we hope that we will be worthy successors.},
  archive      = {J_ALJ},
  author       = {Dorin, Alan and Stepney, Susan},
  doi          = {10.1162/artl_e_00350},
  journal      = {Artificial Life},
  number       = {2},
  pages        = {73-74},
  shortjournal = {Artif. Life},
  title        = {Editorial: News from the new co-editors in chief},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Measuring behavioral similarity of cellular automata.
<em>ALJ</em>, <em>27</em>(1), 62–71. (<a
href="https://doi.org/10.1162/artl_a_00337">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conway&#39;s Game of Life is the best-known cellular automaton. It is a classic model of emergence and self-organization, it is Turing-complete, and it can simulate a universal constructor. The Game of Life belongs to the set of semi-totalistic cellular automata, a family with 262,144 members. Many of these automata may deserve as much attention as the Game of Life, if not more. The challenge we address here is to provide a structure for organizing this large family, to make it easier to find interesting automata, and to understand the relations between automata. Packard and Wolfram ( 1985 ) divided the family into four classes, based on the observed behaviors of the rules. Eppstein ( 2010 ) proposed an alternative four-class system, based on the forms of the rules. Instead of a class-based organization, we propose a continuous high-dimensional vector space, where each automaton is represented by a point in the space. The distance between two automata in this space corresponds to the differences in their behavioral characteristics. Nearest neighbors in the space have similar behaviors. This space should make it easier for researchers to see the structure of the family of semi-totalistic rules and to find the hidden gems in the family.},
  archive      = {J_ALJ},
  author       = {Turney, Peter D.},
  doi          = {10.1162/artl_a_00337},
  journal      = {Artificial Life},
  number       = {1},
  pages        = {62-71},
  shortjournal = {Artif. Life},
  title        = {Measuring behavioral similarity of cellular automata},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The impossibility of automating ambiguity. <em>ALJ</em>,
<em>27</em>(1), 44–61. (<a
href="https://doi.org/10.1162/artl_a_00336">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {On the one hand, complexity science and enactive and embodied cognitive science approaches emphasize that people, as complex adaptive systems, are ambiguous, indeterminable, and inherently unpredictable. On the other, Machine Learning (ML) systems that claim to predict human behaviour are becoming ubiquitous in all spheres of social life. I contend that ubiquitous Artificial Intelligence (AI) and ML systems are close descendants of the Cartesian and Newtonian worldview in so far as they are tools that fundamentally sort, categorize, and classify the world, and forecast the future. Through the practice of clustering, sorting, and predicting human behaviour and action, these systems impose order, equilibrium, and stability to the active, fluid, messy, and unpredictable nature of human behaviour and the social world at large. Grounded in complexity science and enactive and embodied cognitive science approaches, this article emphasizes why people, embedded in social systems, are indeterminable and unpredictable. When ML systems “pick up” patterns and clusters, this often amounts to identifying historically and socially held norms, conventions, and stereotypes. Machine prediction of social behaviour, I argue, is not only erroneous but also presents real harm to those at the margins of society.},
  archive      = {J_ALJ},
  author       = {Birhane, Abeba},
  doi          = {10.1162/artl_a_00336},
  journal      = {Artificial Life},
  number       = {1},
  pages        = {44-61},
  shortjournal = {Artif. Life},
  title        = {The impossibility of automating ambiguity},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Evolution of autopoiesis and multicellularity in the game
of life. <em>ALJ</em>, <em>27</em>(1), 26–43. (<a
href="https://doi.org/10.1162/artl_a_00334">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently we introduced a model of symbiosis, Model-S , based on the evolution of seed patterns in Conway&#39;s Game of Life. In the model, the fitness of a seed pattern is measured by one-on-one competitions in the Immigration Game, a two-player variation of the Game of Life. Our previous article showed that Model-S can serve as a highly abstract, simplified model of biological life: (1) The initial seed pattern is analogous to a genome. (2) The changes as the game runs are analogous to the development of the phenome. (3) Tournament selection in Model-S is analogous to natural selection in biology. (4) The Immigration Game in Model-S is analogous to competition in biology. (5) The first three layers in Model-S are analogous to biological reproduction. (6) The fusion of seed patterns in Model-S is analogous to symbiosis. The current article takes this analogy two steps further: (7) Autopoietic structures in the Game of Life ( still lifes , oscillators , and spaceships —collectively known as ashes ) are analogous to cells in biology. (8) The seed patterns in the Game of Life give rise to multiple, diverse, cooperating autopoietic structures, analogous to multicellular biological life. We use the apgsearch software (Ash Pattern Generator Search), developed by Adam Goucher for the study of ashes, to analyze autopoiesis and multicellularity in Model-S. We find that the fitness of evolved seed patterns in Model-S is highly correlated with the diversity and quantity of multicellular autopoietic structures.},
  archive      = {J_ALJ},
  author       = {Turney, Peter D.},
  doi          = {10.1162/artl_a_00334},
  journal      = {Artificial Life},
  number       = {1},
  pages        = {26-43},
  shortjournal = {Artif. Life},
  title        = {Evolution of autopoiesis and multicellularity in the game of life},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). On the emergence of intersexual selection: Arbitrary trait
preference improves female-male coevolution. <em>ALJ</em>,
<em>27</em>(1), 15–25. (<a
href="https://doi.org/10.1162/artl_a_00335">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sexual selection is a fundamental aspect of evolution for all eukaryotic organisms with mating types. This article suggests intersexual selection is best viewed as a mechanism with which to compensate for the unavoidable dynamics of coevolution between sexes that emerge with isogamy. Using the NKCS model it is shown by varying fitness landscape size, ruggedness, and connectedness, how a purely arbitrary trait preference sexual selection mechanism proves beneficial with high dependence between the sexes. This is found to be the case whether one or both sexes exploit such intersexual selection.},
  archive      = {J_ALJ},
  author       = {Bull, Larry},
  doi          = {10.1162/artl_a_00335},
  journal      = {Artificial Life},
  number       = {1},
  pages        = {15-25},
  shortjournal = {Artif. Life},
  title        = {On the emergence of intersexual selection: Arbitrary trait preference improves female-male coevolution},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The expected number of viable autocatalytic sets in chemical
reaction systems. <em>ALJ</em>, <em>27</em>(1), 1–14. (<a
href="https://doi.org/10.1162/artl_a_00333">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The emergence of self-sustaining autocatalytic networks in chemical reaction systems has been studied as a possible mechanism for modeling how living systems first arose. It has been known for several decades that such networks will form within systems of polymers (under cleavage and ligation reactions) under a simple process of random catalysis, and this process has since been mathematically analyzed. In this paper, we provide an exact expression for the expected number of self-sustaining autocatalytic networks that will form in a general chemical reaction system, and the expected number of these networks that will also be uninhibited (by some molecule produced by the system). Using these equations, we are able to describe the patterns of catalysis and inhibition that maximize or minimize the expected number of such networks. We apply our results to derive a general theorem concerning the trade-off between catalysis and inhibition, and to provide some insight into the extent to which the expected number of self-sustaining autocatalytic networks coincides with the probability that at least one such system is present.},
  archive      = {J_ALJ},
  author       = {Kauffman, Stuart and Steel, Mike},
  doi          = {10.1162/artl_a_00333},
  journal      = {Artificial Life},
  number       = {1},
  pages        = {1-14},
  shortjournal = {Artif. Life},
  title        = {The expected number of viable autocatalytic sets in chemical reaction systems},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
</ul>

</body>
</html>
