<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TACL_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tacl---93">TACL - 93</h2>
<ul>
<li><details>
<summary>
(2021). Word representation learning in multimodal pre-trained
transformers: An intrinsic evaluation. <em>TACL</em>, <em>9</em>,
1563–1579. (<a href="https://doi.org/10.1162/tacl_a_00443">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. This study carries out a systematic intrinsic evaluation of the semantic representations learned by state-of-the-art pre-trained multimodal Transformers. These representations are claimed to be task-agnostic and shown to help on many downstream language-and-vision tasks. However, the extent to which they align with human semantic intuitions remains unclear. We experiment with various models and obtain static word representations from the contextualized ones they learn. We then evaluate them against the semantic judgments provided by human speakers. In line with previous evidence, we observe a generalized advantage of multimodal representations over language- only ones on concrete word pairs, but not on abstract ones. On the one hand, this confirms the effectiveness of these models to align language and vision, which results in better semantic representations for concepts that are grounded in images. On the other hand, models are shown to follow different representation learning patterns, which sheds some light on how and when they perform multimodal integration.},
  archive      = {J_TACL},
  author       = {Pezzelle, Sandro and Takmaz, Ece and Fernández, Raquel},
  doi          = {10.1162/tacl_a_00443},
  journal      = {Transactions of the Association for Computational Linguistics},
  pages        = {1563-1579},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Word representation learning in multimodal pre-trained transformers: An intrinsic evaluation},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Idiomatic expression identification using semantic
compatibility. <em>TACL</em>, <em>9</em>, 1546–1562. (<a
href="https://doi.org/10.1162/tacl_a_00442">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Idiomatic expressions are an integral part of natural language and constantly being added to a language. Owing to their non-compositionality and their ability to take on a figurative or literal meaning depending on the sentential context, they have been a classical challenge for NLP systems. To address this challenge, we study the task of detecting whether a sentence has an idiomatic expression and localizing it when it occurs in a figurative sense. Prior research for this task has studied specific classes of idiomatic expressions offering limited views of their generalizability to new idioms. We propose a multi-stage neural architecture with attention flow as a solution. The network effectively fuses contextual and lexical information at different levels using word and sub-word representations. Empirical evaluations on three of the largest benchmark datasets with idiomatic expressions of varied syntactic patterns and degrees of non-compositionality show that our proposed model achieves new state-of-the-art results. A salient feature of the model is its ability to identify idioms unseen during training with gains from 1.4\% to 30.8\% over competitive baselines on the largest dataset.},
  archive      = {J_TACL},
  author       = {Zeng, Ziheng and Bhat, Suma},
  doi          = {10.1162/tacl_a_00442},
  journal      = {Transactions of the Association for Computational Linguistics},
  pages        = {1546-1562},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Idiomatic expression identification using semantic compatibility},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Quantifying cognitive factors in lexical decline.
<em>TACL</em>, <em>9</em>, 1529–1545. (<a
href="https://doi.org/10.1162/tacl_a_00441">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We adopt an evolutionary view on language change in which cognitive factors (in addition to social ones) affect the fitness of words and their success in the linguistic ecosystem. Specifically, we propose a variety of psycholinguistic factors—semantic, distributional, and phonological—that we hypothesize are predictive of lexical decline, in which words greatly decrease in frequency over time. Using historical data across three languages (English, French, and German), we find that most of our proposed factors show a significant difference in the expected direction between each curated set of declining words and their matched stable words. Moreover, logistic regression analyses show that semantic and distributional factors are significant in predicting declining words. Further diachronic analysis reveals that declining words tend to decrease in the diversity of their lexical contexts over time, gradually narrowing their ‘ecological niches’.},
  archive      = {J_TACL},
  author       = {Francis, David and Rabinovich, Ella and Samir, Farhan and Mortensen, David and Stevenson, Suzanne},
  doi          = {10.1162/tacl_a_00441},
  journal      = {Transactions of the Association for Computational Linguistics},
  pages        = {1529-1545},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Quantifying cognitive factors in lexical decline},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Explanation-based human debugging of NLP models: A survey.
<em>TACL</em>, <em>9</em>, 1508–1528. (<a
href="https://doi.org/10.1162/tacl_a_00440">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Debugging a machine learning model is hard since the bug usually involves the training data and the learning process. This becomes even harder for an opaque deep learning model if we have no clue about how the model actually works. In this survey, we review papers that exploit explanations to enable humans to give feedback and debug NLP models. We call this problem explanation-based human debugging (EBHD). In particular, we categorize and discuss existing work along three dimensions of EBHD (the bug context, the workflow, and the experimental setting), compile findings on how EBHD components affect the feedback providers, and highlight open problems that could be future research directions.},
  archive      = {J_TACL},
  author       = {Lertvittayakumjorn, Piyawat and Toni, Francesca},
  doi          = {10.1162/tacl_a_00440},
  journal      = {Transactions of the Association for Computational Linguistics},
  pages        = {1508-1528},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Explanation-based human debugging of NLP models: A survey},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Instance-based neural dependency parsing. <em>TACL</em>,
<em>9</em>, 1493–1507. (<a
href="https://doi.org/10.1162/tacl_a_00439">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Interpretable rationales for model predictions are crucial in practical applications. We develop neural models that possess an interpretable inference process for dependency parsing. Our models adopt instance-based inference, where dependency edges are extracted and labeled by comparing them to edges in a training set. The training edges are explicitly used for the predictions; thus, it is easy to grasp the contribution of each edge to the predictions. Our experiments show that our instance-based models achieve competitive accuracy with standard neural models and have the reasonable plausibility of instance-based explanations.},
  archive      = {J_TACL},
  author       = {Ouchi, Hiroki and Suzuki, Jun and Kobayashi, Sosuke and Yokoi, Sho and Kuribayashi, Tatsuki and Yoshikawa, Masashi and Inui, Kentaro},
  doi          = {10.1162/tacl_a_00439},
  journal      = {Transactions of the Association for Computational Linguistics},
  pages        = {1493-1507},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Instance-based neural dependency parsing},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Planning with learned entity prompts for abstractive
summarization. <em>TACL</em>, <em>9</em>, 1475–1492. (<a
href="https://doi.org/10.1162/tacl_a_00438">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We introduce a simple but flexible mechanism to learn an intermediate plan to ground the generation of abstractive summaries. Specifically, we prepend (or prompt) target summaries with entity chains—ordered sequences of entities mentioned in the summary. Transformer-based sequence-to-sequence models are then trained to generate the entity chain and then continue generating the summary conditioned on the entity chain and the input. We experimented with both pretraining and finetuning with this content planning objective. When evaluated on CNN/DailyMail, XSum, SAMSum, and BillSum, we demonstrate empirically that the grounded generation with the planning objective improves entity specificity and planning in summaries for all datasets, and achieves state-of-the-art performance on XSum and SAMSum in terms of rouge. Moreover, we demonstrate empirically that planning with entity chains provides a mechanism to control hallucinations in abstractive summaries. By prompting the decoder with a modified content plan that drops hallucinated entities, we outperform state-of-the-art approaches for faithfulness when evaluated automatically and by humans.},
  archive      = {J_TACL},
  author       = {Narayan, Shashi and Zhao, Yao and Maynez, Joshua and Simões, Gonçalo and Nikolaev, Vitaly and McDonald, Ryan},
  doi          = {10.1162/tacl_a_00438},
  journal      = {Transactions of the Association for Computational Linguistics},
  pages        = {1475-1492},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Planning with learned entity prompts for abstractive summarization},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Experts, errors, and context: A large-scale study of human
evaluation for machine translation. <em>TACL</em>, <em>9</em>,
1460–1474. (<a href="https://doi.org/10.1162/tacl_a_00437">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Human evaluation of modern high-quality machine translation systems is a difficult problem, and there is increasing evidence that inadequate evaluation procedures can lead to erroneous conclusions. While there has been considerable research on human evaluation, the field still lacks a commonly accepted standard procedure. As a step toward this goal, we propose an evaluation methodology grounded in explicit error analysis, based on the Multidimensional Quality Metrics (MQM) framework. We carry out the largest MQM research study to date, scoring the outputs of top systems from the WMT 2020 shared task in two language pairs using annotations provided by professional translators with access to full document context. We analyze the resulting data extensively, finding among other results a substantially different ranking of evaluated systems from the one established by the WMT crowd workers, exhibiting a clear preference for human over machine output. Surprisingly, we also find that automatic metrics based on pre-trained embeddings can outperform human crowd workers. We make our corpus publicly available for further research.},
  archive      = {J_TACL},
  author       = {Freitag, Markus and Foster, George and Grangier, David and Ratnakar, Viresh and Tan, Qijun and Macherey, Wolfgang},
  doi          = {10.1162/tacl_a_00437},
  journal      = {Transactions of the Association for Computational Linguistics},
  pages        = {1460-1474},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Experts, errors, and context: A large-scale study of human evaluation for machine translation},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Differentiable subset pruning of transformer heads.
<em>TACL</em>, <em>9</em>, 1442–1459. (<a
href="https://doi.org/10.1162/tacl_a_00436">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Multi-head attention, a collection of several attention mechanisms that independently attend to different parts of the input, is the key ingredient in the Transformer. Recent work has shown, however, that a large proportion of the heads in a Transformer’s multi-head attention mechanism can be safely pruned away without significantly harming the performance of the model; such pruning leads to models that are noticeably smaller and faster in practice. Our work introduces a new head pruning technique that we term differentiable subset pruning. ntuitively, our method learns per- head importance variables and then enforces a user-specified hard constraint on the number of unpruned heads. he importance variables are learned via stochastic gradient descent. e conduct experiments on natural language inference and machine translation; we show that differentiable subset pruning performs comparably or better than previous works while offering precise control of the sparsity level.1},
  archive      = {J_TACL},
  author       = {Li, Jiaoda and Cotterell, Ryan and Sachan, Mrinmaya},
  doi          = {10.1162/tacl_a_00436},
  journal      = {Transactions of the Association for Computational Linguistics},
  pages        = {1442-1459},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Differentiable subset pruning of transformer heads},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Weisfeiler-leman in the bamboo: Novel AMR graph metrics and
a benchmark for AMR graph similarity. <em>TACL</em>, <em>9</em>,
1425–1441. (<a href="https://doi.org/10.1162/tacl_a_00435">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Several metrics have been proposed for assessing the similarity of (abstract) meaning representations (AMRs), but little is known about how they relate to human similarity ratings. Moreover, the current metrics have complementary strengths and weaknesses: Some emphasize speed, while others make the alignment of graph structures explicit, at the price of a costly alignment step.In this work we propose new Weisfeiler-Leman AMR similarity metrics that unify the strengths of previous metrics, while mitigating their weaknesses. Specifically, our new metrics are able to match contextualized substructures and induce n:m alignments between their nodes. Furthermore, we introduce a Benchmark for AMR Metrics based on Overt Objectives (Bamboo), the first benchmark to support empirical assessment of graph-based MR similarity metrics. Bamboo maximizes the interpretability of results by defining multiple overt objectives that range from sentence similarity objectives to stress tests that probe a metric’s robustness against meaning-altering and meaning- preserving graph transformations. We show the benefits of Bamboo by profiling previous metrics and our own metrics. Results indicate that our novel metrics may serve as a strong baseline for future work.},
  archive      = {J_TACL},
  author       = {Opitz, Juri and Daza, Angel and Frank, Anette},
  doi          = {10.1162/tacl_a_00435},
  journal      = {Transactions of the Association for Computational Linguistics},
  pages        = {1425-1441},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Weisfeiler-leman in the bamboo: Novel AMR graph metrics and a benchmark for AMR graph similarity},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Self-diagnosis and self-debiasing: A proposal for reducing
corpus-based bias in NLP. <em>TACL</em>, <em>9</em>, 1408–1424. (<a
href="https://doi.org/10.1162/tacl_a_00434">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. ⚠ This paper contains prompts and model outputs that are offensive in nature.When trained on large, unfiltered crawls from the Internet, language models pick up and reproduce all kinds of undesirable biases that can be found in the data: They often generate racist, sexist, violent, or otherwise toxic language. As large models require millions of training examples to achieve good performance, it is difficult to completely prevent them from being exposed to such content. In this paper, we first demonstrate a surprising finding: Pretrained language models recognize, to a considerable degree, their undesirable biases and the toxicity of the content they produce. We refer to this capability as self-diagnosis. Based on this finding, we then propose a decoding algorithm that, given only a textual description of the undesired behavior, reduces the probability of a language model producing problematic text. We refer to this approach as self-debiasing. Self-debiasing does not rely on manually curated word lists, nor does it require any training data or changes to the model’s parameters. While we by no means eliminate the issue of language models generating biased text, we believe our approach to be an important step in this direction.1},
  archive      = {J_TACL},
  author       = {Schick, Timo and Udupa, Sahana and Schütze, Hinrich},
  doi          = {10.1162/tacl_a_00434},
  journal      = {Transactions of the Association for Computational Linguistics},
  pages        = {1408-1424},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Self-diagnosis and self-debiasing: A proposal for reducing corpus-based bias in NLP},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Erratum: Measuring and improving consistency in pretrained
language models. <em>TACL</em>, <em>9</em>, 1407. (<a
href="https://doi.org/10.1162/tacl_x_00455">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. During production of this paper, an error was introduced to the formula on the bottom of the right column of page 1020. In the last two terms of the formula, the n and m subscripts were swapped. The correct formula is:Lc=∑n=1k∑m=n+1kDKL(Qnri∥Qmri)+DKL(Qmri∥Qnri)The paper has been updated.},
  archive      = {J_TACL},
  author       = {Elazar, Yanai and Kassner, Nora and Ravfogel, Shauli and Ravichander, Abhilasha and Hovy, Eduard and Schütze, Hinrich and Goldberg, Yoav},
  doi          = {10.1162/tacl_x_00455},
  journal      = {Transactions of the Association for Computational Linguistics},
  pages        = {1407},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Erratum: Measuring and improving consistency in pretrained language models},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MKQA: A linguistically diverse benchmark for multilingual
open domain question answering. <em>TACL</em>, <em>9</em>, 1389–1406.
(<a href="https://doi.org/10.1162/tacl_a_00433">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Progress in cross-lingual modeling depends on challenging, realistic, and diverse evaluation sets. We introduce Multilingual Knowledge Questions and Answers (MKQA), an open- domain question answering evaluation set comprising 10k question-answer pairs aligned across 26 typologically diverse languages (260k question-answer pairs in total). Answers are based on heavily curated, language- independent data representation, making results comparable across languages and independent of language-specific passages. With 26 languages, this dataset supplies the widest range of languages to-date for evaluating question answering. We benchmark a variety of state- of-the-art methods and baselines for generative and extractive question answering, trained on Natural Questions, in zero shot and translation settings. Results indicate this dataset is challenging even in English, but especially in low-resource languages.1},
  archive      = {J_TACL},
  author       = {Longpre, Shayne and Lu, Yi and Daiber, Joachim},
  doi          = {10.1162/tacl_a_00433},
  journal      = {Transactions of the Association for Computational Linguistics},
  pages        = {1389-1406},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {MKQA: A linguistically diverse benchmark for multilingual open domain question answering},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A biologically plausible parser. <em>TACL</em>, <em>9</em>,
1374–1388. (<a href="https://doi.org/10.1162/tacl_a_00432">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We describe a parser of English effectuated by biologically plausible neurons and synapses, and implemented through the Assembly Calculus, a recently proposed computational framework for cognitive function. We demonstrate that this device is capable of correctly parsing reasonably nontrivial sentences.1 While our experiments entail rather simple sentences in English, our results suggest that the parser can be extended beyond what we have implemented, to several directions encompassing much of language. For example, we present a simple Russian version of the parser, and discuss how to handle recursion, embedding, and polysemy.},
  archive      = {J_TACL},
  author       = {Mitropolsky, Daniel and Collins, Michael J. and Papadimitriou, Christos H.},
  doi          = {10.1162/tacl_a_00432},
  journal      = {Transactions of the Association for Computational Linguistics},
  pages        = {1374-1388},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {A biologically plausible parser},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Model compression for domain adaptation through causal
effect estimation. <em>TACL</em>, <em>9</em>, 1355–1373. (<a
href="https://doi.org/10.1162/tacl_a_00431">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Recent improvements in the predictive quality of natural language processing systems are often dependent on a substantial increase in the number of model parameters. This has led to various attempts of compressing such models, but existing methods have not considered the differences in the predictive power of various model components or in the generalizability of the compressed models. To understand the connection between model compression and out-of-distribution generalization, we define the task of compressing language representation models such that they perform best in a domain adaptation setting. We choose to address this problem from a causal perspective, attempting to estimate the average treatment effect (ATE) of a model component, such as a single layer, on the model’s predictions. Our proposed ATE-guided Model Compression scheme (AMoC), generates many model candidates, differing by the model components that were removed. Then, we select the best candidate through a stepwise regression model that utilizes the ATE to predict the expected performance on the target domain. AMoC outperforms strong baselines on dozens of domain pairs across three text classification and sequence tagging tasks.1},
  archive      = {J_TACL},
  author       = {Rotman, Guy and Feder, Amir and Reichart, Roi},
  doi          = {10.1162/tacl_a_00431},
  journal      = {Transactions of the Association for Computational Linguistics},
  pages        = {1355-1373},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Model compression for domain adaptation through causal effect estimation},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On generative spoken language modeling from raw audio.
<em>TACL</em>, <em>9</em>, 1336–1354. (<a
href="https://doi.org/10.1162/tacl_a_00430">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We introduce Generative Spoken Language Modeling, the task of learning the acoustic and linguistic characteristics of a language from raw audio (no text, no labels), and a set of metrics to automatically evaluate the learned representations at acoustic and linguistic levels for both encoding and generation. We set up baseline systems consisting of a discrete speech encoder (returning pseudo-text units), a generative language model (trained on pseudo- text), and a speech decoder (generating a waveform from pseudo-text) all trained without supervision and validate the proposed metrics with human evaluation. Across 3 speech encoders (CPC, wav2vec 2.0, HuBERT), we find that the number of discrete units (50, 100, or 200) matters in a task-dependent and encoder- dependent way, and that some combinations approach text-based systems.1},
  archive      = {J_TACL},
  author       = {Lakhotia, Kushal and Kharitonov, Eugene and Hsu, Wei-Ning and Adi, Yossi and Polyak, Adam and Bolte, Benjamin and Nguyen, Tu-Anh and Copet, Jade and Baevski, Alexei and Mohamed, Abdelrahman and Dupoux, Emmanuel},
  doi          = {10.1162/tacl_a_00430},
  journal      = {Transactions of the Association for Computational Linguistics},
  pages        = {1336-1354},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {On generative spoken language modeling from raw audio},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Partially supervised named entity recognition via the
expected entity ratio loss. <em>TACL</em>, <em>9</em>, 1320–1335. (<a
href="https://doi.org/10.1162/tacl_a_00429">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We study learning named entity recognizers in the presence of missing entity annotations. We approach this setting as tagging with latent variables and propose a novel loss, the Expected Entity Ratio, to learn models in the presence of systematically missing tags. We show that our approach is both theoretically sound and empirically useful. Experimentally, we find that it meets or exceeds performance of strong and state-of-the-art baselines across a variety of languages, annotation scenarios, and amounts of labeled data. In particular, we find that it significantly outperforms the previous state-of-the-art methods from Mayhew et al. (2019) and Li et al. (2021) by +12.7 and +2.3 F1 score in a challenging setting with only 1,000 biased annotations, averaged across 7 datasets. We also show that, when combined with our approach, a novel sparse annotation scheme outperforms exhaustive annotation for modest annotation budgets.1},
  archive      = {J_TACL},
  author       = {Effland, Thomas and Collins, Michael},
  doi          = {10.1162/tacl_a_00429},
  journal      = {Transactions of the Association for Computational Linguistics},
  pages        = {1320-1335},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Partially supervised named entity recognition via the expected entity ratio loss},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Continual learning for grounded instruction generation by
observing human following behavior. <em>TACL</em>, <em>9</em>,
1303–1319. (<a href="https://doi.org/10.1162/tacl_a_00428">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We study continual learning for natural language instruction generation, by observing human users’ instruction execution. We focus on a collaborative scenario, where the system both acts and delegates tasks to human users using natural language. We compare user execution of generated instructions to the original system intent as an indication to the system’s success communicating its intent. We show how to use this signal to improve the system’s ability to generate instructions via contextual bandit learning. In interaction with real users, our system demonstrates dramatic improvements in its ability to generate language over time.},
  archive      = {J_TACL},
  author       = {Kojima, Noriyuki and Suhr, Alane and Artzi, Yoav},
  doi          = {10.1162/tacl_a_00428},
  journal      = {Transactions of the Association for Computational Linguistics},
  pages        = {1303-1319},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Continual learning for grounded instruction generation by observing human following behavior},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Lexically aware semi-supervised learning for OCR
post-correction. <em>TACL</em>, <em>9</em>, 1285–1302. (<a
href="https://doi.org/10.1162/tacl_a_00427">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Much of the existing linguistic data in many languages of the world is locked away in non- digitized books and documents. Optical character recognition (OCR) can be used to produce digitized text, and previous work has demonstrated the utility of neural post-correction methods that improve the results of general- purpose OCR systems on recognition of less- well-resourced languages. However, these methods rely on manually curated post- correction data, which are relatively scarce compared to the non-annotated raw images that need to be digitized. In this paper, we present a semi-supervised learning method that makes it possible to utilize these raw images to improve performance, specifically through the use of self-training, a technique where a model is iteratively trained on its own outputs. In addition, to enforce consistency in the recognized vocabulary, we introduce a lexically aware decoding method that augments the neural post-correction model with a count-based language model constructed from the recognized texts, implemented using weighted finite-state automata (WFSA) for efficient and effective decoding. Results on four endangered languages demonstrate the utility of the proposed method, with relative error reductions of 15\%–29\%, where we find the combination of self-training and lexically aware decoding essential for achieving consistent improvements.1},
  archive      = {J_TACL},
  author       = {Rijhwani, Shruti and Rosenblum, Daisy and Anastasopoulos, Antonios and Neubig, Graham},
  doi          = {10.1162/tacl_a_00427},
  journal      = {Transactions of the Association for Computational Linguistics},
  pages        = {1285-1302},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Lexically aware semi-supervised learning for OCR post-correction},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Structured self-supervised pretraining for commonsense
knowledge graph completion. <em>TACL</em>, <em>9</em>, 1268–1284. (<a
href="https://doi.org/10.1162/tacl_a_00426">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. To develop commonsense-grounded NLP applications, a comprehensive and accurate commonsense knowledge graph (CKG) is needed. It is time-consuming to manually construct CKGs and many research efforts have been devoted to the automatic construction of CKGs. Previous approaches focus on generating concepts that have direct and obvious relationships with existing concepts and lack an capability to generate unobvious concepts. In this work, we aim to bridge this gap. We propose a general graph-to-paths pretraining framework that leverages high-order structures in CKGs to capture high-order relationships between concepts. We instantiate this general framework to four special cases: long path, path-to-path, router, and graph-node-path. Experiments on two datasets demonstrate the effectiveness of our methods. The code will be released via the public GitHub repository.},
  archive      = {J_TACL},
  author       = {Huang, Jiayuan and Du, Yangkai and Tao, Shuting and Xu, Kun and Xie, Pengtao},
  doi          = {10.1162/tacl_a_00426},
  journal      = {Transactions of the Association for Computational Linguistics},
  pages        = {1268-1284},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Structured self-supervised pretraining for commonsense knowledge graph completion},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Quantifying social biases in NLP: A generalization and
empirical comparison of extrinsic fairness metrics. <em>TACL</em>,
<em>9</em>, 1249–1267. (<a
href="https://doi.org/10.1162/tacl_a_00425">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Measuring bias is key for better understanding and addressing unfairness in NLP/ML models. This is often done via fairness metrics, which quantify the differences in a model’s behaviour across a range of demographic groups. In this work, we shed more light on the differences and similarities between the fairness metrics used in NLP. First, we unify a broad range of existing metrics under three generalized fairness metrics, revealing the connections between them. Next, we carry out an extensive empirical comparison of existing metrics and demonstrate that the observed differences in bias measurement can be systematically explained via differences in parameter choices for our generalized metrics.},
  archive      = {J_TACL},
  author       = {Czarnowska, Paula and Vyas, Yogarshi and Shah, Kashif},
  doi          = {10.1162/tacl_a_00425},
  journal      = {Transactions of the Association for Computational Linguistics},
  pages        = {1249-1267},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Quantifying social biases in NLP: A generalization and empirical comparison of extrinsic fairness metrics},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On the difficulty of translating free-order case-marking
languages. <em>TACL</em>, <em>9</em>, 1233–1248. (<a
href="https://doi.org/10.1162/tacl_a_00424">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Identifying factors that make certain languages harder to model than others is essential to reach language equality in future Natural Language Processing technologies. Free-order case-marking languages, such as Russian, Latin, or Tamil, have proved more challenging than fixed-order languages for the tasks of syntactic parsing and subject-verb agreement prediction. In this work, we investigate whether this class of languages is also more difficult to translate by state-of-the-art Neural Machine Translation (NMT) models. Using a variety of synthetic languages and a newly introduced translation challenge set, we find that word order flexibility in the source language only leads to a very small loss of NMT quality, even though the core verb arguments become impossible to disambiguate in sentences without semantic cues. The latter issue is indeed solved by the addition of case marking. However, in medium- and low-resource settings, the overall NMT quality of fixed-order languages remains unmatched.},
  archive      = {J_TACL},
  author       = {Bisazza, Arianna and Üstün, Ahmet and Sportel, Stephan},
  doi          = {10.1162/tacl_a_00424},
  journal      = {Transactions of the Association for Computational Linguistics},
  pages        = {1233-1248},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {On the difficulty of translating free-order case-marking languages},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Controllable summarization with constrained markov decision
process. <em>TACL</em>, <em>9</em>, 1213–1232. (<a
href="https://doi.org/10.1162/tacl_a_00423">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We study controllable text summarization, which allows users to gain control on a particular attribute (e.g., length limit) of the generated summaries. In this work, we propose a novel training framework based on Constrained Markov Decision Process (CMDP), which conveniently includes a reward function along with a set of constraints, to facilitate better summarization control. The reward function encourages the generation to resemble the human-written reference, while the constraints are used to explicitly prevent the generated summaries from violating user-imposed requirements. Our framework can be applied to control important attributes of summarization, including length, covered entities, and abstractiveness, as we devise specific constraints for each of these aspects. Extensive experiments on popular benchmarks show that our CMDP framework helps generate informative summaries while complying with a given attribute’s requirement.1},
  archive      = {J_TACL},
  author       = {Chan, Hou Pong and Wang, Lu and King, Irwin},
  doi          = {10.1162/tacl_a_00423},
  journal      = {Transactions of the Association for Computational Linguistics},
  pages        = {1213-1232},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Controllable summarization with constrained markov decision process},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Memory-based semantic parsing. <em>TACL</em>, <em>9</em>,
1197–1212. (<a href="https://doi.org/10.1162/tacl_a_00422">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We present a memory-based model for context- dependent semantic parsing. Previous approaches focus on enabling the decoder to copy or modify the parse from the previous utterance, assuming there is a dependency between the current and previous parses. In this work, we propose to represent contextual information using an external memory. We learn a context memory controller that manages the memory by maintaining the cumulative meaning of sequential user utterances. We evaluate our approach on three semantic parsing benchmarks. Experimental results show that our model can better process context-dependent information and demonstrates improved performance without using task-specific decoders.},
  archive      = {J_TACL},
  author       = {Jain, Parag and Lapata, Mirella},
  doi          = {10.1162/tacl_a_00422},
  journal      = {Transactions of the Association for Computational Linguistics},
  pages        = {1197-1212},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Memory-based semantic parsing},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Identity-based patterns in deep convolutional networks:
Generative adversarial phonology and reduplication. <em>TACL</em>,
<em>9</em>, 1180–1196. (<a
href="https://doi.org/10.1162/tacl_a_00421">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. This paper models unsupervised learning of an identity-based pattern (or copying) in speech called reduplication from raw continuous data with deep convolutional neural networks. We use the ciwGAN architecture (Beguš, 2021a) in which learning of meaningful representations in speech emerges from a requirement that the CNNs generate informative data. We propose a technique to wug-test CNNs trained on speech and, based on four generative tests, argue that the network learns to represent an identity-based pattern in its latent space. By manipulating only two categorical variables in the latent space, we can actively turn an unreduplicated form into a reduplicated form with no other substantial changes to the output in the majority of cases. We also argue that the network extends the identity-based pattern to unobserved data. Exploration of how meaningful representations of identity-based patterns emerge in CNNs and how the latent space variables outside of the training range correlate with identity-based patterns in the output has general implications for neural network interpretability.},
  archive      = {J_TACL},
  author       = {Beguš, Gašper},
  doi          = {10.1162/tacl_a_00421},
  journal      = {Transactions of the Association for Computational Linguistics},
  pages        = {1180-1196},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Identity-based patterns in deep convolutional networks: Generative adversarial phonology and reduplication},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). What helps transformers recognize conversational structure?
Importance of context, punctuation, and labels in dialog act
recognition. <em>TACL</em>, <em>9</em>, 1163–1179. (<a
href="https://doi.org/10.1162/tacl_a_00420">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Dialog acts can be interpreted as the atomic units of a conversation, more fine-grained than utterances, characterized by a specific communicative function. The ability to structure a conversational transcript as a sequence of dialog acts—dialog act recognition, including the segmentation—is critical for understanding dialog. We apply two pre-trained transformer models, XLNet and Longformer, to this task in English and achieve strong results on Switchboard Dialog Act and Meeting Recorder Dialog Act corpora with dialog act segmentation error rates (DSER) of 8.4\% and 14.2\%. To understand the key factors affecting dialog act recognition, we perform a comparative analysis of models trained under different conditions. We find that the inclusion of a broader conversational context helps disambiguate many dialog act classes, especially those infrequent in the training data. The presence of punctuation in the transcripts has a massive effect on the models’ performance, and a detailed analysis reveals specific segmentation patterns observed in its absence. Finally, we find that the label set specificity does not affect dialog act segmentation performance. These findings have significant practical implications for spoken language understanding applications that depend heavily on a good-quality segmentation being available.},
  archive      = {J_TACL},
  author       = {Żelasko, Piotr and Pappagari, Raghavendra and Dehak, Najim},
  doi          = {10.1162/tacl_a_00420},
  journal      = {Transactions of the Association for Computational Linguistics},
  pages        = {1163-1179},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {What helps transformers recognize conversational structure? importance of context, punctuation, and labels in dialog act recognition},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). ParsiNLU: A suite of language understanding challenges for
persian. <em>TACL</em>, <em>9</em>, 1147–1162. (<a
href="https://doi.org/10.1162/tacl_a_00419">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Despite the progress made in recent years in addressing natural language understanding (NLU) challenges, the majority of this progress remains to be concentrated on resource-rich languages like English. This work focuses on Persian language, one of the widely spoken languages in the world, and yet there are few NLU datasets available for this language. The availability of high-quality evaluation datasets is a necessity for reliable assessment of the progress on different NLU tasks and domains. We introduce ParsiNLU, the first benchmark in Persian language that includes a range of language understanding tasks—reading comprehension, textual entailment, and so on. These datasets are collected in a multitude of ways, often involving manual annotations by native speakers. This results in over 14.5k new instances across 6 distinct NLU tasks. Additionally, we present the first results on state-of-the-art monolingual and multilingual pre-trained language models on this benchmark and compare them with human performance, which provides valuable insights into our ability to tackle natural language understanding challenges in Persian. We hope ParsiNLU fosters further research and advances in Persian language understanding.1},
  archive      = {J_TACL},
  author       = {Khashabi, Daniel and Cohan, Arman and Shakeri, Siamak and Hosseini, Pedram and Pezeshkpour, Pouya and Alikhani, Malihe and Aminnaseri, Moin and Bitaab, Marzieh and Brahman, Faeze and Ghazarian, Sarik and Gheini, Mozhdeh and Kabiri, Arman and Mahabagdi, Rabeeh Karimi and Memarrast, Omid and Mosallanezhad, Ahmadreza and Noury, Erfan and Raji, Shahab and Rasooli, Mohammad Sadegh and Sadeghi, Sepideh and Azer, Erfan Sadeqi and Samghabadi, Niloofar Safi and Shafaei, Mahsa and Sheybani, Saber and Tazarv, Ali and Yaghoobzadeh, Yadollah},
  doi          = {10.1162/tacl_a_00419},
  journal      = {Transactions of the Association for Computational Linguistics},
  pages        = {1147-1162},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {ParsiNLU: A suite of language understanding challenges for persian},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A statistical analysis of summarization evaluation metrics
using resampling methods. <em>TACL</em>, <em>9</em>, 1132–1146. (<a
href="https://doi.org/10.1162/tacl_a_00417">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. The quality of a summarization evaluation metric is quantified by calculating the correlation between its scores and human annotations across a large number of summaries. Currently, it is unclear how precise these correlation estimates are, nor whether differences between two metrics’ correlations reflect a true difference or if it is due to mere chance. In this work, we address these two problems by proposing methods for calculating confidence intervals and running hypothesis tests for correlations using two resampling methods, bootstrapping and permutation. After evaluating which of the proposed methods is most appropriate for summarization through two simulation experiments, we analyze the results of applying these methods to several different automatic evaluation metrics across three sets of human annotations. We find that the confidence intervals are rather wide, demonstrating high uncertainty in the reliability of automatic metrics. Further, although many metrics fail to show statistical improvements over ROUGE, two recent works, QAEval and BERTScore, do so in some evaluation settings.1},
  archive      = {J_TACL},
  author       = {Deutsch, Daniel and Dror, Rotem and Roth, Dan},
  doi          = {10.1162/tacl_a_00417},
  journal      = {Transactions of the Association for Computational Linguistics},
  pages        = {1132-1146},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {A statistical analysis of summarization evaluation metrics using resampling methods},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MasakhaNER: Named entity recognition for african languages.
<em>TACL</em>, <em>9</em>, 1116–1131. (<a
href="https://doi.org/10.1162/tacl_a_00416">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We take a step towards addressing the under- representation of the African continent in NLP research by bringing together different stakeholders to create the first large, publicly available, high-quality dataset for named entity recognition (NER) in ten African languages. We detail the characteristics of these languages to help researchers and practitioners better understand the challenges they pose for NER tasks. We analyze our datasets and conduct an extensive empirical evaluation of state- of-the-art methods across both supervised and transfer learning settings. Finally, we release the data, code, and models to inspire future research on African NLP.1},
  archive      = {J_TACL},
  author       = {Adelani, David Ifeoluwa and Abbott, Jade and Neubig, Graham and D’souza, Daniel and Kreutzer, Julia and Lignos, Constantine and Palen-Michel, Chester and Buzaaba, Happy and Rijhwani, Shruti and Ruder, Sebastian and Mayhew, Stephen and Azime, Israel Abebe and Muhammad, Shamsuddeen H. and Emezue, Chris Chinenye and Nakatumba-Nabende, Joyce and Ogayo, Perez and Anuoluwapo, Aremu and Gitau, Catherine and Mbaye, Derguene and Alabi, Jesujoba and Yimam, Seid Muhie and Gwadabe, Tajuddeen Rabiu and Ezeani, Ignatius and Niyongabo, Rubungo Andre and Mukiibi, Jonathan and Otiende, Verrah and Orife, Iroro and David, Davis and Ngom, Samba and Adewumi, Tosin and Rayson, Paul and Adeyemi, Mofetoluwa and Muriuki, Gerald and Anebi, Emmanuel and Chukwuneke, Chiamaka and Odu, Nkiruka and Wairagala, Eric Peter and Oyerinde, Samuel and Siro, Clemencia and Bateesa, Tobius Saul and Oloyede, Temilola and Wambui, Yvonne and Akinode, Victor and Nabagereka, Deborah and Katusiime, Maurice and Awokoya, Ayodele and MBOUP, Mouhamadane and Gebreyohannes, Dibora and Tilaye, Henok and Nwaike, Kelechi and Wolde, Degaga and Faye, Abdoulaye and Sibanda, Blessing and Ahia, Orevaoghene and Dossou, Bonaventure F. P. and Ogueji, Kelechi and DIOP, Thierno Ibrahima and Diallo, Abdoulaye and Akinfaderin, Adewale and Marengereke, Tendai and Osei, Salomey},
  doi          = {10.1162/tacl_a_00416},
  journal      = {Transactions of the Association for Computational Linguistics},
  pages        = {1116-1131},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {MasakhaNER: Named entity recognition for african languages},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). PAQ: 65 million probably-asked questions and what you can do
with them. <em>TACL</em>, <em>9</em>, 1098–1115. (<a
href="https://doi.org/10.1162/tacl_a_00415">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Open-domain Question Answering models that directly leverage question-answer (QA) pairs, such as closed-book QA (CBQA) models and QA-pair retrievers, show promise in terms of speed and memory compared with conventional models which retrieve and read from text corpora. QA-pair retrievers also offer interpretable answers, a high degree of control, and are trivial to update at test time with new knowledge. However, these models fall short of the accuracy of retrieve-and-read systems, as substantially less knowledge is covered by the available QA-pairs relative to text corpora like Wikipedia. To facilitate improved QA-pair models, we introduce Probably Asked Questions (PAQ), a very large resource of 65M automatically generated QA-pairs. We introduce a new QA-pair retriever, RePAQ, to complement PAQ. We find that PAQ preempts and caches test questions, enabling RePAQ to match the accuracy of recent retrieve-and-read models, whilst being significantly faster. Using PAQ, we train CBQA models which outperform comparable baselines by 5\%, but trail RePAQ by over 15\%, indicating the effectiveness of explicit retrieval. RePAQ can be configured for size (under 500MB) or speed (over 1K questions per second) while retaining high accuracy. Lastly, we demonstrate RePAQ’s strength at selective QA, abstaining from answering when it is likely to be incorrect. This enables RePAQ to “back-off” to a more expensive state-of-the-art model, leading to a combined system which is both more accurate and 2x faster than the state-of-the-art model alone.},
  archive      = {J_TACL},
  author       = {Lewis, Patrick and Wu, Yuxiang and Liu, Linqing and Minervini, Pasquale and Küttler, Heinrich and Piktus, Aleksandra and Stenetorp, Pontus and Riedel, Sebastian},
  doi          = {10.1162/tacl_a_00415},
  journal      = {Transactions of the Association for Computational Linguistics},
  pages        = {1098-1115},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {PAQ: 65 million probably-asked questions and what you can do with them},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). He thinks he knows better than the doctors: BERT for event
factuality fails on pragmatics. <em>TACL</em>, <em>9</em>, 1081–1097.
(<a href="https://doi.org/10.1162/tacl_a_00414">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We investigate how well BERT performs on predicting factuality in several existing English datasets, encompassing various linguistic constructions. Although BERT obtains a strong performance on most datasets, it does so by exploiting common surface patterns that correlate with certain factuality labels, and it fails on instances where pragmatic reasoning is necessary. Contrary to what the high performance suggests, we are still far from having a robust system for factuality prediction.},
  archive      = {J_TACL},
  author       = {Jiang, Nanjiang and de Marneffe, Marie-Catherine},
  doi          = {10.1162/tacl_a_00414},
  journal      = {Transactions of the Association for Computational Linguistics},
  pages        = {1081-1097},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {He thinks he knows better than the doctors: BERT for event factuality fails on pragmatics},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Compressing large-scale transformer-based models: A case
study on BERT. <em>TACL</em>, <em>9</em>, 1061–1080. (<a
href="https://doi.org/10.1162/tacl_a_00413">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Pre-trained Transformer-based models have achieved state-of-the-art performance for various Natural Language Processing (NLP) tasks. However, these models often have billions of parameters, and thus are too resource- hungry and computation-intensive to suit low- capability devices or applications with strict latency requirements. One potential remedy for this is model compression, which has attracted considerable research attention. Here, we summarize the research in compressing Transformers, focusing on the especially popular BERT model. In particular, we survey the state of the art in compression for BERT, we clarify the current best practices for compressing large-scale Transformer models, and we provide insights into the workings of various methods. Our categorization and analysis also shed light on promising future research directions for achieving lightweight, accurate, and generic NLP models.},
  archive      = {J_TACL},
  author       = {Ganesh, Prakhar and Chen, Yao and Lou, Xin and Khan, Mohammad Ali and Yang, Yin and Sajjad, Hassan and Nakov, Preslav and Chen, Deming and Winslett, Marianne},
  doi          = {10.1162/tacl_a_00413},
  journal      = {Transactions of the Association for Computational Linguistics},
  pages        = {1061-1080},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Compressing large-scale transformer-based models: A case study on BERT},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Provable limitations of acquiring meaning from ungrounded
form: What will future language models understand? <em>TACL</em>,
<em>9</em>, 1047–1060. (<a
href="https://doi.org/10.1162/tacl_a_00412">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Language models trained on billions of tokens have recently led to unprecedented results on many NLP tasks. This success raises the question of whether, in principle, a system can ever “understand” raw text without access to some form of grounding. We formally investigate the abilities of ungrounded systems to acquire meaning. Our analysis focuses on the role of “assertions”: textual contexts that provide indirect clues about the underlying semantics. We study whether assertions enable a system to emulate representations preserving semantic relations like equivalence. We find that assertions enable semantic emulation of languages that satisfy a strong notion of semantic transparency. However, for classes of languages where the same expression can take different values in different contexts, we show that emulation can become uncomputable. Finally, we discuss differences between our formal model and natural language, exploring how our results generalize to a modal setting and other semantic relations. Together, our results suggest that assertions in code or language do not provide sufficient signal to fully emulate semantic representations. We formalize ways in which ungrounded language models appear to be fundamentally limited in their ability to “understand”.},
  archive      = {J_TACL},
  author       = {Merrill, William and Goldberg, Yoav and Schwartz, Roy and Smith, Noah A.},
  doi          = {10.1162/tacl_a_00412},
  journal      = {Transactions of the Association for Computational Linguistics},
  pages        = {1047-1060},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Provable limitations of acquiring meaning from ungrounded form: What will future language models understand?},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Narrative question answering with cutting-edge open-domain
QA techniques: A comprehensive study. <em>TACL</em>, <em>9</em>,
1032–1046. (<a href="https://doi.org/10.1162/tacl_a_00411">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Recent advancements in open-domain question answering (ODQA), that is, finding answers from large open-domain corpus like Wikipedia, have led to human-level performance on many datasets. However, progress in QA over book stories (Book QA) lags despite its similar task formulation to ODQA. This work provides a comprehensive and quantitative analysis about the difficulty of Book QA: (1) We benchmark the research on the NarrativeQA dataset with extensive experiments with cutting-edge ODQA techniques. This quantifies the challenges Book QA poses, as well as advances the published state-of-the-art with a ∼7\% absolute improvement on ROUGE-L. (2) We further analyze the detailed challenges in Book QA through human studies.1 Our findings indicate that the event-centric questions dominate this task, which exemplifies the inability of existing QA models to handle event-oriented scenarios.},
  archive      = {J_TACL},
  author       = {Mou, Xiangyang and Yang, Chenghao and Yu, Mo and Yao, Bingsheng and Guo, Xiaoxiao and Potdar, Saloni and Su, Hui},
  doi          = {10.1162/tacl_a_00411},
  journal      = {Transactions of the Association for Computational Linguistics},
  pages        = {1032-1046},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Narrative question answering with cutting-edge open-domain QA techniques: A comprehensive study},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Measuring and improving consistency in pretrained language
models. <em>TACL</em>, <em>9</em>, 1012–1031. (<a
href="https://doi.org/10.1162/tacl_a_00410">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Consistency of a model—that is, the invariance of its behavior under meaning-preserving alternations in its input—is a highly desirable property in natural language processing. In this paper we study the question: Are Pretrained Language Models (PLMs) consistent with respect to factual knowledge? To this end, we create ParaRel🤘, a high-quality resource of cloze-style query English paraphrases. It contains a total of 328 paraphrases for 38 relations. Using ParaRel🤘, we show that the consistency of all PLMs we experiment with is poor— though with high variance between relations. Our analysis of the representational spaces of PLMs suggests that they have a poor structure and are currently not suitable for representing knowledge robustly. Finally, we propose a method for improving model consistency and experimentally demonstrate its effectiveness.1},
  archive      = {J_TACL},
  author       = {Elazar, Yanai and Kassner, Nora and Ravfogel, Shauli and Ravichander, Abhilasha and Hovy, Eduard and Schütze, Hinrich and Goldberg, Yoav},
  doi          = {10.1162/tacl_a_00410},
  journal      = {Transactions of the Association for Computational Linguistics},
  pages        = {1012-1031},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Measuring and improving consistency in pretrained language models},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Maintaining common ground in dynamic environments.
<em>TACL</em>, <em>9</em>, 995–1011. (<a
href="https://doi.org/10.1162/tacl_a_00409">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Common grounding is the process of creating and maintaining mutual understandings, which is a critical aspect of sophisticated human communication. While various task settings have been proposed in existing literature, they mostly focus on creating common ground under a static context and ignore the aspect of maintaining them overtime under dynamic context. In this work, we propose a novel task setting to study the ability of both creating and maintaining common ground in dynamic environments. Based on our minimal task formulation, we collected a large-scale dataset of 5,617 dialogues to enable fine-grained evaluation and analysis of various dialogue systems. Through our dataset analyses, we highlight novel challenges introduced in our setting, such as the usage of complex spatio-temporal expressions to create and maintain common ground. Finally, we conduct extensive experiments to assess the capabilities of our baseline dialogue system and discuss future prospects of our research.},
  archive      = {J_TACL},
  author       = {Udagawa, Takuma and Aizawa, Akiko},
  doi          = {10.1162/tacl_a_00409},
  journal      = {Transactions of the Association for Computational Linguistics},
  pages        = {995-1011},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Maintaining common ground in dynamic environments},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multimodal pretraining unmasked: A meta-analysis and a
unified framework of vision-and-language BERTs. <em>TACL</em>,
<em>9</em>, 978–994. (<a
href="https://doi.org/10.1162/tacl_a_00408">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Large-scale pretraining and task-specific fine- tuning is now the standard methodology for many tasks in computer vision and natural language processing. Recently, a multitude of methods have been proposed for pretraining vision and language BERTs to tackle challenges at the intersection of these two key areas of AI. These models can be categorized into either single-stream or dual-stream encoders. We study the differences between these two categories, and show how they can be unified under a single theoretical framework. We then conduct controlled experiments to discern the empirical differences between five vision and language BERTs. Our experiments show that training data and hyperparameters are responsible for most of the differences between the reported results, but they also reveal that the embedding layer plays a crucial role in these massive models.},
  archive      = {J_TACL},
  author       = {Bugliarello, Emanuele and Cotterell, Ryan and Okazaki, Naoaki and Elliott, Desmond},
  doi          = {10.1162/tacl_a_00408},
  journal      = {Transactions of the Association for Computational Linguistics},
  pages        = {978-994},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Multimodal pretraining unmasked: A meta-analysis and a unified framework of vision-and-language BERTs},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). How can we know when language models know? On the
calibration of language models for question answering. <em>TACL</em>,
<em>9</em>, 962–977. (<a
href="https://doi.org/10.1162/tacl_a_00407">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Recent works have shown that language models (LM) capture different types of knowledge regarding facts or common sense. However, because no model is perfect, they still fail to provide appropriate answers in many cases. In this paper, we ask the question, “How can we know when language models know, with confidence, the answer to a particular query?” We examine this question from the point of view of calibration, the property of a probabilistic model’s predicted probabilities actually being well correlated with the probabilities of correctness. We examine three strong generative models—T5, BART, and GPT-2—and study whether their probabilities on QA tasks are well calibrated, finding the answer is a relatively emphatic no. We then examine methods to calibrate such models to make their confidence scores correlate better with the likelihood of correctness through fine-tuning, post-hoc probability modification, or adjustment of the predicted outputs or inputs. Experiments on a diverse range of datasets demonstrate the effectiveness of our methods. We also perform analysis to study the strengths and limitations of these methods, shedding light on further improvements that may be made in methods for calibrating LMs. We have released the code at https://github.com/jzbjyb/lm-calibration.},
  archive      = {J_TACL},
  author       = {Jiang, Zhengbao and Araki, Jun and Ding, Haibo and Neubig, Graham},
  doi          = {10.1162/tacl_a_00407},
  journal      = {Transactions of the Association for Computational Linguistics},
  pages        = {962-977},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {How can we know when language models know? on the calibration of language models for question answering},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Unsupervised abstractive opinion summarization by generating
sentences with tree-structured topic guidance. <em>TACL</em>,
<em>9</em>, 945–961. (<a
href="https://doi.org/10.1162/tacl_a_00406">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. This paper presents a novel unsupervised abstractive summarization method for opinionated texts. While the basic variational autoencoder-based models assume a unimodal Gaussian prior for the latent code of sentences, we alternate it with a recursive Gaussian mixture, where each mixture component corresponds to the latent code of a topic sentence and is mixed by a tree-structured topic distribution. By decoding each Gaussian component, we generate sentences with tree-structured topic guidance, where the root sentence conveys generic content, and the leaf sentences describe specific topics. Experimental results demonstrate that the generated topic sentences are appropriate as a summary of opinionated texts, which are more informative and cover more input contents than those generated by the recent unsupervised summarization model (Bražinskas et al., 2020). Furthermore, we demonstrate that the variance of latent Gaussians represents the granularity of sentences, analogous to Gaussian word embedding (Vilnis and McCallum, 2015).},
  archive      = {J_TACL},
  author       = {Isonuma, Masaru and Mori, Junichiro and Bollegala, Danushka and Sakata, Ichiro},
  doi          = {10.1162/tacl_a_00406},
  journal      = {Transactions of the Association for Computational Linguistics},
  pages        = {945-961},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Unsupervised abstractive opinion summarization by generating sentences with tree-structured topic guidance},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Relevance-guided supervision for OpenQA with ColBERT.
<em>TACL</em>, <em>9</em>, 929–944. (<a
href="https://doi.org/10.1162/tacl_a_00405">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Systems for Open-Domain Question Answering (OpenQA) generally depend on a retriever for finding candidate passages in a large corpus and a reader for extracting answers from those passages. In much recent work, the retriever is a learned component that uses coarse-grained vector representations of questions and passages. We argue that this modeling choice is insufficiently expressive for dealing with the complexity of natural language questions. To address this, we define ColBERT-QA, which adapts the scalable neural retrieval model ColBERT to OpenQA. ColBERT creates fine-grained interactions between questions and passages. We propose an efficient weak supervision strategy that iteratively uses ColBERT to create its own training data. This greatly improves OpenQA retrieval on Natural Questions, SQuAD, and TriviaQA, and the resulting system attains state-of-the-art extractive OpenQA performance on all three datasets.},
  archive      = {J_TACL},
  author       = {Khattab, Omar and Potts, Christopher and Zaharia, Matei},
  doi          = {10.1162/tacl_a_00405},
  journal      = {Transactions of the Association for Computational Linguistics},
  pages        = {929-944},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Relevance-guided supervision for OpenQA with ColBERT},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Neural modeling for named entities and morphology (NEMO2).
<em>TACL</em>, <em>9</em>, 909–928. (<a
href="https://doi.org/10.1162/tacl_a_00404">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Named Entity Recognition (NER) is a fundamental NLP task, commonly formulated as classification over a sequence of tokens. Morphologically rich languages (MRLs) pose a challenge to this basic formulation, as the boundaries of named entities do not necessarily coincide with token boundaries, rather, they respect morphological boundaries. To address NER in MRLs we then need to answer two fundamental questions, namely, what are the basic units to be labeled, and how can these units be detected and classified in realistic settings (i.e., where no gold morphology is available). We empirically investigate these questions on a novel NER benchmark, with parallel token- level and morpheme-level NER annotations, which we develop for Modern Hebrew, a morphologically rich-and-ambiguous language. Our results show that explicitly modeling morphological boundaries leads to improved NER performance, and that a novel hybrid architecture, in which NER precedes and prunes morphological decomposition, greatly outperforms the standard pipeline, where morphological decomposition strictly precedes NER, setting a new performance bar for both Hebrew NER and Hebrew morphological decomposition tasks.},
  archive      = {J_TACL},
  author       = {Bareket, Dan and Tsarfaty, Reut},
  doi          = {10.1162/tacl_a_00404},
  journal      = {Transactions of the Association for Computational Linguistics},
  pages        = {909-928},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Neural modeling for named entities and morphology (NEMO2)},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sensitivity as a complexity measure for sequence
classification tasks. <em>TACL</em>, <em>9</em>, 891–908. (<a
href="https://doi.org/10.1162/tacl_a_00403">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We introduce a theoretical framework for understanding and predicting the complexity of sequence classification tasks, using a novel extension of the theory of Boolean function sensitivity. The sensitivity of a function, given a distribution over input sequences, quantifies the number of disjoint subsets of the input sequence that can each be individually changed to change the output. We argue that standard sequence classification methods are biased towards learning low-sensitivity functions, so that tasks requiring high sensitivity are more difficult. To that end, we show analytically that simple lexical classifiers can only express functions of bounded sensitivity, and we show empirically that low-sensitivity functions are easier to learn for LSTMs. We then estimate sensitivity on 15 NLP tasks, finding that sensitivity is higher on challenging tasks collected in GLUE than on simple text classification tasks, and that sensitivity predicts the performance both of simple lexical classifiers and of vanilla BiLSTMs without pretrained contextualized embeddings. Within a task, sensitivity predicts which inputs are hard for such simple models. Our results suggest that the success of massively pretrained contextual representations stems in part because they provide representations from which information can be extracted by low-sensitivity decoders.},
  archive      = {J_TACL},
  author       = {Hahn, Michael and Jurafsky, Dan and Futrell, Richard},
  doi          = {10.1162/tacl_a_00403},
  journal      = {Transactions of the Association for Computational Linguistics},
  pages        = {891-908},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Sensitivity as a complexity measure for sequence classification tasks},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Neural event semantics for grounded language understanding.
<em>TACL</em>, <em>9</em>, 875–890. (<a
href="https://doi.org/10.1162/tacl_a_00402">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We present a new conjunctivist framework, neural event semantics (NES), for compositional grounded language understanding. Our approach treats all words as classifiers that compose to form a sentence meaning by multiplying output scores. These classifiers apply to spatial regions (events) and NES derives its semantic structure from language by routing events to different classifier argument inputs via soft attention. NES is trainable end-to-end by gradient descent with minimal supervision. We evaluate our method on compositional grounded language tasks in controlled synthetic and real-world settings. NES offers stronger generalization capability than standard function-based compositional frameworks, while improving accuracy over state-of-the-art neural methods on real-world language tasks.},
  archive      = {J_TACL},
  author       = {Buch, Shyamal and Fei-Fei, Li and Goodman, Noah D.},
  doi          = {10.1162/tacl_a_00402},
  journal      = {Transactions of the Association for Computational Linguistics},
  pages        = {875-890},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Neural event semantics for grounded language understanding},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Gender bias in machine translation. <em>TACL</em>,
<em>9</em>, 845–874. (<a
href="https://doi.org/10.1162/tacl_a_00401">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Machine translation (MT) technology has facilitated our daily tasks by providing accessible shortcuts for gathering, processing, and communicating information. However, it can suffer from biases that harm users and society at large. As a relatively new field of inquiry, studies of gender bias in MT still lack cohesion. This advocates for a unified framework to ease future research. To this end, we: i) critically review current conceptualizations of bias in light of theoretical insights from related disciplines, ii) summarize previous analyses aimed at assessing gender bias in MT, iii) discuss the mitigating strategies proposed so far, and iv) point toward potential directions for future work.},
  archive      = {J_TACL},
  author       = {Savoldi, Beatrice and Gaido, Marco and Bentivogli, Luisa and Negri, Matteo and Turchi, Marco},
  doi          = {10.1162/tacl_a_00401},
  journal      = {Transactions of the Association for Computational Linguistics},
  pages        = {845-874},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Gender bias in machine translation},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Let’s play mono-poly: BERT can reveal words’ polysemy level
and partitionability into senses. <em>TACL</em>, <em>9</em>, 825–844.
(<a href="https://doi.org/10.1162/tacl_a_00400">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pre-trained language models (LMs) encode rich information about linguistic structure but their knowledge about lexical polysemy remains unclear. We propose a novel experimental setup for analyzing this knowledge in LMs specifically trained for different languages (English, French, Spanish, and Greek) and in multilingual BERT. We perform our analysis on datasets carefully designed to reflect different sense distributions, and control for parameters that are highly correlated with polysemy such as frequency and grammatical category. We demonstrate that BERT-derived representations reflect words’ polysemy level and their partitionability into senses. Polysemy-related information is more clearly present in English BERT embeddings, but models in other languages also manage to establish relevant distinctions between words at different polysemy levels. Our results contribute to a better understanding of the knowledge encoded in contextualized representations and open up new avenues for multilingual lexical semantics research.},
  archive      = {J_TACL},
  author       = {Garí Soler, Aina and Apidianaki, Marianna},
  doi          = {10.1162/tacl_a_00400},
  journal      = {Transactions of the Association for Computational Linguistics},
  pages        = {825-844},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Let’s play mono-poly: BERT can reveal words’ polysemy level and partitionability into senses},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Soloist: Building task bots at scale with transfer learning
and machine teaching. <em>TACL</em>, <em>9</em>, 807–824. (<a
href="https://doi.org/10.1162/tacl_a_00399">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We present a new method, Soloist,1 that uses transfer learning and machine teaching to build task bots at scale. We parameterize classical modular task-oriented dialog systems using a Transformer-based auto-regressive language model, which subsumes different dialog modules into a single neural model. We pre-train, on heterogeneous dialog corpora, a task-grounded response generation model, which can generate dialog responses grounded in user goals and real-world knowledge for task completion. The pre-trained model can be efficiently adapted to accomplish new tasks with a handful of task-specific dialogs via machine teaching, where training samples are generated by human teachers interacting with the system. Experiments show that (i)Soloist creates new state-of-the-art on well-studied task-oriented dialog benchmarks, including CamRest676 and MultiWOZ; (ii) in the few-shot fine-tuning settings, Soloist significantly outperforms existing methods; and (iii) the use of machine teaching substantially reduces the labeling cost of fine-tuning. The pre-trained models and codes are available at https://aka.ms/soloist.},
  archive      = {J_TACL},
  author       = {Peng, Baolin and Li, Chunyuan and Li, Jinchao and Shayandeh, Shahin and Liden, Lars and Gao, Jianfeng},
  doi          = {10.1162/tacl_a_00399},
  journal      = {Transactions of the Association for Computational Linguistics},
  pages        = {807-824},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Soloist: Building task bots at scale with transfer learning and machine teaching},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). QED: A framework and dataset for explanations in question
answering. <em>TACL</em>, <em>9</em>, 790–806. (<a
href="https://doi.org/10.1162/tacl_a_00398">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A question answering system that in addition to providing an answer provides an explanation of the reasoning that leads to that answer has potential advantages in terms of debuggability, extensibility, and trust. To this end, we propose QED, a linguistically informed, extensible framework for explanations in question answering. A QED explanation specifies the relationship between a question and answer according to formal semantic notions such as referential equality, sentencehood, and entailment. We describe and publicly release an expert-annotated dataset of QED explanations built upon a subset of the Google Natural Questions dataset, and report baseline models on two tasks—post- hoc explanation generation given an answer, and joint question answering and explanation generation. In the joint setting, a promising result suggests that training on a relatively small amount of QED data can improve question answering. In addition to describing the formal, language-theoretic motivations for the QED approach, we describe a large user study showing that the presence of QED explanations significantly improves the ability of untrained raters to spot errors made by a strong neural QA baseline.},
  archive      = {J_TACL},
  author       = {Lamm, Matthew and Palomaki, Jennimaria and Alberti, Chris and Andor, Daniel and Choi, Eunsol and Soares, Livio Baldini and Collins, Michael},
  doi          = {10.1162/tacl_a_00398},
  journal      = {Transactions of the Association for Computational Linguistics},
  pages        = {790-806},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {QED: A framework and dataset for explanations in question answering},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Towards question-answering as an automatic metric for
evaluating the content quality of a summary. <em>TACL</em>, <em>9</em>,
774–789. (<a href="https://doi.org/10.1162/tacl_a_00397">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. A desirable property of a reference-based evaluation metric that measures the content quality of a summary is that it should estimate how much information that summary has in common with a reference. Traditional text overlap based metrics such as ROUGE fail to achieve this because they are limited to matching tokens, either lexically or via embeddings. In this work, we propose a metric to evaluate the content quality of a summary using question-answering (QA). QA-based methods directly measure a summary’s information overlap with a reference, making them fundamentally different than text overlap metrics. We demonstrate the experimental benefits of QA-based metrics through an analysis of our proposed metric, QAEval. QAEval outperforms current state-of-the-art metrics on most evaluations using benchmark datasets, while being competitive on others due to limitations of state-of-the-art models. Through a careful analysis of each component of QAEval, we identify its performance bottlenecks and estimate that its potential upper-bound performance surpasses all other automatic metrics, approaching that of the gold-standard Pyramid Method.1},
  archive      = {J_TACL},
  author       = {Deutsch, Daniel and Bedrax-Weiss, Tania and Roth, Dan},
  doi          = {10.1162/tacl_a_00397},
  journal      = {Transactions of the Association for Computational Linguistics},
  pages        = {774-789},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Towards question-answering as an automatic metric for evaluating the content quality of a summary},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Joint universal syntactic and semantic parsing.
<em>TACL</em>, <em>9</em>, 756–773. (<a
href="https://doi.org/10.1162/tacl_a_00396">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While numerous attempts have been made to jointly parse syntax and semantics, high performance in one domain typically comes at the price of performance in the other. This trade-off contradicts the large body of research focusing on the rich interactions at the syntax–semantics interface. We explore multiple model architectures that allow us to exploit the rich syntactic and semantic annotations contained in the Universal Decompositional Semantics (UDS) dataset, jointly parsing Universal Dependencies and UDS to obtain state-of-the-art results in both formalisms. We analyze the behavior of a joint model of syntax and semantics, finding patterns supported by linguistic theory at the syntax–semantics interface. We then investigate to what degree joint modeling generalizes to a multilingual setting, where we find similar trends across 8 languages.},
  archive      = {J_TACL},
  author       = {Stengel-Eskin, Elias and Murray, Kenton and Zhang, Sheng and White, Aaron Steven and Van Durme, Benjamin},
  doi          = {10.1162/tacl_a_00396},
  journal      = {Transactions of the Association for Computational Linguistics},
  pages        = {756-773},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Joint universal syntactic and semantic parsing},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Revisiting negation in neural machine translation.
<em>TACL</em>, <em>9</em>, 740–755. (<a
href="https://doi.org/10.1162/tacl_a_00395">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we evaluate the translation of negation both automatically and manually, in English–German (EN–DE) and English– Chinese (EN–ZH). We show that the ability of neural machine translation (NMT) models to translate negation has improved with deeper and more advanced networks, although the performance varies between language pairs and translation directions. The accuracy of manual evaluation in EN→DE, DE→EN, EN→ZH, and ZH→EN is 95.7\%, 94.8\%, 93.4\%, and 91.7\%, respectively. In addition, we show that under-translation is the most significant error type in NMT, which contrasts with the more diverse error profile previously observed for statistical machine translation. To better understand the root of the under-translation of negation, we study the model’s information flow and training data. While our information flow analysis does not reveal any deficiencies that could be used to detect or fix the under-translation of negation, we find that negation is often rephrased during training, which could make it more difficult for the model to learn a reliable link between source and target negation. We finally conduct intrinsic analysis and extrinsic probing tasks on negation, showing that NMT models can distinguish negation and non-negation tokens very well and encode a lot of information about negation in hidden states but nevertheless leave room for improvement.},
  archive      = {J_TACL},
  author       = {Tang, Gongbo and Rönchen, Philipp and Sennrich, Rico and Nivre, Joakim},
  doi          = {10.1162/tacl_a_00395},
  journal      = {Transactions of the Association for Computational Linguistics},
  pages        = {740-755},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Revisiting negation in neural machine translation},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Classifying argumentative relations using logical mechanisms
and argumentation schemes. <em>TACL</em>, <em>9</em>, 721–739. (<a
href="https://doi.org/10.1162/tacl_a_00394">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While argument mining has achieved significant success in classifying argumentative relations between statements (support, attack, and neutral), we have a limited computational understanding of logical mechanisms that constitute those relations. Most recent studies rely on black-box models, which are not as linguistically insightful as desired. On the other hand, earlier studies use rather simple lexical features, missing logical relations between statements. To overcome these limitations, our work classifies argumentative relations based on four logical and theory-informed mechanisms between two statements, namely, (i) factual consistency, (ii) sentiment coherence, (iii) causal relation, and (iv) normative relation. We demonstrate that our operationalization of these logical mechanisms classifies argumentative relations without directly training on data labeled with the relations, significantly better than several unsupervised baselines. We further demonstrate that these mechanisms also improve supervised classifiers through representation learning.},
  archive      = {J_TACL},
  author       = {Jo, Yohan and Bang, Seojin and Reed, Chris and Hovy, Eduard},
  doi          = {10.1162/tacl_a_00394},
  journal      = {Transactions of the Association for Computational Linguistics},
  pages        = {721-739},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Classifying argumentative relations using logical mechanisms and argumentation schemes},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Strong equivalence of TAG and CCG. <em>TACL</em>,
<em>9</em>, 707–720. (<a
href="https://doi.org/10.1162/tacl_a_00393">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tree-adjoining grammar (TAG) and combinatory categorial grammar (CCG) are two well-established mildly context-sensitive grammar formalisms that are known to have the same expressive power on strings (i.e., generate the same class of string languages). It is demonstrated that their expressive power on trees also essentially coincides. In fact, CCG without lexicon entries for the empty string and only first-order rules of degree at most 2 are sufficient for its full expressive power.},
  archive      = {J_TACL},
  author       = {Schiffer, Lena Katharina and Maletti, Andreas},
  doi          = {10.1162/tacl_a_00393},
  journal      = {Transactions of the Association for Computational Linguistics},
  pages        = {707-720},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Strong equivalence of TAG and CCG},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Revisiting few-shot relation classification: Evaluation data
and classification schemes. <em>TACL</em>, <em>9</em>, 691–706. (<a
href="https://doi.org/10.1162/tacl_a_00392">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We explore few-shot learning (FSL) for relation classification (RC). Focusing on the realistic scenario of FSL, in which a test instance might not belong to any of the target categories (none-of-the-above, [NOTA]), we first revisit the recent popular dataset structure for FSL, pointing out its unrealistic data distribution. To remedy this, we propose a novel methodology for deriving more realistic few-shot test data from available datasets for supervised RC, and apply it to the TACRED dataset. This yields a new challenging benchmark for FSL-RC, on which state of the art models show poor performance. Next, we analyze classification schemes within the popular embedding-based nearest-neighbor approach for FSL, with respect to constraints they impose on the embedding space. Triggered by this analysis, we propose a novel classification scheme in which the NOTA category is represented as learned vectors, shown empirically to be an appealing option for FSL.},
  archive      = {J_TACL},
  author       = {Sabo, Ofer and Elazar, Yanai and Goldberg, Yoav and Dagan, Ido},
  doi          = {10.1162/tacl_a_00392},
  journal      = {Transactions of the Association for Computational Linguistics},
  pages        = {691-706},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Revisiting few-shot relation classification: Evaluation data and classification schemes},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Efficient computation of expectations under spanning tree
distributions. <em>TACL</em>, <em>9</em>, 675–690. (<a
href="https://doi.org/10.1162/tacl_a_00391">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We give a general framework for inference in spanning tree models. We propose unified algorithms for the important cases of first-order expectations and second-order expectations in edge-factored, non-projective spanning-tree models. Our algorithms exploit a fundamental connection between gradients and expectations, which allows us to derive efficient algorithms. These algorithms are easy to implement with or without automatic differentiation software. We motivate the development of our framework with several cautionary tales of previous research, which has developed numerous inefficient algorithms for computing expectations and their gradients. We demonstrate how our framework efficiently computes several quantities with known algorithms, including the expected attachment score, entropy, and generalized expectation criteria. As a bonus, we give algorithms for quantities that are missing in the literature, including the KL divergence. In all cases, our approach matches the efficiency of existing algorithms and, in several cases, reduces the runtime complexity by a factor of the sentence length. We validate the implementation of our framework through runtime experiments. We find our algorithms are up to 15 and 9 times faster than previous algorithms for computing the Shannon entropy and the gradient of the generalized expectation objective, respectively.},
  archive      = {J_TACL},
  author       = {Zmigrod, Ran and Vieira, Tim and Cotterell, Ryan},
  doi          = {10.1162/tacl_a_00391},
  journal      = {Transactions of the Association for Computational Linguistics},
  pages        = {675-690},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Efficient computation of expectations under spanning tree distributions},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Pretraining the noisy channel model for task-oriented
dialogue. <em>TACL</em>, <em>9</em>, 657–674. (<a
href="https://doi.org/10.1162/tacl_a_00390">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Direct decoding for task-oriented dialogue is known to suffer from the explaining-away effect, manifested in models that prefer short and generic responses. Here we argue for the use of Bayes’ theorem to factorize the dialogue task into two models, the distribution of the context given the response, and the prior for the response itself. This approach, an instantiation of the noisy channel model, both mitigates the explaining-away effect and allows the principled incorporation of large pretrained models for the response prior. We present extensive experiments showing that a noisy channel model decodes better responses compared to direct decoding and that a two-stage pretraining strategy, employing both open-domain and task-oriented dialogue data, improves over randomly initialized models.},
  archive      = {J_TACL},
  author       = {Liu, Qi and Yu, Lei and Rimell, Laura and Blunsom, Phil},
  doi          = {10.1162/tacl_a_00390},
  journal      = {Transactions of the Association for Computational Linguistics},
  pages        = {657-674},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Pretraining the noisy channel model for task-oriented dialogue},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Self-supervised regularization for text classification.
<em>TACL</em>, <em>9</em>, 641–656. (<a
href="https://doi.org/10.1162/tacl_a_00389">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Text classification is a widely studied problem and has broad applications. In many real-world problems, the number of texts for training classification models is limited, which renders these models prone to overfitting. To address this problem, we propose SSL-Reg, a data-dependent regularization approach based on self-supervised learning (SSL). SSL (Devlin et al., 2019a) is an unsupervised learning approach that defines auxiliary tasks on input data without using any human-provided labels and learns data representations by solving these auxiliary tasks. In SSL-Reg, a supervised classification task and an unsupervised SSL task are performed simultaneously. The SSL task is unsupervised, which is defined purely on input texts without using any human- provided labels. Training a model using an SSL task can prevent the model from being overfitted to a limited number of class labels in the classification task. Experiments on 17 text classification datasets demonstrate the effectiveness of our proposed method. Code is available at https://github.com/UCSD-AI4H/SSReg.},
  archive      = {J_TACL},
  author       = {Zhou, Meng and Li, Zechen and Xie, Pengtao},
  doi          = {10.1162/tacl_a_00389},
  journal      = {Transactions of the Association for Computational Linguistics},
  pages        = {641-656},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Self-supervised regularization for text classification},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Evaluating document coherence modeling. <em>TACL</em>,
<em>9</em>, 621–640. (<a
href="https://doi.org/10.1162/tacl_a_00388">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. While pretrained language models (LMs) have driven impressive gains over morpho-syntactic and semantic tasks, their ability to model discourse and pragmatic phenomena is less clear. As a step towards a better understanding of their discourse modeling capabilities, we propose a sentence intrusion detection task. We examine the performance of a broad range of pretrained LMs on this detection task for English. Lacking a dataset for the task, we introduce INSteD, a novel intruder sentence detection dataset, containing 170,000+ documents constructed from English Wikipedia and CNN news articles. Our experiments show that pretrained LMs perform impressively in in-domain evaluation, but experience a substantial drop in the cross-domain setting, indicating limited generalization capacity. Further results over a novel linguistic probe dataset show that there is substantial room for improvement, especially in the cross- domain setting.},
  archive      = {J_TACL},
  author       = {Shen, Aili and Mistica, Meladel and Salehi, Bahar and Li, Hang and Baldwin, Timothy and Qi, Jianzhong},
  doi          = {10.1162/tacl_a_00388},
  journal      = {Transactions of the Association for Computational Linguistics},
  pages        = {621-640},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Evaluating document coherence modeling},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). There once was a really bad poet, it was automated but you
didn’t know it. <em>TACL</em>, <em>9</em>, 605–620. (<a
href="https://doi.org/10.1162/tacl_a_00387">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Limerick generation exemplifies some of the most difficult challenges faced in poetry generation, as the poems must tell a story in only five lines, with constraints on rhyme, stress, and meter. To address these challenges, we introduce LimGen, a novel and fully automated system for limerick generation that outperforms state-of-the-art neural network-based poetry models, as well as prior rule-based poetry models. LimGen consists of three important pieces: the Adaptive Multi-Templated Constraint algorithm that constrains our search to the space of realistic poems, the Multi-Templated Beam Search algorithm which searches efficiently through the space, and the probabilistic Storyline algorithm that provides coherent storylines related to a user-provided prompt word. The resulting limericks satisfy poetic constraints and have thematically coherent storylines, which are sometimes even funny (when we are lucky).},
  archive      = {J_TACL},
  author       = {Wang, Jianyou and Zhang, Xiaoxuan and Zhou, Yuren and Suh, Christopher and Rudin, Cynthia},
  doi          = {10.1162/tacl_a_00387},
  journal      = {Transactions of the Association for Computational Linguistics},
  pages        = {605-620},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {There once was a really bad poet, it was automated but you didn’t know it},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Context-aware adversarial training for name regularity bias
in named entity recognition. <em>TACL</em>, <em>9</em>, 586–604. (<a
href="https://doi.org/10.1162/tacl_a_00386">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. In this work, we examine the ability of NER models to use contextual information when predicting the type of an ambiguous entity. We introduce NRB, a new testbed carefully designed to diagnose Name Regularity Bias of NER models. Our results indicate that all state-of-the-art models we tested show such a bias; BERT fine-tuned models significantly outperforming feature-based (LSTM-CRF) ones on NRB, despite having comparable (sometimes lower) performance on standard benchmarks.To mitigate this bias, we propose a novel model-agnostic training method that adds learnable adversarial noise to some entity mentions, thus enforcing models to focus more strongly on the contextual signal, leading to significant gains on NRB. Combining it with two other training strategies, data augmentation and parameter freezing, leads to further gains.},
  archive      = {J_TACL},
  author       = {Ghaddar, Abbas and Langlais, Philippe and Rashid, Ahmad and Rezagholizadeh, Mehdi},
  doi          = {10.1162/tacl_a_00386},
  journal      = {Transactions of the Association for Computational Linguistics},
  pages        = {586-604},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Context-aware adversarial training for name regularity bias in named entity recognition},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Decoupling the role of data, attention, and losses in
multimodal transformers. <em>TACL</em>, <em>9</em>, 570–585. (<a
href="https://doi.org/10.1162/tacl_a_00385">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Recently, multimodal transformer models have gained popularity because their performance on downstream tasks suggests they learn rich visual-linguistic representations. Focusing on zero-shot image retrieval tasks, we study three important factors that can impact the quality of learned representations: pretraining data, the attention mechanism, and loss functions. By pretraining models on six datasets, we observe that dataset noise and language similarity to our downstream task are important indicators of model performance. Through architectural analysis, we learn that models with a multimodal attention mechanism can outperform deeper models with modality-specific attention mechanisms. Finally, we show that successful contrastive losses used in the self-supervised learning literature do not yield similar performance gains when used in multimodal transformers.},
  archive      = {J_TACL},
  author       = {Hendricks, Lisa Anne and Mellor, John and Schneider, Rosalia and Alayrac, Jean-Baptiste and Nematzadeh, Aida},
  doi          = {10.1162/tacl_a_00385},
  journal      = {Transactions of the Association for Computational Linguistics},
  pages        = {570-585},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Decoupling the role of data, attention, and losses in multimodal transformers},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dialogue state tracking with incremental reasoning.
<em>TACL</em>, <em>9</em>, 557–569. (<a
href="https://doi.org/10.1162/tacl_a_00384">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Tracking dialogue states to better interpret user goals and feed downstream policy learning is a bottleneck in dialogue management. Common practice has been to treat it as a problem of classifying dialogue content into a set of pre-defined slot-value pairs, or generating values for different slots given the dialogue history. Both have limitations on considering dependencies that occur on dialogues, and are lacking of reasoning capabilities. This paper proposes to track dialogue states gradually with reasoning over dialogue turns with the help of the back-end data. Empirical results demonstrate that our method outperforms the state-of-the-art methods in terms of joint belief accuracy for MultiWOZ 2.1, a large-scale human--human dialogue dataset across multiple domains.},
  archive      = {J_TACL},
  author       = {Liao, Lizi and Long, Le Hong and Ma, Yunshan and Lei, Wenqiang and Chua, Tat-Seng},
  doi          = {10.1162/tacl_a_00384},
  journal      = {Transactions of the Association for Computational Linguistics},
  pages        = {557-569},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Dialogue state tracking with incremental reasoning},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Characterizing english variation across social media
communities with BERT. <em>TACL</em>, <em>9</em>, 538–556. (<a
href="https://doi.org/10.1162/tacl_a_00383">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Much previous work characterizing language variation across Internet social groups has focused on the types of words used by these groups. We extend this type of study by employing BERT to characterize variation in the senses of words as well, analyzing two months of English comments in 474 Reddit communities. The specificity of different sense clusters to a community, combined with the specificity of a community’s unique word types, is used to identify cases where a social group’s language deviates from the norm. We validate our metrics using user-created glossaries and draw on sociolinguistic theories to connect language variation with trends in community behavior. We find that communities with highly distinctive language are medium-sized, and their loyal and highly engaged users interact in dense networks.},
  archive      = {J_TACL},
  author       = {Lucy, Li and Bamman, David},
  doi          = {10.1162/tacl_a_00383},
  journal      = {Transactions of the Association for Computational Linguistics},
  pages        = {538-556},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Characterizing english variation across social media communities with BERT},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Optimizing over subsequences generates context-sensitive
languages. <em>TACL</em>, <em>9</em>, 528–537. (<a
href="https://doi.org/10.1162/tacl_a_00382">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Phonological generalizations are finite-state. While Optimality Theory is a popular framework for modeling phonology, it is known to generate non-finite-state mappings and languages. This paper demonstrates that Optimality Theory is capable of generating non-context-free languages, contributing to the characterization of its generative capacity. This is achieved with minimal modification to the theory as it is standardly employed.},
  archive      = {J_TACL},
  author       = {Lamont, Andrew},
  doi          = {10.1162/tacl_a_00382},
  journal      = {Transactions of the Association for Computational Linguistics},
  pages        = {528-537},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Optimizing over subsequences generates context-sensitive languages},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Data-to-text generation with macro planning. <em>TACL</em>,
<em>9</em>, 510–527. (<a
href="https://doi.org/10.1162/tacl_a_00381">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Recent approaches to data-to-text generation have adopted the very successful encoder-decoder architecture or variants thereof. These models generate text that is fluent (but often imprecise) and perform quite poorly at selecting appropriate content and ordering it coherently. To overcome some of these issues, we propose a neural model with a macro planning stage followed by a generation stage reminiscent of traditional methods which embrace separate modules for planning and surface realization. Macro plans represent high level organization of important content such as entities, events, and their interactions; they are learned from data and given as input to the generator. Extensive experiments on two data-to-text benchmarks (RotoWire and MLB) show that our approach outperforms competitive baselines in terms of automatic and human evaluation.},
  archive      = {J_TACL},
  author       = {Puduppully, Ratish and Lapata, Mirella},
  doi          = {10.1162/tacl_a_00381},
  journal      = {Transactions of the Association for Computational Linguistics},
  pages        = {510-527},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Data-to-text generation with macro planning},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Iterative paraphrastic augmentation with discriminative span
alignment. <em>TACL</em>, <em>9</em>, 494–509. (<a
href="https://doi.org/10.1162/tacl_a_00380">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We introduce a novel paraphrastic augmentation strategy based on sentence-level lexically constrained paraphrasing and discriminative span alignment. Our approach allows for the large-scale expansion of existing datasets or the rapid creation of new datasets using a small, manually produced seed corpus. We demonstrate our approach with experiments on the Berkeley FrameNet Project, a large-scale language understanding effort spanning more than two decades of human labor. With four days of training data collection for a span alignment model and one day of parallel compute, we automatically generate and release to the community 495,300 unique (Frame,Trigger) pairs in diverse sentential contexts, a roughly 50-fold expansion atop FrameNet v1.7. The resulting dataset is intrinsically and extrinsically evaluated in detail, showing positive results on a downstream task.},
  archive      = {J_TACL},
  author       = {Culkin, Ryan and Hu, J. Edward and Stengel-Eskin, Elias and Qin, Guanghui and Durme, Benjamin Van},
  doi          = {10.1162/tacl_a_00380},
  journal      = {Transactions of the Association for Computational Linguistics},
  pages        = {494-509},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Iterative paraphrastic augmentation with discriminative span alignment},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Neural OCR post-hoc correction of historical corpora.
<em>TACL</em>, <em>9</em>, 479–493. (<a
href="https://doi.org/10.1162/tacl_a_00379">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Optical character recognition (OCR) is crucial for a deeper access to historical collections. OCR needs to account for orthographic variations, typefaces, or language evolution (i.e., new letters, word spellings), as the main source of character, word, or word segmentation transcription errors. For digital corpora of historical prints, the errors are further exacerbated due to low scan quality and lack of language standardization.For the task of OCR post-hoc correction, we propose a neural approach based on a combination of recurrent (RNN) and deep convolutional network (ConvNet) to correct OCR transcription errors. At character level we flexibly capture errors, and decode the corrected output based on a novel attention mechanism. Accounting for the input and output similarity, we propose a new loss function that rewards the model’s correcting behavior.Evaluation on a historical book corpus in German language shows that our models are robust in capturing diverse OCR transcription errors and reduce the word error rate of 32.3\% by more than 89\%.},
  archive      = {J_TACL},
  author       = {Lyu, Lijun and Koutraki, Maria and Krickl, Martin and Fetahu, Besnik},
  doi          = {10.1162/tacl_a_00379},
  journal      = {Transactions of the Association for Computational Linguistics},
  pages        = {479-493},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Neural OCR post-hoc correction of historical corpora},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A computational framework for slang generation.
<em>TACL</em>, <em>9</em>, 462–478. (<a
href="https://doi.org/10.1162/tacl_a_00378">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Slang is a common type of informal language, but its flexible nature and paucity of data resources present challenges for existing natural language systems. We take an initial step toward machine generation of slang by developing a framework that models the speaker’s word choice in slang context. Our framework encodes novel slang meaning by relating the conventional and slang senses of a word while incorporating syntactic and contextual knowledge in slang usage. We construct the framework using a combination of probabilistic inference and neural contrastive learning. We perform rigorous evaluations on three slang dictionaries and show that our approach not only outperforms state-of-the-art language models, but also better predicts the historical emergence of slang word usages from 1960s to 2000s. We interpret the proposed models and find that the contrastively learned semantic space is sensitive to the similarities between slang and conventional senses of words. Our work creates opportunities for the automated generation and interpretation of informal language.},
  archive      = {J_TACL},
  author       = {Sun, Zhewei and Zemel, Richard and Xu, Yang},
  doi          = {10.1162/tacl_a_00378},
  journal      = {Transactions of the Association for Computational Linguistics},
  pages        = {462-478},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {A computational framework for slang generation},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Decontextualization: Making sentences stand-alone.
<em>TACL</em>, <em>9</em>, 447–461. (<a
href="https://doi.org/10.1162/tacl_a_00377">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Models for question answering, dialogue agents, and summarization often interpret the meaning of a sentence in a rich context and use that meaning in a new context. Taking excerpts of text can be problematic, as key pieces may not be explicit in a local window. We isolate and define the problem of sentence decontextualization: taking a sentence together with its context and rewriting it to be interpretable out of context, while preserving its meaning. We describe an annotation procedure, collect data on the Wikipedia corpus, and use the data to train models to automatically decontextualize sentences. We present preliminary studies that show the value of sentence decontextualization in a user-facing task, and as preprocessing for systems that perform document understanding. We argue that decontextualization is an important subtask in many downstream applications, and that the definitions and resources provided can benefit tasks that operate on sentences that occur in a richer context.},
  archive      = {J_TACL},
  author       = {Choi, Eunsol and Palomaki, Jennimaria and Lamm, Matthew and Kwiatkowski, Tom and Das, Dipanjan and Collins, Michael},
  doi          = {10.1162/tacl_a_00377},
  journal      = {Transactions of the Association for Computational Linguistics},
  pages        = {447-461},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Decontextualization: Making sentences stand-alone},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An error analysis framework for shallow surface realization.
<em>TACL</em>, <em>9</em>, 429–446. (<a
href="https://doi.org/10.1162/tacl_a_00376">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. The metrics standardly used to evaluate Natural Language Generation (NLG) models, such as BLEU or METEOR, fail to provide information on which linguistic factors impact performance. Focusing on Surface Realization (SR), the task of converting an unordered dependency tree into a well-formed sentence, we propose a framework for error analysis which permits identifying which features of the input affect the models’ results. This framework consists of two main components: (i) correlation analyses between a wide range of syntactic metrics and standard performance metrics and (ii) a set of techniques to automatically identify syntactic constructs that often co-occur with low performance scores. We demonstrate the advantages of our framework by performing error analysis on the results of 174 system runs submitted to the Multilingual SR shared tasks; we show that dependency edge accuracy correlate with automatic metrics thereby providing a more interpretable basis for evaluation; and we suggest ways in which our framework could be used to improve models and data. The framework is available in the form of a toolkit which can be used both by campaign organizers to provide detailed, linguistically interpretable feedback on the state of the art in multilingual SR, and by individual researchers to improve models and datasets.1},
  archive      = {J_TACL},
  author       = {Shimorina, Anastasia and Parmentier, Yannick and Gardent, Claire},
  doi          = {10.1162/tacl_a_00376},
  journal      = {Transactions of the Association for Computational Linguistics},
  pages        = {429-446},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {An error analysis framework for shallow surface realization},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Parameter space factorization for zero-shot learning across
tasks and languages. <em>TACL</em>, <em>9</em>, 410–428. (<a
href="https://doi.org/10.1162/tacl_a_00374">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Most combinations of NLP tasks and language varieties lack in-domain examples for supervised training because of the paucity of annotated data. How can neural models make sample-efficient generalizations from task–language combinations with available data to low-resource ones? In this work, we propose a Bayesian generative model for the space of neural parameters. We assume that this space can be factorized into latent variables for each language and each task. We infer the posteriors over such latent variables based on data from seen task–language combinations through variational inference. This enables zero-shot classification on unseen combinations at prediction time. For instance, given training data for named entity recognition (NER) in Vietnamese and for part-of-speech (POS) tagging in Wolof, our model can perform accurate predictions for NER in Wolof. In particular, we experiment with a typologically diverse sample of 33 languages from 4 continents and 11 families, and show that our model yields comparable or better results than state-of-the-art, zero-shot cross-lingual transfer methods. Our code is available at github.com/cambridgeltl/parameter-factorization.},
  archive      = {J_TACL},
  author       = {Ponti, Edoardo M. and Vulić, Ivan and Cotterell, Ryan and Parovic, Marinela and Reichart, Roi and Korhonen, Anna},
  doi          = {10.1162/tacl_a_00374},
  journal      = {Transactions of the Association for Computational Linguistics},
  pages        = {410-428},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Parameter space factorization for zero-shot learning across tasks and languages},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SummEval: Re-evaluating summarization evaluation.
<em>TACL</em>, <em>9</em>, 391–409. (<a
href="https://doi.org/10.1162/tacl_a_00373">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. The scarcity of comprehensive up-to-date studies on evaluation metrics for text summarization and the lack of consensus regarding evaluation protocols continue to inhibit progress. We address the existing shortcomings of summarization evaluation methods along five dimensions: 1) we re-evaluate 14 automatic evaluation metrics in a comprehensive and consistent fashion using neural summarization model outputs along with expert and crowd-sourced human annotations; 2) we consistently benchmark 23 recent summarization models using the aforementioned automatic evaluation metrics; 3) we assemble the largest collection of summaries generated by models trained on the CNN/DailyMail news dataset and share it in a unified format; 4) we implement and share a toolkit that provides an extensible and unified API for evaluating summarization models across a broad range of automatic metrics; and 5) we assemble and share the largest and most diverse, in terms of model types, collection of human judgments of model-generated summaries on the CNN/Daily Mail dataset annotated by both expert judges and crowd-source workers. We hope that this work will help promote a more complete evaluation protocol for text summarization as well as advance research in developing evaluation metrics that better correlate with human judgments.},
  archive      = {J_TACL},
  author       = {Fabbri, Alexander R. and Kryściński, Wojciech and McCann, Bryan and Xiong, Caiming and Socher, Richard and Radev, Dragomir},
  doi          = {10.1162/tacl_a_00373},
  journal      = {Transactions of the Association for Computational Linguistics},
  pages        = {391-409},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {SummEval: Re-evaluating summarization evaluation},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Unsupervised learning of KB queries in task-oriented
dialogs. <em>TACL</em>, <em>9</em>, 374–390. (<a
href="https://doi.org/10.1162/tacl_a_00372">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Task-oriented dialog (TOD) systems often need to formulate knowledge base (KB) queries corresponding to the user intent and use the query results to generate system responses. Existing approaches require dialog datasets to explicitly annotate these KB queries—these annotations can be time consuming, and expensive. In response, we define the novel problems of predicting the KB query and training the dialog agent, without explicit KB query annotation. For query prediction, we propose a reinforcement learning (RL) baseline, which rewards the generation of those queries whose KB results cover the entities mentioned in subsequent dialog. Further analysis reveals that correlation among query attributes in KB can significantly confuse memory augmented policy optimization (MAPO), an existing state of the art RL agent.To address this, we improve the MAPO baseline with simple but important modifications suited to our task. To train the full TOD system for our setting, we propose a pipelined approach: it independently predicts when to make a KB query (query position predictor), then predicts a KB query at the predicted position (query predictor), and uses the results of predicted query in subsequent dialog (next response predictor). Overall, our work proposes first solutions to our novel problem, and our analysis highlights the research challenges in training TOD systems without query annotation.},
  archive      = {J_TACL},
  author       = {Raghu, Dinesh and Gupta, Nikhil and Mausam, Mausam},
  doi          = {10.1162/tacl_a_00372},
  journal      = {Transactions of the Association for Computational Linguistics},
  pages        = {374-390},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Unsupervised learning of KB queries in task-oriented dialogs},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptive semiparametric language models. <em>TACL</em>,
<em>9</em>, 362–373. (<a
href="https://doi.org/10.1162/tacl_a_00371">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We present a language model that combines a large parametric neural network (i.e., a transformer) with a non-parametric episodic memory component in an integrated architecture. Our model uses extended short-term context by caching local hidden states—similar to transformer-XL—and global long-term memory by retrieving a set of nearest neighbor tokens at each timestep. We design a gating function to adaptively combine multiple information sources to make a prediction. This mechanism allows the model to use either local context, short-term memory, or long-term memory (or any combination of them) on an ad hoc basis depending on the context. Experiments on word-based and character-based language modeling datasets demonstrate the efficacy of our proposed method compared to strong baselines.},
  archive      = {J_TACL},
  author       = {Yogatama, Dani and de Masson d’Autume, Cyprien and Kong, Lingpeng},
  doi          = {10.1162/tacl_a_00371},
  journal      = {Transactions of the Association for Computational Linguistics},
  pages        = {362-373},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Adaptive semiparametric language models},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Did aristotle use a laptop? A question answering benchmark
with implicit reasoning strategies. <em>TACL</em>, <em>9</em>, 346–361.
(<a href="https://doi.org/10.1162/tacl_a_00370">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. A key limitation in current datasets for multi-hop reasoning is that the required steps for answering the question are mentioned in it explicitly. In this work, we introduce StrategyQA, a question answering (QA) benchmark where the required reasoning steps are implicit in the question, and should be inferred using a strategy. A fundamental challenge in this setup is how to elicit such creative questions from crowdsourcing workers, while covering a broad range of potential strategies. We propose a data collection procedure that combines term-based priming to inspire annotators, careful control over the annotator population, and adversarial filtering for eliminating reasoning shortcuts. Moreover, we annotate each question with (1) a decomposition into reasoning steps for answering it, and (2) Wikipedia paragraphs that contain the answers to each step. Overall, StrategyQA includes 2,780 examples, each consisting of a strategy question, its decomposition, and evidence paragraphs. Analysis shows that questions in StrategyQA are short, topic-diverse, and cover a wide range of strategies. Empirically, we show that humans perform well (87\%) on this task, while our best baseline reaches an accuracy of ∼ 66\%.},
  archive      = {J_TACL},
  author       = {Geva, Mor and Khashabi, Daniel and Segal, Elad and Khot, Tushar and Roth, Dan and Berant, Jonathan},
  doi          = {10.1162/tacl_a_00370},
  journal      = {Transactions of the Association for Computational Linguistics},
  pages        = {346-361},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sparse, dense, and attentional representations for text
retrieval. <em>TACL</em>, <em>9</em>, 329–345. (<a
href="https://doi.org/10.1162/tacl_a_00369">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Dual encoders perform retrieval by encoding documents and queries into dense low-dimensional vectors, scoring each document by its inner product with the query. We investigate the capacity of this architecture relative to sparse bag-of-words models and attentional neural networks. Using both theoretical and empirical analysis, we establish connections between the encoding dimension, the margin between gold and lower-ranked documents, and the document length, suggesting limitations in the capacity of fixed-length encodings to support precise retrieval of long documents. Building on these insights, we propose a simple neural model that combines the efficiency of dual encoders with some of the expressiveness of more costly attentional architectures, and explore sparse-dense hybrids to capitalize on the precision of sparse retrieval. These models outperform strong alternatives in large-scale retrieval.},
  archive      = {J_TACL},
  author       = {Luan, Yi and Eisenstein, Jacob and Toutanova, Kristina and Collins, Michael},
  doi          = {10.1162/tacl_a_00369},
  journal      = {Transactions of the Association for Computational Linguistics},
  pages        = {329-345},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Sparse, dense, and attentional representations for text retrieval},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). EDITOR: An edit-based transformer with repositioning for
neural machine translation with soft lexical constraints. <em>TACL</em>,
<em>9</em>, 311–328. (<a
href="https://doi.org/10.1162/tacl_a_00368">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We introduce an Edit-Based TransfOrmer with Repositioning (EDITOR), which makes sequence generation flexible by seamlessly allowing users to specify preferences in output lexical choice. Building on recent models for non-autoregressive sequence generation (Gu et al., 2019), EDITOR generates new sequences by iteratively editing hypotheses. It relies on a novel reposition operation designed to disentangle lexical choice from word positioning decisions, while enabling efficient oracles for imitation learning and parallel edits at decoding time. Empirically, EDITOR uses soft lexical constraints more effectively than the Levenshtein Transformer (Gu et al., 2019) while speeding up decoding dramatically compared to constrained beam search (Post and Vilar, 2018). EDITOR also achieves comparable or better translation quality with faster decoding speed than the Levenshtein Transformer on standard Romanian-English, English-German, and English-Japanese machine translation tasks.},
  archive      = {J_TACL},
  author       = {Xu, Weijia and Carpuat, Marine},
  doi          = {10.1162/tacl_a_00368},
  journal      = {Transactions of the Association for Computational Linguistics},
  pages        = {311-328},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {EDITOR: An edit-based transformer with repositioning for neural machine translation with soft lexical constraints},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Aligning faithful interpretations with their social
attribution. <em>TACL</em>, <em>9</em>, 294–310. (<a
href="https://doi.org/10.1162/tacl_a_00367">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We find that the requirement of model interpretations to be faithful is vague and incomplete. With interpretation by textual highlights as a case study, we present several failure cases. Borrowing concepts from social science, we identify that the problem is a misalignment between the causal chain of decisions (causal attribution) and the attribution of human behavior to the interpretation (social attribution). We reformulate faithfulness as an accurate attribution of causality to the model, and introduce the concept of aligned faithfulness: faithful causal chains that are aligned with their expected social behavior. The two steps of causal attribution and social attribution together complete the process of explaining behavior. With this formalization, we characterize various failures of misaligned faithful highlight interpretations, and propose an alternative causal chain to remedy the issues. Finally, we implement highlight explanations of the proposed causal format using contrastive explanations.},
  archive      = {J_TACL},
  author       = {Jacovi, Alon and Goldberg, Yoav},
  doi          = {10.1162/tacl_a_00367},
  journal      = {Transactions of the Association for Computational Linguistics},
  pages        = {294-310},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Aligning faithful interpretations with their social attribution},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Extractive opinion summarization in quantized transformer
spaces. <em>TACL</em>, <em>9</em>, 277–293. (<a
href="https://doi.org/10.1162/tacl_a_00366">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We present the Quantized Transformer (QT), an unsupervised system for extractive opinion summarization. QT is inspired by Vector- Quantized Variational Autoencoders, which we repurpose for popularity-driven summarization. It uses a clustering interpretation of the quantized space and a novel extraction algorithm to discover popular opinions among hundreds of reviews, a significant step towards opinion summarization of practical scope. In addition, QT enables controllable summarization without further training, by utilizing properties of the quantized space to extract aspect-specific summaries. We also make publicly available Space, a large-scale evaluation benchmark for opinion summarizers, comprising general and aspect-specific summaries for 50 hotels. Experiments demonstrate the promise of our approach, which is validated by human studies where judges showed clear preference for our method over competitive baselines.},
  archive      = {J_TACL},
  author       = {Angelidis, Stefanos and Amplayo, Reinald Kim and Suhara, Yoshihiko and Wang, Xiaolan and Lapata, Mirella},
  doi          = {10.1162/tacl_a_00366},
  journal      = {Transactions of the Association for Computational Linguistics},
  pages        = {277-293},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Extractive opinion summarization in quantized transformer spaces},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Morphology matters: A multilingual language modeling
analysis. <em>TACL</em>, <em>9</em>, 261–276. (<a
href="https://doi.org/10.1162/tacl_a_00365">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Prior studies in multilingual language modeling (e.g., Cotterell et al., 2018; Mielke et al., 2019) disagree on whether or not inflectional morphology makes languages harder to model. We attempt to resolve the disagreement and extend those studies. We compile a larger corpus of 145 Bible translations in 92 languages and a larger number of typological features.1 We fill in missing typological data for several languages and consider corpus-based measures of morphological complexity in addition to expert-produced typological features. We find that several morphological measures are significantly associated with higher surprisal when LSTM models are trained with BPE-segmented data. We also investigate linguistically motivated subword segmentation strategies like Morfessor and Finite-State Transducers (FSTs) and find that these segmentation strategies yield better performance and reduce the impact of a language’s morphology on language modeling.},
  archive      = {J_TACL},
  author       = {Park, Hyunji Hayley and Zhang, Katherine J. and Haley, Coleman and Steimel, Kenneth and Liu, Han and Schwartz, Lane},
  doi          = {10.1162/tacl_a_00365},
  journal      = {Transactions of the Association for Computational Linguistics},
  pages        = {261-276},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Morphology matters: A multilingual language modeling analysis},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Supertagging the long tail with tree-structured decoding of
complex categories. <em>TACL</em>, <em>9</em>, 243–260. (<a
href="https://doi.org/10.1162/tacl_a_00364">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Although current CCG supertaggers achieve high accuracy on the standard WSJ test set, few systems make use of the categories’ internal structure that will drive the syntactic derivation during parsing. The tagset is traditionally truncated, discarding the many rare and complex category types in the long tail. However, supertags are themselves trees. Rather than give up on rare tags, we investigate constructive models that account for their internal structure, including novel methods for tree-structured prediction. Our best tagger is capable of recovering a sizeable fraction of the long-tail supertags and even generates CCG categories that have never been seen in training, while approximating the prior state of the art in overall tag accuracy with fewer parameters. We further investigate how well different approaches generalize to out-of-domain evaluation sets.},
  archive      = {J_TACL},
  author       = {Prange, Jakob and Schneider, Nathan and Srikumar, Vivek},
  doi          = {10.1162/tacl_a_00364},
  journal      = {Transactions of the Association for Computational Linguistics},
  pages        = {243-260},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Supertagging the long tail with tree-structured decoding of complex categories},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Infusing finetuning with semantic dependencies.
<em>TACL</em>, <em>9</em>, 226–242. (<a
href="https://doi.org/10.1162/tacl_a_00363">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. For natural language processing systems, two kinds of evidence support the use of text representations from neural language models “pretrained” on large unannotated corpora: performance on application-inspired benchmarks (Peters et al., 2018, inter alia), and the emergence of syntactic abstractions in those representations (Tenney et al., 2019, inter alia). On the other hand, the lack of grounded supervision calls into question how well these representations can ever capture meaning (Bender and Koller, 2020). We apply novel probes to recent language models— specifically focusing on predicate-argument structure as operationalized by semantic dependencies (Ivanova et al., 2012)—and find that, unlike syntax, semantics is not brought to the surface by today’s pretrained models. We then use convolutional graph encoders to explicitly incorporate semantic parses into task-specific finetuning, yielding benefits to natural language understanding (NLU) tasks in the GLUE benchmark. This approach demonstrates the potential for general-purpose (rather than task-specific) linguistic supervision, above and beyond conventional pretraining and finetuning. Several diagnostics help to localize the benefits of our approach.1},
  archive      = {J_TACL},
  author       = {Wu, Zhaofeng and Peng, Hao and Smith, Noah A.},
  doi          = {10.1162/tacl_a_00363},
  journal      = {Transactions of the Association for Computational Linguistics},
  pages        = {226-242},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Infusing finetuning with semantic dependencies},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). WikiAsp: A dataset for multi-domain aspect-based
summarization. <em>TACL</em>, <em>9</em>, 211–225. (<a
href="https://doi.org/10.1162/tacl_a_00362">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Aspect-based summarization is the task of generating focused summaries based on specific points of interest. Such summaries aid efficient analysis of text, such as quickly understanding reviews or opinions from different angles. However, due to large differences in the type of aspects for different domains (e.g., sentiment, product features), the development of previous models has tended to be domain-specific. In this paper, we propose WikiAsp,1 a large-scale dataset for multi-domain aspect- based summarization that attempts to spur research in the direction of open-domain aspect-based summarization. Specifically, we build the dataset using Wikipedia articles from 20 different domains, using the section titles and boundaries of each article as a proxy for aspect annotation. We propose several straightforward baseline models for this task and conduct experiments on the dataset. Results highlight key challenges that existing summarization models face in this setting, such as proper pronoun handling of quoted sources and consistent explanation of time-sensitive events.},
  archive      = {J_TACL},
  author       = {Hayashi, Hiroaki and Budania, Prashant and Wang, Peng and Ackerson, Chris and Neervannan, Raj and Neubig, Graham},
  doi          = {10.1162/tacl_a_00362},
  journal      = {Transactions of the Association for Computational Linguistics},
  pages        = {211-225},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {WikiAsp: A dataset for multi-domain aspect-based summarization},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Latent compositional representations improve systematic
generalization in grounded question answering. <em>TACL</em>,
<em>9</em>, 195–210. (<a
href="https://doi.org/10.1162/tacl_a_00361">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Answering questions that involve multi-step reasoning requires decomposing them and using the answers of intermediate steps to reach the final answer. However, state-of-the-art models in grounded question answering often do not explicitly perform decomposition, leading to difficulties in generalization to out-of-distribution examples. In this work, we propose a model that computes a representation and denotation for all question spans in a bottom-up, compositional manner using a CKY-style parser. Our model induces latent trees, driven by end-to-end (the answer) supervision only. We show that this inductive bias towards tree structures dramatically improves systematic generalization to out-of- distribution examples, compared to strong baselines on an arithmetic expressions benchmark as well as on C losure, a dataset that focuses on systematic generalization for grounded question answering. On this challenging dataset, our model reaches an accuracy of 96.1\%, significantly higher than prior models that almost perfectly solve the task on a random, in-distribution split.},
  archive      = {J_TACL},
  author       = {Bogin, Ben and Subramanian, Sanjay and Gardner, Matt and Berant, Jonathan},
  doi          = {10.1162/tacl_a_00361},
  journal      = {Transactions of the Association for Computational Linguistics},
  pages        = {195-210},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Latent compositional representations improve systematic generalization in grounded question answering},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). KEPLER: A unified model for knowledge embedding and
pre-trained language representation. <em>TACL</em>, <em>9</em>, 176–194.
(<a href="https://doi.org/10.1162/tacl_a_00360">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Pre-trained language representation models (PLMs) cannot well capture factual knowledge from text. In contrast, knowledge embedding (KE) methods can effectively represent the relational facts in knowledge graphs (KGs) with informative entity embeddings, but conventional KE models cannot take full advantage of the abundant textual information. In this paper, we propose a unified model for Knowledge Embedding and Pre-trained LanguagERepresentation (KEPLER), which can not only better integrate factual knowledge into PLMs but also produce effective text-enhanced KE with the strong PLMs. In KEPLER, we encode textual entity descriptions with a PLM as their embeddings, and then jointly optimize the KE and language modeling objectives. Experimental results show that KEPLER achieves state-of-the-art performances on various NLP tasks, and also works remarkably well as an inductive KE model on KG link prediction. Furthermore, for pre-training and evaluating KEPLER, we construct Wikidata5M1 , a large-scale KG dataset with aligned entity descriptions, and benchmark state-of-the-art KE methods on it. It shall serve as a new KE benchmark and facilitate the research on large KG, inductive KE, and KG with text. The source code can be obtained from https://github.com/THU-KEG/KEPLER.},
  archive      = {J_TACL},
  author       = {Wang, Xiaozhi and Gao, Tianyu and Zhu, Zhaocheng and Zhang, Zhengyan and Liu, Zhiyuan and Li, Juanzi and Tang, Jian},
  doi          = {10.1162/tacl_a_00360},
  journal      = {Transactions of the Association for Computational Linguistics},
  pages        = {176-194},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {KEPLER: A unified model for knowledge embedding and pre-trained language representation},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Amnesic probing: Behavioral explanation with amnesic
counterfactuals. <em>TACL</em>, <em>9</em>, 160–175. (<a
href="https://doi.org/10.1162/tacl_a_00359">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. A growing body of work makes use of probing in order to investigate the working of neural models, often considered black boxes. Recently, an ongoing debate emerged surrounding the limitations of the probing paradigm. In this work, we point out the inability to infer behavioral conclusions from probing results, and offer an alternative method that focuses on how the information is being used, rather than on what information is encoded. Our method, Amnesic Probing, follows the intuition that the utility of a property for a given task can be assessed by measuring the influence of a causal intervention that removes it from the representation. Equipped with this new analysis tool, we can ask questions that were not possible before, for example, is part-of-speech information important for word prediction? We perform a series of analyses on BERT to answer these types of questions. Our findings demonstrate that conventional probing performance is not correlated to task importance, and we call for increased scrutiny of claims that draw behavioral or causal conclusions from probing results.1},
  archive      = {J_TACL},
  author       = {Elazar, Yanai and Ravfogel, Shauli and Jacovi, Alon and Goldberg, Yoav},
  doi          = {10.1162/tacl_a_00359},
  journal      = {Transactions of the Association for Computational Linguistics},
  pages        = {160-175},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Amnesic probing: Behavioral explanation with amnesic counterfactuals},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On the relationships between the grammatical genders of
inanimate nouns and their co-occurring adjectives and verbs.
<em>TACL</em>, <em>9</em>, 139–159. (<a
href="https://doi.org/10.1162/tacl_a_00355">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We use large-scale corpora in six different gendered languages, along with tools from NLP and information theory, to test whether there is a relationship between the grammatical genders of inanimate nouns and the adjectives used to describe those nouns. For all six languages, we find that there is a statistically significant relationship. We also find that there are statistically significant relationships between the grammatical genders of inanimate nouns and the verbs that take those nouns as direct objects, as indirect objects, and as subjects. We defer deeper investigation of these relationships for future work.},
  archive      = {J_TACL},
  author       = {Williams, Adina and Cotterell, Ryan and Wolf-Sonkin, Lawrence and Blasi, Damián and Wallach, Hanna},
  doi          = {10.1162/tacl_a_00355},
  journal      = {Transactions of the Association for Computational Linguistics},
  pages        = {139-159},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {On the relationships between the grammatical genders of inanimate nouns and their co-occurring adjectives and verbs},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Recursive non-autoregressive graph-to-graph transformer for
dependency parsing with iterative refinement. <em>TACL</em>, <em>9</em>,
120–138. (<a href="https://doi.org/10.1162/tacl_a_00358">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We propose the Recursive Non-autoregressive Graph-to-Graph Transformer architecture (RNGTr) for the iterative refinement of arbitrary graphs through the recursive application of a non-autoregressive Graph-to-Graph Transformer and apply it to syntactic dependency parsing. We demonstrate the power and effectiveness of RNGTr on several dependency corpora, using a refinement model pre-trained with BERT. We also introduce Syntactic Transformer (SynTr), a non-recursive parser similar to our refinement model. RNGTr can improve the accuracy of a variety of initial parsers on 13 languages from the Universal Dependencies Treebanks, English and Chinese Penn Treebanks, and the German CoNLL2009 corpus, even improving over the new state-of-the-art results achieved by SynTr, significantly improving the state-of-the-art for all corpora tested.},
  archive      = {J_TACL},
  author       = {Mohammadshahi, Alireza and Henderson, James},
  doi          = {10.1162/tacl_a_00358},
  journal      = {Transactions of the Association for Computational Linguistics},
  pages        = {120-138},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Recursive non-autoregressive graph-to-graph transformer for dependency parsing with iterative refinement},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Modeling content and context with deep relational learning.
<em>TACL</em>, <em>9</em>, 100–119. (<a
href="https://doi.org/10.1162/tacl_a_00357">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Building models for realistic natural language tasks requires dealing with long texts and accounting for complicated structural dependencies. Neural-symbolic representations have emerged as a way to combine the reasoning capabilities of symbolic methods, with the expressiveness of neural networks. However, most of the existing frameworks for combining neural and symbolic representations have been designed for classic relational learning tasks that work over a universe of symbolic entities and relations. In this paper, we present DRaiL, an open-source declarative framework for specifying deep relational models, designed to support a variety of NLP scenarios. Our framework supports easy integration with expressive language encoders, and provides an interface to study the interactions between representation, inference and learning.},
  archive      = {J_TACL},
  author       = {Pacheco, Maria Leonor and Goldwasser, Dan},
  doi          = {10.1162/tacl_a_00357},
  journal      = {Transactions of the Association for Computational Linguistics},
  pages        = {100-119},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Modeling content and context with deep relational learning},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Augmenting transformers with KNN-based composite memory for
dialog. <em>TACL</em>, <em>9</em>, 82–99. (<a
href="https://doi.org/10.1162/tacl_a_00356">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Various machine learning tasks can benefit from access to external information of different modalities, such as text and images. Recent work has focused on learning architectures with large memories capable of storing this knowledge. We propose augmenting generative Transformer neural networks with KNN-based Information Fetching (KIF) modules. Each KIF module learns a read operation to access fixed external knowledge. We apply these modules to generative dialog modeling, a challenging task where information must be flexibly retrieved and incorporated to maintain the topic and flow of conversation. We demonstrate the effectiveness of our approach by identifying relevant knowledge required for knowledgeable but engaging dialog from Wikipedia, images, and human-written dialog utterances, and show that leveraging this retrieved information improves model performance, measured by automatic and human evaluation.},
  archive      = {J_TACL},
  author       = {Fan, Angela and Gardent, Claire and Braud, Chloé and Bordes, Antoine},
  doi          = {10.1162/tacl_a_00356},
  journal      = {Transactions of the Association for Computational Linguistics},
  pages        = {82-99},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Augmenting transformers with KNN-based composite memory for dialog},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deciphering undersegmented ancient scripts using phonetic
prior. <em>TACL</em>, <em>9</em>, 69–81. (<a
href="https://doi.org/10.1162/tacl_a_00354">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Most undeciphered lost languages exhibit two characteristics that pose significant decipherment challenges: (1) the scripts are not fully segmented into words; (2) the closest known language is not determined. We propose a decipherment model that handles both of these challenges by building on rich linguistic constraints reflecting consistent patterns in historical sound change. We capture the natural phonological geometry by learning character embeddings based on the International Phonetic Alphabet (IPA). The resulting generative framework jointly models word segmentation and cognate alignment, informed by phonological constraints. We evaluate the model on both deciphered languages (Gothic, Ugaritic) and an undeciphered one (Iberian). The experiments show that incorporating phonetic geometry leads to clear and consistent gains. Additionally, we propose a measure for language closeness which correctly identifies related languages for Gothic and Ugaritic. For Iberian, the method does not show strong evidence supporting Basque as a related language, concurring with the favored position by the current scholarship.1},
  archive      = {J_TACL},
  author       = {Luo, Jiaming and Hartmann, Frederik and Santus, Enrico and Barzilay, Regina and Cao, Yuan},
  doi          = {10.1162/tacl_a_00354},
  journal      = {Transactions of the Association for Computational Linguistics},
  pages        = {69-81},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Deciphering undersegmented ancient scripts using phonetic prior},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Efficient content-based sparse attention with routing
transformers. <em>TACL</em>, <em>9</em>, 53–68. (<a
href="https://doi.org/10.1162/tacl_a_00353">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Self-attention has recently been adopted for a wide range of sequence modeling problems. Despite its effectiveness, self-attention suffers from quadratic computation and memory requirements with respect to sequence length. Successful approaches to reduce this complexity focused on attending to local sliding windows or a small set of locations independent of content. Our work proposes to learn dynamic sparse attention patterns that avoid allocating computation and memory to attend to content unrelated to the query of interest. This work builds upon two lines of research: It combines the modeling flexibility of prior work on content-based sparse attention with the efficiency gains from approaches based on local, temporal sparse attention. Our model, the Routing Transformer, endows self-attention with a sparse routing module based on online k-means while reducing the overall complexity of attention to O(n1.5d) from O(n2d) for sequence length n and hidden dimension d. We show that our model outperforms comparable sparse attention models on language modeling on Wikitext-103 (15.8 vs 18.3 perplexity), as well as on image generation on ImageNet-64 (3.43 vs 3.44 bits/dim) while using fewer self-attention layers. Additionally, we set a new state-of-the-art on the newly released PG-19 data-set, obtaining a test perplexity of 33.2 with a 22 layer Routing Transformer model trained on sequences of length 8192. We open-source the code for Routing Transformer in Tensorflow.1},
  archive      = {J_TACL},
  author       = {Roy, Aurko and Saffar, Mohammad and Vaswani, Ashish and Grangier, David},
  doi          = {10.1162/tacl_a_00353},
  journal      = {Transactions of the Association for Computational Linguistics},
  pages        = {53-68},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Efficient content-based sparse attention with routing transformers},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Conversation graph: Data augmentation, training, and
evaluation for non-deterministic dialogue management. <em>TACL</em>,
<em>9</em>, 36–52. (<a
href="https://doi.org/10.1162/tacl_a_00352">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Task-oriented dialogue systems typically rely on large amounts of high-quality training data or require complex handcrafted rules. However, existing datasets are often limited in size con- sidering the complexity of the dialogues. Additionally, conventional training signal in- ference is not suitable for non-deterministic agent behavior, namely, considering multiple actions as valid in identical dialogue states. We propose the Conversation Graph (ConvGraph), a graph-based representation of dialogues that can be exploited for data augmentation, multi- reference training and evaluation of non- deterministic agents. ConvGraph generates novel dialogue paths to augment data volume and diversity. Intrinsic and extrinsic evaluation across three datasets shows that data augmentation and/or multi-reference training with ConvGraph can improve dialogue success rates by up to 6.4\%.},
  archive      = {J_TACL},
  author       = {Gritta, Milan and Lampouras, Gerasimos and Iacobacci, Ignacio},
  doi          = {10.1162/tacl_a_00352},
  journal      = {Transactions of the Association for Computational Linguistics},
  pages        = {36-52},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Conversation graph: Data augmentation, training, and evaluation for non-deterministic dialogue management},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Revisiting multi-domain machine translation. <em>TACL</em>,
<em>9</em>, 17–35. (<a
href="https://doi.org/10.1162/tacl_a_00351">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. When building machine translation systems, one often needs to make the best out of heterogeneous sets of parallel data in training, and to robustly handle inputs from unexpected domains in testing. This multi-domain scenario has attracted a lot of recent work that fall under the general umbrella of transfer learning. In this study, we revisit multi-domain machine translation, with the aim to formulate the motivations for developing such systems and the associated expectations with respect to performance. Our experiments with a large sample of multi-domain systems show that most of these expectations are hardly met and suggest that further work is needed to better analyze the current behaviour of multi-domain systems and to make them fully hold their promises.},
  archive      = {J_TACL},
  author       = {Pham, MinhQuang and Crego, Josep Maria and Yvon, François},
  doi          = {10.1162/tacl_a_00351},
  journal      = {Transactions of the Association for Computational Linguistics},
  pages        = {17-35},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Revisiting multi-domain machine translation},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Reducing confusion in active learning for part-of-speech
tagging. <em>TACL</em>, <em>9</em>, 1–16. (<a
href="https://doi.org/10.1162/tacl_a_00350">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Active learning (AL) uses a data selection algorithm to select useful training samples to minimize annotation cost. This is now an essential tool for building low-resource syntactic analyzers such as part-of-speech (POS) taggers. Existing AL heuristics are generally designed on the principle of selecting uncertain yet representative training instances, where annotating these instances may reduce a large number of errors. However, in an empirical study across six typologically diverse languages (German, Swedish, Galician, North Sami, Persian, and Ukrainian), we found the surprising result that even in an oracle scenario where we know the true uncertainty of predictions, these current heuristics are far from optimal. Based on this analysis, we pose the problem of AL as selecting instances that maximally reduce the confusion between particular pairs of output tags. Extensive experimentation on the aforementioned languages shows that our proposed AL strategy outperforms other AL strategies by a significant margin. We also present auxiliary results demonstrating the importance of proper calibration of models, which we ensure through cross-view training, and analysis demonstrating how our proposed strategy selects examples that more closely follow the oracle data distribution. The code is publicly released here.1},
  archive      = {J_TACL},
  author       = {Chaudhary, Aditi and Anastasopoulos, Antonios and Sheikh, Zaid and Neubig, Graham},
  doi          = {10.1162/tacl_a_00350},
  journal      = {Transactions of the Association for Computational Linguistics},
  pages        = {1-16},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Reducing confusion in active learning for part-of-speech tagging},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
</ul>

</body>
</html>
