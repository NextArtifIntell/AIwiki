<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>NECO_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="neco---103">NECO - 103</h2>
<ul>
<li><details>
<summary>
(2021). Bayesian quadrature optimization for probability threshold
robustness measure. <em>NECO</em>, <em>33</em>(12), 3413–3466. (<a
href="https://doi.org/10.1162/neco_a_01442">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many product development problems, the performance of the product is governed by two types of parameters: design parameters and environmental parameters. While the former is fully controllable, the latter varies depending on the environment in which the product is used. The challenge of such a problem is to find the design parameter that maximizes the probability that the performance of the product will meet the desired requisite level given the variation of the environmental parameter. In this letter, we formulate this practical problem as active learning (AL) problems and propose efficient algorithms with theoretically guaranteed performance. Our basic idea is to use a gaussian process (GP) model as the surrogate model of the product development process and then to formulate our AL problems as Bayesian quadrature optimization problems for probabilistic threshold robustness (PTR) measure. We derive credible intervals for the PTR measure and propose AL algorithms for the optimization and level set estimation of the PTR measure. We clarify the theoretical properties of the proposed algorithms and demonstrate their efficiency in both synthetic and real-world product development problems.},
  archive      = {J_NECO},
  author       = {Iwazaki, Shogo and Inatsu, Yu and Takeuchi, Ichiro},
  doi          = {10.1162/neco_a_01442},
  journal      = {Neural Computation},
  number       = {12},
  pages        = {3413-3466},
  shortjournal = {Neural Comput.},
  title        = {Bayesian quadrature optimization for probability threshold robustness measure},
  volume       = {33},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Semisupervised ordinal regression based on empirical risk
minimization. <em>NECO</em>, <em>33</em>(12), 3361–3412. (<a
href="https://doi.org/10.1162/neco_a_01445">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ordinal regression is aimed at predicting an ordinal class label. In this letter, we consider its semisupervised formulation, in which we have unlabeled data along with ordinal-labeled data to train an ordinal regressor. There are several metrics to evaluate the performance of ordinal regression, such as the mean absolute error, mean zero-one error, and mean squared error. However, the existing studies do not take the evaluation metric into account, restrict model choice, and have no theoretical guarantee. To overcome these problems, we propose a novel generic framework for semisupervised ordinal regression based on the empirical risk minimization principle that is applicable to optimizing all of the metrics mentioned above. In addition, our framework has flexible choices of models, surrogate losses, and optimization algorithms without the common geometric assumption on unlabeled data such as the cluster assumption or manifold assumption. We provide an estimation error bound to show that our risk estimator is consistent. Finally, we conduct experiments to show the usefulness of our framework.},
  archive      = {J_NECO},
  author       = {Tsuchiya, Taira and Charoenphakdee, Nontawat and Sato, Issei and Sugiyama, Masashi},
  doi          = {10.1162/neco_a_01445},
  journal      = {Neural Computation},
  number       = {12},
  pages        = {3361-3412},
  shortjournal = {Neural Comput.},
  title        = {Semisupervised ordinal regression based on empirical risk minimization},
  volume       = {33},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Asymmetric weights and retrieval practice in an
autoassociative neural network model of paired-associate learning.
<em>NECO</em>, <em>33</em>(12), 3351–3360. (<a
href="https://doi.org/10.1162/neco_a_01444">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rizzuto and Kahana ( 2001 ) applied an autoassociative Hopfield network to a paired-associate word learning experiment in which (1) participants studied word pairs (e.g., ABSENCE-HOLLOW), (2) were tested in one direction (ABSENCE-?) on a first test, and (3) were tested in the same direction again or in the reverse direction (?-HOLLOW) on a second test. The model contained a correlation parameter to capture the dependence between forward versus backward learning between the two words of a word pair, revealing correlation values close to 1.0 for all participants, consistent with neural network models that use the same weight for communication in both directions between nodes. We addressed several limitations of the model simulations and proposed two new models incorporating retrieval practice learning (e.g., the effect of the first test on the second) that fit the accuracy data more effectively, revealing substantially lower correlation values (average of .45 across participants, with zero correlation for some participants). In addition, we analyzed recall latencies, finding that second test recall was faster in the same direction after a correct first test. Only a model with stochastic retrieval practice learning predicted this effect. In conclusion, recall accuracy and recall latency suggest asymmetric learning, particularly in light of retrieval practice effects.},
  archive      = {J_NECO},
  author       = {Aenugu, Sneha and Huber, David E.},
  doi          = {10.1162/neco_a_01444},
  journal      = {Neural Computation},
  number       = {12},
  pages        = {3351-3360},
  shortjournal = {Neural Comput.},
  title        = {Asymmetric weights and retrieval practice in an autoassociative neural network model of paired-associate learning},
  volume       = {33},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Simple convolutional-based models: Are they learning the
task or the data? <em>NECO</em>, <em>33</em>(12), 3334–3350. (<a
href="https://doi.org/10.1162/neco_a_01446">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks (CNNs) evolved from Fukushima&#39;s neocognitron model, which is based on the ideas of Hubel and Wiesel about the early stages of the visual cortex. Unlike other branches of neocognitron-based models, the typical CNN is based on end-to-end supervised learning by backpropagation and removes the focus from built-in invariance mechanisms, using pooling not as a way to tolerate small shifts but as a regularization tool that decreases model complexity. These properties of end-to-end supervision and flexibility of structure allow the typical CNN to become highly tuned to the training data, leading to extremely high accuracies on typical visual pattern recognition data sets. However, in this work, we hypothesize that there is a flip side to this capability, a hidden overfitting. More concretely, a supervised, backpropagation based CNN will outperform a neocognitron/map transformation cascade (MTC) when trained and tested inside the same data set. Yet if we take both models trained and test them on the same task but on another data set (without retraining), the overfitting appears. Other neocognitron descendants like the What-Where model go in a different direction. In these models, learning remains unsupervised, but more structure is added to capture invariance to typical changes. Knowing that, we further hypothesize that if we repeat the same experiments with this model, the lack of supervision may make it worse than the typical CNN inside the same data set, but the added structure will make it generalize even better to another one. To put our hypothesis to the test, we choose the simple task of handwritten digit classification and take two well-known data sets of it: MNIST and ETL-1. To try to make the two data sets as similar as possible, we experiment with several types of preprocessing. However, regardless of the type in question, the results align exactly with expectation.},
  archive      = {J_NECO},
  author       = {Sa-Couto, Luis and Wichert, Andreas},
  doi          = {10.1162/neco_a_01446},
  journal      = {Neural Computation},
  number       = {12},
  pages        = {3334-3350},
  shortjournal = {Neural Comput.},
  title        = {Simple convolutional-based models: Are they learning the task or the data?},
  volume       = {33},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Burster reconstruction considering unmeasurable variables in
the epileptor model. <em>NECO</em>, <em>33</em>(12), 3288–3333. (<a
href="https://doi.org/10.1162/neco_a_01443">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Epilepsy is one of the most common brain disorders worldwide, affecting millions of people every year. Although significant effort has been put into better understanding it and mitigating its effects, the conventional treatments are not fully effective. Advances in computational neuroscience, using mathematical dynamic models that represent brain activities at different scales, have enabled addressing epilepsy from a more theoretical standpoint. In particular, the recently proposed Epileptor model stands out among these models, because it represents well the main features of seizures, and the results from its simulations have been consistent with experimental observations. In addition, there has been an increasing interest in designing control techniques for Epileptor that might lead to possible realistic feedback controllers in the future. However, such approaches rely on knowing all of the states of the model, which is not the case in practice. The work explored in this letter aims to develop a state observer to estimate Epileptor&#39;s unmeasurable variables, as well as reconstruct the respective so-called bursters. Furthermore, an alternative modeling is presented for enhancing the convergence speed of an observer. The results show that the proposed approach is efficient under two main conditions: when the brain is undergoing a seizure and when a transition from the healthy to the epileptiform activity occurs.},
  archive      = {J_NECO},
  author       = {Brogin, João Angelo Ferres and Faber, Jean and Bueno, Douglas Domingues},
  doi          = {10.1162/neco_a_01443},
  journal      = {Neural Computation},
  number       = {12},
  pages        = {3288-3333},
  shortjournal = {Neural Comput.},
  title        = {Burster reconstruction considering unmeasurable variables in the epileptor model},
  volume       = {33},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning the synaptic and intrinsic membrane dynamics
underlying working memory in spiking neural network models.
<em>NECO</em>, <em>33</em>(12), 3264–3287. (<a
href="https://doi.org/10.1162/neco_a_01409">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recurrent neural network (RNN) models trained to perform cognitive tasks are a useful computational tool for understanding how cortical circuits execute complex computations. However, these models are often composed of units that interact with one another using continuous signals and overlook parameters intrinsic to spiking neurons. Here, we developed a method to directly train not only synaptic-related variables but also membrane-related parameters of a spiking RNN model. Training our model on a wide range of cognitive tasks resulted in diverse yet task-specific synaptic and membrane parameters. We also show that fast membrane time constants and slow synaptic decay dynamics naturally emerge from our model when it is trained on tasks associated with working memory (WM). Further dissecting the optimized parameters revealed that fast membrane properties are important for encoding stimuli, and slow synaptic dynamics are needed for WM maintenance. This approach offers a unique window into how connectivity patterns and intrinsic neuronal properties contribute to complex dynamics in neural populations.},
  archive      = {J_NECO},
  author       = {Li, Yinghao and Kim, Robert and Sejnowski, Terrence J.},
  doi          = {10.1162/neco_a_01409},
  journal      = {Neural Computation},
  number       = {12},
  pages        = {3264-3287},
  shortjournal = {Neural Comput.},
  title        = {Learning the synaptic and intrinsic membrane dynamics underlying working memory in spiking neural network models},
  volume       = {33},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On PDE characterization of smooth hierarchical functions
computed by neural networks. <em>NECO</em>, <em>33</em>(12), 3204–3263.
(<a href="https://doi.org/10.1162/neco_a_01441">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural networks are versatile tools for computation, having the ability to approximate a broad range of functions. An important problem in the theory of deep neural networks is expressivity; that is, we want to understand the functions that are computable by a given network. We study real, infinitely differentiable (smooth) hierarchical functions implemented by feedforward neural networks via composing simpler functions in two cases: (1) each constituent function of the composition has fewer inputs than the resulting function and (2) constituent functions are in the more specific yet prevalent form of a nonlinear univariate function (e.g., tanh) applied to a linear multivariate function. We establish that in each of these regimes, there exist nontrivial algebraic partial differential equations (PDEs) that are satisfied by the computed functions. These PDEs are purely in terms of the partial derivatives and are dependent only on the topology of the network. Conversely, we conjecture that such PDE constraints, once accompanied by appropriate nonsingularity conditions and perhaps certain inequalities involving partial derivatives, guarantee that the smooth function under consideration can be represented by the network. The conjecture is verified in numerous examples, including the case of tree architectures, which are of neuroscientific interest. Our approach is a step toward formulating an algebraic description of functional spaces associated with specific neural networks, and may provide useful new tools for constructing neural networks.},
  archive      = {J_NECO},
  author       = {Filom, Khashayar and Farhoodi, Roozbeh and Kording, Konrad Paul},
  doi          = {10.1162/neco_a_01441},
  journal      = {Neural Computation},
  number       = {12},
  pages        = {3204-3263},
  shortjournal = {Neural Comput.},
  title        = {On PDE characterization of smooth hierarchical functions computed by neural networks},
  volume       = {33},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A correspondence between normalization strategies in
artificial and biological neural networks. <em>NECO</em>,
<em>33</em>(12), 3179–3203. (<a
href="https://doi.org/10.1162/neco_a_01439">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A fundamental challenge at the interface of machine learning and neuroscience is to uncover computational principles that are shared between artificial and biological neural networks. In deep learning, normalization methods such as batch normalization, weight normalization, and their many variants help to stabilize hidden unit activity and accelerate network training, and these methods have been called one of the most important recent innovations for optimizing deep networks. In the brain, homeostatic plasticity represents a set of mechanisms that also stabilize and normalize network activity to lie within certain ranges, and these mechanisms are critical for maintaining normal brain function. In this article, we discuss parallels between artificial and biological normalization methods at four spatial scales: normalization of a single neuron&#39;s activity, normalization of synaptic weights of a neuron, normalization of a layer of neurons, and normalization of a network of neurons. We argue that both types of methods are functionally equivalent—that is, both push activation patterns of hidden units toward a homeostatic state, where all neurons are equally used—and we argue that such representations can improve coding capacity, discrimination, and regularization. As a proof of concept, we develop an algorithm, inspired by a neural normalization technique called synaptic scaling , and show that this algorithm performs competitively against existing normalization methods on several data sets. Overall, we hope this bidirectional connection will inspire neuroscientists and machine learners in three ways: to uncover new normalization algorithms based on established neurobiological principles; to help quantify the trade-offs of different homeostatic plasticity mechanisms used in the brain; and to offer insights about how stability may not hinder, but may actually promote, plasticity.},
  archive      = {J_NECO},
  author       = {Shen, Yang and Wang, Julia and Navlakha, Saket},
  doi          = {10.1162/neco_a_01439},
  journal      = {Neural Computation},
  number       = {12},
  pages        = {3179-3203},
  shortjournal = {Neural Comput.},
  title        = {A correspondence between normalization strategies in artificial and biological neural networks},
  volume       = {33},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Task-agnostic continual learning using online variational
bayes with fixed-point updates. <em>NECO</em>, <em>33</em>(11),
3139–3177. (<a href="https://doi.org/10.1162/neco_a_01430">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Catastrophic forgetting is the notorious vulnerability of neural networks to the changes in the data distribution during learning. This phenomenon has long been considered a major obstacle for using learning agents in realistic continual learning settings. A large body of continual learning research assumes that task boundaries are known during training. However, only a few works consider scenarios in which task boundaries are unknown or not well defined: task-agnostic scenarios. The optimal Bayesian solution for this requires an intractable online Bayes update to the weights posterior. We aim to approximate the online Bayes update as accurately as possible. To do so, we derive novel fixed-point equations for the online variational Bayes optimization problem for multivariate gaussian parametric distributions. By iterating the posterior through these fixed-point equations, we obtain an algorithm (FOO-VB) for continual learning that can handle nonstationary data distribution using a fixed architecture and without using external memory (i.e., without access to previous data). We demonstrate that our method (FOO-VB) outperforms existing methods in task-agnostic scenarios. FOO-VB Pytorch implementation is available at https://github.com/chenzeno/FOO-VB .},
  archive      = {J_NECO},
  author       = {Zeno, Chen and Golan, Itay and Hoffer, Elad and Soudry, Daniel},
  doi          = {10.1162/neco_a_01430},
  journal      = {Neural Computation},
  number       = {11},
  pages        = {3139-3177},
  shortjournal = {Neural Comput.},
  title        = {Task-agnostic continual learning using online variational bayes with fixed-point updates},
  volume       = {33},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Asymptotic input-output relationship predicts electric field
effect on sublinear dendritic integration of AMPA synapses.
<em>NECO</em>, <em>33</em>(11), 3102–3138. (<a
href="https://doi.org/10.1162/neco_a_01438">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An extracellular electric field (EF) induces transmembrane polarizations on extremely inhomogeneous spaces. Evidence shows that EF-induced somatic polarization in pyramidal cells can modulate the neuronal input-output (I/O) function. However, it remains unclear whether and how dendritic polarization participates in the dendritic integration and contributes to the neuronal I/O function. To this end, we built a computational model of a simplified pyramidal cell with multi-dendritic tufts, one dendritic trunk, and one soma to describe the interactions among EF, dendritic integration, and somatic output, in which the EFs were modeled by inserting inhomogeneous extracellular potentials. We aimed to establish the underlying relationship between dendritic polarization and dendritic integration by analyzing the dynamics of subthreshold membrane potentials in response to AMPA synapses in the presence of constant EFs. The model-based singular perturbation analysis showed that the equilibrium mapping of a fast subsystem can serve as the asymptotic subthreshold I/O relationship for sublinear dendritic integration. This allows us to predict the tendency of EF-mediated dendritic integration by showing how EF changes modify equilibrium mapping. EF-induced hyperpolarization of distal dendrites receiving synapses inputs was found to play a key role in facilitating the AMPA receptor-evoked excitatory postsynaptic potential (EPSP) by enhancing the driving force of synaptic inputs. A significantly higher efficacy of EF modulation effect on global AMPA-type dendritic integration was found compared with local AMPA-type dendritic integration. During the generation of an action potential (AP), the relative contribution of EF-modulated dendritic integration and EF-induced somatic polarization was determined to show their collaboration in promoting or inhibiting the somatic excitability, depending on the EF polarity. These findings are crucial for understanding the EF modulation effect on neuronal computation, which provides insight into the modulation mechanism of noninvasive brain modulation.},
  archive      = {J_NECO},
  author       = {Fan, Yaqin and Wei, Xile and Yi, Guosheng and Lu, Meili and Wang, Jiang and Deng, Bin},
  doi          = {10.1162/neco_a_01438},
  journal      = {Neural Computation},
  number       = {11},
  pages        = {3102-3138},
  shortjournal = {Neural Comput.},
  title        = {Asymptotic input-output relationship predicts electric field effect on sublinear dendritic integration of AMPA synapses},
  volume       = {33},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Expansion of information in the binary autoencoder with
random binary weights. <em>NECO</em>, <em>33</em>(11), 3073–3101. (<a
href="https://doi.org/10.1162/neco_a_01435">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This letter studies the expansion and preservation of information in a binary autoencoder where the hidden layer is larger than the input. Such expansion is widespread in biological neural networks, as in the olfactory system of a fruit fly or the projection of thalamic inputs to the neocortex. We analyze the threshold model, the kWTA model, and the binary matching pursuit model to find how the sparsity and the dimension of the encoding influence the input reconstruction, similarity preservation, and mutual information across layers. It is shown that the sparser activation of the hidden layer is preferable for preserving information between the input and the output layers. All three models show optimal similarity preservation at dense, not sparse, hidden layer activation. Furthermore, with a large enough hidden layer, it is possible to get zero reconstruction error for any input just by varying the thresholds of neurons. However, we show that the preference for sparsity is due to the noise in the weight matrix between layers. A fixed number of nonzero connections to every neuron achieves better information preservation and input reconstruction for the dense hidden layer activation. The theoretical results give useful insight into models of neural computation based on sparse binary representation and association memory.},
  archive      = {J_NECO},
  author       = {Osaulenko, Viacheslav M.},
  doi          = {10.1162/neco_a_01435},
  journal      = {Neural Computation},
  number       = {11},
  pages        = {3073-3101},
  shortjournal = {Neural Comput.},
  title        = {Expansion of information in the binary autoencoder with random binary weights},
  volume       = {33},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Temporal variabilities provide additional category-related
information in object category decoding: A systematic comparison of
informative EEG features. <em>NECO</em>, <em>33</em>(11), 3027–3072. (<a
href="https://doi.org/10.1162/neco_a_01436">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How does the human brain encode visual object categories? Our understanding of this has advanced substantially with the development of multivariate decoding analyses. However, conventional electroencephalography (EEG) decoding predominantly uses the mean neural activation within the analysis window to extract category information. Such temporal averaging overlooks the within-trial neural variability that is suggested to provide an additional channel for the encoding of information about the complexity and uncertainty of the sensory input. The richness of temporal variabilities, however, has not been systematically compared with the conventional mean activity. Here we compare the information content of 31 variability-sensitive features against the mean of activity, using three independent highly varied data sets. In whole-trial decoding, the classical event-related potential (ERP) components of P2a and P2b provided information comparable to those provided by original magnitude data (OMD) and wavelet coefficients (WC), the two most informative variability-sensitive features. In time-resolved decoding, the OMD and WC outperformed all the other features (including the mean), which were sensitive to limited and specific aspects of temporal variabilities, such as their phase or frequency. The information was more pronounced in the theta frequency band, previously suggested to support feedforward visual processing. We concluded that the brain might encode the information in multiple aspects of neural variabilities simultaneously such as phase, amplitude, and frequency rather than mean per se. In our active categorization data set, we found that more effective decoding of the neural codes corresponds to better prediction of behavioral performance. Therefore, the incorporation of temporal variabilities in time-resolved decoding can provide additional category information and improved prediction of behavior.},
  archive      = {J_NECO},
  author       = {Karimi-Rouzbahani, Hamid and Shahmohammadi, Mozhgan and Vahab, Ehsan and Setayeshi, Saeed and Carlson, Thomas},
  doi          = {10.1162/neco_a_01436},
  journal      = {Neural Computation},
  number       = {11},
  pages        = {3027-3072},
  shortjournal = {Neural Comput.},
  title        = {Temporal variabilities provide additional category-related information in object category decoding: A systematic comparison of informative EEG features},
  volume       = {33},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Completion of the infeasible actions of others: Goal
inference by dynamical invariant. <em>NECO</em>, <em>33</em>(11),
2996–3026. (<a href="https://doi.org/10.1162/neco_a_01437">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To help another person, we need to infer his or her goal and intention and then perform the action that he or she was unable to perform to meet the intended goal. In this study, we investigate a computational mechanism for inferring someone&#39;s intention and goal from that person&#39;s incomplete action to enable the action to be completed on his or her behalf. As a minimal and idealized motor control task of this type, we analyzed single-link pendulum control tasks by manipulating the underlying goals. By analyzing behaviors generated by multiple types of these tasks, we found that a type of fractal dimension of movements is characteristic of the difference in the underlying motor controllers, which reflect the difference in the underlying goals. To test whether an incomplete action can be completed using this property of the action trajectory, we demonstrated that the simulated pendulum controller can perform an action in the direction of the underlying goal by using the fractal dimension as a criterion for similarity in movements.},
  archive      = {J_NECO},
  author       = {Torii, Takuma and Hidaka, Shohei},
  doi          = {10.1162/neco_a_01437},
  journal      = {Neural Computation},
  number       = {11},
  pages        = {2996-3026},
  shortjournal = {Neural Comput.},
  title        = {Completion of the infeasible actions of others: Goal inference by dynamical invariant},
  volume       = {33},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dynamic spatiotemporal pattern recognition with recurrent
spiking neural network. <em>NECO</em>, <em>33</em>(11), 2971–2995. (<a
href="https://doi.org/10.1162/neco_a_01432">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Our real-time actions in everyday life reflect a range of spatiotemporal dynamic brain activity patterns, the consequence of neuronal computation with spikes in the brain. Most existing models with spiking neurons aim at solving static pattern recognition tasks such as image classification. Compared with static features, spatiotemporal patterns are more complex due to their dynamics in both space and time domains. Spatiotemporal pattern recognition based on learning algorithms with spiking neurons therefore remains challenging. We propose an end-to-end recurrent spiking neural network model trained with an algorithm based on spike latency and temporal difference backpropagation. Our model is a cascaded network with three layers of spiking neurons where the input and output layers are the encoder and decoder, respectively. In the hidden layer, the recurrently connected neurons with transmission delays carry out high-dimensional computation to incorporate the spatiotemporal dynamics of the inputs. The test results based on the data sets of spiking activities of the retinal neurons show that the proposed framework can recognize dynamic spatiotemporal patterns much better than using spike counts. Moreover, for 3D trajectories of a human action data set, the proposed framework achieves a test accuracy of 83.6\% on average. Rapid recognition is achieved through the learning methodology–based on spike latency and the decoding process using the first spike of the output neurons. Taken together, these results highlight a new model to extract information from activity patterns of neural computation in the brain and provide a novel approach for spike-based neuromorphic computing.},
  archive      = {J_NECO},
  author       = {Shen, Jiangrong and Liu, Jian K. and Wang, Yueming},
  doi          = {10.1162/neco_a_01432},
  journal      = {Neural Computation},
  number       = {11},
  pages        = {2971-2995},
  shortjournal = {Neural Comput.},
  title        = {Dynamic spatiotemporal pattern recognition with recurrent spiking neural network},
  volume       = {33},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Flexible transmitter network. <em>NECO</em>,
<em>33</em>(11), 2951–2970. (<a
href="https://doi.org/10.1162/neco_a_01431">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current neural networks are mostly built on the MP model, which usually formulates the neuron as executing an activation function on the real-valued weighted aggregation of signals received from other neurons. This letter proposes the flexible transmitter (FT) model, a novel bio-plausible neuron model with flexible synaptic plasticity. The FT model employs a pair of parameters to model the neurotransmitters between neurons and puts up a neuron-exclusive variable to record the regulated neurotrophin density. Thus, the FT model can be formulated as a two-variable, two-valued function, taking the commonly used MP neuron model as its particular case. This modeling manner makes the FT model biologically more realistic and capable of handling complicated data, even spatiotemporal data. To exhibit its power and potential, we present the flexible transmitter network (FTNet), which is built on the most common fully connected feedforward architecture taking the FT model as the basic building block. FTNet allows gradient calculation and can be implemented by an improved backpropagation algorithm in the complex-valued domain. Experiments on a broad range of tasks show that FTNet has power and potential in processing spatiotemporal data. This study provides an alternative basic building block in neural networks and exhibits the feasibility of developing artificial neural networks with neuronal plasticity.},
  archive      = {J_NECO},
  author       = {Zhang, Shao-Qun and Zhou, Zhi-Hua},
  doi          = {10.1162/neco_a_01431},
  journal      = {Neural Computation},
  number       = {11},
  pages        = {2951-2970},
  shortjournal = {Neural Comput.},
  title        = {Flexible transmitter network},
  volume       = {33},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Replay in deep learning: Current approaches and missing
biological elements. <em>NECO</em>, <em>33</em>(11), 2908–2950. (<a
href="https://doi.org/10.1162/neco_a_01433">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Replay is the reactivation of one or more neural patterns that are similar to the activation patterns experienced during past waking experiences. Replay was first observed in biological neural networks during sleep, and it is now thought to play a critical role in memory formation, retrieval, and consolidation. Replay-like mechanisms have been incorporated in deep artificial neural networks that learn over time to avoid catastrophic forgetting of previous knowledge. Replay algorithms have been successfully used in a wide range of deep learning methods within supervised, unsupervised, and reinforcement learning paradigms. In this letter, we provide the first comprehensive comparison between replay in the mammalian brain and replay in artificial neural networks. We identify multiple aspects of biological replay that are missing in deep learning systems and hypothesize how they could be used to improve artificial neural networks.},
  archive      = {J_NECO},
  author       = {Hayes, Tyler L. and Krishnan, Giri P. and Bazhenov, Maxim and Siegelmann, Hava T. and Sejnowski, Terrence J. and Kanan, Christopher},
  doi          = {10.1162/neco_a_01433},
  journal      = {Neural Computation},
  number       = {11},
  pages        = {2908-2950},
  shortjournal = {Neural Comput.},
  title        = {Replay in deep learning: Current approaches and missing biological elements},
  volume       = {33},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Parametric UMAP embeddings for representation and
semisupervised learning. <em>NECO</em>, <em>33</em>(11), 2881–2907. (<a
href="https://doi.org/10.1162/neco_a_01434">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {UMAP is a nonparametric graph-based dimensionality reduction algorithm using applied Riemannian geometry and algebraic topology to find low-dimensional embeddings of structured data. The UMAP algorithm consists of two steps: (1) computing a graphical representation of a data set (fuzzy simplicial complex) and (2) through stochastic gradient descent, optimizing a low-dimensional embedding of the graph. Here, we extend the second step of UMAP to a parametric optimization over neural network weights, learning a parametric relationship between data and embedding. We first demonstrate that parametric UMAP performs comparably to its nonparametric counterpart while conferring the benefit of a learned parametric mapping (e.g., fast online embeddings for new data). We then explore UMAP as a regularization, constraining the latent distribution of autoencoders, parametrically varying global structure preservation, and improving classifier accuracy for semisupervised learning by capturing structure in unlabeled data. 1},
  archive      = {J_NECO},
  author       = {Sainburg, Tim and McInnes, Leland and Gentner, Timothy Q.},
  doi          = {10.1162/neco_a_01434},
  journal      = {Neural Computation},
  number       = {11},
  pages        = {2881-2907},
  shortjournal = {Neural Comput.},
  title        = {Parametric UMAP embeddings for representation and semisupervised learning},
  volume       = {33},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multilinear common component analysis via kronecker product
representation. <em>NECO</em>, <em>33</em>(10), 2853–2880. (<a
href="https://doi.org/10.1162/neco_a_01425">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of extracting a common structure from multiple tensor data sets. For this purpose, we propose multilinear common component analysis (MCCA) based on Kronecker products of mode-wise covariance matrices. MCCA constructs a common basis represented by linear combinations of the original variables that lose little information of the multiple tensor data sets. We also develop an estimation algorithm for MCCA that guarantees mode-wise global convergence. Numerical studies are conducted to show the effectiveness of MCCA.},
  archive      = {J_NECO},
  author       = {Yoshikawa, Kohei and Kawano, Shuichi},
  doi          = {10.1162/neco_a_01425},
  journal      = {Neural Computation},
  number       = {10},
  pages        = {2853-2880},
  shortjournal = {Neural Comput.},
  title        = {Multilinear common component analysis via kronecker product representation},
  volume       = {33},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Integration of leaky-integrate-and-fire neurons in standard
machine learning architectures to generate hybrid networks: A surrogate
gradient approach. <em>NECO</em>, <em>33</em>(10), 2827–2852. (<a
href="https://doi.org/10.1162/neco_a_01424">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Up to now, modern machine learning (ML) has been based on approximating big data sets with high-dimensional functions, taking advantage of huge computational resources. We show that biologically inspired neuron models such as the leaky-integrate-and-fire (LIF) neuron provide novel and efficient ways of information processing. They can be integrated in machine learning models and are a potential target to improve ML performance. Thus, we have derived simple update rules for LIF units to numerically integrate the differential equations. We apply a surrogate gradient approach to train the LIF units via backpropagation. We demonstrate that tuning the leak term of the LIF neurons can be used to run the neurons in different operating modes, such as simple signal integrators or coincidence detectors. Furthermore, we show that the constant surrogate gradient, in combination with tuning the leak term of the LIF units, can be used to achieve the learning dynamics of more complex surrogate gradients. To prove the validity of our method, we applied it to established image data sets (the Oxford 102 flower data set, MNIST), implemented various network architectures, used several input data encodings and demonstrated that the method is suitable to achieve state-of-the-art classification performance. We provide our method as well as further surrogate gradient methods to train spiking neural networks via backpropagation as an open-source KERAS package to make it available to the neuroscience and machine learning community. To increase the interpretability of the underlying effects and thus make a small step toward opening the black box of machine learning, we provide interactive illustrations, with the possibility of systematically monitoring the effects of parameter changes on the learning characteristics.},
  archive      = {J_NECO},
  author       = {Gerum, Richard C. and Schilling, Achim},
  doi          = {10.1162/neco_a_01424},
  journal      = {Neural Computation},
  number       = {10},
  pages        = {2827-2852},
  shortjournal = {Neural Comput.},
  title        = {Integration of leaky-integrate-and-fire neurons in standard machine learning architectures to generate hybrid networks: A surrogate gradient approach},
  volume       = {33},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Realizing active inference in variational message passing:
The outcome-blind certainty seeker. <em>NECO</em>, <em>33</em>(10),
2762–2826. (<a href="https://doi.org/10.1162/neco_a_01422">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Active inference is a state-of-the-art framework in neuroscience that offers a unified theory of brain function. It is also proposed as a framework for planning in AI. Unfortunately, the complex mathematics required to create new models can impede application of active inference in neuroscience and AI research. This letter addresses this problem by providing a complete mathematical treatment of the active inference framework in discrete time and state spaces and the derivation of the update equations for any new model. We leverage the theoretical connection between active inference and variational message passing as described by John Winn and Christopher M. Bishop in 2005. Since variational message passing is a well-defined methodology for deriving Bayesian belief update equations, this letter opens the door to advanced generative models for active inference. We show that using a fully factorized variational distribution simplifies the expected free energy, which furnishes priors over policies so that agents seek unambiguous states. Finally, we consider future extensions that support deep tree searches for sequential policy optimization based on structure learning and belief propagation.},
  archive      = {J_NECO},
  author       = {Champion, Théophile and Grześ, Marek and Bowman, Howard},
  doi          = {10.1162/neco_a_01422},
  journal      = {Neural Computation},
  number       = {10},
  pages        = {2762-2826},
  shortjournal = {Neural Comput.},
  title        = {Realizing active inference in variational message passing: The outcome-blind certainty seeker},
  volume       = {33},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multibranch formal neuron: An internally nonlinear learning
unit. <em>NECO</em>, <em>33</em>(10), 2736–2761. (<a
href="https://doi.org/10.1162/neco_a_01428">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The transformation of synaptic input into action potential in nerve cells is strongly influenced by the morphology of the dendritic arbor as well as the synaptic efficacy map. The multiplicity of dendritic branches strikingly enables a single cell to act as a highly nonlinear processing element. Studies have also found functional synaptic clustering whereby synapses that encode a common sensory feature are spatially clustered together on the branches. Motivated by these findings, here we introduce a multibranch formal model of the neuron that can integrate synaptic inputs nonlinearly through collective action of its dendritic branches and yields synaptic clustering. An analysis in support of its use as a computational building block is offered. Also offered is an accompanying gradient descent–based learning algorithm. The model unit spans a wide spectrum of nonlinearities, including the parity problem, and can outperform the multilayer perceptron in generalizing to unseen data. The occurrence of synaptic clustering boosts the generalization efficiency of the unit, which may also be the answer for the puzzling ubiquity of synaptic clustering in the real neurons. Our theoretical analysis is backed up by simulations. The study could pave the way to new artificial neural networks.},
  archive      = {J_NECO},
  author       = {Güler, Marifi},
  doi          = {10.1162/neco_a_01428},
  journal      = {Neural Computation},
  number       = {10},
  pages        = {2736-2761},
  shortjournal = {Neural Comput.},
  title        = {Multibranch formal neuron: An internally nonlinear learning unit},
  volume       = {33},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Chance-constrained active inference. <em>NECO</em>,
<em>33</em>(10), 2710–2735. (<a
href="https://doi.org/10.1162/neco_a_01427">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Active inference (ActInf) is an emerging theory that explains perception and action in biological agents in terms of minimizing a free energy bound on Bayesian surprise. Goal-directed behavior is elicited by introducing prior beliefs on the underlying generative model. In contrast to prior beliefs, which constrain all realizations of a random variable, we propose an alternative approach through chance constraints, which allow for a (typically small) probability of constraint violation, and demonstrate how such constraints can be used as intrinsic drivers for goal-directed behavior in ActInf. We illustrate how chance-constrained ActInf weights all imposed (prior) constraints on the generative model, allowing, for example, for a trade-off between robust control and empirical chance constraint violation. Second, we interpret the proposed solution within a message passing framework. Interestingly, the message passing interpretation is not only relevant to the context of ActInf, but also provides a general-purpose approach that can account for chance constraints on graphical models. The chance constraint message updates can then be readily combined with other prederived message update rules without the need for custom derivations. The proposed chance-constrained message passing framework thus accelerates the search for workable models in general and can be used to complement message-passing formulations on generative neural models.},
  archive      = {J_NECO},
  author       = {van de Laar, Thijs and Şenöz, İsmail and Özçelikkale, Ayça and Wymeersch, Henk},
  doi          = {10.1162/neco_a_01427},
  journal      = {Neural Computation},
  number       = {10},
  pages        = {2710-2735},
  shortjournal = {Neural Comput.},
  title        = {Chance-constrained active inference},
  volume       = {33},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Tracking fast and slow changes in synaptic weights from
simultaneously observed pre- and postsynaptic spiking. <em>NECO</em>,
<em>33</em>(10), 2682–2709. (<a
href="https://doi.org/10.1162/neco_a_01426">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Synapses change on multiple timescales, ranging from milliseconds to minutes, due to a combination of both short- and long-term plasticity. Here we develop an extension of the common generalized linear model to infer both short- and long-term changes in the coupling between a pre- and postsynaptic neuron based on observed spiking activity. We model short-term synaptic plasticity using additive effects that depend on the presynaptic spike timing, and we model long-term changes in both synaptic weight and baseline firing rate using point process adaptive smoothing. Using simulations, we first show that this model can accurately recover time-varying synaptic weights (1) for both depressing and facilitating synapses, (2) with a variety of long-term changes (including realistic changes, such as due to STDP), (3) with a range of pre and postsynaptic firing rates, and (4) for both excitatory and inhibitory synapses. We then apply our model to two experimentally recorded putative synaptic connections. We find that simultaneously tracking fast changes in synaptic weights, slow changes in synaptic weights, and unexplained variations in baseline firing is essential. Omitting any one of these factors can lead to spurious inferences for the others. Altogether, this model provides a flexible framework for tracking short- and long-term variation in spike transmission.},
  archive      = {J_NECO},
  author       = {Wei, Ganchao and Stevenson, Ian H.},
  doi          = {10.1162/neco_a_01426},
  journal      = {Neural Computation},
  number       = {10},
  pages        = {2682-2709},
  shortjournal = {Neural Comput.},
  title        = {Tracking fast and slow changes in synaptic weights from simultaneously observed pre- and postsynaptic spiking},
  volume       = {33},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Restricted boltzmann machines as models of interacting
variables. <em>NECO</em>, <em>33</em>(10), 2646–2681. (<a
href="https://doi.org/10.1162/neco_a_01420">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the type of distributions that restricted Boltzmann machines (RBMs) with different activation functions can express by investigating the effect of the activation function of the hidden nodes on the marginal distribution they impose on observed binary nodes. We report an exact expression for these marginals in the form of a model of interacting binary variables with the explicit form of the interactions depending on the hidden node activation function. We study the properties of these interactions in detail and evaluate how the accuracy with which the RBM approximates distributions over binary variables depends on the hidden node activation function and the number of hidden nodes. When the inferred RBM parameters are weak, an intuitive pattern is found for the expression of the interaction terms, which reduces substantially the differences across activation functions. We show that the weak parameter approximation is a good approximation for different RBMs trained on the MNIST data set. Interestingly, in these cases, the mapping reveals that the inferred models are essentially low order interaction models.},
  archive      = {J_NECO},
  author       = {Bulso, Nicola and Roudi, Yasser},
  doi          = {10.1162/neco_a_01420},
  journal      = {Neural Computation},
  number       = {10},
  pages        = {2646-2681},
  shortjournal = {Neural Comput.},
  title        = {Restricted boltzmann machines as models of interacting variables},
  volume       = {33},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Stimulus-driven and spontaneous dynamics in
excitatory-inhibitory recurrent neural networks for sequence
representation. <em>NECO</em>, <em>33</em>(10), 2603–2645. (<a
href="https://doi.org/10.1162/neco_a_01418">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recurrent neural networks (RNNs) have been widely used to model sequential neural dynamics (“neural sequences”) of cortical circuits in cognitive and motor tasks. Efforts to incorporate biological constraints and Dale&#39;s principle will help elucidate the neural representations and mechanisms of underlying circuits. We trained an excitatory-inhibitory RNN to learn neural sequences in a supervised manner and studied the representations and dynamic attractors of the trained network. The trained RNN was robust to trigger the sequence in response to various input signals and interpolated a time-warped input for sequence representation. Interestingly, a learned sequence can repeat periodically when the RNN evolved beyond the duration of a single sequence. The eigenspectrum of the learned recurrent connectivity matrix with growing or damping modes, together with the RNN&#39;s nonlinearity, were adequate to generate a limit cycle attractor. We further examined the stability of dynamic attractors while training the RNN to learn two sequences. Together, our results provide a general framework for understanding neural sequence representation in the excitatory-inhibitory RNN.},
  archive      = {J_NECO},
  author       = {Rajakumar, Alfred and Rinzel, John and Chen, Zhe S.},
  doi          = {10.1162/neco_a_01418},
  journal      = {Neural Computation},
  number       = {10},
  pages        = {2603-2645},
  shortjournal = {Neural Comput.},
  title        = {Stimulus-driven and spontaneous dynamics in excitatory-inhibitory recurrent neural networks for sequence representation},
  volume       = {33},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Statistical properties of color matching functions.
<em>NECO</em>, <em>33</em>(9), 2578–2601. (<a
href="https://doi.org/10.1162/neco_a_01421">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In trichromats, color vision entails the projection of an infinite-dimensional space (the one containing all possible electromagnetic power spectra) onto the three-dimensional space that modulates the activity of the three types of cones. This drastic reduction in dimensionality gives rise to metamerism, that is, the perceptual chromatic equivalence between two different light spectra. The classes of equivalence of metamerism are revealed by color-matching experiments in which observers adjust the intensity of three monochromatic light beams of three preset wavelengths (the primaries ) to produce a mixture that is perceptually equal to a given monochromatic target stimulus. Here we use the linear relation between the color matching functions and the absorption probabilities of each type of cone to find particularly useful triplets of primaries. As a second goal, we also derive an analytical description of the trial-to-trial variability and the correlations of color matching functions stemming from Poissonian noise in photon capture. We analyze how the statistical properties of the responses to color-matching experiments vary with the retinal composition and the wavelengths of peak absorption probability, and compare them with experimental data on subject-to-subject variability obtained previously.},
  archive      = {J_NECO},
  author       = {da Fonseca, María and Samengo, Inés},
  doi          = {10.1162/neco_a_01421},
  journal      = {Neural Computation},
  number       = {9},
  pages        = {2578-2601},
  shortjournal = {Neural Comput.},
  title        = {Statistical properties of color matching functions},
  volume       = {33},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On neural associative memory structures: Storage and
retrieval of sequences in a chain of tournaments. <em>NECO</em>,
<em>33</em>(9), 2550–2577. (<a
href="https://doi.org/10.1162/neco_a_01417">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Associative memories enjoy many interesting properties in terms of error correction capabilities, robustness to noise, storage capacity, and retrieval performance, and their usage spans over a large set of applications. In this letter, we investigate and extend tournament-based neural networks, originally proposed by Jiang, Gripon, Berrou, and Rabbat ( 2016 ), a novel sequence storage associative memory architecture with high memory efficiency and accurate sequence retrieval. We propose a more general method for learning the sequences, which we call feedback tournament-based neural networks. The retrieval process is also extended to both directions: forward and backward—in other words, any large-enough segment of a sequence can produce the whole sequence. Furthermore, two retrieval algorithms, cache-winner and explore-winner, are introduced to increase the retrieval performance. Through simulation results, we shed light on the strengths and weaknesses of each algorithm.},
  archive      = {J_NECO},
  author       = {Mofrad, Asieh Abolpour and Mofrad, Samaneh Abolpour and Yazidi, Anis and Parker, Matthew Geoffrey},
  doi          = {10.1162/neco_a_01417},
  journal      = {Neural Computation},
  number       = {9},
  pages        = {2550-2577},
  shortjournal = {Neural Comput.},
  title        = {On neural associative memory structures: Storage and retrieval of sequences in a chain of tournaments},
  volume       = {33},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Do neural networks for segmentation understand insideness?
<em>NECO</em>, <em>33</em>(9), 2511–2549. (<a
href="https://doi.org/10.1162/neco_a_01413">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The insideness problem is an aspect of image segmentation that consists of determining which pixels are inside and outside a region. Deep neural networks (DNNs) excel in segmentation benchmarks, but it is unclear if they have the ability to solve the insideness problem as it requires evaluating long-range spatial dependencies. In this letter, we analyze the insideness problem in isolation, without texture or semantic cues, such that other aspects of segmentation do not interfere in the analysis. We demonstrate that DNNs for segmentation with few units have sufficient complexity to solve the insideness for any curve. Yet such DNNs have severe problems with learning general solutions. Only recurrent networks trained with small images learn solutions that generalize well to almost any curve. Recurrent networks can decompose the evaluation of long-range dependencies into a sequence of local operations, and learning with small images alleviates the common difficulties of training recurrent networks with a large number of unrolling steps.},
  archive      = {J_NECO},
  author       = {Villalobos, Kimberly and Štih, Vilim and Ahmadinejad, Amineh and Sundaram, Shobhita and Dozier, Jamell and Francl, Andrew and Azevedo, Frederico and Sasaki, Tomotake and Boix, Xavier},
  doi          = {10.1162/neco_a_01413},
  journal      = {Neural Computation},
  number       = {9},
  pages        = {2511-2549},
  shortjournal = {Neural Comput.},
  title        = {Do neural networks for segmentation understand insideness?},
  volume       = {33},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Least kth-order and rényi generative adversarial networks.
<em>NECO</em>, <em>33</em>(9), 2473–2510. (<a
href="https://doi.org/10.1162/neco_a_01416">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate the use of parameterized families of information-theoretic measures to generalize the loss functions of generative adversarial networks (GANs) with the objective of improving performance. A new generator loss function, least k th-order GAN (L k GAN), is introduced, generalizing the least squares GANs (LSGANs) by using a k th-order absolute error distortion measure with k ≥ 1 (which recovers the LSGAN loss function when k = 2 ⁠ ). It is shown that minimizing this generalized loss function under an (unconstrained) optimal discriminator is equivalent to minimizing the k th-order Pearson-Vajda divergence. Another novel GAN generator loss function is next proposed in terms of Rényi cross-entropy functionals with order α &gt; 0 ⁠ , α ≠ 1 ⁠ . It is demonstrated that this Rényi-centric generalized loss function, which provably reduces to the original GAN loss function as α → 1 ⁠ , preserves the equilibrium point satisfied by the original GAN based on the Jensen-Rényi divergence, a natural extension of the Jensen-Shannon divergence. Experimental results indicate that the proposed loss functions, applied to the MNIST and CelebA data sets, under both DCGAN and StyleGAN architectures, confer performance benefits by virtue of the extra degrees of freedom provided by the parameters k and α ⁠ , respectively. More specifically, experiments show improvements with regard to the quality of the generated images as measured by the Fréchet inception distance score and training stability. While it was applied to GANs in this study, the proposed approach is generic and can be used in other applications of information theory to deep learning, for example, the issues of fairness or privacy in artificial intelligence.},
  archive      = {J_NECO},
  author       = {Bhatia, Himesh and Paul, William and Alajaji, Fady and Gharesifard, Bahman and Burlina, Philippe},
  doi          = {10.1162/neco_a_01416},
  journal      = {Neural Computation},
  number       = {9},
  pages        = {2473-2510},
  shortjournal = {Neural Comput.},
  title        = {Least kth-order and rényi generative adversarial networks},
  volume       = {33},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Few-shot learning in spiking neural networks by
multi-timescale optimization. <em>NECO</em>, <em>33</em>(9), 2439–2472.
(<a href="https://doi.org/10.1162/neco_a_01423">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning new concepts rapidly from a few examples is an open issue in spike-based machine learning. This few-shot learning imposes substantial challenges to the current learning methodologies of spiking neuron networks (SNNs) due to the lack of task-related priori knowledge. The recent learning-to-learn (L2L) approach allows SNNs to acquire priori knowledge through example-level learning and task-level optimization. However, existing L2L-based frameworks do not target the neural dynamics (i.e., neuronal and synaptic parameter changes) on different timescales. This diversity of temporal dynamics is an important attribute in spike-based learning, which facilitates the networks to rapidly acquire knowledge from very few examples and gradually integrate this knowledge. In this work, we consider the neural dynamics on various timescales and provide a multi-timescale optimization (MTSO) framework for SNNs. This framework introduces an adaptive-gated LSTM to accommodate two different timescales of neural dynamics: short-term learning and long-term evolution. Short-term learning is a fast knowledge acquisition process achieved by a novel surrogate gradient online learning (SGOL) algorithm, where the LSTM guides gradient updating of SNN on a short timescale through an adaptive learning rate and weight decay gating. The long-term evolution aims to slowly integrate acquired knowledge and form a priori, which can be achieved by optimizing the LSTM guidance process to tune SNN parameters on a long timescale. Experimental results demonstrate that the collaborative optimization of multi-timescale neural dynamics can make SNNs achieve promising performance for the few-shot learning tasks.},
  archive      = {J_NECO},
  author       = {Jiang, Runhao and Zhang, Jie and Yan, Rui and Tang, Huajin},
  doi          = {10.1162/neco_a_01423},
  journal      = {Neural Computation},
  number       = {9},
  pages        = {2439-2472},
  shortjournal = {Neural Comput.},
  title        = {Few-shot learning in spiking neural networks by multi-timescale optimization},
  volume       = {33},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bridging m/EEG source imaging and independent component
analysis frameworks using biologically inspired sparsity priors.
<em>NECO</em>, <em>33</em>(9), 2408–2438. (<a
href="https://doi.org/10.1162/neco_a_01415">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electromagnetic source imaging (ESI) and independent component analysis (ICA) are two popular and apparently dissimilar frameworks for M/EEG analysis. This letter shows that the two frameworks can be linked by choosing biologically inspired source sparsity priors. We demonstrate that ESI carried out by the sparse Bayesian learning (SBL) algorithm yields source configurations composed of a few active regions that are also maximally independent from one another. In addition, we extend the standard SBL approach to source imaging in two important directions. First, we augment the generative model of M/EEG to include artifactual sources. Second, we modify SBL to allow for efficient model inversion with sequential data. We refer to this new algorithm as recursive SBL (RSBL), a source estimation filter with potential for online and offline imaging applications. We use simulated data to verify that RSBL can accurately estimate and demix cortical and artifactual sources under different noise conditions. Finally, we show that on real error-related EEG data, RSBL can yield single-trial source estimates in agreement with the experimental literature. Overall, by demonstrating that ESI can produce maximally independent sources while simultaneously localizing them in cortical space, we bridge the gap between the ESI and ICA frameworks for M/EEG analysis.},
  archive      = {J_NECO},
  author       = {Ojeda, Alejandro and Kreutz-Delgado, Kenneth and Mishra, Jyoti},
  doi          = {10.1162/neco_a_01415},
  journal      = {Neural Computation},
  number       = {9},
  pages        = {2408-2438},
  shortjournal = {Neural Comput.},
  title        = {Bridging M/EEG source imaging and independent component analysis frameworks using biologically inspired sparsity priors},
  volume       = {33},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Emergence of content-agnostic information processing by a
robot using active inference, visual attention, working memory, and
planning. <em>NECO</em>, <em>33</em>(9), 2353–2407. (<a
href="https://doi.org/10.1162/neco_a_01412">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generalization by learning is an essential cognitive competency for humans. For example, we can manipulate even unfamiliar objects and can generate mental images before enacting a preplan. How is this possible? Our study investigated this problem by revisiting our previous study (Jung, Matsumoto, &amp; Tani, 2019 ), which examined the problem of vision-based, goal-directed planning by robots performing a task of block stacking. By extending the previous study, our work introduces a large network comprising dynamically interacting submodules, including visual working memory (VWMs), a visual attention module, and an executive network. The executive network predicts motor signals, visual images, and various controls for attention, as well as masking of visual information. The most significant difference from the previous study is that our current model contains an additional VWM. The entire network is trained by using predictive coding and an optimal visuomotor plan to achieve a given goal state is inferred using active inference. Results indicate that our current model performs significantly better than that used in Jung et al. ( 2019 ), especially when manipulating blocks with unlearned colors and textures. Simulation results revealed that the observed generalization was achieved because content-agnostic information processing developed through synergistic interaction between the second VWM and other modules during the course of learning, in which memorizing image contents and transforming them are dissociated. This letter verifies this claim by conducting both qualitative and quantitative analysis of simulation results.},
  archive      = {J_NECO},
  author       = {Queiẞer, Jeffrey Frederic and Jung, Minju and Matsumoto, Takazumi and Tani, Jun},
  doi          = {10.1162/neco_a_01412},
  journal      = {Neural Computation},
  number       = {9},
  pages        = {2353-2407},
  shortjournal = {Neural Comput.},
  title        = {Emergence of content-agnostic information processing by a robot using active inference, visual attention, working memory, and planning},
  volume       = {33},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A biologically plausible neural network for multichannel
canonical correlation analysis. <em>NECO</em>, <em>33</em>(9),
2309–2352. (<a href="https://doi.org/10.1162/neco_a_01414">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cortical pyramidal neurons receive inputs from multiple distinct neural populations and integrate these inputs in separate dendritic compartments. We explore the possibility that cortical microcircuits implement canonical correlation analysis (CCA), an unsupervised learning method that projects the inputs onto a common subspace so as to maximize the correlations between the projections. To this end, we seek a multichannel CCA algorithm that can be implemented in a biologically plausible neural network. For biological plausibility, we require that the network operates in the online setting and its synaptic update rules are local. Starting from a novel CCA objective function, we derive an online optimization algorithm whose optimization steps can be implemented in a single-layer neural network with multicompartmental neurons and local non-Hebbian learning rules. We also derive an extension of our online CCA algorithm with adaptive output rank and output whitening. Interestingly, the extension maps onto a neural network whose neural architecture and synaptic updates resemble neural circuitry and non-Hebbian plasticity observed in the cortex.},
  archive      = {J_NECO},
  author       = {Lipshutz, David and Bahroun, Yanis and Golkar, Siavash and Sengupta, Anirvan M. and Chklovskii, Dmitri B.},
  doi          = {10.1162/neco_a_01414},
  journal      = {Neural Computation},
  number       = {9},
  pages        = {2309-2352},
  shortjournal = {Neural Comput.},
  title        = {A biologically plausible neural network for multichannel canonical correlation analysis},
  volume       = {33},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Pathological spectra of the fisher information metric and
its variants in deep neural networks. <em>NECO</em>, <em>33</em>(8),
2274–2307. (<a href="https://doi.org/10.1162/neco_a_01411">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Fisher information matrix (FIM) plays an essential role in statistics and machine learning as a Riemannian metric tensor or a component of the Hessian matrix of loss functions. Focusing on the FIM and its variants in deep neural networks (DNNs), we reveal their characteristic scale dependence on the network width, depth, and sample size when the network has random weights and is sufficiently wide. This study covers two widely used FIMs for regression with linear output and for classification with softmax output. Both FIMs asymptotically show pathological eigenvalue spectra in the sense that a small number of eigenvalues become large outliers depending on the width or sample size, while the others are much smaller. It implies that the local shape of the parameter space or loss landscape is very sharp in a few specific directions while almost flat in the other directions. In particular, the softmax output disperses the outliers and makes a tail of the eigenvalue density spread from the bulk. We also show that pathological spectra appear in other variants of FIMs: one is the neural tangent kernel; another is a metric for the input signal and feature space that arises from feedforward signal propagation. Thus, we provide a unified perspective on the FIM and its variants that will lead to more quantitative understanding of learning in large-scale DNNs.},
  archive      = {J_NECO},
  author       = {Karakida, Ryo and Akaho, Shotaro and Amari, Shun-ichi},
  doi          = {10.1162/neco_a_01411},
  journal      = {Neural Computation},
  number       = {8},
  pages        = {2274-2307},
  shortjournal = {Neural Comput.},
  title        = {Pathological spectra of the fisher information metric and its variants in deep neural networks},
  volume       = {33},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Randomized self-organizing map. <em>NECO</em>,
<em>33</em>(8), 2241–2273. (<a
href="https://doi.org/10.1162/neco_a_01406">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a variation of the self-organizing map algorithm by considering the random placement of neurons on a two-dimensional manifold, following a blue noise distribution from which various topologies can be derived. These topologies possess random (but controllable) discontinuities that allow for a more flexible self-organization, especially with high-dimensional data. The proposed algorithm is tested on one-, two- and three-dimensional tasks, as well as on the MNIST handwritten digits data set and validated using spectral analysis and topological data analysis tools. We also demonstrate the ability of the randomized self-organizing map to gracefully reorganize itself in case of neural lesion and/or neurogenesis.},
  archive      = {J_NECO},
  author       = {Rougier, Nicolas P. and Detorakis, Georgios Is.},
  doi          = {10.1162/neco_a_01406},
  journal      = {Neural Computation},
  number       = {8},
  pages        = {2241-2273},
  shortjournal = {Neural Comput.},
  title        = {Randomized self-organizing map},
  volume       = {33},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021c). Storage capacity of quaternion-valued hopfield neural
networks with dual connections. <em>NECO</em>, <em>33</em>(8),
2226–2240. (<a href="https://doi.org/10.1162/neco_a_01405">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A complex-valued Hopfield neural network (CHNN) is a multistate Hopfield model. A quaternion-valued Hopfield neural network (QHNN) with a twin-multistate activation function was proposed to reduce the number of weight parameters of CHNN. Dual connections (DCs) are introduced to the QHNNs to improve the noise tolerance. The DCs take advantage of the noncommutativity of quaternions and consist of two weights between neurons. A QHNN with DCs provides much better noise tolerance than a CHNN. Although a CHNN and a QHNN with DCs have the samenumber of weight parameters, the storage capacity of projection rule for QHNNs with DCs is half of that for CHNNs and equals that of conventional QHNNs. The small storage capacity of QHNNs with DCs is caused by projection rule, not the architecture. In this work, the ebbian rule is introduced and proved by stochastic analysis that the storage capacity of a QHNN with DCs is 0.8 times as many as that of a CHNN.},
  archive      = {J_NECO},
  author       = {Kobayashi, Masaki},
  doi          = {10.1162/neco_a_01405},
  journal      = {Neural Computation},
  number       = {8},
  pages        = {2226-2240},
  shortjournal = {Neural Comput.},
  title        = {Storage capacity of quaternion-valued hopfield neural networks with dual connections},
  volume       = {33},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Power function error initialization can improve convergence
of backpropagation learning in neural networks for classification.
<em>NECO</em>, <em>33</em>(8), 2193–2225. (<a
href="https://doi.org/10.1162/neco_a_01407">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Supervised learning corresponds to minimizing a loss or cost function expressing the differences between model predictions y n and the target values t n given by the training data. In neural networks, this means backpropagating error signals through the transposed weight matrixes from the output layer toward the input layer. For this, error signals in the output layer are typically initialized by the difference y n - t n ⁠ , which is optimal for several commonly used loss functions like cross-entropy or sum of squared errors. Here I evaluate a more general error initialization method using power functions | y n - t n | q for q &gt; 0 ⁠ , corresponding to a new family of loss functions that generalize cross-entropy. Surprisingly, experiments on various learning tasks reveal that a proper choice of q can significantly improve the speed and convergence of backpropagation learning, in particular in deep and recurrent neural networks. The results suggest two main reasons for the observed improvements. First, compared to cross-entropy, the new loss functions provide better fits to the distribution of error signals in the output layer and therefore maximize the model&#39;s likelihood more efficiently. Second, the new error initialization procedure may often provide a better gradient-to-loss ratio over a broad range of neural output activity, thereby avoiding flat loss landscapes with vanishing gradients.},
  archive      = {J_NECO},
  author       = {Knoblauch, Andreas},
  doi          = {10.1162/neco_a_01407},
  journal      = {Neural Computation},
  number       = {8},
  pages        = {2193-2225},
  shortjournal = {Neural Comput.},
  title        = {Power function error initialization can improve convergence of backpropagation learning in neural networks for classification},
  volume       = {33},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Artificial neural variability for deep learning: On
overfitting, noise memorization, and catastrophic forgetting.
<em>NECO</em>, <em>33</em>(8), 2163–2192. (<a
href="https://doi.org/10.1162/neco_a_01403">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning is often criticized by two serious issues that rarely exist in natural nervous systems: overfitting and catastrophic forgetting. It can even memorize randomly labeled data, which has little knowledge behind the instance-label pairs. When a deep network continually learns over time by accommodating new tasks, it usually quickly overwrites the knowledge learned from previous tasks. Referred to as the neural variability , it is well known in neuroscience that human brain reactions exhibit substantial variability even in response to the same stimulus. This mechanism balances accuracy and plasticity/flexibility in the motor learning of natural nervous systems. Thus, it motivates us to design a similar mechanism, named artificial neural variability (ANV), that helps artificial neural networks learn some advantages from “natural” neural networks. We rigorously prove that ANV plays as an implicit regularizer of the mutual information between the training data and the learned model. This result theoretically guarantees ANV a strictly improved generalizability, robustness to label noise, and robustness to catastrophic forgetting. We then devise a neural variable risk minimization (NVRM) framework and neural variable optimizers to achieve ANV for conventional network architectures in practice. The empirical studies demonstrate that NVRM can effectively relieve overfitting, label noise memorization, and catastrophic forgetting at negligible costs.},
  archive      = {J_NECO},
  author       = {Xie, Zeke and He, Fengxiang and Fu, Shaopeng and Sato, Issei and Tao, Dacheng and Sugiyama, Masashi},
  doi          = {10.1162/neco_a_01403},
  journal      = {Neural Computation},
  number       = {8},
  pages        = {2163-2192},
  shortjournal = {Neural Comput.},
  title        = {Artificial neural variability for deep learning: On overfitting, noise memorization, and catastrophic forgetting},
  volume       = {33},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Direction matters: On influence-preserving graph
summarization and max-cut principle for directed graphs. <em>NECO</em>,
<em>33</em>(8), 2128–2162. (<a
href="https://doi.org/10.1162/neco_a_01402">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Summarizing large-scale directed graphs into small-scale representations is a useful but less-studied problem setting. Conventional clustering approaches, based on Min-Cut-style criteria, compress both the vertices and edges of the graph into the communities, which lead to a loss of directed edge information. On the other hand, compressing the vertices while preserving the directed-edge information provides a way to learn the small-scale representation of a directed graph. The reconstruction error, which measures the edge information preserved by the summarized graph, can be used to learn such representation. Compared to the original graphs, the summarized graphs are easier to analyze and are capable of extracting group-level features, useful for efficient interventions of population behavior. In this letter, we present a model, based on minimizing reconstruction error with nonnegative constraints, which relates to a Max-Cut criterion that simultaneously identifies the compressed nodes and the directed compressed relations between these nodes. A multiplicative update algorithm with column-wise normalization is proposed. We further provide theoretical results on the identifiability of the model and the convergence of the proposed algorithms. Experiments are conducted to demonstrate the accuracy and robustness of the proposed method.},
  archive      = {J_NECO},
  author       = {Xu, Wenkai and Niu, Gang and Hyvärinen, Aapo and Sugiyama, Masashi},
  doi          = {10.1162/neco_a_01402},
  journal      = {Neural Computation},
  number       = {8},
  pages        = {2128-2162},
  shortjournal = {Neural Comput.},
  title        = {Direction matters: On influence-preserving graph summarization and max-cut principle for directed graphs},
  volume       = {33},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning brain dynamics with coupled low-dimensional
nonlinear oscillators and deep recurrent networks. <em>NECO</em>,
<em>33</em>(8), 2087–2127. (<a
href="https://doi.org/10.1162/neco_a_01401">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many natural systems, especially biological ones, exhibit complex multivariate nonlinear dynamical behaviors that can be hard to capture by linear autoregressive models. On the other hand, generic nonlinear models such as deep recurrent neural networks often require large amounts of training data, not always available in domains such as brain imaging; also, they often lack interpretability. Domain knowledge about the types of dynamics typically observed in such systems, such as a certain type of dynamical systems models, could complement purely data-driven techniques by providing a good prior. In this work, we consider a class of ordinary differential equation (ODE) models known as van der Pol (VDP) oscil lators and evaluate their ability to capture a low-dimensional representation of neural activity measured by different brain imaging modalities, such as calcium imaging (CaI) and fMRI, in different living organisms: larval zebrafish, rat, and human. We develop a novel and efficient approach to the nontrivial problem of parameters estimation for a network of coupled dynamical systems from multivariate data and demonstrate that the resulting VDP models are both accurate and interpretable, as VDP&#39;s coupling matrix reveals anatomically meaningful excitatory and inhibitory interactions across different brain subsystems. VDP outperforms linear autoregressive models (VAR) in terms of both the data fit accuracy and the quality of insight provided by the coupling matrices and often tends to generalize better to unseen data when predicting future brain activity, being comparable to and sometimes better than the recurrent neural networks (LSTMs). Finally, we demonstrate that our (generative) VDP model can also serve as a data-augmentation tool leading to marked improvements in predictive accuracy of recurrent neural networks. Thus, our work contributes to both basic and applied dimensions of neuroimaging: gaining scientific insights and improving brain-based predictive models, an area of potentially high practical importance in clinical diagnosis and neurotechnology.},
  archive      = {J_NECO},
  author       = {Abrevaya, Germán and Dumas, Guillaume and Aravkin, Aleksandr Y. and Zheng, Peng and Gagnon-Audet, Jean-Christophe and Kozloski, James and Polosecki, Pablo and Lajoie, Guillaume and Cox, David and Dawson, Silvina Ponce and Cecchi, Guillermo and Rish, Irina},
  doi          = {10.1162/neco_a_01401},
  journal      = {Neural Computation},
  number       = {8},
  pages        = {2087-2127},
  shortjournal = {Neural Comput.},
  title        = {Learning brain dynamics with coupled low-dimensional nonlinear oscillators and deep recurrent networks},
  volume       = {33},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Frequency selectivity of neural circuits with heterogeneous
discrete transmission delays. <em>NECO</em>, <em>33</em>(8), 2068–2086.
(<a href="https://doi.org/10.1162/neco_a_01404">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neurons are connected to other neurons by axons and dendrites that conduct signals with finite velocities, resulting in delays between the firing of a neuron and the arrival of the resultant impulse at other neurons. Since delays greatly complicate the analytical treatment and interpretation of models, they are usually neglected or taken to be uniform, leading to a lack in the comprehension of the effects of delays in neural systems. This letter shows that heterogeneous transmission delays make small groups of neurons respond selectively to inputs with differing frequency spectra. By studying a single integrate-and-fire neuron receiving correlated time-shifted inputs, it is shown how the frequency response is linked to both the strengths and delay times of the afferent connections. The results show that incorporating delays alters the functioning of neural networks, and changes the effect that neural connections and synaptic strengths have.},
  archive      = {J_NECO},
  author       = {Houben, Akke Mats},
  doi          = {10.1162/neco_a_01404},
  journal      = {Neural Computation},
  number       = {8},
  pages        = {2068-2086},
  shortjournal = {Neural Comput.},
  title        = {Frequency selectivity of neural circuits with heterogeneous discrete transmission delays},
  volume       = {33},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Simulating and predicting dynamical systems with spatial
semantic pointers. <em>NECO</em>, <em>33</em>(8), 2033–2067. (<a
href="https://doi.org/10.1162/neco_a_01410">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While neural networks are highly effective at learning task-relevant representations from data, they typically do not learn representations with the kind of symbolic structure that is hypothesized to support high-level cognitive processes, nor do they naturally model such structures within problem domains that are continuous in space and time. To fill these gaps, this work exploits a method for defining vector representations that bind discrete (symbol-like) entities to points in continuous topological spaces in order to simulate and predict the behavior of a range of dynamical systems. These vector representations are spatial semantic pointers (SSPs), and we demonstrate that they can (1) be used to model dynamical systems involving multiple objects represented in a symbol-like manner and (2) be integrated with deep neural networks to predict the future of physical trajectories. These results help unify what have traditionally appeared to be disparate approaches in machine learning.},
  archive      = {J_NECO},
  author       = {Voelker, Aaron R. and Blouw, Peter and Choo, Xuan and Dumont, Nicole Sandra-Yaffa and Stewart, Terrence C. and Eliasmith, Chris},
  doi          = {10.1162/neco_a_01410},
  journal      = {Neural Computation},
  number       = {8},
  pages        = {2033-2067},
  shortjournal = {Neural Comput.},
  title        = {Simulating and predicting dynamical systems with spatial semantic pointers},
  volume       = {33},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). NetPyNE implementation and scaling of the potjans-diesmann
cortical microcircuit model. <em>NECO</em>, <em>33</em>(7), 1993–2032.
(<a href="https://doi.org/10.1162/neco_a_01400">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Potjans-Diesmann cortical microcircuit model is a widely used model originally implemented in NEST. Here, we reimplemented the model using NetPyNE, a high-level Python interface to the NEURON simulator, and reproduced the findings of the original publication. We also implemented a method for scaling the network size that preserves first- and second-order statistics, building on existing work on network theory. Our new implementation enabled the use of more detailed neuron models with multicompartmental morphologies and multiple biophysically realistic ion channels. This opens the model to new research, including the study of dendritic processing, the influence of individual channel parameters, the relation to local field potentials, and other multiscale interactions. The scaling method we used provides flexibility to increase or decrease the network size as needed when running these CPU-intensive detailed simulations. Finally, NetPyNE facilitates modifying or extending the model using its declarative language; optimizing model parameters; running efficient, large-scale parallelized simulations; and analyzing the model through built-in methods, including local field potential calculation and information flow measures.},
  archive      = {J_NECO},
  author       = {Romaro, Cecilia and Najman, Fernando Araujo and Lytton, William W. and Roque, Antonio C. and Dura-Bernal, Salvador},
  doi          = {10.1162/neco_a_01400},
  journal      = {Neural Computation},
  number       = {7},
  pages        = {1993-2032},
  shortjournal = {Neural Comput.},
  title        = {NetPyNE implementation and scaling of the potjans-diesmann cortical microcircuit model},
  volume       = {33},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A computational study on synaptic plasticity regulation and
information processing in neuron-astrocyte networks. <em>NECO</em>,
<em>33</em>(7), 1970–1992. (<a
href="https://doi.org/10.1162/neco_a_01399">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Postsynaptic ionotropic receptors critically shape synaptic currents and underpin their activity-dependent plasticity. In recent years, regulation of expression of these receptors by slow inward and outward currents mediated by gliotransmitter release from astrocytes has come under scrutiny as a potentially important mechanism for the regulation of synaptic information transfer. In this study, we consider a model of astrocyte-regulated synapses to investigate this hypothesis at the level of layered networks of interacting neurons and astrocytes. Our simulations hint that gliotransmission sustains the transfer function across layers, although it decorrelates the neuronal activity from the signal pattern. Overall, our results make clear how astrocytes could transform neuronal activity by inducing a lowfrequency modulation of postsynaptic activity.},
  archive      = {J_NECO},
  author       = {Vuillaume, Roman and Lorenzo, Jhunlyn and Binczak, Stéphane and Jacquir, Sabir},
  doi          = {10.1162/neco_a_01399},
  journal      = {Neural Computation},
  number       = {7},
  pages        = {1970-1992},
  shortjournal = {Neural Comput.},
  title        = {A computational study on synaptic plasticity regulation and information processing in neuron-astrocyte networks},
  volume       = {33},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Analysis of EEG data using complex geometric
structurization. <em>NECO</em>, <em>33</em>(7), 1942–1969. (<a
href="https://doi.org/10.1162/neco_a_01398">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electroencephalogram (EEG) is a common tool used to understand brain activities. The data are typically obtained by placing electrodes at the surface of the scalp and recording the oscillations of currents passing through the electrodes. These oscillations can sometimes lead to various interpretations, depending on, for example, the subject&#39;s health condition, the experiment carried out, the sensitivity of the tools used, or human manipulations. The data obtained over time can be considered a time series. There is evidence in the literature that epilepsy EEG data may be chaotic. Either way, the Embedding Theory in dynamical systems suggests that time series from a complex system could be used to reconstruct its phase space under proper conditions. In this letter, we propose an analysis of epilepsy EEG time series data based on a novel approach dubbed complex geometric structurization. Complex geometric structurization stems from the construction of strange attractors using Embedding Theory from dynamical systems. The complex geometric structures are themselves obtained using a geometry tool, the α -shapes from shape analysis. Initial analyses show a proof of concept in that these complex structures capture the expected changes brain in lobes under consideration. Further, a deeper analysis suggests that these complex structures can be used as biomarkers for seizure changes.},
  archive      = {J_NECO},
  author       = {Kwessi, E. A. and Edwards, L. J.},
  doi          = {10.1162/neco_a_01398},
  journal      = {Neural Computation},
  number       = {7},
  pages        = {1942-1969},
  shortjournal = {Neural Comput.},
  title        = {Analysis of EEG data using complex geometric structurization},
  volume       = {33},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Classification of autism spectrum disorder from EEG-based
functional brain connectivity analysis. <em>NECO</em>, <em>33</em>(7),
1914–1941. (<a href="https://doi.org/10.1162/neco_a_01394">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autism is a psychiatric condition that is typically diagnosed with behavioral assessment methods. Recent years have seen a rise in the number of children with autism. Since this could have serious health and socioeconomic consequences, it is imperative to investigate how to develop strategies for an early diagnosis that might pave the way to an adequate intervention. In this study, the phase-based functional brain connectivity derived from electroencephalogram (EEG) in a machine learning framework was used to classify the children with autism and typical children in an experimentally obtained data set of 12 autism spectrum disorder (ASD) and 12 typical children. Specifically, the functional brain connectivity networks have quantitatively been characterized by graph-theoretic parameters computed from three proposed approaches based on a standard phase-locking value, which were used as the features in a machine learning environment. Our study was successfully classified between two groups with approximately 95.8\% accuracy, 100\% sensitivity, and 92\% specificity through the trial-averaged phase-locking value (PLV) approach and cubic support vector machine (SVM). This work has also shown that significant changes in functional brain connectivity in ASD children have been revealed at theta band using the aggregated graph-theoretic features. Therefore, the findings from this study offer insight into the potential use of functional brain connectivity as a tool for classifying ASD children.},
  archive      = {J_NECO},
  author       = {Alotaibi, Noura and Maharatna, Koushik},
  doi          = {10.1162/neco_a_01394},
  journal      = {Neural Computation},
  number       = {7},
  pages        = {1914-1941},
  shortjournal = {Neural Comput.},
  title        = {Classification of autism spectrum disorder from EEG-based functional brain connectivity analysis},
  volume       = {33},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Skip-connected self-recurrent spiking neural networks with
joint intrinsic parameter and synaptic weight training. <em>NECO</em>,
<em>33</em>(7), 1886–1913. (<a
href="https://doi.org/10.1162/neco_a_01393">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an important class of spiking neural networks (SNNs), recurrent spiking neural networks (RSNNs) possess great computational power and have been widely used for processing sequential data like audio and text. However, most RSNNs suffer from two problems. First, due to the lack of architectural guidance, random recurrent connectivity is often adopted, which does not guarantee good performance. Second, training of RSNNs is in general challenging, bottlenecking achievable model accuracy. To address these problems, we propose a new type of RSNN, skip-connected self-recurrent SNNs (ScSr-SNNs). Recurrence in ScSr-SNNs is introduced by adding self-recurrent connections to spiking neurons. The SNNs with self-recurrent connections can realize recurrent behaviors similar to those of more complex RSNNs, while the error gradients can be more straightforwardly calculated due to the mostly feedforward nature of the network. The network dynamics is enriched by skip connections between nonadjacent layers. Moreover, we propose a new backpropagation (BP) method, backpropagated intrinsic plasticity (BIP), to boost the performance of ScSr-SNNs further by training intrinsic model parameters. Unlike standard intrinsic plasticity rules that adjust the neuron&#39;s intrinsic parameters according to neuronal activity, the proposed BIP method optimizes intrinsic parameters based on the backpropagated error gradient of a well-defined global loss function in addition to synaptic weight training. Based on challenging speech, neuromorphic speech, and neuromorphic image data sets, the proposed ScSr-SNNs can boost performance by up to 2.85\% compared with other types of RSNNs trained by state-of-the-art BP methods.},
  archive      = {J_NECO},
  author       = {Zhang, Wenrui and Li, Peng},
  doi          = {10.1162/neco_a_01393},
  journal      = {Neural Computation},
  number       = {7},
  pages        = {1886-1913},
  shortjournal = {Neural Comput.},
  title        = {Skip-connected self-recurrent spiking neural networks with joint intrinsic parameter and synaptic weight training},
  volume       = {33},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). X-DC: Explainable deep clustering based on learnable
spectrogram templates. <em>NECO</em>, <em>33</em>(7), 1853–1885. (<a
href="https://doi.org/10.1162/neco_a_01392">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks (DNNs) have achieved substantial predictive performance in various speech processing tasks. Particularly, it has been shown that a monaural speech separation task can be successfully solved with a DNN-based method called deep clustering (DC), which uses a DNN to describe the process of assigning a continuous vector to each time-frequency (TF) bin and measure how likely each pair of TF bins is to be dominated by the same speaker. In DC, the DNN is trained so that the embedding vectors for the TF bins dominated by the same speaker are forced to get close to each other. One concern regarding DC is that the embedding process described by a DNN has a black-box structure, which is usually very hard to interpret. The potential weakness owing to the noninterpretable black box structure is that it lacks the flexibility of addressing the mismatch between training and test conditions (caused by reverberation, for instance). To overcome this limitation, in this letter, we propose the concept of explainable deep clustering (X-DC), whose network architecture can be interpreted as a process of fitting learnable spectrogram templates to an input spectrogram followed by Wiener filtering. During training, the elements of the spectrogram templates and their activations are constrained to be nonnegative, which facilitates the sparsity of their values and thus improves interpretability. The main advantage of this framework is that it naturally allows us to incorporate a model adaptation mechanism into the network thanks to its physically interpretable structure. We experimentally show that the proposed X-DC enables us to visualize and understand the clues for the model to determine the embedding vectors while achieving speech separation performance comparable to that of the original DC models.},
  archive      = {J_NECO},
  author       = {Watanabe, Chihiro and Kameoka, Hirokazu},
  doi          = {10.1162/neco_a_01392},
  journal      = {Neural Computation},
  number       = {7},
  pages        = {1853-1885},
  shortjournal = {Neural Comput.},
  title        = {X-DC: Explainable deep clustering based on learnable spectrogram templates},
  volume       = {33},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Lifelong classification in open world with limited storage
requirements. <em>NECO</em>, <em>33</em>(7), 1818–1852. (<a
href="https://doi.org/10.1162/neco_a_01391">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This letter focuses on the problem of lifelong classification in the open world, the goal of which is to achieve an endless process of learning. However, incremental data sets (like the streaming data) in the open world, where the new classes may be emerging, are unsuited for classical classification methods. For addressing this problem, existing methods usually retrain the whole observed data sets with the complex computation and the expensive storage cost. This letter attempts to improve the performance of classification in the open world and decomposes the problem into three subproblems: (1) to reject unknown instances, (2) to classify accepted instances, and (3) to cut the cost of learning. Rejecting unknown instances refers to recognize those instances whose classes are unknown according to the learner, which could reduce the computation of the retraining process and eliminate the storage of historical data sets. We employ outlier detection for rejecting instances and a variant artificial neural network for classifying with fewer weights. Results on several experiments show that the work is effective. Source code can be found at https://github.com/wangbi1988/Lifelong-learning-in-Open-World-Classification .},
  archive      = {J_NECO},
  author       = {Bi, Wang and Yang, Chen and XueLian, Li and JunFu, Chen},
  doi          = {10.1162/neco_a_01391},
  journal      = {Neural Computation},
  number       = {7},
  pages        = {1818-1852},
  shortjournal = {Neural Comput.},
  title        = {Lifelong classification in open world with limited storage requirements},
  volume       = {33},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). From univariate to multivariate coupling between continuous
signals and point processes: A mathematical framework. <em>NECO</em>,
<em>33</em>(7), 1751–1817. (<a
href="https://doi.org/10.1162/neco_a_01389">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time series data sets often contain heterogeneous signals, composed of both continuously changing quantities and discretely occurring events. The coupling between these measurements may provide insights into key underlying mechanisms of the systems under study. To better extract this information, we investigate the asymptotic statistical properties of coupling measures between continuous signals and point processes. We first introduce martingale stochastic integration theory as a mathematical model for a family of statistical quantities that include the phase locking value, a classical coupling measure to characterize complex dynamics. Based on the martingale central limit theorem, we can then derive the asymptotic gaussian distribution of estimates of such coupling measure that can be exploited for statistical testing. Second, based on multivariate extensions of this result and random matrix theory, we establish a principled way to analyze the low-rank coupling between a large number of point processes and continuous signals. For a null hypothesis of no coupling, we establish sufficient conditions for the empirical distribution of squared singular values of the matrix to converge, as the number of measured signals increases, to the well-known Marchenko-Pastur (MP) law, and the largest squared singular value converges to the upper end of the MP support. This justifies a simple thresholding approach to assess the significance of multivariate coupling. Finally, we illustrate with simulations the relevance of our univariate and multivariate results in the context of neural time series, addressing how to reliably quantify the interplay between multichannel local field potential signals and the spiking activity of a large population of neurons.},
  archive      = {J_NECO},
  author       = {Safavi, Shervin and Logothetis, Nikos K. and Besserve, Michel},
  doi          = {10.1162/neco_a_01389},
  journal      = {Neural Computation},
  number       = {7},
  pages        = {1751-1817},
  shortjournal = {Neural Comput.},
  title        = {From univariate to multivariate coupling between continuous signals and point processes: A mathematical framework},
  volume       = {33},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Nonlinear decoding of natural images from large-scale
primate retinal ganglion recordings. <em>NECO</em>, <em>33</em>(7),
1719–1750. (<a href="https://doi.org/10.1162/neco_a_01395">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decoding sensory stimuli from neural activity can provide insight into how the nervous system might interpret the physical environment, and facilitates the development of brain-machine interfaces. Nevertheless, the neural decoding problem remains a significant open challenge. Here, we present an efficient nonlinear decoding approach for inferring natural scene stimuli from the spiking activities of retinal ganglion cells (RGCs). Our approach uses neural networks to improve on existing decoders in both accuracy and scalability. Trained and validated on real retinal spike data from more than 1000 simultaneously recorded macaque RGC units, the decoder demonstrates the necessity of nonlinear computations for accurate decoding of the fine structures of visual stimuli. Specifically, high-pass spatial features of natural images can only be decoded using nonlinear techniques, while low-pass features can be extracted equally well by linear and nonlinear methods. Together, these results advance the state of the art in decoding natural stimuli from large populations of neurons.},
  archive      = {J_NECO},
  author       = {Kim, Young Joon and Brackbill, Nora and Batty, Eleanor and Lee, JinHyung and Mitelut, Catalin and Tong, William and Chichilnisky, E. J. and Paninski, Liam},
  doi          = {10.1162/neco_a_01395},
  journal      = {Neural Computation},
  number       = {7},
  pages        = {1719-1750},
  shortjournal = {Neural Comput.},
  title        = {Nonlinear decoding of natural images from large-scale primate retinal ganglion recordings},
  volume       = {33},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Erratum to “a mean-field description of bursting dynamics in
spiking neural networks with short-term adaptation” by richard gast,
helmut schmidt, and thomas r. Knösche (neural computation, september
2020, vol. 32, no. 9, pp. 1615–1634). <em>NECO</em>, <em>33</em>(6),
1717. (<a href="https://doi.org/10.1162/neco_c_01397">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To implement synaptic depression in our model, we introduced an adaptation variable A to our model, governed by equations 3.2 to 3.4. These equations represent synaptic depression as a convolution with an alpha kernel with rate α and timescale τA. We solved the convolution integral to express it as a set of differential equations (see equations 3.5 and 3.6). However, equation 3.6 contains an erroneous scaling of the input firing rate r with the capitation time constant τA. The convolution integral in equation 3.2 can be expressed as a second-order differential equation: (7.1)τAA¨=-2A˙-AτA+αr.The equation can be transformed into a set of two coupled first-order differential equations under a simple change of variables: (7.2)τAA˙=B,(7.3)τAB˙=-2B-A+ατAr.Equation 7.3 differs from equation 3.6 merely by a constant scaling of r with τA. Since we have never varied τA in this study, this has no impact on our results. Still, the reported values of α for which we find synchronized bursting hold only for synaptic depression given by equations 7.2 and 7.3.},
  archive      = {J_NECO},
  doi          = {10.1162/neco_c_01397},
  journal      = {Neural Computation},
  number       = {6},
  pages        = {1717},
  shortjournal = {Neural Comput.},
  title        = {Erratum to “A mean-field description of bursting dynamics in spiking neural networks with short-term adaptation” by richard gast, helmut schmidt, and thomas r. knösche (Neural computation, september 2020, vol. 32, no. 9, pp. 1615–1634)},
  volume       = {33},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Noise robust projection rule for klein hopfield neural
networks. <em>NECO</em>, <em>33</em>(6), 1698–1716. (<a
href="https://doi.org/10.1162/neco_a_01385">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multistate Hopfield models, such as complex-valued Hopfield neural networks (CHNNs), have been used as multistate neural associative memories. Quaternion-valued Hopfield neural networks (QHNNs) reduce the number of weight parameters of CHNNs. The CHNNs and QHNNs have weak noise tolerance by the inherent property of rotational invariance. Klein Hopfield neural networks (KHNNs) improve the noise tolerance by resolving rotational invariance. However, the KHNNs have another disadvantage of self-feedback, a major factor of deterioration in noise tolerance. In this work, the stability conditions of KHNNs are extended. Moreover, the projection rule for KHNNs is modified using the extended conditions. The proposed projection rule improves the noise tolerance by a reduction in self-feedback. Computer simulations support that the proposed projection rule improves the noise tolerance of KHNNs.},
  archive      = {J_NECO},
  author       = {Kobayashi, Masaki},
  doi          = {10.1162/neco_a_01385},
  journal      = {Neural Computation},
  number       = {6},
  pages        = {1698-1716},
  shortjournal = {Neural Comput.},
  title        = {Noise robust projection rule for klein hopfield neural networks},
  volume       = {33},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A framework of learning through empirical gain maximization.
<em>NECO</em>, <em>33</em>(6), 1656–1697. (<a
href="https://doi.org/10.1162/neco_a_01384">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop in this letter a framework of empirical gain maximization (EGM) to address the robust regression problem where heavy-tailed noise or outliers may be present in the response variable. The idea of EGM is to approximate the density function of the noise distribution instead of approximating the truth function directly as usual. Unlike the classical maximum likelihood estimation that encourages equal importance of all observations and could be problematic in the presence of abnormal observations, EGM schemes can be interpreted from a minimum distance estimation viewpoint and allow the ignorance of those observations. Furthermore, we show that several well-known robust nonconvex regression paradigms, such as Tukey regression and truncated least square regression, can be reformulated into this new framework. We then develop a learning theory for EGM by means of which a unified analysis can be conducted for these well-established but not fully understood regression approaches. This new framework leads to a novel interpretation of existing bounded nonconvex loss functions. Within this new framework, the two seemingly irrelevant terminologies, the well-known Tukey&#39;s biweight loss for robust regression and the triweight kernel for nonparametric smoothing, are closely related. More precisely, we show that Tukey&#39;s biweight loss can be derived from the triweight kernel. Other frequently employed bounded nonconvex loss functions in machine learning, such as the truncated square loss, the Geman-McClure loss, and the exponential squared loss, can also be reformulated from certain smoothing kernels in statistics. In addition, the new framework enables us to devise new bounded nonconvex loss functions for robust learning.},
  archive      = {J_NECO},
  author       = {Feng, Yunlong and Wu, Qiang},
  doi          = {10.1162/neco_a_01384},
  journal      = {Neural Computation},
  number       = {6},
  pages        = {1656-1697},
  shortjournal = {Neural Comput.},
  title        = {A framework of learning through empirical gain maximization},
  volume       = {33},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Online mental fatigue monitoring via indirect brain dynamics
evaluation. <em>NECO</em>, <em>33</em>(6), 1616–1655. (<a
href="https://doi.org/10.1162/neco_a_01382">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Driver mental fatigue leads to thousands of traffic accidents. The increasing quality and availability of low-cost electroencephalogram (EEG) systems offer possibilities for practical fatigue monitoring. However, non-data-driven methods, designed for practical, complex situations, usually rely on handcrafted data statistics of EEG signals. To reduce human involvement, we introduce a data-driven methodology for online mental fatigue detection: self-weight ordinal regression (SWORE). Reaction time (RT), referring to the length of time people take to react to an emergency, is widely considered an objective behavioral measure for mental fatigue state. Since regression methods are sensitive to extreme RTs, we propose an indirect RT estimation based on preferences to explore the relationship between EEG and RT, which generalizes to any scenario when an objective fatigue indicator is available. In particular, SWORE evaluates the noisy EEG signals from multiple channels in terms of two states: shaking state and steady state. Modeling the shaking state can discriminate the reliable channels from the uninformative ones, while modeling the steady state can suppress the task-nonrelevant fluctuation within each channel. In addition, an online generalized Bayesian moment matching (online GBMM) algorithm is proposed to online-calibrate SWORE efficiently per participant. Experimental results with 40 participants show that SWORE can maximally achieve consistent with RT, demonstrating the feasibility and adaptability of our proposed framework in practical mental fatigue estimation.},
  archive      = {J_NECO},
  author       = {Pan, Yuangang and Tsang, Ivor W. and Lyu, Yueming and Singh, Avinash K. and Lin, Chin-Teng},
  doi          = {10.1162/neco_a_01382},
  journal      = {Neural Computation},
  number       = {6},
  pages        = {1616-1655},
  shortjournal = {Neural Comput.},
  title        = {Online mental fatigue monitoring via indirect brain dynamics evaluation},
  volume       = {33},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Shaping dynamics with multiple populations in low-rank
recurrent networks. <em>NECO</em>, <em>33</em>(6), 1572–1615. (<a
href="https://doi.org/10.1162/neco_a_01381">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An emerging paradigm proposes that neural computations can be understood at the level of dynamic systems that govern low-dimensional trajectories of collective neural activity. How the connectivity structure of a network determines the emergent dynamical system, however, remains to be clarified. Here we consider a novel class of models, gaussian-mixture, low-rank recurrent networks in which the rank of the connectivity matrix and the number of statistically defined populations are independent hyperparameters. We show that the resulting collective dynamics form a dynamical system, where the rank sets the dimensionality and the population structure shapes the dynamics. In particular, the collective dynamics can be described in terms of a simplified effective circuit of interacting latent variables. While having a single global population strongly restricts the possible dynamics, we demonstrate that if the number of populations is large enough, a rank R network can approximate any R -dimensional dynamical system.},
  archive      = {J_NECO},
  author       = {Beiran, Manuel and Dubreuil, Alexis and Valente, Adrian and Mastrogiuseppe, Francesca and Ostojic, Srdjan},
  doi          = {10.1162/neco_a_01381},
  journal      = {Neural Computation},
  number       = {6},
  pages        = {1572-1615},
  shortjournal = {Neural Comput.},
  title        = {Shaping dynamics with multiple populations in low-rank recurrent networks},
  volume       = {33},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Might a single neuron solve interesting machine learning
problems through successive computations on its dendritic tree?
<em>NECO</em>, <em>33</em>(6), 1554–1571. (<a
href="https://doi.org/10.1162/neco_a_01390">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Physiological experiments have highlighted how the dendrites of biological neurons can nonlinearly process distributed synaptic inputs. However, it is unclear how aspects of a dendritic tree, such as its branched morphology or its repetition of presynaptic inputs, determine neural computation beyond this apparent nonlinearity. Here we use a simple model where the dendrite is implemented as a sequence of thresholded linear units. We manipulate the architecture of this model to investigate the impacts of binary branching constraints and repetition of synaptic inputs on neural computation. We find that models with such manipulations can perform well on machine learning tasks, such as Fashion MNIST or Extended MNIST. We find that model performance on these tasks is limited by binary tree branching and dendritic asymmetry and is improved by the repetition of synaptic inputs to different dendritic branches. These computational experiments further neuroscience theory on how different dendritic properties might determine neural computation of clearly defined tasks.},
  archive      = {J_NECO},
  author       = {Jones, Ilenna Simone and Kording, Konrad Paul},
  doi          = {10.1162/neco_a_01390},
  journal      = {Neural Computation},
  number       = {6},
  pages        = {1554-1571},
  shortjournal = {Neural Comput.},
  title        = {Might a single neuron solve interesting machine learning problems through successive computations on its dendritic tree?},
  volume       = {33},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Reinforcement learning in sparse-reward environments with
hindsight policy gradients. <em>NECO</em>, <em>33</em>(6), 1498–1553.
(<a href="https://doi.org/10.1162/neco_a_01387">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A reinforcement learning agent that needs to pursue different goals across episodes requires a goal-conditional policy. In addition to their potential to generalize desirable behavior to unseen goals, such policies may also enable higher-level planning based on subgoals. In sparse-reward environments, the capacity to exploit information about the degree to which an arbitrary goal has been achieved while another goal was intended appears crucial to enabling sample efficient learning. However, reinforcement learning agents have only recently been endowed with such capacity for hindsight. In this letter, we demonstrate how hindsight can be introduced to policy gradient methods, generalizing this idea to a broad class of successful algorithms. Our experiments on a diverse selection of sparse-reward environments show that hindsight leads to a remarkable increase in sample efficiency.},
  archive      = {J_NECO},
  author       = {Rauber, Paulo and Ummadisingu, Avinash and Mutz, Filipe and Schmidhuber, Jürgen},
  doi          = {10.1162/neco_a_01387},
  journal      = {Neural Computation},
  number       = {6},
  pages        = {1498-1553},
  shortjournal = {Neural Comput.},
  title        = {Reinforcement learning in sparse-reward environments with hindsight policy gradients},
  volume       = {33},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Critical point-finding methods reveal gradient-flat regions
of deep network losses. <em>NECO</em>, <em>33</em>(6), 1469–1497. (<a
href="https://doi.org/10.1162/neco_a_01388">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the fact that the loss functions of deep neural networks are highly nonconvex, gradient-based optimization algorithms converge to approximately the same performance from many random initial points. One thread of work has focused on explaining this phenomenon by numerically characterizing the local curvature near critical points of the loss function, where the gradients are near zero. Such studies have reported that neural network losses enjoy a no-bad-local-minima property, in disagreement with more recent theoretical results. We report here that the methods used to find these putative critical points suffer from a bad local minima problem of their own: they often converge to or pass through regions where the gradient norm has a stationary point. We call these gradient-flat regions , since they arise when the gradient is approximately in the kernel of the Hessian, such that the loss is locally approximately linear, or flat, in the direction of the gradient. We describe how the presence of these regions necessitates care in both interpreting past results that claimed to find critical points of neural network losses and in designing second-order methods for optimizing neural networks.},
  archive      = {J_NECO},
  author       = {Frye, Charles G. and Simon, James and Wadia, Neha S. and Ligeralde, Andrew and DeWeese, Michael R. and Bouchard, Kristofer E.},
  doi          = {10.1162/neco_a_01388},
  journal      = {Neural Computation},
  number       = {6},
  pages        = {1469-1497},
  shortjournal = {Neural Comput.},
  title        = {Critical point-finding methods reveal gradient-flat regions of deep network losses},
  volume       = {33},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On the achievability of blind source separation for
high-dimensional nonlinear source mixtures. <em>NECO</em>,
<em>33</em>(6), 1433–1468. (<a
href="https://doi.org/10.1162/neco_a_01378">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For many years, a combination of principal component analysis (PCA) and independent component analysis (ICA) has been used for blind source separation (BSS). However, it remains unclear why these linear methods work well with real-world data that involve nonlinear source mixtures. This work theoretically validates that a cascade of linear PCA and ICA can solve a nonlinear BSS problem accurately—when the sensory inputs are generated from hidden sources via nonlinear mappings with sufficient dimensionality. Our proposed theorem, termed the asymptotic linearization theorem, theoretically guarantees that applying linear PCA to the inputs can reliably extract a subspace spanned by the linear projections from every hidden source as the major components—and thus projecting the inputs onto their major eigenspace can effectively recover a linear transformation of the hidden sources. Then subsequent application of linear ICA can separate all the true independent hidden sources accurately. Zero-element-wise-error nonlinear BSS is asymptotically attained when the source dimensionality is large and the input dimensionality is sufficiently larger than the source dimensionality. Our proposed theorem is validated analytically and numerically. Moreover, the same computation can be performed by using Hebbian-like plasticity rules, implying the biological plausibility of this nonlinear BSS strategy. Our results highlight the utility of linear PCA and ICA for accurately and reliably recovering nonlinearly mixed sources and suggest the importance of employing sensors with sufficient dimensionality to identify true hidden sources of real-world data.},
  archive      = {J_NECO},
  author       = {Isomura, Takuya and Toyoizumi, Taro},
  doi          = {10.1162/neco_a_01378},
  journal      = {Neural Computation},
  number       = {6},
  pages        = {1433-1468},
  shortjournal = {Neural Comput.},
  title        = {On the achievability of blind source separation for high-dimensional nonlinear source mixtures},
  volume       = {33},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Predictive processing in cognitive robotics: A review.
<em>NECO</em>, <em>33</em>(5), 1402–1432. (<a
href="https://doi.org/10.1162/neco_a_01383">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predictive processing has become an influential framework in cognitive sciences. This framework turns the traditional view of perception upside down, claiming that the main flow of information processing is realized in a top-down, hierarchical manner. Furthermore, it aims at unifying perception, cognition, and action as a single inferential process. However, in the related literature, the predictive processing framework and its associated schemes, such as predictive coding, active inference, perceptual inference, and free-energy principle, tend to be used interchangeably. In the field of cognitive robotics, there is no clear-cut distinction on which schemes have been implemented and under which assumptions. In this letter, working definitions are set with the main aim of analyzing the state of the art in cognitive robotics research working under the predictive processing framework as well as some related nonrobotic models. The analysis suggests that, first, research in both cognitive robotics implementations and nonrobotic models needs to be extended to the study of how multiple exteroceptive modalities can be integrated into prediction error minimization schemes. Second, a relevant distinction found here is that cognitive robotics implementations tend to emphasize the learning of a generative model, while in nonrobotics models, it is almost absent. Third, despite the relevance for active inference, few cognitive robotics implementations examine the issues around control and whether it should result from the substitution of inverse models with proprioceptive predictions. Finally, limited attention has been placed on precision weighting and the tracking of prediction error dynamics. These mechanisms should help to explore more complex behaviors and tasks in cognitive robotics research under the predictive processing framework.},
  archive      = {J_NECO},
  author       = {Ciria, Alejandra and Schillaci, Guido and Pezzulo, Giovanni and Hafner, Verena V. and Lara, Bruno},
  doi          = {10.1162/neco_a_01383},
  journal      = {Neural Computation},
  number       = {5},
  pages        = {1402-1432},
  shortjournal = {Neural Comput.},
  title        = {Predictive processing in cognitive robotics: A review},
  volume       = {33},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hierarchical dynamical model for multiple cortical neural
decoding. <em>NECO</em>, <em>33</em>(5), 1372–1401. (<a
href="https://doi.org/10.1162/neco_a_01380">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motor brain machine interfaces (BMIs) interpret neural activities from motor-related cortical areas in the brain into movement commands to control a prosthesis. As the subject adapts to control the neural prosthesis, the medial prefrontal cortex (mPFC), upstream of the primary motor cortex (M1), is heavily involved in reward-guided motor learning. Thus, considering mPFC and M1 functionality within a hierarchical structure could potentially improve the effectiveness of BMI decoding while subjects are learning. The commonly used Kalman decoding method with only one simple state model may not be able to represent the multiple brain states that evolve over time as well as along the neural pathway. In addition, the performance of Kalman decoders degenerates in heavy-tailed nongaussian noise, which is usually generated due to the nonlinear neural system or influences of movement-related noise in online neural recording. In this letter, we propose a hierarchical model to represent the brain states from multiple cortical areas that evolve along the neural pathway. We then introduce correntropy theory into the hierarchical structure to address the heavy-tailed noise existing in neural recordings. We test the proposed algorithm on in vivo recordings collected from the mPFC and M1 of two rats when the subjects were learning to perform a lever-pressing task. Compared with the classic Kalman filter, our results demonstrate better movement decoding performance due to the hierarchical structure that integrates the past failed trial information over multisite recording and the combination with correntropy criterion to deal with noisy heavy-tailed neural recordings.},
  archive      = {J_NECO},
  author       = {Liu, Xi and Shen, Xiang and Chen, Shuhang and Zhang, Xiang and Huang, Yifan and Wang, Yueming and Wang, Yiwen},
  doi          = {10.1162/neco_a_01380},
  journal      = {Neural Computation},
  number       = {5},
  pages        = {1372-1401},
  shortjournal = {Neural Comput.},
  title        = {Hierarchical dynamical model for multiple cortical neural decoding},
  volume       = {33},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Detecting scene-plausible perceptible backdoors in trained
DNNs without access to the training set. <em>NECO</em>, <em>33</em>(5),
1329–1371. (<a href="https://doi.org/10.1162/neco_a_01376">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Backdoor data poisoning attacks add mislabeled examples to the training set, with an embedded backdoor pattern, so that the classifier learns to classify to a target class whenever the backdoor pattern is present in a test sample. Here, we address posttraining detection of scene-plausible perceptible backdoors, a type of backdoor attack that can be relatively easily fashioned, particularly against DNN image classifiers. A post-training defender does not have access to the potentially poisoned training set, only to the trained classifier, as well as some unpoisoned examples that need not be training samples. Without the poisoned training set, the only information about a backdoor pattern is encoded in the DNN&#39;s trained weights. This detection scenario is of great import considering legacy and proprietary systems, cell phone apps, as well as training outsourcing, where the user of the classifier will not have access to the entire training set. We identify two important properties of scene-plausible perceptible backdoor patterns, spatial invariance and robustness, based on which we propose a novel detector using the maximum achievable misclassification fraction (MAMF) statistic. We detect whether the trained DNN has been backdoor-attacked and infer the source and target classes. Our detector outperforms existing detectors and, coupled with an imperceptible backdoor detector, helps achieve posttraining detection of most evasive backdoors of interest.},
  archive      = {J_NECO},
  author       = {Xiang, Zhen and Miller, David J. and Wang, Hang and Kesidis, George},
  doi          = {10.1162/neco_a_01376},
  journal      = {Neural Computation},
  number       = {5},
  pages        = {1329-1371},
  shortjournal = {Neural Comput.},
  title        = {Detecting scene-plausible perceptible backdoors in trained DNNs without access to the training set},
  volume       = {33},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Contrastive similarity matching for supervised learning.
<em>NECO</em>, <em>33</em>(5), 1300–1328. (<a
href="https://doi.org/10.1162/neco_a_01374">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel biologically plausible solution to the credit assignment problem motivated by observations in the ventral visual pathway and trained deep neural networks. In both, representations of objects in the same category become progressively more similar, while objects belonging to different categories become less similar. We use this observation to motivate a layer-specific learning goal in a deep network: each layer aims to learn a representational similarity matrix that interpolates between previous and later layers. We formulate this idea using a contrastive similarity matching objective function and derive from it deep neural networks with feedforward, lateral, and feedback connections and neurons that exhibit biologically plausible Hebbian and anti-Hebbian plasticity. Contrastive similarity matching can be interpreted as an energy-based learning algorithm, but with significant differences from others in how a contrastive function is constructed.},
  archive      = {J_NECO},
  author       = {Qin, Shanshan and Mudur, Nayantara and Pehlevan, Cengiz},
  doi          = {10.1162/neco_a_01374},
  journal      = {Neural Computation},
  number       = {5},
  pages        = {1300-1328},
  shortjournal = {Neural Comput.},
  title        = {Contrastive similarity matching for supervised learning},
  volume       = {33},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Parameter estimation in multiple dynamic synaptic coupling
model using bayesian point process state-space modeling framework.
<em>NECO</em>, <em>33</em>(5), 1269–1299. (<a
href="https://doi.org/10.1162/neco_a_01375">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is of great interest to characterize the spiking activity of individual neurons in a cell ensemble. Many different mechanisms, such as synaptic coupling and the spiking activity of itself and its neighbors, drive a cell&#39;s firing properties. Though this is a widely studied modeling problem, there is still room to develop modeling solutions by simplifications embedded in previous models. The first shortcut is that synaptic coupling mechanisms in previous models do not replicate the complex dynamics of the synaptic response. The second is that the number of synaptic connections in these models is an order of magnitude smaller than in an actual neuron. In this research, we push this barrier by incorporating a more accurate model of the synapse and propose a system identification solution that can scale to a network incorporating hundreds of synaptic connections. Although a neuron has hundreds of synaptic connections, only a subset of these connections significantly contributes to its spiking activity. As a result, we assume the synaptic connections are sparse, and to characterize these dynamics, we propose a Bayesian point-process state-space model that lets us incorporate the sparsity of synaptic connections within the regularization technique into our framework. We develop an extended expectation-maximization. algorithm to estimate the free parameters of the proposed model and demonstrate the application of this methodology to the problem of estimating the parameters of many dynamic synaptic connections. We then go through a simulation example consisting of the dynamic synapses across a range of parameter values and show that the model parameters can be estimated using our method. We also show the application of the proposed algorithm in the intracellular data that contains 96 presynaptic connections and assess the estimation accuracy of our method using a combination of goodness-of-fit measures.},
  archive      = {J_NECO},
  author       = {Amidi, Yalda and Nazari, Behzad and Sadri, Saeid and Yousefi, Ali},
  doi          = {10.1162/neco_a_01375},
  journal      = {Neural Computation},
  number       = {5},
  pages        = {1269-1299},
  shortjournal = {Neural Comput.},
  title        = {Parameter estimation in multiple dynamic synaptic coupling model using bayesian point process state-space modeling framework},
  volume       = {33},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Classification from pairwise similarities/dissimilarities
and unlabeled data via empirical risk minimization. <em>NECO</em>,
<em>33</em>(5), 1234–1268. (<a
href="https://doi.org/10.1162/neco_a_01373">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pairwise similarities and dissimilarities between data points are often obtained more easily than full labels of data in real-world classification problems. To make use of such pairwise information, an empirical risk minimization approach has been proposed, where an unbiased estimator of the classification risk is computed from only pairwise similarities and unlabeled data. However, this approach has not yet been able to handle pairwise dissimilarities. Semisupervised clustering methods can incorporate both similarities and dissimilarities into their framework; however, they typically require strong geometrical assumptions on the data distribution such as the manifold assumption, which may cause severe performance deterioration. In this letter, we derive an unbiased estimator of the classification risk based on all of similarities and dissimilarities and unlabeled data. We theoretically establish an estimation error bound and experimentally demonstrate the practical usefulness of our empirical risk minimization method.},
  archive      = {J_NECO},
  author       = {Shimada, Takuya and Bao, Han and Sato, Issei and Sugiyama, Masashi},
  doi          = {10.1162/neco_a_01373},
  journal      = {Neural Computation},
  number       = {5},
  pages        = {1234-1268},
  shortjournal = {Neural Comput.},
  title        = {Classification from pairwise Similarities/Dissimilarities and unlabeled data via empirical risk minimization},
  volume       = {33},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Training spiking neural networks in the strong coupling
regime. <em>NECO</em>, <em>33</em>(5), 1199–1233. (<a
href="https://doi.org/10.1162/neco_a_01379">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recurrent neural networks trained to perform complex tasks can provide insight into the dynamic mechanism that underlies computations performed by cortical circuits. However, due to a large number of unconstrained synaptic connections, the recurrent connectivity that emerges from network training may not be biologically plausible. Therefore, it remains unknown if and how biological neural circuits implement dynamic mechanisms proposed by the models. To narrow this gap, we developed a training scheme that, in addition to achieving learning goals, respects the structural and dynamic properties of a standard cortical circuit model: strongly coupled excitatory-inhibitory spiking neural networks. By preserving the strong mean excitatory and inhibitory coupling of initial networks, we found that most of trained synapses obeyed Dale&#39;s law without additional constraints, exhibited large trial-to-trial spiking variability, and operated in inhibition-stabilized regime. We derived analytical estimates on how training and network parameters constrained the changes in mean synaptic strength during training. Our results demonstrate that training recurrent neural networks subject to strong coupling constraints can result in connectivity structure and dynamic regime relevant to cortical circuits.},
  archive      = {J_NECO},
  author       = {Kim, Christopher M. and Chow, Carson C.},
  doi          = {10.1162/neco_a_01379},
  journal      = {Neural Computation},
  number       = {5},
  pages        = {1199-1233},
  shortjournal = {Neural Comput.},
  title        = {Training spiking neural networks in the strong coupling regime},
  volume       = {33},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Toward a kernel-based uncertainty decomposition framework
for data and models. <em>NECO</em>, <em>33</em>(5), 1164–1198. (<a
href="https://doi.org/10.1162/neco_a_01372">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This letter introduces a new framework for quantifying predictive uncertainty for both data and models that relies on projecting the data into a gaussian reproducing kernel Hilbert space (RKHS) and transforming the data probability density function (PDF) in a way that quantifies the flow of its gradient as a topological potential field (quantified at all points in the sample space). This enables the decomposition of the PDF gradient flow by formulating it as a moment decomposition problem using operators from quantum physics, specifically Schrödinger&#39;s formulation. We experimentally show that the higher-order moments systematically cluster the different tail regions of the PDF, thereby providing unprecedented discriminative resolution of data regions having high epistemic uncertainty. In essence, this approach decomposes local realizations of the data PDF in terms of uncertainty moments. We apply this framework as a surrogate tool for predictive uncertainty quantification of point-prediction neural network models, overcoming various limitations of conventional Bayesian-based uncertainty quantification methods. Experimental comparisons with some established methods illustrate performance advantages that our framework exhibits.},
  archive      = {J_NECO},
  author       = {Singh, Rishabh and Principe, Jose C.},
  doi          = {10.1162/neco_a_01372},
  journal      = {Neural Computation},
  number       = {5},
  pages        = {1164-1198},
  shortjournal = {Neural Comput.},
  title        = {Toward a kernel-based uncertainty decomposition framework for data and models},
  volume       = {33},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The refractory period matters: Unifying mechanisms of
macroscopic brain waves. <em>NECO</em>, <em>33</em>(5), 1145–1163. (<a
href="https://doi.org/10.1162/neco_a_01371">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The relationship between complex brain oscillations and the dynamics of individual neurons is poorly understood. Here we utilize maximum caliber, a dynamical inference principle, to build a minimal yet general model of the collective (mean field) dynamics of large populations of neurons. In agreement with previous experimental observations, we describe a simple, testable mechanism, involving only a single type of neuron, by which many of these complex oscillatory patterns may emerge. Our model predicts that the refractory period of neurons, which has often been neglected, is essential for these behaviors.},
  archive      = {J_NECO},
  author       = {Weistuch, Corey and Mujica-Parodi, Lilianne R. and Dill, Ken},
  doi          = {10.1162/neco_a_01371},
  journal      = {Neural Computation},
  number       = {5},
  pages        = {1145-1163},
  shortjournal = {Neural Comput.},
  title        = {The refractory period matters: Unifying mechanisms of macroscopic brain waves},
  volume       = {33},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Joint structure and parameter optimization of multiobjective
sparse neural network. <em>NECO</em>, <em>33</em>(4), 1113–1143. (<a
href="https://doi.org/10.1162/neco_a_01368">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work addresses the problem of network pruning and proposes a novel joint training method based on a multiobjective optimization model. Most of the state-of-the-art pruning methods rely on user experience for selecting the sparsity ratio of the weight matrices or tensors, and thus suffer from severe performance reduction with inappropriate user-defined parameters. Moreover, networks might be inferior due to the inefficient connecting architecture search, especially when it is highly sparse. It is revealed in this work that the network model might maintain sparse characteristic in the early stage of the backpropagation (BP) training process, and evolutionary computation-based algorithms can accurately discover the connecting architecture with satisfying network performance. In particular, we establish a multiobjective sparse model for network pruning and propose an efficient approach that combines BP training and two modified multiobjective evolutionary algorithms (MOEAs). The BP algorithm converges quickly, and the two MOEAs can search for the optimal sparse structure and refine the weights, respectively. Experiments are also included to prove the benefits of the proposed algorithm. We show that the proposed method can obtain a desired Pareto front (PF), leading to a better pruning result comparing to the state-of-the-art methods, especially when the network structure is highly sparse.},
  archive      = {J_NECO},
  author       = {Huang, Junhao and Sun, Weize and Huang, Lei},
  doi          = {10.1162/neco_a_01368},
  journal      = {Neural Computation},
  number       = {4},
  pages        = {1113-1143},
  shortjournal = {Neural Comput.},
  title        = {Joint structure and parameter optimization of multiobjective sparse neural network},
  volume       = {33},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Low-dimensional manifolds support multiplexed integrations
in recurrent neural networks. <em>NECO</em>, <em>33</em>(4), 1063–1112.
(<a href="https://doi.org/10.1162/neco_a_01366">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the learning dynamics and the representations emerging in recurrent neural networks (RNNs) trained to integrate one or multiple temporal signals. Combining analytical and numerical investigations, we characterize the conditions under which an RNN with n neurons learns to integrate D ( ≪ n ) scalar signals of arbitrary duration. We show, for linear, ReLU, and sigmoidal neurons, that the internal state lives close to a D -dimensional manifold, whose shape is related to the activation function. Each neuron therefore carries, to various degrees, information about the value of all integrals. We discuss the deep analogy between our results and the concept of mixed selectivity forged by computational neuroscientists to interpret cortical recordings.},
  archive      = {J_NECO},
  author       = {Fanthomme, Arnaud and Monasson, Rémi},
  doi          = {10.1162/neco_a_01366},
  journal      = {Neural Computation},
  number       = {4},
  pages        = {1063-1112},
  shortjournal = {Neural Comput.},
  title        = {Low-dimensional manifolds support multiplexed integrations in recurrent neural networks},
  volume       = {33},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A generalization of spatial monte carlo integration.
<em>NECO</em>, <em>33</em>(4), 1037–1062. (<a
href="https://doi.org/10.1162/neco_a_01365">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatial Monte Carlo integration (SMCI) is an extension of standard Monte Carlo integration and can approximate expectations on Markov random fields with high accuracy. SMCI was applied to pairwise Boltzmann machine (PBM) learning, achieving superior results over those of some existing methods. The approximation level of SMCI can be altered, and it was proved that a higher-order approximation of SMCI is statistically more accurate than a lower-order approximation. However, SMCI as proposed in previous studies suffers from a limitation that prevents the application of a higher-order method to dense systems. This study makes two contributions. First, a generalization of SMCI (called generalized SMCI (GSMCI)) is proposed, which allows a relaxation of the above-mentioned limitation; moreover, a statistical accuracy bound of GSMCI is proved. Second, a new PBM learning method based on SMCI is proposed, which is obtained by combining SMCI and persistent contrastive divergence. The proposed learning method significantly improves learning accuracy.},
  archive      = {J_NECO},
  author       = {Yasuda, Muneki and Uchizawa, Kei},
  doi          = {10.1162/neco_a_01365},
  journal      = {Neural Computation},
  number       = {4},
  pages        = {1037-1062},
  shortjournal = {Neural Comput.},
  title        = {A generalization of spatial monte carlo integration},
  volume       = {33},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep network with approximation error being reciprocal of
width to power of square root of depth. <em>NECO</em>, <em>33</em>(4),
1005–1036. (<a href="https://doi.org/10.1162/neco_a_01364">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A new network with super-approximation power is introduced. This network is built with Floor ( ⁠ ⌊ x ⌋ ⁠ ) or ReLU ( ⁠ max { 0 , x } ⁠ ) activation function in each neuron; hence, we call such networks Floor-ReLU networks. For any hyperparameters N ∈ N + and L ∈ N + ⁠ , we show that Floor-ReLU networks with width max { d , 5 N + 13 } and depth 64 d L + 3 can uniformly approximate a Hölder function f on [ 0 , 1 ] d with an approximation error 3 λ d α / 2 N - α L ⁠ , where α ∈ ( 0 , 1 ] and λ are the Hölder order and constant, respectively. More generally for an arbitrary continuous function f on [ 0 , 1 ] d with a modulus of continuity ω f ( · ) ⁠ , the constructive approximation rate is ω f ( d N - L ) + 2 ω f ( d ) N - L ⁠ . As a consequence, this new class of networks overcomes the curse of dimensionality in approximation power when the variation of ω f ( r ) as r → 0 is moderate (e.g., ω f ( r ) ≲ r α for Hölder continuous functions), since the major term to be considered in our approximation rate is essentially d times a function of N and L independent of d within the modulus of continuity.},
  archive      = {J_NECO},
  author       = {Shen, Zuowei and Yang, Haizhao and Zhang, Shijun},
  doi          = {10.1162/neco_a_01364},
  journal      = {Neural Computation},
  number       = {4},
  pages        = {1005-1036},
  shortjournal = {Neural Comput.},
  title        = {Deep network with approximation error being reciprocal of width to power of square root of depth},
  volume       = {33},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Real-time decoding of attentional states using closed-loop
EEG neurofeedback. <em>NECO</em>, <em>33</em>(4), 967–1004. (<a
href="https://doi.org/10.1162/neco_a_01363">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sustained attention is a cognitive ability to maintain task focus over extended periods of time (Mackworth, 1948 ; Chun, Golomb, &amp; Turk-Browne, 2011). In this study, scalp electroencephalography (EEG) signals were processed in real time using a 32 dry-electrode system during a sustained visual attention task. An attention training paradigm was implemented, as designed in DeBettencourt, Cohen, Lee, Norman, and Turk-Browne ( 2015 ) in which the composition of a sequence of blended images is updated based on the participant&#39;s decoded attentional level to a primed image category. It was hypothesized that a single neurofeedback training session would improve sustained attention abilities. Twenty-two participants were trained on a single neurofeedback session with behavioral pretraining and posttraining sessions within three consecutive days. Half of the participants functioned as controls in a double-blinded design and received sham neurofeedback. During the neurofeedback session, attentional states to primed categories were decoded in real time and used to provide a continuous feedback signal customized to each participant in a closed-loop approach. We report a mean classifier decoding error rate of 34.3\% (chance = 50\%). Within the neurofeedback group, there was a greater level of task-relevant attentional information decoded in the participant&#39;s brain before making a correct behavioral response than before an incorrect response. This effect was not visible in the control group (interaction p = 7 . 23 e - 4), which strongly indicates that we were able to achieve a meaningful measure of subjective attentional state in real time and control participants&#39; behavior during the neurofeedback session. We do not provide conclusive evidence whether the single neurofeedback session per se provided lasting effects in sustained attention abilities. We developed a portable EEG neurofeedback system capable of decoding attentional states and predicting behavioral choices in the attention task at hand. The neurofeedback code framework is Python based and open source, and it allows users to actively engage in the development of neurofeedback tools for scientific and translational use.},
  archive      = {J_NECO},
  author       = {Tuckute, Greta and Hansen, Sofie Therese and Kjaer, Troels Wesenberg and Hansen, Lars Kai},
  doi          = {10.1162/neco_a_01363},
  journal      = {Neural Computation},
  number       = {4},
  pages        = {967-1004},
  shortjournal = {Neural Comput.},
  title        = {Real-time decoding of attentional states using closed-loop EEG neurofeedback},
  volume       = {33},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Flexible frequency switching in adult mouse visual cortex is
mediated by competition between parvalbumin and somatostatin expressing
interneurons. <em>NECO</em>, <em>33</em>(4), 926–966. (<a
href="https://doi.org/10.1162/neco_a_01369">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neuronal networks in rodent primary visual cortex (V1) can generate oscillations in different frequency bands depending on the network state and the level of visual stimulation. High-frequency gamma rhythms, for example, dominate the network&#39;s spontaneous activity in adult mice but are attenuated upon visual stimulation, during which the network switches to the beta band instead. The spontaneous local field potential (LFP) of juvenile mouse V1, however, mainly contains beta rhythms and presenting a stimulus does not elicit drastic changes in network oscillations. We study, in a spiking neuron network model, the mechanism in adult mice allowing for flexible switches between multiple frequency bands and contrast this to the network structure in juvenile mice that lack this flexibility. The model comprises excitatory pyramidal cells (PCs) and two types of interneurons: the parvalbumin-expressing (PV) and the somatostatinexpressing (SOM) interneuron. In accordance with experimental findings, the pyramidal-PV and pyramidal-SOM cell subnetworks are associated with gamma and beta oscillations, respectively. In our model, they are both generated via a pyramidal-interneuron gamma (PING) mechanism, wherein the PCs drive the oscillations. Furthermore, we demonstrate that large but not small visual stimulation activates SOM cells, which shift the frequency of resting-state gamma oscillations produced by the pyramidal-PV cell subnetwork so that beta rhythms emerge. Finally, we show that this behavior is obtained for only a subset of PV and SOM interneuron projection strengths, indicating that their influence on the PCs should be balanced so that they can compete for oscillatory control of the PCs. In sum, we propose a mechanism by which visual beta rhythms can emerge from spontaneous gamma oscillations in a network model of the mouse V1; for this mechanism to reproduce V1 dynamics in adult mice, balance between the effective strengths of PV and SOM cells is required.},
  archive      = {J_NECO},
  author       = {Domhof, Justin W. M. and Tiesinga, Paul H. E.},
  doi          = {10.1162/neco_a_01369},
  journal      = {Neural Computation},
  number       = {4},
  pages        = {926-966},
  shortjournal = {Neural Comput.},
  title        = {Flexible frequency switching in adult mouse visual cortex is mediated by competition between parvalbumin and somatostatin expressing interneurons},
  volume       = {33},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The remarkable robustness of surrogate gradient learning for
instilling complex function in spiking neural networks. <em>NECO</em>,
<em>33</em>(4), 899–925. (<a
href="https://doi.org/10.1162/neco_a_01367">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Brains process information in spiking neural networks. Their intricate connections shape the diverse functions these networks perform. Yet how network connectivity relates to function is poorly understood, and the functional capabilities of models of spiking networks are still rudimentary. The lack of both theoretical insight and practical algorithms to find the necessary connectivity poses a major impediment to both studying information processing in the brain and building efficient neuromorphic hardware systems. The training algorithms that solve this problem for artificial neural networks typically rely on gradient descent. But doing so in spiking networks has remained challenging due to the nondifferentiable nonlinearity of spikes. To avoid this issue, one can employ surrogate gradients to discover the required connectivity. However, the choice of a surrogate is not unique, raising the question of how its implementation influences the effectiveness of the method. Here, we use numerical simulations to systematically study how essential design parameters of surrogate gradients affect learning performance on a range of classification problems. We show that surrogate gradient learning is robust to different shapes of underlying surrogate derivatives, but the choice of the derivative&#39;s scale can substantially affect learning performance. When we combine surrogate gradients with suitable activity regularization techniques, spiking networks perform robust information processing at the sparse activity limit. Our study provides a systematic account of the remarkable robustness of surrogate gradient learning and serves as a practical guide to model functional spiking neural networks.},
  archive      = {J_NECO},
  author       = {Zenke, Friedemann and Vogels, Tim P.},
  doi          = {10.1162/neco_a_01367},
  journal      = {Neural Computation},
  number       = {4},
  pages        = {899-925},
  shortjournal = {Neural Comput.},
  title        = {The remarkable robustness of surrogate gradient learning for instilling complex function in spiking neural networks},
  volume       = {33},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). How convolutional neural network architecture biases learned
opponency and color tuning. <em>NECO</em>, <em>33</em>(4), 858–898. (<a
href="https://doi.org/10.1162/neco_a_01356">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent work suggests that changing convolutional neural network (CNN) architecture by introducing a bottleneck in the second layer can yield changes in learned function. To understand this relationship fully requires a way of quantitatively comparing trained networks. The fields of electrophysiology and psychophysics have developed a wealth of methods for characterizing visual systems that permit such comparisons. Inspired by these methods, we propose an approach to obtaining spatial and color tuning curves for convolutional neurons that can be used to classify cells in terms of their spatial and color opponency. We perform these classifications for a range of CNNs with different depths and bottleneck widths. Our key finding is that networks with a bottleneck show a strong functional organization: almost all cells in the bottleneck layer become both spatially and color opponent, and cells in the layer following the bottleneck become nonopponent. The color tuning data can further be used to form a rich understanding of how color a network encodes color. As a concrete demonstration, we show that shallower networks without a bottleneck learn a complex nonlinear color system, whereas deeper networks with tight bottlenecks learn a simple channel opponent code in the bottleneck layer. We develop a method of obtaining a hue sensitivity curve for a trained CNN that enables high-level insights that complement the low-level findings from the color tuning data. We go on to train a series of networks under different conditions to ascertain the robustness of the discussed results. Ultimately our methods and findings coalesce with prior art, strengthening our ability to interpret trained CNNs and furthering our understanding of the connection between architecture and learned representation. Trained models and code for all experiments are available at https://github.com/ecs-vlc/opponency .},
  archive      = {J_NECO},
  author       = {Harris, Ethan and Mihai, Daniela and Hare, Jonathon},
  doi          = {10.1162/neco_a_01356},
  journal      = {Neural Computation},
  number       = {4},
  pages        = {858-898},
  shortjournal = {Neural Comput.},
  title        = {How convolutional neural network architecture biases learned opponency and color tuning},
  volume       = {33},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The effect of class imbalance on precision-recall curves.
<em>NECO</em>, <em>33</em>(4), 853–857. (<a
href="https://doi.org/10.1162/neco_a_01362">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this note, I study how the precision of a binary classifier depends on the ratio r of positive to negative cases in the test set, as well as the classifier&#39;s true and false-positive rates. This relationship allows prediction of how the precision-recall curve will change with r ⁠ , which seems not to be well known. It also allows prediction of how F β and the precision gain and recall gain measures of Flach and Kull ( 2015 ) vary with r ⁠ .},
  archive      = {J_NECO},
  author       = {Williams, Christopher K. I.},
  doi          = {10.1162/neco_a_01362},
  journal      = {Neural Computation},
  number       = {4},
  pages        = {853-857},
  shortjournal = {Neural Comput.},
  title        = {The effect of class imbalance on precision-recall curves},
  volume       = {33},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Mapping low-dimensional dynamics to high-dimensional neural
activity: A derivation of the ring model from the neural engineering
framework. <em>NECO</em>, <em>33</em>(3), 827–852. (<a
href="https://doi.org/10.1162/neco_a_01361">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Empirical estimates of the dimensionality of neural population activity are often much lower than the population size. Similar phenomena are also observed in trained and designed neural network models. These experimental and computational results suggest that mapping low-dimensional dynamics to high-dimensional neural space is a common feature of cortical computation. Despite the ubiquity of this observation, the constraints arising from such mapping are poorly understood. Here we consider a specific example of mapping low-dimensional dynamics to high-dimensional neural activity—the neural engineering framework. We analytically solve the framework for the classic ring model—a neural network encoding a static or dynamic angular variable. Our results provide a complete characterization of the success and failure modes for this model. Based on similarities between this and other frameworks, we speculate that these results could apply to more general scenarios.},
  archive      = {J_NECO},
  author       = {Barak, Omri and Romani, Sandro},
  doi          = {10.1162/neco_a_01361},
  journal      = {Neural Computation},
  number       = {3},
  pages        = {827-852},
  shortjournal = {Neural Comput.},
  title        = {Mapping low-dimensional dynamics to high-dimensional neural activity: A derivation of the ring model from the neural engineering framework},
  volume       = {33},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Unsupervised discovery, control, and disentanglement of
semantic attributes with applications to anomaly detection.
<em>NECO</em>, <em>33</em>(3), 802–826. (<a
href="https://doi.org/10.1162/neco_a_01359">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Our work focuses on unsupervised and generative methods that address the following goals: (1) learning unsupervised generative representations that discover latent factors controlling image semantic attributes, (2) studying how this ability to control attributes formally relates to the issue of latent factor disentanglement, clarifying related but dissimilar concepts that had been confounded in the past, and (3) developing anomaly detection methods that leverage representations learned in the first goal. For goal 1, we propose a network architecture that exploits the combination of multiscale generative models with mutual information (MI) maximization. For goal 2, we derive an analytical result, lemma 1 , that brings clarity to two related but distinct concepts: the ability of generative networks to control semantic attributes of images they generate, resulting from MI maximization, and the ability to disentangle latent space representations, obtained via total correlation minimization. More specifically, we demonstrate that maximizing semantic attribute control encourages disentanglement of latent factors. Using lemma 1 and adopting MI in our loss function, we then show empirically that for image generation tasks, the proposed approach exhibits superior performance as measured in the quality and disentanglement of the generated images when compared to other state-of-the-art methods, with quality assessed via the Fréchet inception distance (FID) and disentanglement via mutual information gap. For goal 3, we design several systems for anomaly detection exploiting representations learned in goal 1 and demonstrate their performance benefits when compared to state-of-the-art generative and discriminative algorithms. Our contributions in representation learning have potential applications in addressing other important problems in computer vision, such as bias and privacy in AI.},
  archive      = {J_NECO},
  author       = {Paul, William and Wang, I-Jeng and Alajaji, Fady and Burlina, Philippe},
  doi          = {10.1162/neco_a_01359},
  journal      = {Neural Computation},
  number       = {3},
  pages        = {802-826},
  shortjournal = {Neural Comput.},
  title        = {Unsupervised discovery, control, and disentanglement of semantic attributes with applications to anomaly detection},
  volume       = {33},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Statistical analysis of decoding performances of diverse
populations of neurons. <em>NECO</em>, <em>33</em>(3), 764–801. (<a
href="https://doi.org/10.1162/neco_a_01355">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A central theme in computational neuroscience is determining the neural correlates of efficient and accurate coding of sensory signals. Diversity, or heterogeneity, of intrinsic neural attributes is known to exist in many brain areas and is thought to significantly affect neural coding. Recent theoretical and experimental work has argued that in uncoupled networks, coding is most accurate at intermediate levels of heterogeneity. Here we consider this question with data from in vivo recordings of neurons in the electrosensory system of weakly electric fish subject to the same realization of noisy stimuli; we use a generalized linear model (GLM) to assess the accuracy of (Bayesian) decoding of stimulus given a population spiking response. The long recordings enable us to consider many uncoupled networks and a relatively wide range of heterogeneity, as well as many instances of the stimuli, thus enabling us to address this question with statistical power. The GLM decoding is performed on a single long time series of data to mimic realistic conditions rather than using trial-averaged data for better model fits. For a variety of fixed network sizes, we generally find that the optimal levels of heterogeneity are at intermediate values, and this holds in all core components of GLM. These results are robust to several measures of decoding performance, including the absolute value of the error, error weighted by the uncertainty of the estimated stimulus, and the correlation between the actual and estimated stimulus. Although a quadratic fit to decoding performance as a function of heterogeneity is statistically significant, the result is highly variable with low R 2 values. Taken together, intermediate levels of neural heterogeneity are indeed a prominent attribute for efficient coding even within a single time series, but the performance is highly variable.},
  archive      = {J_NECO},
  author       = {Wendling, Kyle P. and Ly, Cheng},
  doi          = {10.1162/neco_a_01355},
  journal      = {Neural Computation},
  number       = {3},
  pages        = {764-801},
  shortjournal = {Neural Comput.},
  title        = {Statistical analysis of decoding performances of diverse populations of neurons},
  volume       = {33},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sophisticated inference. <em>NECO</em>, <em>33</em>(3),
713–763. (<a href="https://doi.org/10.1162/neco_a_01351">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Active inference offers a first principle account of sentient behavior, from which special and important cases—for example, reinforcement learning, active learning, Bayes optimal inference, Bayes optimal design—can be derived. Active inference finesses the exploitation-exploration dilemma in relation to prior preferences by placing information gain on the same footing as reward or value. In brief, active inference replaces value functions with functionals of (Bayesian) beliefs, in the form of an expected (variational) free energy. In this letter, we consider a sophisticated kind of active inference using a recursive form of expected free energy. Sophistication describes the degree to which an agent has beliefs about beliefs. We consider agents with beliefs about the counterfactual consequences of action for states of affairs and beliefs about those latent states. In other words, we move from simply considering beliefs about “what would happen if I did that” to “what I would believe about what would happen if I did that.” The recursive form of the free energy functional effectively implements a deep tree search over actions and outcomes in the future. Crucially, this search is over sequences of belief states as opposed to states per se. We illustrate the competence of this scheme using numerical simulations of deep decision problems.},
  archive      = {J_NECO},
  author       = {Friston, Karl and Da Costa, Lancelot and Hafner, Danijar and Hesp, Casper and Parr, Thomas},
  doi          = {10.1162/neco_a_01351},
  journal      = {Neural Computation},
  number       = {3},
  pages        = {713-763},
  shortjournal = {Neural Comput.},
  title        = {Sophisticated inference},
  volume       = {33},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Active inference: Demystified and compared. <em>NECO</em>,
<em>33</em>(3), 674–712. (<a
href="https://doi.org/10.1162/neco_a_01357">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Active inference is a first principle account of how autonomous agents operate in dynamic, nonstationary environments. This problem is also considered in reinforcement learning, but limited work exists on comparing the two approaches on the same discrete-state environments. In this letter, we provide (1) an accessible overview of the discrete-state formulation of active inference, highlighting natural behaviors in active inference that are generally engineered in reinforcement learning, and (2) an explicit discrete-state comparison between active inference and reinforcement learning on an OpenAI gym baseline. We begin by providing a condensed overview of the active inference literature, in particular viewing the various natural behaviors of active inference agents through the lens of reinforcement learning. We show that by operating in a pure belief-based setting, active inference agents can carry out epistemic exploration—and account for uncertainty about their environment—in a Bayes-optimal fashion. Furthermore, we show that the reliance on an explicit reward signal in reinforcement learning is removed in active inference, where reward can simply be treated as another observation we have a preference over; even in the total absence of rewards, agent behaviors are learned through preference learning. We make these properties explicit by showing two scenarios in which active inference agents can infer behaviors in reward-free environments compared to both Q-learning and Bayesian model-based reinforcement learning agents and by placing zero prior preferences over rewards and learning the prior preferences over the observations corresponding to reward. We conclude by noting that this formalism can be applied to more complex settings (e.g., robotic arm movement, Atari games) if appropriate generative models can be formulated. In short, we aim to demystify the behavior of active inference agents by presenting an accessible discrete state-space and time formulation and demonstrate these behaviors in a OpenAI gym environment, alongside reinforcement learning agents.},
  archive      = {J_NECO},
  author       = {Sajid, Noor and Ball, Philip J. and Parr, Thomas and Friston, Karl J.},
  doi          = {10.1162/neco_a_01357},
  journal      = {Neural Computation},
  number       = {3},
  pages        = {674-712},
  shortjournal = {Neural Comput.},
  title        = {Active inference: Demystified and compared},
  volume       = {33},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Implicit regularization and momentum algorithms in
nonlinearly parameterized adaptive control and prediction.
<em>NECO</em>, <em>33</em>(3), 590–673. (<a
href="https://doi.org/10.1162/neco_a_01360">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stable concurrent learning and control of dynamical systems is the subject of adaptive control. Despite being an established field with many practical applications and a rich theory, much of the development in adaptive control for nonlinear systems revolves around a few key algorithms. By exploiting strong connections between classical adaptive nonlinear control techniques and recent progress in optimization and machine learning, we show that there exists considerable untapped potential in algorithm development for both adaptive nonlinear control and adaptive dynamics prediction. We begin by introducing first-order adaptation laws inspired by natural gradient descent and mirror descent. We prove that when there are multiple dynamics consistent with the data, these non-Euclidean adaptation laws implicitly regularize the learned model. Local geometry imposed during learning thus may be used to select parameter vectors—out of the many that will achieve perfect tracking or prediction—for desired properties such as sparsity. We apply this result to regularized dynamics predictor and observer design, and as concrete examples, we consider Hamiltonian systems, Lagrangian systems, and recurrent neural networks. We subsequently develop a variational formalism based on the Bregman Lagrangian. We show that its Euler Lagrange equations lead to natural gradient and mirror descent-like adaptation laws with momentum, and we recover their first-order analogues in the infinite friction limit. We illustrate our analyses with simulations demonstrating our theoretical results.},
  archive      = {J_NECO},
  author       = {Boffi, Nicholas M. and Slotine, Jean-Jacques E.},
  doi          = {10.1162/neco_a_01360},
  journal      = {Neural Computation},
  number       = {3},
  pages        = {590-673},
  shortjournal = {Neural Comput.},
  title        = {Implicit regularization and momentum algorithms in nonlinearly parameterized adaptive control and prediction},
  volume       = {33},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). From biophysical to integrate-and-fire modeling.
<em>NECO</em>, <em>33</em>(3), 563–589. (<a
href="https://doi.org/10.1162/neco_a_01353">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a methodology to extract a low-dimensional integrate-and-fire model from an arbitrarily detailed single-compartment biophysical model. The method aims at relating the modulation of maximal conductance parameters in the biophysical model to the modulation of parameters in the proposed integrate-and-fire model. The approach is illustrated on two well-documented examples of cellular neuromodulation: the transition between type I and type II excitability and the transition between spiking and bursting.},
  archive      = {J_NECO},
  author       = {Van Pottelbergh, Tomas and Drion, Guillaume and Sepulchre, Rodolphe},
  doi          = {10.1162/neco_a_01353},
  journal      = {Neural Computation},
  number       = {3},
  pages        = {563-589},
  shortjournal = {Neural Comput.},
  title        = {From biophysical to integrate-and-fire modeling},
  volume       = {33},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Stability conditions of bicomplex-valued hopfield neural
networks. <em>NECO</em>, <em>33</em>(2), 552–562. (<a
href="https://doi.org/10.1162/neco_a_01350">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hopfield neural networks have been extended using hypercomplex numbers. The algebra of bicomplex numbers, also referred to as commutative quaternions, is a number system of dimension 4. Since the multiplication is commutative, many notions and theories of linear algebra, such as determinant, are available, unlike quaternions. A bicomplex-valued Hopfield neural network (BHNN) has been proposed as a multistate neural associative memory. However, the stability conditions have been insufficient for the projection rule. In this work, the stability conditions are extended and applied to improvement of the projection rule. The computer simulations suggest improved noise tolerance.},
  archive      = {J_NECO},
  author       = {Kobayashi, Masaki},
  doi          = {10.1162/neco_a_01350},
  journal      = {Neural Computation},
  number       = {2},
  pages        = {552-562},
  shortjournal = {Neural Comput.},
  title        = {Stability conditions of bicomplex-valued hopfield neural networks},
  volume       = {33},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A novel neural model with lateral interaction for learning
tasks. <em>NECO</em>, <em>33</em>(2), 528–551. (<a
href="https://doi.org/10.1162/neco_a_01345">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel neural model with lateral interaction for learning tasks. The model consists of two functional fields: an elementary field to extract features and a high-level field to store and recognize patterns. Each field is composed of some neurons with lateral interaction, and the neurons in different fields are connected by the rules of synaptic plasticity. The model is established on the current research of cognition and neuroscience, making it more transparent and biologically explainable. Our proposed model is applied to data classification and clustering. The corresponding algorithms share similar processes without requiring any parameter tuning and optimization processes. Numerical experiments validate that the proposed model is feasible in different learning tasks and superior to some state-of-the-art methods, especially in small sample learning, one-shot learning, and clustering.},
  archive      = {J_NECO},
  author       = {Jin, Dequan and Qin, Ziyan and Yang, Murong and Chen, Penghe},
  doi          = {10.1162/neco_a_01345},
  journal      = {Neural Computation},
  number       = {2},
  pages        = {528-551},
  shortjournal = {Neural Comput.},
  title        = {A novel neural model with lateral interaction for learning tasks},
  volume       = {33},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Enhanced equivalence projective simulation: A framework for
modeling formation of stimulus equivalence classes. <em>NECO</em>,
<em>33</em>(2), 483–527. (<a
href="https://doi.org/10.1162/neco_a_01346">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Formation of stimulus equivalence classes has been recently modeled through equivalence projective simulation (EPS), a modified version of a projective simulation (PS) learning agent. PS is endowed with an episodic memory that resembles the internal representation in the brain and the concept of cognitive maps. PS flexibility and interpretability enable the EPS model and, consequently the model we explore in this letter, to simulate a broad range of behaviors in matching-to-sample experiments. The episodic memory, the basis for agent decision making, is formed during the training phase. Derived relations in the EPS model that are not trained directly but can be established via the network&#39;s connections are computed on demand during the test phase trials by likelihood reasoning. In this letter, we investigate the formation of derived relations in the EPS model using network enhancement (NE), an iterative diffusion process, that yields an offline approach to the agent decision making at the testing phase. The NE process is applied after the training phase to denoise the memory network so that derived relations are formed in the memory network and retrieved during the testing phase. During the NE phase, indirect relations are enhanced, and the structure of episodic memory changes. This approach can also be interpreted as the agent&#39;s replay after the training phase, which is in line with recent findings in behavioral and neuroscience studies. In comparison with EPS, our model is able to model the formation of derived relations and other features such as the nodal effect in a more intrinsic manner. Decision making in the test phase is not an ad hoc computational method, but rather a retrieval and update process of the cached relations from the memory network based on the test trial. In order to study the role of parameters on agent performance, the proposed model is simulated and the results discussed through various experimental settings.},
  archive      = {J_NECO},
  author       = {Mofrad, Asieh Abolpou and Yazidi, Anis and Mofrad, Samaneh Abolpour and Hammer, Hugo L. and Arntzen, Erik},
  doi          = {10.1162/neco_a_01346},
  journal      = {Neural Computation},
  number       = {2},
  pages        = {483-527},
  shortjournal = {Neural Comput.},
  title        = {Enhanced equivalence projective simulation: A framework for modeling formation of stimulus equivalence classes},
  volume       = {33},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Whence the expected free energy? <em>NECO</em>,
<em>33</em>(2), 447–482. (<a
href="https://doi.org/10.1162/neco_a_01354">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The expected free energy (EFE) is a central quantity in the theory of active inference. It is the quantity that all active inference agents are mandated to minimize through action, and its decomposition into extrinsic and intrinsic value terms is key to the balance of exploration and exploitation that active inference agents evince. Despite its importance, the mathematical origins of this quantity and its relation to the variational free energy (VFE) remain unclear. In this letter, we investigate the origins of the EFE in detail and show that it is not simply ”the free energy in the future.” We present a functional that we argue is the natural extension of the VFE but actively discourages exploratory behavior, thus demonstrating that exploration does not directly follow from free energy minimization into the future. We then develop a novel objective, the free energy of the expected future (FEEF), which possesses both the epistemic component of the EFE and an intuitive mathematical grounding as the divergence between predicted and desired futures.},
  archive      = {J_NECO},
  author       = {Millidge, Beren and Tschantz, Alexander and Buckley, Christopher L.},
  doi          = {10.1162/neco_a_01354},
  journal      = {Neural Computation},
  number       = {2},
  pages        = {447-482},
  shortjournal = {Neural Comput.},
  title        = {Whence the expected free energy?},
  volume       = {33},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deeply felt affect: The emergence of valence in deep active
inference. <em>NECO</em>, <em>33</em>(2), 398–446. (<a
href="https://doi.org/10.1162/neco_a_01341">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The positive-negative axis of emotional valence has long been recognized as fundamental to adaptive behavior, but its origin and underlying function have largely eluded formal theorizing and computational modeling. Using deep active inference, a hierarchical inference scheme that rests on inverting a model of how sensory data are generated, we develop a principled Bayesian model of emotional valence. This formulation asserts that agents infer their valence state based on the expected precision of their action model—an internal estimate of overall model fitness (“subjective fitness”). This index of subjective fitness can be estimated within any environment and exploits the domain generality of second-order beliefs (beliefs about beliefs). We show how maintaining internal valence representations allows the ensuing affective agent to optimize confidence in action selection preemptively. Valence representations can in turn be optimized by leveraging the (Bayes-optimal) updating term for subjective fitness, which we label affective charge (AC). AC tracks changes in fitness estimates and lends a sign to otherwise unsigned divergences between predictions and outcomes. We simulate the resulting affective inference by subjecting an in silico affective agent to a T-maze paradigm requiring context learning, followed by context reversal. This formulation of affective inference offers a principled account of the link between affect, (mental) action, and implicit metacognition. It characterizes how a deep biological system can infer its affective state and reduce uncertainty about such inferences through internal action (i.e., top-down modulation of priors that underwrite confidence). Thus, we demonstrate the potential of active inference to provide a formal and computationally tractable account of affect. Our demonstration of the face validity and potential utility of this formulation represents the first step within a larger research program. Next, this model can be leveraged to test the hypothesized role of valence by fitting the model to behavioral and neuronal responses.},
  archive      = {J_NECO},
  author       = {Hesp, Casper and Smith, Ryan and Parr, Thomas and Allen, Micah and Friston, Karl J. and Ramstead, Maxwell J. D.},
  doi          = {10.1162/neco_a_01341},
  journal      = {Neural Computation},
  number       = {2},
  pages        = {398-446},
  shortjournal = {Neural Comput.},
  title        = {Deeply felt affect: The emergence of valence in deep active inference},
  volume       = {33},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Predicting the ease of human category learning using radial
basis function networks. <em>NECO</em>, <em>33</em>(2), 376–397. (<a
href="https://doi.org/10.1162/neco_a_01349">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Our goal is to understand and optimize human concept learning by predicting the ease of learning of a particular exemplar or category. We propose a method for estimating ease values , quantitative measures of ease of learning, as an alternative to conducting costly empirical training studies. Our method combines a psychological embedding of domain exemplars with a pragmatic categorization model. The two components are integrated using a radial basis function network (RBFN) that predicts ease values. The free parameters of the RBFN are fit using human similarity judgments, circumventing the need to collect human training data to fit more complex models of human categorization. We conduct two category-training experiments to validate predictions of the RBFN. We demonstrate that an instance-based RBFN outperforms both a prototype-based RBFN and an empirical approach using the raw data. Although the human data were collected across diverse experimental conditions, the predicted ease values strongly correlate with human learning performance. Training can be sequenced by (predicted) ease, achieving what is known as fading in the psychology literature and curriculum learning in the machine-learning literature, both of which have been shown to facilitate learning.},
  archive      = {J_NECO},
  author       = {Roads, Brett D. and Mozer, Michael C.},
  doi          = {10.1162/neco_a_01349},
  journal      = {Neural Computation},
  number       = {2},
  pages        = {376-397},
  shortjournal = {Neural Comput.},
  title        = {Predicting the ease of human category learning using radial basis function networks},
  volume       = {33},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Enhanced signal detection by adaptive decorrelation of
interspike intervals. <em>NECO</em>, <em>33</em>(2), 341–375. (<a
href="https://doi.org/10.1162/neco_a_01347">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spike trains with negative interspike interval (ISI) correlations, in which long/short ISIs are more likely followed by short/long ISIs, are common in many neurons. They can be described by stochastic models with a spike-triggered adaptation variable. We analyze a phenomenon in these models where such statistically dependent ISI sequences arise in tandem with quasi-statistically independent and identically distributed (quasi-IID) adaptation variable sequences. The sequences of adaptation states and resulting ISIs are linked by a nonlinear decorrelating transformation. We establish general conditions on a family of stochastic spiking models that guarantee this quasi-IID property and establish bounds on the resulting baseline ISI correlations. Inputs that elicit weak firing rate changes in samples with many spikes are known to be more detectible when negative ISI correlations are present because they reduce spike count variance; this defines a variance-reduced firing rate coding benchmark. We performed a Fisher information analysis on these adapting models exhibiting ISI correlations to show that a spike pattern code based on the quasi-IID property achieves the upper bound of detection performance, surpassing rate codes with the same mean rate—including the variance-reduced rate code benchmark—by 20\% to 30\%. The information loss in rate codes arises because the benefits of reduced spike count variance cannot compensate for the lower firing rate gain due to adaptation. Since adaptation states have similar dynamics to synaptic responses, the quasi-IID decorrelation transformation of the spike train is plausibly implemented by downstream neurons through matched postsynaptic kinetics. This provides an explanation for observed coding performance in sensory systems that cannot be accounted for by rate coding, for example, at the detection threshold where rate changes can be insignificant.},
  archive      = {J_NECO},
  author       = {Nesse, William H. and Maler, Leonard and Longtin, André},
  doi          = {10.1162/neco_a_01347},
  journal      = {Neural Computation},
  number       = {2},
  pages        = {341-375},
  shortjournal = {Neural Comput.},
  title        = {Enhanced signal detection by adaptive decorrelation of interspike intervals},
  volume       = {33},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning in volatile environments with the bayes factor
surprise. <em>NECO</em>, <em>33</em>(2), 269–340. (<a
href="https://doi.org/10.1162/neco_a_01352">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Surprise-based learning allows agents to rapidly adapt to nonstationary stochastic environments characterized by sudden changes. We show that exact Bayesian inference in a hierarchical model gives rise to a surprise-modulated trade-off between forgetting old observations and integrating them with the new ones. The modulation depends on a probability ratio, which we call the Bayes Factor Surprise, that tests the prior belief against the current belief. We demonstrate that in several existing approximate algorithms, the Bayes Factor Surprise modulates the rate of adaptation to new observations. We derive three novel surprise-based algorithms, one in the family of particle filters, one in the family of variational learning, and one in the family of message passing, that have constant scaling in observation sequence length and particularly simple update dynamics for any distribution in the exponential family. Empirical results show that these surprise-based algorithms estimate parameters better than alternative approximate approaches and reach levels of performance comparable to computationally more expensive algorithms. The Bayes Factor Surprise is related to but different from the Shannon Surprise. In two hypothetical experiments, we make testable predictions for physiological indicators that dissociate the Bayes Factor Surprise from the Shannon Surprise. The theoretical insight of casting various approaches as surprise-based learning, as well as the proposed online algorithms, may be applied to the analysis of animal and human behavior and to reinforcement learning in nonstationary environments.},
  archive      = {J_NECO},
  author       = {Liakoni, Vasiliki and Modirshanechi, Alireza and Gerstner, Wulfram and Brea, Johanni},
  doi          = {10.1162/neco_a_01352},
  journal      = {Neural Computation},
  number       = {2},
  pages        = {269-340},
  shortjournal = {Neural Comput.},
  title        = {Learning in volatile environments with the bayes factor surprise},
  volume       = {33},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Information-theoretic representation learning for
positive-unlabeled classification. <em>NECO</em>, <em>33</em>(1),
244–268. (<a href="https://doi.org/10.1162/neco_a_01337">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in weakly supervised classification allow us to train a classifier from only positive and unlabeled (PU) data. However, existing PU classification methods typically require an accurate estimate of the class-prior probability, a critical bottleneck particularly for high-dimensional data. This problem has been commonly addressed by applying principal component analysis in advance, but such unsupervised dimension reduction can collapse the underlying class structure. In this letter, we propose a novel representation learning method from PU data based on the information-maximization principle. Our method does not require class-prior estimation and thus can be used as a preprocessing method for PU classification. Through experiments, we demonstrate that our method, combined with deep neural networks, highly improves the accuracy of PU class-prior estimation, leading to state-of-the-art PU classification performance.},
  archive      = {J_NECO},
  author       = {Sakai, Tomoya and Niu, Gang and Sugiyama, Masashi},
  doi          = {10.1162/neco_a_01337},
  journal      = {Neural Computation},
  number       = {1},
  pages        = {244-268},
  shortjournal = {Neural Comput.},
  title        = {Information-theoretic representation learning for positive-unlabeled classification},
  volume       = {33},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust stability analysis of delayed stochastic neural
networks via wirtinger-based integral inequality. <em>NECO</em>,
<em>33</em>(1), 227–243. (<a
href="https://doi.org/10.1162/neco_a_01344">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We discuss stability analysis for uncertain stochastic neural networks (SNNs) with time delay in this letter. By constructing a suitable Lyapunov-Krasovskii functional (LKF) and utilizing Wirtinger inequalities for estimating the integral inequalities, the delay-dependent stochastic stability conditions are derived in terms of linear matrix inequalities (LMIs). We discuss the parameter uncertainties in terms of norm-bounded conditions in the given interval with constant delay. The derived conditions ensure that the global, asymptotic stability of the states for the proposed SNNs. We verify the effectiveness and applicability of the proposed criteria with numerical examples.},
  archive      = {J_NECO},
  author       = {Suresh, R. and Manivannan, A.},
  doi          = {10.1162/neco_a_01344},
  journal      = {Neural Computation},
  number       = {1},
  pages        = {227-243},
  shortjournal = {Neural Comput.},
  title        = {Robust stability analysis of delayed stochastic neural networks via wirtinger-based integral inequality},
  volume       = {33},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An EM algorithm for capsule regression. <em>NECO</em>,
<em>33</em>(1), 194–226. (<a
href="https://doi.org/10.1162/neco_a_01336">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate a latent variable model for multinomial classification inspired by recent capsule architectures for visual object recognition (Sabour, Frosst, &amp; Hinton, 2017 ). Capsule architectures use vectors of hidden unit activities to encode the pose of visual objects in an image, and they use the lengths of these vectors to encode the probabilities that objects are present. Probabilities from different capsules can also be propagated through deep multilayer networks to model the part-whole relationships of more complex objects. Notwithstanding the promise of these networks, there still remains much to understand about capsules as primitive computing elements in their own right. In this letter, we study the problem of capsule regression—a higher-dimensional analog of logistic, probit, and softmax regression in which class probabilities are derived from vectors of competing magnitude. To start, we propose a simple capsule architecture for multinomial classification: the architecture has one capsule per class, and each capsule uses a weight matrix to compute the vector of hidden unit activities for patterns it seeks to recognize. Next, we show how to model these hidden unit activities as latent variables, and we use a squashing nonlinearity to convert their magnitudes as vectors into normalized probabilities for multinomial classification. When different capsules compete to recognize the same pattern, the squashing nonlinearity induces nongaussian terms in the posterior distribution over their latent variables. Nevertheless, we show that exact inference remains tractable and use an expectation-maximization procedure to derive least-squares updates for each capsule&#39;s weight matrix. We also present experimental results to demonstrate how these ideas work in practice.},
  archive      = {J_NECO},
  author       = {Saul, Lawrence K.},
  doi          = {10.1162/neco_a_01336},
  journal      = {Neural Computation},
  number       = {1},
  pages        = {194-226},
  shortjournal = {Neural Comput.},
  title        = {An EM algorithm for capsule regression},
  volume       = {33},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Associated learning: Decomposing end-to-end backpropagation
based on autoencoders and target propagation. <em>NECO</em>,
<em>33</em>(1), 174–193. (<a
href="https://doi.org/10.1162/neco_a_01335">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Backpropagation (BP) is the cornerstone of today&#39;s deep learning algorithms, but it is inefficient partially because of backward locking, which means updating the weights of one layer locks the weight updates in the other layers. Consequently, it is challenging to apply parallel computing or a pipeline structure to update the weights in different layers simultaneously. In this letter, we introduce a novel learning structure, associated learning (AL), that modularizes the network into smaller components, each of which has a local objective. Because the objectives are mutually independent, AL can learn the parameters in different layers independently and simultaneously, so it is feasible to apply a pipeline structure to improve the training throughput. Specifically, this pipeline structure improves the complexity of the training time from O ( n ℓ ) ⁠ , which is the time complexity when using BP and stochastic gradient descent (SGD) for training, to O ( n + ℓ ) ⁠ , where n is the number of training instances and ℓ is the number of hidden layers. Surprisingly, even though most of the parameters in AL do not directly interact with the target variable, training deep models by this method yields accuracies comparable to those from models trained using typical BP methods, in which all parameters are used to predict the target variable. Consequently, because of the scalability and the predictive power demonstrated in the experiments, AL deserves further study to determine the better hyperparameter settings, such as activation function selection, learning rate scheduling, and weight initialization, to accumulate experience, as we have done over the years with the typical BP method. In addition, perhaps our design can also inspire new network designs for deep learning. Our implementation is available at https://github.com/SamYWK/Associated_Learning .},
  archive      = {J_NECO},
  author       = {Kao, Yu-Wei and Chen, Hung-Hsuan},
  doi          = {10.1162/neco_a_01335},
  journal      = {Neural Computation},
  number       = {1},
  pages        = {174-193},
  shortjournal = {Neural Comput.},
  title        = {Associated learning: Decomposing end-to-end backpropagation based on autoencoders and target propagation},
  volume       = {33},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). New insights into learning with correntropy-based
regression. <em>NECO</em>, <em>33</em>(1), 157–173. (<a
href="https://doi.org/10.1162/neco_a_01334">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stemming from information-theoretic learning, the correntropy criterion and its applications to machine learning tasks have been extensively studied and explored. Its application to regression problems leads to the robustness-enhanced regression paradigm: correntropy-based regression. Having drawn a great variety of successful real-world applications, its theoretical properties have also been investigated recently in a series of studies from a statistical learning viewpoint. The resulting big picture is that correntropy-based regression regresses toward the conditional mode function or the conditional mean function robustly under certain conditions. Continuing this trend and going further, in this study, we report some new insights into this problem. First, we show that under the additive noise regression model, such a regression paradigm can be deduced from minimum distance estimation, implying that the resulting estimator is essentially a minimum distance estimator and thus possesses robustness properties. Second, we show that the regression paradigm in fact provides a unified approach to regression problems in that it approaches the conditional mean, the conditional mode, and the conditional median functions under certain conditions. Third, we present some new results when it is used to learn the conditional mean function by developing its error bounds and exponential convergence rates under conditional ( ⁠ 1 + ε ⁠ )-moment assumptions. The saturation effect on the established convergence rates, which was observed under ( ⁠ 1 + ε ⁠ )-moment assumptions, still occurs, indicating the inherent bias of the regression estimator. These novel insights deepen our understanding of correntropy-based regression, help cement the theoretic correntropy framework, and enable us to investigate learning schemes induced by general bounded nonconvex loss functions.},
  archive      = {J_NECO},
  author       = {Feng, Yunlong},
  doi          = {10.1162/neco_a_01334},
  journal      = {Neural Computation},
  number       = {1},
  pages        = {157-173},
  shortjournal = {Neural Comput.},
  title        = {New insights into learning with correntropy-based regression},
  volume       = {33},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Efficient actor-critic reinforcement learning with
embodiment of muscle tone for posture stabilization of the human arm.
<em>NECO</em>, <em>33</em>(1), 129–156. (<a
href="https://doi.org/10.1162/neco_a_01333">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This letter proposes a new idea to improve learning efficiency in reinforcement learning (RL) with the actor-critic method used as a muscle controller for posture stabilization of the human arm. Actor-critic RL (ACRL) is used for simulations to realize posture controls in humans or robots using muscle tension control. However, it requires very high computational costs to acquire a better muscle control policy for desirable postures. For efficient ACRL, we focused on embodiment that is supposed to potentially achieve efficient controls in research fields of artificial intelligence or robotics. According to the neurophysiology of motion control obtained from experimental studies using animals or humans, the pedunculopontine tegmental nucleus (PPTn) induces muscle tone suppression, and the midbrain locomotor region (MLR) induces muscle tone promotion. PPTn and MLR modulate the activation levels of mutually antagonizing muscles such as flexors and extensors in a process through which control signals are translated from the substantia nigra reticulata to the brain stem. Therefore, we hypothesized that the PPTn and MLR could control muscle tone, that is, the maximum values of activation levels of mutually antagonizing muscles using different sigmoidal functions for each muscle; then we introduced antagonism function models (AFMs) of PPTn and MLR for individual muscles, incorporating the hypothesis into the process to determine the activation level of each muscle based on the output of the actor in ACRL. ACRL with AFMs representing the embodiment of muscle tone successfully achieved posture stabilization in five joint motions of the right arm of a human adult male under gravity in predetermined target angles at an earlier period of learning than the learning methods without AFMs. The results obtained from this study suggest that the introduction of embodiment of muscle tone can enhance learning efficiency in posture stabilization disorders of humans or humanoid robots.},
  archive      = {J_NECO},
  author       = {Iwamoto, Masami and Kato, Daichi},
  doi          = {10.1162/neco_a_01333},
  journal      = {Neural Computation},
  number       = {1},
  pages        = {129-156},
  shortjournal = {Neural Comput.},
  title        = {Efficient actor-critic reinforcement learning with embodiment of muscle tone for posture stabilization of the human arm},
  volume       = {33},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Passive nonlinear dendritic interactions as a computational
resource in spiking neural networks. <em>NECO</em>, <em>33</em>(1),
96–128. (<a href="https://doi.org/10.1162/neco_a_01338">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nonlinear interactions in the dendritic tree play a key role in neural computation. Nevertheless, modeling frameworks aimed at the construction of large-scale, functional spiking neural networks, such as the Neural Engineering Framework, tend to assume a linear superposition of postsynaptic currents. In this letter, we present a series of extensions to the Neural Engineering Framework that facilitate the construction of networks incorporating Dale&#39;s principle and nonlinear conductance-based synapses. We apply these extensions to a two-compartment LIF neuron that can be seen as a simple model of passive dendritic computation. We show that it is possible to incorporate neuron models with input-dependent nonlinearities into the Neural Engineering Framework without compromising high-level function and that nonlinear postsynaptic currents can be systematically exploited to compute a wide variety of multivariate, band-limited functions, including the Euclidean norm, controlled shunting, and nonnegative multiplication. By avoiding an additional source of spike noise, the function approximation accuracy of a single layer of two-compartment LIF neurons is on a par with or even surpasses that of two-layer spiking neural networks up to a certain target function bandwidth.},
  archive      = {J_NECO},
  author       = {Stöckel, Andreas and Eliasmith, Chris},
  doi          = {10.1162/neco_a_01338},
  journal      = {Neural Computation},
  number       = {1},
  pages        = {96-128},
  shortjournal = {Neural Comput.},
  title        = {Passive nonlinear dendritic interactions as a computational resource in spiking neural networks},
  volume       = {33},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). NMDA receptor alterations after mild traumatic brain injury
induce deficits in memory acquisition and recall. <em>NECO</em>,
<em>33</em>(1), 67–95. (<a
href="https://doi.org/10.1162/neco_a_01343">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mild traumatic brain injury (mTBI) presents a significant health concern with potential persisting deficits that can last decades. Although a growing body of literature improves our understanding of the brain network response and corresponding underlying cellular alterations after injury, the effects of cellular disruptions on local circuitry after mTBI are poorly understood. Our group recently reported how mTBI in neuronal networks affects the functional wiring of neural circuits and how neuronal inactivation influences the synchrony of coupled microcircuits. Here, we utilized a computational neural network model to investigate the circuit-level effects of N-methyl D-aspartate receptor dysfunction. The initial increase in activity in injured neurons spreads to downstream neurons, but this increase was partially reduced by restructuring the network with spike-timing-dependent plasticity. As a model of network-based learning, we also investigated how injury alters pattern acquisition, recall, and maintenance of a conditioned response to stimulus. Although pattern acquisition and maintenance were impaired in injured networks, the greatest deficits arose in recall of previously trained patterns. These results demonstrate how one specific mechanism of cellular-level damage in mTBI affects the overall function of a neural network and point to the importance of reversing cellular-level changes to recover important properties of learning and memory in a microcircuit.},
  archive      = {J_NECO},
  author       = {Gabrieli, David and Schumm, Samantha N. and Vigilante, Nicholas F. and Meaney, David F.},
  doi          = {10.1162/neco_a_01343},
  journal      = {Neural Computation},
  number       = {1},
  pages        = {67-95},
  shortjournal = {Neural Comput.},
  title        = {NMDA receptor alterations after mild traumatic brain injury induce deficits in memory acquisition and recall},
  volume       = {33},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Conductance-based adaptive exponential integrate-and-fire
model. <em>NECO</em>, <em>33</em>(1), 41–66. (<a
href="https://doi.org/10.1162/neco_a_01342">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The intrinsic electrophysiological properties of single neurons can be described by a broad spectrum of models, from realistic Hodgkin-Huxley-type models with numerous detailed mechanisms to the phenomenological models. The adaptive exponential integrate-and-fire (AdEx) model has emerged as a convenient middle-ground model. With a low computational cost but keeping biophysical interpretation of the parameters, it has been extensively used for simulations of large neural networks. However, because of its current-based adaptation, it can generate unrealistic behaviors. We show the limitations of the AdEx model, and to avoid them, we introduce the conductance-based adaptive exponential integrate-and-fire model (CAdEx). We give an analysis of the dynamics of the CAdEx model and show the variety of firing patterns it can produce. We propose the CAdEx model as a richer alternative to perform network simulations with simplified models reproducing neuronal intrinsic properties.},
  archive      = {J_NECO},
  author       = {Górski, Tomasz and Depannemaecker, Damien and Destexhe, Alain},
  doi          = {10.1162/neco_a_01342},
  journal      = {Neural Computation},
  number       = {1},
  pages        = {41-66},
  shortjournal = {Neural Comput.},
  title        = {Conductance-based adaptive exponential integrate-and-fire model},
  volume       = {33},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Flexible working memory through selective gating and
attentional tagging. <em>NECO</em>, <em>33</em>(1), 1–40. (<a
href="https://doi.org/10.1162/neco_a_01339">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Working memory is essential: it serves to guide intelligent behavior of humans and nonhuman primates when task-relevant stimuli are no longer present to the senses. Moreover, complex tasks often require that multiple working memory representations can be flexibly and independently maintained, prioritized, and updated according to changing task demands. Thus far, neural network models of working memory have been unable to offer an integrative account of how such control mechanisms can be acquired in a biologically plausible manner. Here, we present WorkMATe, a neural network architecture that models cognitive control over working memory content and learns the appropriate control operations needed to solve complex working memory tasks. Key components of the model include a gated memory circuit that is controlled by internal actions, encoding sensory information through untrained connections, and a neural circuit that matches sensory inputs to memory content. The network is trained by means of a biologically plausible reinforcement learning rule that relies on attentional feedback and reward prediction errors to guide synaptic updates. We demonstrate that the model successfully acquires policies to solve classical working memory tasks, such as delayed recognition and delayed pro-saccade/anti-saccade tasks. In addition, the model solves much more complex tasks, including the hierarchical 12-AX task or the ABAB ordered recognition task, both of which demand an agent to independently store and updated multiple items separately in memory. Furthermore, the control strategies that the model acquires for these tasks subsequently generalize to new task contexts with novel stimuli, thus bringing symbolic production rule qualities to a neural network architecture. As such, WorkMATe provides a new solution for the neural implementation of flexible memory control.},
  archive      = {J_NECO},
  author       = {Kruijne, Wouter and Bohte, Sander M. and Roelfsema, Pieter R. and Olivers, Christian N. L.},
  doi          = {10.1162/neco_a_01339},
  journal      = {Neural Computation},
  number       = {1},
  pages        = {1-40},
  shortjournal = {Neural Comput.},
  title        = {Flexible working memory through selective gating and attentional tagging},
  volume       = {33},
  year         = {2021},
}
</textarea>
</details></li>
</ul>

</body>
</html>
