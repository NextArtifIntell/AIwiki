<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>DINT_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="dint---47">DINT - 47</h2>
<ul>
<li><details>
<summary>
(2021). Contents index to volume 3. <em>DINT</em>, <em>3</em>(4),
i–vi. (<a href="https://doi.org/10.1162/dint_x_00112">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {EditorialEditors&#39; NotePeter Wittenburg &amp;amp; George Strawn 2021,3(1):1–4doi:10.1162/dint_e_00068Research PaperOpenKG Chain: A Blockchain Infrastructure for Open Knowledge GraphsHuajun Chen, Ning Hu, Guilin Qi, Haofen Wang, Zhen Bi, Jie Li &amp;amp; Fan Yang 2021,3(2):205–227doi:10.1162/dint_a_00095Probabilistic Tractable Models in Mixed Discrete-continuous DomainsAndreas Bueff, Stefanie Speichert &amp;amp; Vaishak Belle 2021,3(2):228–260doi:10.1162/dint_a_00064Transdisciplinary Convergence: Intelligent Infrastructure for Sustainable DevelopmentYi Shen 2021,3(2):261–273doi:10.1162/dint_a_00063Deep Learning with Heterogeneous Graph Embeddings for Mortality Prediction from Electronic Health RecordsTingyi Wanyan, Hossein Honarvar, Ariful Azad, Ying Ding &amp;amp; Benjamin S. Glicksberg 2021,3(3):329–339doi:10.1162/dint_a_00097Integrated “Generate, Make, and Test” for Formulated Products Using Knowledge GraphsSagar Sunkle, Deepak Jain, Krati Saxena, Ashwini Patil, Tushita Singh, Beena Rai &amp;amp; Vinay Kulkarni 2021,3(3):340–375doi:10.1162/dint_a_00096A Knowledge Graph Based Approach to Social Science SurveysJeff Z. Pan, Elspeth Edelstein, Patrik Bansky &amp;amp; Adam Wyner 2021,3(4): 477–506doi:10.1162/dint_a_00107Exploring the Current Practices, Costs and Benefits of FAIR Implementation in Pharmaceutical Research and Development: A Qualitative Interview StudyEbtisam Alharbi, Rigina Skeva, Nick Juty, Caroline Jay &amp;amp; Carole Goble 2021,3(4): 507–527doi:10.1162/dint_a_00109Data PaperAn Evaluation of Chinese Human-computer Dialogue TechnologyZixian Feng, Caihai Zhu, Weinan Zhang, Zhigang Chen, Wanxiang Che, Minlie Huang &amp;amp; Linlin Li 2021,3(2):274–286doi:10.1162/dint_a_00090Overview of SMP-CAIL2020-Argmine: The Interactive Argument-pair Extraction in Judgement Document ChallengeJian Yuan, Zhongyu Wei, Yixu Gao, Wei Chen, Yun Song, Donghua Zhao, Jinglei Ma, Zhen Hu, Shaokun Zou, Donghai Li &amp;amp; Xuanjing Huang 2021,3(2):287–307doi:10.1162/dint_a_00094A Data Collection on Secondary School Students&#39; STEM Performance and Reading Practices in an Emerging CountryQuan-Hoang Vuong, Viet-Phuong La, Manh-Toan Ho, Thanh-Hang Pham, Thu-Trang Vuong, Ha-My Vuong &amp;amp; Minh-Hoang Nguyen 2021,3(2): 308–328doi:10.1162/dint_a_00091Overview of CCKS 2020 Task 3: Named Entity Recognition and Event Extraction in Chinese Electronic Medical RecordsXia Li, Qinghua Wen, Hu Lin, Zengtao Jiao &amp;amp; Jiangtao Zhang 2021,3(3):376–388doi:10.1162/dint_a_00093Semi-supervised Noisy Label Learning for Chinese Clinical Named Entity RecognitionZhucong Li, Zhen Gan, Baoli Zhang, Yubo Chen, Jing Wan, Kang Liu, Jun Zhao &amp;amp; Shengping Liu 2021,3(3): 389–401doi:10.1162/dint_a_00099Medical Named Entity Recognition from Un-labelled Medical Records Based on Pre-trained Language Models and Domain DictionaryChaojie Wen, Tao Chen, Xudong Jia &amp;amp; Jiang Zhu 2021,3(3): 402–417doi:10.1162/dint_a_00105Data Set and Evaluation of Automated Construction of Financial Knowledge GraphWenguang Wang, Yonglin Xu, Chunhui Du, Yunwen Chen, Yijie Wang &amp;amp; Hui Wen 2021,3(3): 418–443doi:10.1162/dint_a_00108A Joint Learning Framework for the CCKS-2020 Financial Event Extraction TaskJiawei Sheng, Qian Li, Yiming Hei, Shu Guo, Bowen Yu, Lihong Wang, Min He, Tingwen Liu &amp;amp; Hongbo Xu 2021,3(3): 444–459doi:10.1162/dint_a_00098A Prior Information Enhanced Extraction Framework for Document-level Financial Event ExtractionHaitao Wang, Tong Zhu, Mingtao Wang, Guoliang Zhang &amp;amp; Wenliang Chen 2021,3(3): 460–476doi:10.1162/dint_a_00103DAMS: A Distributed Analytics Metadata SchemaSascha Welten, Laurenz Neumann, Yeliz Ucer Yediel, Luiz Olavo Bonino da Silva Santos, Stefan Decker &amp;amp; Oya Beyan 2021,3(4): 528–547doi:10.1162/dint_a_00100AOL4PS: A Large-scale Data Set for Personalized SearchQian Guo, Wei Chen &amp;amp; Huaiyu Wan 2021,3(4): 548–567doi:10.1162/dint_a_00104Few-shot Learning for Named Entity Recognition Based on BERT and Two-level Model FusionYuan Gong, Lu Mao &amp;amp; Changliang Li 2021,3(4): 568–577doi:10.1162/dint_a_00102Multifaceted Interactions between Urban Humans and Biodiversity-related Concepts: A Developing-countryData SetMinh-Hoang Nguyen 2021,3(4): 578–605doi:10.1162/dint_a_00110A Multinational Data Set of Game Players&#39; Behaviors in a Virtual World and Environmental PerceptionsQuan-Hoang Vuong, Manh-Toan Ho, Viet-Phuong La, Tam-Tri Le, Thanh Huyen T. Nguyen &amp;amp; Minh-Hoang Nguyen 2021,3(4):606–630doi:10.1162/dint_a_00111CommentaryPolitics and Open Science: How the European Open Science Cloud Became Reality (the Untold Story)Jean-Claude Burgelman 2021,3(1):5–19doi:10.1162/dint_a_00069Comments to Jean-Claude Burgelman&#39;s Article Politics and Open Science: How the European Open Science Cloud Became Reality (the Untold Story)—The Twin Challenge of the Hard-core Change and the Cultural Shift. The Role of the Chorus in the Greek dramaEdit Herczog 2021,3(1):20–23doi:10.1162/dint_a_00070Comments to Jean-Claude Burgelman&#39;s Article Politics and Open Science:How the European Open Science Cloud Became Reality (the Untold Story) Hanifeh Khayyeri 2021,3(1):24–25doi:10.1162/dint_a_00071Comments to Jean-Claude Burgelman&#39;s Article Politics and Open Science:How the European Open Science Cloud Became Reality (the Untold Story) Dimitris Koureas 2021,3(1):26–28doi:10.1162/dint_a_00072Comments to Jean-Claude Burgelman&#39;s Article Politics and Open Science: How the European Open Science Cloud Became Reality (the Untold Story)Natalia Manola 2021,3(1):29–31doi:10.1162/dint_a_00073Comments to Jean-Claude Burgelman&#39;s Article Politics and Open Science: How the European Open Science Cloud Became Reality (the Untold Story)—“EOSC is a bigger ME” and the Dunning Kruger EffectBarend Mons 2021,3(1):32–39doi:10.1162/dint_a_00074Comments to Jean-Claude Burgelman&#39;s Article Politics and Open Science: How the European Open Science Cloud Became Reality (the Untold Story) Per Öster 2021,3(1):40–42doi:10.1162/dint_a_00089Comments to Jean-Claude Burgelman&#39;s Article Politics and Open Science: How the European Open Science Cloud Became Reality (the Untold Story)George Strawn 2021,3(1):43–46doi:10.1162/dint_a_00075Comments to Jean-Claude Burgelman&#39;s Article Politics and Open Science: How the European Open Science Cloud Became Reality (the Untold Story)Peter Wittenburg 2021,3(1):47–51doi:10.1162/dint_a_00076Vision PaperAbout Open Science and Autonomy of SciencePaolo Budroni 2021,3(1):52–63doi:10.1162/dint_a_00077Open Science—A Question of TrustJonathan Clark 2021,3(1):64–70doi:10.1162/dint_a_00078Building Momentum to Realign Incentives to Support Open ScienceHeather Joseph 2021,3(1):71–78doi:10.1162/dint_a_00079On the Complexities of Federating Research Data InfrastructuresAtif Latif, Fidan Limani &amp;amp; Klaus Tochtermann 2021,3(1):79–87doi:10.1162/dint_a_00080Open Science and the Hype CycleGeorge Strawn 2021,3(1):88–94doi:10.1162/dint_a_00081Open Science and Data SciencePeter Wittenburg 2021,3(1):95–105doi:10.1162/dint_a_00082Embedding Open Science in RealityJohn Wood 2021,3(1):106–115doi:10.1162/dint_a_00083Not Ready for Convergence in Data InfrastructuresKeith Jeffery, Peter Wittenburg, Larry Lannom, George Strawn, Claudia Biniossek, Dirk Betz &amp;amp; Christophe Blanchi 2021,3(1):116–135doi:10.1162/dint_a_00084Practice PaperEU-Citizen.Science: A Platform for Mainstreaming Citizen Science and Open Science in EuropeKatherin Wagenknecht, Tim Woods, Francisco García Sanz, Margaret Gold, Anne Bowser, Simone Rüfenacht, Luigi Ceccaroni &amp;amp; Jaume Piera 2021,3(1):136–149doi:10.1162/dint_a_00085Implementation of an Open Science Instruction Program for UndergraduatesSharon Hanna, Jason Pither &amp;amp; Mathew Vis-Dunbar 2021,3(1):150–161doi:10.1162/dint_a_00086Developing Open RDI and Education in Finnish Universities of Applied SciencesAnne Kärki, Seliina Päällysaho, Kaisa Jaalama, Juhani Talvela, Anttoni Lehto &amp;amp; Hannu Hyyppä 2021,3(1):162–175doi:10.1162/dint_a_00066Key Aspects of Open Data in Finnish RDI Cooperation between Higher Education and BusinessesSeliina Päällysaho, Jaana Latvanen, Anttoni Lehto, Jaakko Riihimaa, Pekka Lahti, Anne Kärki &amp;amp; Helena Puhakka-Tarvainen 2021,3(1):176–188doi:10.1162/dint_a_00065Research Data Management Implementation at Peking University Library: Foster and Promote Open Science and Open DataHua Nie, Pengcheng Luo &amp;amp; Ping Fu 2021,3(1):189–204doi:10.1162/dint_a_00088Implementation PaperImplementation of the FAIR Data Principles for Exploratory Biomarker Data from Clinical TrialsAlexander Arefolov, Laura Adam, Shoshana Brown, Yelena Budovskaya, Cong Chen, Diya Das, Chen Farhy, Rebecca Ferguson, Hongmei Huang, Kimberly Kanigel, Christina Lu, Oksana Polesskaya, Tracy Staton, Rajeev Tajhya, Maryann Whitley, Jee-Yeon Wong, Xiangpei Zeng &amp;amp; Mark McCreary 2021,3(4):631–662doi:10.1162/dint_a_00106},
  archive      = {J_DINT},
  doi          = {10.1162/dint_x_00112},
  journal      = {Data Intelligence},
  number       = {4},
  pages        = {i-vi},
  shortjournal = {Data Intell.},
  title        = {Contents index to volume 3},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Implementation of the FAIR data principles for exploratory
biomarker data from clinical trials. <em>DINT</em>, <em>3</em>(4),
631–662. (<a href="https://doi.org/10.1162/dint_a_00106">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The FAIR data guiding principles have been recently developed and widely adopted to improve the Findability, Accessibility, Interoperability, and Reuse of digital assets in the face of an exponential increase of data volume and complexity. The FAIR data principles have been formulated on a general level and the technological implementation of these principles remains up to the industries and organizations working on maximizing the value of their data. Here, we describe the data management and curation methodologies and best practices developed for FAIRification of clinical exploratory biomarker data collected from over 250 clinical studies. We discuss the data curation effort involved, the resulting output, and the business and scientific impact of our work. Finally, we propose prospective planning for FAIR data to optimize data management efforts and maximize data value.},
  archive      = {J_DINT},
  author       = {Arefolov, Alexander and Adam, Laura and Brown, Shoshana and Budovskaya, Yelena and Chen, Cong and Das, Diya and Farhy, Chen and Ferguson, Rebecca and Huang, Hongmei and Kanigel, Kimberly and Lu, Christina and Polesskaya, Oksana and Staton, Tracy and Tajhya, Rajeev and Whitley, Maryann and Wong, Jee-Yeon and Zeng, Xiangpei and McCreary, Mark},
  doi          = {10.1162/dint_a_00106},
  journal      = {Data Intelligence},
  number       = {4},
  pages        = {631-662},
  shortjournal = {Data Intell.},
  title        = {Implementation of the FAIR data principles for exploratory biomarker data from clinical trials},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A multinational data set of game players’ behaviors in a
virtual world and environmental perceptions. <em>DINT</em>,
<em>3</em>(4), 606–630. (<a
href="https://doi.org/10.1162/dint_a_00111">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video gaming has been rising rapidly to become one of the primary entertainment media, especially during the COVID-19 pandemic. Playing video games has been reported to associate with many psychological and behavioral traits. However, little is known about the connections between game players&#39; behaviors in the virtual environment and environmental perceptions. Thus, the current data set offers valuable resources regarding environmental worldviews and behaviors in the virtual world of 640 Animal Crossing: New Horizons (ACNH) game players from 29 countries around the globe. The data set consists of six major categories: 1) socio-demographic profile, 2) COVID-19 concern, 3) environmental perception, 4) game-playing habit, 5) in-game behavior, and 6) game-playing feeling. By making this data set open, we aim to provide policymakers, game producers, and researchers with valuable resources for understanding the interactions between behaviors in the virtual world and environmental perceptions, which could help produce video games in compliance with the United Nations (UN) Sustainable Development Goals.},
  archive      = {J_DINT},
  author       = {Vuong, Quan-Hoang and Ho, Manh-Toan and La, Viet-Phuong and Le, Tam-Tri and Nguyen, Thanh Huyen T. and Nguyen, Minh-Hoang},
  doi          = {10.1162/dint_a_00111},
  journal      = {Data Intelligence},
  number       = {4},
  pages        = {606-630},
  shortjournal = {Data Intell.},
  title        = {A multinational data set of game players&#39; behaviors in a virtual world and environmental perceptions},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multifaceted interactions between urban humans and
biodiversity-related concepts: A developing-country data set.
<em>DINT</em>, <em>3</em>(4), 578–605. (<a
href="https://doi.org/10.1162/dint_a_00110">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Urban humans and biodiversity-related concepts are interacting with each other in many negative and positive ways. The biodiversity provides a wide array of provision and cultural-ecological services to urban residents, but it is being overexploited to the point of crisis. The crisis is largely driven by the expanding illegal wildlife trade in developing countries with a high urbanization rate and biodiversity level like Vietnam. While supply-side measures are ineffective in reducing biodiversity loss, researchers have suggested demand-side measures as supplements, such as social marketing campaigns and law enforcement in urban areas. Moreover, urban residents are also potential visitors to urban public parks and national parks, which helps generate finance for biodiversity preservation and conservation in those places. Understanding how urban residents&#39; perceptions towards biodiversity and biodiversity-related behaviors can help improve the effectiveness of conservation efforts and sustainable urban development. Thus, this article presents a data set of 535 urban residents&#39; wildlife consumption behaviors, multifaceted perceptions and interactions with biodiversity-related concepts, and nature-based recreation demand. The data set is constructed with six major categories: 1) wildlife product consumption, 2) general biodiversity perceptions, 3) biodiversity at home and neighborhood, 4) public park visitation and motivations, 5) national park visitation and motivations, and 6) socio-demographic profiles. These resources are expected to support researchers in enriching the lax literature regarding the role of urban residents in biodiversity conservation and preservation, and help policymakers to find insights for building up an “eco-surplus culture” among urban residents through effective public communication and policymaking.},
  archive      = {J_DINT},
  author       = {Minh-Hoang, Nguyen},
  doi          = {10.1162/dint_a_00110},
  journal      = {Data Intelligence},
  number       = {4},
  pages        = {578-605},
  shortjournal = {Data Intell.},
  title        = {Multifaceted interactions between urban humans and biodiversity-related concepts: A developing-country data set},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Few-shot learning for named entity recognition based on BERT
and two-level model fusion. <em>DINT</em>, <em>3</em>(4), 568–577. (<a
href="https://doi.org/10.1162/dint_a_00102">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, as a basic task of military document information extraction, Named Entity Recognition (NER) for military documents has received great attention. In 2020, China Conference on Knowledge Graph and Semantic Computing (CCKS) and System Engineering Research Institute of Academy of Military Sciences (AMS) issued the NER task for test evaluation, which requires the recognition of four types of entities including Test Elements (TE), Performance Indicators (PI), System Components (SC) and Task Scenarios (TS). Due to the particularity and confidentiality of the military field, only 400 items of annotated data are provided by the organizer. In this paper, the task is regarded as a few-shot learning problem for NER, and a method based on BERT and two-level model fusion is proposed. Firstly, the proposed method is based on several basic models fine tuned by BERT on the training data. Then, a two-level fusion strategy applied to the prediction results of multiple basic models is proposed to alleviate the over-fitting problem. Finally, the labeling errors are eliminated by post-processing. This method achieves F1 score of 0.7203 on the test set of the evaluation task.},
  archive      = {J_DINT},
  author       = {Gong, Yuan and Mao, Lu and Li, Changliang},
  doi          = {10.1162/dint_a_00102},
  journal      = {Data Intelligence},
  number       = {4},
  pages        = {568-577},
  shortjournal = {Data Intell.},
  title        = {Few-shot learning for named entity recognition based on BERT and two-level model fusion},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). AOL4PS: A large-scale data set for personalized search.
<em>DINT</em>, <em>3</em>(4), 548–567. (<a
href="https://doi.org/10.1162/dint_a_00104">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Personalized search is a promising way to improve the quality of Websearch, and it has attracted much attention from both academic and industrial communities. Much of the current related research is based on commercial search engine data, which can not be released publicly for such reasons as privacy protection and information security. This leads to a serious lack of accessible public data sets in this field. The few publicly available data sets have not become widely used in academia because of the complexity of the processing process required to study personalized search methods. The lack of data sets together with the difficulties of data processing has brought obstacles to fair comparison and evaluation of personalized search models. In this paper, we constructed a large-scale data set AOL4PS to evaluate personalized search methods, collected and processed from AOL query logs. We present the complete and detailed data processing and construction process. Specifically, to address the challenges of processing time and storage space demands brought by massive data volumes, we optimized the process of data set construction and proposed an improved BM25 algorithm. Experiments are performed on AOL4PS with some classic and state-of-the-art personalized search methods, and the experiment results demonstrate that AOL4PS can measure the effect of personalized search models.},
  archive      = {J_DINT},
  author       = {Guo, Qian and Chen, Wei and Wan, Huaiyu},
  doi          = {10.1162/dint_a_00104},
  journal      = {Data Intelligence},
  number       = {4},
  pages        = {548-567},
  shortjournal = {Data Intell.},
  title        = {AOL4PS: A large-scale data set for personalized search},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DAMS: A distributed analytics metadata schema.
<em>DINT</em>, <em>3</em>(4), 528–547. (<a
href="https://doi.org/10.1162/dint_a_00100">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, implementations enabling Distributed Analytics (DA) have gained considerable attention due to their ability to perform complex analysis tasks on decentralised data by bringing the analysis to the data. These concepts propose privacy-enhancing alternatives to data centralisation approaches, which have restricted applicability in case of sensitive data due to ethical, legal or social aspects. Nevertheless, the immanent problem of DA-enabling architectures is the black-box-alike behaviour of the highly distributed components originating from the lack of semantically enriched descriptions, particularly the absence of basic metadata for data sets or analysis tasks. To approach the mentioned problems, we propose a metadata schema for DA infrastructures, which provides a vocabulary to enrich the involved entities with descriptive semantics. We initially perform a requirement analysis with domain experts to reveal necessary metadata items, which represents the foundation of our schema. Afterwards, we transform the obtained domain expert knowledge into user stories and derive the most significant semantic content. In the final step, we enable machine-readability via RDF(S) and SHACL serialisations. We deploy our schema in a proof-of-concept monitoring dashboard to validate its contribution to the transparency of DA architectures. Additionally, we evaluate the schema&#39;s compliance with the FAIR principles. The evaluation shows that the schema succeeds in increasing transparency while being compliant with most of the FAIR principles. Because a common metadata model is critical for enhancing the compatibility between multiple DA infrastructures, our work lowers data access and analysis barriers. It represents an initial and infrastructure-independent foundation for the FAIRification of DA and the underlying scientific data management.},
  archive      = {J_DINT},
  author       = {Welten, Sascha and Neumann, Laurenz and Yediel, Yeliz Ucer and da Silva Santos, Luiz Olavo Bonino and Decker, Stefan and Beyan, Oya},
  doi          = {10.1162/dint_a_00100},
  journal      = {Data Intelligence},
  number       = {4},
  pages        = {528-547},
  shortjournal = {Data Intell.},
  title        = {DAMS: A distributed analytics metadata schema},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Exploring the current practices, costs and benefits of FAIR
implementation in pharmaceutical research and development: A qualitative
interview study. <em>DINT</em>, <em>3</em>(4), 507–527. (<a
href="https://doi.org/10.1162/dint_a_00109">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The findable, accessible, interoperable, reusable (FAIR) principles for scientific data management and stewardship aim to facilitate data reuse at scale by both humans and machines. Research and development (R&amp;amp;D) in the pharmaceutical industry is becoming increasingly data driven, but managing its data assets according to FAIR principles remains costly and challenging. To date, little scientific evidence exists about how FAIR is currently implemented in practice, what its associated costs and benefits are, and how decisions are made about the retrospective FAIRification of data sets in pharmaceutical R&amp;amp;D. This paper reports the results of semi-structured interviews with 14 pharmaceutical professionals who participate in various stages of drug R&amp;amp;D in seven pharmaceutical businesses. Inductive thematic analysis identified three primary themes of the benefits and costs of FAIRification, and the elements that influence the decision-making process for FAIRifying legacy data sets. Participants collectively acknowledged the potential contribution of FAIRification to data reusability in diverse research domains and the subsequent potential for cost-savings. Implementation costs, however, were still considered a barrier by participants, with the need for considerable expenditure in terms of resources, and cultural change. How decisions were made about FAIRification was influenced by legal and ethical considerations, management commitment, and data prioritisation. The findings have significant implications for those in the pharmaceutical R&amp;amp;D industry who are engaged in driving FAIR implementation, and for external parties who seek to better understand existing practices and challenges.},
  archive      = {J_DINT},
  author       = {Alharbi, Ebtisam and Skeva, Rigina and Juty, Nick and Jay, Caroline and Goble, Carole},
  doi          = {10.1162/dint_a_00109},
  journal      = {Data Intelligence},
  number       = {4},
  pages        = {507-527},
  shortjournal = {Data Intell.},
  title        = {Exploring the current practices, costs and benefits of FAIR implementation in pharmaceutical research and development: A qualitative interview study},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A knowledge graph based approach to social science surveys.
<em>DINT</em>, <em>3</em>(4), 477–506. (<a
href="https://doi.org/10.1162/dint_a_00107">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent success of knowledge graphs has spurred interest in applying them in open science, such as on intelligent survey systems for scientists. However, efforts to understand the quality of candidate survey questions provided by these methods have been limited. Indeed, existing methods do not consider the type of on-the-fly content planning that is possible for face-to-face surveys and hence do not guarantee that selection of subsequent questions is based on response to previous questions in a survey. To address this limitation, we propose a dynamic and informative solution for an intelligent survey system that is based on knowledge graphs. To illustrate our proposal, we look into social science surveys, focusing on ordering the questions of a questionnaire component by their level of acceptance, along with conditional triggers that further customise participants&#39; experience. Our main findings are: (i) evaluation of the proposed approach shows that the dynamic component can be beneficial in terms of lowering the number of questions asked per variable, thus allowing more informative data to be collected in a survey of equivalent length; and (ii) a primary advantage of the proposed approach is that it enables grouping of participants according to their responses, so that participants are not only served appropriate follow-up questions, but their responses to these questions may be analysed in the context of some initial categorisation. We believe that the proposed approach can easily be applied to other social science surveys based on grouping definitions in their contexts. The knowledge-graph-based intelligent survey approach proposed in our work allows online questionnaires to approach face-to-face interaction in their level of informativity and responsiveness, as well as duplicating certain advantages of interview-based data collection.},
  archive      = {J_DINT},
  author       = {Pan, Jeff Z. and Edelstein, Elspeth and Bansky, Patrik and Wyner, Adam},
  doi          = {10.1162/dint_a_00107},
  journal      = {Data Intelligence},
  number       = {4},
  pages        = {477-506},
  shortjournal = {Data Intell.},
  title        = {A knowledge graph based approach to social science surveys},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A prior information enhanced extraction framework for
document-level financial event extraction. <em>DINT</em>, <em>3</em>(3),
460–476. (<a href="https://doi.org/10.1162/dint_a_00103">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Document-level financial event extraction (DFEE) is the task of detecting events and extracting the corresponding event arguments in financial documents, which plays an important role in information extraction in the financial domain. This task is challenging as the financial documents are generally long text and event arguments of one event may be scattered in different sentences. To address this issue, we proposed a novel Prior Information Enhanced Extraction framework (PIEE) for DFEE, leveraging prior information from both event types and pre-trained language models. Specifically, PIEE consists of three components: event detection, event argument extraction, and event table filling. In event detection, we identify the event type. Then, the event type is explicitly used for event argument extraction. Meanwhile, the implicit information within language models also provides considerable cues for event arguments localization. Finally, all the event arguments are filled in an event table by a set of predefined heuristic rules. To demonstrate the effectiveness of our proposed framework, we participated in the share task of CCKS2020 Task 4-2: Document-level Event Arguments Extraction. On both Leaderboard A and Leaderboard B, PIEE took the first place and significantly outperformed the other systems.},
  archive      = {J_DINT},
  author       = {Wang, Haitao and Zhu, Tong and Wang, Mingtao and Zhang, Guoliang and Chen, Wenliang},
  doi          = {10.1162/dint_a_00103},
  journal      = {Data Intelligence},
  number       = {3},
  pages        = {460-476},
  shortjournal = {Data Intell.},
  title        = {A prior information enhanced extraction framework for document-level financial event extraction},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A joint learning framework for the CCKS-2020 financial event
extraction task. <em>DINT</em>, <em>3</em>(3), 444–459. (<a
href="https://doi.org/10.1162/dint_a_00098">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a winning solution for the CCKS-2020 financial event extraction task, where the goal is to identify event types, triggers and arguments in sentences across multiple event types. In this task, we focus on resolving two challenging problems (i.e., low resources and element overlapping) by proposing a joint learning framework, named SaltyFishes. We first formulate the event extraction task as a joint probability model. By sharing parameters in the model across different types, we can learn to adapt to low-resource events based on high-resource events. We further address the element overlapping problems by a mechanism of Conditional Layer Normalization, achieving even better extraction accuracy. The overall approach achieves an F1-score of 87.8\% which ranks the first place in the competition.},
  archive      = {J_DINT},
  author       = {Sheng, Jiawei and Li, Qian and Hei, Yiming and Guo, Shu and Yu, Bowen and Wang, Lihong and He, Min and Liu, Tingwen and Xu, Hongbo},
  doi          = {10.1162/dint_a_00098},
  journal      = {Data Intelligence},
  number       = {3},
  pages        = {444-459},
  shortjournal = {Data Intell.},
  title        = {A joint learning framework for the CCKS-2020 financial event extraction task},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Data set and evaluation of automated construction of
financial knowledge graph. <em>DINT</em>, <em>3</em>(3), 418–443. (<a
href="https://doi.org/10.1162/dint_a_00108">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the technological development of entity extraction, relationship extraction, knowledge reasoning, and entity linking, the research on knowledge graph has been carried out in full swing in recent years. To better promote the development of knowledge graph, especially in the Chinese language and in the financial industry, we built a high-quality data set, named financial research report knowledge graph (FR2KG), and organized the automated construction of financial knowledge graph evaluation at the 2020 China Knowledge Graph and Semantic Computing Conference (CCKS2020). FR2KG consists of 17,799 entities, 26,798 relationship triples, and 1,328 attribute triples covering 10 entity types, 19 relationship types, and 6 attributes. Participants are required to develop a constructor that will automatically construct a financial knowledge graph based on the FR2KG. In addition, we summarized the technologies for automatically constructing knowledge graphs, and introduced the methods used by the winners and the results of this evaluation.},
  archive      = {J_DINT},
  author       = {Wang, Wenguang and Xu, Yonglin and Du, Chunhui and Chen, Yunwen and Wang, Yijie and Wen, Hui},
  doi          = {10.1162/dint_a_00108},
  journal      = {Data Intelligence},
  number       = {3},
  pages        = {418-443},
  shortjournal = {Data Intell.},
  title        = {Data set and evaluation of automated construction of financial knowledge graph},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Medical named entity recognition from un-labelled medical
records based on pre-trained language models and domain dictionary.
<em>DINT</em>, <em>3</em>(3), 402–417. (<a
href="https://doi.org/10.1162/dint_a_00105">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical named entity recognition (NER) is an area in which medical named entities are recognized from medical texts, such as diseases, drugs, surgery reports, anatomical parts, and examination documents. Conventional medical NER methods do not make full use of un-labelled medical texts embedded in medical documents. To address this issue, we proposed a medical NER approach based on pre-trained language models and a domain dictionary. First, we constructed a medical entity dictionary by extracting medical entities from labelled medical texts and collecting medical entities from other resources, such as the Yidu-N4K data set. Second, we employed this dictionary to train domain-specific pre-trained language models using un-labelled medical texts. Third, we employed a pseudo labelling mechanism in un-labelled medical texts to automatically annotate texts and create pseudo labels. Fourth, the BiLSTM-CRF sequence tagging model was used to fine-tune the pre-trained language models. Our experiments on the un-labelled medical texts, which were extracted from Chinese electronic medical records, show that the proposed NER approach enables the strict and relaxed F1 scores to be 88.7\% and 95.3\%, respectively.},
  archive      = {J_DINT},
  author       = {Wen, Chaojie and Chen, Tao and Jia, Xudong and Zhu, Jiang},
  doi          = {10.1162/dint_a_00105},
  journal      = {Data Intelligence},
  number       = {3},
  pages        = {402-417},
  shortjournal = {Data Intell.},
  title        = {Medical named entity recognition from un-labelled medical records based on pre-trained language models and domain dictionary},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Semi-supervised noisy label learning for chinese clinical
named entity recognition. <em>DINT</em>, <em>3</em>(3), 389–401. (<a
href="https://doi.org/10.1162/dint_a_00099">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper describes our approach for the Chinese clinical named entity recognition (CNER) task organized by the 2020 China Conference on Knowledge Graph and Semantic Computing (CCKS) competition. In this task, we need to identify the entity boundary and category labels of six entities from Chinese electronic medical record (EMR). We constructed a hybrid system composed of a semi-supervised noisy label learning model based on adversarial training and a rule post-processing module. The core idea of the hybrid system is to reduce the impact of data noise by optimizing the model results. Besides, we used post-processing rules to correct three cases of redundant labeling, missing labeling, and wrong labeling in the model prediction results. Our method proposed in this paper achieved strict criteria of 0.9156 and relax criteria of 0.9660 on the final test set, ranking first.},
  archive      = {J_DINT},
  author       = {Li, Zhucong and Gan, Zhen and Zhang, Baoli and Chen, Yubo and Wan, Jing and Liu, Kang and Zhao, Jun and Liu, Shengping},
  doi          = {10.1162/dint_a_00099},
  journal      = {Data Intelligence},
  number       = {3},
  pages        = {389-401},
  shortjournal = {Data Intell.},
  title        = {Semi-supervised noisy label learning for chinese clinical named entity recognition},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Overview of CCKS 2020 task 3: Named entity recognition and
event extraction in chinese electronic medical records. <em>DINT</em>,
<em>3</em>(3), 376–388. (<a
href="https://doi.org/10.1162/dint_a_00093">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The China Conference on Knowledge Graph and Semantic Computing (CCKS) 2020 Evaluation Task 3 presented clinical named entity recognition and event extraction for the Chinese electronic medical records. Two annotated data sets and some other additional resources for these two subtasks were provided for participators. This evaluation competition attracted 354 teams and 46 of them successfully submitted the valid results. The pre-trained language models are widely applied in this evaluation task. Data argumentation and external resources are also helpful.},
  archive      = {J_DINT},
  author       = {Li, Xia and Wen, Qinghua and Lin, Hu and Jiao, Zengtao and Zhang, Jiangtao},
  doi          = {10.1162/dint_a_00093},
  journal      = {Data Intelligence},
  number       = {3},
  pages        = {376-388},
  shortjournal = {Data Intell.},
  title        = {Overview of CCKS 2020 task 3: Named entity recognition and event extraction in chinese electronic medical records},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Integrated “generate, make, and test” for formulated
products using knowledge graphs. <em>DINT</em>, <em>3</em>(3), 340–375.
(<a href="https://doi.org/10.1162/dint_a_00096">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the multi-billion dollar formulated product industry, state of the art continues to rely heavily on experts during the “generate, make and test” steps of formulation design. We propose automation aids to each step with a knowledge graph of relevant information as the central artifact. The generate step usually focuses on coming up with new recipes for intended formulation. We propose to aid the experts who generally carry out this step manually by providing a recommendation system and a templating system on top of the knowledge graph. Using the former, the expert can create a recipe from scratch using historical formulations and related data. With the latter, the expert starts with a recipe template created by our system and substitutes the requisite constituents to form a recipe. In the current state of practice, the three steps mentioned above operate in a fragmented manner wherein observations from one step do not aid other steps in a streamlined manner. Instead of manually operated labs for the make and test steps, we assume automated or robotic labs and in-silico testing, respectively. Using two formulations, namely face cream and an exterior coating, we show how the knowledge graph may help integrate and streamline the communication between the generate, the make, and the test steps. Our initial exploration shows considerable promise.},
  archive      = {J_DINT},
  author       = {Sunkle, Sagar and Jain, Deepak and Saxena, Krati and Patil, Ashwini and Singh, Tushita and Rai, Beena and Kulkarni, Vinay},
  doi          = {10.1162/dint_a_00096},
  journal      = {Data Intelligence},
  number       = {3},
  pages        = {340-375},
  shortjournal = {Data Intell.},
  title        = {Integrated “Generate, make, and test” for formulated products using knowledge graphs},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep learning with heterogeneous graph embeddings for
mortality prediction from electronic health records. <em>DINT</em>,
<em>3</em>(3), 329–339. (<a
href="https://doi.org/10.1162/dint_a_00097">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computational prediction of in-hospital mortality in the setting of an intensive care unit can help clinical practitioners to guide care and make early decisions for interventions. As clinical data are complex and varied in their structure and components, continued innovation of modelling strategies is required to identify architectures that can best model outcomes. In this work, we trained a Heterogeneous Graph Model (HGM) on electronic health record (EHR) data and used the resulting embedding vector as additional information added to a Convolutional Neural Network (CNN) model for predicting in-hospital mortality. We show that the additional information provided by including time as a vector in the embedding captured the relationships between medical concepts, lab tests, and diagnoses, which enhanced predictive performance. We found that adding HGM to a CNN model increased the mortality prediction accuracy up to 4\%. This framework served as a foundation for future experiments involving different EHR data types on important healthcare prediction tasks.},
  archive      = {J_DINT},
  author       = {Wanyan, Tingyi and Honarvar, Hossein and Azad, Ariful and Ding, Ying and Glicksberg, Benjamin S.},
  doi          = {10.1162/dint_a_00097},
  journal      = {Data Intelligence},
  number       = {3},
  pages        = {329-339},
  shortjournal = {Data Intell.},
  title        = {Deep learning with heterogeneous graph embeddings for mortality prediction from electronic health records},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A data collection on secondary school students’ STEM
performance and reading practices in an emerging country. <em>DINT</em>,
<em>3</em>(2), 336–356. (<a
href="https://doi.org/10.1162/dint_a_00091">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Science, technology, engineering, and mathematics (STEM) education has become a critical factor in promoting sustainable development. Meanwhile, book reading is still an essential method for cognitive development and knowledge acquisition. In developing countries where STEM teaching and learning resources are limited, book reading is an important educational tool to promote STEM. Nevertheless, public data sets about STEM education and book reading behaviors in emerging countries are scarce. This article, therefore, aims to present a data set of 4,966 secondary school students from a school-based data collection in Vietnam. The data set comprises of five major categories: 1) students&#39; personal information (including STEM performance), 2) family-related information, 3) book reading preferences, 4) book reading frequency/habits, and 5) classroom activities. By introducing the designing principles, the data collection method, and the variables in the data set, we aim to provide researchers, policymakers, and educators with well-validated resources and guidelines to conduct low-cost research, pedagogical programs in emerging countries.},
  archive      = {J_DINT},
  author       = {Vuong, Quan-Hoang and La, Viet-Phuong and Ho, Manh-Toan and Pham, Thanh-Hang and Vuong, Thu-Trang and Vuong, Ha-My and Nguyen, Minh-Hoang},
  doi          = {10.1162/dint_a_00091},
  journal      = {Data Intelligence},
  number       = {2},
  pages        = {336-356},
  shortjournal = {Data Intell.},
  title        = {A data collection on secondary school students&#39; STEM performance and reading practices in an emerging country},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Cross-context news corpus for protest event-related
knowledge base construction. <em>DINT</em>, <em>3</em>(2), 308–335. (<a
href="https://doi.org/10.1162/dint_a_00092">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We describe a gold standard corpus of protest events that comprise various local and international English language sources from various countries. The corpus contains document-, sentence-, and token-level annotations. This corpus facilitates creating machine learning models that automatically classify news articles and extract protest event-related information, constructing knowledge bases that enable comparative social and political science studies. For each news source, the annotation starts with random samples of news articles and continues with samples drawn using active learning. Each batch of samples is annotated by two social and political scientists, adjudicated by an annotation supervisor, and improved by identifying annotation errors semi-automatically. We found that the corpus possesses the variety and quality that are necessary to develop and benchmark text classification and event extraction systems in a cross-context setting, contributing to the generalizability and robustness of automated text processing systems. This corpus and the reported results will establish a common foundation in automated protest event collection studies, which is currently lacking in the literature.},
  archive      = {J_DINT},
  author       = {Hürriyetoğlu, Ali and Yörük, Erdem and Mutlu, Osman and Duruşan, Fırat and Yoltar, Çağrı and Yüret, Deniz and Gürel, Burak},
  doi          = {10.1162/dint_a_00092},
  journal      = {Data Intelligence},
  number       = {2},
  pages        = {308-335},
  shortjournal = {Data Intell.},
  title        = {Cross-context news corpus for protest event-related knowledge base construction},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Overview of SMP-CAIL2020-argmine: The interactive
argument-pair extraction in judgement document challenge. <em>DINT</em>,
<em>3</em>(2), 287–307. (<a
href="https://doi.org/10.1162/dint_a_00094">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. In this paper we present the results of the Interactive Argument-Pair Extraction in Judgement Document Challenge held by both the Chinese AI and Law Challenge (CAIL) and the Chinese National Social Media Processing Conference (SMP), and introduce the related data set - SMP-CAIL2020-Argmine. The task challenged participants to choose the correct argument among five candidates proposed by the defense to refute or acknowledge the given argument made by the plaintiff, providing the full context recorded in the judgement documents of both parties. We received entries from 63 competing teams, 38 of which scored higher than the provided baseline model (BERT) in the first phase and entered the second phase. The best performing system in the two phases achieved accuracy of 0.856 and 0.905, respectively. In this paper, we will present the results of the competition and a summary of the systems, highlighting commonalities and innovations among participating systems. The SMP-CAIL2020-Argmine data set and baseline models① have been already released.},
  archive      = {J_DINT},
  author       = {Yuan, Jian and Wei, Zhongyu and Gao, Yixu and Chen, Wei and Song, Yun and Zhao, Donghua and Ma, Jinglei and Hu, Zhen and Zou, Shaokun and Li, Donghai and Huang, Xuanjing},
  doi          = {10.1162/dint_a_00094},
  journal      = {Data Intelligence},
  number       = {2},
  pages        = {287-307},
  shortjournal = {Data Intell.},
  title        = {Overview of SMP-CAIL2020-argmine: The interactive argument-pair extraction in judgement document challenge},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An evaluation of chinese human-computer dialogue technology.
<em>DINT</em>, <em>3</em>(2), 274–286. (<a
href="https://doi.org/10.1162/dint_a_00090">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. There is a growing interest in developing human-computer dialogue systems which is an important branch in the field of artificial intelligence (AI). However, the evaluation of large-scale Chinese human-computer dialogues is still a challenging task. To attract more attention to dialogue evaluation work, we held the fourth Evaluation of Chinese Human-Computer Dialogue Technology (ECDT). It consists of few-shot learning in spoken language understanding (SLU) (Task 1) and knowledge-driven multi-turn dialogue competition (Task 2), the data sets of which are provided by Harbin Institute of Technology and Tsinghua University. In this paper, we will introduce the evaluation tasks and data sets in detail. Meanwhile, we will also analyze the evaluation results and the existing problems in the evaluation.},
  archive      = {J_DINT},
  author       = {Feng, Zixian and Zhu, Caihai and Zhang, Weinan and Chen, Zhigang and Che, Wanxiang and Huang, Minlie and Li, Linlin},
  doi          = {10.1162/dint_a_00090},
  journal      = {Data Intelligence},
  number       = {2},
  pages        = {274-286},
  shortjournal = {Data Intell.},
  title        = {An evaluation of chinese human-computer dialogue technology},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Transdisciplinary convergence: Intelligent infrastructure
for sustainable development. <em>DINT</em>, <em>3</em>(2), 261–273. (<a
href="https://doi.org/10.1162/dint_a_00063">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. The fast-developing intelligent infrastructure landscape catalyzes transformative new relationships of human, technology, and environment and requires new socio-technical configurations of information practice and knowledge work. With a focus on data as the source of intelligence, this paper aims to explore the shifting scenarios and indicative features of data science solutions for intelligent system applications and identify the evolving knowledge spaces and integrative learning practices in the “smart” landscape. It projects and discusses the democratization of data science platforms, the distribution of data intelligence on the edge, and the transition from vertical to horizontal data solutions in solving intelligent system problems. Through mapping the changing data research landscape, this work further reveals essential new roles of knowledge architects and social engineers in enabling dynamic data linking, interaction, and exploration for transdisciplinary data convergence.},
  archive      = {J_DINT},
  author       = {Shen, Yi},
  doi          = {10.1162/dint_a_00063},
  journal      = {Data Intelligence},
  number       = {2},
  pages        = {261-273},
  shortjournal = {Data Intell.},
  title        = {Transdisciplinary convergence: Intelligent infrastructure for sustainable development},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Probabilistic tractable models in mixed discrete-continuous
domains. <em>DINT</em>, <em>3</em>(2), 228–260. (<a
href="https://doi.org/10.1162/dint_a_00064">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We study the problem of the unsupervised learning of graphical models in mixed discrete-continuous domains. The problem of unsupervised learning of such models in discrete domains alone is notoriously challenging, compounded by the fact that inference is computationally demanding. The situation is generally believed to be significantly worse in discrete-continuous domains: estimating the unknown probability distribution of given samples is often limited in practice to a handful of parametric forms, and in addition to that, computing conditional queries need to carefully handle low-probability regions in safety-critical applications. In recent years, the regime of tractable learning has emerged, which attempts to learn a graphical model that permits efficient inference. Most of the results in this regime are based on arithmetic circuits, for which inference is linear in the size of the obtained circuit. In this work, we show how, with minimal modifications, such regimes can be generalized by leveraging efficient density estimation schemes based on piecewise polynomial approximations. Our framework is realized on a recent computational abstraction that permits efficient inference for a range of queries in the underlying language. Our empirical results show that our approach is effective, and allows a study of the trade-off between the granularity of the learned model and its predictive power.},
  archive      = {J_DINT},
  author       = {Bueff, Andreas and Speichert, Stefanie and Belle, Vaishak},
  doi          = {10.1162/dint_a_00064},
  journal      = {Data Intelligence},
  number       = {2},
  pages        = {228-260},
  shortjournal = {Data Intell.},
  title        = {Probabilistic tractable models in mixed discrete-continuous domains},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). OpenKG chain: A blockchain infrastructure for open knowledge
graphs. <em>DINT</em>, <em>3</em>(2), 205–227. (<a
href="https://doi.org/10.1162/dint_a_00095">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. The early concept of knowledge graph originates from the idea of the semantic Web, which aims at using structured graphs to model the knowledge of the world and record the relationships that exist between things. Currently publishing knowledge bases as open data on the Web has gained significant attention. In China, Chinese Information Processing Society of China (CIPS) launched the OpenKG in 2015 to foster the development of Chinese Open Knowledge Graphs. Unlike existing open knowledge-based programs, OpenKG chain is envisioned as a blockchain-based open knowledge infrastructure. This article introduces the first attempt at the implementation of sharing knowledge graphs on OpenKG chain, a blockchain-based trust network. We have completed the test of the underlying blockchain platform, and the on-chain test of OpenKG&#39;s data set and tool set sharing as well as fine-grained knowledge crowdsourcing at the triple level. We have also proposed novel definitions: K-Point and OpenKG Token, which can be considered to be a measurement of knowledge value and user value. 1,033 knowledge contributors have been involved in two months of testing on the blockchain, and the cumulative number of on-chain recordings triggered by real knowledge consumers has reached 550,000 with an average daily peak value of more than 10,000. For the first time, we have tested and realized on-chain sharing of knowledge at entity/triple granularity level. At present, all operations on the data sets and tool sets at OpenKG.CN, as well as the triplets at OpenBase, are recorded on the chain, and corresponding value will also be generated and assigned in a trusted mode. Via this effort, OpenKG chain looks forward to providing a more credible and traceable knowledge-sharing platform for the knowledge graph community.},
  archive      = {J_DINT},
  author       = {Chen, Huajun and Hu, Ning and Qi, Guilin and Wang, Haofen and Bi, Zhen and Li, Jie and Yang, Fan},
  doi          = {10.1162/dint_a_00095},
  journal      = {Data Intelligence},
  number       = {2},
  pages        = {205-227},
  shortjournal = {Data Intell.},
  title        = {OpenKG chain: A blockchain infrastructure for open knowledge graphs},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Research data management implementation at peking university
library: Foster and promote open science and open data. <em>DINT</em>,
<em>3</em>(1), 189–204. (<a
href="https://doi.org/10.1162/dint_a_00088">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Research Data Management (RDM) has become increasingly important for more and more academic institutions. Using the Peking University Open Research Data Repository (PKU-ORDR) project as an example, this paper will review a library-based university-wide open research data repository project and related RDM services implementation process including project kickoff, needs assessment, partnerships establishment, software investigation and selection, software customization, as well as data curation services and training. Through the review, some issues revealed during the stages of the implementation process are also discussed and addressed in the paper such as awareness of research data, demands from data providers and users, data policies and requirements from home institution, requirements from funding agencies and publishers, the collaboration between administrative units and libraries, and concerns from data providers and users. The significance of the study is that the paper shows an example of creating an Open Data repository and RDM services for other Chinese academic libraries planning to implement their RDM services for their home institutions. The authors of the paper have also observed since the PKU-ORDR and RDM services implemented in 2015, the Peking University Library (PKUL) has helped numerous researchers to support the entire research life cycle and enhanced Open Science (OS) practices on campus, as well as impacted the national OS movement in China through various national events and activities hosted by the PKUL.},
  archive      = {J_DINT},
  author       = {Nie, Hua and Luo, Pengcheng and Fu, Ping},
  doi          = {10.1162/dint_a_00088},
  journal      = {Data Intelligence},
  number       = {1},
  pages        = {189-204},
  shortjournal = {Data Intell.},
  title        = {Research data management implementation at peking university library: Foster and promote open science and open data},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Key aspects of open data in finnish RDI cooperation between
higher education and businesses. <em>DINT</em>, <em>3</em>(1), 176–188.
(<a href="https://doi.org/10.1162/dint_a_00065">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. The article highlights aspects that should be considered during an open Research, Development, and Innovation (RDI) process cycle to improve the utilization of research data and foster open cooperation between higher education and businesses. The viewpoint here is in publicly funded joint research projects of the universities of applied sciences (UAS), the concept is, however, applicable in other higher education and research organizations as well. There are various challenges related to research data management in general as well as to the openness and reuse of data and results. The findings of this article are based on the results of a two-day expert workshop, and these results are interlinked with five phases of an open RDI process cycle: planning, implementation, documentation, sharing, and commercialization. Various drivers and barriers can be identified in different stages of the process. On a general level, special attention must be paid to critical factors such as ownership and sharing of data and results, confidential information and business secrets as well as following the requirements of the Open Science (OS) policies of the participating organizations and funders. This article also highlights several best practices that should be considered in each phase of an open RDI process cycle with businesses.},
  archive      = {J_DINT},
  author       = {Päällysaho, Seliina and Latvanen, Jaana and Lehto, Anttoni and Riihimaa, Jaakko and Lahti, Pekka and Kärki, Anne and Puhakka-Tarvainen, Helena},
  doi          = {10.1162/dint_a_00065},
  journal      = {Data Intelligence},
  number       = {1},
  pages        = {176-188},
  shortjournal = {Data Intell.},
  title        = {Key aspects of open data in finnish RDI cooperation between higher education and businesses},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Developing open RDI and education in finnish universities of
applied sciences. <em>DINT</em>, <em>3</em>(1), 162–175. (<a
href="https://doi.org/10.1162/dint_a_00066">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Open Science (OS) and Research has reached mixed maturity levels in Finland. The meaning of the national project in the ecosystem of Finnish universities of applied sciences (UAS) is to enhance and elaborate OS and Open Education (OE) activities. Future actions were defined based on a survey and interviews carried out in the Finnish UAS sector during 2018 and 2019. The aim of both data collections was to evaluate the current status and attitudes towards open Research, Development, and Innovation (RDI) among staff members. Another purpose was to define the need for internal support services concerning open RDI and OE and to identify knowledge gaps. The results revealed several gaps in understanding OS and OE initiatives. Real-life actions were mostly vague, and the respondents experienced the need for support. On the other hand, the attitudes towards open RDI were positive, and the issue aroused questions and reflections. This study revealed gaps in knowledge and actions in Finnish UAS sectors. These results have been the basis of development actions such as joint workshops, educational webinars, and common instructions. The future plan includes the establishment of an experts&#39; network for supporting open RDI and Education.},
  archive      = {J_DINT},
  author       = {Kärki, Anne and Päällysaho, Seliina and Jaalama, Kaisa and Talvela, Juhani and Lehto, Anttoni and Hyyppä, Hannu},
  doi          = {10.1162/dint_a_00066},
  journal      = {Data Intelligence},
  number       = {1},
  pages        = {162-175},
  shortjournal = {Data Intell.},
  title        = {Developing open RDI and education in finnish universities of applied sciences},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Implementation of an open science instruction program for
undergraduates. <em>DINT</em>, <em>3</em>(1), 150–161. (<a
href="https://doi.org/10.1162/dint_a_00086">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. The scientific, social, and economic advantages that accrue from Open Science (OS) practices—ways of doing research that emphasize reproducibility, transparency, and accessibility at all stages of the research cycle—are now widely recognized in nations around the world and by international bodies such as the United Nations and the Organization for Economic Cooperation and Development. However, program wide or coordinated instruction of undergraduate students in OS practices remains uncommon. At the University of British Columbia in Canada, we have started to develop a comprehensive undergraduate OS program that can be adapted to and woven into diverse subject curricula. We report on the context and planning of the pilot module of the program, “Open Science 101”, its implementation in first-year Biology in Fall 2019, and qualitative results of an attitudinal survey of students following their course.},
  archive      = {J_DINT},
  author       = {Hanna, Sharon and Pither, Jason and Vis-Dunbar, Mathew},
  doi          = {10.1162/dint_a_00086},
  journal      = {Data Intelligence},
  number       = {1},
  pages        = {150-161},
  shortjournal = {Data Intell.},
  title        = {Implementation of an open science instruction program for undergraduates},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). EU-citizen.science: A platform for mainstreaming citizen
science and open science in europe. <em>DINT</em>, <em>3</em>(1),
136–149. (<a href="https://doi.org/10.1162/dint_a_00085">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Citizen Science (CS) is a prominent field of application for Open Science (OS), and the two have strong synergies, such as: advocating for the data and metadata generated through science to be made publicly available [1]; supporting more equitable collaboration between different types of scientists and citizens; and facilitating knowledge transfer to a wider range of audiences [2]. While primarily targeted at CS, the EU-Citizen. Science platform can also support OS. One of its key functions is to act as a knowledge hub to aggregate, disseminate and promote experience and know-how; for example, by profiling CS projects and collecting tools, resources and training materials relevant to both fields. To do this, the platform has developed an information architecture that incorporates the public participation in scientific research (PPSR)—Common Conceptual Model①. This model consists of the Project Metadata Model, the Dataset Metadata Model and the Observation Data Model, which were specifically developed for CS initiatives. By implementing these, the platform will strengthen the interoperating arrangements that exist between other, similar platforms (e.g., BioCollect and SciStarter) to ensure that CS and OS continue to grow globally in terms of participants, impact and fields of application.},
  archive      = {J_DINT},
  author       = {Wagenknecht, Katherin and Woods, Tim and Sanz, Francisco García and Gold, Margaret and Bowser, Anne and Rüfenacht, Simone and Ceccaroni, Luigi and Piera, Jaume},
  doi          = {10.1162/dint_a_00085},
  journal      = {Data Intelligence},
  number       = {1},
  pages        = {136-149},
  shortjournal = {Data Intell.},
  title        = {EU-Citizen.Science: A platform for mainstreaming citizen science and open science in europe},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Not ready for convergence in data infrastructures.
<em>DINT</em>, <em>3</em>(1), 116–135. (<a
href="https://doi.org/10.1162/dint_a_00084">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Much research is dependent on Information and Communication Technologies (ICT). Researchers in different research domains have set up their own ICT systems (data labs) to support their research, from data collection (observation, experiment, simulation) through analysis (analytics, visualisation) to publication. However, too frequently the Digital Objects (DOs) upon which the research results are based are not curated and thus neither available for reproduction of the research nor utilization for other (e.g., multidisciplinary) research purposes. The key to curation is rich metadata recording not only a description of the DO and the conditions of its use but also the provenance – the trail of actions performed on the DO along the research workflow. There are increasing real-world requirements for multidisciplinary research. With DOs in domain-specific ICT systems (silos), commonly with inadequate metadata, such research is hindered. Despite wide agreement on principles for achieving FAIR (findable, accessible, interoperable, and reusable) utilization of research data, current practices fall short. FAIR DOs offer a way forward. The paradoxes, barriers and possible solutions are examined. The key is persuading the researcher to adopt best practices which implies decreasing the cost (easy to use autonomic tools) and increasing the benefit (incentives such as acknowledgement and citation) while maintaining researcher independence and flexibility.},
  archive      = {J_DINT},
  author       = {Jeffery, Keith and Wittenburg, Peter and Lannom, Larry and Strawn, George and Biniossek, Claudia and Betz, Dirk and Blanchi, Christophe},
  doi          = {10.1162/dint_a_00084},
  journal      = {Data Intelligence},
  number       = {1},
  pages        = {116-135},
  shortjournal = {Data Intell.},
  title        = {Not ready for convergence in data infrastructures},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Embedding open science in reality. <em>DINT</em>,
<em>3</em>(1), 106–115. (<a
href="https://doi.org/10.1162/dint_a_00083">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. The Open Science (OS) movement has achieved extraordinary results in very few years. In this paper I argue it is now necessary to embed OS in the wider ecosystem of research and innovation, acknowledging some of the outstanding issues that need to be resolved as it beds down into the way research is done in the future. By sticking to a purest approach to OS its impact and current momentum may be lost. Digital technologies and global connectivity have ensured that OS is here to stay and will continue to expand its influence in the future. However, OS cannot stand aloof from what is the reality of what is happening elsewhere otherwise it will do a disservice to itself and the challenges facing the world.},
  archive      = {J_DINT},
  author       = {Wood, John},
  doi          = {10.1162/dint_a_00083},
  journal      = {Data Intelligence},
  number       = {1},
  pages        = {106-115},
  shortjournal = {Data Intell.},
  title        = {Embedding open science in reality},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Open science and data science. <em>DINT</em>,
<em>3</em>(1), 95–105. (<a
href="https://doi.org/10.1162/dint_a_00082">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Data Science (DS) as defined by Jim Gray is an emerging paradigm in all research areas to help finding non-obvious patterns of relevance in large distributed data collections. “Open Science by Design” (OSD), i.e., making artefacts such as data, metadata, models, and algorithms available and re-usable to peers and beyond as early as possible, is a pre-requisite for a flourishing DS landscape. However, a few major aspects can be identified hampering a fast transition: (1) The classical “Open Science by Publication” (OSP) is not sufficient any longer since it serves different functions, leads to non-acceptable delays and is associated with high curation costs. Changing data lab practices towards OSD requires more fundamental changes than OSP. 2) The classical publication-oriented models for metrics, mainly informed by citations, will not work anymore since the roles of contributors are more difficult to assess and will often change, i.e., other ways for assigning incentives and recognition need to be found. (3) The huge investments in developing DS skills and capacities by some global companies and strong countries is leading to imbalances and fears by different stakeholders hampering the acceptance of Open Science (OS). (4) Finally, OSD will depend on the availability of a global infrastructure fostering an integrated and interoperable data domain—“one data-domain” as George Strawn calls it—which is still not visible due to differences about the technological key pillars. OS therefore is a need for DS, but it will take much more time to implement it than we may have expected.},
  archive      = {J_DINT},
  author       = {Wittenburg, Peter},
  doi          = {10.1162/dint_a_00082},
  journal      = {Data Intelligence},
  number       = {1},
  pages        = {95-105},
  shortjournal = {Data Intell.},
  title        = {Open science and data science},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Open science and the hype cycle. <em>DINT</em>,
<em>3</em>(1), 88–94. (<a
href="https://doi.org/10.1162/dint_a_00081">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. The introduction of a new technology or innovation is often accompanied by “ups and downs” in its fortunes. Gartner Inc. defined a so-called hype cycle to describe a general pattern that many innovations experience: technology trigger, peak of inflated expectations, trough of disillusionment, slope of enlightenment, and plateau of productivity. This article will compare the ongoing introduction of Open Science (OS) with the hype cycle model and speculate on the relevance of that model to OS. Lest the title of this article mislead the reader, be assured that the author believes that OS should happen and that it will happen. However, I also believe that the path to OS will be longer than many of us had hoped. I will give a brief history of the today&#39;s “semi-open” science, define what I mean by OS, define the hype cycle and where OS is now on that cycle, and finally speculate what it will take to traverse the cycle and rise to its plateau of productivity (as described by Gartner).},
  archive      = {J_DINT},
  author       = {Strawn, George},
  doi          = {10.1162/dint_a_00081},
  journal      = {Data Intelligence},
  number       = {1},
  pages        = {88-94},
  shortjournal = {Data Intell.},
  title        = {Open science and the hype cycle},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On the complexities of federating research data
infrastructures. <em>DINT</em>, <em>3</em>(1), 79–87. (<a
href="https://doi.org/10.1162/dint_a_00080">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Federated Research Data Infrastructures aim to provide seamless access to research data along with services to facilitate the researchers in performing their data management tasks. During our research on Open Science (OS), we have built cross-disciplinary federated infrastructures for different types of (open) digital resources: Open Data (OD), Open Educational Resources (OER), and open access documents. In each case, our approach targeted only the resource “metadata”. Based on this experience, we identified some challenges that we had to overcome again and again: lack of (i) harvesters, (ii) common metadata models and (iii) metadata mapping tools. In this paper, we report on the challenges we faced in the federated infrastructure projects we were involved with. We structure the report based on the three challenges listed above.},
  archive      = {J_DINT},
  author       = {Latif, Atif and Limani, Fidan and Tochtermann, Klaus},
  doi          = {10.1162/dint_a_00080},
  journal      = {Data Intelligence},
  number       = {1},
  pages        = {79-87},
  shortjournal = {Data Intell.},
  title        = {On the complexities of federating research data infrastructures},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Building momentum to realign incentives to support open
science. <em>DINT</em>, <em>3</em>(1), 71–78. (<a
href="https://doi.org/10.1162/dint_a_00079">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. The COVID-19 pandemic highlights the urgent need to strengthen global scientific collaboration, and to ensure the fundamental right to universal access to scientific progress and its applications. Open Science (OS) is central to achieving these goals. It aims to make science accessible, transparent, and effective by providing barrier-free access to scientific publications, data, and infrastructures, along with open software, Open Educational Resources, and open technologies. OS also promotes public trust in science at a time when it has never been more important to do so. Over the past decade, momentum towards the widespread adoption of OS practices has been primarily driven by declarations (e.g., DORA, the Leiden Manifesto). These serve an important role, but for OS to truly take root, researchers also must be fully incentivized and rewarded for its practice. This requires research funders and academic leaders to take the lead in collaborating, with researchers in designing, and implementing new incentive structures, and to actively work to socialize these throughout the research ecosystem. The US National Academies of Science, Engineering, and Medicine (NASEM) Roundtable on Aligning Research Incentives for OS is one such effort. This paper examines the strategy behind convening the Roundtable, its current participant makeup, focus, and outputs. It also explores how this approach might be expanded and adapted throughout the global OS community.},
  archive      = {J_DINT},
  author       = {Joseph, Heather},
  doi          = {10.1162/dint_a_00079},
  journal      = {Data Intelligence},
  number       = {1},
  pages        = {71-78},
  shortjournal = {Data Intell.},
  title        = {Building momentum to realign incentives to support open science},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Open science—a question of trust. <em>DINT</em>,
<em>3</em>(1), 64–70. (<a
href="https://doi.org/10.1162/dint_a_00078">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Collaboration and the sharing of knowledge is at the heart of Open Science (OS). However, we need to know that the knowledge we find and share is really what it purports to be; and we need to know that the authors we hope to collaborate with are really the people they claim to be. In this paper, the author argues that a prerequisite for OS is trust and that persistent identifiers help to build that trust. The persistent identifier systems must themselves be trustworthy and they must be able to connect the user or their machine to the information they need now and into the future. Infrastructure is rather like plumbing: It goes unnoticed and unappreciated until it fails. This paper puts infrastructure for persistent identifiers in the spotlight as a core component of OS.},
  archive      = {J_DINT},
  author       = {Clark, Jonathan},
  doi          = {10.1162/dint_a_00078},
  journal      = {Data Intelligence},
  number       = {1},
  pages        = {64-70},
  shortjournal = {Data Intell.},
  title        = {Open Science—A question of trust},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). About open science and autonomy of science. <em>DINT</em>,
<em>3</em>(1), 52–63. (<a
href="https://doi.org/10.1162/dint_a_00077">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. This article invites us to a concise walk through the past, offering insights defined by the major challenges science encountered during the centuries. Some lessons for today and tomorrow are enumerated in the three sections of the article, and they go beyond the relatively few perspectives offered by today&#39;s Data Science: Open Science (OS) is what has always happened and is nothing new, because science has always sought to be open. Esthetical values played a relevant role in the past. Former scientists recognized the intrinsic relation between the way they opened science and the way they followed the principles of beauty and the sense of esthetic. Their groundbreaking heritage still inspires us in being ready to open new ways in science. Whereas Latin was the original lingua franca of European science, and English is the recent lingua franca, the new lingua franca is software. Pieces of software are the filter, which connect researchers to the world, through layers of data. They assist in observing, in choosing, and in selecting. Open scientists should be aware of the fact that their autonomy in science depends on the quality of these pieces. Another lesson is that ethics—regarded as a source of innovative activities—must be a core component of innovative processes in OS, because society needs a responsible use of data and algorithms in corresponding practices that serve OS.},
  archive      = {J_DINT},
  author       = {Budroni, Paolo},
  doi          = {10.1162/dint_a_00077},
  journal      = {Data Intelligence},
  number       = {1},
  pages        = {52-63},
  shortjournal = {Data Intell.},
  title        = {About open science and autonomy of science},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Comments to jean-claude burgelman’s article politics and
open science: How the european open science cloud became reality (the
untold story). <em>DINT</em>, <em>3</em>(1), 47–51. (<a
href="https://doi.org/10.1162/dint_a_00076">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Coming from an institute that was devoted to analysing data streams of different sorts from its beginning to understand how the human brain is processing language and how language is supporting cognition, building efficient data infrastructures of different scope was a key to research excellence. While first local infrastructures were sufficient, it became apparent in the 90s that local data would not be sufficient anymore to satisfy all research needs. It was a logical step to first take responsibilities in setting up the specific DOBES (Dokumentation bedrohter Sprachen) infrastructure focussing on languages of the world, then the community-wide CLARIN RI (European Research Infrastructure for Language Resources and Technology) and later the cross-disciplinary EUDAT data infrastructure [1,2,3]. Realising the huge heterogeneity in data practices, it was also a logical step to start the Research Data Alliance (RDA) [4] as a truly bottom-up initiative to discuss harmonisation across disciplines and across borders. On this background, determined by always looking for concrete results, the European Open Science Cloud (EOSC) process had Kafka-esc characteristics to me, despite the many interactions I had with EOSC key persons and other colleagues involved. Talking at a level where the technological core remained widely absent was difficult to do for me. Due to Jean-Claude Burgelman&#39;s (JCB) excellent paper I finally understood that excluding the discussions about the core was the only chance to get EOSC accepted. Of course, the discussions about the EOSC core would have to happen at a certain moment and obviously eternal types of disputes would determine these discussions. Therefore, the fallback on the analogy with Greek tragedies was an excellent idea by JCB.EOSC has grown in bizarre times in so far as we are all using the global web as a primary medium for exchanging information on the one hand, however, recognising that the architecture for global data management needs to be changed on the other hand. Global players started the Data Transfer Project [5] indicating that even big companies are looking for alternatives, yet without admitting that metadata and identification will be key to success. Even T. Berners-Lee stated recently that “there is a feeling, a zeitgeist, that change is really overdue” [6]. Some believe that FAIR Digital Objects [7,8] are the way out to solve some critical core aspects of the evolving global data infrastructure, while others are preferring a smooth process of upgrading services or believe that we should leave solution finding to the big companies, since they have the money to enforce a big change.In parallel, we have engaged discussions in all sectors about the questions associated with data sovereignty: who has access to data, who will be able to sell services on data, will we in using data become dependent on monopolies again? EOSC was a natural policy response in Europe to these questions. It is also a policy response to other imminent questions in Data Science (DS): will we end up in Digital Dark Times, will DS lack all kinds of responsibility and accountability? We also realise that large communities such as the librarians that played such a great role in classical science are looking to find their place in this new digital data landscape and that countries want to claim leadership in specific areas of expertise and services. The landscape is highly complex and competitive, and until now researchers did not really play a primary role in these debates. In fact, deep insights indicate that researchers in the labs did not care yet about EOSC [9].To me having EOSC is indeed a great policy achievement for Europe and, for everyone visible, it put global data management as a priority on the agenda. As JCB admits, it is yet an empty box. While I was always tying to convince policy players to start discussing the technological questions at an early point in time, I now realised after having participated in some EOSC discussions, that due to the differences in approaches and interests it was indispensable for the key actors to leave the technological core in the dark as long as possible. After having read JCB&#39;s paper, it is the first time that I understood that Europe must be happy now to have a policy instrument, although it does not mean that Europe is ready to make proper advances in future. Due to decisions about the governance structure the “Brussels bubble” is now replaced by a bubble in which member state (MS) delegates and existing initiatives widely define the rules of the game. Delegates of these stakeholders often seem to mix short-term interests with arguments about technological strategies, making it hard to believe that wise decisions can be taken.The “EOSC emptiness” for me includes two primary and related questions: (1) What is the ambition behind all our investments, i.e. are we targeting for 5, 10, 50 or 100+ years? (2) Do we believe that smooth evolution of existing services will provide the solution for the huge challenges or are we ready to dare thinking about disruptive steps which implies more risks? EC and member states (MSs) are investing so huge sums in the coming decade that it is important to give clear answers to these questions. With the current vagueness of the goals, everyone can sell anything what is being done under the label EOSC, even scientifically useless portals supported by millions of Euro can be sold as EOSC contributions. This will not help Europe taking a leading role.George Strawn hints to one of the major paradoxes that we are faced with, when he argues that “standards are good for science, but bad for the scientists” [10]. Deep insights show that most researchers in the labs like to continue with the methods they are used to and only like to change, if they see clear short-term benefits. This implies that researchers, in general, will not push innovative infrastructure work which comes along with new standards, regulations and eventually disruptions. They were not interested in technological innovations such as TCP/IP despite its revolutionary character, they only became interested when new tools became available enriching the set of options for their research—an attitude which is acceptable given the pressure to show results. Until now EOSC discussions were far away from the researchers in the labs. EOSC was mainly discussed in a bubble of librarians, archivists, research infrastructure and e-Infrastructure officials being ready to sign declarations motivated in many cases by rather opportunistic attitudes. We should not interpret this as a critique since it is a real challenge to organise a broad discussion process about initiatives such as EOSC. And here I would like to slightly disagree with JCB: to me, EOSC until now is indeed an invention from Brussels if we include all stakeholders dependent on funding streams from EC. Since we need to accept that big innovative steps in infrastructure technology were, in general, pushed ahead by others than the mass of researchers, we need to rely on voices that take a strategic view combined with insights in technological trends. Therefore, I appreciate the driving role of the EC until now.Now, EOSC is a fact in European policy, but only wise decisions will make it a success and prevent another Greek tragedy. Starting the discussions about the EOSC core is not too late, since finding decisions about the core of global data infrastructures will take more time than I hoped when starting RDA, for example. Also, the FAIR principles were a huge step, but we start realising that they are not sufficient to serve as the solution.EOSC will have to overcome some of its weaknesses to be successful. (1) Although the creation of large infrastructures is of national interests, EOSC needs to find a way to separate technological discussions from political interests. (2) It needs to identify its ambitions—if all the funds will be spent to make small steps, it will not succeed, and decisions will be overruled by the next technology wave. Nevertheless, relying on a federated approach was a wise decision, since so much money has already been spent on building important components such as repositories. (3) Different approaches have been suggested, their potential needs to be investigated by investing reasonable sums in test beds and reference architectures and taking risks. Politics needs to have experts who explored the landscape of solutions. Having used the cloud metaphor in EOSC was probably a big error, since too many of my colleagues took it literally. Investing in cloud services, however, makes sense to make Europe independent with respect to large digital stores, but cloud technology is now very much known and does not address the core issues to be solved on the way towards an integrated and interoperable data domain (I2D2). (4) Given a decision about the scope of ambitions, EOSC needs to be able to organise a discussion process with widely independent experts to identify trends and evaluate the possible development of technologies.JCB refers to the GAIA-X project [11], but this initiative was also established as an empty box without conceptual views. It seems that also European industry does not have a clear view about the key challenges for establishing the I2D2 which we will need in a few decades.Finally, after having observed the EOSC process my question is whether Europe will be in the state of characterising the I2D2 domain and thus infer directions for investments that will be ground-breaking. We did not understand the core of the information wave and seem to not be able to organise a discussion process allowing us to anticipate the data wave and come to conclusions which is another paradox given the fact that European colleagues were and are pushing the agendas in RDA and around FAIR.},
  archive      = {J_DINT},
  author       = {Wittenburg, Peter},
  doi          = {10.1162/dint_a_00076},
  journal      = {Data Intelligence},
  number       = {1},
  pages        = {47-51},
  shortjournal = {Data Intell.},
  title        = {Comments to jean-claude burgelman&#39;s article politics and open science: How the european open science cloud became reality (the untold story)},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Comments to jean-claude burgelman’s article politics and
open science: How the european open science cloud became reality (the
untold story). <em>DINT</em>, <em>3</em>(1), 43–46. (<a
href="https://doi.org/10.1162/dint_a_00075">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {I have heard Europeans joke about themselves saying, “America innovates, Asia imitates, and Europe hesitates.” However, EOSC is a case where Europe is innovating while America is hesitating. I&#39;m a committed believer in Open Data (OD) and Open Science (OS), so I salute their leadership. And I appreciate Jean-Claude&#39;s giving us an insider&#39;s view as to how this leadership (and its associated challenges) came about. I had the good fortune of “being on the inside” in some of the projects which led to today&#39;s global Internet, so my brief remarks will focus on describing some of the salient features of the Internet development that may (or may not) apply to EOSC development.Jean-Claude says, “The track record of good European (high tech) ideas going nowhere because once a good idea is launched, making it a reality is obstructed by discussions on process, aggregation of particular interests, territorial fights, etc. is larger than its success record.”Jean-Claude and his colleagues had to fight political battles up front, whereas the Internet began as a political stealth project, because it was not perceived to be important at that time and only one agency funded its development. On the other hand, EOSC is widely perceived to be important and hence it is political. And as Jean-Claude said, political visibility may be the EOSC&#39;s greatest challenge. The Internet had gestation period of thirty years of US Federal agency support, but no interagency politics was involved. (Interagency politics in the US are similar to interstate politics in the EU.)This gestation period was comprised of three sequential “middle-out” projects (middle-out meaning single agency projects). The EOSC implementors are planning for a much shorter gestation period. As such, they have compressed into one top-down EOSC project what the Internet experienced over thirty years. I&#39;m not sure we know enough about Data Science yet to meet a very short timeline. And I hope that the EOSC effort will engage more data scientists than data users. Otherwise, EOSC will be at best an incremental improvement.Jean-Claude also says, “Democracy and inclusiveness come with a rightful price to be paid, but it should not be at the cost of immobilism or non-delivery by watering it down.” Unfortunately, that&#39;s often the challenge when the politics must precede the science.In the 1960s, APRA (now DARPA), an agency in the US Department of Defense, had cart blanche to think long-term, money to invest in long-term projects, and protection from interference by the Department of Defense. The original goal of the ARPAnet was to allow any terminal in the ARPA offices to connect to any computer, regardless of which vendor, in their contractor&#39;s offices. This was impossible at that time, because computer vendors preferred lock-in. The first version was “quick and dirty” as is often a good idea for software developments because it is not clear until after done what or how you should have implemented in the first place. The second version of the ARPAnet software was defined by the TCP/IP protocols, which have stood the test of time. The ARPAnet functionality centered on three general services: remote login to computers (today called apps), file transfer (today called the Web), and email (today called email and texting). The ARPAnet had no specific applications in mind beyond the network research required to implement these three general services.I would hope that EOSC will also seek to provide a few general services, such as making data findable, accessible, interoperable, and reusable (FAIR). And perhaps those services will be made available by developing novel, disruptive technologies. The Internet projects created the disruptive technology of packet switching. The telecommunications industry did not recognize packet switching as a threat to their long-established mode of circuit switching until it was too late. Might a similar disruptive technology emerge from the EOSC project?Jean-Claude says, “Policy makers do recognize disruption relatively quickly but are quite often only prepared to tackle disruption via policies that guarantee continuity of business.” But, reporting what a commercial data provider told him, “If EOSC will exist, … the private players will be asked to subcontract capacity (hosting or services) as we are amongst the only ones able to provide it. So, that earns business for us.” Moreover, as I will say below, those data providers will probably want to extend the EOSC to an EOC (an open cloud for more than science).In an apparently unrelated development, a high-level committee of US scientists recommend in 1983 that the Federal government provide expanded access to supercomputers for US scientists, because both Europe and Asia were doing a better job of providing such access for their scientists (another case of the US hesitating)! NSF accepted the obligation and established six supercomputer centers across the country. As a “corollary” to the supercomputing initiative, the scientists recommended that a computer network should be established to interconnect those centers with the 100 US research universities. At that time, the only functioning vender-neutral network was the ARPAnet, so decision was made to develop an “NSFnet” based on the TCP/IP protocols. Thus the NSFnet came into being as a very special purpose network. In my opinion, there would have been no support, policy or financial, for a general purpose network for higher education at that time. That support came only after the NSFnet was installed and working.The EOSC project is very different in this regard, too. It is recognized as important and EU support demonstrates that. It is too late to recommend that they begin this project twenty years ago! And even if we could turn back the clock, there would not have been broad support for the EOSC in 2000.However, looking ahead to the next innovations, The EU might want to consider establishing its own ARPA-Europe (advanced research projects agency). At least two other US agencies have done so, and so is the UK①. An ARPA-Europe might very well have had the foresight to begin work leading to the EOSC in 2000.NSF designed a three-tier architecture for the NSFnet. There was a nation-wide backbone network, regional networks, and campus networks. The regionals interconnected the research universities and the supercomputing centers in their geographical region and provided them a connection to the national backbone. Because of this architecture, the regional networks were able to allow private sector companies to also join, subject to an “acceptable use” policy stating that they could only use the network for open research. At the same time, NSF offered small awards for other colleges and universities to connect to the regional networks. The result was that by the early 1990s there were approximately 1,000 connections to the NSFnet, well beyond the original goal of 100 research universities. More surprisingly still, in 1991 the number of non-university/college connections was equal to the higher education connections. At this point, private sector entrepreneurs could see that the Internet was going to be important and began lobbying Congress to “get the government out of their way.”Over the ten years of NSFnet&#39;s existence, private sector partners had provided the fiber cables and developed its routers, so part of the private sector was fully engaged. And NSF was working hard to “get out of the way” by re-architecting the network for privatization and commercialization. So in 1995 when the NSFnet was retired, there were “trained” corporations ready to meet the national demand which exploded that year. And as everyone knows, by the year 2,000 the Internet was well on the way to becoming the network infrastructure for the world.Regarding the EOSC, I suspect that private sector companies will also want to use the services provided by it, which would enhance its usefulness and provide additional support. I hope that policy will allow the universal use of EOSC in the long run, just as the Internet use became universal.},
  archive      = {J_DINT},
  author       = {Strawn, George},
  doi          = {10.1162/dint_a_00075},
  journal      = {Data Intelligence},
  number       = {1},
  pages        = {43-46},
  shortjournal = {Data Intell.},
  title        = {Comments to jean-claude burgelman&#39;s article politics and open science: How the european open science cloud became reality (the untold story)},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Comments to jean-claude burgelman’s article politics and
open science: How the european open science cloud became reality (the
untold story). <em>DINT</em>, <em>3</em>(1), 40–42. (<a
href="https://doi.org/10.1162/dint_a_00089">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Is the evolution of how science is done primarily heuristic or driven by policy makers? Reading the intriguing story of the inception of the European Open Science Cloud (EOSC) by J-C Burgelman one could assume the latter. But the story also reveals that the policy makers&#39; shaping of concepts and planning of funding models is a mere reaction to the turns of the broad stream of research. Looking from the science side the trend towards data intensive science started for sure already in the 90&#39;s with advancements in particularly sensor technologies rapidly increasing the amount of data collected. The development of Charge-coupled Devices (CCDs) and its impact on astronomy is the often-cited example but it was a trend building up all over experimental research taking benefit of developments within fast electronics and communication, or leaps in efficiency when shifting experimental technology, e.g. going from chromatography methods to fluoresces based methods for genome sequencing. On top of it the introduction of parallel supercomputers almost completely built out of commercial off-the-shelf components enabled the possibility to actually analyse and produce data in an unprecedented cost-efficient way.The resulting obvious need to combine and interoperate various networked resources was met by computer science and maybe the most successful work was concluded in the “The GRID, Blueprint for a New Computing Infrastructure”①. This result of work during the 90&#39;s by Foster and Kesselman became immensely popular even in industry. Though, on the other side of the IT-bubble industry took another route. Introduction of Service Oriented Architecture (SOA), Web-services, and cloud computing became the industry standard implementation of the ubiquitous computing promise in “The GRID”. Soon it also became part of the researchers&#39; toolbox. Meaning that more complex workflows could be realised in connecting remote experiments, data resources, computing facilities, and whatever needed to reach the objectives of and advanced the research enterprise.The European Commission supported a large number of Grid-projects during the first decade of the millennium and the technology and concepts were particularly adopted by the high-energy physics community which still use them for analysing and simulating data related to the Large Hadron Collider (LHC) through a global network, or grid of sites.It deserves a more careful investigation but it is my feeling that particularly the life-science community was leading in taking another route and jumped on the train of SOA, Web-services, and cloud computing when it started to move with some speed somewhere after 2005. The inherit need of the specific research and the smart utilisation of the available technologies have resulted in a very well organised community when it comes to gathering of common open core data resources; sharing results that are findable, accessible, interoperable, and reusable; and very much of everything else that we are seeking with EOSC. In this aspect, a life-science, or definitely a bioinformatics EOSC implementation is already in place and also heavily used by the pharmaceutical industry which can access many of the core data resources at the same terms and conditions as publicly funded research.What is the point in looking backwards for some more than a decade old advances? Just to show that where we are now is not a new place. Yes, we experience another turn in how research is done and we are now looking for a societal response to that change. How can the society at large support, but also take optimal benefit of where research is right now? EOSC is, if we succeed, part of the answer, but we need to make sure that it can stay creative and innovative in enabling opportunities for research and society at large to thrive together.High-energy physics and life-sciences are mentioned as areas driving the development of concepts for enabling of research, but examples can be found elsewhere. More important is what research areas and research needs will show the direction for the next leap, maybe humanities or environmental sciences? I would put my bet there somewhere but the important message is that inspiration and innovation will, and must come from the research needs. Only then can growth minded policy makers, in their best moments, accelerate the development by introducing new frameworks and steering of funding, that will ultimately benefit research and the whole society.},
  archive      = {J_DINT},
  author       = {Öster, Per},
  doi          = {10.1162/dint_a_00089},
  journal      = {Data Intelligence},
  number       = {1},
  pages        = {40-42},
  shortjournal = {Data Intell.},
  title        = {Comments to jean-claude burgelman&#39;s article politics and open science: How the european open science cloud became reality (the untold story)},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Comments to jean-claude burgelman’s article politics and
open science: How the european open science cloud became reality (the
untold story) —“EOSC is a bigger ME” and the dunning kruger effect.
<em>DINT</em>, <em>3</em>(1), 32–39. (<a
href="https://doi.org/10.1162/dint_a_00074">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This personal reaction is written from multiple perspectives. First and foremost, as the corresponding author of the original FAIR article. Second as the chair of the first High Level Expert Group (HLEG) of European Open Science Cloud (EOSC) (which is how I met Jean-Claude) and third from my current GO FAIR and CODATA perspective. None of what I write below is to be seen as a formal position of any of the organisations I am associated with.Let me start by stating that, after some periods silent of hope and of deep despair, I now strongly feel that, with the governance of the EOSC Association in place, EOSC will become a success after all. It will still be critical that the Association involves the member states (MSs) and actual researchers in an agile and non-bureaucratic manner, for which we need bottom-up mechanisms such as operated by the Research Data Alliance (RDA) and GO FAIR. But a balancing formal entity operating along the formalised Strategic Research and Innovation Agenda [1] and the Partnership proposal as well as the various “declarations” including the recent one under the German presidency [2] are an excellent guiding roadmap to a successful EOSC, obviously in global context.That said, at the risk of sounding like broken record, this reaction should also look at the points where it went “almost” wrong, as we should try and learn from our mistakes. I may make some enemies—or strengthen the opinion of existing ones—in the process, but then, a wise old friend, who also wrote one of the reactions once told me: “Barend, unless you made some enemies you probably lived in vain.” So I will speak my mind (“what&#39;s new&#39;?). I also like to say that ”EOSC“ brought me some real new friends for life!First of all, the fact that quickly after its inception FAIR became a hype term①, which was probably partly even accelerated by the prominent role it played in early EOSC discussions with EC&#39;s Director General, also has its downsides. Like for the term “AI”, everyone co-opts the term and some start watering the concept down to a bloodless caricature from what it originally meant. In the case of FAIR this includes removing the central notion of machine actionability, mis-characterising it as a standard, conflating it with “open”, only linking it to data sensu stricto, ignoring software, algorithms and more. In general terms, people that sometimes seem to have never read the original article [3], the most flagrant abuse of the term I have heard (obviously not from an active researcher) is this: “If data are Findable, Accessible and Interoperable it is ”automatically&#39; Reusable.“ This is of course ”swearing in FAIR church“ as the R (principles R1–3) [3] clearly state that rich provenance and reuse conditions are critical and in particular the provenance. The decision whether (even high quality) data are fit for purpose (reuse in a particular study) is a critical step and is imho (in my humble opinion) at the basis of the reproducibility problem we currently face. Therefore, I would like to re-emphaisize here my current one liner to summarise the aim of the FAIR guiding principles: ”The Machine Knows what I mean“. Those who feel that FAIR is too ambitious and for instance promote that ”achieving F and A is enough for now“ in my humble opinion fail to see the disruptive character of the solutions we need to make EOSC and its sister around the globe a real paradigm shift towards Open Science (OS). Or they are just trying to preserve the status quo and move incrementally at a pace they can follow.This nicely bridges to the first observation on EOSC as such. I indeed think that the first “Communication” that needed 126 iterations mentioned by Jean-Claude, which happened in the same time frame as our “HLEG-1” period, was symptomatic for a basic flaw in the discussions, which haunts us still today. Conflating the “ICT”/HPC (or basic e-infrastructure) with the data and end user applications for analytics, has caused an enormous hurdle. In the entire journey of the HLEG we had to carefully navigate around this cliff and it is still a highly controversial issue today. This part was the “Dunning Kruger effect” [4] pur sang: The “other side is easy” (because I am not hindered by any knowledge about it) and is “more or less already done” (because I do not understand the complexity). This is not only true for the active researchers who cannot use the current e-infrastructure efficiently (and naturally that is “entirely the fault of the nerds who build things I do not understand or cannot operate”), but also for e-infrastructure engineers who know everything about ICT and “thus” (?) also about data (because “that is just ones and zeros”) as Jean- Claude also noted. I also believe however, that it is a mistake to completely separate e-infrastructure for the data and services layer, as the e-infrastructure should route (and understand at least at middleware level) what processes are needed on the data and how the FAIR services “run”. Nowadays (after many iterations) I use the diagram below (Figure 1) to explain that all three basic elements of the “Internet of FAIR Data and Services” are needed. Each of them should be adorned with FAIR (machine actionable) metadata to seamlessly form a Web of FAIR Data and Services on top of the current, proven Internet backbones, thus forming the “Internet of FAIR Data and Services”, eventually creating an “Internet for Social Machines” [5] where people and machines can both efficiently use all services, independently and in collaboration.This does absolutely not mean that the foundation (e-infrastructure) of the triangle is “trivial” or “can be reused as is”. Not only middleware, but also the crucial and fundamental concept of FDOs needs to be developed in close collaboration between data and computer experts and is largely domain-agnostic.The seamless combination will become the principle “package” of information that machines (and also people) can understand and act upon. Major infrastructure builders should actually co-lead this, while domain scientists need to decide on which data formats and metadata schemes (i.e. FAIR Implementation Profiles [9]) should be built on this basic schema.Together with the Dunning Kruger effect, too many overlapping and redundant projects supporting the talking/meeting/landscaping, re-landscaping and re-re landscaping&#39; has resulted in what I became to call the “EOSC is a bigger Me syndrome”. On the one hand, countless people voluntarily invested (and still invest) their time in the development of the EOSC, but others seem to only see EOSC as “yet another way to collect EC funding for their current solutions that are in my opinion not future- and OS proof. This misbalance between people investing their own time and effort based on intrinsic motivation and vision and on the other hand the ”reliance on EC subsidy“ caused a dichotomy during the scoping years of EOSC between disruptive and ”preservative“ approaches. The heavy reliance on EC subsidy also largely ignored the subsidiarity principle [10] and the fact that 90\% of the eventual infrastructures and services that we need for EOSC will be paid by the MSs. Also data and research intensive industry was largely kept out of the loop, which was another mistake I have frequently pointed out. This helped to create and sustain the ”Brussels Bubble“ that Jean-Claude described. The Association will hopefully reverse that trend.Finally, the influence on the HLEG report of the then-commissioner was rather profound. The report was not only delayed almost 6 months after its proposed publication version, but there is also a nice additional “untold story” here: The originally proposed title of the report was: “A Cloud on the 2020 Horizon”. In my original foreword I explained the slightly “glooming” connotation of that title. When the report was finally approved, it appeared that the title had been unilaterally changed into “Realising A European Open Science Cloud [11]”. Not only did I have to hastily change my foreword (because it made no sense anymore) but also, my notorious statement that the “result” should neither be “European” (only), nor Open (only) nor (only) for Science and certainly not (just) a “Cloud” was entirely ignored in changing that title. But it again emphasises the “This is an EC thing” context, with the associated risk for confiscation of the concept by the “usual suspects” in EC subsidy land. However, I feel after three years of intensive deliberations, which may be considered lightning fast on the geological time scale, see George&#39;s reaction, we can conclude that most of the original HLEG recommendations are well-represented in the basic guiding documents of the EOSC Association, which makes me a happy man at the end of this crazy year.That leads me to the final observation: As a result of the (quote from Jean-Claude): “non-paper seen as the political turning point in support of EOSC” [12], GO FAIR (Global Open FAIR) [13] was started, originally by Germany and The Netherlands and soon joined by France as a temporary “kick-start”, bottom-up approach to accelerate EOSC (see also recommendation I-2.1. in the HLEG report, annex 1).Soon, GO FAIR became really global and the agile modus operandi of practical Implementation Networks yielded a number of crucial approaches to speed up the adoption of the FAIR guiding principles and the hourglass approach [14]. Now, late 2020, when the EOSC Association is a fact, GO FAIR (1.0) has achieved its goals (early implementation steps) and we need to reflect on its future. Next to the intrinsic value of the active GO FAIR IN community [15] as such, several particular assets that I need to mention here are the development of the FAIR Implementation Profile and Metadata4Machines approach, the development of easy to install FAIR data points for open, FAIR metadata publication and indexing, and last but not least the international effort (involving many players, also outside the direct GO FAIR initiative) to develop the minimal specs of the FDO framework [7] in a more specified form than when coined in the FAIR expert group report [5]. These assets (all open source and open access) can be carried over, not only to EOSC, but will have much wider, international, impact most likely leading to a continuation of GO FAIR (2.0) beyond its original time scope, namely three years, the predicted time it would take to complete the international policy and bureaucracy process to reach the status of a formal association as we have today. I hope the leaders of the Association will optimally learn from the successes and failures and near-road-accidents of the last three years and see EOSC as the European contribution to a “Global Open Science Commons”, also known as the Internet of FAIR Data and Services, in full, open collaboration with the international organisations that are now joining forces in the Data Together initiative [16]. After all, the major challenges we face are global, so is the research needed to face them and so are the solutions we hope to fiend. I fully trust the current leadership of the association to make that vision reality.Policy recommendationsGovernance recommendationsImplementation recommendations},
  archive      = {J_DINT},
  author       = {Mons, Barend},
  doi          = {10.1162/dint_a_00074},
  journal      = {Data Intelligence},
  number       = {1},
  pages        = {32-39},
  shortjournal = {Data Intell.},
  title        = {Comments to jean-claude burgelman&#39;s article politics and open science: How the european open science cloud became reality (the untold story) —“EOSC is a bigger ME” and the dunning kruger effect},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Comments to jean-claude burgelman’s article politics and
open science: How the european open science cloud became reality (the
untold story). <em>DINT</em>, <em>3</em>(1), 29–31. (<a
href="https://doi.org/10.1162/dint_a_00073">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It has been both a pleasure and an agony to read the story and the drama behind the genesis of the European Open Science Cloud. Especially as many of us have been involved it its shaping in some way or the other from the beginning. OpenAIRE has been a key driver in open access in publications since 2005 and, while this is still a core goal, we have gradually shifted efforts to open research data from 2012 onwards.Embracing Open Science (OS) and therefore EOSC has been a no-brainer. Regardless of whether we agreed at times with the process, the many unknowns along the way, the key people involved, or not involved, we fully recognize that EOSC, both as a concept and as a policy initiative, has succeeded in mobilizing European players on a common goal: OS. Using it as a means to boost collaboration and innovation, it has helped policy makers place data driven science within the wider national agendas for digital transformation. In concrete, measurable terms.So, kudos to Mr. Burgelman and his team for their relentless efforts to bring this together: overcoming obstacles, ignoring practical challenges during the design phase by abstracting them, with the final goal to get on board the high-level policy makers (top-down) and the infrastructure community (bottom-up) at the same time. Not an easy task to create, shape and sell a vision that different groups, coming from different perspectives can relate to, especially in a European environment which often resists to changes.Having now reached a safer place, some reflections from the “community” may be of interest and value:The grandness of the name seemed to do its work and have an effect for the high-level political support, as Brussels seems to mobilize for grandiose visions. It has also done its work for attracting international attention. At the same time, the all-encompassing title has often confused the community on what this really is about. Is it a cloud, or not? Is it about Open Data (OD)? Or only about FAIR? Is it about research data or does it include other research results like open access publications and software? In the end, it seems it is about everything, depending on where someone is coming from. Retrospectively, a more appropriate name would be to substitute Cloud with Commons.Managing expectations so they don&#39;t manage you. Expectations have been set very high from the beginning. The EC set the mood for a “sail or fail” initiative. Often with a haste to make it happen before the community had time to digest and position themselves in the emerging environment. As the narrative shows, it was indeed a sail, but into rough waters at times, needing navigation skills, strength, desire, clear destination. All were there, apart from the latter. And as the destination was not, or is not yet clear to all, expectations varied upon who you talked with. In numerous presentations to non-initiated groups, I tried to respond to questions of the sort “when will be EOSC ready to use”, making it clear that they perceived EOSC as a free-at-the-point of use cloud platform with some value-added perks. In contrast, the familiar crowd (infrastructures) has kept expectations low, entering EOSC with eyes wide open and a fair share of scepticism, as in many top-down initiatives. Why? Because infrastructure providers know that commitment and continuous funding is of key importance to make such a grand initiative work.The decision to build on existing infrastructures and investments has correctly been there from the beginning. Before EOSC appeared on the scene, many pan-European research and e-Infrastructures, OpenAIRE one of them, had already worked with this model: building on national infrastructure and investments; federating research results, resources; building and operating human networks who are key in aligning policies and propagating practices. All in a bottom-up manner. Trying to meet the top-down in rather unknown waters, often produced more confusion in the community.Balancing different views of FAIRness and openness have been there in the OpenAIRE community from the beginning. The discussion on openness has been diluted along the way. FAIR has been proven to be an excellent marketing tool, easily digestible as a concept, but hard and costly to implement in its entirety for the whole data lifecycle. So most often the discussion stays in the context of metadata. Even if we anticipate FAIR to pay in the long term, open research data with an acceptable quality will bring quicker uptake and return of investment (RoI). So it is important that as a starting point EOSC redoubles Europe&#39;s efforts to make more and better quality data available for public research at no cost.Labelling EOSC as the “Web of FAIR research data” limits its long-term vision as a facilitator for harnessing European data intensive research. It may be sufficient for the starting years, where we need to bring the community together under common understanding and simple rules. But data aren&#39;t a set-it-and-forget-it thing. Its real value comes with actionable delivery. Therefore, we cannot afford to have data and analytics worlds collide. EOSC needs to turn this collision into a constructive convergence, incorporate both data and analytics tools and capabilities into its stack. Fast forwarding into the (near) future, EOSC should facilitate the Data as a service (DaaS) paradigm: having data in the forefront, to implement a data management strategy that uses the cloud to deliver data storage, integration, processing, and/or analytics services via a network connection.The 6-year drama has just been the beginning of the EOSC journey. The community has, or is in the process of, fully embraced it. EOSC has put Europe in the forefront of OS globally and all eyes are on us. The work on the policy has provided this boost, and now it is in our hands to make it happen. And, we cannot afford not to.},
  archive      = {J_DINT},
  author       = {Manola, Natalia},
  doi          = {10.1162/dint_a_00073},
  journal      = {Data Intelligence},
  number       = {1},
  pages        = {29-31},
  shortjournal = {Data Intell.},
  title        = {Comments to jean-claude burgelman&#39;s article politics and open science: How the european open science cloud became reality (the untold story)},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Comments to jean-claude burgelman’s article politics and
open science: How the european open science cloud became reality (the
untold story). <em>DINT</em>, <em>3</em>(1), 26–28. (<a
href="https://doi.org/10.1162/dint_a_00072">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It has been genuinely a pleasure reading through Jean-Claude Burgelman&#39;s draft. It provided me with clear and straight forward narratives as responses to questions regarding the origins of EOSC.I want to start formulating a response to this draft by recognising the good place EOSC is today. A strong mission statement and a realisation that EOSC goes above and beyond the technical aspirations its name actually suggests. Today, EOSC concept and implementation actions clearly and powerfully encompass the sum of the socio-technical components that form unique value chains across data-driven science, from ideation to impact. These components are predominantly delivered by a multitude of scientific disciplines, through scientific communities, their respective practices and linked technical solutions supporting data lifecycles. It is probably the involvement of these communities, not just as users of EOSC but as core stakeholders, that helps EOSC transition from an opaque political statement to a tangible socio-technical ecosystem of people, data and services for science.Looking back at JCB&#39;s opening, I cannot but express gratitude and admiration towards JCB and his colleagues for realising early the forthcoming transformation of scientific practice, in response to the unprecedented production of scientific data. Also, I recognise their commitment and perseverance to place this topic high enough on the agenda of science-policy for Europe. JCB refers to a series of workshops that had taken place in 2014 and cites unanimity around “the key problems of this explosion of data science will be: non-discriminatory access to the data, interoperability across disciplines and making sure these data are managed with respect for European sensitivities”. As such, it is highlighted that the main bottlenecks for effectively leveraging European scientific resources in the new big Open Data (OD) era would be mostly socio-technical. Despite early and explicit messages that flagged the need for a more inclusive and broader conceptualisation and implementation strategy for EOSC, the years after its first inception were predominantly focused (through linked investments) in developing the underlying e-infrastructure commons. An attempt to federate existing national and European public infrastructures that provide computing and storage services. The consolidation of the highly fragmented landscape of e-infrastructures assumed a central role in the vision of EOSC.JCB states that the predominant focus in developing the idea of EOSC was to “get the end-users on board, the scientists”. I would argue that if the investment was geared towards getting the end-users on board the concept of EOSC, that probably did not yield the results needed at that point. By 2017, the European research landscape had already made significant investments in developing their discipline-specific Research Infrastructures (RI). Organisations that were driven by the needs of the communities and supported by member states (MSs). In 2017, we already had tens of those RIs in preparation, construction or operational phase. These RIs managed to address their respective communities&#39; needs, providing valuable end-user services, whilst making use of a multitude of underlying computing, storage, and data resources. As such, they quickly turned into major de facto stakeholders of EOSC. In our article back in 2016 (Koureas et al., https://doi.org/10.3897/rio.2.e9933) we specifically touched upon the socio-technical distance between end-users and e-infrastructures. We referred to solutions that would allow communities of practice to benefit from a central e-infrastructure service bus, using community-driven intermediate structures (such as research infrastructures or virtual research environments) to bridge the gap, a model that would resemble the value chain of wholesale (e-infrastructure commons) -&amp;gt; retail (community infrastructures) -&amp;gt; user (researchers) approach.JCB lists, in a remarkably interesting way, the steps taken to bring us in the current realisation of what EOSC can be and offer. EOSC is, in its inception, an innovative concept, which would partly disrupt current established practices and hopefully improve the efficiency of data-driven scientific practices. As such, it was deemed to go through its own hype cycle. Though it is still difficult to assess whether we are still in the troughs of disillusionment or climbing the slope of enlightenment, we can for sure assert that the peak of inflated expectations was primed with a misguided understanding that the success EOSC hinges on the orchestration of distributed technological solutions towards end-users. As a wider community of stakeholders, we have failed to fully realise that the equation from technological capacity to scientific innovation is heavily nuanced between communities. Aspects related to community traditions, established practices and trust mechanisms play a pivotal role in completing each of those equations. In the next steps in completing the EOSC mission, we ought not only to acknowledge these domain-specific aspects but encompass them and work with them to improve the ability of researchers to benefit from the investments in EOSC.},
  archive      = {J_DINT},
  author       = {Koureas, Dimitris},
  doi          = {10.1162/dint_a_00072},
  journal      = {Data Intelligence},
  number       = {1},
  pages        = {26-28},
  shortjournal = {Data Intell.},
  title        = {Comments to jean-claude burgelman&#39;s article politics and open science: How the european open science cloud became reality (the untold story)},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Comments to jean-claude burgelman’s article politics and
open science: How the european open science cloud became reality (the
untold story). <em>DINT</em>, <em>3</em>(1), 24–25. (<a
href="https://doi.org/10.1162/dint_a_00071">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {I appreciated Burgleman&#39;s entertaining description of how European Open Science Cloud (EOSC) became a reality. The paper tells the story from the “Brussel Bubble” perspective, to use the author&#39;s terminology. In doing so, he sheds light on many important aspects of the path to the necessary policies and stakeholder engagement around EOSC.I recognise many of the discussions, fears and frustrations along the way. When I first heard of EOSC I must admit that I was sceptical too, like many other policy makers Burgleman mentions. The vision of EOSC was not as clear as it is today, it was cloudy. There were many different types of arguments on why Europe needed EOSC and what it was supposed to achieve. After years of discussions and iterations, I think that what Burgelman refers to as co-creation has resulted in objectives that many can agree to and align with.EOSC rapidly gained a lot of momentum and support from the EC and the Council but it took longer for the broader group of stakeholders to recognise its value. There came a tidal wave of expert opinions about Open Science (OS), where many researchers were critical to the whole idea. It is and was of course essential to have the researchers on board, but EOSC was not an obvious bottom-up initiative. It was disruptive and was inherently going to be met with some resistance. Researchers feared, perhaps some still do, that EOSC policies would force them to give away their research data and produce piles of unusable but Open Data (OD). If it hadn&#39;t been for the FAIR principles, I think we still would have been busy discussing such topics today. However, I recognise that those who implement EOSC still have a huge task (and responsibility) ahead to ensure that the initiative reaches its full potential.Burgleman claims the single biggest mistake was misjudging the democratic basis that underpins the decision making in each member states (MSs). The processes at national level are different to those of the EC. MSs must take a much broader perspective, considering the society at large in their countries and the priorities of their constituents. This means that they do not always end up drawing the same conclusions as a group of experts would do. To me, this is one of the key reasons why EC and MSs often need iterations of discussions to finally arrive at something that everyone can agree on. The differences in perspective are very valuable and I think EOSC needs both the democratic and expert component to be truly successful.Thousands of stakeholders and policy makers have been involved in getting the EOSC ball rolling. The establishment of the EOSC Association marks a big milestone as the vision now has a formal platform where the stakeholders are working hard to detail the objectives and priorities. The value of EOSC and the work of its community has already been shown during the Corona pandemic, where researchers are sharing their findings to more rapidly find a cure.As excited as I am to follow the implementation of EOSC, I am equally excited to discover the spill-overs from the initiative. CERN was built to create peace and better research, but also led to the invention of the World Wide Web. EOSC sets out to create a Web of FAIR data and enable better data intensive research. Perhaps it will help us find a path to data sovereignty that increases collaboration and thus leads to stability and peace in a rapidly changing world.},
  archive      = {J_DINT},
  author       = {Khayyeri, Hanifeh},
  doi          = {10.1162/dint_a_00071},
  journal      = {Data Intelligence},
  number       = {1},
  pages        = {24-25},
  shortjournal = {Data Intell.},
  title        = {Comments to jean-claude burgelman&#39;s article politics and open science: How the european open science cloud became reality (the untold story)},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Comments to jean-claude burgelman’s article politics and
open science: How the european open science cloud became reality (the
untold story)—the twin challenge of the hard-core change and the
cultural shift. The role of the chorus in the greek drama.
<em>DINT</em>, <em>3</em>(1), 20–23. (<a
href="https://doi.org/10.1162/dint_a_00070">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In 2014 I have ended my “decade mirabilis” in the European Parliament, after the European Enlargement in 2004, when ten countries entered to the European Parliament. I had a pleasure to work on several industrial and technological and research related legislations. The transitions due to the changed legislation show that the most difficult element of the transition is always twofold: To be aware of the changed legislation is difficult, but the cultural shift is a real challenge.I remember the sunny day in Washington in July 2014, when John Wood, then Council member of the Research Data Alliance (RDA) explained to the transatlantic legislators (Congressmen and Members of the EP) the future of science and research, due to Open Data (OD). Besides John&#39;s enthusiasm it was a communication failure as it was too technical for most of the audience. The event had two consequences: (i) RDA issued a paper “Harvest the Data” with the support of Science Business, to explain the value of data to the policymakers (civil servants and politicians), (ii) I became a profound believer of OD and Open Science (OS) as it combined the two challenges of my experience as Member of the European Parliament: Technical and Cultural twinning of the fundamental challenges ahead, occurring all over the world. For most of the professionals, data and data management was the burden, and data sharing was not even part of their tasks. However, like always there was a small group of visionary people driving the change. In the duration of the six years covered in the article of JC Burgelman, I found a new mission and put lots of voluntary efforts to support those pioneers. My vision was not simply to achieve the gradual change: my vision was and is to achieve the “data scope” which is like the microscope or the telescope in the past, opens new research fields which were not possible before. (The professionals call it interdisciplinary research). I am very honoured to serve the RDA community as a Council member since 2016 and being the European Co-chair for the FAIR DMM WG① to build global compromise around the FAIR maturity criteria as a critical step to the future.Jean Claude Burgelman&#39;s article is an important chapter of the OS history as not simply an insider but the main actor of the Greek Drama. Following his narrative, I will write my observations about the real main actor of any Greek drama: the Chorus. The Greek Drama is driven by the Chorus, representing the community, led by the coryphaeus, often more than just one. The face of the chorus members is covered by a mask to impersonalize the faces and allow for different roles. The chorus divides the different scenes and moves the story forward. In the EOSC drama the different camps and leading personalities were the chorus and coryphaeus.Chorus 2016. Parallel to the Amsterdam declaration RDA organised the conference in Amsterdam, where Barend Mons, Eva Mendez, Sandra Collins and myself were in the panel moderated by John Wood. The chorus was divided intwo parts: Some argued for fast revolutionary change turning to “machine actionable” data, while others argued for an evolution, moving slowly by sorting out the most important data challenges ahead. This dichotomy is a main characteristic of the OD movement. There are progressive value-based groups and personalities in both camps. (I must admit I could also observe some transactional thinkers, saving or creating status quo for their benefits too).Chorus 2017. The first EOSC Summit was the first time the researchers exchanged views with the European Commission, including Commissioner Moedas, Director General of DG RTD, Robert Jan Smits and the respective Directors from DG RTD and CNECT. The dichotomy between the chorus: doers arguing for “moving and doing” versus the perfectionists “do not move before we know all details” was crystal clear. In my contribution I used the analogy of Christopher Columbus: if he would know all the details of his trip, he would not discover America. (Today the Minimum Viable Product is largely used and known② but was less known in 2017).Following the EOSC Summit, the 140 organizations signing to the Declaration, along with the Amsterdam declaration of the political leaders, was sufficient to prepare the next scene of the drama.Chorus 2018: The community started driving EOSC through several H2020 projects, both horizontal and domain specific. It is the moment when the first High Level Group report, also known as the FAIR principles started to enter to the daily narrative of the researchers. The EOSC events were populated. The vivid discussions had two main directions:Chorus 2019: The two processes followed their own paths. The EOSC Executive Board has got a strategic ally when the Governing Board has been created representing the policymakers from each EU country. In addition, the Working Groups were populated by large number of professionals engineering the different elements of EOSC. At the same time, the RDA FAIR data maturity model Working Group started with full speed to elaborate FAIR maturity criteria. To do it in the bottom up driven global method with the editorial support from the EU we could analyze the fifteen existing FAIR assessment methods (developed in regions and domains). The third element of the chorus was the OS movement, converged in the OS Policy platform, several national initiatives, and European projects. It quickly become clear that the change in culture, will be one where the dividing interests were difficult to be aligned, and where not only the resistance but the risk of opt-out will be present. Here the transition takes longer time and facilitation of the transition remains a future task at all levels.Chorus 2020: The EOSC EB and GB with the support of the community in the projects and working groups achieved its milestones to agree the organizational model and to establish the legal entity. The Strategic Research and Innovation agenda approved by the legislators is foreseen. The RDA FAIR DMM WG achieved its milestone to publish the recommendation. We could agree on the FAIR Maturity criteria (several criteria for each letter of FAIR). The two processes met again in the EOSC FAIR WG metrics report led by Francoise Genova, where the first set of the FAIR DMM Maturity criteria became the EOSC metrics criteria. The WG moves to the next stage as a Maintenance Group to further work on the criteria, and probably revisit the principles. It should be done bottom up and only periodically changed, every two or three years only (depending on the need).The year 2020 is a “annus miserabilis” due to COVID 19. The FAIR DMM criteria recommendation was ready just in time to be used when defining rules how data can be collected to fight the pandemic.We are not in the beginning of the end, but at the end of the beginning. To reach out the 1.7 million researchers, to include social sciences, to make the change at global stage are the next priorities.Six years are long when you are following it day to day. Comparing to any other huge transitions, it is the shortest transition I witnessed. Once again it has been proved that parallel twin transition of professional expertise and societal acceptance is difficult but possible. As the structure of the Greek drama obliges: The main acts and actors interrupted and advanced by the Chorus.As a tribute to JC Burgelman, I quote GB Shaw in his play Mrs. Warren&#39;s Profession (1893): “People are always blaming circumstances for what they are. I don&#39;t believe in circumstances. The people who get on in this world are the people who get up and look for the circumstances they want, and, if they can&#39;t find them, make them.”This article is a tribute to the chorus. All members of the chorus. Those who were the frontrunners, agreeing or disputing time to time but common in their constructive approach. Those who are in the EU and those who are around the world to push for change. Those who made a mass work in all formations formally and informally, some of them paid, but most of them volunteers. Those who originally resisted, but turned out to be supporters or at least became less vocal. Those who are in the beginning of their OS journey. Those who will define the future to come. And those who will benefit from the change. The society, the raison d&#39;être of everything we do.},
  archive      = {J_DINT},
  author       = {Herczog, Edit},
  doi          = {10.1162/dint_a_00070},
  journal      = {Data Intelligence},
  number       = {1},
  pages        = {20-23},
  shortjournal = {Data Intell.},
  title        = {Comments to jean-claude burgelman&#39;s article politics and open science: How the european open science cloud became reality (the untold Story)—The twin challenge of the hard-core change and the cultural shift. the role of the chorus in the greek drama},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Politics and open science: How the european open science
cloud became reality (the untold story). <em>DINT</em>, <em>3</em>(1),
5–19. (<a href="https://doi.org/10.1162/dint_a_00069">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. This article will document how the European Open Science Cloud (EOSC) emerged as one of the key policy intentions to foster Open Science (OS) in Europe. It will describe some of the typical, non-rational roadblocks on the way to implement EOSC. The article will also argue that the only way Europe can take care of its research data in a way that fits the European specificities fully, is by supporting EOSC.},
  archive      = {J_DINT},
  author       = {Burgelman, Jean-Claude},
  doi          = {10.1162/dint_a_00069},
  journal      = {Data Intelligence},
  number       = {1},
  pages        = {5-19},
  shortjournal = {Data Intell.},
  title        = {Politics and open science: How the european open science cloud became reality (the untold story)},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Editors’ note. <em>DINT</em>, <em>3</em>(1), 1–4. (<a
href="https://doi.org/10.1162/dint_e_00068">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In 2019 the German Leibniz research organization sponsored a conference on Open Science (OS) with the idea to publish some of the presented papers in the Data Intelligence journal. Becoming engaged as editors, we recognized that the term “Open Science” was coined about 10 years ago with the intention as pointed out by Michael Nielson: “OS is the idea that scientific knowledge of all kinds should be openly shared as early as is practical in the discovery process”. Crow and Tanenbaum① stated in 2020 that with OS a great return of investment could be achieved: for each invested dollar about 140 dollars were returned. However, after having participated in many meetings where the ideal of OS was presented repeatedly, after having read many policy papers from many different research organizations and funders, and after having realized that the practices in the data labs have not changed substantially yet, we decided that it is time to review the state of OS in a broader manner.The conference presentations, especially those four and an additional one from the Library of the Peking University, China, selected for publication showed that librarians were pushing activities to foster OS without much support from the hierarchies in the research organizations to influence practices. We must thank the librarians for their energy, but the effect of this activity was that the concept of “Open by Design” shifted to the concept of “Open by Publication” and that researchers tend to believe that OS is something some librarians will do for them at the end of projects. It is not only the experience of COVID-19 which demonstrated that this concept change is not appropriate to foster data-driven research. Not only in the medical sector it is a must to exchange digital objects, be it data, metadata, software or other research artifacts, as quickly as possible. This is true for other research areas as well, just think of data about earthquakes, climatic influences, etc. And indeed, researchers exchange data very quickly amongst their peers, i.e., in limited personal circuits. OS, however, is meant to replace this accidental practice of sharing by a systematic approach which can be compared to the change to systematically publish research results with the help of journal papers centuries ago.It should be noted that “Open by Design” implies (1) to carry out systematic exchange from the beginning and not have to wait for years until publications have been created, (2) to apply suitable mechanisms to the required documentation immediately and not to engage curators to do the hard and expensive documentation and curation work after years, (3) to exchange the whole richness of data as being generated and not just the few data sets that are connected to publications. “OS by Design”, however, requires changing practices in the labs, which is much harder to achieve and will not be liked by researchers as long as efficient support tools are missing.Being aware of differences between policy level statements and data lab practices, we thought that it would be important for a special issue on OS to not just include papers from the conference, but to ask a few distinguished colleagues with different backgrounds to write a paper on their view on OS. With the exception of one colleague who was under an enormous time pressure due to COVID-19 research all accepted our invitation. The results are eight invited papers about OS and one paper describing data lab practices based on deep insights into about 70 research infrastructure projects.The statements indeed show a broad spectrum of opinions. Paolo Budroni, a philosopher by education, puts our discussions on OS into the historical context indicating that openness was always an important issue influenced by the technological possibilities. Heather Joseph, a librarian by training, makes an excellent statement pro OS very much aligned with official policy reports on OS. Jonathan Clark, with his strong publishing background, puts the importance of trust in data and the value of links between digital objects into the centre of an OS domain. John Wood, based on his many years of experience with research projects, argues that we need to lower the expectations to make OS feasible. Klaus Tochtermann, a computer scientist by background and active in developing large research infrastructures in Germany, demonstrates that already in the area of integrating metadata across disciplines there are many roadblocks to overcome. George Strawn, based on his experience with getting the Internet started and other IT projects, argues that the usual hype cycle curve will become true again and that it will take time until realistic OS scenarios will become daily practices. Peter Wittenburg, based on his involvement in setting up large research infrastructures and his close relation with many data labs, also argues that implementing a fair “OS by Design” scenario will take time due to several non-technical roadblocks.The contribution of Jean Claude Burgelman is special in this context since he was one of the key persons who solved the policy puzzle to get final agreements of all member states to make the European Open Science Cloud (EOSC) happen, which is an initiative for building an infrastructure to pave the way towards OS in Europe. This serious and fair description of complex political activities that created many frustrations but finally were successful is in our views a unique document worth elaborating on. We therefore asked a few persons who were involved in these processes arguing from highly different backgrounds and points of views to respond with comments on this paper. Again, all six experts whom we asked to participate in this exercise in addition to the editors agreed to make statements: Natalia Manola (IT researcher and OpenAIRE infrastructure chair), Edit Herzcog (RDA council member and ex-member of European Parliament), Per Öster (CSC Director and involved in e-Infrastructures), Barend Mons (Bioinformatics researcher, chair of early EOSC boards and GO FAIR leader), Hanifeh Khayyeri (member of Swedish Research Council and of EOSC Board) and Dimitris Koureas (Biodiversity researcher and leader of DISSCO research infrastructure). We see this elaboration about the EOSC process as a start for further open discussions which should take place in 2021 and which may help to shape EOSC and thus help in establishing an OS domain.Another paper by Keith Jefferey et. al. was added which describes in broader terms the current practices in the data labs inspired by deep and recent insights into about 70 research infrastructures in Europe. It is meant to indicate how distant OS policy and data practices in the data labs still are and which hurdles need to be overcome to make progress in “OS by Design” affecting the practices.Due to the scientific and economic perspectives we believe that there is no doubt that OS will become a daily practice for all researchers. But we should be aware that setting up systematic procedures implementing “OS by Design” and thus covering the richness of Digital Objects in terms of volumes and types will still take some time. Essential roadblocks need to be overcome, unrealistic expectations need to be reduced, tools and mechanisms need to be developed that are attractive for the researchers to adapt their practices, and the gap between policy level documents and practices needs to be closed.It is time to thank the authors of the conference papers, of the invited statements on OS and the comments to Jean Claude&#39;s note on EOSC for their excellent contributions and to thank Fenghong Liu who put forth the idea to organize such an issue and the editorial office for all their efforts to get the special issue of the Data Intelligence Journal on OS published.January 2021Guest Editors:},
  archive      = {J_DINT},
  author       = {Wittenburg, Peter and Strawn, George},
  doi          = {10.1162/dint_e_00068},
  journal      = {Data Intelligence},
  number       = {1},
  pages        = {1-4},
  shortjournal = {Data Intell.},
  title        = {Editors&#39; note},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
</ul>

</body>
</html>
