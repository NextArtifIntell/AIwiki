<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>JPDC_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="jpdc---177">JPDC - 177</h2>
<ul>
<li><details>
<summary>
(2021). Resource allocation using dynamic pricing auction mechanism
for supporting emergency demands in cloud computing. <em>JPDC</em>,
<em>158</em>, 213–226. (<a
href="https://doi.org/10.1016/j.jpdc.2021.07.016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud resources provide various categories of (VM) virtual machine requests which is assigned with clients for an exact timespan. Currently, the method of VM scheduling within the Cloud environment is decided by a fixed-price scheduling algorithm , during which the user pays a fixed amount per unit time so as to obtain the resources. However, such a scheduling algorithm is not effective for Cloud even though the Cloud resources are dynamically allocated and released. To address this issue, the adaptive scheduling algorithm called as Dynamic pricing based Combinatorial Auction allocation mechanism is proposed. It will be used to increase the resource utilization as well as user satisfaction through dynamic pricing with the combinatorial auction. Our proposed market-based scheduling algorithm uses the principle of auction mechanism for the purpose of extends the satisfaction of Cloud suppliers and clients. This technique reconstructs the current preferences of resource allotment so as to allot resources in advance for emergent virtual machine demands. Then Collective-target augmentation numerical prototype is demonstrated, which forms the minimal execution equivalent range connecting physical machines with virtual machines, and the objective of resource allotment is to obtain minimal quantity of physical machines. The simulation experimental outcomes express that the proposed scheduling methodology and Collective-target augmentation numerical prototypes are capable to adequately increase the quality of service (QoS), improves profit of suppliers and resource utilization.},
  archive      = {J_JPDC},
  author       = {R. Anantha Kumar and K. Kartheeban},
  doi          = {10.1016/j.jpdc.2021.07.016},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {213-226},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Resource allocation using dynamic pricing auction mechanism for supporting emergency demands in cloud computing},
  volume       = {158},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Improved distributed approximation for steiner tree in the
CONGEST model. <em>JPDC</em>, <em>158</em>, 196–212. (<a
href="https://doi.org/10.1016/j.jpdc.2021.08.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we present two deterministic distributed algorithms for the Steiner tree (ST) problem in the CONGEST model. The first algorithm computes a 2 ( 1 − 1 / ℓ ) 2(1−1/ℓ) -approximate ST using ⁎ O ( S + n log ⁎ ⁡ n ) O(S+nlog⁎⁡n) rounds and O ( m S + n 3 / 2 ) O(mS+n3/2) messages for a graph of n nodes and m edges, where S is the shortest path diameter of the graph and ℓ is the number of leaf nodes in the optimal ST. It improves the round complexity of the best distributed ST algorithm known so far, which is O ˜ ( S + min { S t , n } ) O˜(S+min{St,n}) [34] , where t is the number of terminal nodes. The second algorithm improves the message complexity of the first one by dropping the additive term of O ( n 3 / 2 ) O(n3/2) at the expense of a logarithmic multiplicative factor in the round complexity. We also show that for graphs with S = O ( log ⁡ n ) S=O(log⁡n) , a 2 ( 1 − 1 / ℓ ) 2(1−1/ℓ) -approximate ST can be deterministically computed using O ˜ ( n ) O˜(n) rounds and O ˜ ( m ) O˜(m) messages and these complexities almost coincide with the results of some of the singularly-optimal minimum spanning tree (MST) algorithms proposed in [15] , [22] , [37] .},
  archive      = {J_JPDC},
  author       = {Parikshit Saikia and Sushanta Karmakar},
  doi          = {10.1016/j.jpdc.2021.08.004},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {196-212},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Improved distributed approximation for steiner tree in the CONGEST model},
  volume       = {158},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Parallel implementation of cellular automata model of
electron-hole transport in a semiconductor. <em>JPDC</em>, <em>158</em>,
186–195. (<a href="https://doi.org/10.1016/j.jpdc.2021.08.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A parallel implementation of a three-dimensional cellular automaton (CA) model of electron — hole transport in a semiconductor is presented. Carriers transport is described by a nonlinear system of drift-diffusion-Poisson equations. This system includes the drift-diffusion equations in divergence form for electrons and holes and the Poisson equation for the potential, the gradient of which enters the drift-diffusion equations as the drift velocity. We solve the drift-diffusion-Poisson system for the three-dimensional case using the CA approach. A regular mesh is introduced in the three-dimensional domain, and the solution is calculated in all lattice cells. The drift-diffusion-Poisson system is solved by an iterative algorithm consisting of two alternating steps. In the first step, the electron and hole concentrations are calculated. In the second step, the drift velocity is calculated as the gradient of the solution to the Poisson equation with the right-hand side depending on the electron and hole concentrations. The correctness of both CA models is tested against the exact solutions of the drift-diffusion and Poisson equations for some special cases. A parallel implementation of the iterative CA algorithm using the domain decomposition method is presented. The efficiency of the parallel code is analyzed. The simulation results are obtained for the model parameters specific to GaN semiconductors.},
  archive      = {J_JPDC},
  author       = {Karl K. Sabelfeld and Sergey Kireev and Anastasiya Kireeva},
  doi          = {10.1016/j.jpdc.2021.08.006},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {186-195},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Parallel implementation of cellular automata model of electron-hole transport in a semiconductor},
  volume       = {158},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A high-performance VLSI array reconfiguration scheme based
on network flow under row and column rerouting. <em>JPDC</em>,
<em>158</em>, 176–185. (<a
href="https://doi.org/10.1016/j.jpdc.2021.08.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The reconfiguration algorithms have been extensively investigated to ensure the reliability and stability for the processor arrays with faults. It is important to reduce the power consumption, capacitance and communication costs in the processors by reducing the interconnection length of the VLSI array. This paper discusses the reconfiguration problem of the high-performance VLSI processor array under the row and column rerouting constraints. A novel method, making use the idea of network flow, is proposed in this paper. Firstly, a network flow model of the VLSI processor array is constructed, such that the high-performance VLSI target array can be obtained by utilizing the minimal cost flow algorithm. Secondly, we propose a new strategy for bottleneck row selection in the logical array using the minimum cut technique, which can find a more suitable bottleneck row. Finally, we conducted reliable experiments to clearly reveal the efficiency of the new rerouting scheme and algorithm in reducing the number of long interconnects. The experimental results show that, for a host array with size of 256×256, the number of long interconnects in the subarray can be reduced by up to 79.22\% and 55.88\% without performance penalty for random faults with density of 1\% and 25\% respectively, when compared with state-of-the-art. In addition, the proposed scheme improves existing algorithm in terms of subarray size. On a 256×256 host array with 25\% faulty density, the average improvement in subarray size is up to 3.77\% compared with state-of-the-art.},
  archive      = {J_JPDC},
  author       = {Hao Ding and Junyan Qian and Lingzhong Zhao and Zhongyi Zhai},
  doi          = {10.1016/j.jpdc.2021.08.005},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {176-185},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {A high-performance VLSI array reconfiguration scheme based on network flow under row and column rerouting},
  volume       = {158},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Energy efficient spiking neural network processing using
approximate arithmetic units and variable precision weights.
<em>JPDC</em>, <em>158</em>, 164–175. (<a
href="https://doi.org/10.1016/j.jpdc.2021.08.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spiking neural networks (SNNs) have been getting more research attention in recent years as the way they process information is suitable for building neuromorphic systems effectively. However, realizing SNNs on hardware is computationally expensive. To improve their efficiency for hardware implementation, a field-programmable gate array (FPGA) based SNN accelerator architecture is proposed and implemented using approximate arithmetic units. To identify the minimal required bit-width for approximate computation without any performance loss, a variable precision method is utilized to represent weights of the SNN. Unlike the conventional reduced precision method applied to all weights uniformly, the proposed variable precision method allows different bit-widths to represent weights and provide the feasibility of maximizing truncation effort for each weight. Four SNNs adopting different network configurations and training datasets are established to compare the performance of proposed accelerator architecture using the variable precision method with the proposed one using the conventional reduced precision method. Based on the experimental results, more than 40\% of the weights require less bit-width when applying the variable precision method instead of the reduced precision method. With the variable precision method, the proposed architecture achieves 28\% fewer ALUTs and 29\% less power consumption than the proposed one using the reduced precision method.},
  archive      = {J_JPDC},
  author       = {Yi Wang and Hao Zhang and Kwang-Il Oh and Jae-Jin Lee and Seok-Bum Ko},
  doi          = {10.1016/j.jpdc.2021.08.003},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {164-175},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Energy efficient spiking neural network processing using approximate arithmetic units and variable precision weights},
  volume       = {158},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Developing parallel programming and soft skills: A project
based learning approach. <em>JPDC</em>, <em>158</em>, 151–163. (<a
href="https://doi.org/10.1016/j.jpdc.2021.07.015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Upon graduation, a computer science student should have a good understanding of the current technology and have the soft skills necessary to secure a position in industry. Considering that typical computers and even the common smartphone are multicore, students should be skilled in parallel programming. Integrating parallel programming and soft skills within courses can help educate students on these essential skills. Our goal is to explore the effectiveness of using Project Based Learning (PBL) to teach these skills when classes are at content capacity. We divide 247 students into 51 diverse groups and assigned five projects, each of two-week duration. We use pre- and post-surveys to measure growth and found that incorporating PBL has a significant effect on the students&#39; parallel programming and soft skills. We show that through teamwork, students collaboratively learn and apply fundamental parallel programming and soft skills without direct guidance, thus demonstrating the effectiveness of PBL. The implementation was conducted in a course that does not traditionally teach parallel programming concepts, but with the use of a PBL approach, students were able to acquire this new knowledge.},
  archive      = {J_JPDC},
  author       = {Awad A. Younis and Rajshekhar Sunderraman and Mike Metzler and Anu G. Bourgeois},
  doi          = {10.1016/j.jpdc.2021.07.015},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {151-163},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Developing parallel programming and soft skills: A project based learning approach},
  volume       = {158},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-GPU systems and unified virtual memory for scientific
applications: The case of the NAS multi-zone parallel benchmarks.
<em>JPDC</em>, <em>158</em>, 138–150. (<a
href="https://doi.org/10.1016/j.jpdc.2021.08.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {GPU-based computing systems have become a widely accepted solution for the high-performance-computing (HPC) domain. GPUs have shown highly competitive performance-per-watt ratios and can exploit an astonishing level of parallelism. However, exploiting the peak performance of such devices is a challenge, mainly due to the combination of two essential aspects of multi-GPU execution: memory allocation and work distribution. Memory allocation determines the data mapping to GPUs, and therefore conditions all work distribution schemes and communication phases in the application. Unified Virtual Memory simplifies the codification of memory allocations, but its effects on performance depend on how data is used by the devices and how the devices&#39; driver is going to orchestrate the data transfers across the system. In this paper we present a multi-GPU and Unified Virtual Memory (UM) implementation of the NAS Multi-Zone Parallel Benchmarks which alternate communication and computation phases offering opportunities to overlap these phases. We analyse the programmability and performance effects of the introduction of the UM support. Our experience shows that the programming efforts for introducing UM are similar to those of having a memory allocation per GPU . On an evaluation environment composed of 2 x IBM Power9 8335-GTH and 4 x GPU NVIDIA V100 (Volta), our UM-based parallelization outperforms the manual memory allocation versions by 1.10x to 1.85x. However, these improvements are highly sensitive to the information forwarded to the devices&#39; driver describing the most convenient location for specific memory regions. We analyse these improvements in terms of the relationship between the computational and communication phases of the applications.},
  archive      = {J_JPDC},
  author       = {Marc González and Enric Morancho},
  doi          = {10.1016/j.jpdc.2021.08.001},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {138-150},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Multi-GPU systems and unified virtual memory for scientific applications: The case of the NAS multi-zone parallel benchmarks},
  volume       = {158},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Performance analysis and optimization for SpMV based on
aligned storage formats on an ARM processor. <em>JPDC</em>,
<em>158</em>, 126–137. (<a
href="https://doi.org/10.1016/j.jpdc.2021.08.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sparse matrix-vector multiplication (SpMV) has always been a hot topic of research for scientific computing and big data processing, but the sparsity and discontinuity of the nonzero elements in a sparse matrix lead to the memory bottleneck of SpMV. In this paper, we propose aligned CSR (ACSR) and aligned ELL (AELL) formats and a parallel SpMV algorithm to utilize NEON SIMD registers on ARM processors . We analyze the impact of SIMD instruction latency, cache access, and cache misses on SpMV with different formats. In the experiments, our SpMV algorithm based on ACSR achieves 1.18x and 1.56x speedup over SpMV based on CSR and SpMV in PETSc, respectively, and AELL achieves 1.21x speedup over ELL. The deviations between the theoretical results and experimental results in the instruction latency and cache access are 10.26\% and 10.51\% in ACSR and 5.68\% and 2.91\% in AELL, respectively.},
  archive      = {J_JPDC},
  author       = {Yufeng Zhang and Wangdong Yang and Kenli Li and Dahai Tang and Keqin Li},
  doi          = {10.1016/j.jpdc.2021.08.002},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {126-137},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Performance analysis and optimization for SpMV based on aligned storage formats on an ARM processor},
  volume       = {158},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Teaching and learning HPC through serious games.
<em>JPDC</em>, <em>158</em>, 115–125. (<a
href="https://doi.org/10.1016/j.jpdc.2021.07.014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Serious games provide pathways for learners to develop intuition about concepts that are new to them. Such games are especially valuable in an educational context because they engage students in active learning and provide a means for students to develop mental models from their experiences. Furthermore, for short term learning experiences, such as informal, ungraded workshops, they act as ice-breaker activities that help build a sense of community among disparate learners. Collaborative games also spawn peer-to-peer discussion. These student experiences generally lead to deeper questions and discussions that allow instructors to uncover more concepts and clarify misunderstandings, all of which reinforce learning. The authors have created and incorporated role playing games within informal High Performance Computing (HPC) courses to provide students with tangible hands-on experiences with HPC concepts. All of the games are designed so that learners work collaboratively and take on a role associated with an HPC system. Engaging in the role playing highlights the strengths, weaknesses, and challenges of HPC. In this article we describe the games, how they were integrated into HPC courses, both in-person and virtual, and the lessons learned by both students and instructors. Though played with professional and university learners, the games are easily generalized to other cohorts.},
  archive      = {J_JPDC},
  author       = {Julia Mullen and Lauren Milechin and Dennis Milechin},
  doi          = {10.1016/j.jpdc.2021.07.014},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {115-125},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Teaching and learning HPC through serious games},
  volume       = {158},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). EasyPAP: A framework for learning parallel programming.
<em>JPDC</em>, <em>158</em>, 94–114. (<a
href="https://doi.org/10.1016/j.jpdc.2021.07.018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents EasyPAP , an easy-to-use programming environment designed to help students to learn parallel programming. EasyPAP features a wide range of 2D computation kernels that the students are invited to parallelize using Pthreads, OpenMP, OpenCL or MPI . Execution of kernels can be interactively visualized, and powerful monitoring tools allow students to observe both the scheduling of computations and the assignment of 2D tiles to threads/processes. By focusing on algorithms and data distribution, students can experiment with diverse code variants and tune multiple parameters, resulting in richer problem exploration and faster progress towards efficient solutions. We present selected lab assignments which illustrate how EasyPAP improves the way students explore parallel programming.},
  archive      = {J_JPDC},
  author       = {Alice Lasserre and Raymond Namyst and Pierre-André Wacrenier},
  doi          = {10.1016/j.jpdc.2021.07.018},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {94-114},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {EasyPAP: A framework for learning parallel programming},
  volume       = {158},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dynamic fault tolerant scheduling with response time
minimization for multiple failures in cloud. <em>JPDC</em>,
<em>158</em>, 80–93. (<a
href="https://doi.org/10.1016/j.jpdc.2021.07.019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing demand for large amount of computing resources, the cloud is widely used for executing large number of independent tasks. In order to successfully execute more tasks and maximize the revenues, the cloud service providers (CSPs) should provide reliable services, while maximizing the resource utilization. Providing better Quality of Service (QoS), while maximizing the resource utilization in the event of failures is a critical research issue which needs to be addressed. In this paper, an Elastic pull-based Dynamic Fault Tolerant (E-DFT) scheduling mechanism is designed for minimizing the response time while executing the backups during multiple failures of independent tasks. A basic core primary backup model is also used and integrated with the backup tasks overlapping (BTO) and backup tasks fusion (BTF) techniques to tolerate multiple simultaneous failures. Simulation results show that the proposed E-DFT scheduling can achieve better performance in terms of guarantee ratio and resource utilization over other existing scheduling algorithms .},
  archive      = {J_JPDC},
  author       = {Pushpanjali Gupta and Prasan Kumar Sahoo and Bharadwaj Veeravalli},
  doi          = {10.1016/j.jpdc.2021.07.019},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {80-93},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Dynamic fault tolerant scheduling with response time minimization for multiple failures in cloud},
  volume       = {158},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Exponential type of many-to-many edge disjoint paths on
ternary n-cubes. <em>JPDC</em>, <em>158</em>, 67–79. (<a
href="https://doi.org/10.1016/j.jpdc.2021.07.020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of most central issues in various interconnection networks of parallel and distributed systems is to find edge-disjoint paths concerned with an information transmission through links. It is universally acknowledged that an interconnection network can be modeled as an undirected simple graph G = ( V , E ) G=(V,E) with processors and physical links between the processors represented as vertices and edges of the G , respectively. The studies on the edge-disjoint paths are closely related to the edge-connectivity. By the well-known Menger theorem, the maximum number of edge disjoint paths connecting any two disjoint connected subgraphs with g vertices in G can also define by the minimum modified edge-cut, called the g -extra edge-connectivity of G λ g ( G ) λg(G) . It is the cardinality of the minimum set of edges in G , if such a set exists, whose deletion disconnects G and leaves every remaining component with at least g vertices. The k -ary n -cube network is known as one of the most attractive interconnection networks for parallel and distributed systems. This article intends to show that for 3 ⌈ n 2 ⌉ + r − ⌊ 3 2 r + e + 1 2 ⌋ ≤ g ≤ 3 ⌈ n 2 ⌉ + r 3⌈n2⌉+r−⌊32r+e+12⌋≤g≤3⌈n2⌉+r , the g -extra edge-connectivity of ternary n -cube (also called 3-ary n -cube) ( n ≥ 3 n≥3 ) exists a concentration behavior, and prove that these corresponding g -extra edge-connectivities of ternary n -cubes are the constants 2 ( ⌊ n 2 ⌋ − r ) 3 ⌈ n 2 ⌉ + r 2(⌊n2⌋−r)3⌈n2⌉+r , where r = 0 , 1 , 2 , ⋯ , ⌊ n 2 ⌋ − 1 r=0,1,2,⋯,⌊n2⌋−1 , and e = 1 e=1 if n is odd and e = 0 e=0 if n is even. The above upper and lower bounds of g are sharp. As the integer g varies within the range between 3 ⌈ n 2 ⌉ + r − ⌊ 3 2 r + e + 1 2 ⌋ 3⌈n2⌉+r−⌊32r+e+12⌋ and 3 ⌈ n 2 ⌉ + r 3⌈n2⌉+r , a necessary and sufficient condition of λ g λg -optimality is found. Our results improve the several previous results.},
  archive      = {J_JPDC},
  author       = {Wenhuan Ma and Mingzu Zhang and Jixiang Meng and Tianlong Ma},
  doi          = {10.1016/j.jpdc.2021.07.020},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {67-79},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Exponential type of many-to-many edge disjoint paths on ternary n-cubes},
  volume       = {158},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A module-based introduction to heterogeneous computing in
core courses. <em>JPDC</em>, <em>158</em>, 56–66. (<a
href="https://doi.org/10.1016/j.jpdc.2021.07.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heterogeneous architectures have emerged as a dominant platform, not only in high-performance computing but also in mobile processing, cloud computing, and the Internet of Things (IoTs). Because the undergraduate computer science curriculum includes so many topics, adding a new course as a required part of the curriculum without increasing the number of hours to graduation is difficult. Integration of heterogeneous computing requires a module-driven approach in which coverage of the topics is broken down into smaller units and dispersed throughout the curriculum. The module-driven approach has been successfully implemented in introducing parallel and distributed computing concepts. In this paper, we present a set of four teaching modules that introduce fundamental concepts in heterogeneous computing in lower-division computer science courses. The goal of these modules is not to teach students how to program heterogeneous systems but rather to expose them to this emerging trend and prepare them for material in future classes. Although concepts are covered at a high level, the modules emphasize active learning and include lab assignments that provide students with hands-on experience. We also present initial evaluation results for two of these modules based on their use in undergraduate courses at Texas State University. The results are quite encouraging both in terms of learning outcomes and student engagement and interest.},
  archive      = {J_JPDC},
  author       = {Apan Qasem and David P. Bunde and Philip Schielke},
  doi          = {10.1016/j.jpdc.2021.07.011},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {56-66},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {A module-based introduction to heterogeneous computing in core courses},
  volume       = {158},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Machine learning for optimal selection of sparse triangular
system solvers on GPUs. <em>JPDC</em>, <em>158</em>, 47–55. (<a
href="https://doi.org/10.1016/j.jpdc.2021.07.013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many numerical algorithms for science and engineering applications require the solution of sparse triangular linear systems ( sptrsv ) as their most costly stage. For this reason, considerable research has been dedicated to produce efficient implementations for almost all high performance computing platforms. In the case of graphics processing units (GPUs), there are several strategies to perform this operation, which translate into a handful of different routines. In general, it is difficult to establish a priori which is the best routine for a given problem, and thus, an automatic procedure able to select the best solver for each matrix can entail large performance benefits. This work extends a previous effort, in which we relied on machine learning techniques to predict the best sptrsv routine for each matrix, by improving both the accuracy and the speed of the selection procedure. Specifically, we focus on the most efficient machine learning techniques regarding the speed of their training and prediction stages; evaluate the artificial generation of sparse matrices to expand our dataset; and propose heuristics to compute approximations of some expensive features. The experimental results show that we can strongly improve the runtime of our procedure without compromising the quality results.},
  archive      = {J_JPDC},
  author       = {Ernesto Dufrechou and Pablo Ezzatti and Manuel Freire and Enrique S. Quintana-Ortí},
  doi          = {10.1016/j.jpdc.2021.07.013},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {47-55},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Machine learning for optimal selection of sparse triangular system solvers on GPUs},
  volume       = {158},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). VSIM: Distributed local structural vertex similarity
calculation on big graphs. <em>JPDC</em>, <em>158</em>, 29–46. (<a
href="https://doi.org/10.1016/j.jpdc.2021.07.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many graph analytical applications, the local structural vertex similarity calculation is an essential prerequisite for advanced graph mining. The similarity calculation finds out all the similar vertex pairs whose local structural similarity scores (like the number of common neighbors, and the Jaccard index of adjacency sets) are above a given threshold. The real-world applications use a wide range of similarity thresholds. However, the existing distributed methods for the problem only optimize for either high thresholds (&gt; 0.7) or low thresholds (&lt; 0.1). To overcome the drawback, we propose a new distributed vertex similarity calculation framework VSIM that is efficient under a broad range of thresholds. VSIM processes static undirected graphs with local structural similarity scores that measure the similarity between vertices based on the first-order topology information. VSIM generates a similarity calculation task for every vertex in the graph and conducts all the tasks in parallel on a distributed computing platform along with a distributed key-value store. Each task finds vertices similar to a given center vertex with two task execution modes. The two modes optimize for high and low thresholds, respectively. Each task picks the suitable mode adaptively according to cost estimation models. We also propose an efficient implementation for VSIM on Apache Spark , with three optimization techniques to reduce communication costs and balance workloads on power-law graphs. The experimental evaluation shows that VSIM outperforms the state-of-the-art distributed methods by up to 67x speedup. VSIM can achieve near-linear node scalability in low-threshold and small cache scenarios.},
  archive      = {J_JPDC},
  author       = {Zhaokang Wang and Shen Wang and Junhong Li and Chunfeng Yuan and Rong Gu and Yihua Huang},
  doi          = {10.1016/j.jpdc.2021.07.009},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {29-46},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {VSIM: Distributed local structural vertex similarity calculation on big graphs},
  volume       = {158},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hammer lightweight graph partitioner based on graph data
volumes. <em>JPDC</em>, <em>158</em>, 16–28. (<a
href="https://doi.org/10.1016/j.jpdc.2021.07.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The graph partitioning challenge is well known and ongoing classical problem. Many heuristic methods tried to propose solutions focusing mainly on load processing and cost-efficiency. With the emergency of big data technology, the graph partitioning challenge became even more demanding, as an imminent need to handle big volume of data in real time. This reveals a new challenge as most of the existing studies does not consider the volume metric with their streaming graph algorithms causing imbalanced workloads and graph storage. With this article, we propose a specific lightweight algorithm which we called “Hammer” Algorithm. Our proposed Hammer algorithm is a streaming graph based on volume metric to ensure optimal load processing and communication cost efficiency. Our proof of concept was done on real world dataset and the Hammer algorithm showed considerable performance against some existing graph partitioning algorithms.},
  archive      = {J_JPDC},
  author       = {Chayma Sakouhi and Abir Khaldi and Henda Ben Ghezala},
  doi          = {10.1016/j.jpdc.2021.07.008},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {16-28},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Hammer lightweight graph partitioner based on graph data volumes},
  volume       = {158},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Probabilistic and temporal failure detectors for solving
distributed problems. <em>JPDC</em>, <em>158</em>, 1–15. (<a
href="https://doi.org/10.1016/j.jpdc.2021.07.017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Failure detectors (FD)s are celebrated for their modularity in solving distributed problems. Algorithms are constructed using FD building blocks . Synchrony assumptions to implement FDs are studied separately and are typically expressed as eventual guarantees that need to hold, after some point in time, forever and deterministically . But in practice, they may hold only probabilistically and temporarily. This paper studies FDs in a realistic system N N , where asynchrony is inflicted by probabilistic synchronous communication . We first address a problem with ⋄ S ⋄S , the weakest FD to solve consensus: an implementation of “consensus with probability 1” is possible in N N without randomness in the algorithm, while an implementation of “ ⋄ S ⋄S with probability 1” is impossible in N N . We introduce ⁎ ⋄ S ⁎ ⋄S⁎ , a new FD with probabilistic and temporal accuracy. We prove that ⁎ ⋄ S ⁎ ⋄S⁎ (i) is implementable in N N and (ii) can replace ⋄ S ⋄S , in several existing deterministic consensus algorithms that use ⋄ S ⋄S , to yield an algorithm that solves “consensus with probability 1”. We extend our results to other FD classes, e.g., ⋄ P ⋄P , and to a larger set of problems (beyond consensus), which we call decisive problems.},
  archive      = {J_JPDC},
  author       = {Rachid Guerraoui and David Kozhaya and Yvonne-Anne Pignolet},
  doi          = {10.1016/j.jpdc.2021.07.017},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {1-15},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Probabilistic and temporal failure detectors for solving distributed problems},
  volume       = {158},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). CS-materials: A system for classifying and analyzing
pedagogical materials to improve adoption of parallel and distributed
computing topics in early CS courses. <em>JPDC</em>, <em>157</em>,
316–330. (<a href="https://doi.org/10.1016/j.jpdc.2021.05.014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The NSF/IEEE-TCPP Parallel and Distributed Computing curriculum guidelines released in 2012 (PDC12) represents an effort to bring more parallel computing concepts into early computer science courses. To date, it has been moderately successful, with the inclusion of some PDC topics in the ACM/IEEE Computer Science curriculum guidelines in 2013 (CS13) and mentions of PDC topics in the Computing Curricula 2020. Additionally, some universities in the U.S. and around the world have started to cover some of these topics in early CS courses. Lack of knowledge of or training in PDC topics among instructors, along with the need to align early CS course content with prescribed learning objectives in the curricula, are often cited as hurdles for adoption in early CS courses. There have been attempts at bringing PDC materials, such as textbook chapters, lecture slides, assignments, and demos to assist instructors of early CS classes. However, the effort required on the part of the instructor to figure out what is relevant to a particular class can be daunting. In this work, we contend that simultaneously classifying pedagogical materials against the CS13 and the PDC12 curriculum guidelines can address some of the challenges faced by instructors and can promote broader adoption of PDC materials in early CS courses. We present CS Materials , a system that can be used to categorize pedagogical materials according to well-known and established curricular guidelines. We show that CS Materials can be leveraged 1) by instructors of early CS courses to find materials that are similar to the one that they use but that also cover PDC topics, and 2) by instructors to check the coverage of topics (and gaps) in a course, and 3) by PDC experts to identify topics for which PDC instructional materials do not exist or are insufficient in order to inform development of additional PDC curricular materials.},
  archive      = {J_JPDC},
  author       = {Alec Goncharow and Matthew Mcquaigue and Erik Saule and Kalpathi Subramanian and Paula Goolkasian and Jamie Payton},
  doi          = {10.1016/j.jpdc.2021.05.014},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {316-330},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {CS-materials: A system for classifying and analyzing pedagogical materials to improve adoption of parallel and distributed computing topics in early CS courses},
  volume       = {157},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Integrating big data and cloud computing topics into the
computing curricula: A modular approach. <em>JPDC</em>, <em>157</em>,
303–315. (<a href="https://doi.org/10.1016/j.jpdc.2021.07.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Big data and cloud computing collectively offer a paradigm shift in the way businesses are now acquiring, using, and managing information technology. This creates the need for every CS student to be equipped with foundational knowledge in this collective paradigm and possess some hands-on experience in deploying and managing big data applications in the cloud. This study argues that, for substantial coverage of big data and cloud computing concepts and skills, the relevant topics need to be integrated into multiple core courses across the CS curriculum rather than creating additional courses and performing a major overhaul of the curriculum. Our approach to including these topics is to develop autonomous competency-based learning modules for specific core courses in which their coverage might find an appropriate context. In this paper, four such modules are discussed, and our classroom experiences during these interventions are documented. Student performance data and survey results show reasonable success in attaining student learning outcomes, enhanced engagement, and interests.},
  archive      = {J_JPDC},
  author       = {Debzani Deb and Muztaba Fuad},
  doi          = {10.1016/j.jpdc.2021.07.012},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {303-315},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Integrating big data and cloud computing topics into the computing curricula: A modular approach},
  volume       = {157},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptive diagonal sparse matrix-vector multiplication on
GPU. <em>JPDC</em>, <em>157</em>, 287–302. (<a
href="https://doi.org/10.1016/j.jpdc.2021.07.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For diagonal sparse matrices that have many long zero sections or scatter points or diagonal deviations from the main diagonal, a great number of zeros need be filled to maintain the diagonal structure while using DIA to store them, which leads to the performance degradation of the existing DIA kernels because the padded zeros consume extra computation and memory resources. This motivates us to present an adaptive sparse matrix-vector multiplication (SpMV) for diagonal sparse matrices on the graphics processing unit (GPU), called DIA-Adaptive, to alleviate the drawback of DIA kernels for these cases. For DIA-Adaptive, there are the following characteristics: (1) two new sparse storage formats, BRCSD (Diagonal Compressed Storage based on Row-Blocks)-I and BRCSD-II, are proposed to adapt it to various types of diagonal sparse matrices besides adopting DIA, and SpMV kernels corresponding to these storage formats are presented; and (2) a search engine is designed to choose the most appropriate storage format from DIA, BRCSD-I, and BRCSD-II for any given diagonal sparse matrix; and (3) a code generator is presented to automatically generate SpMV kernels. Using DIA-Adaptive, the ideal storage format and kernel are automatically chosen for any given diagonal sparse matrix, and thus high performance is achieved. Experimental results show that our proposed DIA-Adaptive is effective, and has high performance and good parallelism, and outperforms the state-of-the-art SpMV algorithms for all test cases.},
  archive      = {J_JPDC},
  author       = {Jiaquan Gao and Yifei Xia and Renjie Yin and Guixia He},
  doi          = {10.1016/j.jpdc.2021.07.007},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {287-302},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Adaptive diagonal sparse matrix-vector multiplication on GPU},
  volume       = {157},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Preemptive scheduling on unrelated machines with fractional
precedence constraints. <em>JPDC</em>, <em>157</em>, 280–286. (<a
href="https://doi.org/10.1016/j.jpdc.2021.07.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many programming models, e.g., MapReduce, introduce precedence constraints between the jobs. This paper formalizes a notion of precedence constraints, called fractional precedence constraints, where the progress of follower jobs only has to lag behind (fractionally) their leads. For a general set of fractional precedence constraints between the jobs, this paper provides a new class of preemptive scheduling algorithms on unrelated machines that have arbitrary processing speeds. In particular, for a given makespan, we establish both sufficient and necessary conditions on the existence of a feasible job schedule, and then propose an efficient scheduling algorithm based on a novel matrix decomposition method, if the sufficient conditions are satisfied. The algorithm is shown to be a Polynomial-Time Approximation Scheme (PTAS), i.e., its solution is able to achieve any feasible makespan with an approximation bound of 1 + ϵ 1+ϵ , for an arbitrary ϵ &gt; 0 ϵ&amp;gt;0 .},
  archive      = {J_JPDC},
  author       = {Vaneet Aggarwal and Tian Lan and Dheeraj Peddireddy},
  doi          = {10.1016/j.jpdc.2021.07.010},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {280-286},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Preemptive scheduling on unrelated machines with fractional precedence constraints},
  volume       = {157},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A taxi dispatch system based on prediction of demand and
destination. <em>JPDC</em>, <em>157</em>, 269–279. (<a
href="https://doi.org/10.1016/j.jpdc.2021.07.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we describe an intelligent taxi dispatch system that has the goal of reducing the waiting time of the passengers and the idle driving distance of the taxis. The system relies on two separate models that predict the probability distributions of the taxi demand and destinations respectively. The models are learned from historical data and use a combination of long short term memory cells and mixture density networks. Using these predictors, taxi dispatch is formulated as a mixed integer programming problem. We validate the performance of the predictors and the overall system on a real world dataset of taxi trips in New York City.},
  archive      = {J_JPDC},
  author       = {Jun Xu and Rouhollah Rahmatizadeh and Ladislau Bölöni and Damla Turgut},
  doi          = {10.1016/j.jpdc.2021.07.002},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {269-279},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {A taxi dispatch system based on prediction of demand and destination},
  volume       = {157},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Efficient classification of private memory blocks.
<em>JPDC</em>, <em>157</em>, 256–268. (<a
href="https://doi.org/10.1016/j.jpdc.2021.07.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Shared memory architectures are pervasive in the multicore technology era. Still, sequential and parallel applications use most of the data as private in a multicore system. Recent proposals using this observation and driven by a classification of private/shared memory data can reduce the coherence directory area or the memory access latency. The effectiveness of these proposals depends on the accuracy of the classification. The existing proposals perform the private/shared classification at page granularity , leading to a miss-classification and reducing the number of detected private memory blocks. We propose a mechanism able to accurately classify memory blocks using the existing translation lookaside buffers (TLB), which increases the effectiveness of proposals relying on a private/shared classification. Our experimental results show that the proposed scheme reduces L1 cache misses by 25\% compared to a page-grain classification approach, which translates into an improvement in system performance by 8.0\% with respect to a page-grain approach.},
  archive      = {J_JPDC},
  author       = {Bhargavi R. Upadhyay and Alberto Ros and Jalpa Shah},
  doi          = {10.1016/j.jpdc.2021.07.005},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {256-268},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Efficient classification of private memory blocks},
  volume       = {157},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MDScale: Scalable multi-GPU bonded and short-range molecular
dynamics. <em>JPDC</em>, <em>157</em>, 243–255. (<a
href="https://doi.org/10.1016/j.jpdc.2021.07.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {GPUs have enabled a drastic change to computing environments, making massively parallel computing possible. Molecular dynamics is a perfect candidate problem for massively parallel computing, but to date it has not taken full advantage of multi-GPU environments due to the difficulty of partitioning molecular dynamics problems and exchanging problem data among compute nodes. These difficulties restrict the use of GPUs to only some of the computations in a full molecular dynamics problem, and hence prevent scalability beyond just a few GPUs. This work presents a scalable parallelization solution for the bonded and short-range forces present in a molecular dynamics problem. Together with existing solutions for long-range forces, it enables highly scalable, parallel molecular dynamics on multi-GPU computing environments. Specifically, the proposed solution divides the molecular volume into independent parts assigned to different GPUs, but it maintains a global bond structure that is efficiently exchanged when atoms move across GPUs. We demonstrate close-to-linear speedup of the proposed solution, simulating the dynamics of gigamolecules with 1 billion atoms on a computing environment with 96 GPUs, and obtaining superior performance to the well known molecular dynamics simulator NAMD.},
  archive      = {J_JPDC},
  author       = {Gonzalo Nicolas Barreales and Marcos Novalbos and Miguel A. Otaduy and Alberto Sanchez},
  doi          = {10.1016/j.jpdc.2021.07.006},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {243-255},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {MDScale: Scalable multi-GPU bonded and short-range molecular dynamics},
  volume       = {157},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Modelling and analysing IoT systems. <em>JPDC</em>,
<em>157</em>, 233–242. (<a
href="https://doi.org/10.1016/j.jpdc.2021.07.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Internet of Things is deeply shaping our society and our lives. Smart devices automatically collect, aggregate and exchange data on our behalf and free us from the drudgery of doing it. These data are often crucial because critical decisions, such as controlling cyber-physical systems, are made depending on them or because they feed learning algorithms. Safety and security issues related to devices and to data can have a major impact on smart systems and can have serious consequences if they oversee essential services, such as delivering power, water, transport, and so on. For this reason, it is crucial to identify the most critical components in a network of devices and to evaluate how they are vulnerable to accidental or to intentional failures. We propose to use the process calculus IoT-LySa to model systems and to apply a Control Flow Analysis to statically predict the manipulation of data, as well as on which data the critical decisions depend, in particular those affecting actuations . By exploiting suitable metrics, we can use the results of the analysis so as to provide system administrators with estimates of the safety and security of their systems.},
  archive      = {J_JPDC},
  author       = {Chiara Bodei and Pierpaolo Degano and Gian-Luigi Ferrari and Letterio Galletta},
  doi          = {10.1016/j.jpdc.2021.07.004},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {233-242},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Modelling and analysing IoT systems},
  volume       = {157},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). RF-RISA: A novel flexible random forest accelerator based on
FPGA. <em>JPDC</em>, <em>157</em>, 220–232. (<a
href="https://doi.org/10.1016/j.jpdc.2021.07.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, FPGA has been utilized to accelerate the Random Forest prediction process to meet the speed requirements of real-time tasks. However, the existing accelerators impose restrictions on the parameters of the accelerated model. The accelerators have to be reconfigured to adapt to a model whose parameters exceed the predefined restrictions. When these accelerators are applied in the scenarios where the model updates or switches frequently, non-trivial time overhead and maintenance costs may be introduced. To solve the above problem, a flexible accelerator RF-RISA, Random Forest Reduced Instruction Set Accelerator, is presented in this paper. Compared with the existing accelerators, RF-RISA eliminates all the restrictions by decoupling the model parameters from its hardware implementation. Specifically, RF-RISA encodes the information of the model into a group of instructions, then the instructions are stored in the memory rather than are hardcoded in the hardware. Meanwhile, a mapping scheme is proposed to map the instructions into the memory dynamically. Finally, a new hardware architecture is designed to support the pipelined computing. The theoretical analysis and experimental results show that the proposed RF-RISA can accelerate a wide range of RF models without reconfiguration. At the same time, it can achieve the same throughput as the state-of-the-art.},
  archive      = {J_JPDC},
  author       = {Shuang Zhao and Shuhui Chen and Hui Yang and Fei Wang and Ziling Wei},
  doi          = {10.1016/j.jpdc.2021.07.001},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {220-232},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {RF-RISA: A novel flexible random forest accelerator based on FPGA},
  volume       = {157},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Evolving PDC curriculum and tools: A study in responding to
technological change. <em>JPDC</em>, <em>157</em>, 201–219. (<a
href="https://doi.org/10.1016/j.jpdc.2021.07.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Much has changed about parallel and distributed computing (PDC) since the author began teaching the topic in the late 1990s. This paper reviews some of the key changes to the field and describes their impacts on his work as a PDC educator. Such changes include: the availability of free implementations of the message passing interface (MPI) for distributed-memory multiprocessors; the development of the Beowulf cluster; the advent of multicore architectures; the development of free multithreading languages and libraries such as OpenMP; the availability of (relatively) inexpensive manycore accelerator devices (e.g., GPUs); the availability of free software platforms like CUDA, OpenACC, OpenCL, and OpenMP for using accelerators; the development of inexpensive single board computers (SBCs) like the Raspberry Pi, and other changes. The paper details the evolution of PDC education at the author&#39;s institution in response to these changes, including curriculum changes, seven different Beowulf cluster designs, and the development of pedagogical tools and techniques specifically for PDC education. The paper also surveys many of the hardware and software infrastructure options available to PDC educators, provides a strategy for choosing among them, and provides practical advice for PDC pedagogy. Through these discussions, the reader may see how much PDC education has changed over the past two decades, identify some areas of PDC that have remained stable during this same time period, and so gain new insight into how to efficiently invest one&#39;s time as a PDC educator.},
  archive      = {J_JPDC},
  author       = {Joel C. Adams},
  doi          = {10.1016/j.jpdc.2021.07.003},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {201-219},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Evolving PDC curriculum and tools: A study in responding to technological change},
  volume       = {157},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Techniques and tools for visually introducing freshmen to
object-based thread abstractions. <em>JPDC</em>, <em>157</em>, 179–200.
(<a href="https://doi.org/10.1016/j.jpdc.2021.05.013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An approach to adding concurrency to an existing undergraduate course should integrate driving problems used in analogies, worked examples, and assignments; amplify the subject/topics of the existing course; and ensure that the addition does not have a negative impact on (a) conceptual focus, and (b) student learning, struggle, and engagement. When concurrency is added to a course covering object-based programming, five related design principles for meeting these, sometimes conflicting, requirements are: (a) analogies, worked examples, and assignments are all implementations of simulations of moving physical objects, (b) the user-interface components of the simulations are created automatically or implemented manually using the MVC design pattern , (c) assignment implementations are layered to follow the logical dependence among concepts, (d) the concurrency aspects of the functional components of the simulations are implemented using reusable loop and design patterns, and (e) students can experiment with concurrency extensions to implementations of worked examples and assignments. We followed these principles in multiple course offerings that added concurrency to two different courses on object-based programming. Our data-based evaluation of these offerings, using new inferencing algorithms, analyzed the number of posts and contributions to class discussion forums; the number of entries in class participation diaries; the number of times an automated test is run before it passes; and the percentage of attempts to implement different aspects of concurrency that yield no success. The results show that adding thread execution and creation, synchronization, and coordination has little or no significant effect on measures of engagement, learning, and struggle.},
  archive      = {J_JPDC},
  author       = {Prasun Dewan and Samuel George and Andrew Wortas and Justin Do},
  doi          = {10.1016/j.jpdc.2021.05.013},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {179-200},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Techniques and tools for visually introducing freshmen to object-based thread abstractions},
  volume       = {157},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Joint optimization of cache placement and request routing in
unreliable networks. <em>JPDC</em>, <em>157</em>, 168–178. (<a
href="https://doi.org/10.1016/j.jpdc.2021.06.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge caching is a prevailing media delivery technology where data is hosted at the edge nodes with computing and storage capability in close proximity to users, in order to expand the backhaul network capacity and enhance users&#39; quality of experience (QoE). The existing work in this area often neglects the fact that large-scale distributed cache networks are not particularly reliable and many edge nodes are prone to failure. In this paper we investigate and develop a novel, cooperative caching mechanism for content placement and request routing. We aim to minimize the content access delay and achieve the optimization in polynomial time, taking into account failures in an unreliable network environment with limited edge storage and bandwidth. We introduce two optimization algorithms: 1) a primal-dual algorithm that is based on the Lagrangian dual decomposition and subgradient method, and 2) a greedy-based approximation algorithm with a proven approximation ratio. Numerical results show that the proposed algorithms outperform other comparative approaches in synthetic and real network environments, and the approximation algorithm is particularly suitable for networking scenarios with sparse node connectivity and resources in short supply.},
  archive      = {J_JPDC},
  author       = {Youmei Song and Tianyu Wo and Renyu Yang and Qi Shen and Jie Xu},
  doi          = {10.1016/j.jpdc.2021.06.006},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {168-178},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Joint optimization of cache placement and request routing in unreliable networks},
  volume       = {157},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A fine-grained anonymous handover authentication protocol
based on consortium blockchain for wireless networks. <em>JPDC</em>,
<em>157</em>, 157–167. (<a
href="https://doi.org/10.1016/j.jpdc.2021.06.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given the ubiquitous nature of wireless networks in today&#39;s society, protecting users&#39; information and identity during handover authentication is paramount. Mobile network operators hope to implement a more flexible and fine-grained authentication to provide better and safer services. In this paper, we propose a new multi-attribute authority attribute-based signature (MA-ABS) scheme that has a constant-size signature. More specifically, we propose an anonymous handover authentication protocol that uses MA-ABS and consortium blockchain. This new protocol not only has fine-grained access control, but it protects the user&#39;s privacy and ensures an efficient and private handover.},
  archive      = {J_JPDC},
  author       = {Guangsong Li and Wei Chen and Bin Zhang and Siqi Lu},
  doi          = {10.1016/j.jpdc.2021.06.007},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {157-167},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {A fine-grained anonymous handover authentication protocol based on consortium blockchain for wireless networks},
  volume       = {157},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Energy saving strategy and nash equilibrium of hybrid P2P
networks. <em>JPDC</em>, <em>157</em>, 145–156. (<a
href="https://doi.org/10.1016/j.jpdc.2021.06.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a penalty strategy with differentiated service rate based on the free riding phenomenon in P2P networks, and establishes an M/M/ c + d queueing model. Based on this model, a sleep/wakeup mechanism is introduced for the peers at the service end, and a single asynchronous vacation strategy is adopted to reduce the energy consumption of the system. In addition, the energy consumption of peers in each state is quantified, and the relationship between the energy consumption and parameters of the system is analyzed. In order to avoid excessive requests for unnecessary services from requesting nodes and increasing energy consumption of the system, this paper analyzes the Nash equilibrium between the arrival rate and the net profit of a single node, and then studies the optimization of social profit. The stationary distribution of queueing model is obtained by the method of matrix geometric solution, the performance indicators of the system are constructed, and the system performance is analyzed by numerical experiments. Experimental results show that the model developed in this paper has a significant penalty effect on free riding behavior, and that the single asynchronous vacation strategy not only saves more than 10\% of the total energy consumption compared with the single synchronous vacation strategy, but also makes the hybrid P2P networks more flexible and efficient.},
  archive      = {J_JPDC},
  author       = {Zhanyou Ma and Changzhen Zhang and Liyuan Zhang and Shunzhi Wang},
  doi          = {10.1016/j.jpdc.2021.06.009},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {145-156},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Energy saving strategy and nash equilibrium of hybrid P2P networks},
  volume       = {157},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A novel MPI+MPI hybrid approach combining MPI-3 shared
memory windows and C11/c++11 memory model. <em>JPDC</em>, <em>157</em>,
125–144. (<a href="https://doi.org/10.1016/j.jpdc.2021.06.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increase of the number of cores in processors used in modern cluster architectures advocates hybrid parallel programming, combining Message Passing Interface (MPI) for internode operations and a shared memory treatment of intranode operations. We propose an MPI+MPI hybrid approach to parallel programming in which shared memory operations are managed by the combination of MPI shared memory windows introduced with MPI-3, C11/C ++ 11 atomic operations and the associated multi-thread memory model. We illustrate the method on fundamental parallel operations (barrier, reduction) and on the ghost update, which is prevalent in many parallel numerical methods. The performance tests on Reedbush-U and Oakbridge-CX systems show that using the C11/C ++ 11 memory model to manage shared memory windows can achieve levels of performance comparable to state of the art MPI implementations, while reducing the variance of execution times as well as increasing the level of synchronization between processes, especially in multiple nodes environments. It also reduces significantly the execution time of ghost updates compared to flat MPI, and the synchronization of shared data with the C ++ 11 memory model is observed to be more efficient than other synchronization methods based on RMA utilities.},
  archive      = {J_JPDC},
  author       = {Lionel Quaranta and Lalith Maddegedara},
  doi          = {10.1016/j.jpdc.2021.06.008},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {125-144},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {A novel MPI+MPI hybrid approach combining MPI-3 shared memory windows and C11/C++11 memory model},
  volume       = {157},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The power of agents in a dispersed system – the
shapley-shubik power index. <em>JPDC</em>, <em>157</em>, 105–124. (<a
href="https://doi.org/10.1016/j.jpdc.2021.06.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, dispersed knowledge – accumulated in several decision tables is considered. Dispersion of knowledge is not a part of the work of the system. We assume that the knowledge is already in the dispersed form when it provides to the system. An advanced process of detecting the relations between the decision tables and constructing coalitions is used. The purpose of this paper is to use the measure to determine the strength of the coalition. With this method, a simple method of combining vectors of decisions generated based on local decision tables was applied. The purpose of using the Shapley-Shubik index was to reduce the computational complexity compared to the approach proposed in the earlier papers. In this paper, the results of experiments are presented, and the two approaches are compared. Based on these results, some conclusions have been drawn.},
  archive      = {J_JPDC},
  author       = {Małgorzata Przybyła-Kasperek},
  doi          = {10.1016/j.jpdc.2021.06.010},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {105-124},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {The power of agents in a dispersed system – the shapley-shubik power index},
  volume       = {157},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MATAR: A performance portability and productivity
implementation of data-oriented design with kokkos. <em>JPDC</em>,
<em>157</em>, 86–104. (<a
href="https://doi.org/10.1016/j.jpdc.2021.03.016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is a need for simple, fast, and memory-efficient multidimensional data structures for dense and sparse storage that arise with numerical methods and in software applications. The data structures must perform equally well across multiple computer architectures , including CPUs and GPUs . For this purpose, we developed MATAR, a C++ software library that allows for simple creation and use of intricate data structures that is also portable across disparate architectures using Kokkos. The performance aspect is achieved by forcing contiguous memory layout (or as close to contiguous as possible) for multidimensional and multi-size dense or sparse MAT rix and AR ray (hence, MATAR) types. Our results show that MATAR has the capability to improve memory utilization, performance, and programmer productivity in scientific computing . This is achieved by fitting more work into the available memory, minimizing memory loads required, and by loading memory in the most efficient order. This document describes the purpose of the work, the implementation of each of the data types , and the resulting performance both in some simple baseline test cases and in an application code.},
  archive      = {J_JPDC},
  author       = {Daniel J. Dunning and Nathaniel R. Morgan and Jacob L. Moore and Eappen Nelluvelil and Tanya V. Tafolla and Robert W. Robey},
  doi          = {10.1016/j.jpdc.2021.03.016},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {86-104},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {MATAR: A performance portability and productivity implementation of data-oriented design with kokkos},
  volume       = {157},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fog computing: A taxonomy, systematic review, current trends
and research challenges. <em>JPDC</em>, <em>157</em>, 56–85. (<a
href="https://doi.org/10.1016/j.jpdc.2021.06.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There has been rapid development in the number of Internet of Things (IoT) connected nodes and devices in our daily life in recent times. With this increase in the number of devices, fog computing has become a well-established paradigm to optimize various key Quality of Service (QoS) requirements such as latency, bandwidth limitation, response time, scalability, privacy and security. In this paper, we present a systematic literature review of fog computing. This review article aims to classify recently published studies and investigate the current status in the area of fog computing. In this work, we have discussed the important characteristics of fog computing frameworks and identified various issues related to its architectural design, QoS metrics, implementation details, applications and communication modes. We have proposed taxonomy for fog computing frameworks based on the existing literature and compared the different research work based on taxonomy. Finally, various open research challenges and promising future directions are highlighted for further research in the area of fog computing.},
  archive      = {J_JPDC},
  author       = {Jagdeep Singh and Parminder Singh and Sukhpal Singh Gill},
  doi          = {10.1016/j.jpdc.2021.06.005},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {56-85},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Fog computing: A taxonomy, systematic review, current trends and research challenges},
  volume       = {157},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Profit maximization for security-aware task offloading in
edge-cloud environment. <em>JPDC</em>, <em>157</em>, 43–55. (<a
href="https://doi.org/10.1016/j.jpdc.2021.05.016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile devices (MDs) and applications are receiving extensive popularity and attracting significant attention. Mobile applications, especially for artificial intelligence (AI) applications, require powerful computation-intensive resources. Hence, running all the AI applications on a single MD introduces high energy consumption and application delay, as it has limited battery capacity and computation resources. Fortunately, the emerging edge-cloud computing (ECC) architecture pushes the computation resource to both the network edge and remote cloud to cope with challenging AI applications. Although the advantage of ECC greatly benefits various mobile applications, data security remains an important open issue in this scenario, which has not been well studied. This paper focuses on the profit maximization (PM) problem for security-aware task offloading in an ECC environment, i.e., considering the tasks from MDs with different service demands, edge nodes should decide them to be processed on the edge node or the remote cloud with a security guarantee. Specifically, we first construct the security model to measure the time overhead for each task under various scenarios. We then formulate the PM problem by jointly considering the security demand and deadline constraints of tasks. Finally, we propose a genetic algorithm-based PM (GA-PM) algorithm, the coding strategy of which considers the task execution location and execution order. Moreover, the crossover and mutation operations are implemented based on the coding strategy. Extensive simulation experiments with various parameters varying demonstrate that our GA-PM can achieve better performance than all the comparison algorithms.},
  archive      = {J_JPDC},
  author       = {Zhongjin Li and Victor Chang and Haiyang Hu and Dongjin Yu and Jidong Ge and Binbin Huang},
  doi          = {10.1016/j.jpdc.2021.05.016},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {43-55},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Profit maximization for security-aware task offloading in edge-cloud environment},
  volume       = {157},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sigmoid: An auto-tuned load balancing algorithm for
heterogeneous systems. <em>JPDC</em>, <em>157</em>, 30–42. (<a
href="https://doi.org/10.1016/j.jpdc.2021.06.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A challenge that heterogeneous system programmers face is leveraging the performance of all the devices that integrate the system. This paper presents Sigmoid, a new load balancing algorithm that efficiently co-executes a single OpenCL data-parallel kernel on all the devices of heterogeneous systems. Sigmoid splits the workload proportionally to the capabilities of the devices, drastically reducing response time and energy consumption. It is designed around several features; it is dynamic, adaptive, guided and effortless, as it does not require the user to give any parameter, adapting to the behaviour of each kernel at runtime. To evaluate Sigmoid&#39;s performance, it has been implemented in Maat, a system abstraction library. Experimental results with different kernel types show that Sigmoid exhibits excellent performance, reaching a utilization of 90\%, together with energy savings up to 20\%, always reducing programming effort compared to OpenCL, and facilitating the portability to other heterogeneous machines.},
  archive      = {J_JPDC},
  author       = {Borja Pérez and E. Stafford and J.L. Bosque and R. Beivide},
  doi          = {10.1016/j.jpdc.2021.06.003},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {30-42},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Sigmoid: An auto-tuned load balancing algorithm for heterogeneous systems},
  volume       = {157},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Practical parallelization of scientific applications with
OpenMP, OpenACC and MPI. <em>JPDC</em>, <em>157</em>, 13–29. (<a
href="https://doi.org/10.1016/j.jpdc.2021.05.017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work aims at distilling a systematic methodology to modernize existing sequential scientific codes with a little re-designing effort, turning an old codebase into modern code, i.e., parallel and robust code. We propose a semi-automatic methodology to parallelize scientific applications designed with a purely sequential programming mindset, possibly using global variables, aliasing , random number generators , and stateful functions. We demonstrate that the same methodology works for the parallelization in the shared memory model (via OpenMP), message passing model (via MPI), and General Purpose Computing on GPU model (via OpenACC). The method is demonstrated parallelizing four real-world sequential codes in the domain of physics and material science. The methodology itself has been distilled in collaboration with MSc students of the Parallel Computing course at the University of Torino, that applied it for the first time to the project works that they presented for the final exam of the course. Every year the course hosts some special lectures from industry representatives, who present how they use parallel computing and offer codes to be parallelized.},
  archive      = {J_JPDC},
  author       = {Marco Aldinucci and Valentina Cesare and Iacopo Colonnelli and Alberto Riccardo Martinelli and Gianluca Mittone and Barbara Cantalupo and Carlo Cavazzoni and Maurizio Drocco},
  doi          = {10.1016/j.jpdc.2021.05.017},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {13-29},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Practical parallelization of scientific applications with OpenMP, OpenACC and MPI},
  volume       = {157},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). PUMIPic: A mesh-based approach to unstructured mesh
particle-in-cell on GPUs. <em>JPDC</em>, <em>157</em>, 1–12. (<a
href="https://doi.org/10.1016/j.jpdc.2021.06.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unstructured mesh particle-in-cell, PIC, simulations executing on the current and next generation of massively parallel systems require new methods for both the mesh and particles to achieve performance and scalability on GPUs . The traditional approach to implementing PIC simulations defines data structures and algorithms in terms of particles with a full copy of the unstructured mesh on every process. To effectively scale the unstructured mesh and particles, mesh-based PIC uses the unstructured mesh as the predominant data structure with the particles stored in terms of the mesh entities. This paper details the PUMIPic library, a framework for developing efficient and performance-portable mesh-based PIC simulations on GPU systems. A pseudo physics simulation based on a five-dimensional gyro-kinetic code for modeling plasma physics is used to examine the performance of PUMIPic. Scaling studies of the unstructured mesh partition and number of particles are performed up to 4096 nodes of the Summit system at Oak Ridge National Laboratory. The studies show that mesh-based PIC can utilize a partitioned mesh and maintain scaling up to system limitations.},
  archive      = {J_JPDC},
  author       = {Gerrett Diamond and Cameron W. Smith and Chonglin Zhang and Eisung Yoon and Mark S. Shephard},
  doi          = {10.1016/j.jpdc.2021.06.004},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {1-12},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {PUMIPic: A mesh-based approach to unstructured mesh particle-in-cell on GPUs},
  volume       = {157},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Cross-domain secure data sharing using blockchain for
industrial IoT. <em>JPDC</em>, <em>156</em>, 176–184. (<a
href="https://doi.org/10.1016/j.jpdc.2021.05.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Industrial Internet of Things (IIoT) enhances smart manufacturing process that escalates productivity through revolutionary techniques. The manufacturing process is sophisticated and complex because of various IoT domains (e.g. industries). A final product is an outcome of the efforts of several departments from different industries. However, this raises the cross-domain communication&#39;s privacy and security issues. Cross-domain data sharing for product manufacturing is a challenging research direction. This paper proposes a centralized cloud-based cross-domain data sharing platform using multiple security gateways. The security gateways use the blockchain to store the information into the centralized cloud. Once the application reported a malicious activity, the centralized cloud verifies the concern from the blockchain. Further, an action is taken against the party that performs malicious activity in the security gateways. The algorithms are designed for authentication and transaction of data. The proposed framework is able the secure data movement among different domains globally. The experiment result demonstrates that the proposed security and privacy framework helps to maintain trust among the industries that collaborate on manufacturing across the domains.},
  archive      = {J_JPDC},
  author       = {Parminder Singh and Mehedi Masud and M. Shamim Hossain and Avinash Kaur},
  doi          = {10.1016/j.jpdc.2021.05.007},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {176-184},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Cross-domain secure data sharing using blockchain for industrial IoT},
  volume       = {156},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Certificateless privacy preserving public auditing for
dynamic shared data with group user revocation in cloud storage.
<em>JPDC</em>, <em>156</em>, 163–175. (<a
href="https://doi.org/10.1016/j.jpdc.2021.06.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing popularity of data sharing among users of a group in clouds, shared data auditing has become an important issue in the cloud auditing field. To address this issue, many shared data auditing schemes have been proposed in the literature based on either public key infrastructure (PKI) or identity-based cryptography (IBC). However, these schemes suffer from issues of complex certificate management or key escrow problem. To address these problems, recently, a certificateless shared auditing scheme was put forward. However, it cannot support data dynamics and does not protect data privacy against a verifier, i.e., the verifier can derive data content when verifying the data integrity, which affects the scheme&#39;s security. This paper proposes certificateless privacy preserving public auditing scheme for dynamic shared data with group user revocation in cloud storage (CLPPPA). CLPPPA protects the privacy of data from the verifier by leveraging a random masking technique. Further, CLPPPA also supports shared data dynamics and group user revocation. We formally prove the security of CLPPPA under computational Diffie-Hellman (CDH) and discrete logarithm (DL) assumptions in the Random Oracle Model (ROM). The performance of CLPPPA is evaluated by theoretical analysis, experimental results, and compared with the state-of-the-art ones. The results demonstrate that CLPPPA achieves desirable efficiency.},
  archive      = {J_JPDC},
  author       = {Jaya Rao Gudeme and Syamkumar Pasupuleti and Ramesh Kandukuri},
  doi          = {10.1016/j.jpdc.2021.06.001},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {163-175},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Certificateless privacy preserving public auditing for dynamic shared data with group user revocation in cloud storage},
  volume       = {156},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Leveraging teaching on demand: Approaching HPC to
undergrads. <em>JPDC</em>, <em>156</em>, 148–162. (<a
href="https://doi.org/10.1016/j.jpdc.2021.05.015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High Performance Computing (HPC) is a highly demanded discipline in companies and institutions. However, as students and also afterwards as professors, we observed a lack of HPC related content in the engineering degrees at our university, including Computer Science. Thus, we designed and offered the engineering students a non-mandatory course entitled “Build your own cluster employing Raspberry Pi” to provide the students with HPC skills. With this course, we covered the basics of supercomputing (hardware, networking, software tools , performance evaluation, cluster management, etc.). This was possible thanks to leveraging the flexibility and versatility of Raspberry Pi devices, and the students&#39; motivation that arose from the hands-on experience. Moreover, the course included a “Teaching on demand” component to let the attendees choose a field to explore, based on their own interests. In this paper, we offer all the details to let anyone fully reproduce the course. Besides, we analyze and evaluate the methodology that let us fulfill our objectives: increase the students&#39; HPC skills and knowledge in such a way that they feel capable of utilizing it in their mid-term professional career.},
  archive      = {J_JPDC},
  author       = {Sandra Catalán and Rocío Carratalá-Sáez and Sergio Iserte},
  doi          = {10.1016/j.jpdc.2021.05.015},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {148-162},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Leveraging teaching on demand: Approaching HPC to undergrads},
  volume       = {156},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SparkDQ: Efficient generic big data quality management on
distributed data-parallel computation. <em>JPDC</em>, <em>156</em>,
132–147. (<a href="https://doi.org/10.1016/j.jpdc.2021.05.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the big data era, large amounts of data are under generation and accumulation in various industries. However, users usually feel hindered by the data quality issues when extracting values from the big data. Thus, data quality issues are gaining more and more attention from data quality management analysts. Cutting-edge solutions like data ETL, data cleaning, and data quality monitoring systems have many deficiencies in capability and efficiency, making it difficult to cope with complicated situations on big data. These problems inspire us to build SparkDQ, a generic distributed data quality management model and framework that provides a series of data quality detection and repair interfaces. Users can quickly build custom tasks of data quality computing for various needs by utilizing these interfaces. In addition, SparkDQ implements a set of algorithms that in a parallel manner with optimizations. These algorithms aim at various data quality goals. We also propose several system-level optimizations, including the job-level optimization with multi-task execution scheduling and the data-level optimization with data state caching. The experimental evaluation shows that the proposed distributed algorithms in SparkDQ run up to 12 times faster compared to the corresponding stand-alone serial and multi-thread algorithms. Compared with the cutting-edge distributed data quality solution Apache Griffin, SparkDQ has more features, and its execution time is only around half of Apache Griffin on average. SparkDQ achieves near-linear data and node scalability.},
  archive      = {J_JPDC},
  author       = {Rong Gu and Yang Qi and Tongyu Wu and Zhaokang Wang and Xiaolong Xu and Chunfeng Yuan and Yihua Huang},
  doi          = {10.1016/j.jpdc.2021.05.012},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {132-147},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {SparkDQ: Efficient generic big data quality management on distributed data-parallel computation},
  volume       = {156},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Preface – special issue advances on high performance
computing for artificial intelligence. <em>JPDC</em>, <em>156</em>, 131.
(<a href="https://doi.org/10.1016/j.jpdc.2021.06.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JPDC},
  author       = {Marcos Dias de Assuncao and Eduardo Rocha Rodrigues and Bruno Raffin},
  doi          = {10.1016/j.jpdc.2021.06.002},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {131},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Preface – special issue advances on high performance computing for artificial intelligence},
  volume       = {156},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hadoop perfect file: A fast and memory-efficient metadata
access archive file to face small files problem in HDFS. <em>JPDC</em>,
<em>156</em>, 119–130. (<a
href="https://doi.org/10.1016/j.jpdc.2021.05.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {HDFS faces several issues when it comes to handling a large number of small files. These issues are well addressed by archive systems , which combine small files into larger ones. They use index files to hold relevant information for retrieving a small file content from the big archive file. However, existing archive-based solutions require significant overheads when retrieving a file content since additional processing and I/Os are needed to acquire the retrieval information before accessing the actual file content, therefore, deteriorating the access efficiency. This paper presents a new archive file named Hadoop Perfect File (HPF). HPF minimizes access overheads by directly accessing metadata from the part of the index file containing the information. It consequently reduces the additional processing and I/Os needed and improves the access efficiency from archive files. Our index system uses two hash functions . Metadata records are distributed across index files using a dynamic hash function. We further build an order-preserving perfect hash function that memorizes the position of a small file&#39;s metadata record within the index file.},
  archive      = {J_JPDC},
  author       = {Yanlong Zhai and Jude Tchaye-Kondi and Kwei-Jay Lin and Liehuang Zhu and Wenjun Tao and Xiaojiang Du and Mohsen Guizani},
  doi          = {10.1016/j.jpdc.2021.05.011},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {119-130},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Hadoop perfect file: A fast and memory-efficient metadata access archive file to face small files problem in HDFS},
  volume       = {156},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bounding schemes for the parallel machine scheduling problem
with DeJong’s learning effect. <em>JPDC</em>, <em>156</em>, 101–118. (<a
href="https://doi.org/10.1016/j.jpdc.2021.05.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, the parallel machine scheduling problem with DeJong&#39;s learning effect, is addressed. The objective function to be minimized is the makespan. This problem is proofed to be NP-Hard in the strong sense. This is the challenging theoretical side of the studied problem. Furthermore, several real-life situations in manufacturing and computer science are modeled using the current problem. Several algorithms intended to solve the studied problem within a reasonable computing time are proposed in literature. Among these algorithms the exact methods, which failed to solve the studied problem to optimality even for small size instances. In this paper several new heuristics and meta-heuristics are proposed. These heuristics are classified into three types. The first type is based on Longest Processing Time ( LPT ) rule. The innovation is the modification of the LPT rule in a way to cope efficiently with the learning effect concept, by randomizing the selection of the next scheduled job. The second type of heuristics, is taking advantage of an exact Branch and Bound algorithm , developed originally for the classical parallel machine scheduling problem. The contribution for this kind of heuristics is lying in the modification of the processing time values, according to a prefixed selected functions. The third type of methods is based in an adaptation of the Genetic Algorithm to the learning effect concept. This adaptation consists in enlarging the area of selection of the parameters values. To assess the performance and the efficiency of the proposed heuristics, a newly developed lower bound is proposed. This lower bound is based on a relaxation of the studied problem, which allows to obtain a minimum cost flow problem. Finally, an extensive experimental study is conducted over a benchmark test problems. The obtained results provide strong evidence that the proposed procedures outperform the earlier existing ones.},
  archive      = {J_JPDC},
  author       = {Mahdi Jemmali and Lotfi Hidri},
  doi          = {10.1016/j.jpdc.2021.05.003},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {101-118},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Bounding schemes for the parallel machine scheduling problem with DeJong&#39;s learning effect},
  volume       = {156},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Mitigating the processor aging through dynamic concurrency
throttling. <em>JPDC</em>, <em>156</em>, 86–100. (<a
href="https://doi.org/10.1016/j.jpdc.2021.05.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increase in the number of cores in a single chip brings better capabilities to exploit thread-level parallelism (TLP). However, since power dissipated per area rises at each new node generation, higher temperatures are achieved, speeding up the aging of hardware components, which may provoke undesired system behavior. Considering that many applications do not scale with the number of cores, their execution with the maximum TLP available will not only degrade performance, but also unnecessarily increase temperature, further accelerating aging. Given that, we propose Hebe, a dynamic concurrency throttling approach that learns at run-time the degree of TLP that reduces the aging for OpenMP applications. Hebe is totally transparent, needing no modifications in the original binary code . With a set of extensive experiments (fifteen benchmarks and four multicore platforms), we show that Hebe outperforms state-of-the-art approaches with very close results from the best possible solution given by an exhaustive search .},
  archive      = {J_JPDC},
  author       = {Thiarles S. Medeiros and Luan Pereira and Fábio D. Rossi and Marcelo C. Luizelli and Antonio Carlos S. Beck and Arthur F. Lorenzon},
  doi          = {10.1016/j.jpdc.2021.05.006},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {86-100},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Mitigating the processor aging through dynamic concurrency throttling},
  volume       = {156},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An improved framework of GPU computing for CFD applications
on structured grids using OpenACC. <em>JPDC</em>, <em>156</em>, 64–85.
(<a href="https://doi.org/10.1016/j.jpdc.2021.05.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work is focused on improving multi-GPU performance of a research CFD code on structured grids. MPI and PGI 18.1 OpenACC directives are used to scale the code up to 16 NVIDIA GPUs. This work shows that using 16 P100 GPUs and 16 V100 GPUs can be up to 30× and 90× faster than 16 Xeon CPU E5-2680v4 cores for three different test cases, respectively. A series of performance issues related to the scaling for the multi-block CFD code are addressed by applying various optimizations. Performance optimizations such as the pack/unpack message method, removing temporary arrays as arguments to procedure calls, allocating global memory for limiters and connected boundary data, reordering non-blocking MPI I_send/I_recv and blocking Wait calls, reducing unnecessary implicit derived type member data movement between the host and the device and the use of GPUDirect can improve the compute utilization, memory throughput, and asynchronous progression in the multi-block CFD code using modern programming features.},
  archive      = {J_JPDC},
  author       = {Weicheng Xue and Charles W. Jackson and Christoper J. Roy},
  doi          = {10.1016/j.jpdc.2021.05.010},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {64-85},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {An improved framework of GPU computing for CFD applications on structured grids using OpenACC},
  volume       = {156},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Teaching parallel and distributed computing concepts in
simulation with WRENCH. <em>JPDC</em>, <em>156</em>, 53–63. (<a
href="https://doi.org/10.1016/j.jpdc.2021.05.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Teaching parallel and distributed computing topics in a hands-on manner is challenging, especially at introductory, undergraduate levels. Participation challenges arise due to the need to provide students with an appropriate compute platform, which is not always possible. Even if a platform is provided to students, not all relevant learning objectives can be achieved via hands-on learning on a single platform. In particular, it is typically not feasible to provide students with platform configurations representative of emerging and future cyberinfrastructure scenarios (e.g., highly distributed, heterogeneous platforms with large numbers of high-end compute nodes). To address these challenges, we have developed a set of pedagogic modules that can be integrated piecemeal into university courses. These modules include simulation-driven activities for students to experience relevant application and platform scenarios hands-on. These activities are supported by simulators developed using the WRENCH simulation framework. After motivating and describing our approach, we present and analyze results obtained from evaluations performed in two consecutive offerings of an undergraduate university course.},
  archive      = {J_JPDC},
  author       = {Henri Casanova and Ryan Tanaka and William Koch and Rafael Ferreira da Silva},
  doi          = {10.1016/j.jpdc.2021.05.009},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {53-63},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Teaching parallel and distributed computing concepts in simulation with WRENCH},
  volume       = {156},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A learning experience toward the understanding of
abstraction-level interactions in parallel applications. <em>JPDC</em>,
<em>156</em>, 38–52. (<a
href="https://doi.org/10.1016/j.jpdc.2021.05.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the curriculum of a Computer Engineering program, concepts like parallelism , concurrency, consistency, or atomicity are usually addressed in separate courses due to their thoroughness and extension. Isolating such concepts in courses helps students not only to focus on specific aspects, but also to experience the reality of working with modern computer systems, where those concepts are often detached in different abstraction levels. However, due to such an isolation, it exists a risk of inducing to the students an absence of interactions between these concepts, and, by extension, between the different abstraction levels of a system. This paper proposes a learning experience showcasing the interactions between abstraction levels addressed in laboratory sessions of different courses. The driving example is a parallel ray tracer. In the different courses, students implement and assemble components of this application from the algorithmic level of the tracer to the assembly instructions required to guarantee atomicity. Each lab focuses on a single abstraction level, but shows students the interactions with the rest of the levels. Technical results and student learning outcomes through the analysis of surveys validate the proposed experience and confirm the students learning improvement with a more integrated view of the system.},
  archive      = {J_JPDC},
  author       = {Alejandro Valero and Rubén Gran-Tejero and Darío Suárez-Gracia and Emanuel A. Georgescu and Joaquín Ezpeleta and Pedro Álvarez and Adolfo Muñoz and Luis M. Ramos and Pablo Ibáñez},
  doi          = {10.1016/j.jpdc.2021.05.008},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {38-52},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {A learning experience toward the understanding of abstraction-level interactions in parallel applications},
  volume       = {156},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SMS observer: A dynamic mechanism to analyze the behavior of
SMS-based malware. <em>JPDC</em>, <em>156</em>, 25–37. (<a
href="https://doi.org/10.1016/j.jpdc.2021.05.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays smartphones become an indispensable tool in many people&#39;s everyday life that makes themselves attractive targets for attackers. Among various malware targeting at smartphones, SMS-based malware is one of the most notorious ones. Though a number of Android dynamic analysis frameworks have been proposed to analyze SMS-based malware, most of these frameworks or some Android tools, such as Google Android Emulator, do not support an app or malware to send SMS messages to a real smartphone; hence, security researchers cannot use them directly to analyze the behavior of SMS-based malware. In our previous work, SMS Helper , we designed an application layer tool to allow an app or malware in an Android emulator to send and receive SMS messages to or from a real smartphone. Based on SMS Helper, this paper proposes an Android dynamic analysis framework, called SMS Observer , to assist security researchers to analyze SMS-based malware. SMS Observer integrates SMS Helper into it as a client agent, meanwhile, and it maintains the integrity of system logs. This paper also figures out a way to detect whether an app is executed in an emulator and describes how to use SMS Observer to prevent such evasion. Experimental results using real-world malware samples show SMS Observer is much more effective in detecting SMS-related behavior of SMS-based malware than existing frameworks, such as Google Android Emulator, Andrubis, CopperDroid, and DroidBox. SMS Observer can analyze sophisticated SMS-based malware samples and provide a comprehensive view of malicious behavior.},
  archive      = {J_JPDC},
  author       = {Chun-Yi Wang and Chi-Yu You and Fu-Hau Hsu and Chia-Hao Lee and Che-Hao Liu and YungYu Zhuang},
  doi          = {10.1016/j.jpdc.2021.05.004},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {25-37},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {SMS observer: A dynamic mechanism to analyze the behavior of SMS-based malware},
  volume       = {156},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An optimal cluster-based routing algorithm for lifetime
maximization of internet of things. <em>JPDC</em>, <em>156</em>, 7–24.
(<a href="https://doi.org/10.1016/j.jpdc.2021.05.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge computing for Internet of Things (IoT) is a promising framework that can help small devices such as low-powered sensor nodes to accomplish complex computational tasks. The limited power supply of sensor nodes is one of their major limitations. A successful approach to improve the network lifetime and the overall scalability of the IoT supported wireless sensor networks (WSNs) is clustering. However, in a clustered IoT supported WSNs, some of the Cluster Heads (CHs) bear more traffic load than the others and therefore die sooner leading to decrease the network lifetime. To overcome this problem and maximize the network lifetime, the load of the CHs must be balanced. This research work suggests a new clustering method to balance the traffic load imposed on the cluster heads in IoT supported WSNs. The proposed clustering method uses a 1.2-approximation algorithm. In addition, we introduce an energy-aware routing algorithm for transmitting data packets from the CHs to their destination. The proposed routing algorithm distributes the communication load of the data packets among more nodes near the destination by a proper segmentation of the area. The simulation results show that the proposed clustering and routing algorithms in addition to being practical for large-scale IoT supported WSNs, cause the network to have a better performance compared to other similar algorithms.},
  archive      = {J_JPDC},
  author       = {Ramin Yarinezhad and Masoud Sabaei},
  doi          = {10.1016/j.jpdc.2021.05.005},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {7-24},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {An optimal cluster-based routing algorithm for lifetime maximization of internet of things},
  volume       = {156},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A note on labeling methods to schedule unit execution time
tasks in the presence of delayed precedence constraints. <em>JPDC</em>,
<em>156</em>, 1–6. (<a
href="https://doi.org/10.1016/j.jpdc.2021.05.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is some evidence that labeling schemes as employed for instance in the famous Coffman-Graham algorithm may provide superior worst-case approximation guarantees than purely path- or level-based list schedules in the presence of (delayed) precedence constraints. In 1989, Bernstein, Rodeh, and Gertner proved that this also holds true for their labeling scheme targeting unit execution time tasks on a single processor provided that all delays imposed by a single task on all of its successors are uniformly either zero or some fixed positive integer. They further conjectured that this superiority is preserved when allowing the delays imposed by a task to differ successor-wise. It is shown in this note however that their labeling scheme as well as more general ones may perform as bad as any list schedule in this case. Moreover, a new lower bound on the worst-case performance of labeling methods in the multiprocessor setting is derived.},
  archive      = {J_JPDC},
  author       = {Sven Mallach},
  doi          = {10.1016/j.jpdc.2021.05.002},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {1-6},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {A note on labeling methods to schedule unit execution time tasks in the presence of delayed precedence constraints},
  volume       = {156},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). PAASH: A privacy-preserving authentication and fine-grained
access control of outsourced data for secure smart health in smart
cities. <em>JPDC</em>, <em>155</em>, 101–119. (<a
href="https://doi.org/10.1016/j.jpdc.2021.05.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Very recently, a number of intelligent applications have evolved in smart cities such as smart grid, smart parking, smart waste management, smart manufacturing, smart home, and smart health, making our cities more smarter. In the context of smart health, smart devices are employed to collect and transmit biomedical information to a medical server, allowing medical practitioners to give better and improved healthcare services. However, considering the sensitive nature of the medical data, any illegal access may have devastating effects on the patient&#39;s health condition. A lightweight privacy-preserving authentication and fine-grained access control scheme for smart health known as PAASH is proposed. In PAASH, a new ultraefficient certificateless signature scheme with compact aggregation is developed to achieve source authentication and data integrity. To achieve efficient user authorization and data confidentiality, a privacy-preserving access control technique using the developed aggregate signature and ciphertext-policy attribute-based encryption (CP-ABE) is implemented. An analysis of PAASH reveals that it can provide desirable privacy and security measures in smart health. Moreover, the formal security demonstrated in the random oracle model (ROM) shows that PAASH is semantically secure under the intractability of the Discrete Logarithm Problem (DLP). The performance evaluation shows that PAASH is par excellence practically suitable for smart health.},
  archive      = {J_JPDC},
  author       = {Sunday Oyinlola Ogundoyin and Ismaila Adeniyi Kamil},
  doi          = {10.1016/j.jpdc.2021.05.001},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {101-119},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {PAASH: A privacy-preserving authentication and fine-grained access control of outsourced data for secure smart health in smart cities},
  volume       = {155},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Scalable flow probe architecture for 100 gbps+ rates on
commodity hardware: Design considerations and approach. <em>JPDC</em>,
<em>155</em>, 87–100. (<a
href="https://doi.org/10.1016/j.jpdc.2021.04.015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The unrelenting growth of data intensive applications has been raising the bar for performance of high speed networks. To cater for this growth, the core segments of networks today are based on 100 Gbps links. Analyzing the huge volume of traffic over these networks is a challenging yet essential task from the perspectives of network administration, network security and law enforcement. In spite of the availability of adequate network, compute, and memory resources, designing traffic analysis solutions performing at line rate for high speed traffic brings up several challenges. Traffic analysis solutions built on commodity compute platforms bring distinct advantages in terms of cost, adaptability, up-gradation and scalability. Keeping this in view, this paper explores the feasibility of designing high speed traffic analysis solutions that can handle 100s of Gbps on commodity compute platforms. It begins this process by analyzing the design issues in line rate handling of traffic on commodity servers with 100 GbE NIC and by bringing out an optimized and scalable packet processing pipeline. Leveraging this processing pipeline, NAPA-FP, a NUMA aware flow probe architecture, has been designed. With an implementation of this architecture on a commodity server, we show that line rate processing of Internet-like traffic from a 100 GbE interface can be achieved with a single NUMA node, by using a suitably configured packet processing path. We also show that its performance scales linearly to multiple 100 Gbps with additional NUMA nodes and NICs. In particular, the implementation on a 4-socket commodity server with 3 × 100 3×100 GbE NICs is able to process 3 × 100 3×100 Gbps Internet-like traffic at line rate using 3 NUMA nodes. The optimizations with respect to resource allocation, sharing and processing pipeline are reported with corroborating experimental results.},
  archive      = {J_JPDC},
  author       = {Raktim Bhattacharjee and R. Rajesh and K.R. Prasanna Kumar and Vinu Paul MV and G. Athithan and A.V. Sahadevan},
  doi          = {10.1016/j.jpdc.2021.04.015},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {87-100},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Scalable flow probe architecture for 100 gbps+ rates on commodity hardware: Design considerations and approach},
  volume       = {155},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Letting future programmers experience performance-related
tasks. <em>JPDC</em>, <em>155</em>, 74–86. (<a
href="https://doi.org/10.1016/j.jpdc.2021.04.014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Programming courses usually focus on software-engineering problems like software decomposition and code maintenance. While computer-science lessons emphasize algorithm complexity, technological problems are usually neglected although they may significantly affect the performance in terms of wall time. As the technological problems are best explained by hands-on experience, we present a set of homework assignments focused on a range of technologies from instruction-level parallelism to GPU programming to cluster computing. These assignments are a product of a decade of development and testing on live subjects – the students of three performance-related software courses at the Faculty of Mathematics and Physics of the Charles University in Prague.},
  archive      = {J_JPDC},
  author       = {David Bednárek and Martin Kruliš and Jakub Yaghob},
  doi          = {10.1016/j.jpdc.2021.04.014},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {74-86},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Letting future programmers experience performance-related tasks},
  volume       = {155},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Efficiency and scalability of multi-lane capsule networks
(MLCN). <em>JPDC</em>, <em>155</em>, 63–73. (<a
href="https://doi.org/10.1016/j.jpdc.2021.04.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Some Deep Neural Networks (DNN) have what we call lanes, or they can be reorganized as such. Lanes are paths in the network which are data-independent and typically learn different features or add resilience to the network. Given their data-independence, lanes are amenable for parallel processing . The Multi-lane CapsNet (MLCN) is a proposed reorganization of the Capsule Network which is shown to achieve better accuracy while bringing highly-parallel lanes. However, the efficiency and scalability of MLCN had not been systematically examined. In this work, we study the MLCN network with multiple GPUs finding that it is 2x more efficient than the original CapsNet when using model-parallelism. We introduce the load balancing problem of distributing heterogeneous lanes in homogeneous or heterogeneous accelerators and show that a simple greedy heuristic can be almost 50\% faster than a naïve random approach. Further, we show that we can generate MLCN models with heterogeneous lanes with a balanced fit for a given set of devices. We describe a Neural Architectural Search generating MLCN models matching the device&#39;s memory that are load balanced . This search discovered models with 18.6\% better accuracy for CIFAR-10 .},
  archive      = {J_JPDC},
  author       = {Vanderson Martins do Rosario and Mauricio Breternitz Jr. and Edson Borin},
  doi          = {10.1016/j.jpdc.2021.04.010},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {63-73},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Efficiency and scalability of multi-lane capsule networks (MLCN)},
  volume       = {155},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The cluster coffer: Teaching HPC on the road. <em>JPDC</em>,
<em>155</em>, 50–62. (<a
href="https://doi.org/10.1016/j.jpdc.2021.04.013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Teaching parallel programming and HPC is a difficult task. There is a large number of sophisticated hardware and software components, each complex on their own and often showing non-intuitive interaction when used in combination. We consider education in HPC among the more difficult topics in computer science due to the fact that larger distributed memory systems are ubiquitous yet inaccessible and intangible to students. In this work, we present the Cluster Coffer, a miniature cluster computer based on 16 ARM compute boards that we believe is suitable for reducing the entry barrier to HPC in teaching and public outreach. We discuss our design goals for providing a portable, inexpensive system that is easy to maintain and repair. We outline the implementation path we took in terms of hardware and software, in order to provide others with the information required to reproduce and extend our work. Finally, we present two use cases for which the Cluster Coffer has been used multiple times, and will continue to be used in the upcoming years.},
  archive      = {J_JPDC},
  author       = {Philipp Gschwandtner and Alexander Hirsch and Peter Thoman and Peter Zangerl and Herbert Jordan and Thomas Fahringer},
  doi          = {10.1016/j.jpdc.2021.04.013},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {50-62},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {The cluster coffer: Teaching HPC on the road},
  volume       = {155},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Efficient traversal of decision tree ensembles with FPGAs.
<em>JPDC</em>, <em>155</em>, 38–49. (<a
href="https://doi.org/10.1016/j.jpdc.2021.04.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {System-on-Chip (SoC) based Field Programmable Gate Arrays (FPGAs) provide a hardware acceleration technology that can be rapidly deployed and tuned, thus providing a flexible solution adaptable to specific design requirements and to changing demands. In this paper, we present three SoC architecture designs for speeding-up inference tasks based on machine learned ensembles of decision trees . We focus on QuickScorer , the state-of-the-art algorithm for the efficient traversal of tree ensembles and present the issues and the advantages related to its deployment on two SoC devices with different capacities. The results of the experiments conducted using publicly available datasets show that the solution proposed is very efficient and scalable. More importantly, it provides almost constant inference times, independently of the number of trees in the model and the number of instances to score. This allows the SoC solution deployed to be fine tuned on the basis of the accuracy and latency constraints of the application scenario considered.},
  archive      = {J_JPDC},
  author       = {Romina Molina and Fernando Loor and Veronica Gil-Costa and Franco Maria Nardini and Raffaele Perego and Salvatore Trani},
  doi          = {10.1016/j.jpdc.2021.04.008},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {38-49},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Efficient traversal of decision tree ensembles with FPGAs},
  volume       = {155},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A cloud framework for problem-based learning on grid
computing. <em>JPDC</em>, <em>155</em>, 24–37. (<a
href="https://doi.org/10.1016/j.jpdc.2021.04.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Training on Grid technologies has traditionally used existing Grid infrastructures to implement the hands-on education activities. However, these infrastructures are insufficient to develop all training skills as they can only be employed for the development of Grid applications, and they are limited for learning the management and configuration of Grid resources . The paper presents a set of educational activities grouped on a Project Based Learning (PBL) framework for training on Grid technologies. A Cloud-based tool has been implemented to provide Grid infrastructures as a Service on the cloud, with enhanced scalability and administration capabilities. The PBL has achieved a high impact in the teaching-learning process, addressing the training in all the necessary skills and efficiently providing Grid infrastructure resources on public clouds at a moderate cost. Finally, we evaluated the students&#39; opinion on the activities achieving a very satisfactory result and a reasonable balance on the complexity of the PBL stages.},
  archive      = {J_JPDC},
  author       = {J. Damian Segrelles Quilis and Germán Moltó and Ignacio Blanquer},
  doi          = {10.1016/j.jpdc.2021.04.012},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {24-37},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {A cloud framework for problem-based learning on grid computing},
  volume       = {155},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A petri net extension for systems of concurrent
communicating agents with durable actions. <em>JPDC</em>, <em>155</em>,
14–23. (<a href="https://doi.org/10.1016/j.jpdc.2021.04.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper provides a true-concurrency approach for the specification and verification of systems of concurrent communicating agents with durable actions. We present high-level Petri nets with durable actions (DaHL) to cope with various details in such complex systems. We define a DaHL module as an open variant of time-dependent colored Petri nets . A DaHL system is a fused set of modules for systems consisting of concurrent agents which can interact with each other. We also introduce hybrid-based reachability graph that covers the entire state space of DaHL systems with a true-concurrency semantics. We show that such reachability graph allows us to check important properties such as deadlock-freeness, liveness, home space, and reversibility, and also to predict timing properties prior to real implementation. A case study is used to model and analyze a simple scenario where autonomous vehicles are able to transport containers freely in an enterprise environment.},
  archive      = {J_JPDC},
  author       = {Khalil Mecheraoui and Irina A. Lomazova and Nabil Belala},
  doi          = {10.1016/j.jpdc.2021.04.011},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {14-23},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {A petri net extension for systems of concurrent communicating agents with durable actions},
  volume       = {155},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On the implementation of memory reclamation methods in a
lock-free hash trie design. <em>JPDC</em>, <em>155</em>, 1–13. (<a
href="https://doi.org/10.1016/j.jpdc.2021.04.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hash tries are a trie-based data structure with nearly ideal characteristics for the implementation of hash maps. Starting from a particular lock-free hash map data structure , named Lock-Free Hash Tries , we focus on solving the problem of memory reclamation without losing the lock-freedom property. To the best of our knowledge, outside garbage collected environments, there is no current implementation of hash maps that is able to reclaim memory in a lock-free manner. To achieve this goal, we propose an approach for memory reclamation specific to Lock-Free Hash Tries that explores the characteristics of its structure in order to achieve efficient memory reclamation with low and well-defined memory bounds. We present and discuss in detail the key algorithms required to easily reproduce our implementation by others. Experimental results show that our approach obtains better results when compared with other state-of-the-art memory reclamation methods and provides a competitive and scalable hash map implementation, if compared to lock-based implementations.},
  archive      = {J_JPDC},
  author       = {Pedro Moreno and Miguel Areias and Ricardo Rocha},
  doi          = {10.1016/j.jpdc.2021.04.007},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {1-13},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {On the implementation of memory reclamation methods in a lock-free hash trie design},
  volume       = {155},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Parallel solving of multiple information-coordinated global
optimization problems. <em>JPDC</em>, <em>154</em>, 153–162. (<a
href="https://doi.org/10.1016/j.jpdc.2021.04.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes an efficient approach for the parallel solution of computationally time consuming problems of multiple global optimization, in which minimized functions can be multiextremal and calculating function values may require huge amounts of computations. The proposed approach is based on the information-statistical theory of global optimization, within which a general computational scheme of global optimization methods is proposed. In the paper, this general scheme is expanded by the possible reuse of search information obtained in the process of computations when solving multiple global optimization problems . Within the framework of the proposed generalized scheme, parallel algorithms are proposed for computational systems with shared and distributed memory. Results of computational experiments demonstrated that the proposed approach can significantly reduce the computational complexity of solving multiple global optimization problems .},
  archive      = {J_JPDC},
  author       = {Victor Gergel and Evgeniy Kozinov},
  doi          = {10.1016/j.jpdc.2021.04.009},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {153-162},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Parallel solving of multiple information-coordinated global optimization problems},
  volume       = {154},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Rival-model penalized self-organizing map enforced DDoS
attack prevention mechanism for software defined network-based cloud
computing environment. <em>JPDC</em>, <em>154</em>, 142–152. (<a
href="https://doi.org/10.1016/j.jpdc.2021.03.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reliability, and safety are considered as the indispensable twins during the adoption of the cloud computing environment since their breaches lead to catastrophic issues in poor resource management and unreliable service quality. To be specific, Distributed Denial of Service (DDoS) attack is determined as the most vulnerable threat in the cloud space as it lowers the ability of the predominant resources’ for preventing their optimal utilization. The advent of Software-Defined Networking (SDN) is estimated to wide open the feasibility in preventing DDoS attacks in the cloud space. In this paper, a Rival-Model Penalized Self-Organizing Map (RMP-SOM) enforced DDoS Attack Prevention Mechanism is proposed for the remarkable prevention of DDoS attack by utilizing the potential characteristics of SDN that focuses on the possibility of facilitating network global perspective, effective investigation of network traffic, and an enhanced process of rule updating. This proposed RMPSOM-SDNDM scheme utilizes the benefits of the constant rate in order to ensure priority to the closest neuron and its neighborhood rather than its farthest rival neuron for facilitating better detection accuracy. The simulation results of the proposed RMPSOM approach confirmed a phenomenal sensitivity, specificity and accuracy rate during the process of detecting DDoS attacks in the cloud space on par with the baseline DDoS mitigation schemes considered from the literature.},
  archive      = {J_JPDC},
  author       = {Pillutla Harikrishna and A. Amuthan},
  doi          = {10.1016/j.jpdc.2021.03.005},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {142-152},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Rival-model penalized self-organizing map enforced DDoS attack prevention mechanism for software defined network-based cloud computing environment},
  volume       = {154},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Proactive auto-scaling for cloud environments using temporal
convolutional neural networks. <em>JPDC</em>, <em>154</em>, 119–141. (<a
href="https://doi.org/10.1016/j.jpdc.2021.04.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Auto-scaling systems can dynamically scale the required resources for cloud-based services at runtime. This is an effective mechanism, enabling services to adapt to environmental changes. These systems establish the foundation for achieving elasticity in the modern cloud computing paradigm . Given the dynamic and uncertain nature of the shared cloud infrastructure, cloud auto-scaling systems are one of the most complex and sophisticated created artifacts, aiming to achieve self-aware, self-adaptive, and dependable runtime scaling. To find an effective solution to this problem, an accurate prediction of the required amount of workload as well as the system metrics for future time periods are needed. Various solutions have already been proposed to tackle this problem. Many solutions make use of machine learning , statistical, and ensemble methods . In this paper, we view the auto-scaling problem as a sequence model and apply the convolutional neural networks to predict the future workload of cloud services. Also, by using neural networks , we obtain a mapping between the predicted workload as well as the real-time and future amounts of the required resources. We have also proposed a decision-making mechanism that takes into account different and sometimes conflicting user criteria resulting in the best-compromised decision. To this aim, we have used TOPSIS as a multi-criteria decision-making method for the decision-making component. In the evaluation section, we have examined the amount of prediction error, the amount of service level agreement violations, as well as the amount of resources&#39; under-utilization. Evaluations demonstrate that the proposed approach for predicting the workload shows a 4 percent improvement over the existing approaches.},
  archive      = {J_JPDC},
  author       = {Ehsan Golshani and Mehrdad Ashtiani},
  doi          = {10.1016/j.jpdc.2021.04.006},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {119-141},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Proactive auto-scaling for cloud environments using temporal convolutional neural networks},
  volume       = {154},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MIRAGE: A consolidation aware migration avoidance genetic
job scheduling algorithm for virtualized data centers. <em>JPDC</em>,
<em>154</em>, 106–118. (<a
href="https://doi.org/10.1016/j.jpdc.2021.03.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern virtualized data centers often rely on virtual machine (VM) migrations to consolidate workload on a single machine for energy saving . But VM migrations have many drawbacks, including performance degradation , service disruption etc. Hence, many approaches have been proposed to minimize the overhead when migrations occur. In contrast, this work aims to proactively avoid migrations from happening in the first place. We have proposed a novel consolidation aware scheduling algorithm to minimize the number of migrations for batch processing systems by taking advantage of the prior knowledge of consolidation strategy and job information. We show the problem can be formulated as an integer linear programming (ILP) problem, and an effective heuristic solution can be found by a genetic algorithm . Both real and synthetic workload traces were used to evaluate our methods. Experimental results showed that, after comparing with two popular job scheduling algorithms, our approach has reduced the number of migrations by more than 25\%.},
  archive      = {J_JPDC},
  author       = {Satyajit Padhy and Jerry Chou},
  doi          = {10.1016/j.jpdc.2021.03.004},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {106-118},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {MIRAGE: A consolidation aware migration avoidance genetic job scheduling algorithm for virtualized data centers},
  volume       = {154},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SCAB - IoTA: Secure communication and authentication for IoT
applications using blockchain. <em>JPDC</em>, <em>154</em>, 94–105. (<a
href="https://doi.org/10.1016/j.jpdc.2021.04.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Internet of Things (IoT) becomes a leading field of computer science, which simplifies the life of human beings. The IoT, devices can interconnect and communicate with each other, in a fully autonomous mode. This autonomy has made the IoT network vulnerable to various security threats. However, due to this autonomy, there is a strong need for reliable security and storage mechanism for authentication to exchange data between IoT devices. To solve this problem, we propose an efficient method known as SCAB-IoTA, which ensures identification and authentication of IoT devices and also provides secure communication in the open environment. Along with authentication and secure communication, the scheme also ensures data integrity. SCAB-IoTA used the blockchain and hybrid cryptosystem (the combination of Advanced Encryption Standard (AES) and Elliptic Curve Digital Signature Algorithm (ECDSA) cryptographic techniques) to enhance the security of IoT applications to prevent various attacks along with less computational, and storage overhead . Furthermore, we have a secure cluster of IoT devices based on angular distance (AD), so that devices can securely communicate without any interruption. A secure and robust trust mechanism has employed to build these clusters, where each IoT device has to authenticate itself before joining the cluster. The obtained results satisfy the IoT security requirements with 60\% reduced computation, storage and communication cost compared with state-of-the-art methods. The security analysis illustrates that SCAB-IoTA withstands against various cyberattacks such as impersonation, message replay, man-in-the-middle, and botnet attacks.},
  archive      = {J_JPDC},
  author       = {Lokendra Vishwakarma and Debasis Das},
  doi          = {10.1016/j.jpdc.2021.04.003},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {94-105},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {SCAB - IoTA: Secure communication and authentication for IoT applications using blockchain},
  volume       = {154},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Predicting networks-on-chip traffic congestion with spiking
neural networks. <em>JPDC</em>, <em>154</em>, 82–93. (<a
href="https://doi.org/10.1016/j.jpdc.2021.03.013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network congestion is one of the critical reasons for degradation of data throughput performance in Networks-on-Chip (NoCs), with delays caused by data-buffer queuing in routers. Local buffer or router congestion impacts on network performance as it gradually spreads to neighbouring routers and beyond. In this paper, we propose a novel approach to NoC traffic prediction using Spiking Neural Networks (SNNs) and focus on predicting local router congestion so as to minimize its impact on the overall NoCs throughput. The key novelty is utilizing SNNs to recognize temporal patterns from NoC router buffers and predicting traffic hotspots. We investigate two neural models , Leaky Integrate and Fire (LIF) and Spike Response Model (SRM) to check performance in terms of prediction coverage. Results on prediction accuracy and precision are reported using a synthetic and real-time multimedia applications with simulation results of the LIF based predictor providing an average accuracy of 88.28\%–96.25\% and precision of 82.09\%–96.73\% as compared to 85.25\%–95.69\% accuracy and 73\% and 98.48\% precision performance of SRM based model when looking at congestion formations 30 clock cycles in advance of the actual hotspot occurrence.},
  archive      = {J_JPDC},
  author       = {Aqib Javed and Jim Harkin and Liam McDaid and Junxiu Liu},
  doi          = {10.1016/j.jpdc.2021.03.013},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {82-93},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Predicting networks-on-chip traffic congestion with spiking neural networks},
  volume       = {154},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Efficient GPU-parallelization of batch plants design using
metaheuristics with parameter tuning. <em>JPDC</em>, <em>154</em>,
74–81. (<a href="https://doi.org/10.1016/j.jpdc.2021.03.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We address a practice-relevant optimization problem : optimizing multi-product batch plants , with a real-world use case study – optimal design of chemical-engineering systems. Our contribution is a novel approach to parallelizing this optimization problem on GPU (Graphics Processing Units) by combining two metaheuristics – Simulated Annealing (SA) and Ant Colony Optimization (ACO). We improve the implementation performance by tuning particular parameters of the ACO metaheuristic. Our tuning approach improves on the previous methods in two respects: (1) we do not have to rely on additional mechanisms like fuzzy logic or algorithms for online tuning; and (2) we use the high computation performance of GPU to speedup the tuning process. By parallelizing the tuning process on modern GPUs, we allow the user to experiment with large volumes of input data and find the optimal values of the ACO parameters in feasible time. Our experiments on NVIDIA GPU show the efficiency of our approach to parameter tuning for the ACO metaheuristic.},
  archive      = {J_JPDC},
  author       = {Andrey Borisenko and Sergei Gorlatch},
  doi          = {10.1016/j.jpdc.2021.03.012},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {74-81},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Efficient GPU-parallelization of batch plants design using metaheuristics with parameter tuning},
  volume       = {154},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). PBBFMM3D: A parallel black-box algorithm for kernel
matrix-vector multiplication. <em>JPDC</em>, <em>154</em>, 64–73. (<a
href="https://doi.org/10.1016/j.jpdc.2021.04.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Kernel matrix-vector product is ubiquitous in many science and engineering applications. However, a naive method requires O ( N 2 ) O(N2) operations, which becomes prohibitive for large-scale problems. To reduce the computation cost, we introduce a parallel method that provably requires O ( N ) O(N) operations and delivers an approximate result within a prescribed tolerance. The distinct feature of our method is that it requires only the ability to evaluate the kernel function, offering a black-box interface to users. Our parallel approach targets multi-core shared-memory machines and is implemented using OpenMP . Numerical results demonstrate up to 19× speedup on 32 cores. We also present a real-world application in geo-statistics, where our parallel method was used to deliver fast principle component analysis of covariance matrices.},
  archive      = {J_JPDC},
  author       = {Ruoxi Wang and Chao Chen and Jonghyun Lee and Eric Darve},
  doi          = {10.1016/j.jpdc.2021.04.005},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {64-73},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {PBBFMM3D: A parallel black-box algorithm for kernel matrix-vector multiplication},
  volume       = {154},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). PPMCK: Privacy-preserving multi-party computing for k-means
clustering. <em>JPDC</em>, <em>154</em>, 54–63. (<a
href="https://doi.org/10.1016/j.jpdc.2021.03.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The powerful resource advantage of the cloud provides a suitable computing environment for data processing. By transferring local computing to the cloud, the efficiency of data processing can be improved. However, the open cloud environment has defects in data privacy-preserving. In order to strengthen the protection of data privacy and ensure the security of multi-party interaction, we propose a privacy-preserving multi-party computing scheme for K-means clustering (PPMCK). PPMCK can preserve data privacy in the cloud and in the local side for each party from multi-party computing. In addition, PPMCK uses homomorphic encryption to protect data privacy. To support the division operation and ciphertext value size comparison with which homomorphic encryption cannot handle, the corresponding measurements are adopted, which make homomorphic encryption work smoothly. The experimental results demonstrate that PPMCK is effective in both data processing and privacy-preserving.},
  archive      = {J_JPDC},
  author       = {Yongkai Fan and Jianrong Bai and Xia Lei and Weiguo Lin and Qian Hu and Guodong Wu and Jiaming Guo and Gang Tan},
  doi          = {10.1016/j.jpdc.2021.03.009},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {54-63},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {PPMCK: Privacy-preserving multi-party computing for K-means clustering},
  volume       = {154},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Packing internally disjoint steiner trees to compute the
κ3-connectivity in augmented cubes. <em>JPDC</em>, <em>154</em>, 42–53.
(<a href="https://doi.org/10.1016/j.jpdc.2021.04.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given a connected graph G and S ⊆ V ( G ) S⊆V(G) with | S | ≥ 2 |S|≥2 , a tree T in G is called an S -Steiner tree (or S -tree for short) if S ⊆ V ( T ) S⊆V(T) . Two S -trees T 1 T1 and T 2 T2 are internally disjoint if E ( T 1 ) ∩ E ( T 2 ) = ∅ E(T1)∩E(T2)=∅ and V ( T 1 ) ∩ V ( T 2 ) = S V(T1)∩V(T2)=S . The packing number of internally disjoint S -trees, denoted as κ G ( S ) κG(S) , is the maximum size of a set of internally disjoint S -trees in G . For an integer k ≥ 2 k≥2 , the generalized k -connectivity (abbr. κ k κk -connectivity) of a graph G is defined as κ k ( G ) = min { κ G ( S ) | S ⊆ V ( G ) κk(G)=min{κG(S)|S⊆V(G) and | S | = k } |S|=k} . The n -dimensional augmented cube, denoted as A Q n AQn , is an important variant of the hypercube that possesses several desired topology properties such as diverse embedding schemes in applications of parallel computing . In this paper, we focus on the study of constructing internally disjoint S -trees with | S | = 3 |S|=3 in A Q n AQn . As a result, we completely determine the κ 3 κ3 -connectivity of A Q n AQn as follows: κ 3 ( A Q 4 ) = 5 κ3(AQ4)=5 and κ 3 ( A Q n ) = 2 n − 2 κ3(AQn)=2n−2 for n = 3 n=3 or n ≥ 5 n≥5 .},
  archive      = {J_JPDC},
  author       = {Chao Wei and Rong-Xia Hao and Jou-Ming Chang},
  doi          = {10.1016/j.jpdc.2021.04.004},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {42-53},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Packing internally disjoint steiner trees to compute the κ3-connectivity in augmented cubes},
  volume       = {154},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Understanding failures through the lifetime of a top-level
supercomputer. <em>JPDC</em>, <em>154</em>, 27–41. (<a
href="https://doi.org/10.1016/j.jpdc.2021.04.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High performance computing systems are required to solve grand challenges in many scientific disciplines. These systems assemble many components to be powerful enough for solving extremely complex problems. An inherent consequence is the intricacy of the interaction of all those components, especially when failures come into the picture. It is crucial to develop an understanding of how these systems fail to design reliable supercomputing platforms in the future. This paper presents the results on studying multi-year failure and workload records of a powerful supercomputer that topped the world rankings. We provide a thorough analysis of the data and characterize the reliability of the system through several dimensions: failure classification, failure-rate modelling, and interplay between failures and workload. The results shed some light on the dynamics of top-level supercomputers and sensitive areas ripe for improvement.},
  archive      = {J_JPDC},
  author       = {Elvis Rojas and Esteban Meneses and Terry Jones and Don Maxwell},
  doi          = {10.1016/j.jpdc.2021.04.001},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {27-41},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Understanding failures through the lifetime of a top-level supercomputer},
  volume       = {154},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A new genetic-based approach for solving k-coverage problem
in directional sensor networks. <em>JPDC</em>, <em>154</em>, 16–26. (<a
href="https://doi.org/10.1016/j.jpdc.2021.03.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the use of directional sensor networks (DSNs) has continued to rise increasingly, which is due to their extensive use in many situations. In such networks, the main problem is how to cover the targets distributed in a defined area and simultaneously prolong the network lifetime as much as possible. The reason of this problem is the limitation of both sensing angle and energy resource of sensors in such networks. This problem gets more complex in cases where targets need to be covered by more than one sensor direction (i.e., each target needs to be monitored for at least k k times). This problem is generally known as target k k -coverage problem which its NP-completeness has been already proved in the literature. The k k -coverage problem can be considered in over-provisioned and under-provisioned environments. In both of these environments, especially in the latter, it is important to create a balanced coverage, as these environments do not have enough sensors to monitor all targets for k k times. Thus, in this paper, we proposed a genetic-based algorithm to solve the problem in over-provisioned environments, then developed the proposed algorithm in another way to solve the problem in under-provisioned networks so that it uses the minimum number of sensors. many experiments were performed to test the efficiency of the proposed algorithm, and the obtained results showed the high capacity of the proposed algorithm in solving the k k -coverage problem in both environments.},
  archive      = {J_JPDC},
  author       = {Abolghasem Alibeiki and Homayun Motameni and Hosein Mohamadi},
  doi          = {10.1016/j.jpdc.2021.03.006},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {16-26},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {A new genetic-based approach for solving k-coverage problem in directional sensor networks},
  volume       = {154},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Inter-kernel communication facility of a distributed
operating system for NoC-based lightweight manycores. <em>JPDC</em>,
<em>154</em>, 1–15. (<a
href="https://doi.org/10.1016/j.jpdc.2021.04.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lightweight manycore processors deliver high performance and scalability by bundling in a single chip hundreds of low-power cores, a distributed memory architecture and Networks-on-Chip (NoCs). Operating Systems (OSes) for these processors feature a distributed design, in which a communication layer enables kernels to exchange information and interoperate. Currently, this communication infrastructure is based on mailboxes , which enable fixed-size message exchanges with low latency. However, this solution is suboptimal because it can neither fully exploit the NoC nor efficiently handle the diversity of OS communication protocols. We propose an Inter-Kernel Communication (IKC) facility that exposes two kernel-level communication abstractions in addition to mailboxes : syncs , for enabling a process to signal and unlock another process remotely, and portals , for handling dense data transfers with high bandwidth. We implemented the proposed facility in Nanvix, the only open-source distributed OS that runs on a baremetal lightweight manycore, and we evaluated our solution on a 288-core processor (Kalray MPPA-256). Our results showed that our IKC facility achieves up to 16.87× and 1.68× better performance than a mailbox -only solution, in synchronization and dense data transfers, respectively.},
  archive      = {J_JPDC},
  author       = {Pedro Henrique Penna and João Vicente Souto and João Fellipe Uller and Márcio Castro and Henrique Freitas and Jean-François Méhaut},
  doi          = {10.1016/j.jpdc.2021.04.002},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {1-15},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Inter-kernel communication facility of a distributed operating system for NoC-based lightweight manycores},
  volume       = {154},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Secure blockchain enabled cyber–physical systems in
healthcare using deep belief network with ResNet model. <em>JPDC</em>,
<em>153</em>, 150–160. (<a
href="https://doi.org/10.1016/j.jpdc.2021.03.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cyber–physical system (CPS) is the incorporation of physical processes with processing and data transmission. Cybersecurity is a primary and challenging issue in healthcare due to the legal and ethical perspective of the patient’s medical data. Therefore, the design of CPS model for healthcare applications requires special attention for ensuring data security. To resolve this issue, this paper proposes a secure intrusion, detection with blockchain based data transmission with classification model for CPS in healthcare sector . The presented model performs data acquisition process using sensor devices and intrusion detection takes place using deep belief network (DBN) model. In addition, the presented model uses a multiple share creation (MSC) model for the generation of multiple shares of the captured image, and thereby achieves privacy and security. Besides, the blockchain technology is applied for secure data transmission to the cloud server, which executes the residual network (ResNet) based classification model to identify the presence of the disease. The experimental validation of the presented model takes place using NSL-KDD 2015, CIDDS-001 and ISIC dataset. The simulation outcome pointed out the effective outcome of the presented model.},
  archive      = {J_JPDC},
  author       = {Gia Nhu Nguyen and Nin Ho Le Viet and Mohamed Elhoseny and K. Shankar and B.B. Gupta and Ahmed A. Abd El-Latif},
  doi          = {10.1016/j.jpdc.2021.03.011},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {150-160},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Secure blockchain enabled cyber–physical systems in healthcare using deep belief network with ResNet model},
  volume       = {153},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Edge computing server placement with capacitated location
allocation. <em>JPDC</em>, <em>153</em>, 130–149. (<a
href="https://doi.org/10.1016/j.jpdc.2021.03.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The deployment of edge computing infrastructure requires a careful placement of the edge servers, with an aim to improve application latencies and reduce data transfer load in opportunistic Internet of Things systems. In the edge server placement, it is important to consider computing capacity, available deployment budget, and hardware requirements for the edge servers and the underlying backbone network topology . In this paper, we thoroughly survey the existing literature in edge server placement, identify gaps and present an extensive set of parameters to be considered. We then develop a novel algorithm, called PACK, for server placement as a capacitated location–allocation problem. PACK minimizes the distances between servers and their associated access points, while taking into account capacity constraints for load balancing and enabling workload sharing between servers. Moreover, PACK considers practical issues such as prioritized locations and reliability. We evaluate the algorithm in two distinct scenarios: one with high capacity servers for edge computing in general, and one with low capacity servers for Fog computing . Evaluations are performed with a data set collected in a real-world network, consisting of both dense and sparse deployments of access points across a city area. The resulting algorithm and related tools are publicly available as open source software .},
  archive      = {J_JPDC},
  author       = {Tero Lähderanta and Teemu Leppänen and Leena Ruha and Lauri Lovén and Erkki Harjula and Mika Ylianttila and Jukka Riekki and Mikko J. Sillanpää},
  doi          = {10.1016/j.jpdc.2021.03.007},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {130-149},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Edge computing server placement with capacitated location allocation},
  volume       = {153},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Analysis of work stealing with latency. <em>JPDC</em>,
<em>153</em>, 119–129. (<a
href="https://doi.org/10.1016/j.jpdc.2021.03.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the impact of communication latency on the classical Work Stealing load balancing algorithm. Our paper extends the reference model in which we introduce a latency parameter. By using a theoretical analysis and simulation, we study the overall impact of this latency on the Makespan (maximum completion time). We derive a new expression of the expected running time of a bag of independent tasks scheduled by Work Stealing. This expression enables us to predict under which conditions a given run will yield acceptable performance. For instance, we can easily calibrate the maximal number of processors to use for a given work/platform combination. All our results are validated through simulation on a wide range of parameters.},
  archive      = {J_JPDC},
  author       = {Nicolas Gast and Mohammed Khatiri and Denis Trystram and Frédéric Wagner},
  doi          = {10.1016/j.jpdc.2021.03.010},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {119-129},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Analysis of work stealing with latency},
  volume       = {153},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On the weakest information on failures to solve mutual
exclusion and consensus in asynchronous crash-prone read/write systems.
<em>JPDC</em>, <em>153</em>, 110–118. (<a
href="https://doi.org/10.1016/j.jpdc.2021.03.015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mutual exclusion and consensus are among the most important coordination problems encountered in asynchronous concurrent systems, whether processes communicate using read/write registers or message passing. Unfortunately, neither can be solved in crash-prone systems, as soon as even a single process may crash. Hence, an important question: which is the weakest information on failures that must be given to the processes so that these problems can be solved whatever the number of crashes. This approach to circumvent impossibility results is known under the name failure detectors . Considering mutual exclusion and consensus in a crash-prone asynchronous system where the processes communicate through read/write registers, this article answers the previous question by presenting two failure detectors. The first, called Quasi-Perfect ( QP ) allows mutual exclusion to be solved in the presence of any number of process crashes. The second, called ⁎ Ω ⁎ Ω⁎ , allows consensus to be solved in the general model where not all but an a priori unknown subset of processes participates in consensus. In addition to algorithms solving each of the previous problems with the help of the associated failure detector, the article shows that QP and ⁎ Ω ⁎ Ω⁎ provides the weakest information on failures needed to solve mutex exclusion and participant-restricted consensus respectively.},
  archive      = {J_JPDC},
  author       = {Carole Delporte-Gallet and Hugues Fauconnier and Michel Raynal},
  doi          = {10.1016/j.jpdc.2021.03.015},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {110-118},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {On the weakest information on failures to solve mutual exclusion and consensus in asynchronous crash-prone read/write systems},
  volume       = {153},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Connected k-coverage in two-dimensional wireless sensor
networks using hexagonal slicing and area stretching. <em>JPDC</em>,
<em>153</em>, 89–109. (<a
href="https://doi.org/10.1016/j.jpdc.2020.12.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of coverage in two-dimensional (2D) wireless sensor networks is challenging and is still open. Precisely, determining the minimum sensor density ( i.e , minimum number of sensors per unit area) that is required to cover a 2D field of interest (FoI), where every point in the field is covered by at least one sensor, is still under investigation. The problem of 2D k -coverage, which requires that every point in a 2D FoI be covered by at least k sensors, where k ≥ 1 k≥1 , is more challenging. In this paper, we attempt to address the 2D connected k -coverage problem, where a 2D FoI is k -covered, while the underlying set of sensors k -covering the field forms a connected network. We propose to solve this problem using an approach based on slicing 2D FoI into convex regular hexagons. Our goal is to achieve k -coverage of a 2D FoI with a minimum number of sensors in order to maximize the network lifetime. First, we compute the minimum sensor density for 2D k -coverage using the regular convex hexagon, which is a 2D paver ( i.e. , covers a 2D field without gaps or overlaps). Indeed, we found that the regular convex hexagon best assimilates the sensors’ sensing disk with respect to our proposed metric, sensing range usage rate . Second, we derive the ratio of the communication range to the sensing range of the sensors to ensure connected k -coverage. Third, we propose an energy-efficient connected k -coverage protocol based on hexagonal slicing and area stretching. To this end, we formulate a multi-objective optimization problem , which computes an optimum solution to the 2D k -coverage problem that meets two requirements: Maximizing the size of the k -covered area, C k Ck , so as to minimize the sensor density to k -cover a 2D FoI ( Requirement 1) and maximizing the area of the sensor locality , L k Lk , i.e. , the region where at least k sensors are located to k -cover C k Ck , so as to minimize the interference between sensors ( Requirement 2). Fourth, we show various simulation results to substantiate our theoretical analysis. We found a close-to-perfect match between our theoretical and simulation results.},
  archive      = {J_JPDC},
  author       = {Habib M. Ammari},
  doi          = {10.1016/j.jpdc.2020.12.008},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {89-109},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Connected k-coverage in two-dimensional wireless sensor networks using hexagonal slicing and area stretching},
  volume       = {153},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An adaptive multi-agent system for task reallocation in a
MapReduce job. <em>JPDC</em>, <em>153</em>, 75–88. (<a
href="https://doi.org/10.1016/j.jpdc.2021.03.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the problem of task reallocation for load-balancing of MapReduce jobs in applications that process large datasets. In this context, we propose a novel strategy based on cooperative agents used to optimize the task scheduling in a single MapReduce job. The novelty of our strategy lies in the ability of agents to identify opportunities within a current unbalanced allocation, which in turn triggers concurrent and one-to-many negotiations amongst agents to locally reallocate some of the tasks within a job. Our contribution is that tasks are reallocated according to the proximity of the resources and they are performed in accordance to the capabilities of the nodes in which agents are situated. To evaluate the adaptivity and responsiveness of our approach, we implement a prototype test-bed and conduct a vast panel of experiments in a heterogeneous environment and by exploring varying hardware configurations . This extensive experimentation reveals that our strategy significantly improves the overall runtime over the classical Hadoop data processing.},
  archive      = {J_JPDC},
  author       = {Quentin Baert and Anne-Cécile Caron and Maxime Morge and Jean-Christophe Routier and Kostas Stathis},
  doi          = {10.1016/j.jpdc.2021.03.008},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {75-88},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {An adaptive multi-agent system for task reallocation in a MapReduce job},
  volume       = {153},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Visual analogy videos for understanding fundamental parallel
scheduling policies. <em>JPDC</em>, <em>153</em>, 64–74. (<a
href="https://doi.org/10.1016/j.jpdc.2021.03.014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Parallel and distributed computing (PDC) education is increasingly gaining greater recognition as a core topic in undergraduate computing degrees. While the application of PDC concepts to software development involves the use of highly-technical tools and libraries typically reserved for advanced courses, PDC educators are seeking pedagogical approaches that can be used to introduce PDC concepts in earlier, introductory courses. This study presents such an approach, and aims to introduce undergraduate students to fundamental PDC concepts without the expectation that they can apply those concepts. The proposed approach is inspired by the success seen in the wider computing education literature, where analogies and visualization have helped students understand other abstract computing topics. The proposed learning resources come in the form of a series of short videos, carefully aligned to a learning activity that guides towards achieving the intended learning outcomes . In addition to being a simple activity to complete with students, evaluations illustrate its value even with minimal guidance from the instructor. The proposed approach is studied as both a synchronous in-class activity guided by the instructor, as well as an asynchronous online self-directed activity. These two studies produced different outcomes with respect to student learning, revealing an important implication for designers of instructional material to consider.},
  archive      = {J_JPDC},
  author       = {Nasser Giacaman and Oliver Sinnen and Joel Adams},
  doi          = {10.1016/j.jpdc.2021.03.014},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {64-74},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Visual analogy videos for understanding fundamental parallel scheduling policies},
  volume       = {153},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Explore unlabeled big data learning to online failure
prediction in safety-aware cloud environment. <em>JPDC</em>,
<em>153</em>, 53–63. (<a
href="https://doi.org/10.1016/j.jpdc.2021.02.025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Proactive fault management is an important problem in many areas of data management, including cloud computing , big data, vision, machine learning and especially for the cross-domain research of distributed computing and AI (Artificial Intelligence). Unfortunately, most real-world online failure prediction is facing the problem that the used data are difficult to label although the failure prediction should be a supervised learning problem. We observe that, in many cases, the large-scale unlabeled data can be classified through feature extraction and clustering for available prediction, and thus ideas from their combination can be brought to bear. Based on this, we have proposed an online failure prediction framework approach UDFP (Unlabeled Data based online Failure Prediction). It introduces the clustering analysis method based on the combination of the KNN (k-nearest neighbor) and the modularity idea to achieve prediction modeling. It is shown analytically that UDFP can mitigate a supervised learning problem for failure prediction in our situation to some extent. Experimental results demonstrate that UDFP, as a framework approach, has avoided the manual tagging workload and the huge difficulties, improved the predictive accuracy , and reduced cost of data management in safety-aware distributed cloud data centers while enhancing fault-tolerant capabilities and robustness.},
  archive      = {J_JPDC},
  author       = {Jia Zhao and Yan Ding and Yunan Zhai and Yuqiang Jiang and Yujuan Zhai and Ming Hu},
  doi          = {10.1016/j.jpdc.2021.02.025},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {53-63},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Explore unlabeled big data learning to online failure prediction in safety-aware cloud environment},
  volume       = {153},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An adaptive synthesis to handle imbalanced big data with
deep siamese network for electricity theft detection in smart grids.
<em>JPDC</em>, <em>153</em>, 44–52. (<a
href="https://doi.org/10.1016/j.jpdc.2021.03.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The bi-directional flow of energy and information in the smart grid makes it possible to record and analyze the electricity consumption profiles of consumers. Because of the increasing rate of inflation over the past few years, people started looking for means to use electricity illegally, termed as electricity theft . Many data analytics techniques are proposed in the literature for electricity theft detection (ETD). These techniques help in the detection of suspected illegal consumers. However, the existing approaches have a low ETD rate either due to improper handling of the imbalanced class problem in a dataset or the selection of inappropriate classifier. In this paper, a robust big data analytics technique is proposed to resolve the aforementioned concerns. Firstly, adaptive synthesis (ADASYN) is applied to handle the imbalanced class problem of data. Secondly convolutional neural network (CNN) and long-short term memory (LSTM) integrated deep siamese network (DSN) are proposed to discriminate the features of both honest and fraudulent consumers. Specifically, the task of feature extraction from weekly energy consumption profiles is handed over to the CNN module while the LSTM module performs the sequence learning. Finally, the DSN contemplates on the shared features provided by the CNN-LSTM and applies final judgment. The data analytics is performed on different train–test ratios of the real-time smart meters’ data. The simulation results validate the proposed model’s effectiveness in terms of high area under the curve, F 1 F1 -Score, precision and recall.},
  archive      = {J_JPDC},
  author       = {Nadeem Javaid and Naeem Jan and Muhammad Umar Javed},
  doi          = {10.1016/j.jpdc.2021.03.002},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {44-52},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {An adaptive synthesis to handle imbalanced big data with deep siamese network for electricity theft detection in smart grids},
  volume       = {153},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Study of interconnect errors, network congestion, and
applications characteristics for throttle prediction on a large scale
HPC system. <em>JPDC</em>, <em>153</em>, 29–43. (<a
href="https://doi.org/10.1016/j.jpdc.2021.03.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Today’s High Performance Computing (HPC) systems contain thousand of nodes which work together to provide performance in the order of petaflops. The performance of these systems depends on various components like processors, memory, and interconnect. Among all, interconnect plays a major role as it glues together all the hardware components in an HPC system. A slow interconnect can impact a scientific application running on multiple processes severely as they rely on fast network messages to communicate and synchronize frequently. Unfortunately, the HPC community lacks a study that explores different interconnect errors, congestion events and applications characteristics on a large-scale HPC system. In our previous work, we process and analyze interconnect data of the Titan supercomputer to develop a thorough understanding of interconnects faults, errors, and congestion events. In this work, we first show how congestion events can impact application performance. We then investigate application characteristics interaction with interconnect errors and network congestion to predict applications encountering congestion with more than 90\% accuracy.},
  archive      = {J_JPDC},
  author       = {Mohit Kumar and Saurabh Gupta and Tirthak Patel and Michael Wilder and Weisong Shi and Song Fu and Christian Engelmann and Devesh Tiwari},
  doi          = {10.1016/j.jpdc.2021.03.001},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {29-43},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Study of interconnect errors, network congestion, and applications characteristics for throttle prediction on a large scale HPC system},
  volume       = {153},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Security-aware task scheduling with deadline constraints on
heterogeneous hybrid clouds. <em>JPDC</em>, <em>153</em>, 15–28. (<a
href="https://doi.org/10.1016/j.jpdc.2021.03.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hybrid cloud is a cost-effective way to address the problem of insufficient resources for satisfying its users’ requirements in a private cloud by elastically scaling up or down its service capability by combining the private cloud and public clouds. However, it is a challenge to schedule tasks on hybrid resources concerning their performance and security requirements. To address the challenge, this paper aims at improving the number of finished tasks with deadline and security requirements and the resource usage cost in heterogeneous hybrid clouds, based on data protection technologies providing various security levels with different overheads for data transfers and task executions in public clouds. We first formulate the problem as a bi-objective binary nonlinear programming (BOBNP) model which is a NP-hard problem. Then, to solve the problem in polynomial time , we propose a Task Scheduling method concerning Security (TSS). To improve the cost, TSS iteratively assigns the task requiring maximum cost of public resources to the local cluster, and rents the public resource with the best cost-performance ratio first for outsourced tasks. To complete as many tasks as possible, TSS assigns tasks cannot be finished by public clouds to the local cloud at first, and employs the idea of Least Slack Time First (LSTF) with Earliest Deadline First (EDF) in each computing node. Extensive experimental results show the superior performance of TSS in satisfying task requirements, and in resource efficiency when task deadlines are not too tight, compared with four hybrid cloud scheduling methods proposed recently.},
  archive      = {J_JPDC},
  author       = {Bo Wang and Changhai Wang and Wanwei Huang and Ying Song and Xiaoyun Qin},
  doi          = {10.1016/j.jpdc.2021.03.003},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {15-28},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Security-aware task scheduling with deadline constraints on heterogeneous hybrid clouds},
  volume       = {153},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Improving multitask performance and energy consumption with
partial-ISA multicores. <em>JPDC</em>, <em>153</em>, 1–14. (<a
href="https://doi.org/10.1016/j.jpdc.2021.02.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern GPPs implement specialized instructions in the form of ISA extensions aiming to increase the performance of emerging applications. These extensions impose a significant overhead in the area and power of the processor because of their specific datapaths (e.g. hardware for SIMD and FP instructions may represent more than half of the core area). Considering that some devices (e.g., edge computing), must be as energy- and area-efficient as possible, and the sporadic usage of specialized instructions in many applications, we propose PHISA multicores. PHISA is composed of heterogeneous cores of the same single base ISA, but asymmetric functionality: some of the cores do not fully implement the costly instruction extensions, making room for the designers to add more efficient cores. We show that PHISA increases performance in (32\%) and reduces energy consumption in (82\%) compared to full-ISA systems with the same power budget, in multi-workload environments.},
  archive      = {J_JPDC},
  author       = {Jeckson Dellagostin Souza and Pedro Henrique Exenberger Becker and Antonio Carlos Schneider Beck},
  doi          = {10.1016/j.jpdc.2021.02.007},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {1-14},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Improving multitask performance and energy consumption with partial-ISA multicores},
  volume       = {153},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A trustworthy industrial data management scheme based on
redactable blockchain. <em>JPDC</em>, <em>152</em>, 167–176. (<a
href="https://doi.org/10.1016/j.jpdc.2021.02.026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Industrial data plays a key role in the industrial internet, and its secure collection problem has been highly valued by researchers. As Industrial Internet of Things (IIoT) devices are geographically dispersed and difficult to link, blockchain technology is usually introduced to solve the security management problem of industrial data. Unfortunately, the IIoT device is not stable, and it may leave incorrect messages in the blockchain, which will be permanently stored with potentially catastrophic consequences . As an effective solution, the redactable blockchain technology can allow people to modify the data on the blockchain. However, the existing redactable blockchain cannot guarantee industrial data security in the industrial internet due to requirements of trusted third parties, large overheads or lack of accountability mechanism. In this paper, we propose a trustworthy industrial data management scheme based on redactable blockchain in the industrial internet. To avoid additional burdens on industrial blockchain systems, a double-blockchain architecture is established to separate trapdoor management transactions. Distributed chameleon hash parameter generation and trapdoor recovery methods can avoid the security problems faced by the centralized organization. The fault-tolerant trapdoor recovery mechanism based on verifiable secret sharing technology as an alternative enhances the security of the system. The blockchain will record various information in the trapdoor management process and use it as evidence for accountability when disputes arise. The theoretical analysis and experiments show that the approach can effectively deal with malicious behaviors and has acceptable overhead.},
  archive      = {J_JPDC},
  author       = {Cheng Zhang and Zhifei Ni and Yang Xu and Entao Luo and Linweiya Chen and Yaoxue Zhang},
  doi          = {10.1016/j.jpdc.2021.02.026},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {167-176},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {A trustworthy industrial data management scheme based on redactable blockchain},
  volume       = {152},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Smart contract service migration mechanism based on
container in edge computing. <em>JPDC</em>, <em>152</em>, 157–166. (<a
href="https://doi.org/10.1016/j.jpdc.2021.02.023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In edge computing , smart contracts usually run in the form of containers on edge nodes. However, the container has process nesting and strong association with image files, which leads to insufficient real-time migration. This article studies the container-based service migration problem in edge computing . In order to reduce the service delay of nodes, we propose a service migration mechanism based on mobility awareness. The service migration mechanism based on mobility awareness triggers service migration according to the service density of the current node, and selects the most suitable service set for migration in the current node with the optimization goal of minimizing service delay, and finally the corresponding destination node is selected for migration according to the migration cost and the moving direction of the device to which the service belongs. Experimental results show that our proposed service migration mechanism based on container can effectively reduce the waiting delay and migration delay in the service process, and optimize the service quality of edge nodes.},
  archive      = {J_JPDC},
  author       = {Luxiu Yin and Pengfei Li and Juan Luo},
  doi          = {10.1016/j.jpdc.2021.02.023},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {157-166},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Smart contract service migration mechanism based on container in edge computing},
  volume       = {152},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A scalable blockchain based trust management in VANET
routing protocol. <em>JPDC</em>, <em>152</em>, 144–156. (<a
href="https://doi.org/10.1016/j.jpdc.2021.02.024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Critical event information dissemination has been proliferating on VANET allowing road safety via connected vehicular communications . Despite the prospect of promising applications in vehicular networks , it faces unresolved challenges that hold the capability to slow down network performance upon deployment, especially in terms of security. Particularly, insider attacks such as Blackhole attacks that are carried out against VANET systems can disrupt the networks’ average performance and prevent communication between vehicles entirely. Many state-of-the-art solutions have been proposed to detect and eliminate such nodes based on reputation systems and broadcast routing. However, if the network consists of multiple malicious nodes, the message dissemination could fail due to broadcast message tampering attack or packet dropping . In this study, we explore to answer the question of “can we improve the insider attacks mitigation in VANET by enhancing the trust in the network system so that the possibility of successful attacks can be reduced?” . To answer this question, in this paper, we present the blockchain-based decentralized trust score framework for the participating nodes to detect and blacklist insider attackers in VANET proactively. We propose a two-level detection system, in which at the first level, neighboring nodes calculate the trust individually. In the second level, a consortium blockchain-based system with authorized Road Side Units (RSUs) as validators, aggregate trust scores for vehicular nodes. Then, based on trust scores reported by the neighboring nodes, the blacklist node tables are dynamically modified. The experimental analysis shows that the proposed system is efficient and scalable in terms of the network’s practical size. Finally, we also present evidence that the proposed system improves the VANET performance by mitigating and blacklisting insider attack launching nodes.},
  archive      = {J_JPDC},
  author       = {Sowmya Kudva and Shahriar Badsha and Shamik Sengupta and Hung La and Ibrahim Khalil and Mohammed Atiquzzaman},
  doi          = {10.1016/j.jpdc.2021.02.024},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {144-156},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {A scalable blockchain based trust management in VANET routing protocol},
  volume       = {152},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A secured distributed detection system based on IPFS and
blockchain for industrial image and video data security. <em>JPDC</em>,
<em>152</em>, 128–143. (<a
href="https://doi.org/10.1016/j.jpdc.2021.02.022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Copyright infringement adversely affects the interest of copyright holders of images and videos which are uploaded to different websites and peer-to-peer image sharing systems. This paper addresses the problem of detecting copyright infringement so that copyright holders are given due credit for their work. There are several images and videos that are shared every day by millions of users with some amount of modification in images and videos originally uploaded by the copyright holders such as photographers, graphic designers , and video providers. Copyright violators, who are not the original creators of multimedia content modify them using image processing and frame modification techniques such as grayscale conversion, cropping, rotation, frame compression, and frame speed manipulation. Then, upload the tampered images and videos. To address this problem, we propose an IPFS-based (InterPlanetary File System-based) decentralized peer-to-peer image and video sharing platform built on blockchain technology . We use a perceptual hash ( pHash ) technique to detect copyright violations of multimedia. When multimedia is to be uploaded to the IPFS, the pHash of the same content is determined and checked against existing pHash values in the blockchain network. Similarity with existing pHash values would result in the multimedia being detected as tampered with. Blockchain technology offers the advantage of non-involvement of a third party and consequently the avoidance of a single point of failure .},
  archive      = {J_JPDC},
  author       = {Randhir Kumar and Rakesh Tripathi and Ningrinla Marchang and Gautam Srivastava and Thippa Reddy Gadekallu and Neal N. Xiong},
  doi          = {10.1016/j.jpdc.2021.02.022},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {128-143},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {A secured distributed detection system based on IPFS and blockchain for industrial image and video data security},
  volume       = {152},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Blockchain-enabled secure communications in smart cities.
<em>JPDC</em>, <em>152</em>, 125–127. (<a
href="https://doi.org/10.1016/j.jpdc.2021.02.021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blockchain is a relative recent research and technological trend, with applications in diverse domains including those associated with a nation’s critical infrastructure sectors (e.g., chemical, commercial facilities, communications, critical manufacturing, dams, defense industrial base, emergency services, and energy). The interest in blockchain is also partly evidenced by the number of submissions we received in this special issue. Of the 54 submissions received, 18 papers were accepted after several rounds of reviews by subject matter experts (i.e., acceptance rate of ∼ ∼ 33.3\%). This special issue presents the research advances and describes existing and emerging challenges outlined in these 18 accepted papers, authored by researchers from institutions in Australia, Canada, China, Denmark, Ireland, New Zealand, Spain, United Kingdom, and United States. These accepted papers also reinforce the importance of collaboration across institutions and countries.},
  archive      = {J_JPDC},
  author       = {Kim-Kwang Raymond Choo and Keke Gai and Luca Chiaraviglio},
  doi          = {10.1016/j.jpdc.2021.02.021},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {125-127},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Blockchain-enabled secure communications in smart cities},
  volume       = {152},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). PARIS: Predicting application resilience using machine
learning. <em>JPDC</em>, <em>152</em>, 111–124. (<a
href="https://doi.org/10.1016/j.jpdc.2021.02.015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The traditional method to study application resilience to errors in HPC applications uses fault injection (FI), a time-consuming approach. While analytical models have been built to overcome the inefficiencies of FI, they lack accuracy. In this paper, we present PARIS, a machine-learning method to predict application resilience that avoids the time-consuming process of random FI and provides higher prediction accuracy than analytical models. PARIS captures the implicit relationship between application characteristics and application resilience, which is difficult to capture using most analytical models. We overcome many technical challenges for feature construction, extraction, and selection to use machine learning in our prediction approach. Our evaluation on 16 HPC benchmarks shows that PARIS achieves high prediction accuracy. PARIS is up to 450x faster than random FI (49x on average). Compared to the state-of-the-art analytical model, PARIS is at least 63\% better in terms of accuracy and has comparable execution time on average.},
  archive      = {J_JPDC},
  author       = {Luanzheng Guo and Dong Li and Ignacio Laguna},
  doi          = {10.1016/j.jpdc.2021.02.015},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {111-124},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {PARIS: Predicting application resilience using machine learning},
  volume       = {152},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Enabling technologies for energy cloud. <em>JPDC</em>,
<em>152</em>, 108–110. (<a
href="https://doi.org/10.1016/j.jpdc.2021.02.020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We are thrilled and delighted to present this special issue, which emphasizes on the novel area of Enabling Technologies for Energy Cloud. This guest editorial provides an overview of all articles accepted for publication in this special issue.},
  archive      = {J_JPDC},
  author       = {Thar Baker and Zehua Guo and Ali Ismail Awad and Shangguang Wang and Benjamin C.M. Fung},
  doi          = {10.1016/j.jpdc.2021.02.020},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {108-110},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Enabling technologies for energy cloud},
  volume       = {152},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A novel cooperative resource provisioning strategy for
multi-cloud load balancing. <em>JPDC</em>, <em>152</em>, 98–107. (<a
href="https://doi.org/10.1016/j.jpdc.2021.02.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paradigm of cloud computing has heralded a new avenue of computing, offering benefits of increased data accessibility with low cost. Continuous Writing Applications (CWA) (e.g., augmented online services for Health Care) have specific requirements on data storage, computation and bandwidth, thus are cost-sensitive with limited budgets and time. Herein, we propose an architecture of multi-cloud service provider (CSP) or “Multi-Cloud” to provide services to CWA, and design a novel resource scheduling algorithm to minimize the system cost. The system models of classic CWAs to tackle the resource requirements of users on MCP are exploited. The study can help to understand the characteristics of different resources and conclude Multi-Cloud being the most attractive to many CWA implementations. Interconnections of multiple CSPs and their load paths (i.e., data passing through possible interconnections) are introduced. We then formulate the problem and present optimal user scheduling based on Minimum First Derivative Length (MFDL) of system load paths. Theoretical analysis demonstrated that the solutions with minimized costs can be achieved by the proposed algorithm, termed “Optimal user Scheduling” for Multi-Cloud (OSMC). Through rigorous simulations regarding different influencing factors, the proposed strategy has proven to be scalable, flexible, and efficient in many practical scenarios.},
  archive      = {J_JPDC},
  author       = {Bo Zhang and Zeng Zeng and Xiupeng Shi and Jianxi Yang and Bharadwaj Veeravalli and Keqin Li},
  doi          = {10.1016/j.jpdc.2021.02.003},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {98-107},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {A novel cooperative resource provisioning strategy for multi-cloud load balancing},
  volume       = {152},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptive resource planning for cloud-based services using
machine learning. <em>JPDC</em>, <em>152</em>, 88–97. (<a
href="https://doi.org/10.1016/j.jpdc.2021.02.018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of using cloud computing resources for services is related to planning the amount of resources needed and their subsequent reservation. This problem occurs both on the side of the customer who tries to minimize the cost of the service and on the side of the cloud provider who wants to make the best use of existing infrastructure without introducing any modifications. In our article, we want to show how the problem of overestimating the utilization of resources for services which use cloud computing can be handled. Solving this problem will allow significant savings to be made by both the customer and the cloud infrastructure provider. The system we have developed demonstrates the considerable utility of machine learning methods when planning cloud resource reservation for network services . The models proposed, which use a multilayer perceptron , have yielded good results for both short- and long-term reservations.},
  archive      = {J_JPDC},
  author       = {Piotr Nawrocki and Mikolaj Grzywacz and Bartlomiej Sniezynski},
  doi          = {10.1016/j.jpdc.2021.02.018},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {88-97},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Adaptive resource planning for cloud-based services using machine learning},
  volume       = {152},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Discriminating flash crowds from DDoS attacks using
efficient thresholding algorithm. <em>JPDC</em>, <em>152</em>, 79–87.
(<a href="https://doi.org/10.1016/j.jpdc.2021.02.019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributed Denial-of-Service attacks have been a challenge to cyberspace, as the attackers send a large number of attack packets similar to the normal traffic, to throttle legitimate flows. These attacks intentionally disrupt the services offered by the systems resulting in heavy cost. A flash crowd or flash event is an unexpected surge in the number of visitors to a particular website resulting in a sudden increase in server load. Flash crowds, which are legitimate flows, are difficult to be discriminated from Distributed Denial-of-Service attacks that are illicit flows. Effective and accurate detection of Distributed Denial of Service attacks still remains a challenge due to the difficulty in its detection and the false alerts generated in the case of flash crowds. There is a trade off between detection rate and false positive rate. This work deals with an efficient and early detection of distributed denial of service attacks and discriminates flash crowd by considering two network traffic parameters such as packet size and destination IP address. Using these traffic features two attributes are computed and its generalized entropies are calculated. The threshold is computed using the mean value of network attributes to detect the attacks. Threshold updater can automatically adjust the threshold values according to the changes in the channel conditions. The data sets used to evaluate the performance of the proposed approach are the MIT Lincoln Laboratory DARPA data set and a data set generated in a University network. Experimental results show this research approach achieves higher detection rate and lower false positives in a much reduced processing time as compared to the existing methods.},
  archive      = {J_JPDC},
  author       = {Jisa David and Ciza Thomas},
  doi          = {10.1016/j.jpdc.2021.02.019},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {79-87},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Discriminating flash crowds from DDoS attacks using efficient thresholding algorithm},
  volume       = {152},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Identifying compromised hosts under APT using DNS request
sequences. <em>JPDC</em>, <em>152</em>, 67–78. (<a
href="https://doi.org/10.1016/j.jpdc.2021.02.017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advanced persistent threats (APTs) have become a major cyber threat to large organizations. To steal confidential data from specific organizations, attackers adopt highly targeted intrusion schemes. Prior to stealing critical data, APT activities hide themselves in legitimate activities and consistently elevate their privileges, making them very difficult to detect. The detection of malicious domains during domain name service (DNS) analysis accounts for the majority of existing detection methods. However, a limited number of available samples and rapidly changing sets of malicious domain names reduce the efficacy of such approaches. By investigating numerous APT reports, we determined that the activities of DNS requests in APT attacks exhibit clear temporal patterns that are ignored by most existing schemes. Therefore, we can analyze the DNS sequences requested by each host and their time-related features to identify compromised hosts. This paper summarizes the patterns of host DNS requests and proposes several assumptions. We take advantage of machine learning to identify compromised hosts by quantifying these assumptions in the form of feature vectors. We deployed the proposed approach into large-scale network environments and experimental evaluations demonstrated that our method is able to detect hosts compromised by APTs efficiently with a precision of 97.3\% and detection rate of 96.2\%.},
  archive      = {J_JPDC},
  author       = {Ming Li and Qiang Li and Guangzhe Xuan and Dong Guo},
  doi          = {10.1016/j.jpdc.2021.02.017},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {67-78},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Identifying compromised hosts under APT using DNS request sequences},
  volume       = {152},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Image super-resolution via enhanced multi-scale residual
network. <em>JPDC</em>, <em>152</em>, 57–66. (<a
href="https://doi.org/10.1016/j.jpdc.2021.02.016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, a very deep convolutional neural network (CNN) has achieved impressive results in image super-resolution (SR). In particular, residual learning techniques are widely used. However, the previously proposed residual block can only extract one single-level semantic feature maps of one single receptive field. Therefore, it is necessary to stack the residual blocks to extract higher-level semantic feature maps, which will significantly deepen the network. While a very deep network is hard to train and limits the representation for reconstructing the hierarchical information. Based on the residual block, we propose an enhanced multi-scale residual network (EMRN) to take advantage of hierarchical image features via dense connected enhanced multi-scale residual blocks (EMRBs). Specifically, the newly proposed residual block (EMRB) is capable of constructing multi-level semantic feature maps by a two-branch inception. The two-branch inception in our proposed EMRB consists of 2 convolutional layers and 4 convolutional layers in each branch respectively, therefore we have different ranges of receptive fields within one single EMRB. Meanwhile, the local feature fusion (LFF) is used in every EMRB to adaptively fuse the local feature maps extracted by the two-branch inception. Furthermore, global feature fusion (GFF) in EMRN is then used to obtain abundant useful features from previous EMRBs and subsequent ones in a holistic manner. Experiments on benchmark datasets suggest that our EMRN performs favorably over the state-of-the-art methods in reconstructing further superior super-resolution (SR) images.},
  archive      = {J_JPDC},
  author       = {MengJie Wang and Xiaomin Yang and Marco Anisetti and Rongzhu Zhang and Marcelo Keese Albertini and Kai Liu},
  doi          = {10.1016/j.jpdc.2021.02.016},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {57-66},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Image super-resolution via enhanced multi-scale residual network},
  volume       = {152},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Django: Bilateral coflow scheduling with predictive
concurrent connections. <em>JPDC</em>, <em>152</em>, 45–56. (<a
href="https://doi.org/10.1016/j.jpdc.2021.01.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For data-parallel frameworks, their communication is highly structured. Coflow is a networking abstraction proposed for their all-or-nothing job-specific semantics. Minimizing coflow completion time (CCT) decreases the completion time of corresponding jobs. However, state-of-the-art coflow scheduling approaches suffer from several drawbacks. On the one hand, both sender-driven and receiver-driven scheduling approaches fail to achieve optimal especially when the bandwidth bottleneck exists. On the other hand, they fail to optimize the number of concurrent connections since the CCT can be prolonged due to too many or too few concurrent connections. In this paper, we propose Django, a bilateral coflow scheduling framework. We first use Support Vector Machine (SVM) as the machine learning model to automatically identify the optimal number of concurrent connections, i.e. , the queue limitation in the switch. Based on the predicted results, we further develop a set of distributed coflow scheduling algorithms in a scalable manner. Testbed experiments and trace-driven simulations show that  Django can estimate the number of concurrent connections with an accuracy of 98\%, reduce the average CCT and 95th percentile CCT by 15\% and 40\%, respectively.},
  archive      = {J_JPDC},
  author       = {Jiaqi Zheng and Liulan Qin and Kexin Liu and Bingchuan Tian and Chen Tian and Bo Li and Guihai Chen},
  doi          = {10.1016/j.jpdc.2021.01.006},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {45-56},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Django: Bilateral coflow scheduling with predictive concurrent connections},
  volume       = {152},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Avoiding data loss and corruption for file transfers with
fast integrity verification. <em>JPDC</em>, <em>152</em>, 33–44. (<a
href="https://doi.org/10.1016/j.jpdc.2021.02.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {End-to-end integrity verification is used to avoid silent data corruption in file transfers by comparing the checksum of files at source and destination end points. However, it increases transfer times significantly as checksum computation requires reading files back from the storage and running compute-intensive hash computation. In this paper, we propose Fast Integrity VERification (FIVER) algorithm which alleviates the overhead of end-to-end integrity verification by overlapping checksum computation with transfer operation and enabling I/O sharing between the two. The results obtained from various network and dataset settings show that FIVER is able to bring down the cost of end-to-end integrity verification from up to 120\% by the state-of-the-art solutions to below 15\%. Moreover, existing implementations of end-to-end integrity verification are vulnerable to permanent data loss in the case of an unexpected power outage due to completing the integrity verification process while data is still on memory. FIVER addresses this issue by enforcing dirty data on memory to be flushed to disk before finishing integrity verification such that power loss during any phase of a transfer would cause integrity verification to fail and transfer application to retransfer lost data.},
  archive      = {J_JPDC},
  author       = {Ahmed Alhussen and Engin Arslan},
  doi          = {10.1016/j.jpdc.2021.02.002},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {33-44},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Avoiding data loss and corruption for file transfers with fast integrity verification},
  volume       = {152},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Performance analysis and optimization opportunities for
NVIDIA automotive GPUs. <em>JPDC</em>, <em>152</em>, 21–32. (<a
href="https://doi.org/10.1016/j.jpdc.2021.02.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advanced Driver Assistance Systems (ADAS) and Autonomous Driving (AD) bring unprecedented performance requirements for automotive systems . Graphic Processing Unit (GPU) based platforms have been deployed with the aim of meeting these requirements, being NVIDIA Jetson TX2 and its high-performance successor, NVIDIA AGX Xavier, relevant representatives. However, to what extent high-performance GPU configurations are appropriate for ADAS and AD workloads remains as an open question. This paper analyzes this concern and provides valuable insights on this question by modeling two recent automotive NVIDIA GPU-based platforms, namely TX2 and AGX Xavier. In particular, our work assesses their microarchitectural parameters against relevant benchmarks, identifying GPU setups delivering increased performance within a similar cost envelope, or decreasing hardware costs while preserving original performance levels. Overall, our analysis identifies opportunities for the optimization of automotive GPUs to further increase system efficiency.},
  archive      = {J_JPDC},
  author       = {Hamid Tabani and Fabio Mazzocchetti and Pedro Benedicte and Jaume Abella and Francisco J. Cazorla},
  doi          = {10.1016/j.jpdc.2021.02.008},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {21-32},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Performance analysis and optimization opportunities for NVIDIA automotive GPUs},
  volume       = {152},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Collaborative execution of fluid flow simulation using
non-uniform decomposition on heterogeneous architectures. <em>JPDC</em>,
<em>152</em>, 11–20. (<a
href="https://doi.org/10.1016/j.jpdc.2021.02.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The demand for computing power, along with the diversity of computational problems, culminated in a variety of heterogeneous architectures . Among them, hybrid architectures combine different specialized hardware into a single chip , comprising a System-on-Chip (SoC). Since these architectures usually have limited resources, efficiently splitting data and tasks between the different hardware is primal to improve performance. In this context, we explore the non-uniform decomposition of the data domain to improve fluid flow simulation performance on heterogeneous architectures . We evaluate two hybrid architectures : one comprised of a general-purpose x86 CPU and a graphics processing unit (GPU) integrated into a single chip (AMD Kaveri SoC), and another comprised by a general-purpose ARM CPU and a Field Programmable Gate Array (FPGA) integrated into the same chip (Intel Arria 10 SoC). We investigate the effects on performance and energy efficiency of data decomposition on each platform’s devices on a collaborative execution. Our case study is the well-known Lattice Boltzmann Method (LBM), where we apply the technique and analyze the performance and energy efficiency of five kernels on both devices on each platform. Our experimental results show that non-uniform partitioning improves the performance of LBM kernels by up to 11.40\% and 15.15\% on AMD Kaveri and Intel Arria 10, respectively. While AMD’s Kaveri platform’s performance efficiency is of up to 10.809 MLUPS with an energy efficiency of 142.881 MLUPKJ, Intel’s Arria 10 platform’s is of up to 1.12 MLUPS and 82.272 MLUPKJ.},
  archive      = {J_JPDC},
  author       = {Gabriel Freytag and Matheus S. Serpa and João V.F. Lima and Paolo Rech and Philippe O.A. Navaux},
  doi          = {10.1016/j.jpdc.2021.02.006},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {11-20},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Collaborative execution of fluid flow simulation using non-uniform decomposition on heterogeneous architectures},
  volume       = {152},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). RDIC: A blockchain-based remote data integrity checking
scheme for IoT in 5G networks. <em>JPDC</em>, <em>152</em>, 1–10. (<a
href="https://doi.org/10.1016/j.jpdc.2021.02.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Internet of things (IoT) is one of the main application scenarios of 5th generation mobile networks (5G). Along with the rapid development of 5G, IoT terminal devices will create big data. Generally, IoT terminal devices are lightweight user equipments, for example, wearable devices . In order to take use of these lightweight terminal devices, it is a feasible way to outsource these created big data to the public cloud. When these data are out of the client’s control, it is of crucial importance to ensure the remote data integrity. To solve the weaknesses of the existing remote data integrity checking schemes, we propose the concept of blockchain-based remote data integrity checking (RDIC) scheme for big data. The new concept makes use of blockchain technique which greatly improves the efficiency and security of RDIC. First, the system model and security definition are given for the proposed RDIC scheme by using blockchain . Then, by using efficient modular arithmetic, RSA digital signature , blockchain, etc , we construct a lightweight blockchain-based RDIC scheme. Finally, we analyze its security and performance. The detailed analysis shows that our scheme is provably secure and lightweight.},
  archive      = {J_JPDC},
  author       = {Huaqun Wang and Debiao He and Jia Yu and Neal N. Xiong and Bin Wu},
  doi          = {10.1016/j.jpdc.2021.02.012},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {1-10},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {RDIC: A blockchain-based remote data integrity checking scheme for IoT in 5G networks},
  volume       = {152},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Intelligent colocation of HPC workloads. <em>JPDC</em>,
<em>151</em>, 125–137. (<a
href="https://doi.org/10.1016/j.jpdc.2021.02.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many HPC applications suffer from a bottleneck in the shared caches, instruction execution units, I/O or memory bandwidth , even though the remaining resources may be underutilized. It is hard for developers and runtime systems to ensure that all critical resources are fully exploited by a single application, so an attractive technique for increasing HPC system utilization is to colocate multiple applications on the same server. When applications share critical resources, however, contention on shared resources may lead to reduced application performance. In this paper, we show that server efficiency can be improved by first modeling the expected performance degradation of colocated applications based on measured hardware performance counters, and then exploiting the model to determine an optimized mix of colocated applications. This paper presents a new intelligent resource manager and makes the following contributions: (1) a new machine learning model to predict the performance degradation of colocated applications based on hardware counters and (2) an intelligent scheduling scheme deployed on an existing resource manager to enable application co-scheduling with minimum performance degradation. Our results show that our approach achieves performance improvements of 7\% (avg) and 12\% (max) compared to the standard policy commonly used by existing job managers.},
  archive      = {J_JPDC},
  author       = {Felippe Vieira Zacarias and Vinicius Petrucci and Rajiv Nishtala and Paul Carpenter and Daniel Mossé},
  doi          = {10.1016/j.jpdc.2021.02.010},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {125-137},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Intelligent colocation of HPC workloads},
  volume       = {151},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DDF library: Enabling functional programming in a task-based
model. <em>JPDC</em>, <em>151</em>, 112–124. (<a
href="https://doi.org/10.1016/j.jpdc.2021.02.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the areas of High-Performance Computing (HPC) and massive data processing (also know as Big Data) have been in a convergence course, since they tend to be deployed on similar hardware. HPC systems have historically performed well in regular, matrix-based computations; on the other hand, Big Data problems have often excelled in fine-grained, data parallel workloads . While HPC programming is mostly task-based, like COMPSs, popular Big Data environments , like Spark, adopt the functional programming paradigm. A careful analysis shows that there are pros and cons to both approaches, and integrating them may yield interesting results. With that reasoning in mind, we have developed DDF, an API and library for COMPSs that allows developers to use Big Data techniques while using that HPC environment. DDF has a functional-based interface, similar to many Data Science tools, that allows us to use dynamic evaluation to adapt the task execution in run time. It brings some of the qualities of Big Data programming, making it easier for application domain experts to write Data Analysis jobs. In this article we discuss the API and evaluate the impact of the techniques used in its implementation that allow a more efficient COMPSs execution. In addition, we present a performance comparison with Spark in several application patterns . The results show that each technique significantly impacts the performance, allowing COMPSs to outperform Spark in many use cases.},
  archive      = {J_JPDC},
  author       = {Lucas M. Ponce and Daniele Lezzi and Rosa M. Badia and Dorgival Guedes},
  doi          = {10.1016/j.jpdc.2021.02.009},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {112-124},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {DDF library: Enabling functional programming in a task-based model},
  volume       = {151},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Granite: A distributed engine for scalable path queries over
temporal property graphs. <em>JPDC</em>, <em>151</em>, 94–111. (<a
href="https://doi.org/10.1016/j.jpdc.2021.02.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Property graphs are a common form of linked data , with path queries used to traverse and explore them for enterprise transactions and mining. Temporal property graphs are a recent variant where time is a first-class entity to be queried over, and their properties and structure vary over time. These are seen in social, telecom, transit and epidemic networks. However, current graph databases and query engines have limited support for temporal relations among graph entities, no support for time-varying entities and/or do not scale on distributed resources. We address this gap by extending a linear path query model over property graphs to include intuitive temporal predicates and aggregation operators over temporal graphs . We design a distributed execution model for these temporal path queries using the interval-centric computing model, and develop a novel cost model to select an efficient execution plan from several. We perform detailed experiments of our G r a n i t e Granite distributed query engine using both static and dynamic temporal property graphs as large as 52 M 52M vertices, 218 M 218M edges and 325 M 325M properties, and a 1600-query workload, derived from the LDBC benchmark. We frequently offer sub-second query latencies on a commodity cluster, which is 149 × 149× – 1140 × 1140× faster compared to industry-leading Neo4J shared-memory graph database and the JanusGraph/Spark distributed graph query engine. G r a n i t e Granite also completes 100\% of the queries for all graphs, compared to only 32–92\% workload completion by the baseline systems. Further, our cost model selects a query plan that is within 10\% of the optimal execution time in 90\% of the cases. Despite the irregular nature of graph processing, we exhibit a weak-scaling efficiency of ≥ 60\% ≥60\% on 8 nodes and ≥ 40\% ≥40\% on 16 nodes, for most query workloads.},
  archive      = {J_JPDC},
  author       = {Shriram Ramesh and Animesh Baranawal and Yogesh Simmhan},
  doi          = {10.1016/j.jpdc.2021.02.004},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {94-111},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Granite: A distributed engine for scalable path queries over temporal property graphs},
  volume       = {151},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Distributed programming of a hyperspectral image
registration algorithm for heterogeneous GPU clusters. <em>JPDC</em>,
<em>151</em>, 86–93. (<a
href="https://doi.org/10.1016/j.jpdc.2021.02.014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperspectral image registration is a relevant task for real-time applications such as environmental disaster management or search and rescue scenarios. The HYFMGPU algorithm was proposed as a single-GPU high-performance solution, but the need for a distributed version has arisen due to the continuous evolution of sensors that generate images with finer spatial and spectral resolutions . In a previous work, we simplified the programming of the multi-device parts of an initial MPI+CUDA multi-GPU implementation of HYFMGPU by means of Hitmap, a library to ease the programming of parallel applications based on distributed arrays. The performance of that Hitmap version was assessed in a homogeneous GPU cluster. In this paper, we extend this implementation by means of new functionalities added to the latest version of Hitmap in order to support arbitrary load distributions for multi-node heterogeneous GPU clusters. Three different load balancing layouts are tested, which prove that selecting a proper layout affects the performance of the code and how this performance is correlated with the use of the GPUs available in the cluster.},
  archive      = {J_JPDC},
  author       = {Jorge Fernández-Fabeiro and Arturo Gonzalez-Escribano and Diego R. Llanos},
  doi          = {10.1016/j.jpdc.2021.02.014},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {86-93},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Distributed programming of a hyperspectral image registration algorithm for heterogeneous GPU clusters},
  volume       = {151},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). TSM2X: High-performance tall-and-skinny matrix–matrix
multiplication on GPUs. <em>JPDC</em>, <em>151</em>, 70–85. (<a
href="https://doi.org/10.1016/j.jpdc.2021.02.013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Linear algebra operations have been widely used in big data analytics and scientific computations. Many works have been done on optimizing linear algebra operations on GPUs with regular-shaped input. However, few works focus on fully utilizing GPU resources when the input is not regular-shaped. Current optimizations do not consider fully utilizing the memory bandwidth and computing power; therefore, they can only achieve sub-optimal performance. In this paper, we propose two efficient algorithms – TSM2R and TSM2L – for two classes of tall-and-skinny matrix–matrix multiplications on GPUs . Both of them focus on optimizing linear algebra operation with at least one of the input matrices tall-and-skinny. Specifically, TSM2R is designed for a large regular-shaped matrix multiplying a tall-and-skinny matrix, while TSM2L is designed for a tall-and-skinny matrix multiplying a small regular-shaped matrix. We implement our proposed algorithms and test on several modern NVIDIA GPU micro-architectures. Experiments show that, compared to the current state-of-the-art works, (1) TSM2R speeds up the computation by 1.6x on average and improves the memory bandwidth utilization and computing power utilization by 18.1\% and 20.5\% on average, respectively, when the regular-shaped matrix size is relatively large or medium; and (2) TSM2L speeds up the computation by 1.9x on average and improves the memory bandwidth utilization by up to 9.3\% on average when the regular-shaped matrix size is relatively small.},
  archive      = {J_JPDC},
  author       = {Cody Rivera and Jieyang Chen and Nan Xiong and Jing Zhang and Shuaiwen Leon Song and Dingwen Tao},
  doi          = {10.1016/j.jpdc.2021.02.013},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {70-85},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {TSM2X: High-performance tall-and-skinny matrix–matrix multiplication on GPUs},
  volume       = {151},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Behavior analysis and blockchain based trust management in
VANETs. <em>JPDC</em>, <em>151</em>, 61–69. (<a
href="https://doi.org/10.1016/j.jpdc.2021.02.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development of vehicular ad hoc networks (VANETs) is facing great challenges. Due to the open environment in VANETs, the false information sent by malicious vehicles not only affects the fairness of information interaction but also seriously threatens the driving safety of normal vehicles. Therefore, the study of trust evaluation and management in VANETs has become hot topics in recent years. In this paper, we propose a trust management model of VANETs based on blockchain . In this model, a hidden markov model (HMM) based vehicle trust evaluation method that improve the accuracy on the detection of malicious behavior is proposed. Besides, a trust management method based on the alliance chain is designed, which greatly improves the efficiency of trust updating and querying on the premise of security. Simulation results show that the model is promising and feasible, effective in the aspects of trust evaluation and trust management.},
  archive      = {J_JPDC},
  author       = {Han Liu and Dezhi Han and Dun Li},
  doi          = {10.1016/j.jpdc.2021.02.011},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {61-69},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Behavior analysis and blockchain based trust management in VANETs},
  volume       = {151},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Generalizing the over operator for parallelization and
order-independency. <em>JPDC</em>, <em>151</em>, 52–60. (<a
href="https://doi.org/10.1016/j.jpdc.2021.02.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The o v e r over operator is commonly used for α α -blending in various visualization techniques. In the current form, it is a binary operator and must strictly follow a specific composition order of all participating operands, hence posing a significant performance limit. In this paper, we derive a set of generic formulas for the o v e r over operator that work with any number of operands and completely remove the restriction on the composition order. We prove the correctness of these formulas and provide a step-by-step illustration in a blending context. We implement both a sequential and a parallel version of the improved o v e r over operator and apply them to the image composition process where operands are received out of order with different arrival time intervals. The performance superiority of the improved o v e r over operator over the original one is established by rigorous theoretical analyses and further validated by extensive experimental results.},
  archive      = {J_JPDC},
  author       = {Dongliang Chu and Chase Q. Wu},
  doi          = {10.1016/j.jpdc.2021.02.001},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {52-60},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Generalizing the over operator for parallelization and order-independency},
  volume       = {151},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Improving the accuracy of energy predictive models for
multicore CPUs by combining utilization and performance events model
variables. <em>JPDC</em>, <em>151</em>, 38–51. (<a
href="https://doi.org/10.1016/j.jpdc.2021.01.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Energy predictive modeling is the leading method for determining the energy consumption of an application. Performance monitoring counters (PMCs) and resource utilizations have been the principal source of model variables primarily due to their high positive correlation with energy consumption. Performance events, however, have come to dominate the landscape due to their better prediction accuracy compared to utilization variables. Recently, the theory of energy of computing has been proposed whose practical implications for constructing accurate and reliable linear energy predictive models are unified in a consistency test that includes a selection criterion of additivity for model variables. In this work, we analyze the prediction accuracy of models employing utilization variables only, PMCs only, and combination of both utilization variables and PMCs, through the lens of this theory for modern multicore CPU platforms. We discover that employing utilization variables only in linear energy predictive models does not capture all the energy-consuming activities during an application execution. However, combination of utilization variables with PMCs that are highly additive and highly correlated with energy consumption, gives the most accurate linear energy predictive model. Our experimental results show that application-specific and platform-level models using both utilization variables and PMCs exhibit up to 3.6 × × and 2.6 × × better average prediction accuracy respectively when compared with models employing utilization variables only and highly additive PMCs only.},
  archive      = {J_JPDC},
  author       = {Arsalan Shahid and Muhammad Fahad and Ravi Reddy Manumachu and Alexey Lastovetsky},
  doi          = {10.1016/j.jpdc.2021.01.007},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {38-51},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Improving the accuracy of energy predictive models for multicore CPUs by combining utilization and performance events model variables},
  volume       = {151},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A novel hybrid resampling algorithm for parallel/distributed
particle filters. <em>JPDC</em>, <em>151</em>, 24–37. (<a
href="https://doi.org/10.1016/j.jpdc.2021.02.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Parallel/Distributed particle filters have been widely used in the estimation of states of dynamic systems by using multiple processing units (PUs). In parallel/distributed particle filters, the centralized resampling needs a central unit (CU) to serve as a hub to execute the global resampling. The centralized scheme is the main obstacle for the improved performance due to its global nature. To reduce the communication cost, the decentralized resampling was proposed, which only conducted the resampling on each PU. Although the decentralized resampling can improve the performance, it suffers from the low accuracy due to the local nature. Therefore, we propose a novel hybrid resampling algorithm to dynamically adjust the intervals between the centralized resampling steps and the decentralized resampling steps based on the measured system convergence. We formulate the proposed algorithm and prove it to be uniformly convergent . Since the proposed algorithm is a generalization of various versions of the hybrid resampling, its proof provides the solid theoretical foundation for their wide adoptions in parallel/distributed particle filters. In the experiments, we evaluate and compare different resampling algorithms including the centralized resampling algorithm, the decentralized resampling algorithm, and different types of existing hybrid resampling algorithms to show the effectiveness and the improved performance of the proposed hybrid resampling algorithm.},
  archive      = {J_JPDC},
  author       = {Xudong Zhang and Liang Zhao and Wei Zhong and Feng Gu},
  doi          = {10.1016/j.jpdc.2021.02.005},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {24-37},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {A novel hybrid resampling algorithm for parallel/distributed particle filters},
  volume       = {151},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Speed-area optimized VLSI architecture of multi-bit cellular
automaton cell based random number generator on FPGA with testable logic
support. <em>JPDC</em>, <em>151</em>, 13–23. (<a
href="https://doi.org/10.1016/j.jpdc.2021.01.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we have addressed a speed-area efficient VLSI implementation of a cellular automaton (CA) based random number generator (RNG) on Field Programmable Gate Arrays (FPGAs), in which each CA cell was proposed to be a multi-bit word in the original algorithm. This is in contrast to typical CA algorithms comprising one bit per CA cell. The original algorithm is shown favorable for FPGA implementations on adopting a fabric conscious approach involving instantiation of physical FPGA primitives. We have supplemented the original architecture with scan path and alternating logic to facilitate fault localization without area and delay overhead. The overheads have been carefully nullified by increasing the utilization ratio of the configured primitives, and exploiting the fast hardwired fabric of the FPGA. Generation of the hardware description of the RNG through Verilog has been automated. Our proposed designs outperform equivalent behavioral implementations expressed at higher levels of abstraction, both in speed and area.},
  archive      = {J_JPDC},
  author       = {Ayan Palchaudhuri and Anindya Sundar Dhar},
  doi          = {10.1016/j.jpdc.2021.01.005},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {13-23},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Speed-area optimized VLSI architecture of multi-bit cellular automaton cell based random number generator on FPGA with testable logic support},
  volume       = {151},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Flexible scheme for reconfiguring 2D mesh-connected VLSI
subarrays under row and column rerouting. <em>JPDC</em>, <em>151</em>,
1–12. (<a href="https://doi.org/10.1016/j.jpdc.2021.01.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the mesh-connected processors, some processor elements (PEs) become ineffective due to high temperature, overload and other factors, which can affect the stability of the system. This paper deals with the problem of reconfiguring the largest possible subarray from the processor with faults under the row and column rerouting constraint. Firstly, a flexible routing scheme , based on dynamic programming , is proposed to construct the local optimal logical columns. Secondly, we discuss and revise the PEs that cannot be connected between every two logical columns under this scheme. Finally, an efficient algorithm is presented to construct the maximum subarray in polynomial time . The experimental results show that, both on the random and clustered fault scenarios, the proposed algorithm under flexible rerouting scheme is capable of constructing the larger scale logical arrays. On a 48 × 48 host array with 15\% fault density, the improvement on the use of fault-free PEs is up to 6.22\% for random faults. On a 256 × 256 host array, the improvement can be up to 85.60\% for clustered faults. Moreover, the proposed algorithm runs faster than previous algorithms under different size arrays and fault densities, the average improvement in running time is up to 99\% compared with state-of-the-art.},
  archive      = {J_JPDC},
  author       = {Hao Ding and Junyan Qian and Bisheng Huang and Lingzhong Zhao and Zhongyi Zhai},
  doi          = {10.1016/j.jpdc.2021.01.003},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {1-12},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Flexible scheme for reconfiguring 2D mesh-connected VLSI subarrays under row and column rerouting},
  volume       = {151},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On the correctness and efficiency of a novel lock-free hash
trie map design. <em>JPDC</em>, <em>150</em>, 184–195. (<a
href="https://doi.org/10.1016/j.jpdc.2021.01.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hash tries are a trie-based data structure with nearly ideal characteristics for the implementation of hash maps. In this paper, we present a novel, simple and scalable hash trie map design that fully supports the concurrent search, insert and remove operations on hash maps. To the best of our knowledge, our proposal is the first that puts together the following characteristics: (i) be lock-free; (ii) use fixed size data structures; and (iii) maintain the access to all internal data structures as persistent memory references. Our design is modular enough to allow different types of configurations aimed for different performances in memory usage and execution time and can be easily implemented in any type of language, library or within other complex data structures. We discuss in detail the key algorithms required to easily reproduce our implementation by others and we present a proof of correctness showing that our proposal is linearizable and lock-free for the search, insert and remove operations. Experimental results show that our proposal is quite competitive when compared against other state-of-the-art proposals implemented in Java.},
  archive      = {J_JPDC},
  author       = {Miguel Areias and Ricardo Rocha},
  doi          = {10.1016/j.jpdc.2021.01.001},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {184-195},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {On the correctness and efficiency of a novel lock-free hash trie map design},
  volume       = {150},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A review of edge computing: Features and resource
virtualization. <em>JPDC</em>, <em>150</em>, 155–183. (<a
href="https://doi.org/10.1016/j.jpdc.2020.12.015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advent of Internet of Things (IoT) connecting billions of mobile and stationary devices to serve real-time applications, cloud computing paradigms face some significant challenges such as high latency and jitter, non-supportive location-awareness and mobility, and non-adaptive communication types. To address these challenges, edge computing paradigms, namely Fog Computing (FC), Mobile Edge Computing (MEC) and Cloudlet , have emerged to shift the digital services from centralized cloud computing to computing at edges. In this article, we analyze cloud and edge computing paradigms from features and pillars perspectives to identify the key motivators of the transitions from one type of virtualized computing paradigm to another one. We then focus on computing and network virtualization techniques as the essence of all these paradigms, and delineate why virtualization features , resource richness and application requirements are the primary factors for the selection of virtualization types in IoT frameworks. Based on these features, we compare the state-of-the-art research studies in the IoT domain. We finally investigate the deployment of virtualized computing and networking resources from performance perspective in an edge-cloud environment, followed by mapping of the existing work to the provided taxonomy for this research domain. The lessons from the reviewed are that the selection of virtualization technique, placement and migration of virtualized resources rely on the requirements of IoT services (i.e., latency, scalability, mobility, multi-tenancy, privacy, and security). As a result, there is a need for prioritizing the requirements, integrating different virtualization techniques , and exploiting a hierarchical edge-cloud architecture.},
  archive      = {J_JPDC},
  author       = {Yaser Mansouri and M. Ali Babar},
  doi          = {10.1016/j.jpdc.2020.12.015},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {155-183},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {A review of edge computing: Features and resource virtualization},
  volume       = {150},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Shortest-path routing for optimal all-to-all
personalized-exchange embedding on hierarchical hypercube networks.
<em>JPDC</em>, <em>150</em>, 139–154. (<a
href="https://doi.org/10.1016/j.jpdc.2021.01.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hypercube (HC) networks ( N = = 2 n n ) provide efficient communication for parallel-and-distributed computing (PDC) but the HC-based multi-processor (MP) system is costly and not scalable, while hierarchical hypercube (HHC) networks ( N = = 2 n n , n = = 2 m m + m ) are less expensive and more scalable. However, the traditional HHC-routing easily conflicts, especially when executing multiple tasks ( k &gt; 2 m m nodes-per-task). In the past, the node-disjoint-path routing could be used to avoid the conflict but that reliable ( S , D ) routing limited S (source) = = 0. Later, the parallel N 2 N (node-to-node) disjoint-path was proposed for reliable routing but limited ( m +1)/2 pairs-of-nodes. Therefore, this study proposes the generalized N 2 N shortest-path (SP) routing (with arbitrary S ) and the parallel N 2 N SP-routing, based on our hypothesis “the shortest-path routing and the proper HHC-partitioning can avoid the HHC-conflict directly”. Next, our innovation and contribution are 1. the GCD (grouping of cross dual-cube) partitioning to solve the HHC-conflict for k ≤ 2 m + 1 k≤2m+1 nodes-per-task, and 2. the SP-ATAPE (all-to-all personalized exchange) embedding on the HHC-MPs. The correctness of the SP-routing was proven and the ATAPE communication was experimented to validate our conflict solution. The ATAPE results confirmed that in any group the GCD mapping could make all tasks ( k = = 2 m + 1 m+1 nodes-per-task), synchronized with the same control, working without the conflict.},
  archive      = {J_JPDC},
  author       = {Nuntipat Phisutthangkoon and Jeeraporn Werapun},
  doi          = {10.1016/j.jpdc.2021.01.004},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {139-154},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Shortest-path routing for optimal all-to-all personalized-exchange embedding on hierarchical hypercube networks},
  volume       = {150},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Spartan: Sparse robust addressable networks. <em>JPDC</em>,
<em>150</em>, 121–138. (<a
href="https://doi.org/10.1016/j.jpdc.2020.12.013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A Peer-to-Peer (P2P) network is a dynamic collection of nodes that connect with each other via virtual overlay links built upon an underlying network (usually, the Internet). P2P networks are highly dynamic and can experience very heavy churn, i.e., a large number of nodes join/leave the network continuously. Thus, building and maintaining a stable overlay network is an important problem that has been studied extensively for two decades. In this paper, we present our P2P overlay network called Sparse Robust Addressable Network (Spartan). Spartan can be quickly and efficiently built in a fully distributed fashion within O ( log n ) O(logn) rounds. Furthermore, the Spartan overlay structure can be maintained, again, in a fully distributed manner despite adversarially controlled churn (i.e., nodes joining and leaving) and significant variation in the number of nodes. Moreover, new nodes can join a committee within O ( 1 ) O(1) rounds and leaving nodes can leave without any notice. The number of nodes in the network lies in [ n , f n ] [n,fn] for any fixed f ≥ 1 f≥1 . Up to ϵ n ϵn nodes (for some small but fixed ϵ &gt; 0 ϵ&amp;gt;0 ) can be adversarially added/deleted within any period of P P rounds for some P ∈ O ( log log n ) P∈O(loglogn) . Despite such uncertainty in the network, Spartan maintains Θ ( n ∕ log n ) Θ(n∕logn) committees that are stable and addressable collections of Θ ( log n ) Θ(logn) nodes each for polynomial ( n ) polynomial(n) rounds with high probability. Spartan’s committees are also capable of performing sustained computation and passing messages between each other. Thus, any protocol designed for static networks can be simulated on Spartan with minimal overhead. This makes Spartan an ideal platform for developing applications. We experimentally show that Spartan will remain robust as long as each committee, on average, contains 24 nodes for networks of size up to 10240.},
  archive      = {J_JPDC},
  author       = {John Augustine and Sumathi Sivasubramaniam},
  doi          = {10.1016/j.jpdc.2020.12.013},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {121-138},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Spartan: Sparse robust addressable networks},
  volume       = {150},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Randomized renaming in shared memory systems. <em>JPDC</em>,
<em>150</em>, 112–120. (<a
href="https://doi.org/10.1016/j.jpdc.2021.01.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Renaming is a task in distributed computing where n n processes are assigned new names from a name space of size m m . The problem is called tight if m = n m=n , and loose if m &gt; n m&amp;gt;n . In recent years renaming came to the fore again and new algorithms were developed. For tight renaming in asynchronous shared memory systems , Alistarh et al. describe a construction based on the AKS network that assigns all names within O ( log n ) O(logn) steps per process. They also show that, depending on the size of the name space, loose renaming can be done considerably faster. For m = ( 1 + ϵ ) ⋅ n m=(1+ϵ)⋅n and constant ϵ ϵ , they achieve a step complexity of O ( log log n ) O(loglogn) . In this paper we consider tight as well as loose renaming and introduce randomized algorithms that achieve their tasks with high probability. The model assumed is the asynchronous shared-memory model against an adaptive adversary. Our algorithm for loose renaming maps n n processes to a name space of size m = ( 1 + 2 ∕ ( log n ) ℓ ) ⋅ n = ( 1 + o ( 1 ) ) ⋅ n m=(1+2∕(logn)ℓ)⋅n=(1+o(1))⋅n performing O ( ℓ ⋅ ( log log n ) 2 ) O(ℓ⋅(loglogn)2) test-and-set operations. In the case of tight renaming, we present a protocol that assigns n n processes to n n names with step complexity O ( log n ) O(logn) , but without the overhead and impracticality of the AKS network. This algorithm utilizes modern hardware features in form of a counting device which is also described in the paper. This device may have the potential to speed up other distributed algorithms as well.},
  archive      = {J_JPDC},
  author       = {Petra Berenbrink and André Brinkmann and Robert Elsässer and Tom Friedetzky and Lars Nagel},
  doi          = {10.1016/j.jpdc.2021.01.002},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {112-120},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Randomized renaming in shared memory systems},
  volume       = {150},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep learning inspired routing in ICN using monte carlo tree
search algorithm. <em>JPDC</em>, <em>150</em>, 104–111. (<a
href="https://doi.org/10.1016/j.jpdc.2020.12.014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Information Centric Networking (ICN) provides caching strategies to improve network performance based on consumer demands from the intermediate routers . It reduces the load on content server, network traffic, and improves end-to-end delay. The content requesters use an Interest packet containing the name of data to express their needs. If such Interest packets are routed efficiently, the end to end delay and throughput of the network could be improved further. This paper describes an efficient method of forwarding Interest packets to retrieve the requested content at the earliest possible time. Here the data source is found and considered as a single player game with content requester as its start state and location of the desired content as final or goal state. The Monte Carlo Tree Search (MCTS) algorithm is used for constructing the path from content requester to concerned data source. For performance evaluation, the proposed scheme is integrated with Leave Copy Down (LCD) and Leave Copy Everywhere (LCE), Cache Less for More (CL4M), and Probability based caching (ProbCache) In ns -3 simulation environment (ndnSim), all these are evaluated in terms of content search latency, server hit ratio, network load, overhead and throughput. Simulation observation reveals that the integration of MCTS significantly improves performance in regard to experimental parameters.},
  archive      = {J_JPDC},
  author       = {Nitul Dutta and Shobhit K. Patel and Vadim Samusenkov and Vigneswaran D.},
  doi          = {10.1016/j.jpdc.2020.12.014},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {104-111},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Deep learning inspired routing in ICN using monte carlo tree search algorithm},
  volume       = {150},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Parallel ensemble methods for causal direction inference.
<em>JPDC</em>, <em>150</em>, 96–103. (<a
href="https://doi.org/10.1016/j.jpdc.2020.12.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inferring the causal direction between two variables from their observation data is one of the most fundamental and challenging topics in data science. A causal direction inference algorithm maps the observation data into a binary value which represents either x x causes y y or y y causes x x . The nature of these algorithms makes the results unstable with the change of data points. Therefore the accuracy of the causal direction inference can be improved significantly by using parallel ensemble frameworks. In this paper, new causal direction inference algorithms based on several ways of parallel ensemble are proposed. Theoretical analyses on accuracy rates are given. Experiments are done on both of the artificial data sets and the real world data sets. The accuracy performances of the methods and their computational efficiencies in parallel computing environment are demonstrated.},
  archive      = {J_JPDC},
  author       = {Yulai Zhang and Jiachen Wang and Gang Cen and Kueiming Lo},
  doi          = {10.1016/j.jpdc.2020.12.012},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {96-103},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Parallel ensemble methods for causal direction inference},
  volume       = {150},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Resisting newborn attacks via shared proof-of-space.
<em>JPDC</em>, <em>150</em>, 85–95. (<a
href="https://doi.org/10.1016/j.jpdc.2020.12.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the cryptocurrency literature, Proof-of-Space has been a potential alternative for permissionless distributed consensus protocols not only due to its recyclable nature but also the potential to support multiple chains simultaneously. Namely, the same storage resource can be contributed to the consensus of more than one chain. However, a direct shared proof of the same storage brings about newborn attacks on new chain launching since holders of a substantial amount of resources can easily devastate a new chain with minor underlying storage at almost no cost, deviating from the decentralized principle of cryptocurrencies. To fix this gap, we propose an innovative framework of single-chain Proof-of-Space and further present a novel multi-chain scheme which resists newborn attacks effectively by elaborately combining shared proof and chain-specific proof of storage. Our framework covers both classical Nakamoto consensus (with one leader per round) and hybrid consensus (with multiple leaders per round). Specific protocols for both cases are presented. A committee-based consensus is leveraged to realize the multiple leader case. We show that both consensus schemes have realized our desired functionality without compromising consistency or liveness.},
  archive      = {J_JPDC},
  author       = {Shuyang Tang and Jilai Zheng and Yao Deng and Qinxiang Cao},
  doi          = {10.1016/j.jpdc.2020.12.011},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {85-95},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Resisting newborn attacks via shared proof-of-space},
  volume       = {150},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Short- and long-term cost and performance optimization for
mobile user equipments. <em>JPDC</em>, <em>150</em>, 69–84. (<a
href="https://doi.org/10.1016/j.jpdc.2020.12.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Task offloading strategy optimization in mobile edge computing (MEC) has always been a hot issue. However, the mobility of a user equipment (UE) seriously affects the UE’s cost and performance. This paper proposes three mobility types depending on whether the mobility characteristic of a UE is known, and formulates an energy minimization problem and a latency minimization problem to optimize the cost and performance, respectively. We first develop greedy strategy based task offloading algorithms for UEs according to their mobility characteristics. However, accurately obtaining the mobility characteristics of the UEs over a long time in practice is a huge challenge, especially in a highly random environment like the MEC. To address the issue, we use a Lyapunov optimization method to develop the algorithms that do not require any prior knowledge of the mobility characteristics to minimize the long-term energy and latency of UEs. Experimental results show that the greedy strategy based algorithms can optimize the cost and performance of UEs by using their mobility characteristics, and perform better than the Lyapunov optimization based algorithms in a short-term. However, the Lyapunov optimization based algorithms perform better than the greedy strategy based algorithms over a long-term.},
  archive      = {J_JPDC},
  author       = {Yan Ding and Kenli Li and Chubo Liu and Zhuo Tang and Keqin Li},
  doi          = {10.1016/j.jpdc.2020.12.006},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {69-84},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Short- and long-term cost and performance optimization for mobile user equipments},
  volume       = {150},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Parallel lossless HSI compression based on RLS filter.
<em>JPDC</em>, <em>150</em>, 60–68. (<a
href="https://doi.org/10.1016/j.jpdc.2020.12.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recent advancement in the field of electronics has led to development of sensors that capture the image of an area or object in spectral-domain along with spatial information. Due to continuity of spectral domain in hyperspectral images , it is difficult to store, process, analyze or transmit the critical information contained in it. Prediction based compression technique is used to reduce this size by a certain level. It predicts the value of a pixel with some error from previous pixels using a filter and finally, encode that error using variable length encoder. The execution time taken by this technique is very high which can be reduced by high performance computing . In this paper, we designed a mechanism to use high-performance computing techniques in the execution of prediction based image compression algorithms. The average execution time of the RLS-filter based compression algorithm is reduced significantly (by a factor of 29 using 2 nodes with 28 cores each, on PARAM SHIVAY supercomputer) with the proposed technique.},
  archive      = {J_JPDC},
  author       = {Yaman Dua and Vinod Kumar and Ravi Shankar Singh},
  doi          = {10.1016/j.jpdc.2020.12.004},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {60-68},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Parallel lossless HSI compression based on RLS filter},
  volume       = {150},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Energy efficient IoT-fog based architectural paradigm for
prevention of dengue fever infection. <em>JPDC</em>, <em>150</em>,
46–59. (<a href="https://doi.org/10.1016/j.jpdc.2020.12.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dengue is one of the most common and widespread infectious illnesses in humans transmitted by female Aedes albopictis. The prevalence of Dengue cases has increased substantially leading to human morbidity. Inadequate availability of healthcare professionals and inaccessibility to healthcare institutions have aggravated the problem. The traditional medical technologies are too antiquated to serve the purpose. The innovative latest technologies like Internet of Things (IoT), Cloud Computing , Fog Computing have made real-time and remote healthcare possible with huge success. In this paper, an IoT based Fog–Cloud enabled system for monitoring, assessment and control of Dengue Fever has been proposed. IoT sensors acquire data about a large spectrum of health as well as environmental factors that contribute to infection. The battery constrained sensors set their sampling rate according to the degree of cruciality that saves power to make battery long lasting. The Fog layer employs Support Vector Machine (SVM) for Dengue infection evaluation with least latency and sends alerts including precautionary measures to the users, hospital officials and government agencies. Moreover, the proposed system utilizes Temporal Network Analysis (TNA) and Google map service to categorize areas as infected, uninfected or risk prone . The experimental results are evaluated by a number of analytical parameters to investigate the effect of proposed system. SVM performs the best in terms of accuracy, recall, specificity, precision and f-measure with values 93\%, 95\%, 89\%, 94\% and 95\% respectively. Furthermore, TNA based outbreak assessment gives valuable inputs for the government institutions to control the outbreak.},
  archive      = {J_JPDC},
  author       = {Sandeep K. Sood and Amandeep Kaur and Vaishali Sood},
  doi          = {10.1016/j.jpdc.2020.12.002},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {46-59},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Energy efficient IoT-fog based architectural paradigm for prevention of dengue fever infection},
  volume       = {150},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). PackStealLB: A scalable distributed load balancer based on
work stealing and workload discretization. <em>JPDC</em>, <em>150</em>,
34–45. (<a href="https://doi.org/10.1016/j.jpdc.2020.12.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The scalability of high-performance, parallel iterative applications is directly affected by how well they use the available computing resources. These applications are subject to load imbalance due to the nature and dynamics of their computations. It is common that high performance systems employ periodic load balancing to tackle this issue. Dynamic load balancing algorithms redistribute the application’s workload using heuristics to circumvent the NP-hard complexity of the problem However, scheduling heuristics must be fast to avoid hindering application performance when distributing the workload on large and distributed environments. In this work, we present a technique for low overhead, high quality scheduling decisions for parallel iterative applications . The technique relies on combined application workload information paired with distributed scheduling algorithms . An initial distributed step among scheduling agents group application tasks in packs of similar load to minimize messages among them. This information is used by our scheduling algorithm, PackStealLB, for its distributed-memory work stealing heuristic. Experimental results showed that PackStealLB is able to improve the performance of a molecular dynamics benchmark by up to 41\%, outperforming other scheduling algorithms in most scenarios over almost one thousand cores.},
  archive      = {J_JPDC},
  author       = {Vinicius Freitas and Laércio L. Pilla and Alexandre de L. Santana and Márcio Castro and Johanne Cohen},
  doi          = {10.1016/j.jpdc.2020.12.005},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {34-45},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {PackStealLB: A scalable distributed load balancer based on work stealing and workload discretization},
  volume       = {150},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Emulous mechanism based multi-objective moth–flame
optimization algorithm. <em>JPDC</em>, <em>150</em>, 15–33. (<a
href="https://doi.org/10.1016/j.jpdc.2020.12.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, there has been growing interest in using metaheuristic algorithms to solve various complex engineering optimization problems . Most of the real-world problems comprise of more than one objective. Due to the inherent difficulty of such problems and lack of proficiency, researchers in different domains often aggregate multiple objectives and use single-objective optimization algorithms to solve them. However, the aggregation-based methods fail to solve the multi-objective problems (MOPs) effectively. Several multi-objective evolutionary algorithms (MOEAs) have been proposed and are being used to solve such problems in the past few years. In this paper, we propose an Emulous Mechanism-based multi-objective Moth–Flame Optimization (EMMFO) algorithm, where the moth positions are updated based on the pairwise competitions between the moths in each generation. The proposed EMMFO is tested on a diverse set of multi-objective benchmark functions like ZDT, DTLZ, WFG, CEC09 special session test suites and four constrained engineering design problems . The results are compared with various state-of-the-art multi-objective algorithms like NSGAII , SPEA2, PESA2, MOEA/D, MOPSO, MOACO, NSMFO, IEMO, CLPSO-LS, MOEA/D-CRA, PAL-SAPSO, and MORBABC/D. Extensive experimental results demonstrate superior optimization performance of the proposed algorithm.},
  archive      = {J_JPDC},
  author       = {Saunhita Sapre and Mini S.},
  doi          = {10.1016/j.jpdc.2020.12.010},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {15-33},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Emulous mechanism based multi-objective moth–flame optimization algorithm},
  volume       = {150},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). When services computing meets blockchain: Challenges and
opportunities. <em>JPDC</em>, <em>150</em>, 1–14. (<a
href="https://doi.org/10.1016/j.jpdc.2020.12.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Services computing can offer a high-level abstraction to support diverse applications via encapsulating various computing infrastructures. Though services computing has greatly boosted the productivity of developers, it is faced with three main challenges: privacy and security risks, information silo, and pricing mechanisms and incentives. The recent advances of blockchain bring opportunities to address the challenges of services computing due to its build-in encryption as well as digital signature schemes, decentralization feature, and intrinsic incentive mechanisms. In this paper, we present a survey to investigate the integration of blockchain with services computing. The integration of blockchain with services computing mainly exhibits merits in two aspects: i) blockchain can potentially address key challenges of services computing and ii) services computing can also promote blockchain development. In particular, we categorize the current literature of services computing based on blockchain into five types: services creation, services discovery, services recommendation , services composition, and services arbitration. Moreover, we generalize Blockchain as a Service (BaaS) architecture and summarize the representative BaaS platforms. In addition, we also outline open issues of blockchain-based services computing and BaaS.},
  archive      = {J_JPDC},
  author       = {Xiaoyun Li and Zibin Zheng and Hong-Ning Dai},
  doi          = {10.1016/j.jpdc.2020.12.003},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {1-14},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {When services computing meets blockchain: Challenges and opportunities},
  volume       = {150},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). BSF: A parallel computation model for scalability estimation
of iterative numerical algorithms on cluster computing systems.
<em>JPDC</em>, <em>149</em>, 193–206. (<a
href="https://doi.org/10.1016/j.jpdc.2020.12.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper examines a novel parallel computation model called bulk synchronous farm (BSF) that focuses on estimating the scalability of compute-intensive iterative algorithms aimed at cluster computing systems. The main advantage of the proposed model is that it allows to estimate the scalability of a parallel algorithm before its implementation. Another important feature of the BSF model is the representation of problem data in the form of lists that greatly simplifies the logic of building applications. In the BSF model, a computer is a set of processor nodes connected by a network and organized according to the master/slave paradigm. A cost metric of the BSF model is presented. This cost metric requires the algorithm to be represented in the form of operations on lists. This allows us to derive an equation that predicts the scalability boundary of a parallel program: the maximum number of processor nodes after which the speedup begins to decrease. The paper includes examples of applying the BSF model to designing and analyzing parallel numerical algorithms. The large-scale computational experiments conducted on a cluster computing system confirm the adequacy of the analytical estimations obtained using the BSF model.},
  archive      = {J_JPDC},
  author       = {Leonid B. Sokolinsky},
  doi          = {10.1016/j.jpdc.2020.12.009},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {193-206},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {BSF: A parallel computation model for scalability estimation of iterative numerical algorithms on cluster computing systems},
  volume       = {149},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A robust approach for barrier-reinforcing in wireless sensor
networks. <em>JPDC</em>, <em>149</em>, 186–192. (<a
href="https://doi.org/10.1016/j.jpdc.2020.12.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wireless sensor network barrier coverage plays an important role in intrusion detection . How to construct a robust barrier is a key research issue. For the initial deployment of barriers, with the depletion of node energy, some node dies prematurely, resulting in the existence of more weak points in the barrier. A method of using the re-deployment of mobile nodes to strengthen the barrier is proposed. This method adopts the set-based max-flow algorithm to calculate the number of weak points that can be strengthened and deploys and schedules movable nodes according to the weak situation to strengthen the barrier. The enhanced barrier has better performance and solves the problem of strengthening the weak points of the barrier. Simulation results show that the proposed algorithm can effectively strengthen the barrier and extend the survival time of the barrier, and the complexity of the algorithm is also relatively low.},
  archive      = {J_JPDC},
  author       = {Omar A. Saraereh and Ashraf Ali and Luae Al-Tarawneh and Imran Khan},
  doi          = {10.1016/j.jpdc.2020.12.007},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {186-192},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {A robust approach for barrier-reinforcing in wireless sensor networks},
  volume       = {149},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A decentralized algorithm to combine topology control with
network coding. <em>JPDC</em>, <em>149</em>, 174–185. (<a
href="https://doi.org/10.1016/j.jpdc.2020.12.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network coding and topology control techniques have been widely used to increase throughput and improve the lifetime of Wireless Sensor Networks (WSNs). This paper considers the simultaneous utilization of these techniques in a WSN and proposes convex non-linear programming. Since solving the problem for a large-scale and dynamic WSN is impractical and almost impossible, Lagrangian, sub-gradient and the decomposition methods are employed to provide a decentralized algorithm. In the proposed algorithm, a node makes the computations by acquiring local knowledge and information from its neighbors. This paper provides a mathematical language to build an analytic foundation for the design of a modularized and decentralized algorithm that provides transmission ranges and routes for a WSN. The simulation results show that increasing the number of sources, sinks, sensors, and traffic load leads to improving the lifetime which is acquired by the proposed algorithm.},
  archive      = {J_JPDC},
  author       = {Moammad Khalily-Dermany},
  doi          = {10.1016/j.jpdc.2020.12.001},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {174-185},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {A decentralized algorithm to combine topology control with network coding},
  volume       = {149},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Efficient shuffle management for DAG computing frameworks
based on the FRQ model. <em>JPDC</em>, <em>149</em>, 163–173. (<a
href="https://doi.org/10.1016/j.jpdc.2020.11.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In large-scale data-parallel analytics, shuffle, namely the cross-network read and the aggregation of partitioned data between tasks with data dependencies , usually bring in large overhead. To reduce shuffle overhead, we present SCache , an open-source plug-in system that particularly focuses on shuffle optimization. SCache adopts heuristic pre-scheduling combining with shuffle size prediction to pre-fetch shuffle data and balance load on each node. Meanwhile, SCache takes full advantage of the system memory to accelerate the shuffle process. We also propose a new performance model called Framework Resources Quantification (FRQ) model to analyze DAG frameworks and evaluate the SCache shuffle optimization. The FRQ model quantifies the utilization of resources and predicts the execution time of each phase of DAG jobs. We have implemented SCache on both Spark and Hadoop MapReduce . The performance of SCache has been evaluated with both simulations and testbed experiments on a 50-node Amazon EC2 cluster. Those evaluations have demonstrated that, by incorporating SCache, the shuffle overhead of Spark can be reduced by nearly 89\%, and the overall completion time of TPC-DS queries improves 40\% on average. On Apache Hadoop MapReduce , SCache optimizes end-to-end Terasort completion time by 15\%.},
  archive      = {J_JPDC},
  author       = {Rui Ren and Chunghsuan Wu and Zhouwang Fu and Tao Song and Yanqiang Liu and Zhengwei Qi and Haibing Guan},
  doi          = {10.1016/j.jpdc.2020.11.008},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {163-173},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Efficient shuffle management for DAG computing frameworks based on the FRQ model},
  volume       = {149},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fast GPU 3D diffeomorphic image registration. <em>JPDC</em>,
<em>149</em>, 149–162. (<a
href="https://doi.org/10.1016/j.jpdc.2020.11.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D image registration is one of the most fundamental and computationally expensive operations in medical image analysis. Here, we present a mixed-precision, Gauss–Newton–Krylov solver for diffeomorphic registration of two images. Our work extends the publicly available CLAIRE library to GPU architectures. Despite the importance of image registration, only a few implementations of large deformation diffeomorphic registration packages support GPUs. Our contributions are new algorithms to significantly reduce the run time of the two main computational kernels in CLAIRE : calculation of derivatives and scattered-data interpolation. We deploy (i) highly-optimized, mixed-precision GPU-kernels for the evaluation of scattered-data interpolation, (ii) replace Fast-Fourier-Transform (FFT)-based first-order derivatives with optimized 8th-order finite differences, and (iii) compare with state-of-the-art CPU and GPU implementations. As a highlight, we demonstrate that we can register 25 6 3 2563 clinical images in less than 6 s on a single NVIDIA Tesla V100. This amounts to over 20 × × speed-up over the current version of CLAIRE and over 30 × × speed-up over existing GPU implementations.},
  archive      = {J_JPDC},
  author       = {Malte Brunn and Naveen Himthani and George Biros and Miriam Mehl and Andreas Mang},
  doi          = {10.1016/j.jpdc.2020.11.006},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {149-162},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Fast GPU 3D diffeomorphic image registration},
  volume       = {149},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DDMTS: A novel dynamic load balancing scheduling scheme
under SLA constraints in cloud computing. <em>JPDC</em>, <em>149</em>,
138–148. (<a href="https://doi.org/10.1016/j.jpdc.2020.11.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud computing is a computing method based on the Internet designed to share resources through virtualization technology . For a large number of requests waiting to be processed, task scheduling is used to reasonably allocate computing resources to requests. With the rapid development of computer hardware and software, deep reinforcement learning (DRL) provides a new direction for better solving task scheduling problems. In this paper, we propose a novel DRL-based dynamic load balancing task scheduling algorithm under service-level agreement (SLA) constraints to reduce the load imbalance of virtual machines (VMs) and task rejection rate. First, we use the DRL method to select a suitable VM for the task and then determine whether to execute the task on the selected VM violates the SLA. If the SLA is violated, the task is refused and feedback a negative reward for DRL training; otherwise, the task is received and executed, and feedback a reward according to the balance of the VMs load after the task is executed. Compared with three other task scheduling algorithms applied to randomly generated benchmark and Google real user workload trace benchmark, the proposed algorithm exhibits the best performance in balancing VMs load and reducing the task rejection rate, improving the overall level of cloud computing services .},
  archive      = {J_JPDC},
  author       = {Zhao Tong and Xiaomei Deng and Hongjian Chen and Jing Mei},
  doi          = {10.1016/j.jpdc.2020.11.007},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {138-148},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {DDMTS: A novel dynamic load balancing scheduling scheme under SLA constraints in cloud computing},
  volume       = {149},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hybrid KNN-join: Parallel nearest neighbor searches
exploiting CPU and GPU architectural features. <em>JPDC</em>,
<em>149</em>, 119–137. (<a
href="https://doi.org/10.1016/j.jpdc.2020.11.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {K Nearest Neighbor ( KNN ) joins are used in scientific domains for data analysis, and are building blocks of several well-known algorithms. KNN -joins find the KNN of all points in a dataset. This paper focuses on a hybrid CPU/GPU approach for low-dimensional KNN -joins, where the GPU may not yield substantial performance gains over parallel CPU algorithms. We utilize a work queue that prioritizes computing data points in high density regions on the GPU , and low density regions on the CPU, thereby taking advantage of each architecture’s relative strengths. Our approach, HybridKNN-Join , effectively augments a state-of-the-art multi-core CPU algorithm. We propose optimizations that ( i ) (i) maximize GPU query throughput by assigning the GPU large batches of work; ( i i ) (ii) increase workload granularity to optimize GPU utilization; and, ( i i i ) (iii) limit load imbalance between CPU and GPU architectures. We compare HybridKNN-Join to one GPU and two parallel CPU reference implementations. Compared to the reference implementations, we find that the hybrid algorithm performs best on larger workloads (dataset size and K). The methods employed in this paper show promise for the general division of work in other hybrid algorithms.},
  archive      = {J_JPDC},
  author       = {Michael Gowanlock},
  doi          = {10.1016/j.jpdc.2020.11.004},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {119-137},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Hybrid KNN-join: Parallel nearest neighbor searches exploiting CPU and GPU architectural features},
  volume       = {149},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Management of geo-distributed intelligence: Deep insight as
a service (DINSaaS) on forged cloud platforms (FCP). <em>JPDC</em>,
<em>149</em>, 103–118. (<a
href="https://doi.org/10.1016/j.jpdc.2020.11.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recent advances in the cyber–physical domains, cloud and edge platforms along with the advanced communication technologies play a crucial role in connecting the globe more than ever, which is creating large volumes of data at astonishing rates and a tsunami of computation within hyper-connectivity. Data analytic tools are evolving rapidly to harvest these explosive increasing data volumes. Deriving meaningful insights from voluminous geo-distributed data of all kinds as a strategic asset is fuelling the innovation, facilitating e-commerce and revolutionizing the industry and businesses in the transition from digital to the intelligent way of doing business. In this perspective, in this study, a philosophical industrial and technological direction involving Deep Insight-as-a-Service (DINSaaS) on Forged Cloud Platforms (FCP) along with Advanced Insight Analytics (AIA), primarily motivated by the global benefit is systematically analysed within sophisticated theoretical knowledge, and consequently, a conceptual geo-distributed framework is proposed to (1) guide the national/international leading organizations, governments, cloud service providers and leading companies in order to establish a scalable framework within the hyperscale geo-distributed infrastructure in which exponentially increasing voluminous Big Data (BD) can be harvested effectively and efficiently, (2) inspire the transformation of BD into wiser abstract formats in Specialized Insight Domains (SID), (3) provide fusion and networking of insights rather than BD in order to obtain globally generated distributed intelligence and help make better decisions and near-real-time predictions, in particular for time-critical latency-sensitive applications, and (4) direct all the stakeholders to rivet the high-quality products and services within Automation of Everything (AoE) by exploiting continuously created and updated insights in dedicated taxonomic SID within large-scale geo-distributed datacenters.},
  archive      = {J_JPDC},
  author       = {Kaya Kuru},
  doi          = {10.1016/j.jpdc.2020.11.009},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {103-118},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Management of geo-distributed intelligence: Deep insight as a service (DINSaaS) on forged cloud platforms (FCP)},
  volume       = {149},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A high efficient multi-robot simultaneous localization and
mapping system using partial computing offloading assisted cloud point
registration strategy. <em>JPDC</em>, <em>149</em>, 89–102. (<a
href="https://doi.org/10.1016/j.jpdc.2020.10.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The robots using visual simultaneous localization and mapping (SLAM) system are generally experiencing excessive power consumption and suffer from depletion of battery energy during the course of working. The intensive computation necessary to complete complicated tasks is overwhelming for inexpensive mobile robots with limited on-board resources. To address this problem, a novel task offloading strategy combined with a new dense point cloud map construction method is proposed in this paper, which is firstly used for the improvement of the system especially in indoor scenes. First, we develop a novel strategy to remotely offload computation-intensive tasks to cloud center so that the tasks that could not originally be achieved locally on the resource-limited robot systems become possible. Second, a modified iterative closest point algorithm (ICP), named fitness score hierarchical ICP algorithm (FS-HICP), is developed to accelerate point cloud registration. The correctness, efficiency, and scalability of the proposed strategy are evaluated with both theoretical analysis and experimental simulations. The results show that the proposed method can effectively reduce the energy consumption while increase the computation capability and speed of the multi-robot visual SLAM system, especially in indoor environment.},
  archive      = {J_JPDC},
  author       = {Biwei Li and Zhenqiang Mi and Yu Guo and Yang Yang and Mohammad S. Obaidat},
  doi          = {10.1016/j.jpdc.2020.10.012},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {89-102},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {A high efficient multi-robot simultaneous localization and mapping system using partial computing offloading assisted cloud point registration strategy},
  volume       = {149},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Toward security as a service: A trusted cloud service
architecture with policy customization. <em>JPDC</em>, <em>149</em>,
76–88. (<a href="https://doi.org/10.1016/j.jpdc.2020.11.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rise of concerns over security and privacy in the cloud, the “security-on-demand” service mode dynamically provides cloud customers with trusted computing environments according to their specific security needs. Major challenges, however, remain to achieve this goal: (1) integrating an auditable, tamper-resistant trust-management mechanism into the cloud infrastructure and (2) building a protocol to guarantee the consistency of customers’ policies during virtual machine (VM) migrations. This study develops a new security-on-demand framework called a “policy-customized trusted cloud service” (PC-TCS) architecture that comprises two core components: an attribute-based signature (ABS)-based remote-attestation scheme to achieve trusted remote attestation with customized security policies and an ABS- and blockchain-based VM-migration protocol to support policy-customized trusted migration. To prove the availability of this architecture, we implemented a PC-TCS prototype based on Xen Hypervisor , the results of which indicate that (1) PC-TCS can be integrated into cloud infrastructure as part of a trusted computing base ; (2) cloud users can customize the security policies of computing environments and validate their enforcement throughout the service life-cycle with the support of PC-TCS; and (3) PC-TCS can support policy-customized remote attestation and policy-customized migration with a minimal impact on performance.},
  archive      = {J_JPDC},
  author       = {Chenlin Huang and Wei Chen and Lu Yuan and Yan Ding and Songlei Jian and Yusong Tan and Hua Chen and Dan Chen},
  doi          = {10.1016/j.jpdc.2020.11.002},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {76-88},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Toward security as a service: A trusted cloud service architecture with policy customization},
  volume       = {149},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Trustzone-based secure lightweight wallet for hyperledger
fabric. <em>JPDC</em>, <em>149</em>, 66–75. (<a
href="https://doi.org/10.1016/j.jpdc.2020.11.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of blockchain-based digital currencies, the security of digital wallets becomes more and more important. As far as we know, there is no safe lightweight wallet in hyperledger fabric. To solve the problem, we proposed a T rustzone-based S ecure L ightweight W allet for H yperledger F abric (hereafter referred to as TSLWHF ). Firstly, we designed an Unspent Transaction Output (UTXO) set of transactions under blockchain and a signature verification mechanism for transactions, which made it possible to implement the lightweight wallet in hyperledger fabric. Then, we implemented a reliable protection mechanism for private keys and wallet’s address, which solved the problem that users’ information might be stolen or replaced. Meanwhile, the transaction verification results are guaranteed not to be tampered by hackers through verifying transactions in Trusted Execution Environment (TEE) and encrypting local block headers. Finally, to demonstrate utility, we deployed the system in hyperledger fabric and trustzone. Experiments showed that the wallet reduces the size of locally stored data while protecting the security of user’s assets. The time spent on TSLWHF to execute a transaction is 0.589s, which improves transaction’s performance compared to Bitcoin wallet.},
  archive      = {J_JPDC},
  author       = {Weiqi Dai and Qinyuan Wang and Zeli Wang and Xiaobin Lin and Deqing Zou and Hai Jin},
  doi          = {10.1016/j.jpdc.2020.11.001},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {66-75},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Trustzone-based secure lightweight wallet for hyperledger fabric},
  volume       = {149},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Communication optimization strategies for distributed deep
neural network training: A survey. <em>JPDC</em>, <em>149</em>, 52–65.
(<a href="https://doi.org/10.1016/j.jpdc.2020.11.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent trends in high-performance computing and deep learning have led to the proliferation of studies on large-scale deep neural network training. However, the frequent communication requirements among computation nodes drastically slow the overall training speeds, which causes bottlenecks in distributed training, particularly in clusters with limited network bandwidths . To mitigate the drawbacks of distributed communications, researchers have proposed various optimization strategies . In this paper, we provide a comprehensive survey of communication strategies from both an algorithm viewpoint and a computer network perspective. Algorithm optimizations focus on reducing the communication volumes used in distributed training, while network optimizations focus on accelerating the communications between distributed devices. At the algorithm level, we describe how to reduce the number of communication rounds and transmitted bits per round. In addition, we elucidate how to overlap computation and communication. At the network level, we discuss the effects caused by network infrastructures, including logical communication schemes and network protocols. Finally, we extrapolate the potential future challenges and new research directions to accelerate communications for distributed deep neural network training.},
  archive      = {J_JPDC},
  author       = {Shuo Ouyang and Dezun Dong and Yemao Xu and Liquan Xiao},
  doi          = {10.1016/j.jpdc.2020.11.005},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {52-65},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Communication optimization strategies for distributed deep neural network training: A survey},
  volume       = {149},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Efficient performance prediction for apache spark.
<em>JPDC</em>, <em>149</em>, 40–51. (<a
href="https://doi.org/10.1016/j.jpdc.2020.10.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spark is a more efficient distributed big data processing framework following Hadoop . It provides users with more than 180 adjustable configuration parameters , and how to choose the optimal configuration automatically to make the Spark application run effectively is challenging. The key to address the above challenge is having the ability to predict the performance of Spark applications in different configurations. This paper proposes a new approach based on Adaboost , which can efficiently and accurately predict the performance of a given application with a given Spark configuration. In our approach, Adaboost is used to build a set of performance models at the stage-level for Spark. To minimize the overhead of the modeling, we use the classic projective sampling, a data mining technique that allows us to collect as few training samples as possible while meeting the accuracy requirements. We evaluate the proposed approach on six typical Spark benchmarks with five input datasets. The experimental results show that our approach is less than the previously proposed approach in prediction error and cost.},
  archive      = {J_JPDC},
  author       = {Guoli Cheng and Shi Ying and Bingming Wang and Yuhang Li},
  doi          = {10.1016/j.jpdc.2020.10.010},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {40-51},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Efficient performance prediction for apache spark},
  volume       = {149},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A blockchain-based roadside unit-assisted authentication and
key agreement protocol for internet of vehicles. <em>JPDC</em>,
<em>149</em>, 29–39. (<a
href="https://doi.org/10.1016/j.jpdc.2020.11.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A fundamental layer of smart cities, the Internet of Vehicles (IoV) can significantly improve transportation efficiency, reduce energy consumption, and traffic accidents . However, because of the vehicle and the RoadSide Units (RSU) use wireless channels for communication, the risk of information being leaked or tampered is highly increased. Therefore, secure and reliable authentication and key agreement protocol is the masterpiece of IoV security. As most of the existing authentication protocols pertain to a centralized structure and single Trusted Authority (TA) network model, all vehicles involved can only perform mutual authentication with one TA through the intermediate node RSU, and thus, the efficiency of these centralized authentication protocols is easily affected by TA’s communication and computing resource bottlenecks. In this article, a blockchain-based authentication and key agreement protocol is designed for the multi-TA network model, moving the computing load of TA down to the RSU to improve the efficiency of authentication. In addition, blockchain technology is used for multiple TAs to manage the ledger that stores vehicle-related information, which results in vehicles that can easily achieve cross-TA authentication. Both formal and informal security analysis and simulation results from ProVerif show that the proposed protocol is secure. Comparisons with other existing work show that the proposed protocol has less computational overhead, higher authentication efficiency, and can resist various common attacks.},
  archive      = {J_JPDC},
  author       = {Zisang Xu and Wei Liang and Kuan-Ching Li and Jianbo Xu and Hai Jin},
  doi          = {10.1016/j.jpdc.2020.11.003},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {29-39},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {A blockchain-based roadside unit-assisted authentication and key agreement protocol for internet of vehicles},
  volume       = {149},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Interlaced: Fully decentralized churn stabilization for skip
graph-based DHTs. <em>JPDC</em>, <em>149</em>, 13–28. (<a
href="https://doi.org/10.1016/j.jpdc.2020.10.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a distributed hash table (DHT) routing overlay, Skip Graph is used in a variety of peer-to-peer (P2P) systems including cloud storage. The overlay connectivity of P2P systems is negatively affected by the arrivals and departures of nodes to and from the system that is known as churn . Preserving connectivity of the overlay network (i.e., the reachability of every pair of nodes) under churn without compromising the overlay latency is a performance challenge in every P2P system including the Skip Graph-based ones. The existing decentralized churn stabilization solutions that are applicable to Skip Graphs mainly optimize the connectivity of the system under churn and do not consider routing latency of overlay as an optimization goal. Additionally, those existing solutions change the message complexity of Skip Graphs, distort its topology, or apply constant message overhead to the system. In this paper, we propose Interlaced , a fully decentralized churn stabilization mechanism for Skip Graphs that provides drastically stronger overlay connectivity and faster search queries without changing the asymptotic complexity of the Skip Graph in terms of storage, computation, and communication. We also propose the Sliding Window De Bruijn Graph ( SWDBG ) as a tool to predict the availability of nodes with high accuracy. Our simulation results show that in comparison to the best existing DHT-based solutions, Interlaced improves the overlay connectivity of the Skip Graph under churn with the gain of about 1 . 73 1.73 times. Likewise, compared to the existing availability prediction approaches for P2P systems , SWDBG is about 1 . 26 1.26 times more accurate. A Skip Graph that benefits from Interlaced and SWDBG is about 2 . 47 2.47 times faster on average in routing the queries under churn compared to the best existing solutions. We also present an adaptive extension of Interlaced to be applied to other DHTs, for example, Kademlia.},
  archive      = {J_JPDC},
  author       = {Yahya Hassanzadeh-Nazarabadi and Alptekin Küpçü and Öznur Özkasap},
  doi          = {10.1016/j.jpdc.2020.10.008},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {13-28},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Interlaced: Fully decentralized churn stabilization for skip graph-based DHTs},
  volume       = {149},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Coalition formation for deadline-constrained resource
procurement in cloud computing. <em>JPDC</em>, <em>149</em>, 1–12. (<a
href="https://doi.org/10.1016/j.jpdc.2020.10.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To attract more customers, a cloud provider tends to give some discounts to a customer if he/she rents a plenty of resources. Under this situation, a group of customers who need homogeneous cloud instances with various deadlines are prone to purchasing resources in a collaborative manner, i.e., using a coalition game, to reduce purchase costs. It is essential to design a mechanism that enables all customers to voluntarily and happily collaborate while ensuring that each customer pays at the lowest cost possible. To address this issue, we propose a mechanism to show collaborative interactions between customers and determine the number of service programs purchased from each provider to charge each cloud customer a minimum cost. We establish a coalition game based on multi-customer resource procurement and prove that there exists a unique optimal solution in the coalition game, while satisfying individual stability and group stability. In addition, the optimal solution is a solution in which the selected service program of each coalition optimizes the cost per customer and maximizes resource utilization. We propose a heuristic Deadline-constrained Resource Coalition Allocation (DRCA) algorithm to calculate the near-optimal solution. A backtracking algorithm is proposed to calculate the pseudo maximum resource utilization of the provided programs by improving the rectangular packing. Extensive experiments are performed to verify the feasibility and effectiveness of the proposed algorithm.},
  archive      = {J_JPDC},
  author       = {Junyan Hu and Kenli Li and Chubo Liu and Jianguo Chen and Keqin Li},
  doi          = {10.1016/j.jpdc.2020.10.004},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {1-12},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Coalition formation for deadline-constrained resource procurement in cloud computing},
  volume       = {149},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). QoS provision in hierarchical and non-hierarchical switch
architectures. <em>JPDC</em>, <em>148</em>, 138–150. (<a
href="https://doi.org/10.1016/j.jpdc.2020.10.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quality of service (QoS) provision has become an important aspect of high-performance computing interconnection networks . Proof of that is the inclusion of mechanisms targeted to the provision of QoS by the main interconnection technologies such as Gigabit Ethernet , Infiniband (IB) and Omni-Path (OPA). A key component of QoS provision is the output scheduling algorithm , which determines when a packet should be transmitted. An ideal scheduling algorithm should satisfy two main properties: good end-to-end latency and implementation simplicity. Table-based schedulers are able to provide these two properties, and because of this, IB and OPA have implemented this approach. In this paper, we present a comparative study in terms of QoS provision between these two dominating interconnection technologies. Those interconnection technologies are also two examples of non-hierarchical and hierarchical switch architectures, respectively, which gives the results of this study greater significance. In order to carry out the study, the Deficit Table scheduler (DTable) has been used. DTable is a table-based scheduling algorithm which offers a good balance between end-to-end latency and implementation cost.},
  archive      = {J_JPDC},
  author       = {Javier Cano-Cano and Francisco J. Andújar and Francisco J. Alfaro-Cortés and José L. Sánchez},
  doi          = {10.1016/j.jpdc.2020.10.009},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {138-150},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {QoS provision in hierarchical and non-hierarchical switch architectures},
  volume       = {148},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). High performance GPU primitives for graph-tensor learning
operations. <em>JPDC</em>, <em>148</em>, 125–137. (<a
href="https://doi.org/10.1016/j.jpdc.2020.10.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph-tensor learning operations extend tensor operations by taking the graph structure into account, which have been applied to diverse domains such as image processing and machine learning . However, the running time of graph-tensor operations increases rapidly with the number of nodes and the dimension of data on nodes, making them impractical for real-time applications. In this paper, we propose a GPU library called cuGraph-Tensor for high-performance graph-tensor learning operations, which consists of eight key operations: graph shift ( g-shift ), graph Fourier transform ( g-FT ), inverse graph Fourier transform (inverse g-FT ), graph filter ( g-filter ), graph convolution ( g-convolution ), graph-tensor product ( g-product ), graph-tensor SVD ( g-SVD ) and graph-tensor QR ( g-QR ). cuGraph-Tensor supports scalar, vector, and matrix data processing on each graph node . We propose optimization techniques on computing, memory accesses, and CPU–GPU communications that significantly improve the performance of the graph-tensor learning operations. Using the optimized operations, cuGraph-Tensor builds a graph data completion application for fast and accurate reconstruction of incomplete graph data. In the experiments, the proposed graph learning operations achieve up to 142 . 12 × 142.12× speedups versus CPU-based GSPBOX and CPU MATLAB implementations running on two Xeon CPUs. The graph data completion application achieves up to 174 . 38 × 174.38× speedups over the CPU MATLAB implementation, and up to 3 . 82 × 3.82× speedups with better accuracy over the GPU-based tensor completion in the cuTensor-tubal library.},
  archive      = {J_JPDC},
  author       = {Tao Zhang and Wang Kan and Xiao-Yang Liu},
  doi          = {10.1016/j.jpdc.2020.10.011},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {125-137},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {High performance GPU primitives for graph-tensor learning operations},
  volume       = {148},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Decentralized learning works: An empirical comparison of
gossip learning and federated learning. <em>JPDC</em>, <em>148</em>,
109–124. (<a href="https://doi.org/10.1016/j.jpdc.2020.10.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning over distributed data stored by many clients has important applications in use cases where data privacy is a key concern or central data storage is not an option. Recently, federated learning was proposed to solve this problem. The assumption is that the data itself is not collected centrally. In a master–worker architecture, the workers perform machine learning over their own data and the master merely aggregates the resulting models without seeing any raw data, not unlike the parameter server approach. Gossip learning is a decentralized alternative to federated learning that does not require an aggregation server or indeed any central component. The natural hypothesis is that gossip learning is strictly less efficient than federated learning due to relying on a more basic infrastructure: only message passing and no cloud resources. In this empirical study, we examine this hypothesis and we present a systematic comparison of the two approaches. The experimental scenarios include a real churn trace collected over mobile phones, continuous and bursty communication patterns, different network sizes and different distributions of the training data over the devices. We also evaluate a number of additional techniques including a compression technique based on sampling, and token account based flow control for gossip learning. We examine the aggregated cost of machine learning in both approaches. Surprisingly, the best gossip variants perform comparably to the best federated learning variants overall, so they offer a fully decentralized alternative to federated learning.},
  archive      = {J_JPDC},
  author       = {István Hegedűs and Gábor Danner and Márk Jelasity},
  doi          = {10.1016/j.jpdc.2020.10.006},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {109-124},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Decentralized learning works: An empirical comparison of gossip learning and federated learning},
  volume       = {148},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An empirical study of i/o separation for burst buffers in
HPC systems. <em>JPDC</em>, <em>148</em>, 96–108. (<a
href="https://doi.org/10.1016/j.jpdc.2020.10.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To meet the exascale I/O requirements for the High-Performance Computing (HPC), a new I/O subsystem, Burst Buffer, based on solid state drives (SSD), has been developed. However, the diverse HPC workloads and the bursty I/O pattern cause severe data fragmentation that requires costly garbage collection (GC) and increases the number of bytes written to the SSD. To address this data fragmentation challenge, a new multi-stream feature has been developed for SSDs. In this work, we develop an I/O Separation scheme called BIOS to leverage this multi-stream feature to group the I/O streams based on the user IDs. We propose a stream-aware scheduling policy based on burst buffer pools in the workload manager, and integrate the BIOS with the workload manager to optimize the I/O separation scheme in burst buffer. We evaluate the proposed framework with a burst buffer I/O traces from Cori Supercomputer including a diverse set of applications. Experimental results show that the BIOS could improve the performance by 1.44x on average and reduce the Write Amplification Factor (WAF) by up to 1.20x. These demonstrate the potential benefits of the I/O separation scheme for solid state storage systems.},
  archive      = {J_JPDC},
  author       = {Donghun Koo and Jaehwan Lee and Jialin Liu and Eun-Kyu Byun and Jae-Hyuck Kwak and Glenn K. Lockwood and Soonwook Hwang and Katie Antypas and Kesheng Wu and Hyeonsang Eom},
  doi          = {10.1016/j.jpdc.2020.10.007},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {96-108},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {An empirical study of I/O separation for burst buffers in HPC systems},
  volume       = {148},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Joint coflow routing and scheduling in leaf-spine data
centers. <em>JPDC</em>, <em>148</em>, 83–95. (<a
href="https://doi.org/10.1016/j.jpdc.2020.09.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Communication in data centers often involves many parallel flows that all share the same performance goal (e.g. to minimize the average completion time). A useful abstraction, coflow , is proposed to express the communication requirements of prevalent data parallel paradigms such as MapReduce and Spark. The multiple coflow routing and scheduling problem makes it challenging to derive a good theoretical performance ratio, as coexisting coflows may compete for the same network resources such as link bandwidths . In this paper, we focus on the coflow problem in one popular data center infrastructure: the Leaf-Spine topology. We first formulate the problem and study the path selection issue on this two-tier structure. In order to minimize the average coflow completion time (CCT), we propose the Multi-hop Coflow Routing and Scheduling strategy (MCRS) and prove that our method has a reasonably good competitive ratio . Extensive experiments and large-scale simulations show that MCRS outperforms the state-of-the-art heuristic schemes under the Leaf-Spine topology.},
  archive      = {J_JPDC},
  author       = {Yang Chen and Jie Wu},
  doi          = {10.1016/j.jpdc.2020.09.007},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {83-95},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Joint coflow routing and scheduling in leaf-spine data centers},
  volume       = {148},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Graph-waving architecture: Efficient execution of graph
applications on GPUs. <em>JPDC</em>, <em>148</em>, 69–82. (<a
href="https://doi.org/10.1016/j.jpdc.2020.10.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing graph frameworks for GPUs adopt a vertex-centric computing model where vertex to thread mapping is applied. When run with irregular graphs, we observe significant load imbalance within SIMD-groups using vertex to thread mapping. Uneven work distribution within SIMD-groups leads to low utilization of SIMD units and inefficient use of memory bandwidth . We introduce Graph-Waving (GW) architecture to improve support for many graph applications on GPUs . It uses vertex to SIMD-group mapping and Scalar-Waving as a mechanism for efficient execution. It also favors a narrow SIMD-group width with a clustered issue approach and reuse of instructions in the front-end. We thoroughly evaluate GW architecture using timing detailed GPGPU-sim simulator with several graph and non-graph benchmarks from a variety of benchmark suites. Our results show that GW architecture provides an average of 4.4x and a maximum of 10x speedup with graph applications, while it obtains 9\% performance improvement with regular and 17\% improvement with irregular benchmarks.},
  archive      = {J_JPDC},
  author       = {Ayse Yilmazer-Metin},
  doi          = {10.1016/j.jpdc.2020.10.005},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {69-82},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Graph-waving architecture: Efficient execution of graph applications on GPUs},
  volume       = {148},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Improvement of recommendation algorithm based on
collaborative deep learning and its parallelization on spark.
<em>JPDC</em>, <em>148</em>, 58–68. (<a
href="https://doi.org/10.1016/j.jpdc.2020.09.014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Collaborative Deep Learning (CDL) utilizes the strong feature learning capability of neural network and the model fitting robustness to solve the problem that the performance of Recommender System drops dramatically when the data is sparse. However, it makes the model training become difficult to maintain when Recommender System faces a large amount of data, and a variety of unpredictable problems will arise. In order to solve the above problems, collaborative deep learning and its parallelization methods were studied in this study, and an improved model CDL-I (CDL with item private node) aiming at item content optimization based on collaborative deep learning was proposed, which improved SDAE on the basis of CDL, added private network nodes; in case of sharing the network parameters of the model, private bias terms were added for each item. As a result, the network may learn the item content parameters in a more targeted manner, thereby enhancing the detection performance of the model on item content in Recommender System. Furthermore, the algorithm was parallelized by splitting the model, and a parallel training CDL-I method was also proposed, which was transplanted to the Spark distributed cluster. The parameters of each part of the model were trained and optimized in parallel to enhance the scale and scalability of data that the model could process. The experiments on multiple real datasets have verified the effectiveness and efficiency of the proposed parallel CDL-I algorithm.},
  archive      = {J_JPDC},
  author       = {Fan Yang and Huaqiong Wang and Jianjing Fu},
  doi          = {10.1016/j.jpdc.2020.09.014},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {58-68},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Improvement of recommendation algorithm based on collaborative deep learning and its parallelization on spark},
  volume       = {148},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Blockchain-based eHealth system for auditable EHRs
manipulation in cloud environments. <em>JPDC</em>, <em>148</em>, 46–57.
(<a href="https://doi.org/10.1016/j.jpdc.2020.10.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development of cloud-assisted electronic health system effectively addresses the drawbacks of traditional medical management system . However, some challenging problems such as security and privacy in data storage and sharing cannot be ignored. First, it is difficult to ensure the integrity of electronic health records (EHRs) during the data outsourcing process. Second, it is difficult to guarantee the privacy and traceability of EHRs during the data sharing process. In this paper, a blockchain-based eHealth system called BCES is proposed to ensure that the manipulation of EHRs can be audited. In BCES, each legitimate query manipulation of data consumers, together with each legitimate outsourcing manipulation of hospitals, will be written into the blockchain as a transaction for permanent storage, which ensures the traceability. At the same time, the attributes-based proxy re-encryption is adopted to achieve fine-grained access control of medical data, and any behavior that threatens the integrity of EHRs will be discovered by the auditor. Due to the traceable and tamper-resistant characteristic of blockchain , any entity that had an illegal manipulation of EHRs will be held accountable to the evidence of our constructed Proof-Chain. Finally, security analysis and performance evaluation demonstrate that this scheme is secure and efficient.},
  archive      = {J_JPDC},
  author       = {Haiping Huang and Xiang Sun and Fu Xiao and Peng Zhu and Wenming Wang},
  doi          = {10.1016/j.jpdc.2020.10.002},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {46-57},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Blockchain-based eHealth system for auditable EHRs manipulation in cloud environments},
  volume       = {148},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Online multimedia retrieval on CPU–GPU platforms with
adaptive work partition. <em>JPDC</em>, <em>148</em>, 31–45. (<a
href="https://doi.org/10.1016/j.jpdc.2020.10.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nearest neighbors search is a core operation found in several online multimedia services . These services have to handle very large databases, while, at the same time, they must minimize the query response times observed by users. This is specially complex because those services deal with fluctuating query workloads (rates). Consequently, they must adapt at run-time to minimize the response times as the load varies. In this paper, we address the aforementioned challenges with a distributed memory parallelization of the product quantization nearest neighbor search, also known as IVFADC, for hybrid CPU–GPU machines. Our parallel IVFADC implements an out-of-GPU memory execution scheme to use the GPU for databases in which the index does not fit in its memory, which is crucial for searching in very large databases. The careful use of CPU and GPU with work stealing led to an average response time reduction of 2.4 × × as compared to using the GPU only. Also, our approach to adapt the system to fluctuating loads, called Dynamic Query Processing Policy (DQPP), attained a response time reduction of up to 5 × × vs. the best static (BS) policy for moderate loads. The system has attained high query processing rates and near-linear scalability in all experiments. We have evaluated our system on a machine with up to 256 NVIDIA V100 GPUs processing a database of 256 billion SIFT features vectors.},
  archive      = {J_JPDC},
  author       = {Rafael Souza and André Fernandes and Thiago S.F.X. Teixeira and George Teodoro and Renato Ferreira},
  doi          = {10.1016/j.jpdc.2020.10.001},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {31-45},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Online multimedia retrieval on CPU–GPU platforms with adaptive work partition},
  volume       = {148},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). IoT architecture for adaptation to transient devices.
<em>JPDC</em>, <em>148</em>, 14–30. (<a
href="https://doi.org/10.1016/j.jpdc.2020.09.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {IoT environments are continuously changing. Changes may come from the service, connectivity, or physical layers of the IoT architecture. Therefore, to function appropriately, the system needs to dynamically adapt to its environment. In previous work, we posited eight challenges to foster adaptation through all architecture layers of IoT systems. In this paper, we address the challenges to manage the inclusion of new devices and devices’ transient connection , by means of dynamic adaptations incorporated into our proposed software architecture for adaptive IoT systems. To manage dynamic adaptations, we extend the reference IoT architecture with our specialized components. In particular, we use (1) ontologies and instances to represent the domain knowledge; (2) a matching algorithm to pair services and IoT devices, taking into account their functional requirements, quality attributes and sensors properties; and (3) a match update algorithm used whenever sensors become (un)available. We evaluate the effectiveness of our solution with respect to the accuracy of matching services and IoT devices, and the response to environment changes.},
  archive      = {J_JPDC},
  author       = {Jairo Ariza and Kelly Garcés and Nicolás Cardozo and Juan Pablo Rodríguez Sánchez and Fernando Jiménez Vargas},
  doi          = {10.1016/j.jpdc.2020.09.012},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {14-30},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {IoT architecture for adaptation to transient devices},
  volume       = {148},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). I/o characteristic discovery for storage system
optimizations. <em>JPDC</em>, <em>148</em>, 1–13. (<a
href="https://doi.org/10.1016/j.jpdc.2020.08.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we introduce a new I/O characteristic discovery methodology for performance optimizations on object-based storage systems. Different from traditional methods that select limited access attributes or heavily reply on domain knowledge about applications’ I/O behaviors, our method enables capturing data-access features as many as possible to eliminate human bias. It utilizes a machine-learning based strategy (principal component analysis, PCA) to derive the most important set of features automatically, and groups data objects with a clustering algorithm (DBSCAN) to reveal I/O characteristics discovered. We have evaluated the proposed I/O characteristic discovery solution based on Sheepdog storage system and further implemented a data prefetching mechanism as a sample use case of this approach. Evaluation results confirm that the proposed solution can successfully identify access patterns and achieve efficient data prefetching by improving the buffer cache hit ratio up to 48.24\%. The overall performance was improved by up to 42\%.},
  archive      = {J_JPDC},
  author       = {Jiang Zhou and Yong Chen and Dong Dai and Yu Zhuang and Weiping Wang},
  doi          = {10.1016/j.jpdc.2020.08.005},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {1-13},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {I/O characteristic discovery for storage system optimizations},
  volume       = {148},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Editorial | journal of parallel and distributed computing -
volume 147. <em>JPDC</em>, <em>147</em>, 268–269. (<a
href="https://doi.org/10.1016/j.jpdc.2020.10.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JPDC},
  author       = {Carlos Reaño and Federico Silla and Blesson Varghese},
  doi          = {10.1016/j.jpdc.2020.10.003},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {268-269},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Editorial | journal of parallel and distributed computing - volume 147},
  volume       = {147},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MAD-c: Multi-stage approximate distributed cluster-combining
for obstacle detection and localization. <em>JPDC</em>, <em>147</em>,
248–267. (<a href="https://doi.org/10.1016/j.jpdc.2020.08.013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The upcoming digitalization in the context of Cyber–physical Systems (CPS), enabled through Internet-of-Things (IoT) infrastructures, require efficient methods for distributed processing of the data, that is generated by multiple sources. We address the problem of obstacle detection and localization through data clustering , which is a common component for data processing in the fusion of multiple point clouds, each obtained by a LIDAR sensor. Such sensors generate data at high rates and can rapidly exhaust traditional methods that centrally gather and process the global data. To that end, we propose MAD-C, an approximate method for distributed data summarization through clustering, that can orthogonally build on known methods for fine-grained point-cloud clustering, and synthesize a decentralized approach, which exploits the distributed processing capacity efficiently and prevents saturation of the communication network. In MAD-C, corresponding to the point-cloud gathered by each LIDAR sensor, local clusters are first identified, each corresponding to an object in the sensed environment from the perspective of the respective sensor. Afterwards, the information about each locally detected object is transformed into a data-summary, computable in a continuous manner, with constant overhead in time and space. The summaries are then combined, in an order-insensitive, concurrent fashion, to produce approximate volumetric representations of the objects in the fused data. We show that the combined summaries, in addition to localizing objects and approximating their volumetric representations, can be used to answer relevant queries regarding the relative position of the objects in environment and a geofence. We evaluate the performance of MAD-C extensively, both analytically and empirically. The empirical evaluation is performed on an IoT test-bed as well as in simulation. Our results show that MAD-C leads to (i) communication savings proportional to the number of points, (ii) multiplicative decrease in the dominating component of the processing complexity and, at the same time, (iii) high accuracy (with RandIndex &gt; 0 . 95 &amp;gt;0.95 ), in comparison to its baseline counterpart for obstacle detection and localization , as well as (iv) linear computational complexity in terms of the number of objects, for the geofence related queries.},
  archive      = {J_JPDC},
  author       = {Amir Keramatian and Vincenzo Gulisano and Marina Papatriantafilou and Philippas Tsigas},
  doi          = {10.1016/j.jpdc.2020.08.013},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {248-267},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {MAD-C: Multi-stage approximate distributed cluster-combining for obstacle detection and localization},
  volume       = {147},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Detection resource allocation scheme for two-layer
cooperative IDSs in smart grids. <em>JPDC</em>, <em>147</em>, 236–247.
(<a href="https://doi.org/10.1016/j.jpdc.2020.09.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although some existing collaborative intrusion detection schemes can increase the detection performance by dynamically allocating detection resources in smart grids, these related works fail to consider the optimization of resource allocation between IDSs under the condition of resource restriction. In this paper, considering the effect of resource restriction, we propose a resource allocation scheme for two-layer collaborative IDSs based on sharing strategies in smart grids. In the first layer of our scheme, we model the interaction between the IDSs and the attackers through a stochastic game based on sharing strategies, where we provide each IDS with two different options for its strategy updating at each stage in the stochastic game. Then the resource updating strategies of the IDSs are obtained through this proposed model. Further, in the second layer we quantify the effect of detection resource restriction, and we propose a resource allocation method under the condition of detection resource restriction, where each IDS can obtain its detection resources according to the results generated by our proposed stochastic game. Based on our experimental analysis, compared with other resource allocation schemes, our proposed scheme can more quickly achieve the Nash equilibrium between the IDSs and the attackers to make the IDSs obtain more rewards, and then can more rationally promote the IDSs to update their detection resources so that the IDSs obtain the optimal detection strategies under the condition of resource restriction. Our proposed scheme can achieve effective detection resource allocation between IDSs for the security of neighborhood area network in smart grids.},
  archive      = {J_JPDC},
  author       = {Zhuoqun Xia and Jingjing Tan and Ke Gu and WeiJia Jia},
  doi          = {10.1016/j.jpdc.2020.09.011},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {236-247},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Detection resource allocation scheme for two-layer cooperative IDSs in smart grids},
  volume       = {147},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On achieving interactive consistency in real-world
distributed systems. <em>JPDC</em>, <em>147</em>, 220–235. (<a
href="https://doi.org/10.1016/j.jpdc.2020.09.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interactive consistency is the problem in which n n distinct nodes, each having its own private value, where up to t t may be Byzantine, run an algorithm that allows all non-faulty nodes to infer the values of each other node. This problem is relevant to critical applications that rely on the combination of the opinions of multiple peers to provide a service. Examples include monitoring a content source to prevent equivocation or to track variability in the content provided, and resolving divergent state amongst the nodes of a distributed system. Previous works assume a fully synchronous system, where one can make strong assumptions such as negligible message delivery delays and/or detection of absent messages. However, practical, real-world systems are mostly asynchronous, i.e., they exhibit only some periods of synchrony during which message delivery is timely, thus requiring a different approach. In this paper, we present a thorough study of practical interactive consistency . We leverage the vast prior work on broadcast and Byzantine consensus algorithms to design, implement and evaluate a set of randomized algorithms , with only a single synchronization barrier and varying message complexities, that can be used to achieve interactive consistency in real-world distributed systems. We present formal proofs of correctness and message complexity of our proposed algorithms. We provide a complete, open-source implementation of each proposed interactive consistency algorithm by building a multi-layered software stack of algorithms that includes several broadcast algorithms, as well as a binary and a multi-valued consensus algorithm. Most of these algorithms have never been implemented and evaluated in a real system before. Finally, we analyze the performance of our suite of algorithms experimentally by testing both single instance and multiple parallel instances of each alternative and present a case study of achieving interactive consistency in a real-world distributed e-voting system.},
  archive      = {J_JPDC},
  author       = {Stathis Maneas and Nikos Chondros and Panos Diamantopoulos and Christos Patsonakis and Mema Roussopoulos},
  doi          = {10.1016/j.jpdc.2020.09.010},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {220-235},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {On achieving interactive consistency in real-world distributed systems},
  volume       = {147},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A multi-GPU biclustering algorithm for binary datasets.
<em>JPDC</em>, <em>147</em>, 209–219. (<a
href="https://doi.org/10.1016/j.jpdc.2020.09.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graphics Processing Units technology (GPU) and CUDA architecture are one of the most used options to adapt machine learning techniques to the huge amounts of complex data that are currently generated. Biclustering techniques are useful for discovering local patterns in datasets. Those of them that have been implemented to use GPU resources in parallel have improved their computational performance. However, this fact does not guarantee that they can successfully process large datasets. There are some important issues that must be taken into account, like the data transfers between CPU and GPU memory or the balanced distribution of workload between the GPU resources. In this paper, a GPU version of one of the fastest biclustering solutions, BiBit, is presented. This implementation, named gBiBit, has been designed to take full advantage of the computational resources offered by GPU devices. Either using a single GPU device or in its multi-GPU mode, gBiBit is able to process large binary datasets. The experimental results have shown that gBiBit improves the computational performance of BiBit, a CPU parallel version and an early GPU version, called ParBiBit and CUBiBit, respectively. gBiBit source code is available at https://github.com/aureliolfdez/gbibit .},
  archive      = {J_JPDC},
  author       = {Aurelio Lopez-Fernandez and Domingo Rodriguez-Baena and Francisco Gomez-Vela and Federico Divina and Miguel Garcia-Torres},
  doi          = {10.1016/j.jpdc.2020.09.009},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {209-219},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {A multi-GPU biclustering algorithm for binary datasets},
  volume       = {147},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Performance analysis of network-on-chip in many-core
processors. <em>JPDC</em>, <em>147</em>, 196–208. (<a
href="https://doi.org/10.1016/j.jpdc.2020.09.013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network-on-chip (NoC) is an integral part of many-core microprocessors . Performance analysis of network-on-chip directly affects the performance of the microprocessor. In this paper we propose a mathematical model to represent packet flow in an NoC as an open feed-forward queuing network . We study the performance of NoC by varying different parameters that includes packet injection rate , packet size, buffer size and number of virtual channels. We also discuss how different flow control algorithms, injection processes , traffic patterns can be incorporated in our model. Apart from the speedup achieved by our model, we also demonstrate that our model can be used to explore various configurations of NoC with minimal error.},
  archive      = {J_JPDC},
  author       = {A. Vijaya Bhaskar and T.G. Venkatesh},
  doi          = {10.1016/j.jpdc.2020.09.013},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {196-208},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Performance analysis of network-on-chip in many-core processors},
  volume       = {147},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Rendezvous algorithms for large-scale modeling and
simulation. <em>JPDC</em>, <em>147</em>, 184–195. (<a
href="https://doi.org/10.1016/j.jpdc.2020.09.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rendezvous algorithms encode a communication pattern that is useful when processors sending data do not know who the receiving processors should be, or vice versa. The idea is to define an intermediate decomposition where datums from different sending processors can ”rendezvous” to perform a computation, in a manner that both the senders and eventual receivers of the results can identify the appropriate rendezvous processor. Originally designed for interpolating between overlaid grids with independent parallel decompositions (Plimpton et al., 2004), we have recently found rendezvous algorithms useful for a variety of operations in particle- or grid-based simulation codes when running large problems on large numbers of processors. In particular, we show they can perform well when a load-balanced intermediate decomposition is randomized and not spatial, requiring all-to-all communication to move data between processors. In this case rendezvous algorithms leverage the large bisection communication bandwidths which parallel machines provide. We describe how rendezvous algorithms work in a scientific computing context and give specific examples for molecular dynamics and Direct Simulation Monte Carlo codes which result in dramatic performance improvements versus simpler algorithms which do not scale as well. We explain how a generic rendezvous algorithm can be implemented, and also point out similarities with the MapReduce paradigm popularized by Google and Hadoop .},
  archive      = {J_JPDC},
  author       = {Steven J. Plimpton and Christopher Knight},
  doi          = {10.1016/j.jpdc.2020.09.001},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {184-195},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Rendezvous algorithms for large-scale modeling and simulation},
  volume       = {147},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Accurate, efficient and scalable training of graph neural
networks. <em>JPDC</em>, <em>147</em>, 166–183. (<a
href="https://doi.org/10.1016/j.jpdc.2020.08.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Neural Networks (GNNs) are powerful deep learning models to generate node embeddings on graphs. When applying deep GNNs on large graphs, it is still challenging to perform training in an efficient and scalable way. We propose a novel parallel training framework. Through sampling small subgraphs as minibatches, we reduce training workload by orders of magnitude compared with state-of-the-art minibatch methods. We then parallelize the key computation steps on tightly-coupled shared memory systems . For graph sampling, we exploit parallelism within and across sampler instances, and propose an efficient data structure supporting concurrent accesses from samplers. The parallel sampler theoretically achieves near-linear speedup with respect to number of processing units. For feature propagation within subgraphs, we improve cache utilization and reduce DRAM traffic by data partitioning. Our partitioning is a 2-approximation strategy for minimizing the communication cost compared to the optimal. We further develop a runtime scheduler to reorder the training operations and adjust the minibatch subgraphs to improve parallel performance. Finally, we generalize the above parallelization strategies to support multiple types of GNN models and graph samplers. The proposed training outperforms the state-of-the-art in scalability, efficiency and accuracy simultaneously. On a 40-core Xeon platform, we achieve 60 × 60× speedup (with AVX) in the sampling step and 20 × 20× speedup in the feature propagation step, compared to the serial implementation . Our algorithm enables fast training of deeper GNNs, as demonstrated by orders of magnitude speedup compared to the Tensorflow implementation. We open-source our code at https://github.com/GraphSAINT/GraphSAINT},
  archive      = {J_JPDC},
  author       = {Hanqing Zeng and Hongkuan Zhou and Ajitesh Srivastava and Rajgopal Kannan and Viktor Prasanna},
  doi          = {10.1016/j.jpdc.2020.08.011},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {166-183},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Accurate, efficient and scalable training of graph neural networks},
  volume       = {147},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). CIC-PIM: Trading spare computing power for memory space in
graph processing. <em>JPDC</em>, <em>147</em>, 152–165. (<a
href="https://doi.org/10.1016/j.jpdc.2020.09.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Shared-memory graph processing is usually more efficient than in a cluster in terms of cost effectiveness, ease of programming and runtime. However, the limited memory capacity of a single machine and the huge sizes of graphs restrains its applicability. Hence, it is imperative to reduce memory footprint . We observe that index compression holds promise and propose CIC-PIM, a lightweight encoding with chunked index compression , to reduce the memory footprint and the runtime of graph algorithms . CIC-PIM aims for significant space saving, real random-access support and high cache efficiency by exploiting the ubiquitous power-law and sparseness features of large scale graphs. The basic idea is to divide index structures into chunks of appropriate size and compress the chunks with our lightweight fixed-length byte-aligned encoding. After CIC-PIM compression, two-fold larger graphs are processed with all data fit in memory, resulting in speedups or fast in-memory processing unattainable previously.},
  archive      = {J_JPDC},
  author       = {Yongxuan Zhang and Hong Jiang and Fang Wang and Yu Hua and Dan Feng and Yongli Cheng and Yuchong Hu and Renzhi Xiao},
  doi          = {10.1016/j.jpdc.2020.09.008},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {152-165},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {CIC-PIM: Trading spare computing power for memory space in graph processing},
  volume       = {147},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fast shared-memory streaming multilevel graph partitioning.
<em>JPDC</em>, <em>147</em>, 140–151. (<a
href="https://doi.org/10.1016/j.jpdc.2020.09.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A fast parallel graph partitioner can benefit many applications by reducing data transfers. The online methods for partitioning graphs have to be fast and they often rely on simple one-pass streaming algorithms, while the offline methods for partitioning graphs contain more involved algorithms and the most successful methods in this category belong to the multilevel approaches. In this work, we assess the feasibility of using streaming graph partitioning algorithms within the multilevel framework. Our end goal is to come up with a fast parallel offline multilevel partitioner that can produce competitive cutsize quality. We rely on a simple but fast and flexible streaming algorithm throughout the entire multilevel framework. This streaming algorithm serves multiple purposes in the partitioning process: a clustering algorithm in the coarsening, an effective algorithm for the initial partitioning, and a fast refinement algorithm in the uncoarsening. Its simple nature also lends itself easily for parallelization . The experiments on various graphs show that our approach is on the average up to 5.1x faster than the multi-threaded MeTiS, which comes at the expense of only 2x worse cutsize.},
  archive      = {J_JPDC},
  author       = {Nazanin Jafari and Oguz Selvitopi and Cevdet Aykanat},
  doi          = {10.1016/j.jpdc.2020.09.004},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {140-151},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Fast shared-memory streaming multilevel graph partitioning},
  volume       = {147},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Rule-based knowledge discovery of satellite imagery using
evolutionary classification tree. <em>JPDC</em>, <em>147</em>, 132–139.
(<a href="https://doi.org/10.1016/j.jpdc.2020.09.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The classification tree (CT) may be used to establish explicit classification rules for Satellite Imagery (SI). However, the accuracy of explicit classification rules attained by this method is poor. Back-propagation networks (BPN) and the support vector machine (SVM) may both be used to establish highly accurate models for predicting the classification of SI. However, neither is able to generate explicit rules. This study proposes the evolutionary classification tree (ECT) as a novel mining rule method. Composed of the particle bee algorithm (PBA) and classification tree (CT), the ECT produces self-organized rules automatically to predict the classification of SI. In ECT, CT serves as the architecture to represent explicit rules and PBA acts as the optimization mechanism to optimize CT in order to fit the experimental data. A total of 600 experimental datasets were used to compare the accuracy and complexity of four model-building techniques: CT, BPN , SVM , and ECT. The results demonstrate the ability of ECT to produce rules that are more accurate than CT and SVM but less accurate than BPN . However, because BPN is black box model, the ability of ECT to generate explicit rules makes ECT the best model for users wanting to mine the explicit rules and knowledge in practical applications.},
  archive      = {J_JPDC},
  author       = {Li-Chuan Lien and Unurjargal Dolgorsuren},
  doi          = {10.1016/j.jpdc.2020.09.003},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {132-139},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Rule-based knowledge discovery of satellite imagery using evolutionary classification tree},
  volume       = {147},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Reliability analysis of the augmented cubes in terms of the
extra edge-connectivity and the component edge-connectivity.
<em>JPDC</em>, <em>147</em>, 124–131. (<a
href="https://doi.org/10.1016/j.jpdc.2020.08.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reliability evaluation of interconnection networks is of significant importance to the design and maintenance of interconnection networks . The extra edge-connectivity and component edge-connectivity are two important parameters for the reliability evaluation of interconnection networks. In this paper, we determine the h h -extra edge-connectivity of an n n -dimensional augmented cube for h ≤ 2 [ n 2 ] h≤2[n2] , n ≥ 2 n≥2 and 2 n − 1 + 2 2 − f 3 ≤ h ≤ 2 n − 1 2n−1+22−f3≤h≤2n−1 where n ≥ 4 n≥4 , f = 0 f=0 when n n is even, and f = 1 f=1 when n n is odd. Moreover, we also determine the r r -component edge-connectivity of an n n -dimensional augmented cube for r ≤ 2 [ n 2 ] + 1 r≤2[n2]+1 , n ≥ 7 n≥7 .},
  archive      = {J_JPDC},
  author       = {Qifan Zhang and Liqiong Xu and Weihua Yang},
  doi          = {10.1016/j.jpdc.2020.08.009},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {124-131},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Reliability analysis of the augmented cubes in terms of the extra edge-connectivity and the component edge-connectivity},
  volume       = {147},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SEAPP: A secure application management framework based on
REST API access control in SDN-enabled cloud environment. <em>JPDC</em>,
<em>147</em>, 108–123. (<a
href="https://doi.org/10.1016/j.jpdc.2020.09.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud computing provides scalable network services and makes network management more flexible by combining Software-Defined Networking (SDN). Through the northbound interface (e.g., REST API) offered by the SDN controller, users can easily deploy diversified applications to access the network resources. However, exploiting the openness of the northbound interface, malicious applications abuse APIs to launch hostile attacks, which poses serious threats to the network. In this paper, we propose SEAPP, a secure application management framework based on REST API access control. Our main idea is to granularly manage application permissions and encrypt REST API calls to defend against malicious attacks . SEAPP includes two components: 1) permissions detection engine identifies the facticity of application permissions by analyzing permission manifests and byte codes and further identifies the legality of permissions with constructed sensitive API list; 2) registration authorization engine executes encrypted registration between applications and controller by virtue of NTRU algorithm and authorizes applications to call the requested REST APIs based on their risk levels after securely authenticating them. Besides, SEAPP is a lightweight logic architecture between application plane and control plane and supports quick deployment and reconfiguration in runtime. Both theoretical analysis and evaluation results show the security and effectiveness of SEAPP. Besides, SEAPP introduces negligible CPU and memory overheads.},
  archive      = {J_JPDC},
  author       = {Tao Hu and Zhen Zhang and Peng Yi and Dong Liang and Ziyong Li and Quan Ren and Yuxiang Hu and Julong Lan},
  doi          = {10.1016/j.jpdc.2020.09.006},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {108-123},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {SEAPP: A secure application management framework based on REST API access control in SDN-enabled cloud environment},
  volume       = {147},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Optimal node-disjoint paths in folded hypercubes.
<em>JPDC</em>, <em>147</em>, 100–107. (<a
href="https://doi.org/10.1016/j.jpdc.2020.09.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The constructions of node-disjoint paths have been well applied to the study of connectivity, diameter, parallel routing, reliability, and fault tolerance of an interconnection network. In order to minimize the transmission cost and latency, the total length and maximal length of the node-disjoint paths should be minimized, respectively. The construction of node-disjoint paths with their maximal length minimized (in the worst case) has been studied previously in folded hypercubes . In this paper, we construct m node-disjoint paths from one source node to other m (not necessarily distinct) target nodes, respectively, in an n -dimensional folded hypercube so that both of their total length and maximal length (in the worst case) are minimized, where m ≤ ≤ n +1. In addition, each path is either shortest or nearly shortest. The construction of these node-disjoint paths can be efficiently carried out in O ( mn 1.5 + m 3 n mn1.5+m3n ) and O ( mn 2 mn2 + n 2 n2 log n + m 3 m3 n ) time, respectively, for odd and even n by taking advantage of two specific routing functions, which provide another strong evidence for the effective applications of routing functions in deriving node-disjoint paths, especially for the variants of hypercubes .},
  archive      = {J_JPDC},
  author       = {Cheng-Nan Lai},
  doi          = {10.1016/j.jpdc.2020.09.005},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {100-107},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Optimal node-disjoint paths in folded hypercubes},
  volume       = {147},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Distributed approximate k-core decomposition and min–max
edge orientation: Breaking the diameter barrier. <em>JPDC</em>,
<em>147</em>, 87–99. (<a
href="https://doi.org/10.1016/j.jpdc.2020.08.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We design distributed algorithms to compute approximate solutions for several related graph optimization problems . All our algorithms have round complexity being logarithmic in the number of nodes of the underlying graph and in particular independent of the graph diameter. By using a primal–dual approach, we develop a 2 ( 1 + ϵ ) 2(1+ϵ) -approximation algorithm for computing the coreness values of the nodes in the underlying graph, as well as a 2 ( 1 + ϵ ) 2(1+ϵ) -approximation algorithm for the min–max edge orientation problem, where the goal is to orient the edges so as to minimize the maximum weighted in-degree. We provide lower bounds showing that the aforementioned algorithms are tight both in terms of the approximation guarantee and the round complexity. Additionally, motivated by the fact that the densest subset problem has an inherent dependency on the diameter of the graph, we study a weaker version that does not suffer from the same limitation. Finally, we conduct experiments on large real-world graphs to evaluate the effectiveness of our algorithms.},
  archive      = {J_JPDC},
  author       = {T.-H. Hubert Chan and Mauro Sozio and Bintao Sun},
  doi          = {10.1016/j.jpdc.2020.08.010},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {87-99},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Distributed approximate k-core decomposition and min–max edge orientation: Breaking the diameter barrier},
  volume       = {147},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). FPC-BI: Fast probabilistic consensus within byzantine
infrastructures. <em>JPDC</em>, <em>147</em>, 77–86. (<a
href="https://doi.org/10.1016/j.jpdc.2020.09.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a novel leaderless protocol (FPC-BI: Fast Probabilistic Consensus within Byzantine Infrastructures) with a low communicational complexity and which allows a set of nodes to come to a consensus on a value of a single bit. The paper makes the assumption that part of the nodes are Byzantine , and are thus controlled by an adversary who intends to either delay the consensus, or break it (this defines that at least a couple of honest nodes come to different conclusions). We prove that, nevertheless, the protocol works with high probability when its parameters are suitably chosen. Along this the paper also provides explicit estimates on the probability that the protocol finalizes in the consensus state in a given time. This protocol could be applied to reaching consensus in decentralized cryptocurrency systems. A special feature of it is that it makes use of a sequence of random numbers which are either provided by a trusted source or generated by the nodes themselves using some decentralized random number generating protocol. This increases the overall trustworthiness of the infrastructure. A core contribution of the paper is that it uses a very weak consensus to obtain a strong consensus on the value of a bit, and which can relate to the validity of a transaction.},
  archive      = {J_JPDC},
  author       = {Serguei Popov and William J. Buchanan},
  doi          = {10.1016/j.jpdc.2020.09.002},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {77-86},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {FPC-BI: Fast probabilistic consensus within byzantine infrastructures},
  volume       = {147},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DLHub: Simplifying publication, discovery, and use of
machine learning models in science. <em>JPDC</em>, <em>147</em>, 64–76.
(<a href="https://doi.org/10.1016/j.jpdc.2020.08.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine Learning (ML) has become a critical tool enabling new methods of analysis and driving deeper understanding of phenomena across scientific disciplines. There is a growing need for “learning systems” to support various phases in the ML lifecycle. While others have focused on supporting model development, training, and inference, few have focused on the unique challenges inherent in science, such as the need to publish and share models and to serve them on a range of available computing resources. In this paper, we present the Data and Learning Hub for science (DLHub), a learning system designed to support these use cases. Specifically, DLHub enables publication of models, with descriptive metadata , persistent identifiers, and flexible access control. It packages arbitrary models into portable servable containers, and enables low-latency, distributed serving of these models on heterogeneous compute resources. We show that DLHub supports low-latency model inference comparable to other model serving systems including TensorFlow Serving, SageMaker, and Clipper, and improved performance, by up to 95\%, with batching and memoization enabled. We also show that DLHub can scale to concurrently serve models on 500 containers. Finally, we describe five case studies that highlight the use of DLHub for scientific applications.},
  archive      = {J_JPDC},
  author       = {Zhuozhao Li and Ryan Chard and Logan Ward and Kyle Chard and Tyler J. Skluzacek and Yadu Babuji and Anna Woodard and Steven Tuecke and Ben Blaiszik and Michael J. Franklin and Ian Foster},
  doi          = {10.1016/j.jpdc.2020.08.006},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {64-76},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {DLHub: Simplifying publication, discovery, and use of machine learning models in science},
  volume       = {147},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Towards an efficient combination of adaptive routing and
queuing schemes in fat-tree topologies. <em>JPDC</em>, <em>147</em>,
46–63. (<a href="https://doi.org/10.1016/j.jpdc.2020.07.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The interconnection network is a key element in High-Performance Computing (HPC) and Datacenter (DC) systems whose performance depends on several design parameters, such as the topology, the switch architecture, and the routing algorithm . Among the most common topologies in HPC systems, the Fat-Tree offers several shortest-path routes between any pair of end-nodes, which allows multi-path routing schemes to balance traffic flows among the available links, thus reducing congestion probability. However, traffic balance cannot solve by itself some congestion situations that may still degrade network performance. Another approach to reduce congestion is queue-based flow separation, but our previous work shows that multi-path routing may spread congested flows across several queues, thus being counterproductive. In this paper, we propose a set of restrictions to improve alternative routes selection for multi-path routing algorithms in Fat-Tree networks, so that they can be positively combined with queuing schemes.},
  archive      = {J_JPDC},
  author       = {Jose Rocher-Gonzalez and Jesus Escudero-Sahuquillo and Pedro J. García and Francisco J. Quiles and Gaspar Mora},
  doi          = {10.1016/j.jpdc.2020.07.009},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {46-63},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Towards an efficient combination of adaptive routing and queuing schemes in fat-tree topologies},
  volume       = {147},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Achieving efficient and privacy-preserving energy trading
based on blockchain and ABE in smart grid. <em>JPDC</em>, <em>147</em>,
34–45. (<a href="https://doi.org/10.1016/j.jpdc.2020.08.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advent of the Industry 4.0 era, the development of smart cities based on the Internet of Things (IoT) has reached a new level. As a key component of the Internet of Things (IoT), the security of wireless sensor networks (WSN) has received widespread attention. Among them, Energy Internet, as an important part to support the construction of smart cities, its security and reliability research is becoming more and more important. In the Energy Internet, distributed energy transaction model is a promising approach to replace the traditional centralized transaction model and has become the leading direction of development in energy trading . As the underlying support, blockchain technology is attracting more and more attention due to its advantages, i.e., integrity and non-repudiation. However, most blockchain-based trading models face the problem of privacy disclosure . In this paper, to solve this problem, Ciphertext-Policy Attribute-Based Encryption (CP-ABE) is introduced as the core algorithm to reconstruct the transaction model. Specifically, we build a general model for distributed transactions called PP-BCETS (Privacy-preserving Blockchain Energy Trading Scheme). It can achieve fine-grained access control through transaction arbitration in the ciphertext form. This design can maximize the protection of privacy information, and considerably improve the security and reliability of the transaction model. Additionally, a credibility-based equity proof consensus mechanism is proposed in PP-BCETS, which can greatly increase the operation efficiency. The security analysis and experimental evaluations are conducted to prove the validity and practicability of our proposed scheme.},
  archive      = {J_JPDC},
  author       = {Zhitao Guan and Xin Lu and Wenti Yang and Longfei Wu and Naiyu Wang and Zijian Zhang},
  doi          = {10.1016/j.jpdc.2020.08.012},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {34-45},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Achieving efficient and privacy-preserving energy trading based on blockchain and ABE in smart grid},
  volume       = {147},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Leveraging InfiniBand controller to configure deadlock-free
routing engines for dragonflies. <em>JPDC</em>, <em>147</em>, 16–33. (<a
href="https://doi.org/10.1016/j.jpdc.2020.07.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Dragonfly topology is currently one of the most popular network topologies in high-performance parallel systems . The interconnection networks of many of these systems are built from components based on the InfiniBand specification. However, due to some constraints in this specification, the available versions of the InfiniBand network controller (OpenSM) do not include routing engines based on some popular deadlock-free routing algorithms proposed theoretically for Dragonflies, such as the one proposed by Kim and Dally based on Virtual-Channel shifting. In this paper we propose a straightforward method to integrate this routing algorithm in OpenSM as a routing engine, explaining in detail the configuration required to support it. We also provide experiment results, obtained both from a real InfiniBand-based cluster and from simulation, to validate the new routing engine and to compare its performance and requirements against other routing engines currently available in OpenSM.},
  archive      = {J_JPDC},
  author       = {German Maglione-Mathey and Jesus Escudero-Sahuquillo and Pedro Javier Garcia and Francisco J. Quiles and Eitan Zahavi},
  doi          = {10.1016/j.jpdc.2020.07.010},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {16-33},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Leveraging InfiniBand controller to configure deadlock-free routing engines for dragonflies},
  volume       = {147},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Empowering mobile crowdsourcing apps with user privacy
control. <em>JPDC</em>, <em>147</em>, 1–15. (<a
href="https://doi.org/10.1016/j.jpdc.2020.07.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile crowdsourcing is being increasingly used by industrial and research communities to build realistic datasets. By leveraging the capabilities of mobile devices , mobile crowdsourcing apps can be used to track participants’ activity and to collect insightful reports from the environment ( e.g. , air quality, network quality). However, most of existing crowdsourced datasets systematically tag data samples with metadata ( e.g. , time and location stamps), which may inevitably lead to user privacy leaks by discarding sensitive information in the wild. This article addresses this critical limitation of the state of the art by proposing a software library that empowers legacy mobile crowdsourcing apps to increase user privacy without compromising the overall quality of the crowdsourced datasets. We propose a decentralized approach, named Fougere , to convey data samples from user devices to third-party servers. By introducing an a priori data anonymization process, we show that Fougere defeats state-of-the-art location-based privacy attacks with little impact on the quality of crowdsourced datasets.},
  archive      = {J_JPDC},
  author       = {Lakhdar Meftah and Romain Rouvoy and Isabelle Chrisment},
  doi          = {10.1016/j.jpdc.2020.07.011},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {1-15},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Empowering mobile crowdsourcing apps with user privacy control},
  volume       = {147},
  year         = {2021},
}
</textarea>
</details></li>
</ul>

</body>
</html>
