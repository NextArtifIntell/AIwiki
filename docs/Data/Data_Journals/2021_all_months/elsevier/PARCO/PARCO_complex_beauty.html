<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>PARCO_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="parco---55">PARCO - 55</h2>
<ul>
<li><details>
<summary>
(2021). On revisiting energy and performance in microservices
applications: A cloud elasticity-driven approach. <em>PARCO</em>,
<em>108</em>, 102858. (<a
href="https://doi.org/10.1016/j.parco.2021.102858">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Monolithic applications are a subject that includes several knowledge areas. Sometimes it can be a challenge to optimize CPU or IO requirements because it is not trivial to recognize the problem itself and improve it. There are many approaches to resolve this situation, where a trending one is the microservices . As a variant of the service-oriented architecture, microservices is a technique that arranges an application as a collection of loosely coupled services. This decomposition enables better software management in cloud-based environments since we can replicate each part individually using cloud elasticity to avoid execution bottlenecks. Also, since elasticity mitigates resource overprovisioning, it favors better energy consumption: the cloud owner can redistribute finite available resources among different tenants, and users can pay less to use the infrastructure. However, elasticity tuning is not trivial and depends on several factors, such as user experience , application architecture, and parameter modeling. Today, we observe a lack of initiatives in the literature that address both performance and energy perspectives to support the execution of microservices applications in the cloud. Concerning this context, this article introduces Elergy as a lightweight proactive elasticity model that provides resource reorganization for a cloud-based microservices application. Its differential approach appears in improving energy consumption by periodically handling the most appropriate amount of resources to execute an application while maintaining or yet improving the performance of CPU-bound applications. Elergy performs these functions proactively, in such a way of preventing future problems related to either resource under- or overprovisioning. The results showed energy consumption reduction and a competitive cost (application time x consumed resources) when comparing Elergy with a non-elastic scenario. Elergy obtained savings from 1.93\% to 27.92\% for energy consumption.},
  archive      = {J_PARCO},
  author       = {Igor Fontana de Nardin and Rodrigo da Rosa Righi and Thiago Roberto Lima Lopes and Cristiano André da Costa and Heon Young Yeom and Harald Köstler},
  doi          = {10.1016/j.parco.2021.102858},
  journal      = {Parallel Computing},
  pages        = {102858},
  shortjournal = {Parallel Comput.},
  title        = {On revisiting energy and performance in microservices applications: A cloud elasticity-driven approach},
  volume       = {108},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). AIOC2: A deep q-learning approach to autonomic i/o
congestion control in lustre. <em>PARCO</em>, <em>108</em>, 102855. (<a
href="https://doi.org/10.1016/j.parco.2021.102855">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In high performance computing systems, I/O congestion is a common problem in large-scale distributed file systems . However, the current implementation mainly requires administrator to manually design low-level implementation and optimization, we proposes an adaptive I/O congestion control framework, named AIOC 2 2 , which can not only adaptively tune the I/O congestion control parameters, but also exploit the deep Q-learning method to start the training parameters and optimize the tuning for different types of workloads from the server and the client at the same time. AIOC 2 2 combines the feedback-based dynamic I/O congestion control and deep Q-learning parameter tuning technology to achieve autonomic I/O congestion control, improve system I/O throughput, and thus reduce I/O latency without human interference. Experimental results show that AIOC 2 2 can greatly reduce the impact of I/O congestion on I/O throughput and I/O latency performance in Lustre clusters. Compared to existing Lustre cluster systems, AIOC 2 2 can increase write I/O throughput by 34.82\% and decrease I/O latency by 26.17\% on average.},
  archive      = {J_PARCO},
  author       = {Wen Cheng and Shijun Deng and Lingfang Zeng and Yang Wang and André Brinkmann},
  doi          = {10.1016/j.parco.2021.102855},
  journal      = {Parallel Computing},
  pages        = {102855},
  shortjournal = {Parallel Comput.},
  title        = {AIOC2: A deep Q-learning approach to autonomic I/O congestion control in lustre},
  volume       = {108},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An international survey on MPI users. <em>PARCO</em>,
<em>108</em>, 102853. (<a
href="https://doi.org/10.1016/j.parco.2021.102853">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Message Passing Interface (MPI) plays a crucial part in the parallel computing ecosystem, a driving force behind many of the high-performance computing (HPC) successes. To maintain its relevance to the user community—and in particular to the growing HPC community at large—the MPI standard needs to identify and understand the MPI users’ concerns and expectations, and adapt accordingly to continue to efficiently bridge the gap between users and hardware. This questionnaire survey was conducted using two online questionnaire frameworks and has gathered more than 850 answers from 42 countries since February 2019. Some of preceding surveys of MPI uses are questionnaire surveys like ours, while others are conducted either by analyzing MPI programs to reveal static behavior or by using profiling tools to analyze the dynamic runtime behavior of MPI jobs. Our survey is different from other questionnaire surveys in terms of its larger number of participants and wide geographic spread. As a result, it is possible to illustrate the current status of MPI users more accurately and with a wider geographical distribution. In this report, we will show some interesting findings, compare the results with preceding studies when possible, and provide some recommendations for MPI Forum based on the findings.},
  archive      = {J_PARCO},
  author       = {Atsushi Hori and Emmanuel Jeannot and George Bosilca and Takahiro Ogura and Balazs Gerofi and Jie Yin and Yutaka Ishikawa},
  doi          = {10.1016/j.parco.2021.102853},
  journal      = {Parallel Computing},
  pages        = {102853},
  shortjournal = {Parallel Comput.},
  title        = {An international survey on MPI users},
  volume       = {108},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Tree cutting approach for domain partitioning on
forest-of-octrees-based block-structured static adaptive mesh refinement
with lattice boltzmann method. <em>PARCO</em>, <em>108</em>, 102851. (<a
href="https://doi.org/10.1016/j.parco.2021.102851">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The aerodynamics simulation code based on the lattice Boltzmann method (LBM) using forest-of-octrees-based block-structured adaptive mesh refinement (AMR) with temporary-fixed refinement was implemented, and its performance was evaluated on GPU-based supercomputers . Although the Space-Filling-Curve-based (SFC) domain partitioning algorithm for the octree-based AMR has been widely used on conventional CPU-based supercomputers, accelerated computation on GPU-based supercomputers revealed a bottleneck due to costly halo data communication. Our new tree cutting approach adopts a hybrid domain partitioning with the coarse structured block decomposition and the SFC partitioning in each block. This hybrid approach improved the locality and the topology of the partitioned sub-domains and reduced the amount of the halo communication to one-third of the original SFC approach. In the strong scaling test, the code achieved maximum × 1 . 82 ×1.82 speedup at the performance of 2207 MLUPS (mega-lattice update per second) on 128 GPUs (NVIDIA® Tesla® V100). In the weak scaling test, the code achieved 9620 MLUPS at 128 GPUs with 4.473 billion grid points, while keeping the parallel efficiency of 93.4\% from 8 to 128 GPUs.},
  archive      = {J_PARCO},
  author       = {Yuta Hasegawa and Takayuki Aoki and Hiromichi Kobayashi and Yasuhiro Idomura and Naoyuki Onodera},
  doi          = {10.1016/j.parco.2021.102851},
  journal      = {Parallel Computing},
  pages        = {102851},
  shortjournal = {Parallel Comput.},
  title        = {Tree cutting approach for domain partitioning on forest-of-octrees-based block-structured static adaptive mesh refinement with lattice boltzmann method},
  volume       = {108},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). GPU algorithms for efficient exascale discretizations.
<em>PARCO</em>, <em>108</em>, 102841. (<a
href="https://doi.org/10.1016/j.parco.2021.102841">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we describe the research and development activities in the Center for Efficient Exascale Discretization within the US Exascale Computing Project, targeting state-of-the-art high-order finite-element algorithms for high-order applications on GPU-accelerated platforms. We discuss the GPU developments in several components of the CEED software stack, including the libCEED, MAGMA, MFEM, libParanumal, and Nek projects. We report performance and capability improvements in several CEED-enabled applications on both NVIDIA and AMD GPU systems.},
  archive      = {J_PARCO},
  author       = {Ahmad Abdelfattah and Valeria Barra and Natalie Beams and Ryan Bleile and Jed Brown and Jean-Sylvain Camier and Robert Carson and Noel Chalmers and Veselin Dobrev and Yohann Dudouit and Paul Fischer and Ali Karakus and Stefan Kerkemeier and Tzanio Kolev and Yu-Hsiang Lan and Elia Merzari and Misun Min and Malachi Phillips and Thilina Rathnayake and Robert Rieben and Thomas Stitt and Ananias Tomboulides and Stanimire Tomov and Vladimir Tomov and Arturo Vargas and Tim Warburton and Kenneth Weiss},
  doi          = {10.1016/j.parco.2021.102841},
  journal      = {Parallel Computing},
  pages        = {102841},
  shortjournal = {Parallel Comput.},
  title        = {GPU algorithms for efficient exascale discretizations},
  volume       = {108},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Porting hypre to heterogeneous computer architectures:
Strategies and experiences. <em>PARCO</em>, <em>108</em>, 102840. (<a
href="https://doi.org/10.1016/j.parco.2021.102840">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Linear systems are occurring in many applications, and solving them can take a large amount of the total simulation time. The high performance library hypre provides a variety of interfaces and linear solvers, including various multigrid methods, that have achieved good scalability on a variety of homogeneous parallel computer architectures . Heterogeneous architectures with nodes that have both CPUs and accelerators provide new challenges, since they require more fine-grained parallelism and reduced data movement between different memories on a single node as well as across nodes. We will discuss our experiences and strategies to port hypre to heterogeneous computers with accelerators, including the design of a new memory model, the use of abstractions, the BoxLoop macros in the structured and semi-structured interfaces, and the restructuring of algebraic multigrid (AMG) into modular components. We present numerical experiments comparing CPU and GPU performance for several test problems.},
  archive      = {J_PARCO},
  author       = {Robert D. Falgout and Ruipeng Li and Björn Sjögreen and Lu Wang and Ulrike Meier Yang},
  doi          = {10.1016/j.parco.2021.102840},
  journal      = {Parallel Computing},
  pages        = {102840},
  shortjournal = {Parallel Comput.},
  title        = {Porting hypre to heterogeneous computer architectures: Strategies and experiences},
  volume       = {108},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Design-time performance modeling of compositional parallel
programs. <em>PARCO</em>, <em>108</em>, 102839. (<a
href="https://doi.org/10.1016/j.parco.2021.102839">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Performance models are powerful instruments for understanding the performance of parallel systems and uncovering their bottlenecks. Already during system design, performance models can help ponder alternative development options. However, creating a performance model – whether theoretically or empirically – for an entire application that does not exist yet is challenging. In this paper, we propose to generate performance models of full programs from performance models of their components using formal composition operators derived from parallel design patterns . As long as the design of the overall system follows such a pattern, its performance model can be predicted with reasonable accuracy without an actual implementation. We demonstrate our approach with design patterns of varying complexity, including pipeline, task pool, and eventually MapReduce , which is representative of a broad class of data-analytics applications.},
  archive      = {J_PARCO},
  author       = {Fabian Czappa and Alexandru Calotoiu and Thomas Höhl and Heiko Mantel and Toni Nguyen and Felix Wolf},
  doi          = {10.1016/j.parco.2021.102839},
  journal      = {Parallel Computing},
  pages        = {102839},
  shortjournal = {Parallel Comput.},
  title        = {Design-time performance modeling of compositional parallel programs},
  volume       = {108},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Measurement and analysis of GPU-accelerated applications
with HPCToolkit. <em>PARCO</em>, <em>108</em>, 102837. (<a
href="https://doi.org/10.1016/j.parco.2021.102837">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To address the challenge of performance analysis on the US DOE’s forthcoming exascale supercomputers , Rice University has been extending its HPCToolkit performance tools to support measurement and analysis of GPU-accelerated applications. To help developers understand the performance of accelerated applications as a whole, HPCToolkit’s measurement and analysis tools attribute metrics to calling contexts that span both CPUs and GPUs. To measure GPU-accelerated applications efficiently, HPCToolkit employs a novel wait-free data structure to coordinate monitoring and attribution of GPU performance. To help developers understand the performance of complex GPU code generated from high-level programming models, HPCToolkit constructs sophisticated approximations of call path profiles for GPU computations. To support fine-grained analysis and tuning, HPCToolkit uses PC sampling and instrumentation to measure and attribute GPU performance metrics to source lines, loops, and inlined code. To supplement fine-grained measurements, HPCToolkit can measure GPU kernel executions using hardware performance counters. To provide a view of how an execution evolves over time, HPCToolkit can collect, analyze, and visualize call path traces within and across nodes. Finally, on NVIDIA GPUs, HPCToolkit can derive and attribute a collection of useful performance metrics based on measurements using GPU PC samples. We illustrate HPCToolkit’s new capabilities for analyzing GPU-accelerated applications with several codes developed as part of the Exascale Computing Project.},
  archive      = {J_PARCO},
  author       = {Keren Zhou and Laksono Adhianto and Jonathon Anderson and Aaron Cherian and Dejan Grubisic and Mark Krentel and Yumeng Liu and Xiaozhu Meng and John Mellor-Crummey},
  doi          = {10.1016/j.parco.2021.102837},
  journal      = {Parallel Computing},
  pages        = {102837},
  shortjournal = {Parallel Comput.},
  title        = {Measurement and analysis of GPU-accelerated applications with HPCToolkit},
  volume       = {108},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Enabling GPU accelerated computing in the SUNDIALS time
integration library. <em>PARCO</em>, <em>108</em>, 102836. (<a
href="https://doi.org/10.1016/j.parco.2021.102836">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As part of the Exascale Computing Project (ECP), a recent focus of development efforts for the SUite of Nonlinear and DIfferential/ALgebraic equation Solvers (SUNDIALS) has been to enable GPU-accelerated time integration in scientific applications at extreme scales. This effort has resulted in several new GPU-enabled implementations of core SUNDIALS data structures , support for programming paradigms which are aware of the heterogeneous architectures , and the introduction of utilities to provide new points of flexibility. In this paper, we discuss our considerations, both internal and external, when designing these new features and present the features themselves. We also present performance results for several of the features on the Summit supercomputer and early access hardware for the Frontier supercomputer, which demonstrate negligible performance overhead resulting from the additional infrastructure and significant speedups when using both NVIDIA and AMD GPUs.},
  archive      = {J_PARCO},
  author       = {Cody J. Balos and David J. Gardner and Carol S. Woodward and Daniel R. Reynolds},
  doi          = {10.1016/j.parco.2021.102836},
  journal      = {Parallel Computing},
  pages        = {102836},
  shortjournal = {Parallel Comput.},
  title        = {Enabling GPU accelerated computing in the SUNDIALS time integration library},
  volume       = {108},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Minimizing development costs for efficient many-core
visualization using MCD3. <em>PARCO</em>, <em>108</em>, 102834. (<a
href="https://doi.org/10.1016/j.parco.2021.102834">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scientific visualization software increasingly needs to support many-core architectures. However, development time is a significant challenge due to the breadth and diversity of both visualization algorithms and architectures. With this work, we introduce a development environment for visualization algorithms on many-core devices that extends the traditional data-parallel primitive (DPP) approach with several existing constructs and an important new construct: meta-DPPs. We refer to our approach as MCD 3 — M eta-DPPs, C onvenience routines, D ata management, D PPs, and D evices. The twin goals of MCD 3 are to reduce developer time and to deliver efficient performance on many-core architectures, and our evaluation considers both of these goals. For development time, we study 57 algorithms implemented in the VTK-m software library and determine that MCD 3 leads to significant savings. For efficient performance, we survey ten studies looking at individual algorithms and determine that the MCD 3 hardware-agnostic approach leads to performance comparable to hardware-specific approaches: sometimes better, sometimes worse, and better in the aggregate. In total, we find that MCD 3 is an effective approach for scientific visualization libraries to support many-core architectures.},
  archive      = {J_PARCO},
  author       = {Kenneth Moreland and Robert Maynard and David Pugmire and Abhishek Yenpure and Allison Vacanti and Matthew Larsen and Hank Childs},
  doi          = {10.1016/j.parco.2021.102834},
  journal      = {Parallel Computing},
  pages        = {102834},
  shortjournal = {Parallel Comput.},
  title        = {Minimizing development costs for efficient many-core visualization using MCD3},
  volume       = {108},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Porting WarpX to GPU-accelerated platforms. <em>PARCO</em>,
<em>108</em>, 102833. (<a
href="https://doi.org/10.1016/j.parco.2021.102833">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {WarpX is a general purpose electromagnetic particle-in-cell code that was originally designed to run on many-core CPU architectures. We describe the strategy, based on the AMReX library, followed to allow WarpX to use the GPU-accelerated nodes on OLCF’s Summit supercomputer , a strategy we believe will extend to the upcoming machines Frontier and Aurora. We summarize the challenges encountered, lessons learned, and give current performance results on a series of relevant benchmark problems.},
  archive      = {J_PARCO},
  author       = {A. Myers and A. Almgren and L.D. Amorim and J. Bell and L. Fedeli and L. Ge and K. Gott and D.P. Grote and M. Hogan and A. Huebl and R. Jambunathan and R. Lehe and C. Ng and M. Rowan and O. Shapoval and M. Thévenet and J.-L. Vay and H. Vincenti and E. Yang and N. Zaïm and W. Zhang and Y. Zhao and E. Zoni},
  doi          = {10.1016/j.parco.2021.102833},
  journal      = {Parallel Computing},
  pages        = {102833},
  shortjournal = {Parallel Comput.},
  title        = {Porting WarpX to GPU-accelerated platforms},
  volume       = {108},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Immortal rays: Rethinking random ray neutron transport on
GPU architectures. <em>PARCO</em>, <em>108</em>, 102832. (<a
href="https://doi.org/10.1016/j.parco.2021.102832">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Random Ray Method (TRRM) is a recently developed adaptation of the Method of Characteristics for neutral particle transport simulations. TRRM has demonstrated excellent performance on 3D nuclear reactor benchmark problems using CPU-based compute systems. When porting to GPU-based systems, however, new performance challenges arise that are unique to processors targeting massive fine-grained parallelism . For smaller problems, or for large problems that are domain decomposed across many computational nodes , the problem size per node has insufficient parallelism to saturate GPU node resources, thus greatly limiting speedup. In this study, we report on a newly developed “immortal ray” variant of TRRM. The immortal ray technique exposes significantly more fine-grained parallelism by fundamentally reformulating the numerical details of ray discretization , resulting in performance tradeoffs with significant overall benefit on GPUs. For very small 2D simulation problems we found the new immortal ray variant allowed for up to a 4.4x speedup when run on a single GPU . For larger 3D simulation problems we found the new variant improved strong scaling by 3x when run on the Summit supercomputer .},
  archive      = {J_PARCO},
  author       = {John R. Tramm and Andrew R. Siegel},
  doi          = {10.1016/j.parco.2021.102832},
  journal      = {Parallel Computing},
  pages        = {102832},
  shortjournal = {Parallel Comput.},
  title        = {Immortal rays: Rethinking random ray neutron transport on GPU architectures},
  volume       = {108},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Toward performance-portable PETSc for GPU-based exascale
systems. <em>PARCO</em>, <em>108</em>, 102831. (<a
href="https://doi.org/10.1016/j.parco.2021.102831">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Portable Extensible Toolkit for Scientific computation (PETSc) library delivers scalable solvers for nonlinear time-dependent differential and algebraic equations and for numerical optimization . The PETSc design for performance portability addresses fundamental GPU accelerator challenges and stresses flexibility and extensibility by separating the programming model used by the application from that used by the library, and it enables application developers to use their preferred programming model, such as Kokkos, RAJA, SYCL, HIP , CUDA, or OpenCL , on upcoming exascale systems . A blueprint for using GPUs from PETSc-based codes is provided, and case studies emphasize the flexibility and high performance achieved on current GPU-based systems.},
  archive      = {J_PARCO},
  author       = {Richard Tran Mills and Mark F. Adams and Satish Balay and Jed Brown and Alp Dener and Matthew Knepley and Scott E. Kruger and Hannah Morgan and Todd Munson and Karl Rupp and Barry F. Smith and Stefano Zampini and Hong Zhang and Junchao Zhang},
  doi          = {10.1016/j.parco.2021.102831},
  journal      = {Parallel Computing},
  pages        = {102831},
  shortjournal = {Parallel Comput.},
  title        = {Toward performance-portable PETSc for GPU-based exascale systems},
  volume       = {108},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Towards performance portability in the spark astrophysical
magnetohydrodynamics solver in the flash-x simulation framework.
<em>PARCO</em>, <em>108</em>, 102830. (<a
href="https://doi.org/10.1016/j.parco.2021.102830">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Simulations of core-collapse supernovae, and other astrophysical phenomena, are quintessential extreme-scale computing challenges. For core-collapse supernova simulations to be carried out by the ExaStar project under the Exascale Computing Project umbrella, a robust, efficient, and state-of-the-art magnetohydrodynamics solver is a critical requirement. In Flash-X , the primary software instrument for ExaStar, a new magnetohydrodynamics solver has been designed and implemented from the ground up to achieve accuracy and efficiency for simulations of complex astrophysical flows. This new solver, dubbed Spark , uses high-order spatial reconstruction, Runge–Kutta time integration, and an efficient cell-centered approach to satisfying the divergence-free condition for the magnetic fields. Spark was written to be optimized for data locality in cache hierarchy of CPUs. Since data locality optimizations for cache hierarchy are not directly compatible with those of accelerators, we have taken the approach of using program synthesis to avoid massive amounts of code replication that would be necessary if we were to maintain two different versions of the solver. Our program synthesis relies on a simple key-dictionary approach, implemented in python, that enables us to assemble the version of the solver suitable for the target hardware from code fragments identified by specific keys. In this paper, we describe the data locality optimizations of the solver for CPUs and accelerators and the program synthesis tools that enable this portability. We also detail the parallel performance of Spark for both CPUs and accelerators.},
  archive      = {J_PARCO},
  author       = {Sean M. Couch and Jared Carlson and Michael Pajkos and Brian W. O’Shea and Anshu Dubey and Tom Klosterman},
  doi          = {10.1016/j.parco.2021.102830},
  journal      = {Parallel Computing},
  pages        = {102830},
  shortjournal = {Parallel Comput.},
  title        = {Towards performance portability in the spark astrophysical magnetohydrodynamics solver in the flash-X simulation framework},
  volume       = {108},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Achieving performance portability in gaussian basis set
density functional theory on accelerator based architectures in
NWChemEx. <em>PARCO</em>, <em>108</em>, 102829. (<a
href="https://doi.org/10.1016/j.parco.2021.102829">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The numerical integration of the exchange–correlation (XC) potential is one of the primary computational bottlenecks in Gaussian basis set Kohn–Sham density functional theory (KS-DFT). To achieve optimal performance and accuracy, care must be taken in this numerical integration to preserve local sparsity as to allow for near linear weak scaling with system size. This leads to an integration scheme with several performance critical kernels which must be hand optimized for each architecture of interest. As the set of available accelerator hardware goes more diverse, a key challenge for developers of KS-DFT software is to maintain performance portability across a wide range of computational architectures. In this work, we examine a modular software design pattern which decouples the implementation details of performance critical kernels from the expression of high-level algorithmic workflows in a device-agnostic language such as C++; thus allowing for developers to target existing and emerging accelerator hardware within a single code base. We consider the efficacy of such a design pattern in the numerical integration of the XC potential by demonstrating its ability to achieve performance portability across a set of accelerator architectures which are representative of those on current and future U.S. Department of Energy Leadership Computing Facilities.},
  archive      = {J_PARCO},
  author       = {David B. Williams-Young and Abhishek Bagusetty and Wibe A. de Jong and Douglas Doerfler and Hubertus J.J. van Dam and Álvaro Vázquez-Mayagoitia and Theresa L. Windus and Chao Yang},
  doi          = {10.1016/j.parco.2021.102829},
  journal      = {Parallel Computing},
  pages        = {102829},
  shortjournal = {Parallel Comput.},
  title        = {Achieving performance portability in gaussian basis set density functional theory on accelerator based architectures in NWChemEx},
  volume       = {108},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A novel hybrid heuristic-based list scheduling algorithm in
heterogeneous cloud computing environment for makespan optimization.
<em>PARCO</em>, <em>108</em>, 102828. (<a
href="https://doi.org/10.1016/j.parco.2021.102828">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An efficient workflow scheduling can potentially exploit heterogeneity of resources in heterogeneous cloud computing ( HCC ) platform commensurate with variable requirement of dependent tasks in a given workflow. Minimizing the total scheduling length, m a k e s p a n makespan , is essential for application performance in heterogeneous computing systems especially in cloud computing environment. The problem of scheduling a set of different dependent tasks onto a set of heterogeneous computational resources is a well-known NP-Hard problem. Therefore, no polynomial scheduling algorithm for computing the optimal solution exists. For approximating a solution to this problem many algorithms have been proposed, but majority of them have low efficiency. In this paper, a novel hybrid heuristic-based list scheduling ( HH-LiSch ) algorithm is presented for solving the dependent task scheduling in HCC systems in a bounded number of the fully connected virtual machines (VMs). The novelty of the current paper is to present the new task priority strategy, find appropriate VM&#39;s slot time, and utilize task duplication technique. Two novel task priority strategies are applied to prioritize tasks in an efficient ordered list. Then, during the scheduling process an insertion-based procedure is called to find an appropriate potential slot time for performing task duplication technique. If it works, the task duplication is added to rudimentary scheduling scheme. In this way, the final scheduling is gradually generated. To validate the work, the experiments are based on six real-world scientific workflows and a random task graph ( RTG ); then, the performance is evaluated in terms of makespan , Schedule Length Ratio ( SLR ), speedup and efficiency . The simulation results prove a significant improvement against other counterparts in literature.},
  archive      = {J_PARCO},
  author       = {Mirsaeid Hosseini Shirvani and Reza Noorian Talouki},
  doi          = {10.1016/j.parco.2021.102828},
  journal      = {Parallel Computing},
  pages        = {102828},
  shortjournal = {Parallel Comput.},
  title        = {A novel hybrid heuristic-based list scheduling algorithm in heterogeneous cloud computing environment for makespan optimization},
  volume       = {108},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Implementation and evaluation of MPI 4.0 partitioned
communication libraries. <em>PARCO</em>, <em>108</em>, 102827. (<a
href="https://doi.org/10.1016/j.parco.2021.102827">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Partitioned point-to-point communication primitives provide a performance-oriented mechanism to support a hybrid parallel programming model and have been included in the upcoming MPI-4.0 standard. These primitives enable an MPI library to transfer parts of the data buffer while the application provides partial contributions using multiple threads or tasks or simply pipelines the buffers sequentially. The focus of this paper is the design and implementation of a layered library that provides the functionality of these newer APIs and supports application development using these newer APIs. This library provides an opportunity to explore potential optimizations and identify further enhancements to the APIs. Initial experience in designing this library along with preliminary performance results are presented. In addition, the library is compared to initial prototype libraries that have recently become available that have been updated to the standard-compliant interface. These prototype libraries were built on remote-memory-access (RMA) primitives, offering insight into different implementation strategies. In general, we observe an interesting trade-off space, with the RMA-based implementation proving more performant for send-side partitioning, with increases in perceived bandwidth 8.9x on average over a single send, compared to the persistent-based implementation, which shows improvements 4.0x on average. In comparing the two implementations, we find that the persistent-based implementation enables more overlap for receive-side partitioning up to 5.37X the RMA library’s overlap, while the RMA-based implementation provides better send-side performance of up to 70\%.},
  archive      = {J_PARCO},
  author       = {Matthew G.F. Dosanjh and Andrew Worley and Derek Schafer and Prema Soundararajan and Sheikh Ghafoor and Anthony Skjellum and Purushotham V. Bangalore and Ryan E. Grant},
  doi          = {10.1016/j.parco.2021.102827},
  journal      = {Parallel Computing},
  pages        = {102827},
  shortjournal = {Parallel Comput.},
  title        = {Implementation and evaluation of MPI 4.0 partitioned communication libraries},
  volume       = {108},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Evaluating MPI resource usage summary statistics.
<em>PARCO</em>, <em>108</em>, 102825. (<a
href="https://doi.org/10.1016/j.parco.2021.102825">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Message Passing Interface (MPI) remains the dominant programming model for scientific applications running on today’s high-performance computing (HPC) systems. This dominance stems from MPI’s powerful semantics for inter-process communication that has enabled scientists to write applications for simulating important physical phenomena. MPI does not, however, specify how messages and synchronization should be carried out. Those details are typically dependent on low-level architecture details and the message characteristics of the application. Therefore, analyzing an application’s MPI resource usage is critical to tuning MPI’s performance on a particular platform. The result of this analysis is typically a discussion of the mean message sizes, queue search lengths and message arrival times for a workload or set of workloads. While a discussion of the arithmetic mean in MPI resource usage might be the most intuitive summary statistic, it is not always the most accurate in terms of representing the underlying data. In this paper, we analyze MPI resource usage for a number of key MPI workloads using an existing MPI trace collector and discrete-event simulator. Our analysis demonstrates that the average, while easy and efficient to calculate, is a useful metric for characterizing latency and bandwidth measurements , but may not be a good representation of application message sizes, match list search depths, or MPI inter-operation times. Additionally, we show that the median and mode are superior choices in many cases. We also observe that the arithmetic mean is not the best representation of central tendency for data that are drawn from distributions that are multi-modal or have heavy tails. The results and analysis of our work provide valuable guidance on how we, as a community, should discuss and analyze MPI resource usage data for scientific applications.},
  archive      = {J_PARCO},
  author       = {Kurt B. Ferreira and Scott Levy},
  doi          = {10.1016/j.parco.2021.102825},
  journal      = {Parallel Computing},
  pages        = {102825},
  shortjournal = {Parallel Comput.},
  title        = {Evaluating MPI resource usage summary statistics},
  volume       = {108},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). GPU accelerated parallel reliability-guided digital volume
correlation with automatic seed selection based on 3D SIFT.
<em>PARCO</em>, <em>108</em>, 102824. (<a
href="https://doi.org/10.1016/j.parco.2021.102824">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Digital volume correlation (DVC) is a powerful and widely used technique for measuring the internal 3D deformation field of a wide range of materials. One of the most popular DVC algorithms is the reliability-guided DVC (RG-DVC) which is good at dealing with large continuous deformation . However, RG-DVC requires a manually specified seed from which computation starts, and suffers from the efficiency due to a huge amount of computation and data dependency . This paper proposes a GPU accelerated parallel reliability-guided DVC algorithm (CuSIFT-RGDVC) on CUDA, which leverages 3D scale-invariant feature transform (3D SIFT) to assist seed selection to realize fully automation and improves performance utilizing GPU computing. In CuSIFT-RGDVC, reliability-guided displacement tracking (RGDT) is rewritten using sorted array-based batch processing mechanism which is a globally sequential locally parallel model, and multi-granularity parallelism is adopted to maximize GPU utilization. The empirical result shows that the proposed CuSIFT-RGDVC provides up to 29.1x speedup compared with our multi-threaded implementation and achieves the same level of computation speed as the state-of-the-art path-independent DVC without sacrificing accuracy.},
  archive      = {J_PARCO},
  author       = {Linchao Cai and Junrong Yang and Shoubin Dong and Zhenyu Jiang},
  doi          = {10.1016/j.parco.2021.102824},
  journal      = {Parallel Computing},
  pages        = {102824},
  shortjournal = {Parallel Comput.},
  title        = {GPU accelerated parallel reliability-guided digital volume correlation with automatic seed selection based on 3D SIFT},
  volume       = {108},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MPI collective communication through a single set of
interfaces: A case for orthogonality. <em>PARCO</em>, <em>107</em>,
102826. (<a href="https://doi.org/10.1016/j.parco.2021.102826">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present and discuss a unified view of and interface for collective communication in the MPI (Message-Passing Interface) standard that in a natural way exploits MPI’s orthogonality of concepts. We observe that the currently separate and different interfaces for sparse and global collective communication can be unified under the global collective communication interfaces, and at the same time lead to leaner and stronger support for stencil-like, sparse collective communication on Cartesian communicators. Our observations not only significantly reduce the number of concrete operation interfaces, but extend the functionality that can be supported by MPI while provisioning for possible future, much more wide-ranging functionality. We suggest to (re)define communicators as the sole carriers of the topological structure over processes that determines the semantics of the collective operations, and to limit the functions that can associate topological information with communicators to the functions for distributed graph topology and inter-communicator creation. As a consequence, one set of interfaces for collective communication operations (in blocking, non-blocking, and persistent variants) will suffice, thereby explicitly eliminating the MPI_Neighbor_ interfaces (in all variants) from the MPI standard. Topological structure will no longer be implied by Cartesian communicators, which in turn will have the sole role of naming processes in a ( d d -dimensional, Euclidean) geometric point-space. The geometric naming can be passed to the topology creating functions as part of the communicator, and guide process rank reordering and topological collective algorithm selection . We also explore ramifications of our proposal for one-sided communication. Concretely, at the price of only one essential, additional function, our suggestion eliminates 10 concrete collective function interfaces from MPI 3.1, and 15 from MPI 4.0, while providing vastly more optimization scope for the MPI library implementation. Interfaces for Cartesian communicators can likewise be simplified and/or eliminated from the MPI standard, as could the general active (post-start) synchronization mechanism for one-sided communication.},
  archive      = {J_PARCO},
  author       = {Jesper Larsson Träff and Sascha Hunold and Guillaume Mercier and Daniel J. Holmes},
  doi          = {10.1016/j.parco.2021.102826},
  journal      = {Parallel Computing},
  pages        = {102826},
  shortjournal = {Parallel Comput.},
  title        = {MPI collective communication through a single set of interfaces: A case for orthogonality},
  volume       = {107},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Optimal task scheduling for partially heterogeneous systems.
<em>PARCO</em>, <em>107</em>, 102815. (<a
href="https://doi.org/10.1016/j.parco.2021.102815">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Task scheduling with communication delays is a strongly NP-hard problem. Previous attempts at finding optimal solutions to this problem have used branch-and-bound state–space search, with promising results. However, the scheduling model used assumes a target system with fully homogeneous processors, which is unrealistic for many real world systems for which task scheduling might be performed. This paper presents an extension to the Allocation-Ordering (AO) state–space model for task scheduling which allows a system with related heterogeneous processors to be modeled, and optimal schedules on such a system to be found. Of particular note, the distinct allocation phase allows this model to efficiently adapt to partially heterogeneous systems , in which subsets of the processors are identical to each other, which significantly helps to reduce the search space. An extensive experimental evaluation shows that the introduction of heterogeneity certainly increases the difficulty of the problem. However, many problem instances solvable using homogeneous processors remain solvable with a heterogeneous target system, made possible by the significant benefit of this model in considering partial heterogeneity.},
  archive      = {J_PARCO},
  author       = {Michael Orr and Oliver Sinnen},
  doi          = {10.1016/j.parco.2021.102815},
  journal      = {Parallel Computing},
  pages        = {102815},
  shortjournal = {Parallel Comput.},
  title        = {Optimal task scheduling for partially heterogeneous systems},
  volume       = {107},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Performance portability through machine learning guided
kernel selection in SYCL libraries. <em>PARCO</em>, <em>107</em>,
102813. (<a href="https://doi.org/10.1016/j.parco.2021.102813">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatically tuning parallel compute kernels allows libraries and frameworks to achieve performance on a wide range of hardware, however these techniques are typically focused on finding optimal kernel parameters for particular input sizes and parameters. General purpose compute libraries must be able to cater to all inputs and parameters provided by a user, and so these techniques are of limited use. Additionally parallel programming frameworks such as SYCL require that the kernels be deployed in a binary format embedded within the library. As such it is impractical to deploy a large number of possible kernel configurations without inflating the library size. Machine learning methods can be used to mitigate against both of these problems and provide performance for general purpose routines with a limited number of kernel configurations . We show that unsupervised clustering methods can be used to select a subset of the possible kernels that should be deployed and that simple classification methods can be trained to select from these kernels at runtime to give good performance. As these techniques are fully automated, relying only on benchmark data, the tuning process for new hardware or problems does not require any developer effort or expertise. We demonstrate that this technique gives competitive performance to vendor specific libraries when used in inference of a large neural network .},
  archive      = {J_PARCO},
  author       = {John Lawson and Mehdi Goli},
  doi          = {10.1016/j.parco.2021.102813},
  journal      = {Parallel Computing},
  pages        = {102813},
  shortjournal = {Parallel Comput.},
  title        = {Performance portability through machine learning guided kernel selection in SYCL libraries},
  volume       = {107},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An optimisation of allreduce communication in
message-passing systems. <em>PARCO</em>, <em>107</em>, 102812. (<a
href="https://doi.org/10.1016/j.parco.2021.102812">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Collective communication, namely the pattern allreduce in message-passing systems, is optimised based on measurements at the installation time of the library. The algorithms used are set up in an initialisation phase of the communication, as so-called persistent collective communication, introduced in the message-passing interface (MPI) standard. Part of our allreduce algorithms are the patterns reduce_scatter and allgatherv which are also considered standalone. For the allreduce pattern for short messages the existing cyclic shift algorithm (Bruck’s algorithm) is applied with a prefix operation. For allreduce and long messages our algorithm is based on reduce_scatter and allgatherv , where the cyclic shift algorithm is applied with a flexible number of communication ports per node. The algorithms for equal message sizes are used with non-equal message sizes together with a heuristic for rank reordering. Medium message sizes are communicated with an incomplete reduce_scatter followed by allgatherv . Furthermore, an optional recursive application of the cyclic shift algorithm is applied. All algorithms are applied at the node level. The data is gathered and scattered by the cores within the node and the communication algorithms are applied across the nodes. In general, our approach outperforms the non-persistent counterpart in established MPI libraries by up to one order of magnitude or shows equal performance, with a few exceptions of number of nodes and message sizes.},
  archive      = {J_PARCO},
  author       = {Andreas Jocksch and Noé Ohana and Emmanuel Lanti and Eirini Koutsaniti and Vasileios Karakasis and Laurent Villard},
  doi          = {10.1016/j.parco.2021.102812},
  journal      = {Parallel Computing},
  pages        = {102812},
  shortjournal = {Parallel Comput.},
  title        = {An optimisation of allreduce communication in message-passing systems},
  volume       = {107},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Optimization of DNS code and visualization of entrainment
and mixing phenomena at cloud edges. <em>PARCO</em>, <em>107</em>,
102811. (<a href="https://doi.org/10.1016/j.parco.2021.102811">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Entrainment and mixing processes occur during the entire life of a cloud. These processes change the droplet size distribution , which determines rain formation and radiative properties. Since it is a microphysical process, it cannot be resolved in large scale weather forecasting models. Small scale simulations such as Direct Numerical Simulations (DNS) are required to resolve the most minute scale of these processes. The DNS of cloud dynamics are performed by integrating two mathematical models, Eulerian and Lagrangian, in a coupled way. Running DNS is a tedious task as it requires a huge amount of computational resources. In this work, we provide a projection of the required resources for running DNS in different size domains. Visualizing these large simulations presents an added challenge, as they generate petabytes of data. Visualization plays a vital role in analyzing and understanding these huge data outputs. Here, we experimented with multiple tools to conduct a visual analysis of this data. Two of these tools are well established and tested technologies : ParaView and VAPOR. The others are emergent technologies in the development phase. This data simulation and visualization, in addition to exploring DNS as mentioned above, provided an opportunity to test and improve development of several tools and methods.},
  archive      = {J_PARCO},
  author       = {Bipin Kumar and Matt Rehme and Neethi Suresh and Nihanth Cherukuru and Stanislaw Jaroszynski and Samual Li and Scott Pearse and Tim Scheitlin and Suryachandra A. Rao and Ravi S. Nanjundiah},
  doi          = {10.1016/j.parco.2021.102811},
  journal      = {Parallel Computing},
  pages        = {102811},
  shortjournal = {Parallel Comput.},
  title        = {Optimization of DNS code and visualization of entrainment and mixing phenomena at cloud edges},
  volume       = {107},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). NVIDIA IndeX accelerated computing for visualizing cholla’s
galactic winds. <em>PARCO</em>, <em>107</em>, 102809. (<a
href="https://doi.org/10.1016/j.parco.2021.102809">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Galactic winds – outflows of gas driven out of galaxies by the combined effects of thousands of supernovae – are a crucial feature of galaxy evolution. Despite their importance, a complete theoretical picture of these winds has been elusive. Simulating the complicated interaction between the hot, high pressure gas created by supernovae and the cooler, high density gas in the galaxy disk requires massive computational resources and sophisticated software. For this purpose, Computational Hydrodynamics On Parallel Architectures (Cholla) has been demonstrated to be a scalable and efficient tool that operates in large, distributed multi-GPU environments at high levels of performance. This additional performance results in unprecedented resolution for this type of simulation and massive amounts of generated data. This raises the requirements for analysis tools that can cope with scale and complexity of the simulated physical processes. To address those challenges, we utilize NVIDIA IndeX as a scalable framework to visualize the simulation output: NVIDIA IndeX features a streaming-based architecture to interactively explore simulation results in large-scale, multi-GPU environments. We utilize customized sampling programs for multi-volume and surface rendering to address analysis questions of galactic wind simulations. This combination of massively parallel simulation and analysis allows us to utilize recent supercomputer capabilities and to speed up the exploration of galactic wind simulations.},
  archive      = {J_PARCO},
  author       = {Evan Schneider and Brant Robertson and Alexander Kuhn and Christopher Lux and Marc Nienhaus},
  doi          = {10.1016/j.parco.2021.102809},
  journal      = {Parallel Computing},
  pages        = {102809},
  shortjournal = {Parallel Comput.},
  title        = {NVIDIA IndeX accelerated computing for visualizing cholla&#39;s galactic winds},
  volume       = {107},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). PEAB: A pool-based distributed evolutionary algorithm model
with buffer. <em>PARCO</em>, <em>106</em>, 102808. (<a
href="https://doi.org/10.1016/j.parco.2021.102808">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pool Model is an asynchronous, loosely coupled distributed evolutionary algorithm (dEA) design architecture. However, the classical Pool Model face some design problems, such as population control, work redundancy, rough selection/replacement strategies, and unreliable connections , etc. In this paper, a novel distributed pool evolutionary algorithm (EA) model with buffer (PEAB) is proposed. PEAB can solve the inherent problems of the Pool Model by using the buffer setting, the Reunion mechanism, and the Migration in Pool (MP) strategy. Besides, PEAB provides stronger population control and more global population selection/replacement strategies. In the experimental part, we compared PEAB with another Pool Model named EvoSpace using a common benchmark. The experiments showed that the convergence rate of PEAB is 59.7\% faster than that of EvoSpace under the respective fastest conditions. PEAB also has a faster reception rate of the first generation and stronger population control. Besides, this paper also tests and analyzes the scalability of PEAB using two other benchmarks. The overall trend of the experiment results suggested that PEAB would be faster with more Workers. Last but not least, this paper studies the effect of the MP strategy on the performance of PEAB, and the results showed that the MP strategy can effectively improve the convergence efficiency.},
  archive      = {J_PARCO},
  author       = {Zhixing Yu and Kejing He and Xiuhong Zou},
  doi          = {10.1016/j.parco.2021.102808},
  journal      = {Parallel Computing},
  pages        = {102808},
  shortjournal = {Parallel Comput.},
  title        = {PEAB: A pool-based distributed evolutionary algorithm model with buffer},
  volume       = {106},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Graph optimization algorithm for low-latency interconnection
networks. <em>PARCO</em>, <em>106</em>, 102805. (<a
href="https://doi.org/10.1016/j.parco.2021.102805">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In various industrial products including parallel computing systems, it is expected that the performance of the whole system will be improved by applying a network topology with smaller diameter and average shortest path length (ASPL). Such network topologies can be defined as the order/degree problem in graph theory by modeling a network topology as an undirected graph . Although previous research indicates that the random graph has both small diameter and ASPL due to the small-world effect, there is still room for improvement. In this paper, we propose an algorithm for the order/degree problem that optimizes random graphs. The feature of the proposed algorithm is that by giving symmetry to the graph, the solution search performance is improved and the calculation time for obtaining the diameter and APSL is greatly reduced. The proposed algorithm is evaluated using various graphs, including a huge graph with one million vertices presented by Graph Golf, an international competition for the order/degree problem. The results show that the proposed algorithm can generate a network topology with sufficiently small diameter and APSL. In addition, we also simulate the latency, performance of the parallel benchmarks, and bisection on the generated network topologies. The results show that the generated network topology performs better than the random topology and a conventional k k -ary n n -cube topology.},
  archive      = {J_PARCO},
  author       = {Masahiro Nakao and Maaki Sakai and Yoshiko Hanada and Hitoshi Murai and Mitsuhisa Sato},
  doi          = {10.1016/j.parco.2021.102805},
  journal      = {Parallel Computing},
  pages        = {102805},
  shortjournal = {Parallel Comput.},
  title        = {Graph optimization algorithm for low-latency interconnection networks},
  volume       = {106},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Computational records with aging hardware: Controlling half
the output of SHA-256. <em>PARCO</em>, <em>106</em>, 102804. (<a
href="https://doi.org/10.1016/j.parco.2021.102804">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {SHA-256 is a secure cryptographic hash function . As such, its output should not have any detectable property. This paper describes three bit strings whose hashes by SHA-256 are nevertheless correlated in a non-trivial way: the first half of their hashes XORs to zero. They were found by “brute-force”, without exploiting any cryptographic weakness in the hash function itself. This does not threaten the security of the hash function and does not have any cryptographic implication. This is an example of a large “combinatorial” computation in which at least 8 . 7 × 1 0 22 8.7×1022 integer operations have been performed. This was made possible by the combination of: (1) recent progress on algorithms for the underlying problem, (2) creative use of “dedicated” hardware accelerators , (3) adapted implementations of the relevant algorithms that could run on massively parallel machines. The actual computation was done on aging hardware. It required seven calendar months using two obsolete second-hand bitcoin mining devices converted into “useful” computational devices. A second step required 570 CPU-years on an 8-year old IBM BlueGene/Q computer, a few weeks before it was scrapped. To the best of our knowledge, this is the first practical 128-bit collision-like result obtained by brute-force, and it is the first bitcoin miner-accelerated computation.},
  archive      = {J_PARCO},
  author       = {Mellila Bouam and Charles Bouillaguet and Claire Delaplace and Camille Noûs},
  doi          = {10.1016/j.parco.2021.102804},
  journal      = {Parallel Computing},
  pages        = {102804},
  shortjournal = {Parallel Comput.},
  title        = {Computational records with aging hardware: Controlling half the output of SHA-256},
  volume       = {106},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Synthesis and feedback on the distribution and
parallelization of FMI-CS-based co-simulations with the DACCOSIM
platform. <em>PARCO</em>, <em>106</em>, 102802. (<a
href="https://doi.org/10.1016/j.parco.2021.102802">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A co-simulation applied to Smart Grids consists of grouping in the same setting models of physical components (among other electrical ones) and models of control units (including communication devices). Combining these models needs to use a generic and robust co-simulation environment instead of developing a specific one. In this context, we developed the DACCOSIM 2017 co-simulation platform based on FMI-CS (Functional Mock-up Interface for CoSimulation) standard to simulate the physical components of a Smart Grid. These components represent the most CPU-consuming part of the co-simulation. However, the tasks of FMI-CS-based applications (FMUs) are exposed as heterogeneous gray boxes with no information concerning their computation and communication volumes. Moreover, all these FMUs frequently communicate with each other by sending a lot of small messages. Consequently, the deployment of an FMI-CS based co-simulation on a distributed architecture is a complex task carried out by DACCOSIM 2017. This paper introduces the development of DACCOSIM-2017, and its experiment on distributed architectures.},
  archive      = {J_PARCO},
  author       = {Cherifa Dad and Jean-Philippe Tavella and Stéphane Vialle},
  doi          = {10.1016/j.parco.2021.102802},
  journal      = {Parallel Computing},
  pages        = {102802},
  shortjournal = {Parallel Comput.},
  title        = {Synthesis and feedback on the distribution and parallelization of FMI-CS-based co-simulations with the DACCOSIM platform},
  volume       = {106},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Callback-based completion notification using MPI
continuations. <em>PARCO</em>, <em>106</em>, 102793. (<a
href="https://doi.org/10.1016/j.parco.2021.102793">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Asynchronous programming models (APM) are gaining more and more traction, allowing applications to expose the available concurrency to a runtime system tasked with coordinating the execution. While MPI has long provided support for multi-threaded communication and non-blocking operations, it falls short of adequately supporting APMs as correctly and efficiently handling MPI communication in different models is still a challenge. We have previously proposed an extension to the MPI standard providing operation completion notifications using callbacks, so-called MPI Continuations. This interface is flexible enough to accommodate a wide range of different APMs. In this paper, we present an extension to the previously described interface that allows for finer control of the behavior of the MPI Continuations interface. We then present some of our first experiences in using the interface in the context of different applications, including the NAS parallel benchmarks, the PaRSEC task-based runtime system, and a load-balancing scheme within an adaptive mesh refinement solver called ExaHyPE. We show that the interface, implemented inside Open MPI, enables low-latency, high-throughput completion notifications that outperform solutions implemented in the application space.},
  archive      = {J_PARCO},
  author       = {Joseph Schuchart and Philipp Samfass and Christoph Niethammer and José Gracia and George Bosilca},
  doi          = {10.1016/j.parco.2021.102793},
  journal      = {Parallel Computing},
  pages        = {102793},
  shortjournal = {Parallel Comput.},
  title        = {Callback-based completion notification using MPI continuations},
  volume       = {106},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sphynx: A parallel multi-GPU graph partitioner for
distributed-memory systems. <em>PARCO</em>, <em>106</em>, 102769. (<a
href="https://doi.org/10.1016/j.parco.2021.102769">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph partitioning has been an important tool to partition the work among several processors to minimize the communication cost and balance the workload. While accelerator-based supercomputers are emerging to be the standard, the use of graph partitioning becomes even more important as applications are rapidly moving to these architectures. However, there is no distributed-memory-parallel, multi-GPU graph partitioner available for applications. We developed a spectral graph partitioner, Sphynx, using the portable, accelerator-friendly stack of the Trilinos framework. In Sphynx, we allow using different preconditioners and exploit their unique advantages. We use Sphynx to systematically evaluate the various algorithmic choices in spectral partitioning with a focus on the GPU performance. We perform those evaluations on two distinct classes of graphs: regular (such as meshes, matrices from finite element methods) and irregular (such as social networks and web graphs), and show that different settings and preconditioners are needed for these graph classes. The experimental results on the Summit supercomputer show that Sphynx is the fastest alternative on irregular graphs in an application-friendly setting and obtains a partitioning quality close to ParMETIS on regular graphs. When compared to nvGRAPH on a single GPU , Sphynx is faster and obtains better balance and better quality partitions. Sphynx provides a good and robust partitioning method across a wide range of graphs for applications looking for a GPU-based partitioner.},
  archive      = {J_PARCO},
  author       = {Seher Acer and Erik G. Boman and Christian A. Glusa and Sivasankaran Rajamanickam},
  doi          = {10.1016/j.parco.2021.102769},
  journal      = {Parallel Computing},
  pages        = {102769},
  shortjournal = {Parallel Comput.},
  title        = {Sphynx: A parallel multi-GPU graph partitioner for distributed-memory systems},
  volume       = {106},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A computational-graph partitioning method for training
memory-constrained DNNs. <em>PARCO</em>, <em>104-105</em>, 102792. (<a
href="https://doi.org/10.1016/j.parco.2021.102792">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many state-of-the-art Deep Neural Networks (DNNs) have substantial memory requirements. Limited device memory becomes a bottleneck when training those models. We propose ParDNN , an automatic, generic, and non-intrusive partitioning strategy for DNNs that are represented as computational graphs. ParDNN decides a placement of DNN’s underlying computational graph operations across multiple devices so that the devices’ memory constraints are met and the training time is minimized. ParDNN is completely independent of the deep learning aspects of a DNN. It requires no modification neither at the model nor at the systems level implementation of its operation kernels. ParDNN partitions DNNs having billions of parameters and hundreds of thousands of operations in seconds to few minutes. Our experiments with TensorFlow on 16 GPUs demonstrate efficient training of 5 very large models while achieving superlinear scaling for both the batch size and training throughput. ParDNN either outperforms or qualitatively improves upon the related work.},
  archive      = {J_PARCO},
  author       = {Fareed Qararyah and Mohamed Wahib and Doğa Dikbayır and Mehmet Esat Belviranli and Didem Unat},
  doi          = {10.1016/j.parco.2021.102792},
  journal      = {Parallel Computing},
  pages        = {102792},
  shortjournal = {Parallel Comput.},
  title        = {A computational-graph partitioning method for training memory-constrained DNNs},
  volume       = {104-105},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). HySet: A hybrid framework for exact set similarity join
using a GPU. <em>PARCO</em>, <em>104-105</em>, 102790. (<a
href="https://doi.org/10.1016/j.parco.2021.102790">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Set similarity join is a fundamental operation used in a wide range of applications such as data mining, data cleaning and entity resolution. Existing methods proposed for set similarity join conform to a filter-verification framework where potential candidate pairs are generated in the filtering phase and then undergo a verification phase to output the final result. Several different kinds of filtering techniques have been proposed and techniques also differentiate in the manner they couple filtering with verification. However, it has been shown that no globally dominant technique exists. Depending on the dataset and query characteristics, each technique has its own strong and weak points. Based on these findings, the main contribution of this work is the development of a hybrid framework for the set similarity join operation for a single GPU-equipped machine setting. Our framework encapsulates a partitioning mechanism to utilize appropriately both the CPU and the GPU . We present all technical details and we show performance speedups up to 3.25x after thorough evaluation.},
  archive      = {J_PARCO},
  author       = {Christos Bellas and Anastasios Gounaris},
  doi          = {10.1016/j.parco.2021.102790},
  journal      = {Parallel Computing},
  pages        = {102790},
  shortjournal = {Parallel Comput.},
  title        = {HySet: A hybrid framework for exact set similarity join using a GPU},
  volume       = {104-105},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A scalable algorithm for the optimization of neural network
architectures. <em>PARCO</em>, <em>104-105</em>, 102788. (<a
href="https://doi.org/10.1016/j.parco.2021.102788">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new scalable method to optimize the architecture of an artificial neural network . The proposed algorithm, called Greedy Search for Neural Network Architecture , aims to determine a neural network with minimal number of layers that is at least as performant as neural networks of the same structure identified by other hyperparameter search algorithms in terms of accuracy and computational cost. Numerical results performed on benchmark datasets show that, for these datasets, our method outperforms state-of-the-art hyperparameter optimization algorithms in terms of attainable predictive performance by the selected neural network architecture, and time-to-solution for the hyperparameter optimization to complete.},
  archive      = {J_PARCO},
  author       = {Massimiliano Lupo Pasini and Junqi Yin and Ying Wai Li and Markus Eisenbach},
  doi          = {10.1016/j.parco.2021.102788},
  journal      = {Parallel Computing},
  pages        = {102788},
  shortjournal = {Parallel Comput.},
  title        = {A scalable algorithm for the optimization of neural network architectures},
  volume       = {104-105},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Improving the i/o of large geophysical models using PnetCDF
and BeeGFS. <em>PARCO</em>, <em>104-105</em>, 102786. (<a
href="https://doi.org/10.1016/j.parco.2021.102786">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large scale geophysical modeling uses high performance computing systems to expedite the solutions of very large, complex systems. High disk latencies, low IOPS, and low read/write data transfer rates are relegating many numerical simulations to I/O bound jobs, where the run time is bound not by CPU rate, but by I/O rate. In this paper we seek to improve the I/O of two geophysical modeling applications and take full advantage of the parallel nature of the programs, as well as the file management system for the large output files. Parallelizing output for these programs is achieved using PnetCDF, a parallel implementation of the netCDF format, and BeeGFS, an open-source parallel file system . Using these solutions, we have significantly decreased the amount of time spent saving data to disk, and give analysis of the features used in relation to PnetCDF with BeeGFS I/O optimization.},
  archive      = {J_PARCO},
  author       = {Jared Brzenski and Christopher Paolini and Jose E. Castillo},
  doi          = {10.1016/j.parco.2021.102786},
  journal      = {Parallel Computing},
  pages        = {102786},
  shortjournal = {Parallel Comput.},
  title        = {Improving the I/O of large geophysical models using PnetCDF and BeeGFS},
  volume       = {104-105},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Optimization with the OpenACC-to-FPGA framework on the arria
10 and stratix 10 FPGAs. <em>PARCO</em>, <em>104-105</em>, 102784. (<a
href="https://doi.org/10.1016/j.parco.2021.102784">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The reconfigurable computing paradigm with field programmable gate arrays (FPGAs) has received renewed interest in the high-performance computing field due to FPGAs’ unique combination of performance and energy efficiency. However, difficulties in programming and optimizing FPGAs have prevented them from being widely accepted as general-purpose computing devices. In accelerator-based heterogeneous computing , portability across diverse heterogeneous devices is also an important issue, but the unique architectural features in FPGAs make this difficult to achieve. To address these issues, a directive-based, high-level FPGA programming and optimization framework was previously developed. In this work, developed optimizations were combined holistically using the directive-based approach to show that each individual benchmark requires a unique set of optimizations to maximize performance. We perform this exploration on Intel Arria 10 and Stratix 10 FPGAs. We also explored the relationships between performance, resource usages, and compilation times, and investigated implications for performance portability . Finally, we present an initial evaluation of a real-world proxy application , LULESH.},
  archive      = {J_PARCO},
  author       = {Jacob Lambert and Seyong Lee and Jeffrey S. Vetter and Allen D. Malony},
  doi          = {10.1016/j.parco.2021.102784},
  journal      = {Parallel Computing},
  pages        = {102784},
  shortjournal = {Parallel Comput.},
  title        = {Optimization with the OpenACC-to-FPGA framework on the arria 10 and stratix 10 FPGAs},
  volume       = {104-105},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Parallel fast multipole method accelerated FFT on HPC
clusters. <em>PARCO</em>, <em>104-105</em>, 102783. (<a
href="https://doi.org/10.1016/j.parco.2021.102783">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With increasing sizes of distributed systems, there comes an increased risk of communication bottlenecks. In the past decade there has been a growing interest in communication-avoiding algorithms. The distributed memory Fast Fourier Transform is an important algorithm which suffers from major communication bottlenecks. In this work, we take a look at an existing communication-avoiding algorithm FMM-FFT, an alternative to FFT which utilizes the Fast Multipole Method (FMM) to reduce communications to a single all-to-all communication. We present a detailed implementation of FMM-FFT relying on modern libraries and demonstrate it on two distinct distributed memory architectures notably a traditional Intel Xeon based HPC cluster and then a Beowulf cluster. We show that while the FMM-FFT is significantly slower than FFT on the traditional HPC cluster, on the Beowulf cluster it outperforms standard FFT, consistently getting speedups of 1.5x or more against FFTW. We then proceed to show how the communication to computation cost metric is important and useful in explaining the performance results of FMM-FFT against standard FFT. The source code pertaining to this work is being made publicly available under a permissive open source licence at Github.},
  archive      = {J_PARCO},
  author       = {Chahak Mehta and Amarnath Karthi and Vishrut Jetly and Bhaskar Chaudhury},
  doi          = {10.1016/j.parco.2021.102783},
  journal      = {Parallel Computing},
  pages        = {102783},
  shortjournal = {Parallel Comput.},
  title        = {Parallel fast multipole method accelerated FFT on HPC clusters},
  volume       = {104-105},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Evaluating adaptive and predictive power management
strategies for optimizing visualization performance on supercomputers.
<em>PARCO</em>, <em>104-105</em>, 102782. (<a
href="https://doi.org/10.1016/j.parco.2021.102782">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Power is becoming an increasingly scarce resource on the next generation of supercomputers , and should be used wisely to improve overall performance. One strategy for improving power usage is hardware overprovisioning, i.e., systems with more nodes than can be run at full power simultaneously without exceeding the system-wide power limit. With this study, we compare two strategies for allocating power throughout an overprovisioned system – adaptation and prediction – in the context of visualization workloads. While adaptation has been suitable for workloads with more regular execution behaviors, it may not be as suitable on visualization workloads, since they can have variable execution behaviors. Our study considers a total of 104 experiments, which vary the rendering workload, power budget, allocation strategy, and node concurrency, including tests processing data sets up to 1 billion cells and using up to 18,432 cores across 512 nodes. Overall, we find that prediction is a superior strategy for this use case, improving performance up to 27\% compared to an adaptive strategy.},
  archive      = {J_PARCO},
  author       = {Stephanie Brink and Matthew Larsen and Hank Childs and Barry Rountree},
  doi          = {10.1016/j.parco.2021.102782},
  journal      = {Parallel Computing},
  pages        = {102782},
  shortjournal = {Parallel Comput.},
  title        = {Evaluating adaptive and predictive power management strategies for optimizing visualization performance on supercomputers},
  volume       = {104-105},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Block red–black MILU(0) preconditioner with relaxation on
GPU. <em>PARCO</em>, <em>103</em>, 102760. (<a
href="https://doi.org/10.1016/j.parco.2021.102760">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To accelerate the Krylov subspace-based linear equation solvers on Graphics Processing Units (GPUs), a stable, efficient and highly parallel preconditioner is essential. One of the strong candidates for such a preconditioner is the combination of the block red–black ordering and the relaxed modified incomplete LU factorization without fill-ins (MILU(0)). In this paper, we present techniques for implementing this type of preconditioner on General-purpose computing on GPU (GPGPU) using OpenACC . Our implementation is designed for 3-dimensional finite-difference computations with 7-point stencil, and the matrix storage format is optimized to realize coalesced memory access. Also, mixed-precision computation is employed to exploit the high single-precision performance of GPUs without sacrificing the accuracy of the computed solution. Extensive numerical tests were performed and the optimal values of various tunable parameters such as the number of blocks in each direction and the number of workers specified in OpenACC clauses are discussed. Performance comparison on NVIDIA Quadro GP100 and Tesla K40t GPUs shows that our solver is much faster than existing libraries like cuSPARSE, MAGMA, ViennaCL, and Ginkgo, especially when multiple linear equations with coefficient matrices sharing the same nonzero pattern are solved.},
  archive      = {J_PARCO},
  author       = {Akemi Shioya and Yusaku Yamamoto},
  doi          = {10.1016/j.parco.2021.102760},
  journal      = {Parallel Computing},
  pages        = {102760},
  shortjournal = {Parallel Comput.},
  title        = {Block red–black MILU(0) preconditioner with relaxation on GPU},
  volume       = {103},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Parallel FFT algorithms for high-order approximations on
three-dimensional compact stencils. <em>PARCO</em>, <em>103</em>,
102757. (<a href="https://doi.org/10.1016/j.parco.2021.102757">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recent development of multicore technologies on modern desktop computers makes parallelization of the proposed numerical approaches a priority in algorithmic research. The main performance improvement of personal computers in the upcoming years will be made based on the increasing number of cores on modern CPUs. This shifts the focus of algorithmic research from the development of sequential numerical methods to parallel methodology. This paper presents an efficient parallel direct algorithm with near-optimal complexity for the compact fourth and sixth-order approximation of the three-dimensional Helmholtz equations (Turkel et al., 2013) with the problem coefficient depending on only one of the coordinate directions. The developed method is based on a combination of the separation of variables technique and a Fast Fourier Transform (FFT) type method. Similar direct solvers for the lower-order approximations of the two and three-dimensional Helmholtz equation were considered in several previous publications by the authors and other researchers (see, e.g. Gryazin et al. (2000); Gryazin (2014); Elman and O’Leary (1998); Elman and O’Leary (1999); Toivanen and Wolfmayr (2020)). The authors also consider a generalization of the presented algorithm to the solution of a wide class of linear systems obtained from approximation on the compact 27-point three-dimensional stencils on the rectangular grids with similar requirements on the stencil coefficients. The general restrictions on the coefficients in the considered class of compact schemes are developed and presented. This class includes the second, fourth and sixth-order compact approximation schemes for the three-dimensional Helmholtz equation considered in this paper and our previous publications (Gryazin et al., 2000; Gryazin, 2014; Gryazin, 2014). As an example of the diversity of applications of the developed general method, the direct parallel implementation of a compact fourth-order approximation scheme for a convection–diffusion equation is considered. Another goal of this paper is to investigate the scalability of the proposed technique in the case of a large linear system using different parallel programming extensions. The results of the implementation of this method in OpenMP, MPI and hybrid programming environments on the multicore computers and multiple node clusters are presented and discussed. The results demonstrate the high efficiency of the proposed direct solvers for many important applications on the structured grid with the corresponding 27-diagonal matrices of sizes up to 1 0 11 1011 by 1 0 11 1011 .},
  archive      = {J_PARCO},
  author       = {Ronald Gonzales and Yury Gryazin and Yun Teck Lee},
  doi          = {10.1016/j.parco.2021.102757},
  journal      = {Parallel Computing},
  pages        = {102757},
  shortjournal = {Parallel Comput.},
  title        = {Parallel FFT algorithms for high-order approximations on three-dimensional compact stencils},
  volume       = {103},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Speedup vs. Quality: Asynchronous and cluster-based
distributed adaptive genetic algorithms for ordered problems.
<em>PARCO</em>, <em>103</em>, 102755. (<a
href="https://doi.org/10.1016/j.parco.2021.102755">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While the main motivation for Parallel Genetic Algorithms (PGAs) has been to improve the scalability of Genetic Algorithms (GAs), techniques and strategies for maintaining population diversity is an equally active research topic. Island Model Genetic Algorithms (IMGAs) represent one of the most mature strategies for developing PGAs in an effective and scalable manner. However, identifying how much migration and which individuals should migrate are open research problems. Meanwhile, recent developments in Adaptive Genetic Algorithms (AGAs) have led to techniques for monitoring and maintaining population diversity in an online manner. The aim of the present work is to introduce adaptive techniques and mechanisms into PGAs in order to determine when, how much and which individuals are most suitable for migration. We present a number of adaptive PGAs that aim to maintain diversity and maximise coverage of the solution space by minimising the overlap between islands. PGAs presented in this work are empirically assessed for their abilities in scalability, ability to find good quality solutions and maintain population diversity in ordered problems. These metrics are compared to existing adaptive and parallel GAs selected from the literature for their performance. We estimated the overhead costs of monitoring diversity and communication would result in a trade off between scalability and search capabilities. Our results suggest that an asynchronous adaptive PGA has the greatest speedup potential. However, while localising adaptive populations by k k -means clustering is less scalable, results indicate that the method is more effective at directing the search in order to reduce the likelihood of islands searching in the same areas of the solution space. For this reason, an adaptive PGA with clustering-based migration demonstrates greater potential in solution quality while maintaining good speedup performance.},
  archive      = {J_PARCO},
  author       = {Ryoma Ohira and Md. Saiful Islam and Humayun Kayesh},
  doi          = {10.1016/j.parco.2021.102755},
  journal      = {Parallel Computing},
  pages        = {102755},
  shortjournal = {Parallel Comput.},
  title        = {Speedup vs. quality: Asynchronous and cluster-based distributed adaptive genetic algorithms for ordered problems},
  volume       = {103},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A scalable and low latency probe-based scheduler for data
analytics frameworks. <em>PARCO</em>, <em>103</em>, 102752. (<a
href="https://doi.org/10.1016/j.parco.2021.102752">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Today’s data analytics frameworks divide jobs into many parallel tasks such that each task operates on a small partition of data in order to execute jobs with low latency. Such frameworks often rely on probe-based distributed schedulers to tackle the challenge of reducing the associated overhead. Unfortunately, the existing solutions do not perform efficiently under workload fluctuations and heterogeneous job durations. This is due to a problem called Head-of-Line blocking , i.e., short tasks are enqueued at workers behind longer tasks. To overcome this problem, we propose Peacock (Khelghatdoust and Gramoli, 0000) [25] a new fully distributed probe-based scheduling method. Unlike the existing methods, Peacock introduces a novel probe rotation technique. Workers form a ring overlay network and rotate probes using elastic queues of workers. It is augmented by a novel starvation-free probe reordering algorithm executed by workers. We evaluate Peacock against two existing state-of-the-art probe based solutions through a trace driven simulation of up to 20,000 workers and a distributed experiment of 100 workers in Apache Spark under Google, Cloudera, and Yahoo! traces. The performance results indicate that Peacock outperforms the state-of-the-art in all cluster sizes and loads. Our distributed experiments confirm our simulation results.},
  archive      = {J_PARCO},
  author       = {Mansour Khelghatdoust and Vincent Gramoli},
  doi          = {10.1016/j.parco.2021.102752},
  journal      = {Parallel Computing},
  pages        = {102752},
  shortjournal = {Parallel Comput.},
  title        = {A scalable and low latency probe-based scheduler for data analytics frameworks},
  volume       = {103},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Visualizing the world’s largest turbulence simulation.
<em>PARCO</em>, <em>102</em>, 102758. (<a
href="https://doi.org/10.1016/j.parco.2021.102758">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We describe a novel, scalable approach for scientific visualization in HPC environments, based on the ray tracing engine Intel® OSPRay associated with VisIt. Part of the software stack of the Leibniz Supercomputing Centre, this method has been applied to the visualization of the largest simulations of interstellar turbulence ever performed, produced on SuperMUC-NG. The hybrid (MPI + Threading Building Blocks) parallelization of OSPRay and VisIt allows efficient scaling up to about 150 thousand cores, making it possible to visualize the data at the full, unprecedented resolution of 1004 8 3 100483 grid elements (about 23 TB per snapshot). Besides presenting the method, its HPC context and future developments, we describe the implications of our visualization in the considered science case: our work brilliantly showcases the stretching-and-folding mechanisms through which astrophysical processes drive turbulence and amplify the magnetic field in the interstellar gas, and how the first structures, the seeds of newborn stars are shaped by this process. We finally observe the similarities between ray tracing and other HPC numerical techniques used in astrophysics, anticipating increasing convergences in the near future.},
  archive      = {J_PARCO},
  author       = {Salvatore Cielo and Luigi Iapichino and Johannes Günther and Christoph Federrath and Elisabeth Mayer and Markus Wiedemann},
  doi          = {10.1016/j.parco.2021.102758},
  journal      = {Parallel Computing},
  pages        = {102758},
  shortjournal = {Parallel Comput.},
  title        = {Visualizing the world’s largest turbulence simulation},
  volume       = {102},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). OpenCL-like offloading with metaprogramming for SX-aurora
TSUBASA. <em>PARCO</em>, <em>102</em>, 102754. (<a
href="https://doi.org/10.1016/j.parco.2021.102754">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents an OpenCL-like offload programming framework for NEC SX-Aurora TSUBASA (SX-Aurora) and also discusses the benefit of employing metaprogramming to describe architecture-specific parts of the programs. Unlike traditional vector systems, one node of an SX-Aurora system consists of a host processor and some vector processors on PCI-Express cards, which are called a vector host and vector engines , respectively. Since the standard OpenCL execution model does not naturally fit in the vector engine, this paper discusses how to adapt the OpenCL specification to SX-Aurora while considering the trade off between performance and code portability. This paper employs OpenCL to minimize non-portable parts of an application code for offload programming, and then metaprogramming to describe the non-portable parts. Performance evaluation results clearly demonstrate that, with a moderate programming effort, the proposed framework can express the collaboration between a vector host and a vector engine so as to make a good use of both of the two different processors. By delegating the right task to the right processor, an OpenCL-like program can fully exploit the performance of SX-Aurora. Moreover, metaprogramming can express vectorization-aware performance optimization to enhance the performance portability across different architectures including SX-Aurora.},
  archive      = {J_PARCO},
  author       = {Hiroyuki Takizawa and Shinji Shiotsuki and Naoki Ebata and Ryusuke Egawa},
  doi          = {10.1016/j.parco.2021.102754},
  journal      = {Parallel Computing},
  pages        = {102754},
  shortjournal = {Parallel Comput.},
  title        = {OpenCL-like offloading with metaprogramming for SX-aurora TSUBASA},
  volume       = {102},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Context switch cost aware joint task merging and scheduling
for deep learning applications. <em>PARCO</em>, <em>102</em>, 102753.
(<a href="https://doi.org/10.1016/j.parco.2021.102753">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning applications executed on mobile devices can provide artificial intelligence services with multi-access edge computing (MEC). Most of existing works on computation offloading neglect the context switch cost, which is one of the dominating factors that affect the finishing time of deep learning applications. This paper thus tries to investigate the context switch cost aware joint task merging and scheduling for deep learning applications. We formulate the problem as a non-linear integer programming, and prove that it is NP-Hard. The objective is to minimize the finishing time of deep learning applications with energy budget constraints. To solve the problem, an efficient joint merging and scheduling algorithm named fusion merging scheduling (FMS) is proposed to fully exploit the potential influence of context switch cost in distributed execution of deep learning tasks. A real-world platform is built to evaluate the performance of proposed algorithm. Experimental results show that FMS can reduce the average finishing time of deep learning applications by 79\%, 63\% and 75\%, respectively, compared with a greedy scheduling algorithm and two existing benchmarks, while achieving similar energy costs.},
  archive      = {J_PARCO},
  author       = {Xin Long and Jigang Wu and Yalan Wu and Long Chen and Yidong Li},
  doi          = {10.1016/j.parco.2021.102753},
  journal      = {Parallel Computing},
  pages        = {102753},
  shortjournal = {Parallel Comput.},
  title        = {Context switch cost aware joint task merging and scheduling for deep learning applications},
  volume       = {102},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Parallel and scalable dunn index for the validation of big
data clusters. <em>PARCO</em>, <em>102</em>, 102751. (<a
href="https://doi.org/10.1016/j.parco.2021.102751">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Parallelizing data clustering algorithms has attracted the interest of many researchers over the past few years. Many efficient parallel algorithms were proposed to build partitioning over a huge volume of data. The effectiveness of these algorithms is attributed to the distribution of data among a cluster of nodes and to the parallel computation models. Although the effectiveness of parallel models to deal with increasing volume of data little work is done on the validation of big clusters. To deal with this issue, we propose a parallel and scalable model, referred to as S-DI (Scalable Dunn Index), to compute the Dunn Index measure for an internal validation of clustering results . Rather than computing the Dunn Index on a single machine in the clustering validation process, the new proposed measure is computed by distributing the partitioning among a cluster of nodes using a customized parallel model under Apache Spark framework. The proposed S-DI is also enhanced by a Sketch and Validate sampling technique which aims to approximate the Dunn Index value by using a small representative data-sample. Different experiments on simulated and real datasets showed a good scalability of our proposed measure and a reliable validation compared to other existing measures when handling large scale data.},
  archive      = {J_PARCO},
  author       = {Chiheb-Eddine Ben Ncir and Abdallah Hamza and Waad Bouaguel},
  doi          = {10.1016/j.parco.2021.102751},
  journal      = {Parallel Computing},
  pages        = {102751},
  shortjournal = {Parallel Comput.},
  title        = {Parallel and scalable dunn index for the validation of big data clusters},
  volume       = {102},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Guest editorial: Virtual special issue on parallel matrix
algorithms and applications (PMAA’18). <em>PARCO</em>, <em>102</em>,
102720. (<a href="https://doi.org/10.1016/j.parco.2020.102720">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_PARCO},
  author       = {Olaf Schenk and Peter Arbenz and Luc Giraud and Wim Vanroose},
  doi          = {10.1016/j.parco.2020.102720},
  journal      = {Parallel Computing},
  pages        = {102720},
  shortjournal = {Parallel Comput.},
  title        = {Guest editorial: Virtual special issue on parallel matrix algorithms and applications (PMAA’18)},
  volume       = {102},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multiscale modeling and cinematic visualization of
photosynthetic energy conversion processes from electronic to cell
scales. <em>PARCO</em>, <em>102</em>, 102698. (<a
href="https://doi.org/10.1016/j.parco.2020.102698">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conversion of sunlight into chemical energy , namely photosynthesis, is the primary energy source of life on Earth. A visualization depicting this process, based on multiscale computational models from electronic to cell scales, is presented in the form of an excerpt from the fulldome show Birth of Planet Earth . This accessible visual narrative shows a lay audience, including children, how the energy of sunlight is captured, converted, and stored through a chain of proteins to power living cells. The visualization is the result of a multi-year collaboration among biophysicists, visualization scientists, and artists, which, in turn, is based on a decade-long experimental-computational collaboration on structural and functional modeling that produced an atomic detail description of a bacterial bioenergetic organelle, the chromatophore. Software advancements necessitated by this project have led to significant performance and feature advances, including hardware-accelerated cinematic ray tracing and instanced visualizations for efficient cell-scale modeling. The energy conversion steps depicted feature an integration of function from electronic to cell levels, spanning nearly 12 orders of magnitude in time scales. This atomic detail description uniquely enables a modern retelling of one of humanity’s earliest stories—the interplay between light and life.},
  archive      = {J_PARCO},
  author       = {Melih Sener and Stuart Levy and John E. Stone and AJ Christensen and Barry Isralewitz and Robert Patterson and Kalina Borkiewicz and Jeffrey Carpenter and C. Neil Hunter and Zaida Luthey-Schulten and Donna Cox},
  doi          = {10.1016/j.parco.2020.102698},
  journal      = {Parallel Computing},
  pages        = {102698},
  shortjournal = {Parallel Comput.},
  title        = {Multiscale modeling and cinematic visualization of photosynthetic energy conversion processes from electronic to cell scales},
  volume       = {102},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). HBPFP-DC: A parallel frequent itemset mining using spark.
<em>PARCO</em>, <em>101</em>, 102738. (<a
href="https://doi.org/10.1016/j.parco.2020.102738">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The frequent itemset mining (FIM) is one of the most important techniques to extract knowledge from data in many real-world applications. Facing big data applications , parallel and distributed solutions are widely studied. However, the frequent itemset mining process is a continuous iteration process . As an in-memory parallel execution model in which all data will be loaded into memory, Spark is especially beneficial for iterative calculations. In the study, we propose a HBPFP-DC (High Balanced Parallel Fp-Growth Considering Data Correlation) algorithm on the Spark platform. HBPFP-DC uses a newly defined node computation workload estimation model to realize the balanced grouping of the calculation tasks among computing nodes, so that each computing node can achieve a completely asynchronous frequent itemset mining only relying on their respective local projection datasets. And, in order to improve the ‘compression factor’ of the tree structure to boost mining efficiency, we consider the correlation among items when performing the above grouping process. Thereby, network and computing consumption by dividing similar items in the same group are significantly decreased. Finally, extensive experiments demonstrate that our proposed solution is efficient and scalable.},
  archive      = {J_PARCO},
  author       = {Yaling Xun and Jifu Zhang and Haifeng Yang and Xiao Qin},
  doi          = {10.1016/j.parco.2020.102738},
  journal      = {Parallel Computing},
  pages        = {102738},
  shortjournal = {Parallel Comput.},
  title        = {HBPFP-DC: A parallel frequent itemset mining using spark},
  volume       = {101},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A new scalable distributed k-means algorithm based on cloud
micro-services for high-performance computing. <em>PARCO</em>,
<em>101</em>, 102736. (<a
href="https://doi.org/10.1016/j.parco.2020.102736">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper aims to propose a distributed clustering method for High performance computing (HPC) models and, its application for medical image processing . The communication cost is one of the great challenges, which minimizes the scalability of parallel and distributed computing models. Indeed, it reduces significantly the performance of HPC systems where these models are assigned to be implemented. In this paper, we present a new distributed k-means method which integrates virtual parallel distributed computing model with a low communication cost mechanism. The k-means method is performed as a distributed service within a cooperative micro-services team which uses asynchronous communication mechanism based on AMQP protocol. We design and implement a parallel and distributed HPC application for MRI image segmentation assigned to be deployed on cloud. Experimental results show that the proposed method (DSCM) and its assigned model reach high degree of scalability. We expect this clustering approach to provide scalable HPC applications for big data clustering .},
  archive      = {J_PARCO},
  author       = {Fatéma Zahra Benchara and Mohamed Youssfi},
  doi          = {10.1016/j.parco.2020.102736},
  journal      = {Parallel Computing},
  pages        = {102736},
  shortjournal = {Parallel Comput.},
  title        = {A new scalable distributed k-means algorithm based on cloud micro-services for high-performance computing},
  volume       = {101},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Parallelization of network motif discovery using star
contraction. <em>PARCO</em>, <em>101</em>, 102734. (<a
href="https://doi.org/10.1016/j.parco.2020.102734">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network motifs are widely used to uncover structural design principles of complex networks. Current sequential network motif discovery algorithms become inefficient as motif size grows, thus parallelization methods have been proposed in the literature. In this study, we use star contraction algorithm to partition complex networks efficiently for parallel discovery of network motifs. We propose two new heuristics to make star contraction more suitable for partitioning of complex networks. The effectiveness of our partitioning strategies is verified using the ESU algorithm for subgraph counting. We also propose a ghost vertices detection algorithm to ensure that all the motifs located in multiple parts are exactly found. We implement our method using MPI libraries and tested on real-life complex networks of different domains. We compared speedups of star contraction algorithm with speedups of other graph partitioning algorithms. Our algorithm obtained better speedups than those of other partitioning algorithms for most cases. Our algorithm provides significant speedups when compared to sequential ESU algorithm allowing discovery of larger network motifs.},
  archive      = {J_PARCO},
  author       = {Esra Ruzgar Ateskan and Kayhan Erciyes and Mehmet Emin Dalkilic},
  doi          = {10.1016/j.parco.2020.102734},
  journal      = {Parallel Computing},
  pages        = {102734},
  shortjournal = {Parallel Comput.},
  title        = {Parallelization of network motif discovery using star contraction},
  volume       = {101},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A thread-adaptive sparse approximate inverse preconditioning
algorithm on multi-GPUs. <em>PARCO</em>, <em>101</em>, 102724. (<a
href="https://doi.org/10.1016/j.parco.2020.102724">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we present an efficient thread-adaptive sparse approximate inverse preconditioning algorithm on multiple GPUs , called GSPAI-Adaptive. For our proposed GSPAI-Adaptive, there are the following novelties: (1) a thread-adaptive allocation strategy is presented for each column of the preconditioner , and (2) a parallel framework of constructing the sparse approximate inverse preconditioner is proposed on multiple GPUs , and (3) each component of the preconditioner is computed in parallel inside a thread group of GPU. Experimental results show that GSPAI-Adaptive is effective, and is advantageous over the popular preconditioning algorithms in two public libraries, and a latest parallel sparse approximate inverse preconditioning algorithm.},
  archive      = {J_PARCO},
  author       = {Jiaquan Gao and Qi Chen and Guixia He},
  doi          = {10.1016/j.parco.2020.102724},
  journal      = {Parallel Computing},
  pages        = {102724},
  shortjournal = {Parallel Comput.},
  title        = {A thread-adaptive sparse approximate inverse preconditioning algorithm on multi-GPUs},
  volume       = {101},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Parallel branch and bound algorithm for solving integer
linear programming models derived from behavioral synthesis.
<em>PARCO</em>, <em>101</em>, 102722. (<a
href="https://doi.org/10.1016/j.parco.2020.102722">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Integer Linear Programming (ILP) formulation of behavioral synthesis allows hardware designers to implement efficient circuits considering resource and timing constraint. However, finding the optimal answer of ILP models is an NP-Hard problem and remains a computational challenge. In this paper, we address this challenge by developing two exact parallel branch and bound algorithms which are capable of solving large-scale ILP models derived from behavioral synthesis. The first algorithm enables sub-node parallelism as well as adaptive branching and memory efficient techniques to accelerate solving ILP models on shared memory multi-core systems. The second algorithm is developed based on node parallelism strategy. We evaluated the proposed algorithms using large ILP models derived from Media Bench Data Flow Graphs . The experimental results indicate both the proposed methods can successfully accelerate behavioral synthesis on multi-core platforms and outperforms IBM ILOG CPLEX (v12.60) MIP solver in solving large ILP models.},
  archive      = {J_PARCO},
  author       = {Mohammad K Fallah and Mahmood Fazlali},
  doi          = {10.1016/j.parco.2020.102722},
  journal      = {Parallel Computing},
  pages        = {102722},
  shortjournal = {Parallel Comput.},
  title        = {Parallel branch and bound algorithm for solving integer linear programming models derived from behavioral synthesis},
  volume       = {101},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Asynchronous parallel stochastic quasi-newton methods.
<em>PARCO</em>, <em>101</em>, 102721. (<a
href="https://doi.org/10.1016/j.parco.2020.102721">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although first-order stochastic algorithms , such as stochastic gradient descent , have been the main force to scale up machine learning models, such as deep neural nets, the second-order quasi-Newton methods start to draw attention due to their effectiveness in dealing with ill-conditioned optimization problems . The L-BFGS method is one of the most widely used quasi-Newton methods. We propose an asynchronous parallel algorithm for stochastic quasi-Newton (AsySQN) method. Unlike prior attempts, which parallelize only the calculation for gradient or the two-loop recursion of L-BFGS, our algorithm is the first one that truly parallelizes L-BFGS with a convergence guarantee. Adopting the variance reduction technique, a prior stochastic L-BFGS, which has not been designed for parallel computing , reaches a linear convergence rate. We prove that our asynchronous parallel scheme maintains the same linear convergence rate but achieves significant speedup. Empirical evaluations in both simulations and benchmark datasets demonstrate the speedup in comparison with the non-parallel stochastic L-BFGS, as well as the better performance than first-order methods in solving ill-conditioned problems.},
  archive      = {J_PARCO},
  author       = {Qianqian Tong and Guannan Liang and Xingyu Cai and Chunjiang Zhu and Jinbo Bi},
  doi          = {10.1016/j.parco.2020.102721},
  journal      = {Parallel Computing},
  pages        = {102721},
  shortjournal = {Parallel Comput.},
  title        = {Asynchronous parallel stochastic quasi-newton methods},
  volume       = {101},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Improved probabilistic i/o scheduling for limited-size
burst-buffers deployed HPC. <em>PARCO</em>, <em>101</em>, 102708. (<a
href="https://doi.org/10.1016/j.parco.2020.102708">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {I/O bottleneck is a critical problem in current High Performance Computing (HPC) systems which hinges the performance scalability of a system. Some techniques, such as I/O scheduling and Burst-Buffering, had been proposed to accelerate data exchange between the compute and storage components on HPC platforms. Probabilistic I/O scheduling, a Markov-chain-based hybrid method combined the above-mentioned two techniques, controls the data transmission considering the whole load states of the Burst-Buffers system to mitigate the I/O congestion caused by unpredictable concurrent I/O bursts. However, this method requires a large amount of computation to make online scheduling, resulting in significant wastage of computing resources and decreased efficiency in scheduling. In this paper, we first introduce the architecture of Burst-Buffers deployed HPC platform, the probabilistic execution model of applications, and the basic probabilistic I/O scheduling method with a proof of its efficiency based on the Markov-chain framework. Then, we propose the modularization technique, as the first improvement, to reduce the repeated computation by isolating the heuristic application selection module from the original method and reusing the application ranking result to adjust the I/O scheduling. Next, we propose the thresholding technique, as the second improvement, to reduce the number of data transferring on burst-buffers by considering the write amplification characteristic of the underlying storage devices. Finally, we conduct extensive simulation experiments to show that our proposed I/O scheduling methods outperform the existing I/O scheduling methods without introducing burst-buffers states and without considering the characteristics of storage devices.},
  archive      = {J_PARCO},
  author       = {Benbo Zha and Hong Shen},
  doi          = {10.1016/j.parco.2020.102708},
  journal      = {Parallel Computing},
  pages        = {102708},
  shortjournal = {Parallel Comput.},
  title        = {Improved probabilistic I/O scheduling for limited-size burst-buffers deployed HPC},
  volume       = {101},
  year         = {2021},
}
</textarea>
</details></li>
</ul>

</body>
</html>
