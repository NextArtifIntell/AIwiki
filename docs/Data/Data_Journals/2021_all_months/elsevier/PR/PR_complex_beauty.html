<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>PR_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="pr---506">PR - 506</h2>
<ul>
<li><details>
<summary>
(2021). GAMI-net: An explainable neural network based on generalized
additive models with structured interactions. <em>PR</em>, <em>120</em>,
108192. (<a href="https://doi.org/10.1016/j.patcog.2021.108192">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The lack of interpretability is an inevitable problem when using neural network models in real applications. In this paper, an explainable neural network based on generalized additive models with structured interactions (GAMI-Net) is proposed to pursue a good balance between prediction accuracy and model interpretability. GAMI-Net is a disentangled feedforward network with multiple additive subnetworks ; each subnetwork consists of multiple hidden layers and is designed for capturing one main effect or one pairwise interaction. Three interpretability aspects are further considered, including a) sparsity , to select the most significant effects for parsimonious representations; b) heredity, a pairwise interaction could only be included when at least one of its parent main effects exists; and c) marginal clarity, to make main effects and pairwise interactions mutually distinguishable. An adaptive training algorithm is developed, where main effects are first trained and then pairwise interactions are fitted to the residuals. Numerical experiments on both synthetic functions and real-world datasets show that the proposed model enjoys superior interpretability and it maintains competitive prediction accuracy in comparison to the explainable boosting machine and other classic machine learning models.},
  archive      = {J_PR},
  author       = {Zebin Yang and Aijun Zhang and Agus Sudjianto},
  doi          = {10.1016/j.patcog.2021.108192},
  journal      = {Pattern Recognition},
  pages        = {108192},
  shortjournal = {Pattern Recognition},
  title        = {GAMI-net: An explainable neural network based on generalized additive models with structured interactions},
  volume       = {120},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A multi-task fully deep convolutional neural network for
contactless fingerprint minutiae extraction. <em>PR</em>, <em>120</em>,
108189. (<a href="https://doi.org/10.1016/j.patcog.2021.108189">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the outbreak and wide spread of novel coronavirus (COVID-19), contactless fingerprint recognition has attracted more attention for personal recognition because it can provide significantly higher user convenience and hygiene than the traditional contact-based fingerprint recognition. However, it is still challenging to achieve a highly accurate recognition due to the low ridge-valley contrast and pose variances of contactless fingerprints. Minutiae points are a kind of ridge flow discontinuities, and robust and accurate extraction is an important step for most automatic fingerprint recognition algorithms. Most of existing methods are based on two stages which locate the minutiae points first and then compute their directions. The two-stage method cannot make full use of location and direction information. In this paper, we propose a multi-task fully deep convolutional neural network for jointly learning the minutiae location detection and its corresponding direction computation which operates directly on the whole gray scale contactless fingerprints. The proposed method consists of offline training and online testing stages. In the training stage, a fully deep convolutional neural network is built for the tasks of minutiae detection and its direction regression, with an attention mechanism to make the direction regression branch concentrate on the minutiae points. A new loss function is proposed to jointly learn the tasks of minutiae detection and its direction regression from the whole fingerprints. In the testing stage, the trained network is applied on the whole contactless fingerprint to generate the minutiae location and direction maps. The proposed multi-task leaning method performs better than the individual single task and it operates directly on the raw gray-scale contactless fingerprints without preprocessing. The results on three contactless fingerprint datasets show the proposed algorithm performs better than other minutiae extraction algorithms and the commercial software.},
  archive      = {J_PR},
  author       = {Zhao Zhang and Shuxin Liu and Manhua Liu},
  doi          = {10.1016/j.patcog.2021.108189},
  journal      = {Pattern Recognition},
  pages        = {108189},
  shortjournal = {Pattern Recognition},
  title        = {A multi-task fully deep convolutional neural network for contactless fingerprint minutiae extraction},
  volume       = {120},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Knowledge memorization and generation for action recognition
in still images. <em>PR</em>, <em>120</em>, 108188. (<a
href="https://doi.org/10.1016/j.patcog.2021.108188">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human action recognition in visual data is one of the most fundamental challenges in computer vision . Existing approaches for this primary goal have been based on video data, often incorporating both color and dynamic flow information. Nevertheless, the majority of the visual data constitute still images, and for this reason, being able to recognize actions in still image is an ultimate objective of visual understanding with an extended list of applications. In this paper, we present a novel method that transfers the knowledge learned from action videos onto images to allow recognition of the principal action depicted in still image. Our intuition is that a generative model for knowledge transfer can be learned by taking advantage of the available action videos in the training stage to bridge images to videos. Based on this, we propose two complementary knowledge-transfer models utilizing fully connected networks to deliver the knowledge extracted from color and motion flow sequences to still images. We introduce a weighted reconstruction and classification loss to steer the generation procedure of the networks. In addition, we describe and analyze the influence of different data augmentation techniques, initialization strategies, and weighting coefficients for improving the performance. We observe that: both the transferred knowledge from color sequences and motion flow sequences can improve the performance of still image based human action recognition; the latter one which provides complementary dynamic information improves the performance a lot. We evaluate our models on two publicly available video based human action recognition datasets: UCF101 and HMDB51. To further validate the generalization ability of the proposed solution, we test the learned models from UCF101 dataset on two still image based human action recognition benchmarks: Willow7 Actions and the Sports. Our results demonstrate that the proposed method outperforms the baseline approaches with more than 2\% accuracy, 3\% accuracy, 3\% accuracy and 5\% mAP on UCF101, HMDB51, Sports and Willow 7 Actions datasets, respectively.},
  archive      = {J_PR},
  author       = {Jian Dong and Wankou Yang and Yazhou Yao and Fatih Porikli},
  doi          = {10.1016/j.patcog.2021.108188},
  journal      = {Pattern Recognition},
  pages        = {108188},
  shortjournal = {Pattern Recognition},
  title        = {Knowledge memorization and generation for action recognition in still images},
  volume       = {120},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Discretization-aware architecture search. <em>PR</em>,
<em>120</em>, 108186. (<a
href="https://doi.org/10.1016/j.patcog.2021.108186">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The search cost of neural architecture search (NAS) has been largely reduced by differentiable architecture search and weight-sharing methods. Such methods optimize a super-network with all possible edges and operations, and determine the optimal sub-network by discretization , i.e. , pruning off operations/edges of small weights. However, the discretization process performed on either operations or edges incurs significant inaccuracy and thus the quality of the architecture is not guaranteed. In this paper, we propose discretization-aware architecture search (DA 2 S), and target at pushing the super-network towards the configuration of desired topology. DA 2 S is implemented with an entropy-based loss term, which can be regularized to differentiable architecture search in a plug-and-play fashion. The regularization is controlled by elaborated continuation functions, so that discretization is adaptive to the dynamic change of edges and operations. Experiments on standard image classification benchmarks demonstrate the effectiveness of our approach, in particular, under imbalanced network configurations that were not studied before. Code is available at github.com/sunsmarterjie/DAAS .},
  archive      = {J_PR},
  author       = {Yunjie Tian and Chang Liu and Lingxi Xie and Jianbin jiao and Qixiang Ye},
  doi          = {10.1016/j.patcog.2021.108186},
  journal      = {Pattern Recognition},
  pages        = {108186},
  shortjournal = {Pattern Recognition},
  title        = {Discretization-aware architecture search},
  volume       = {120},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Mixability of integral losses: A key to efficient online
aggregation of functional and probabilistic forecasts. <em>PR</em>,
<em>120</em>, 108175. (<a
href="https://doi.org/10.1016/j.patcog.2021.108175">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we extend the setting of the online prediction with expert advice to function-valued forecasts. At each step of the online game several experts predict a function, and the learner has to efficiently aggregate these functional forecasts into a single forecast. We adapt basic mixable (and exponentially concave) loss functions to compare functional predictions and prove that these adaptations are also mixable (exp-concave). We call this phenomenon mixability (exp-concavity) of integral loss functions. As an application of our main result, we prove that various loss functions used for probabilistic forecasting are mixable (exp-concave). The considered losses include Sliced Continuous Ranked Probability Score, Energy-Based Distance, Optimal Transport Costs &amp; Sliced Wasserstein-2 distance, Beta-2 &amp; Kullback-Leibler divergences, Characteristic function and Maximum Mean Discrepancies.},
  archive      = {J_PR},
  author       = {Alexander Korotin and Vladimir V’yugin and Evgeny Burnaev},
  doi          = {10.1016/j.patcog.2021.108175},
  journal      = {Pattern Recognition},
  pages        = {108175},
  shortjournal = {Pattern Recognition},
  title        = {Mixability of integral losses: A key to efficient online aggregation of functional and probabilistic forecasts},
  volume       = {120},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Graph representation learning for road type classification.
<em>PR</em>, <em>120</em>, 108174. (<a
href="https://doi.org/10.1016/j.patcog.2021.108174">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel learning-based approach to graph representations of road networks employing state-of-the-art graph convolutional neural networks . Our approach is applied to realistic road networks of 17 cities from Open Street Map. While edge features are crucial to generate descriptive graph representations of road networks, graph convolutional networks usually rely on node features only. We show that the highly representative edge features can still be integrated into such networks by applying a line graph transformation. We also propose a method for neighborhood sampling based on a topological neighborhood composed of both local and global neighbors. We compare the performance of learning representations using different types of neighborhood aggregation functions in transductive and inductive tasks and in supervised and unsupervised learning . Furthermore, we propose a novel aggregation approach, Graph Attention Isomorphism Network, GAIN 1 1 . Our results show that GAIN outperforms state-of-the-art methods on the road type classification problem.},
  archive      = {J_PR},
  author       = {Zahra Gharaee and Shreyas Kowshik and Oliver Stromann and Michael Felsberg},
  doi          = {10.1016/j.patcog.2021.108174},
  journal      = {Pattern Recognition},
  pages        = {108174},
  shortjournal = {Pattern Recognition},
  title        = {Graph representation learning for road type classification},
  volume       = {120},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Effective action recognition with embedded key point shifts.
<em>PR</em>, <em>120</em>, 108172. (<a
href="https://doi.org/10.1016/j.patcog.2021.108172">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal feature extraction is an essential technique in video-based action recognition. Key points have been utilized in skeleton-based action recognition methods but they require costly key point annotation. In this paper, we propose a novel temporal feature extraction module, named Key Point Shifts Embedding Module ( K P S E M KPSEM ), to adaptively extract channel-wise key point shifts across video frames without key point annotation. Key points are adaptively extracted as feature points with maximum feature values at split regions and key point shifts are the spatial displacements of corresponding key points. The key point shifts are encoded as the overall temporal features via linear embedding layers in a multi-set manner. Our method achieves competitive performance through embedding key point shifts with trivial computational cost, achieving the state-of-the-art performance of 78.81\% 78.81\% on Mini-Kinetics and competitive performance on UCF101, Something-Something-v1 and HMDB51 datasets.},
  archive      = {J_PR},
  author       = {Haozhi Cao and Yuecong Xu and Jianfei Yang and Kezhi Mao and Jianxiong Yin and Simon See},
  doi          = {10.1016/j.patcog.2021.108172},
  journal      = {Pattern Recognition},
  pages        = {108172},
  shortjournal = {Pattern Recognition},
  title        = {Effective action recognition with embedded key point shifts},
  volume       = {120},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A two-level framework for place recognition with 3D LiDAR
based on spatial relation graph. <em>PR</em>, <em>120</em>, 108171. (<a
href="https://doi.org/10.1016/j.patcog.2021.108171">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of robotics, due to the complexity of real environments, place recognition using the 3D LiDAR is always a challenging problem. The spatial relations of internal structures underlying the LiDAR data from different places are distinguishable, which can be used to describe the environment. In this paper, we utilize the spatial relations of internal structures and propose a two-level framework for 3D LiDAR place recognition based on the spatial relation graph (SRG). At first, the proposed framework segments the point cloud into multiple clusters, then the features of the clusters and the spatial relation descriptors (SRDs) between the clusters are extracted, and the point cloud is represented by the SRG, which uses the clusters as the nodes and their spatial relations as the edges. After that, we propose a two-level matching model in which two different models are fused for accurately and efficiently matching the SRGs, including the upper-level searching model (U-LSM) and lower-level matching model (L-LMM). In the U-LSM, an incremental bag-of-words model is used to search for candidate SRGs through the distribution of the SRDs in the SRG. In the L-LMM, we utilize the improved spectral method to calculate similarities between the current SRG and the candidates. The experimental results demonstrate that our framework achieves good precision, recall and viewpoint robustness on both public benchmarks and self-built campus dataset.},
  archive      = {J_PR},
  author       = {Yansong Gong and Fengchi Sun and Jing Yuan and Wenbin Zhu and Qinxuan Sun},
  doi          = {10.1016/j.patcog.2021.108171},
  journal      = {Pattern Recognition},
  pages        = {108171},
  shortjournal = {Pattern Recognition},
  title        = {A two-level framework for place recognition with 3D LiDAR based on spatial relation graph},
  volume       = {120},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Manifold learning with structured subspace for multi-label
feature selection. <em>PR</em>, <em>120</em>, 108169. (<a
href="https://doi.org/10.1016/j.patcog.2021.108169">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, multi-label learning is ubiquitous in practical applications, in which multi-label data is always confronted with the curse of high-dimensional features. Feature selection has been shown to effectively improve learning performance by selecting discriminative features . Conventional multi-label feature selection only focuses on associating input features with corresponding labels while neglecting the potential structural information, i.e., instance correlations and label correlations. To tackle this problem, we propose manifold learning with structured subspace for multi-label feature selection. Specifically, we first uncover a latent subspace for a more compact and accurate data representation, and take advantage of the subspace to explore the correlations among instances. Then, we explore label correlations in manifold learning to guarantee the global and local structural consistency of labels. Besides, l 2 , 1 l2,1 -norm is introduced into loss function and sparse regularization to facilitate feature selection process. A detail optimization algorithm is presented to solve the objective function of the proposed method. Extensive experiments on real-world data show the superiority of the proposed method under various metrics.},
  archive      = {J_PR},
  author       = {Yuling Fan and Jinghua Liu and Peizhong Liu and Yongzhao Du and Weiyao Lan and Shunxiang Wu},
  doi          = {10.1016/j.patcog.2021.108169},
  journal      = {Pattern Recognition},
  pages        = {108169},
  shortjournal = {Pattern Recognition},
  title        = {Manifold learning with structured subspace for multi-label feature selection},
  volume       = {120},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Progressive global perception and local polishing network
for lung infection segmentation of COVID-19 CT images. <em>PR</em>,
<em>120</em>, 108168. (<a
href="https://doi.org/10.1016/j.patcog.2021.108168">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a progressive global perception and local polishing (PCPLP) network is proposed to automatically segment the COVID-19-caused pneumonia infections in computed tomography (CT) images. The proposed PCPLP follows an encoder-decoder architecture. Particularly, the encoder is implemented as a computationally efficient fully convolutional network (FCN). In this study, a multi-scale multi-level feature recursive aggregation (mmFRA) network is used to integrate multi-scale features (viz. global guidance features and local refinement features) with multi-level features (viz. high-level semantic features, middle-level comprehensive features, and low-level detailed features). Because of this innovative aggregation of features, an edge-preserving segmentation map can be produced in a boundary-aware multiple supervision (BMS) way. Furthermore, both global perception and local perception are devised. On the one hand, a global perception module (GPM) providing a holistic estimation of potential lung infection regions is employed to capture more complementary coarse-structure information from different pyramid levels by enlarging the receptive fields without substantially increasing the computational burden. On the other hand, a local polishing module (LPM), which provides a fine prediction of the segmentation regions, is applied to explicitly heighten the fine-detail information and reduce the dilution effect of boundary knowledge. Comprehensive experimental evaluations demonstrate the effectiveness of the proposed PCPLP in boosting the learning ability to identify the lung infected regions with clear contours accurately. Our model is superior remarkably to the state-of-the-art segmentation models both quantitatively and qualitatively on a real CT dataset of COVID-19.},
  archive      = {J_PR},
  author       = {Nan Mu and Hongyu Wang and Yu Zhang and Jingfeng Jiang and Jinshan Tang},
  doi          = {10.1016/j.patcog.2021.108168},
  journal      = {Pattern Recognition},
  pages        = {108168},
  shortjournal = {Pattern Recognition},
  title        = {Progressive global perception and local polishing network for lung infection segmentation of COVID-19 CT images},
  volume       = {120},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Training object detectors from few weakly-labeled and many
unlabeled images. <em>PR</em>, <em>120</em>, 108164. (<a
href="https://doi.org/10.1016/j.patcog.2021.108164">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weakly-supervised object detection attempts to limit the amount of supervision by dispensing the need for bounding boxes , but still assumes image-level labels on the entire training set. In this work, we study the problem of training an object detector from one or few images with image-level labels and a larger set of completely unlabeled images. This is an extreme case of semi-supervised learning where the labeled data are not enough to bootstrap the learning of a detector. Our solution is to train a weakly-supervised student detector model from image-level pseudo-labels generated on the unlabeled set by a teacher classifier model , bootstrapped by region-level similarities to labeled images. Building upon the recent representative weakly-supervised pipeline PCL [1], our method can use more unlabeled images to achieve performance competitive or superior to many recent weakly-supervised detection solutions. Code will be made available at https://github.com/zhaohui-yang/NSOD.},
  archive      = {J_PR},
  author       = {Zhaohui Yang and Miaojing Shi and Chao Xu and Vittorio Ferrari and Yannis Avrithis},
  doi          = {10.1016/j.patcog.2021.108164},
  journal      = {Pattern Recognition},
  pages        = {108164},
  shortjournal = {Pattern Recognition},
  title        = {Training object detectors from few weakly-labeled and many unlabeled images},
  volume       = {120},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Automated delineation of corneal layers on OCT images using
a boundary-guided CNN. <em>PR</em>, <em>120</em>, 108158. (<a
href="https://doi.org/10.1016/j.patcog.2021.108158">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate segmentation of corneal layers depicted on optical coherence tomography (OCT) images is very helpful for quantitatively assessing and diagnosing corneal diseases ( e.g. , keratoconus and dry eye). In this study, we presented a novel boundary-guided convolutional neural network (CNN) architecture (BG-CNN) to simultaneously extract different corneal layers and delineate their boundaries. The developed BG-CNN architecture used three convolutional blocks to construct two network modules on the basis of the classical U-Net network. We trained and validated the network on a dataset consisting of 1,712 OCT images acquired on 121 subjects using a 10-fold cross-validation method. Our experiments showed an average dice similarity coefficient (DSC) of 0.9691, an intersection over union (IOU) of 0.9411, and a Hausdorff distance (HD) of 7.4423 pixels. Compared with several other classical networks, namely U-Net, Attention U-Net, Asymmetric U-Net, BiO-Net, CE-Net, CPFnte, M-Net, and Deeplabv3, on the same dataset, the developed network demonstrated a promising performance, suggesting its unique strength in segmenting corneal layers depicted on OCT images.},
  archive      = {J_PR},
  author       = {Lei Wang and Meixiao Shen and Qian Chang and Ce Shi and Yang Chen and Yuheng Zhou and Yanchun Zhang and Jiantao Pu and Hao Chen},
  doi          = {10.1016/j.patcog.2021.108158},
  journal      = {Pattern Recognition},
  pages        = {108158},
  shortjournal = {Pattern Recognition},
  title        = {Automated delineation of corneal layers on OCT images using a boundary-guided CNN},
  volume       = {120},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Part-guided graph convolution networks for person
re-identification. <em>PR</em>, <em>120</em>, 108155. (<a
href="https://doi.org/10.1016/j.patcog.2021.108155">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, part-based deep models have achieved promising performance in person re-identification (Re-ID), yet these models ignore the inter-local relationship of the corresponding parts among pedestrian images and the intra-local relationship between adjacent parts in one pedestrian image. As a result, the feature representations are hard to learn the information from the same parts of other pedestrian images and are lack of the contextual information of pedestrian. In this paper, we propose a novel deep graph model named Part-Guided Graph Convolution Network (PGCN) for person Re-ID, which could simultaneously learn the inter-local relationship and the intra-local relationship for feature representations. Specifically, we construct the inter-local graph using the local features extracted from the same parts of pedestrian images and build the adjacency matrix using the similarity so as to mine the inter-local relationship. Meanwhile, we construct the intra-local graph using the local features extracted from different body parts in one pedestrian image, and propose the fractional dynamic mechanism (FDM) to accurately describe the correlations between adjacent parts in the optimization process. Finally, after the graph convolutional operation, the inter-local relationship and the intra-local relationship are injected into the feature representations of pedestrian images. Extensive experiments are conducted on Market-1501, CUHK03, DukeMTMC-reID and MSMT17, and the experimental results show the proposed PGCN exceeds state-of-the-art methods by an overwhelming margin.},
  archive      = {J_PR},
  author       = {Zhong Zhang and Haijia Zhang and Shuang Liu and Yuan Xie and Tariq S. Durrani},
  doi          = {10.1016/j.patcog.2021.108155},
  journal      = {Pattern Recognition},
  pages        = {108155},
  shortjournal = {Pattern Recognition},
  title        = {Part-guided graph convolution networks for person re-identification},
  volume       = {120},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Selection of diverse features with a diverse regularization.
<em>PR</em>, <em>120</em>, 108154. (<a
href="https://doi.org/10.1016/j.patcog.2021.108154">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many embedded feature selection methods ignore the correlation among the important features. To reduce correlation, some models introduce constraints to impose sparsity on features, some try to exploit the similarity and group features without changing the objective function. In this paper, we propose diverse feature selection (DFS), which simultaneously performs feature clustering and selection. Given a dataset with known class labels, we separate the features into a set of feature clusters where the features in the same cluster have a higher correlation with each other than with the features in different clusters. A diverse regularization (DR) is proposed to reduce the linear and nonlinear correlations among important features. Using this regularization , DFS can select features that are both informative and diverse. The experimental results on seven image datasets, five gene datasets as well as four other datasets demonstrate the superior performance of DFS.},
  archive      = {J_PR},
  author       = {Weichan Zhong and Xiaojun Chen and Qingyao Wu and Min Yang and Joshua Zhexue Huang},
  doi          = {10.1016/j.patcog.2021.108154},
  journal      = {Pattern Recognition},
  pages        = {108154},
  shortjournal = {Pattern Recognition},
  title        = {Selection of diverse features with a diverse regularization},
  volume       = {120},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Knowledge base graph embedding module design for visual
question answering model. <em>PR</em>, <em>120</em>, 108153. (<a
href="https://doi.org/10.1016/j.patcog.2021.108153">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a knowledge base graph embedding module is constructed to extend the versatility of knowledge-based VQA (Visual Question Answering) models. The knowledge base graph embedding module constructed in this paper extracts core entities from images and text, and maps them as knowledge base entities, then extracts the sub-graphs closely related to the core entities, and converts the sub-graphs into low-dimensional vectors to realize sub-graph embedding. In order to achieve good subgraph embedding, we first extracted two experimental knowledge bases with rich semantics from DBpedia: DBV and DBA. Based on these two knowledge bases, this paper selects several excellent models in knowledge base embedding as test models, including SE (structured embedding),SME(semantic matching energy function), and TransE model to produce link prediction. The results show that there is a clear correspondence between the entities of the DBV, which can achieve excellent node embedding. And the TransE model can achieve a good knowledge base embedding, so we built the knowledge base graph embedding module based on TransE. And then we construct a VQA model ( KBSN ) based on the knowledge base graph embedding. Experimental results on VQA2.0 and KB-VQA data sets prove that the knowledge base graph embedding module improves the accuracy.},
  archive      = {J_PR},
  author       = {Wenfeng Zheng and Lirong Yin and Xiaobing Chen and Zhiyang Ma and Shan Liu and Bo Yang},
  doi          = {10.1016/j.patcog.2021.108153},
  journal      = {Pattern Recognition},
  pages        = {108153},
  shortjournal = {Pattern Recognition},
  title        = {Knowledge base graph embedding module design for visual question answering model},
  volume       = {120},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Weighted clustering: Towards solving the user’s dilemma.
<em>PR</em>, <em>120</em>, 108152. (<a
href="https://doi.org/10.1016/j.patcog.2021.108152">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper makes a major step towards addressing a long-standing challenge in cluster analysis, known as the user’s dilemma , which is the problem of selecting an appropriate clustering algorithm for a specific task. A formal approach for addressing this challenge relies on the identification of succinct, user-friendly properties that capture formal differences amongst clustering techniques . While helpful for gaining insight into the nature of clustering paradigms, there is a theory-practice gap that has so far limited the utility of this approach: Formal properties typically highlight advantages of classical linkage-based algorithms, while practical experience shows that center-based methods are preferable for many applications. We present simple new properties that delineate core differences between common clustering paradigms and overcome this theory-practice gap. The properties we present give a formal understanding of the advantages of center-based approaches for some applications and insight into when different clustering paradigms should be used. These properties address how sensitive algorithms are to changes in element frequencies, which we capture in a generalized setting where every element is associated with a real-valued weight. To complement extensive formal analysis, we discuss how these properties can be applied in practice.},
  archive      = {J_PR},
  author       = {Margareta Ackerman and Shai Ben-David and Simina Brânzei and David Loker},
  doi          = {10.1016/j.patcog.2021.108152},
  journal      = {Pattern Recognition},
  pages        = {108152},
  shortjournal = {Pattern Recognition},
  title        = {Weighted clustering: Towards solving the user’s dilemma},
  volume       = {120},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Conditional information gain networks as sparse mixture of
experts. <em>PR</em>, <em>120</em>, 108151. (<a
href="https://doi.org/10.1016/j.patcog.2021.108151">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural network models owe their representational power and high performance in classification tasks to the high number of learnable parameters. Running deep neural network models in limited-resource environments is a problematic task. Models employing conditional computing aim to reduce the computational burden while retaining model performance on par with more complex neural network models . This paper, proposes a new model, Conditional Information Gain Networks as Sparse Mixture of Experts (sMoE-CIGNs). A CIGN model is a neural tree that allows conditionally skipping parts of the tree based on routing mechanisms inserted into the architecture. These routing mechanisms are based on differentiable Information Gain objectives. CIGN groups semantically similar samples in the leaves, enabling simpler classifiers to focus on differentiating between similar classes. This lets the CIGN model attain high classification performances with lighter models. We further improve the basic CIGN model by proposing a sparse mixture of experts model for difficult to classify samples that may get routed to suboptimal branches. If a sample has routing confidence higher than a specific threshold, the sample may be routed towards multiple child nodes. The classification decision can then be taken as a mixture of these expert decisions. We learn the optimal routing thresholds by Bayesian Optimization over a validation set by minimizing a weighted loss, including the classification accuracy and the number of multiplication and accumulations (MAC). We show the effectiveness of the CIGN models enhanced with the Sparse Mixture of Experts approach with extensive tests on MNIST, Fashion MNIST, CIFAR 100 and UCI-USPS datasets, as well as comparisons with methods from the literature. sMoE-CIGN models can retain high generalization performance , on par with a thick unconditional model while keeping the operation burden at the same level with a much thinner model. 1},
  archive      = {J_PR},
  author       = {Ufuk Can Bicici and Lale Akarun},
  doi          = {10.1016/j.patcog.2021.108151},
  journal      = {Pattern Recognition},
  pages        = {108151},
  shortjournal = {Pattern Recognition},
  title        = {Conditional information gain networks as sparse mixture of experts},
  volume       = {120},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-label feature selection via manifold regularization
and dependence maximization. <em>PR</em>, <em>120</em>, 108149. (<a
href="https://doi.org/10.1016/j.patcog.2021.108149">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection is able to select more discriminative features for classification and plays an important role in multi-label learning to alleviate the effect of the curse of dimensionality. Recently, the multi-label feature selection methods based on the sparse regression model have received increasing attentions. However, most of these methods directly project original data space to label space in the regression model, which is inappropriate because the linear assumption between data space and label space doesn&#39;t hold in most cases. In the paper, we propose a feature selection method named multi-label feature selection via manifold regularization and dependence maximization (MRDM). In the regression model of MRDM, the original data space is projected to a low-dimensional manifold space, which not only has the same topological structure with the original data, but also has a strong dependence with the class labels. Then, an objective function involving l 2 , 1 l2,1 -norm regularization is formulated, and an alternating optimization-based iterative algorithm is designed to obtain the sparse coefficients for multi-label feature selection. Extensive experiments on various multi-label data sets demonstrate the superiority of the proposed method compared with some state-of-the-art multi-label feature selection methods.},
  archive      = {J_PR},
  author       = {Rui Huang and Zhejun Wu},
  doi          = {10.1016/j.patcog.2021.108149},
  journal      = {Pattern Recognition},
  pages        = {108149},
  shortjournal = {Pattern Recognition},
  title        = {Multi-label feature selection via manifold regularization and dependence maximization},
  volume       = {120},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Improving the accuracy of global forecasting models using
time series data augmentation. <em>PR</em>, <em>120</em>, 108148. (<a
href="https://doi.org/10.1016/j.patcog.2021.108148">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Forecasting models that are trained across sets of many time series, known as Global Forecasting Models (GFM), have shown recently promising results in forecasting competitions and real-world applications, outperforming many state-of-the-art univariate forecasting techniques. In most cases, GFMs are implemented using deep neural networks , and in particular Recurrent Neural Networks (RNN), which require a sufficient amount of time series to estimate their numerous model parameters. However, many time series databases have only a limited number of time series. In this study, we propose a novel, data augmentation based forecasting framework that is capable of improving the baseline accuracy of the GFM models in less data-abundant settings. We use three time series augmentation techniques: GRATIS, moving block bootstrap (MBB), and dynamic time warping barycentric averaging (DBA) to synthetically generate a collection of time series. The knowledge acquired from these augmented time series is then transferred to the original dataset using two different approaches: the pooled approach and the transfer learning approach. When building GFMs, in the pooled approach, we train a model on the augmented time series alongside the original time series dataset, whereas in the transfer learning approach, we adapt a pre-trained model to the new dataset. In our evaluation on competition and real-world time series datasets, our proposed variants can significantly improve the baseline accuracy of GFM models and outperform state-of-the-art univariate forecasting methods.},
  archive      = {J_PR},
  author       = {Kasun Bandara and Hansika Hewamalage and Yuan-Hao Liu and Yanfei Kang and Christoph Bergmeir},
  doi          = {10.1016/j.patcog.2021.108148},
  journal      = {Pattern Recognition},
  pages        = {108148},
  shortjournal = {Pattern Recognition},
  title        = {Improving the accuracy of global forecasting models using time series data augmentation},
  volume       = {120},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A unified weight learning and low-rank regression model for
robust complex error modeling. <em>PR</em>, <em>120</em>, 108147. (<a
href="https://doi.org/10.1016/j.patcog.2021.108147">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the most important problems in regression-based error model is modeling the complex representation error caused by various corruptions and environment changes in images. For example, in robust face recognition, images are often affected by varying types and levels of corruptions, such as random pixel corruptions, block occlusions, or disguises. However, existing works are not robust enough to solve this problem due to they cannot model the complex corrupted errors very well. In this paper, we address this problem by a unified sparse weight learning and low-rank approximation regression model, which enables the random noises and contiguous occlusions in images to be treated simultaneously. For the random noise, we define a generalized correntropy (GC) function to match the error distribution. For the structured error caused by occlusions or disguises, we propose a GC function based rank approximation to measure the rank of error matrices. Since the proposed objective function is non-convex, an effective iterative optimization algorithm is developed to achieve the optimal weight learning and low-rank approximation. Extensive experimental results on three public face databases show that the proposed model can fit the error distribution and structure very well, thus obtain better recognition accuracies in comparison with the existing methods.},
  archive      = {J_PR},
  author       = {Miaohua Zhang and Yongsheng Gao and Jun Zhou},
  doi          = {10.1016/j.patcog.2021.108147},
  journal      = {Pattern Recognition},
  pages        = {108147},
  shortjournal = {Pattern Recognition},
  title        = {A unified weight learning and low-rank regression model for robust complex error modeling},
  volume       = {120},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Generalized pyramid co-attention with learnable aggregation
net for video question answering. <em>PR</em>, <em>120</em>, 108145. (<a
href="https://doi.org/10.1016/j.patcog.2021.108145">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video based visual question answering (V-VQA) remains challenging at the intersection of vision and language. In this paper, we propose a novel architecture, namely Generalized Pyramid Co-attention with Learnable Aggregation Net (GPC) to address two central problems: 1) how to deploy co-attention to V-VQA task considering the complex and diverse content of videos; and 2) how to aggregate the frame-level features (or word-level features) without destroying the feature distributions and temporal information. To solve the first problem, we propose a Generalized Pyramid Co-attention structure with a novel diversity learning module to explicitly encourage attention accuracy and diversity. And we first instantiate it into a Multi-path Pyramid Co-attention (MPC) to capture diverse feature. Then we find each attention branch of original co-attention mechanism does not interact with the others, which results in coarse attention maps. So we extend the MPC structure to a Cascaded Pyramid Transformer Co-attention (CPTC) module in which we replace co-attention with transformer co-attention. To solve the second problem, we propose a new learnable aggregation method with a set of evidence gates. It automatically aggregates adaptively-weighted frame-level features (or word-level features) to extract rich video (or question) context semantic information. With evidence gates, it then further chooses the most related signals representing the evidence information to predict the answer. Extensive validations on the two V-VQA datasets, TGIF-QA and TVQA show that both our proposed MPC and CPTC achieve the state-of-the-art performance and CPTC performs better under various settings and metrics. Code and model have been released at: https://github.com/lixiangpengcs/LAD-Net-for-VideoQA .},
  archive      = {J_PR},
  author       = {Lianli Gao and Tangming Chen and Xiangpeng Li and Pengpeng Zeng and Lei Zhao and Yuan-Fang Li},
  doi          = {10.1016/j.patcog.2021.108145},
  journal      = {Pattern Recognition},
  pages        = {108145},
  shortjournal = {Pattern Recognition},
  title        = {Generalized pyramid co-attention with learnable aggregation net for video question answering},
  volume       = {120},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Explainable boosted linear regression for time series
forecasting. <em>PR</em>, <em>120</em>, 108144. (<a
href="https://doi.org/10.1016/j.patcog.2021.108144">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time series forecasting involves collecting and analyzing past observations to develop a model to extrapolate such observations into the future. Forecasting of future events is important in many fields to support decision making as it contributes to reducing the future uncertainty. We propose explainable boosted linear regression (EBLR) algorithm for time series forecasting, which is an iterative method that starts with a base model, and explains the model’s errors through regression trees. At each iteration, the path leading to highest error is added as a new variable to the base model. In this regard, our approach can be considered as an improvement over general time series models since it enables incorporating nonlinear features by residual explanation. More importantly, use of the single rule that contributes to the error most enables access to interpretable results. The proposed approach extends to probabilistic forecasting through generating prediction intervals based on the empirical error distribution. We conduct a detailed numerical study with EBLR and compare against various other approaches. We observe that EBLR substantially improves the base model performance through extracted features, and provide a comparable performance to other well established approaches. The interpretability of the model predictions and high predictive accuracy of EBLR makes it a promising method for time series forecasting.},
  archive      = {J_PR},
  author       = {Igor Ilic and Berk Görgülü and Mucahit Cevik and Mustafa Gökçe Baydoğan},
  doi          = {10.1016/j.patcog.2021.108144},
  journal      = {Pattern Recognition},
  pages        = {108144},
  shortjournal = {Pattern Recognition},
  title        = {Explainable boosted linear regression for time series forecasting},
  volume       = {120},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Temporal consistent portrait video segmentation.
<em>PR</em>, <em>120</em>, 108143. (<a
href="https://doi.org/10.1016/j.patcog.2021.108143">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We explore a new video segmentation task , named portrait video segmentation (PVS), which aims to automatically segment the dominant person throughout a given portrait video. To achieve accurate and temporal-coherent segmentation results, a feature reconstruction based PVS method is developed under the meta-learning framework. Due to the dramatic pose variation and severe occlusion in portrait videos, feature reconstruction using existing optical flow models usually suffers from severe ghosting effects in reconstructed features. We mitigate this issue by presenting a soft correspondence network (SCN), which learns to facilitate feature reconstruction in an unsupervised fashion by softly assigning each pixel displacement probabilities between portrait frames. Based on the proposed SCN, a novel portrait segmentation network (PSN) is further designed, which explores the reconstructed features through feature aggregation blocks (FABs), yielding more reliable segmentation results. To capture temporal and target-specific cues, the parameters of FABs are determined by a meta-updater network which is trained offline in the meta-level. In addition, we introduce a new PVS dataset with high-quality segmentation annotations. Experimental results clearly demonstrate the effectiveness of the proposed PVS method .},
  archive      = {J_PR},
  author       = {Yifan Wang and Wenbo Zhang and Lijun Wang and Fenghua Yang and Huchuan Lu},
  doi          = {10.1016/j.patcog.2021.108143},
  journal      = {Pattern Recognition},
  pages        = {108143},
  shortjournal = {Pattern Recognition},
  title        = {Temporal consistent portrait video segmentation},
  volume       = {120},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learnable low-rank latent dictionary for subspace
clustering. <em>PR</em>, <em>120</em>, 108142. (<a
href="https://doi.org/10.1016/j.patcog.2021.108142">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, Self-Expressive-based Subspace Clustering (SESC) has been widely applied in pattern clustering and machine learning as it aims to learn a representation that can faithfully reflect the correlation between data points. However, most existing SESC methods directly use the original data as the dictionary, which miss the intrinsic structure (e.g., low-rank and nonlinear) of the real-word data. To address this problem, we propose a novel Projection Low-Rank Subspace Clustering (PLRSC) method by integrating feature extraction and subspace clustering into a unified framework. In particular, PLRSC learns a projection transformation to extract the low-dimensional features and utilizes a low-rank regularizer to ensure the informative and important structures of the extracted features. The extracted low-rank features effectively enhance the self-expressive property of the dictionary. Furthermore, we extend PLRSC to a nonlinear version (i.e., NPLRSC) by integrating a nonlinear activator into the projection transformation. NPLRSC cannot only effectively extract features but also guarantee the data structure of the extracted features. The corresponding optimization problem is solved by the Alternating Direction Method (ADM), and we also prove that the algorithm converges to a stationary point. Experimental results on the real-world datasets validate the superior of our model over the existing subspace clustering methods.},
  archive      = {J_PR},
  author       = {Yesong Xu and Shuo Chen and Jun Li and Lei Luo and Jian Yang},
  doi          = {10.1016/j.patcog.2021.108142},
  journal      = {Pattern Recognition},
  pages        = {108142},
  shortjournal = {Pattern Recognition},
  title        = {Learnable low-rank latent dictionary for subspace clustering},
  volume       = {120},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sparse semi-supervised heterogeneous interbattery bayesian
analysis. <em>PR</em>, <em>120</em>, 108141. (<a
href="https://doi.org/10.1016/j.patcog.2021.108141">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Bayesian approach to feature extraction, known as factor analysis (FA), has been widely studied in machine learning to obtain a latent representation of the data. An adequate selection of the probabilities and priors of these bayesian models allows the model to better adapt to the data nature (i.e. heterogeneity, sparsity), obtaining a more representative latent space. The objective of this article is to propose a general FA framework capable of modelling any problem. To do so, we start from the Bayesian Inter-Battery Factor Analysis (BIBFA) model, enhancing it with new functionalities to be able to work with heterogeneous data , to include feature selection, and to handle missing values as well as semi-supervised problems. The performance of the proposed model, Sparse Semi-supervised Heterogeneous Interbattery Bayesian Analysis (SSHIBA), has been tested on different scenarios to evaluate each one of its novelties, showing not only a great versatility and an interpretability gain, but also outperforming most of the state-of-the-art algorithms.},
  archive      = {J_PR},
  author       = {Carlos Sevilla-Salcedo and Vanessa Gómez-Verdejo and Pablo M. Olmos},
  doi          = {10.1016/j.patcog.2021.108141},
  journal      = {Pattern Recognition},
  pages        = {108141},
  shortjournal = {Pattern Recognition},
  title        = {Sparse semi-supervised heterogeneous interbattery bayesian analysis},
  volume       = {120},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Certainty driven consistency loss on multi-teacher networks
for semi-supervised learning. <em>PR</em>, <em>120</em>, 108140. (<a
href="https://doi.org/10.1016/j.patcog.2021.108140">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the successful approaches in semi-supervised learning is based on the consistency regularization . Typically, a student model is trained to be consistent with teacher prediction for the inputs under different perturbations. To be successful, the prediction targets given by teacher should have good quality, otherwise the student can be misled by teacher. Unfortunately, existing methods do not assess the quality of the teacher targets. In this paper, we propose a novel Certainty-driven Consistency Loss (CCL) that exploits the predictive uncertainty in the consistency loss to let the student dynamically learn from reliable targets. Specifically, we propose two approaches, i.e. Filtering CCL and Temperature CCL to either filter out uncertain predictions or pay less attention on them in the consistency regularization. We further introduce a novel decoupled framework to encourage model difference. Experimental results on SVHN, CIFAR-10, and CIFAR-100 demonstrate the advantages of our method over a few existing methods.},
  archive      = {J_PR},
  author       = {Lu Liu and Robby T. Tan},
  doi          = {10.1016/j.patcog.2021.108140},
  journal      = {Pattern Recognition},
  pages        = {108140},
  shortjournal = {Pattern Recognition},
  title        = {Certainty driven consistency loss on multi-teacher networks for semi-supervised learning},
  volume       = {120},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Doubly supervised parameter transfer classifier for
diagnosis of breast cancer with imbalanced ultrasound imaging
modalities. <em>PR</em>, <em>120</em>, 108139. (<a
href="https://doi.org/10.1016/j.patcog.2021.108139">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The bimodal ultrasound, namely B-mode ultrasound (BUS) and elastography ultrasound (EUS), provide complementary information to improve the diagnostic accuracy of breast cancers. However, in clinical practice, it is easier to acquire the labeled BUS images than the paired bimodal ultrasound data with shared labels due to the lack of EUS devices, especially in many rural hospitals. Thus, the single-modal BUS-based computer-aided diagnosis (CAD) generally has wide applications. Transfer learning (TL) can promote a BUS-based CAD model by transferring additional knowledge from EUS modality. To make full use of labeled paired bimodal data and the additional single-modal BUS images for knowledge transfer, a novel doubly supervised parameter transfer classifier (DSPTC) is proposed to well handle the TL between imbalanced modalities with the guidance of label information. Specifically, the proposed DSPTC consists of two loss functions corresponding to the paired bimodal ultrasound data with shared labels and the unpaired images with different labels, respectively. The former uses the loss function in the specially designed TL paradigm of support vector machine plus, while the latter adopts the Hilbert-Schmidt Independence Criterion (HSIC) for knowledge transfer between the unpaired images, which consist of the single-modal BUS images and the EUS images from the paired bimodal data. Consequently, the doubly supervised knowledge transfer is implemented by way of parameter transfer in a unified optimization framework. Two experiments are designed to evaluate the proposed DSPTC for the ultrasound-based diagnosis of breast cancers. The experimental results indicate that DSPTC outperforms all the compared algorithms, suggesting its wide potential applications.},
  archive      = {J_PR},
  author       = {Xiaoyan Fei and Shichong Zhou and Xiangmin Han and Jun Wang and Shihui Ying and Cai Chang and Weijun Zhou and Jun Shi},
  doi          = {10.1016/j.patcog.2021.108139},
  journal      = {Pattern Recognition},
  pages        = {108139},
  shortjournal = {Pattern Recognition},
  title        = {Doubly supervised parameter transfer classifier for diagnosis of breast cancer with imbalanced ultrasound imaging modalities},
  volume       = {120},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Top-rank convolutional neural network and its application to
medical image-based diagnosis. <em>PR</em>, <em>120</em>, 108138. (<a
href="https://doi.org/10.1016/j.patcog.2021.108138">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Top-rank learning identifies a real-valued ranking function that will provide more absolute top samples. These are highly reliable positive samples that are ranked higher than the highest-ranked negative samples. Therefore, top-rank learning is useful for tasks that require reliable decisions. Additionally, it inherits the merits of the ranking functions, such as robustness to the unbalanced condition. However, conventional top-rank learning tasks are formulated as linear or kernel-based problems and are thus limited in coping with complicated tasks. In this study, we propose a Top-rank convolutional neural network (TopRank CNN) to realize top-rank learning with representation learning for complicated tasks. Given that the original objective function of top-rank learning suffers from overfitting, we employ the p p -norm relaxation of the original loss function in the proposed method. We prove the usefulness of TopRank CNN experimentally with medical diagnosis tasks that require reliable decisions and robustness to the unbalanced condition.},
  archive      = {J_PR},
  author       = {Yan Zheng and Yuchen Zheng and Daiki Suehiro and Seiichi Uchida},
  doi          = {10.1016/j.patcog.2021.108138},
  journal      = {Pattern Recognition},
  pages        = {108138},
  shortjournal = {Pattern Recognition},
  title        = {Top-rank convolutional neural network and its application to medical image-based diagnosis},
  volume       = {120},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-label feature selection considering label
supplementation. <em>PR</em>, <em>120</em>, 108137. (<a
href="https://doi.org/10.1016/j.patcog.2021.108137">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-label feature selection is an efficient technique to alleviate the high dimensionality for multi-label learning. Existing multi-label feature selection methods based on information theory either deal with labels individually or treat all label relationships as redundancy. However, two important and being ignored issues are the different effects of label relationships and the dynamic changes of label relationships in measuring different candidate features. To address these issues, we first distinguish three types of label relationships: label independence, label redundancy and label supplementation. Second, we consider the changes of label relationships based on different features. By analyzing the differences and the changes of label relationships, two new methods named LSMFS and MLSMFS are proposed, which extracts all supplementary information and the maximum supplementary information of features for each label from other labels, respectively. Finally, experiments on fifteen benchmark multi-label data sets demonstrate the effectiveness of the proposed methods against nine other methods.},
  archive      = {J_PR},
  author       = {Ping Zhang and Guixia Liu and Wanfu Gao and Jiazhi Song},
  doi          = {10.1016/j.patcog.2021.108137},
  journal      = {Pattern Recognition},
  pages        = {108137},
  shortjournal = {Pattern Recognition},
  title        = {Multi-label feature selection considering label supplementation},
  volume       = {120},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Human trajectory prediction and generation using LSTM models
and GANs. <em>PR</em>, <em>120</em>, 108136. (<a
href="https://doi.org/10.1016/j.patcog.2021.108136">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human trajectory prediction is an important topic in several application domains, ranging from self-driving cars to environment design and planning, from socially-aware robots to intelligent tracking systems. This complex subject comes with different challenges, such as human-space interaction, human-human interaction, multimodality, and generalizability . Currently, these challenges, especially generalizability , have not been completely explored by state-of-the-art works. This work attempts to fill this gap by proposing and defining new methods and metrics to help understand trajectories. In particular, new deep learning models based on Long Short-Term Memory and Generative Adversarial Network architectures are used in both unimodal and multimodal contexts. These approaches are evaluated with new error metrics, which normalize some biases in standard metrics. Tests have been assessed using newly collected datasets characterized by a higher diversity and lower linearity than those used in state-of-the-art works. The results prove that the proposed models and datasets are comparable to and yield better generalizability than state-of-the-art works. Moreover, we also prove that our datasets better represent multimodal scenarios (allowing for multiple possible behaviors) and that human trajectories are moderately influenced by their spatial region and slightly influenced by their date and time.},
  archive      = {J_PR},
  author       = {Luca Rossi and Marina Paolanti and Roberto Pierdicca and Emanuele Frontoni},
  doi          = {10.1016/j.patcog.2021.108136},
  journal      = {Pattern Recognition},
  pages        = {108136},
  shortjournal = {Pattern Recognition},
  title        = {Human trajectory prediction and generation using LSTM models and GANs},
  volume       = {120},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep neural networks ensemble to detect COVID-19 from CT
scans. <em>PR</em>, <em>120</em>, 108135. (<a
href="https://doi.org/10.1016/j.patcog.2021.108135">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Research on Coronavirus Disease 2019 (COVID-19) detection methods has increased in the last months as more accurate automated toolkits are required. Recent studies show that CT scan images contain useful information to detect the COVID-19 disease. However, the scarcity of large and well balanced datasets limits the possibility of using detection approaches in real diagnostic contexts as they are unable to generalize. Indeed, the performance of these models quickly becomes inadequate when applied to samples captured in different contexts (e.g., different equipment or populations) from those used in the training phase. In this paper, a novel ensemble-based approach for more accurate COVID-19 disease detection using CT scan images is proposed. This work exploits transfer learning using pre-trained deep networks (e.g., VGG, Xception, and ResNet) evolved with a genetic algorithm , combined into an ensemble architecture for the classification of clustered images of lung lobes. The study is validated on a new dataset obtained as an integration of existing ones. The results of the experimental evaluation show that the ensemble classifier ensures effective performance, also exhibiting better generalization capabilities.},
  archive      = {J_PR},
  author       = {Lerina Aversano and Mario Luca Bernardi and Marta Cimitile and Riccardo Pecori},
  doi          = {10.1016/j.patcog.2021.108135},
  journal      = {Pattern Recognition},
  pages        = {108135},
  shortjournal = {Pattern Recognition},
  title        = {Deep neural networks ensemble to detect COVID-19 from CT scans},
  volume       = {120},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning graph edit distance by graph neural networks.
<em>PR</em>, <em>120</em>, 108132. (<a
href="https://doi.org/10.1016/j.patcog.2021.108132">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The emergence of geometric deep learning as a novel framework to deal with graph-based representations has faded away traditional approaches in favor of completely new methodologies. In this paper, we propose a new framework able to combine the advances on deep metric learning with traditional approximations of the graph edit distance. Hence, we propose an efficient graph distance based on the novel field of geometric deep learning. Our method employs a message passing neural network to capture the graph structure, and thus, leveraging this information for its use on a distance computation. The performance of the proposed graph distance is validated on two different scenarios. On the one hand, in a graph retrieval of handwritten words i.e. keyword spotting, showing its superior performance when compared with (approximate) graph edit distance benchmarks. On the other hand, demonstrating competitive results for graph similarity learning when compared with the current state-of-the-art on a recent benchmark dataset.},
  archive      = {J_PR},
  author       = {Pau Riba and Andreas Fischer and Josep Lladós and Alicia Fornés},
  doi          = {10.1016/j.patcog.2021.108132},
  journal      = {Pattern Recognition},
  pages        = {108132},
  shortjournal = {Pattern Recognition},
  title        = {Learning graph edit distance by graph neural networks},
  volume       = {120},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). 3D object recognition through a size function resulting from
an invariant topological feature. <em>PR</em>, <em>120</em>, 108131. (<a
href="https://doi.org/10.1016/j.patcog.2021.108131">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a critical points based descriptor for 3 D 3D objects recognition is presented. It is based on the topological invariant provided by the critical points of the 3 D 3D object. The critical points and the links between them are represented by a size function resulting from a measure function that captures the surface displacement along the 3 D 3D object, and that encompasses invariance to affine transformations, articulations and torsions. In order to tackle the problems of partial matching of the 3 D 3D objects, a well-suited metric learning method is used to weight the matchings according to their relevance. The proposed method’s performance was validated by different collections of 3 D 3D objects. The obtained scores are favorably comparable to the related work.},
  archive      = {J_PR},
  author       = {Mohammed Ayoub Alaoui Mhamdi and Djemel Ziou},
  doi          = {10.1016/j.patcog.2021.108131},
  journal      = {Pattern Recognition},
  pages        = {108131},
  shortjournal = {Pattern Recognition},
  title        = {3D object recognition through a size function resulting from an invariant topological feature},
  volume       = {120},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Community-based k-shell decomposition for identifying
influential spreaders. <em>PR</em>, <em>120</em>, 108130. (<a
href="https://doi.org/10.1016/j.patcog.2021.108130">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How to identify the most influential nodes in a network for the maximization of influence spread is a great challenge. Known methods like k k -shell decomposition determine core nodes who individually might be the most influential spreaders for the spreading originating in a single origin. However, these techniques are not suitable for determining multiple origins that together lead to the most effective spreading. The reason is that core nodes are often found to be located closely to each other, which results in large overlapping regions rather than spreading far across the network. In this paper, we propose a new algorithm, called community-based k k -shell decomposition , by which a network can be viewed as multiple hierarchically ordered structures each branching off from the innermost shell to the periphery shell. To alleviate the overlap problem, our algorithm pursues a greedy strategy that preferably selects core nodes from different communities in the network, thus maximizing the joint influence of multiple origins. We systematically evaluate our algorithm against competing algorithms on multiple networks with varying network characteristics, and find that our algorithm outperforms other algorithms on networks that exhibit community structures, and the stronger communities, the better performance.},
  archive      = {J_PR},
  author       = {Peng Gang Sun and Qiguang Miao and Steffen Staab},
  doi          = {10.1016/j.patcog.2021.108130},
  journal      = {Pattern Recognition},
  pages        = {108130},
  shortjournal = {Pattern Recognition},
  title        = {Community-based k-shell decomposition for identifying influential spreaders},
  volume       = {120},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dynamic and reliable subtask tracker with general schatten
p-norm regularization. <em>PR</em>, <em>120</em>, 108129. (<a
href="https://doi.org/10.1016/j.patcog.2021.108129">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Some multi-task trackers adopt an inaccurate shrink strategy to treat different rank components equally. Thus, their flexibility is vulnerable to some tracking challenges. To resolve this problem, we propose a spatial-aware reliable multi-subtask tracker via weighted Schatten p p -norm regularization (SLRT-W), which dynamically chooses the suitable and reliable subset of the whole subtasks for tracking. Its major merits not only assign the flexible weights to different subtask rank components depending on their tracking contribution, but also preserve consistent spatial layout structure and correspondence of layered multi-subtask. Specifically, multiple layered subtasks correspond to different target subregions , they are cooperative and complement. A weighted Schatten p p -norm is introduced to adaptively shrink different multi-subtask rank components, and emphasize important components as reliable ones. Then, a structured hyper-graph regularized term simultaneously exploits the intrinsic geometry correspondence among multiple layers of subtasks, and spatial layout structure inside each layer. We devise an alternatively generalized iterated shrinkage method to optimize the multi-subtask Schatten p p -norm minimization. Finally, a robust decision-evaluation strategy is developed to choose the reliable multi-subtask tracking combination. Encouraging results on some challenging benchmarks demonstrate the proposed tracker performs favorably in robustness and accuracy, against some state-of-the-art trackers.},
  archive      = {J_PR},
  author       = {Baojie Fan and Yang Cong and Jiandong Tian and Yandong Tang},
  doi          = {10.1016/j.patcog.2021.108129},
  journal      = {Pattern Recognition},
  pages        = {108129},
  shortjournal = {Pattern Recognition},
  title        = {Dynamic and reliable subtask tracker with general schatten p-norm regularization},
  volume       = {120},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Point cloud denoising using non-local collaborative
projections. <em>PR</em>, <em>120</em>, 108128. (<a
href="https://doi.org/10.1016/j.patcog.2021.108128">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point cloud is important for object detection and recognition. The main challenge of point cloud denoising is to preserve the geometric structures. Several state-of-the-art point cloud denoising methods focus only on analyzing local geometric information , which is sensitive to noise and outliers. In this paper, we propose a novel point cloud denoising algorithm based on the characteristics of non-local self-similarity. First, we present an adaptive curvature threshold to select the edge points and tune their corresponding normals, which can preserve the sharp details. Then we propose a structure-aware descriptor called projective height vector to capture the local height variations by normal height projection and the most similar non-local projective height vectors are grouped into a height matrix to enhance the structure representation. Moreover, the proposed structure descriptor is invariant with rigid transformation. Finally, an improved weighted nuclear norm minimization is proposed to optimize the height matrix and reconstruct a high-quality point cloud. Rather than treating each singular value independently, each component in our proposed weight definition connects with the most important components to preserve the major structural information. Experiments on synthetic and scanned point cloud datasets demonstrate that our algorithm outperforms state-of-the-art methods in terms of reconstruction accuracy and structure preservation.},
  archive      = {J_PR},
  author       = {Yiyao Zhou and Rui Chen and Yiqiang Zhao and Xiding Ai and Guoqing Zhou},
  doi          = {10.1016/j.patcog.2021.108128},
  journal      = {Pattern Recognition},
  pages        = {108128},
  shortjournal = {Pattern Recognition},
  title        = {Point cloud denoising using non-local collaborative projections},
  volume       = {120},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). FoCL: Feature-oriented continual learning for generative
models. <em>PR</em>, <em>120</em>, 108127. (<a
href="https://doi.org/10.1016/j.patcog.2021.108127">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a general framework in continual learning for generative models : Feature-oriented Continual Learning (FoCL). Unlike previous works that aim to solve the catastrophic forgetting problem by introducing regularization in the parameter space or image space, FoCL imposes regularization in the feature space. We show in our experiments that FoCL has faster adaptation to distributional changes in sequentially arriving tasks, and achieves state-of-the-art performance for generative models in task incremental learning. We discuss choices of combined regularization spaces towards different use case scenarios for boosted performance, e.g., tasks that have high variability in the background. Finally, we introduce a forgetfulness measure that fairly evaluates the degree to which a model suffers from forgetting. Interestingly, the analysis of our proposed forgetfulness score also implies that FoCL tends to have a mitigated forgetting for future tasks.},
  archive      = {J_PR},
  author       = {Qicheng Lao and Mehrzad Mortazavi and Marzieh Tahaei and Francis Dutil and Thomas Fevens and Mohammad Havaei},
  doi          = {10.1016/j.patcog.2021.108127},
  journal      = {Pattern Recognition},
  pages        = {108127},
  shortjournal = {Pattern Recognition},
  title        = {FoCL: Feature-oriented continual learning for generative models},
  volume       = {120},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Normalized class coherence change-based kNN for
classification of imbalanced data. <em>PR</em>, <em>120</em>, 108126.
(<a href="https://doi.org/10.1016/j.patcog.2021.108126">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {k k NN is a widely used machine learning algorithm in many different domains because of its fairly good performance in actual cases and its simplicity. This study aims to enhance the performance of k k NN for imbalanced datasets, a topic that has been relatively ignored in k k NN research. The proposed k k NN algorithm, called normalized class coherence change-based k k -nearest neighbor (NCC-NN) algorithm, determines the label of a test sample by computing the normalized class coherence changes at class and sample levels for every possible class and assigning the sample to the class with the maximum value. It considers the tendency that the minority classes usually show the lower-class coherence than the majority class. NCC- k k NN also utilizes the adaptive k k for the class coherence, which is calculated in a weighted manner to reduce the sensitivity to the selection of k k . NCC- k k NN was applied to 20 benchmark datasets with varying class imbalance and coherence, and its performance was compared with that of five k k NN algorithms, SMOTE and MetaCost with standard k k NN as a base classifier . The proposed NCC- k k NN outperformed the other k k NN algorithms in classification of imbalanced data , especially for imbalanced data with low positive class coherence.},
  archive      = {J_PR},
  author       = {Kyoungok Kim},
  doi          = {10.1016/j.patcog.2021.108126},
  journal      = {Pattern Recognition},
  pages        = {108126},
  shortjournal = {Pattern Recognition},
  title        = {Normalized class coherence change-based kNN for classification of imbalanced data},
  volume       = {120},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Semi-supervised multi-layer convolution kernel learning in
credit evaluation. <em>PR</em>, <em>120</em>, 108125. (<a
href="https://doi.org/10.1016/j.patcog.2021.108125">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many practical credit evaluation problems, a lot of manpower as well as financial and material resources are required to label samples. Therefore, in the process of labeling, only a small number of samples with category labels can be obtained to train classification models and a large number of customer samples is abandoned without category labels. To solve this problem, we introduce a semi-supervised support vector machine (SVM) technology and combines it with a multi-layer convolution kernel to construct a semi-supervised multi-layer convolution kernel SVM (SSMCK) for category customer credit assessment data sets. We first use a basic solution of the generalized differential operator to generate a base convolution kernel function in the H 1 H1 space, and then use the multi-layer strategy of deep learning to construct the multi-layer convolution kernel in the H 2 H2 and H 3 H3 space (called the family of multi-layer convolution kernel) by using the kernel functions in the H 1 H1 space. We further propose a semi-supervised multi-layer convolution kernel SVM algorithm based on the category center estimation and develop two novel SSMCK methods to improve the classification ability: the SSMCK based on multi-kernel learning (SSMCK-MKL) and the SSMCK based on alternative optimization (SSMCK-AO). Finally, experimental verification and analysis is carried out on three customer credit evaluation data sets. The results show that our methods outperforms or are comparable to some the state-of-the-art credit evaluation models.},
  archive      = {J_PR},
  author       = {Lixiang Xu and Lixin Cui and Thomas Weise and Xinlu Li and Zhize Wu and Feiping Nie and Enhong Chen and Yuanyan Tang},
  doi          = {10.1016/j.patcog.2021.108125},
  journal      = {Pattern Recognition},
  pages        = {108125},
  shortjournal = {Pattern Recognition},
  title        = {Semi-supervised multi-layer convolution kernel learning in credit evaluation},
  volume       = {120},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Meta-learning based relation and representation learning
networks for single-image deraining. <em>PR</em>, <em>120</em>, 108124.
(<a href="https://doi.org/10.1016/j.patcog.2021.108124">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single-image deraining is a kind of computer vision task that aims to restore the image that be degraded by rain streaks, which motivates existing methods to either directly translate the rainy image to its clean one, or indirectly learn the rain residual based on the prior information. However, both methodologies harm the generalization ability due to the limited diversity of the training samples, comparing with the endless varieties of the real-world rainy images. Such fact inspires us to take the merit of meta-learning and propose a meta-learning based representation learning network to learn the transferable embeddings of the rainy/clean images, while their discrepancies are characterized by the relation vector, which is generated by the subsequent meta-learning based relation learning network. These networks are leveraged into the meta-learning based deraining network (MLDN) to enhance the generalization ability by removing the latent relation vector from the transferable embedding of the rainy image and generate high-quality deraining result. Superior performance is achieved by MLDN, which has averaged 4\%\% better than the state-of-the-arts.},
  archive      = {J_PR},
  author       = {Xinjian Gao and Yang Wang and Jun Cheng and Mingliang Xu and Meng Wang},
  doi          = {10.1016/j.patcog.2021.108124},
  journal      = {Pattern Recognition},
  pages        = {108124},
  shortjournal = {Pattern Recognition},
  title        = {Meta-learning based relation and representation learning networks for single-image deraining},
  volume       = {120},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Statistical mechanical analysis for unweighted and weighted
stock market networks. <em>PR</em>, <em>120</em>, 108123. (<a
href="https://doi.org/10.1016/j.patcog.2021.108123">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Financial markets are time-evolving complex systems containing different financial entities, such as banks, corporations and institutions that interact through transactions and respond to external economic and political events. They can be conveniently represented as a network structure. In this paper, we analyse the unweighted and weighted market networks from a statistical mechanical perspective. In particular, we propose a novel thermodynamic analogy to characterise the dynamic structural properties of time-evolving networks. The intricate pattern of edge connections in the network is modelled by using a heat bath analogy in which particles occupy the energy states according to the Boltzmann distribution . According to this analogy the occupation of the energy states is determined by the temperature of the heat bath, and the spectrum of energy states of the network is determined by the number of nodes and edges. For unweighted networks , the binary representation of the elements in the adjacency matrix can be modelled as a statistical ensemble, using the corresponding partition function to compute thermodynamic network characterisations. For weighted networks, on the other hand, the derived thermodynamic quantities together with their distribution of fluctuations identify the salient structure in the network evolution. We conduct experiments on time-evolving stock exchanges using data for the S&amp;P500 Index Stock Exchanges over the past decade. The thermodynamic characterisations provide an excellent framework to identify epochs in which there is significant variance in network structure during financial crises induced by economic and political events.},
  archive      = {J_PR},
  author       = {Jianjia Wang and Xingchen Guo and Weimin Li and Xing Wu and Zhihong Zhang and Edwin R. Hancock},
  doi          = {10.1016/j.patcog.2021.108123},
  journal      = {Pattern Recognition},
  pages        = {108123},
  shortjournal = {Pattern Recognition},
  title        = {Statistical mechanical analysis for unweighted and weighted stock market networks},
  volume       = {120},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A survey on dorsal hand vein biometrics. <em>PR</em>,
<em>120</em>, 108122. (<a
href="https://doi.org/10.1016/j.patcog.2021.108122">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Biometrics technology is one of the most important and effective solutions for personal authentication. In recent years, as one of the emerging biometrics technologies, dorsal hand vein (DHV) biometrics has received a lot of attention. In fact, DHV biometrics has been studied for more than 30 years, during which different problems related to DHV recognition have been addressed. In this paper, we conduct a comprehensive survey on the state-of-the-art in DHV biometrics. Nearly all important aspects of DHV biometrics have been summarized, including the developmental history, data acquisition, databases, preprocessing algorithms, feature extraction and matching algorithms, information fusion schemes and commercial products. We also discuss the challenges and future directions in DHV biometrics research.},
  archive      = {J_PR},
  author       = {Wei Jia and Wei Xia and Bob Zhang and Yang Zhao and Lunke Fei and Wenxiong Kang and Di Huang and Guodong Guo},
  doi          = {10.1016/j.patcog.2021.108122},
  journal      = {Pattern Recognition},
  pages        = {108122},
  shortjournal = {Pattern Recognition},
  title        = {A survey on dorsal hand vein biometrics},
  volume       = {120},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Modulating scalable gaussian processes for expressive
statistical learning. <em>PR</em>, <em>120</em>, 108121. (<a
href="https://doi.org/10.1016/j.patcog.2021.108121">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For a learning task, Gaussian process (GP) is interested in learning the statistical relationship between inputs and outputs, since it offers not only the prediction mean but also the associated variability. The vanilla GP however is hard to learn complicated distribution with the property of, e.g., heteroscedastic noise, multi-modality and non-stationarity, from massive data due to the Gaussian marginal and the cubic complexity. To this end, this article studies new scalable GP paradigms including the non-stationary heteroscedastic GP, the mixture of GPs and the latent GP, which introduce additional latent variables to modulate the outputs or inputs in order to learn richer, non-Gaussian statistical representation. Particularly, we resort to different variational inference strategies to arrive at analytical or tighter evidence lower bounds (ELBOs) of the marginal likelihood for efficient and effective model training. Extensive numerical experiments against state-of-the-art GP and neural network (NN) counterparts on various tasks verify the superiority of these scalable modulated GPs, especially the scalable latent GP, for learning diverse data distributions.},
  archive      = {J_PR},
  author       = {Haitao Liu and Yew-Soon Ong and Xiaomo Jiang and Xiaofang Wang},
  doi          = {10.1016/j.patcog.2021.108121},
  journal      = {Pattern Recognition},
  pages        = {108121},
  shortjournal = {Pattern Recognition},
  title        = {Modulating scalable gaussian processes for expressive statistical learning},
  volume       = {120},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Real-time and light-weighted unsupervised video object
segmentation network. <em>PR</em>, <em>120</em>, 108120. (<a
href="https://doi.org/10.1016/j.patcog.2021.108120">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video object segmentation is one of the most practical computer vision tasks, especially in the unsupervised case, which has no manually labeled segmentation mask at the beginning of a video sequence. In this paper, we propose a new real-time unsupervised video object segmentation network . Based on the encoder-decoder framework, we present a Dynamic ASPP module and a RNN-Conv module. The former adds a dynamic selection mechanism into the Astrous Spatial Pyramid Pooling structure, and then the dilated convolutional kernels adaptively select appropriate features according to the scales by the channel attention mechanism . Compared with directly concatenating the dilated convolutional features, dynamically selecting feature maps reduces the amount of parameters and makes the module more efficient. The RNN-Conv module incorporates the RNN units with external convolutional blocks, aggregating the temporal features of a video sequence with the spatial information extracted by the convolutional network . We stack this module to extract deeper spatiotemporal features than the traditional RNN network. This module helps to avoid the gradient disappearance and explosion during network training. We test our network on the popular video object segmentation datasets. The experiment results demonstrate the effectiveness of our model. 1},
  archive      = {J_PR},
  author       = {Zongji Zhao and Sanyuan Zhao and Jianbing Shen},
  doi          = {10.1016/j.patcog.2021.108120},
  journal      = {Pattern Recognition},
  pages        = {108120},
  shortjournal = {Pattern Recognition},
  title        = {Real-time and light-weighted unsupervised video object segmentation network},
  volume       = {120},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Spatial reasoning for few-shot object detection.
<em>PR</em>, <em>120</em>, 108118. (<a
href="https://doi.org/10.1016/j.patcog.2021.108118">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although modern object detectors rely heavily on a significant amount of training data, humans can easily detect novel objects using a few training examples. The mechanism of the human visual system is to interpret spatial relationships among various objects and this process enables us to exploit contextual information by considering the co-occurrence of objects. Thus, we propose a spatial reasoning framework that detects novel objects with only a few training examples in a context. We infer geometric relatedness between novel and base RoIs (Region-of-Interests) to enhance the feature representation of novel categories using an object detector well trained on base categories. We employ a graph convolutional network as the RoIs and their relatedness are defined as nodes and edges, respectively. Furthermore, we present spatial data augmentation to overcome the few-shot environment where all objects and bounding boxes in an image are resized randomly. Using the PASCAL VOC and MS COCO datasets, we demonstrate that the proposed method significantly outperforms the state-of-the-art methods and verify its efficacy through extensive ablation studies.},
  archive      = {J_PR},
  author       = {Geonuk Kim and Hong-Gyu Jung and Seong-Whan Lee},
  doi          = {10.1016/j.patcog.2021.108118},
  journal      = {Pattern Recognition},
  pages        = {108118},
  shortjournal = {Pattern Recognition},
  title        = {Spatial reasoning for few-shot object detection},
  volume       = {120},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Correlation-based structural dropout for convolutional
neural networks. <em>PR</em>, <em>120</em>, 108117. (<a
href="https://doi.org/10.1016/j.patcog.2021.108117">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks (CNNs) easily suffer from the over-fitting problem since they are often over-parameterized in the case of small training datasets. The conventional dropout that drops feature units randomly works well for fully connected networks, but fails to regularize CNNs well due to high spatial correlation of the intermediate features, which allows the dropped information to flow through the network, thus leading to the problem of under-dropping. To better regularize CNNs, some structural dropout methods such as SpatialDropout and DropBlock have been proposed by dropping feature units in continuous regions randomly. However, these methods may suffer from the over-dropping problem by discarding the critical discriminative features , thus limiting the performance of CNNs. To address these issues, we propose a novel structural dropout method, Correlation based Dropout (CorrDrop), to regularize CNNs by dropping feature units based on feature correlation. Unlike the previous dropout methods, our CorrDrop can focus on the discriminative information and drops features in a spatial-wise or channel-wise manner. Extensive experiments on different datasets, network architectures , and various tasks (e.g., image classification and object localization) demonstrate the superiority of our method over other methods.},
  archive      = {J_PR},
  author       = {Yuyuan Zeng and Tao Dai and Bin Chen and Shu-Tao Xia and Jian Lu},
  doi          = {10.1016/j.patcog.2021.108117},
  journal      = {Pattern Recognition},
  pages        = {108117},
  shortjournal = {Pattern Recognition},
  title        = {Correlation-based structural dropout for convolutional neural networks},
  volume       = {120},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hierarchical object relationship constrained monocular depth
estimation. <em>PR</em>, <em>120</em>, 108116. (<a
href="https://doi.org/10.1016/j.patcog.2021.108116">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Monocular depth estimation has been gaining growing momentum in recent years. Despite significant advances of this task, due to the inherent difficulty of reliably capturing contextual cues from RGB images , it remains challenging to accurately predict depth in scenes with complicated and cluttered spatial arrangement of objects. Instead of naively utilizing the primary features in the single RGB image, in this paper we propose a hierarchical object relationship constrained network for monocular depth estimation, which could enable accurate and smooth depth prediction from monocular RGB image. The key idea of our method is to exploit object-centric hierarchical relationship as contextual constraints to compensate for the regularity of spatial depth changing. In particular, we design a semantics-guided CNN network to encode the original image into a global context feature map and encode the objects’ relationship into a local relationship feature map simultaneously, so that we can leverage such effective and consolidated coding scheme over scenario samples to guide the depth prediction in a more accurate way. Benefiting from the local-to-global context constraints, our method can well respect the global depth changing and preserve the local depth details at the same time. In addition, our approach could make full use of the hierarchical semantic relationship across inner-object components and neighboring objects to define depth changing constraints. We conduct extensive experiments and make comprehensive evaluations on widely-used public datasets, and the experiments confirm that our method outperforms most state-of-the-art depth estimation methods in preserving the local details in depth.},
  archive      = {J_PR},
  author       = {Shuai Li and Jiaying Shi and Wenfeng Song and Aimin Hao and Hong Qin},
  doi          = {10.1016/j.patcog.2021.108116},
  journal      = {Pattern Recognition},
  pages        = {108116},
  shortjournal = {Pattern Recognition},
  title        = {Hierarchical object relationship constrained monocular depth estimation.},
  volume       = {120},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Enhanced anomaly scores for isolation forests. <em>PR</em>,
<em>120</em>, 108115. (<a
href="https://doi.org/10.1016/j.patcog.2021.108115">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Isolation Forest represents a variant of Random Forest largely and successfully employed for outlier detection . The main idea is that outliers are likely to get isolated in a tree after few splits. The anomaly score is therefore a function inversely related to the leaf depth. This paper proposes enhanced anomaly scores of the Isolation Forest by making two different contributions. The first consists in weighing the path traversed by an object to obtain a more informative anomaly score. The second contribution employs a different aggregation function to combine the tree scores. We thoroughly evaluate the proposed methodology by testing it on sixteen datasets.},
  archive      = {J_PR},
  author       = {Antonella Mensi and Manuele Bicego},
  doi          = {10.1016/j.patcog.2021.108115},
  journal      = {Pattern Recognition},
  pages        = {108115},
  shortjournal = {Pattern Recognition},
  title        = {Enhanced anomaly scores for isolation forests},
  volume       = {120},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Potential anchoring for imbalanced data classification.
<em>PR</em>, <em>120</em>, 108114. (<a
href="https://doi.org/10.1016/j.patcog.2021.108114">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data imbalance remains one of the factors negatively affecting the performance of contemporary machine learning algorithms . One of the most common approaches to reducing the negative impact of data imbalance is preprocessing the original dataset with data-level strategies. In this paper we propose a unified framework for imbalanced data over- and undersampling. The proposed approach utilizes radial basis functions to preserve the original shape of the underlying class distributions during the resampling process. This is done by optimizing the positions of generated synthetic observations with respect to the proposed potential resemblance loss. The final Potential Anchoring algorithm combines over- and undersampling within the proposed framework. The results of the experiments conducted on 60 imbalanced datasets show outperformance of Potential Anchoring over state-of-the-art resampling algorithms, including previously proposed methods that utilize radial basis functions to model class potential. Furthermore, the results of the analysis based on the proposed data complexity index show that Potential Anchoring is particularly well suited for handling naturally complex (i.e. not affected by the presence of noise) datasets.},
  archive      = {J_PR},
  author       = {Michał Koziarski},
  doi          = {10.1016/j.patcog.2021.108114},
  journal      = {Pattern Recognition},
  pages        = {108114},
  shortjournal = {Pattern Recognition},
  title        = {Potential anchoring for imbalanced data classification},
  volume       = {120},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). GGAC: Multi-relational image gated GCN with attention
convolutional binary neural tree for identifying disease with chest
x-rays. <em>PR</em>, <em>120</em>, 108113. (<a
href="https://doi.org/10.1016/j.patcog.2021.108113">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Using medical images for disease identification is an important application in the medical field. Graph Convolutional Network (GCN) is proposed to model multi-relational image and generate more informative image representations. Recently, the relations between medical images are utilized to identify diseases. This paper proposes a Gated GCN with Attention Convolutional Binary Neural Tree (GGAC) for Multi-Relational Image Identifying Disease. GGAC extracts the discriminative features of the image, strengthen the ability to model medical images, understands images representation deeply and then well captures the multi-modal relation between images. Firstly, an Attention Convolutional Binary Neural Tree based on the attention mechanism is designed to extract the fine-grained features of the images, and use the attention conversion operation on the edge of the tree structure to enhance the network’s acquisition of key image features . Secondly, a Gated GCN is proposed to improve GCN performance by solving the problem of the weight distribution of different neighbors in the same-order neighborhood. Thirdly, a GCN propagation rule is used to transfer messages in multi-relational Graph and then solves the message passing problem of high-dimensional feature data in GCN. Finally, we verify GGAC on a multi-relational graph constructed on the Chest X-rays14. It can be seen from the experiment that overfitting and underfitting can be solved to a certain extent through the extraction and inference of the features of the multi-relational graph, and then GGAC has better performance than the state-of-the-art methods, and keeps good in model complexity.},
  archive      = {J_PR},
  author       = {Bing Yang and Yan Kang and Lan Zhang and Hao Li},
  doi          = {10.1016/j.patcog.2021.108113},
  journal      = {Pattern Recognition},
  pages        = {108113},
  shortjournal = {Pattern Recognition},
  title        = {GGAC: Multi-relational image gated GCN with attention convolutional binary neural tree for identifying disease with chest X-rays},
  volume       = {120},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Generalized error path algorithm. <em>PR</em>, <em>120</em>,
108112. (<a href="https://doi.org/10.1016/j.patcog.2021.108112">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Model selection with cross validation (CV) is very popular in machine learning . However, CV with grid and other common search strategies cannot guarantee to find the model with minimum CV error, which is often the ultimate goal of model selection. Recently, various solution path algorithms have been proposed for several important learning algorithms including support vector classification, Lasso, and so on. However, they still do not guarantee to find the model with minimum CV error. In this paper, we first show that the solution paths produced by various algorithms have the property of piecewise linearity. Then, we prove that a large class of error (or loss) functions are piecewise constant, linear, or quadratic w.r.t. the regularization parameter , based on the solution path. Finally, we propose a new generalized error path algorithm (GEP), and prove that it will find the model with minimum CV error in a finite number of steps for the entire range of the regularization parameter. The experimental results on a variety of datasets not only confirm our theoretical findings, but also show that the best model with our GEP has better generalization error on the test data, compared to the grid search, manual search, and random search.},
  archive      = {J_PR},
  author       = {Bin Gu and Charles X. Ling},
  doi          = {10.1016/j.patcog.2021.108112},
  journal      = {Pattern Recognition},
  pages        = {108112},
  shortjournal = {Pattern Recognition},
  title        = {Generalized error path algorithm},
  volume       = {120},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MetaMed: Few-shot medical image classification using
gradient-based meta-learning. <em>PR</em>, <em>120</em>, 108111. (<a
href="https://doi.org/10.1016/j.patcog.2021.108111">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The occurrence of long-tailed distributions and unavailability of high-quality annotated images is a common phenomenon in medical datasets. The use of conventional Deep Learning techniques to obtain an unbiased model with high generalization accuracy for such datasets is a challenging task. Thus, we formulated a few-shot learning problem and presented a meta-learning-based “MetaMed” approach. The model presented here can adapt to rare disease classes with the availability of few images, and less compute. MetaMed is validated on three publicly accessible medical datasets – Pap smear, BreakHis, and ISIC 2018. We used advanced image augmentation techniques like CutOut, MixUp, and CutMix to overcome the problem of over-fitting. Our approach has shown promising results on all the three datasets with an accuracy of more than 70\%. Inclusion of advanced augmentation techniques regularizes the model and increases the generalization capability by  2–5\%. Comparative analysis of MetaMed against transfer learning demonstrated that MetaMed classifies images with a higher confidence score and on average outperforms transfer learning for 3, 5, and 10-shot tasks for both 2-way and 3-way classification.},
  archive      = {J_PR},
  author       = {Rishav Singh and Vandana Bharti and Vishal Purohit and Abhinav Kumar and Amit Kumar Singh and Sanjay Kumar Singh},
  doi          = {10.1016/j.patcog.2021.108111},
  journal      = {Pattern Recognition},
  pages        = {108111},
  shortjournal = {Pattern Recognition},
  title        = {MetaMed: Few-shot medical image classification using gradient-based meta-learning},
  volume       = {120},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DDAT: Dual domain adaptive translation for low-resolution
face verification in the wild. <em>PR</em>, <em>120</em>, 108107. (<a
href="https://doi.org/10.1016/j.patcog.2021.108107">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-resolution (LR) face verification has received much attention because of its wide applicability in real scenarios, especially in long-distance surveillance. However, the poor quality and scarcity of training data make the accuracy far from satisfactory. In this paper, we propose an end-to-end LR face translation and verification framework to improve the generation quality of face images and face verification accuracy simultaneously. We design a dual domain adaptive structure to generate high-quality images. On one hand, the structure can reduce the domain gap between training data and test data. On the other hand, the structure preserves identity consistency and low-level attributes. Meanwhile, in order to make the whole model more robust, we treat the generated images of the target domain as an extension of the training data. We conduct extensive comparative experiments on multiple benchmark data sets. Experimental results verify that our method achieves improved results in high-quality face generation and LR face verification. In particular, our model DDAT reduces FID to 18.63 and 39.55 on the source and the target domain from 254.7 and 206.19 of the up-sampling results, respectively. Our method outperforms competing approaches by more than 10 percentage points in terms of face verification accuracy on multiple surveillance benchmarks.},
  archive      = {J_PR},
  author       = {Qianfen Jiao and Rui Li and Wenming Cao and Jian Zhong and Si Wu and Hau-San Wong},
  doi          = {10.1016/j.patcog.2021.108107},
  journal      = {Pattern Recognition},
  pages        = {108107},
  shortjournal = {Pattern Recognition},
  title        = {DDAT: Dual domain adaptive translation for low-resolution face verification in the wild},
  volume       = {120},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Accuracy vs. Complexity: A trade-off in visual question
answering models. <em>PR</em>, <em>120</em>, 108106. (<a
href="https://doi.org/10.1016/j.patcog.2021.108106">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual Question Answering (VQA) has emerged as a Visual Turing Test to validate the reasoning ability of AI agents. The pivot to existing VQA models is the joint embedding that is learned by combining the visual features from an image and the semantic features from a given question. Consequently, a large body of literature has focused on developing complex joint embedding strategies coupled with visual attention mechanisms to effectively capture the interplay between these two modalities. However, modelling the visual and semantic features in a high dimensional (joint embedding) space is computationally expensive, and more complex models often result in trivial improvements in the VQA accuracy. In this work, we systematically study the trade-off between the model complexity and the performance on the VQA task. VQA models have a diverse architecture comprising of pre-processing, feature extraction, multimodal fusion, attention and final classification stages. We specifically focus on the effect of “multi-modal fusion” in VQA models that is typically the most expensive step in a VQA pipeline. Our thorough experimental evaluation leads us to three proposals, one optimized for minimal complexity, one for balanced complexity-accuracy and the last one for state-of-the-art VQA performance.},
  archive      = {J_PR},
  author       = {Moshiur Farazi and Salman Khan and Nick Barnes},
  doi          = {10.1016/j.patcog.2021.108106},
  journal      = {Pattern Recognition},
  pages        = {108106},
  shortjournal = {Pattern Recognition},
  title        = {Accuracy vs. complexity: A trade-off in visual question answering models},
  volume       = {120},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MVDRNet: Multi-view diabetic retinopathy detection by
combining DCNNs and attention mechanisms. <em>PR</em>, <em>120</em>,
108104. (<a href="https://doi.org/10.1016/j.patcog.2021.108104">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diabetic retinopathy (DR) detection has attracted much attention recently, and the deep learning algorithms have gained traction in this area. At present, DR screening by deep learning algorithms is often based on single-view fundus images, which usually leads to an unsatisfactory accuracy of DR grading due to the incomplete lesion features. In this paper, we proposed a novel diabetic retinopathy detection convolutional network for automatic DR detection by integrating multi-view fundus images. Compared to existing single-view DCNN-based DR detection methods, the proposed method has the following advantages. First, our method fully utilizes the lesion features from the retina with a field-of-view around 120 ∘ − 150 ∘ 120∘−150∘ . Second, by introducing the attention mechanisms, more attention will be paid on the influential view and the performance can be improved. Besides, we also assign large weights to important channels in the network for effective feature extraction. Experiments are conducted on our collected multi-view DR dataset contained 15,468 images, in which each eye sample provides four-view images. The experimental results indicate that using multi-view images is suitable for automatic DR detection and our proposed method is superior to other benchmarking methods.},
  archive      = {J_PR},
  author       = {Xiaoling Luo and Zuhui Pu and Yong Xu and Wai Keung Wong and Jingyong Su and Xiaoyan Dou and Baikang Ye and Jiying Hu and Lisha Mou},
  doi          = {10.1016/j.patcog.2021.108104},
  journal      = {Pattern Recognition},
  pages        = {108104},
  shortjournal = {Pattern Recognition},
  title        = {MVDRNet: Multi-view diabetic retinopathy detection by combining DCNNs and attention mechanisms},
  volume       = {120},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Reducing magnetic resonance image spacing by learning
without ground-truth. <em>PR</em>, <em>120</em>, 108103. (<a
href="https://doi.org/10.1016/j.patcog.2021.108103">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-quality magnetic resonance (MR) image, i.e., with near isotropic voxel spacing, is desirable in various scenarios of medical image analysis. However, many MR images are acquired using good in-plane resolution but large spacing between slices in clinical practice. In this work, we propose a novel deep-learning-based super-resolution algorithm to generate high-resolution (HR) MR images of small slice spacing from low-resolution (LR) inputs of large slice spacing. Notice that real HR images are needed in most existing deep-learning-based methods to supervise the training, but in clinical scenarios, usually they will not be acquired. Therefore, our unique goal herein is to design and train the super-resolution network without real HR ground-truth. Specifically, two-staged training is used in our method. In the first stage, HR images of reduced slice spacing are synthesized from real LR images using variational auto-encoder (VAE). Although these synthesized HR images of reduced slice spacing are as realistic as possible, they may still suffer from unexpected morphing induced by VAE, implying that the synthesized HR images cannot be paired with the real LR images in terms of anatomical structure details. In the second stage, we degrade the synthesized HR images to generate corresponding LR-HR image pairs and train a super-resolution network based on these synthesized pairs. The underlying mechanism is that such a super-resolution network is less vulnerable to anatomical variability . Experiments on knee MR images successfully demonstrate the effectiveness of our proposed solution to reduce the slice spacing for better rendering .},
  archive      = {J_PR},
  author       = {Kai Xuan and Liping Si and Lichi Zhang and Zhong Xue and Yining Jiao and Weiwu Yao and Dinggang Shen and Dijia Wu and Qian Wang},
  doi          = {10.1016/j.patcog.2021.108103},
  journal      = {Pattern Recognition},
  pages        = {108103},
  shortjournal = {Pattern Recognition},
  title        = {Reducing magnetic resonance image spacing by learning without ground-truth},
  volume       = {120},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Explainable deep learning for efficient and robust pattern
recognition: A survey of recent developments. <em>PR</em>, <em>120</em>,
108102. (<a href="https://doi.org/10.1016/j.patcog.2021.108102">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning has recently achieved great success in many visual recognition tasks. However, the deep neural networks (DNNs) are often perceived as black-boxes, making their decision less understandable to humans and prohibiting their usage in safety-critical applications. This guest editorial introduces the thirty papers accepted for the Special Issue on Explainable Deep Learning for Efficient and Robust Pattern Recognition. They are grouped into three main categories: explainable deep learning methods, efficient deep learning via model compression and acceleration, as well as robustness and stability in deep learning. For each of the three topics, a survey of the representative works and latest developments is presented, followed by the brief introduction of the accepted papers belonging to this topic. The special issue should be of high relevance to the reader interested in explainable deep learning methods for efficient and robust pattern recognition applications and it helps promoting the future research directions in this field.},
  archive      = {J_PR},
  author       = {Xiao Bai and Xiang Wang and Xianglong Liu and Qiang Liu and Jingkuan Song and Nicu Sebe and Been Kim},
  doi          = {10.1016/j.patcog.2021.108102},
  journal      = {Pattern Recognition},
  pages        = {108102},
  shortjournal = {Pattern Recognition},
  title        = {Explainable deep learning for efficient and robust pattern recognition: A survey of recent developments},
  volume       = {120},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Copula-based conformal prediction for multi-target
regression. <em>PR</em>, <em>120</em>, 108101. (<a
href="https://doi.org/10.1016/j.patcog.2021.108101">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There are relatively few works dealing with conformal prediction for multi-task learning issues, and this is particularly true for multi-target regression. This paper focuses on the problem of providing valid (i.e., frequency calibrated) multi-variate predictions. To do so, we propose to use copula functions for inductive conformal prediction, and illustrate our proposal by applying it to deep neural networks and random forests . We show that the proposed method ensures efficiency and validity for multi-target regression problems on various data sets.},
  archive      = {J_PR},
  author       = {Soundouss Messoudi and Sébastien Destercke and Sylvain Rousseau},
  doi          = {10.1016/j.patcog.2021.108101},
  journal      = {Pattern Recognition},
  pages        = {108101},
  shortjournal = {Pattern Recognition},
  title        = {Copula-based conformal prediction for multi-target regression},
  volume       = {120},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Semantic manifold modularization-based ranking for image
recommendation. <em>PR</em>, <em>120</em>, 108100. (<a
href="https://doi.org/10.1016/j.patcog.2021.108100">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the Internet confronts the multimedia explosion, it becomes urgent to investigate personalized recommendation for alleviating information overload and improving users’ experience. Most personalized recommendation approaches pay their attention to collaborative filtering over users’ interactions, which suffers greatly from the highly sparse interactions. In image recommendation, visual correlations among images that users consumed provide a piece of intrinsic evidence to reveal users’ interests. It inspires us to investigate image recommendation over the dense visual graph of images instead of the sparse user interaction graph. In this paper, we propose a semantic manifold modularization-based ranking (MMR) for image recommendation. MMR leverages the dense visual manifold to propagate users’ historical records and infer user-image correlations for image recommendation. Especially, it constrains interest propagation within semantic visual compact groups by manifold modularization to make a tradeoff between users’ personality and graph smoothness in propagation. Experimental results demonstrate that user-consumed visual correlations play actively to capture users’ interests, and the proposed MMR can infer user-image correlations via visual manifold propagation for image recommendation.},
  archive      = {J_PR},
  author       = {Meng Jian and Jingjing Guo and Chenlin Zhang and Ting Jia and Lifang Wu and Xun Yang and Lina Huo},
  doi          = {10.1016/j.patcog.2021.108100},
  journal      = {Pattern Recognition},
  pages        = {108100},
  shortjournal = {Pattern Recognition},
  title        = {Semantic manifold modularization-based ranking for image recommendation},
  volume       = {120},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Randomized trees for time series representation and
similarity. <em>PR</em>, <em>120</em>, 108097. (<a
href="https://doi.org/10.1016/j.patcog.2021.108097">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most of the temporal data mining tasks require representations to capture important characteristics of time series. Representation learning is challenging when time series differ in distributional characteristics and/or show irregularities such as varying lengths and missing observations. Moreover, when time series are multivariate, interactions between variables should be modeled efficiently. This study proposes a unified, flexible time series representation learning framework for both univariate and multivariate time series called Rand-TS. Rand-TS models density characteristics of each time series as a time-varying Gaussian distribution using random decision trees and embeds density information into a sparse vector. Rand-TS can work with time series of various lengths and missing observations, furthermore, it allows using customized features. We illustrate the classification performance of Rand-TS on 113 univariate, 19 multivariate along with 15 univariate time series with varying lengths from UCR database. The results show that in addition to its flexibility, Rand-TS provides competitive classification performance.},
  archive      = {J_PR},
  author       = {Berk Görgülü and Mustafa Gökçe Baydoğan},
  doi          = {10.1016/j.patcog.2021.108097},
  journal      = {Pattern Recognition},
  pages        = {108097},
  shortjournal = {Pattern Recognition},
  title        = {Randomized trees for time series representation and similarity},
  volume       = {120},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep robust multilevel semantic hashing for multi-label
cross-modal retrieval. <em>PR</em>, <em>120</em>, 108084. (<a
href="https://doi.org/10.1016/j.patcog.2021.108084">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hashing based cross-modal retrieval has recently made significant progress. But straightforward embedding data from different modalities involving rich semantics into a joint Hamming space will inevitably produce false codes due to the intrinsic modality discrepancy and noises. We present a novel deep Robust Multilevel Semantic Hashing (RMSH) for more accurate multi-label cross-modal retrieval. It seeks to preserve fine-grained similarity among data with rich semantics,i.e., multi-label, while explicitly require distances between dissimilar points to be larger than a specific value for strong robustness. For this, we give an effective bound of this value based on the information coding-theoretic analysis, and the above goals are embodied into a margin-adaptive triplet loss. Furthermore, we introduce pseudo-codes via fusing multiple hash codes to explore seldom-seen semantics, alleviating the sparsity problem of similarity information. Experiments on three benchmarks show the validity of the derived bounds, and our method achieves state-of-the-art performance.},
  archive      = {J_PR},
  author       = {Ge Song and Xiaoyang Tan and Jun Zhao and Ming Yang},
  doi          = {10.1016/j.patcog.2021.108084},
  journal      = {Pattern Recognition},
  pages        = {108084},
  shortjournal = {Pattern Recognition},
  title        = {Deep robust multilevel semantic hashing for multi-label cross-modal retrieval},
  volume       = {120},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Non-uniform motion deblurring with blurry component divided
guidance. <em>PR</em>, <em>120</em>, 108082. (<a
href="https://doi.org/10.1016/j.patcog.2021.108082">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blind image deblurring is a fundamental and challenging computer vision problem, which aims to recover both the blur kernel and the latent sharp image from only a blurry observation. Despite the superiority of deep learning methods in image deblurring have displayed, there still exists a major challenge with various non-uniform motion blur . Previous methods simply take all the image features as the input to the decoder, which handles different degrees ( e.g. large blur, small blur) simultaneously, leading to challenges for sharp image generation . To tackle the above problems, we present a deep two-branch network to deal with blurry images via a component divided module, which divides an image into two components based on the representation of blurry degree. Specifically, two component attentive blocks are employed to learn attention maps to exploit useful deblurring feature representations on both large and small blurry regions. Then, the blur-aware features are fed into two-branch reconstruction decoders respectively. In addition, a new feature fusion mechanism, orientation-based feature fusion , is proposed to merge sharp features of the two branches. Both qualitative and quantitative experimental results show that our method performs favorably against the state-of-the-art approaches.},
  archive      = {J_PR},
  author       = {Pei Wang and Wei Sun and Qingsen Yan and Axi Niu and Rui Li and Yu Zhu and Jinqiu Sun and Yanning Zhang},
  doi          = {10.1016/j.patcog.2021.108082},
  journal      = {Pattern Recognition},
  pages        = {108082},
  shortjournal = {Pattern Recognition},
  title        = {Non-uniform motion deblurring with blurry component divided guidance},
  volume       = {120},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Knowledge-aware deep framework for collaborative skin lesion
segmentation and melanoma recognition. <em>PR</em>, <em>120</em>,
108075. (<a href="https://doi.org/10.1016/j.patcog.2021.108075">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning techniques have shown their superior performance in dermatologist clinical inspection. Nevertheless, melanoma diagnosis is still a challenging task due to the difficulty of incorporating the useful dermatologist clinical knowledge into the learning process. In this paper, we propose a novel knowledge-aware deep framework that incorporates some clinical knowledge into collaborative learning of two important melanoma diagnosis tasks, i.e., skin lesion segmentation and melanoma recognition. Specifically, to exploit the knowledge of morphological expressions of the lesion region and also the periphery region for melanoma identification, a lesion-based pooling and shape extraction (LPSE) scheme is designed, which transfers the structure information obtained from skin lesion segmentation into melanoma recognition. Meanwhile, to pass the skin lesion diagnosis knowledge from melanoma recognition to skin lesion segmentation, an effective diagnosis guided feature fusion (DGFF) strategy is designed. Moreover, we propose a recursive mutual learning mechanism that further promotes the inter-task cooperation, and thus iteratively improves the joint learning capability of the model for both skin lesion segmentation and melanoma recognition. Experimental results on two publicly available skin lesion datasets show the effectiveness of the proposed method for melanoma analysis.},
  archive      = {J_PR},
  author       = {Xiaohong Wang and Xudong Jiang and Henghui Ding and Yuqian Zhao and Jun Liu},
  doi          = {10.1016/j.patcog.2021.108075},
  journal      = {Pattern Recognition},
  pages        = {108075},
  shortjournal = {Pattern Recognition},
  title        = {Knowledge-aware deep framework for collaborative skin lesion segmentation and melanoma recognition},
  volume       = {120},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A nested u-shape network with multi-scale upsample attention
for robust retinal vascular segmentation. <em>PR</em>, <em>120</em>,
107998. (<a href="https://doi.org/10.1016/j.patcog.2021.107998">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a new nested U-shape attention network (NUA-Net) with improved robustness of lesions for effective vascular segmentation in retinal imaging. Unlike most of the current deep learning approaches which rely on vanilla upsample module to recover distinguishable features for segmentation, our attention-based multi-scale network extends the U-shape segmentation network by introducing a novel multi-scale upsample attention (MSUA) module to enhance vessel features in a hierarchical structure. The new approach connects encoder-decoder branches through a nested skip-connection pyramid architecture to extract discriminating retinal features from the rich local details. Experimental evaluations on five publicly available databases DRIVE, STARE, CHASE_DB, IOSTAR and HRF show the NUA-Net achieves 0.8043–0.8511 (Sensitivity), 0.9741–0.99 (Specificity) and 0.9646–0.9794 (Accuracy) respectively. The benchmark by cross-testing and separate-testing presents a state-of-the-art performance and better vessel preservation compared with other approaches.},
  archive      = {J_PR},
  author       = {Ruohan Zhao and Qin Li and Jianrong Wu and Jane You},
  doi          = {10.1016/j.patcog.2021.107998},
  journal      = {Pattern Recognition},
  pages        = {107998},
  shortjournal = {Pattern Recognition},
  title        = {A nested U-shape network with multi-scale upsample attention for robust retinal vascular segmentation},
  volume       = {120},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). CapsNet regularization and its conjugation with ResNet for
signature identification. <em>PR</em>, <em>120</em>, 107851. (<a
href="https://doi.org/10.1016/j.patcog.2021.107851">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new regularization term for CapsNet that significantly improves the generalization power of the original method from small training data while requiring much fewer parameters, making it suitable for large input images. We also propose a very efficient DNN architecture that integrates CapsNet with ResNet to obtain the advantages of the two architectures. CapsNet allows a powerful understanding of the objects’ components and their positions, while ResNet provides efficient feature extraction and description. Our approach is general, and we demonstrate it on the problem of signature identification from images. To show our approach superiority, we provide several evaluations with different protocols. We also show that our approach outperforms the state-of-the-art on this problem with thorough experiments on three publicly available datasets CEDAR, MCYT, and UTSig.},
  archive      = {J_PR},
  author       = {Mahdi Jampour and Saeid Abbaasi and Malihe Javidi},
  doi          = {10.1016/j.patcog.2021.107851},
  journal      = {Pattern Recognition},
  pages        = {107851},
  shortjournal = {Pattern Recognition},
  title        = {CapsNet regularization and its conjugation with ResNet for signature identification},
  volume       = {120},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Accurate detection of COVID-19 patients based on distance
biased naïve bayes (DBNB) classification strategy. <em>PR</em>,
<em>119</em>, 108110. (<a
href="https://doi.org/10.1016/j.patcog.2021.108110">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {COVID-19, as an infectious disease, has shocked the world and still threatens the lives of billions of people. Early detection of COVID-19 patients is an important issue for treating and controlling the disease from spreading. In this paper, a new strategy for detecting COVID-19 infected patients will be introduced, which is called Distance Biased Naïve Bayes (DBNB). The novelty of DBNB as a proposed classification strategy is concentrated in two contributions. The first is a new feature selection technique called Advanced Particle Swarm Optimization (APSO) which elects the most informative and significant features for diagnosing COVID-19 patients. APSO is a hybrid method based on both filter and wrapper methods to provide accurate and significant features for the next classification phase . The considered features are extracted from Laboratory findings for different cases of people, some of whom are COVID-19 infected while some are not. APSO consists of two sequential feature selection stages, namely; Initial Selection Stage (IS 2 ) and Final Selection Stage (FS 2 ). IS 2 uses filter technique to quickly select the most important features for diagnosing COVID-19 patients while removing the redundant and ineffective ones. This behavior minimizes the computational cost in FS 2 , which is the next stage of APSO. FS 2 uses Binary Particle Swarm Optimization (BPSO) as a wrapper method for accurate feature selection. The second contribution of this paper is a new classification model , which combines evidence from statistical and distance based classification models. The proposed classification technique avoids the problems of the traditional NB and consists of two modules; Weighted Naïve Bayes Module (WNBM) and Distance Reinforcement Module (DRM). The proposed DBNB tries to accurately detect infected patients with the minimum time penalty based on the most effective features selected by APSO. DBNB has been compared with recent COVID-19 diagnose strategies. Experimental results have shown that DBNB outperforms recent COVID-19 diagnose strategies as it introduce the maximum accuracy with the minimum time penalty.},
  archive      = {J_PR},
  author       = {Warda M. Shaban and Asmaa H. Rabie and Ahmed I. Saleh and M.A. Abo-Elsoud},
  doi          = {10.1016/j.patcog.2021.108110},
  journal      = {Pattern Recognition},
  pages        = {108110},
  shortjournal = {Pattern Recognition},
  title        = {Accurate detection of COVID-19 patients based on distance biased naïve bayes (DBNB) classification strategy},
  volume       = {119},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SCOAT-net: A novel network for segmenting COVID-19 lung
opacification from CT images. <em>PR</em>, <em>119</em>, 108109. (<a
href="https://doi.org/10.1016/j.patcog.2021.108109">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic segmentation of lung opacification from computed tomography (CT) images shows excellent potential for quickly and accurately quantifying the infection of Coronavirus disease 2019 (COVID-19) and judging the disease development and treatment response. However, some challenges still exist, including the complexity and variability features of the opacity regions, the small difference between the infected and healthy tissues, and the noise of CT images. Due to limited medical resources, it is impractical to obtain a large amount of data in a short time, which further hinders the training of deep learning models. To answer these challenges, we proposed a novel spatial- and channel-wise coarse-to-fine attention network (SCOAT-Net), inspired by the biological vision mechanism, for the segmentation of COVID-19 lung opacification from CT images. With the UNet++ as basic structure, our SCOAT-Net introduces the specially designed spatial-wise and channel-wise attention modules, which serve to collaboratively boost the attention learning of the network and extract the efficient features of the infected opacification regions at the pixel and channel levels. Experiments show that our proposed SCOAT-Net achieves better results compared to several state-of-the-art image segmentation networks and has acceptable generalization ability.},
  archive      = {J_PR},
  author       = {Shixuan Zhao and Zhidan Li and Yang Chen and Wei Zhao and Xingzhi Xie and Jun Liu and Di Zhao and Yongjie Li},
  doi          = {10.1016/j.patcog.2021.108109},
  journal      = {Pattern Recognition},
  pages        = {108109},
  shortjournal = {Pattern Recognition},
  title        = {SCOAT-net: A novel network for segmenting COVID-19 lung opacification from CT images},
  volume       = {119},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Mutual information regularized identity-aware facial
expression recognition in compressed video. <em>PR</em>, <em>119</em>,
108105. (<a href="https://doi.org/10.1016/j.patcog.2021.108105">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How to extract effective expression representations that invariant to the identity-specific attributes is a long-lasting problem for facial expression recognition (FER). Most of the previous methods process the RGB images of a sequence, while we argue that the off-the-shelf and valuable expression-related muscle movement is already embedded in the compression format. In this paper, we target to explore the inter-subject variations eliminated facial expression representation in the compressed video domain. In the up to two orders of magnitude compressed domain, we can explicitly infer the expression from the residual frames and possibly extract identity factors from the I frame with a pre-trained face recognition network . By enforcing the marginal independence of them, the expression feature is expected to be purer for the expression and be robust to identity shifts. Specifically, we propose a novel collaborative min-min game for mutual information (MI) minimization in latent space. We do not need the identity label or multiple expression samples from the same person for identity elimination. Moreover, when the apex frame is annotated in the dataset, the complementary constraint can be further added to regularize the feature-level game. In testing, only the compressed residual frames are required to achieve expression prediction. Our solution can achieve comparable or better performance than the recent decoded image-based methods on the typical FER benchmarks with about 3 times faster inference.},
  archive      = {J_PR},
  author       = {Xiaofeng Liu and Linghao Jin and Xu Han and Jane You},
  doi          = {10.1016/j.patcog.2021.108105},
  journal      = {Pattern Recognition},
  pages        = {108105},
  shortjournal = {Pattern Recognition},
  title        = {Mutual information regularized identity-aware facial expression recognition in compressed video},
  volume       = {119},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Unsupervised neural domain adaptation for document image
binarization. <em>PR</em>, <em>119</em>, 108099. (<a
href="https://doi.org/10.1016/j.patcog.2021.108099">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Binarization is a well-known image processing task, whose objective is to separate the foreground of an image from the background. One of the many tasks for which it is useful is that of preprocessing document images in order to identify relevant information, such as text or symbols. The wide variety of document types, alphabets, and formats makes binarization challenging. There are multiple proposals with which to solve this problem, from classical manually-adjusted methods, to more recent approaches based on machine learning . The latter techniques require a large amount of training data in order to obtain good results; however, labeling a portion of each existing collection of documents is not feasible in practice. This is a common problem in supervised learning, which can be addressed by using the so-called Domain Adaptation (DA) techniques. These techniques take advantage of the knowledge learned in one domain, for which labeled data are available, to apply it to other domains for which there are no labeled data. This paper proposes a method that combines neural networks and DA in order to carry out unsupervised document binarization. However, when both the source and target domains are very similar, this adaptation could be detrimental. Our methodology, therefore, first measures the similarity between domains in an innovative manner in order to determine whether or not it is appropriate to apply the adaptation process. The results reported in the experimentation, when evaluating up to 20 possible combinations among five different domains, show that our proposal successfully deals with the binarization of new document domains without the need for labeled data.},
  archive      = {J_PR},
  author       = {Francisco J. Castellanos and Antonio-Javier Gallego and Jorge Calvo-Zaragoza},
  doi          = {10.1016/j.patcog.2021.108099},
  journal      = {Pattern Recognition},
  pages        = {108099},
  shortjournal = {Pattern Recognition},
  title        = {Unsupervised neural domain adaptation for document image binarization},
  volume       = {119},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A survey on text generation using generative adversarial
networks. <em>PR</em>, <em>119</em>, 108098. (<a
href="https://doi.org/10.1016/j.patcog.2021.108098">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work presents a thorough review concerning recent studies and text generation advancements using Generative Adversarial Networks . The usage of adversarial learning for text generation is promising as it provides alternatives to generate the so-called “natural” language. Nevertheless, adversarial text generation is not a simple task as its foremost architecture, the Generative Adversarial Networks , were designed to cope with continuous information (image) instead of discrete data (text). Thus, most works are based on three possible options, i.e., Gumbel-Softmax differentiation, Reinforcement Learning , and modified training objectives. All alternatives are reviewed in this survey as they present the most recent approaches for generating text using adversarial-based techniques. The selected works were taken from renowned databases, such as Science Direct, IEEEXplore, Springer, Association for Computing Machinery , and arXiv, whereas each selected work has been critically analyzed and assessed to present its objective, methodology, and experimental results.},
  archive      = {J_PR},
  author       = {Gustavo H. de Rosa and João P. Papa},
  doi          = {10.1016/j.patcog.2021.108098},
  journal      = {Pattern Recognition},
  pages        = {108098},
  shortjournal = {Pattern Recognition},
  title        = {A survey on text generation using generative adversarial networks},
  volume       = {119},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Principal component analysis in the wavelet domain.
<em>PR</em>, <em>119</em>, 108096. (<a
href="https://doi.org/10.1016/j.patcog.2021.108096">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a new principal component analysis method in the wavelet domain , which is useful for dimension reduction and feature extraction of multiple non-stationary time series. The proposed method is constructed using a novel combination of eigenanalysis and the local wavelet spectrum defined in the locally stationary wavelet process. Therefore, we can expect the proposed method to reflect a more generalized non-stationary time series beyond some limited types of signals that existing methods have performed. We investigate the theoretical results of estimated principal components and their loadings. The results of numerical examples, including the analysis of real seismic data and financial data, show the promising empirical properties of the proposed approach.},
  archive      = {J_PR},
  author       = {Yaeji Lim and Junhyeon Kwon and Hee-Seok Oh},
  doi          = {10.1016/j.patcog.2021.108096},
  journal      = {Pattern Recognition},
  pages        = {108096},
  shortjournal = {Pattern Recognition},
  title        = {Principal component analysis in the wavelet domain},
  volume       = {119},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Skeleton-based human action evaluation using graph
convolutional network for monitoring alzheimer’s progression.
<em>PR</em>, <em>119</em>, 108095. (<a
href="https://doi.org/10.1016/j.patcog.2021.108095">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human action evaluation (HAE) involves judgments about the abnormality and quality of human actions. If performed effectively, HAE based on skeleton data can be used to monitor the outcomes of behavioral therapies for Alzheimer’s disease (AD). In this paper, we propose a two-task graph convolutional network (2T-GCN) to represent skeleton data for HAE tasks involving abnormality detection and quality evaluation. The network is first evaluated using the UI-PRMD dataset and demonstrates accurate abnormality detection. Regarding quality evaluation, in addition to laboratory-collected UI-PRMD data, we test the network on a set of real exercise data collected from patients with AD. A numerical score indicating the degree to which actions deviate from normal is taken to reflect the severity of AD; thus, we apply 2T-GCN to determine such scores. Experimental results show that numerical scores for certain exercises performed by patients with AD are consistent with their AD severity level as identified by clinical staff. This corroboration highlights the potential of our approach for monitoring AD and other neurodegenerative diseases.},
  archive      = {J_PR},
  author       = {Bruce X.B. Yu and Yan Liu and Keith C.C. Chan and Qintai Yang and Xiaoying Wang},
  doi          = {10.1016/j.patcog.2021.108095},
  journal      = {Pattern Recognition},
  pages        = {108095},
  shortjournal = {Pattern Recognition},
  title        = {Skeleton-based human action evaluation using graph convolutional network for monitoring alzheimer’s progression},
  volume       = {119},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Assessment of dispersion patterns for negative stress
detection from electroencephalographic signals. <em>PR</em>,
<em>119</em>, 108094. (<a
href="https://doi.org/10.1016/j.patcog.2021.108094">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Negative stress, or distress, represents a serious problem in advanced societies given its adverse consequences for health. Many studies have focused on the detection of distress from physiological signals such as the electroencephalogram (EEG). To this respect, the combination of regularity-based quadratic sample entropy (QSampEn) and symbolic amplitude-aware permutation entropy (AAPE) has reported valuable outcomes in distress recognition. In the present work, the recently introduced symbolic metric called dispersion entropy (DispEn) is applied for the first time to the same problem. Statistically significant results reported by the single metric have demonstrated its capability for calm and distress detection. Furthermore, relevant differences have been found between the combination of QSampEn with either AAPE or DispEn, finding that the assessment of ordinal and dispersion patterns leads to distinct and complementary outcomes. Finally, the combination of the three entropy metrics has considerably overcome the results ever reported by other indices in similar studies.},
  archive      = {J_PR},
  author       = {Beatriz García-Martínez and Antonio Fernández-Caballero and Raúl Alcaraz and Arturo Martínez-Rodrigo},
  doi          = {10.1016/j.patcog.2021.108094},
  journal      = {Pattern Recognition},
  pages        = {108094},
  shortjournal = {Pattern Recognition},
  title        = {Assessment of dispersion patterns for negative stress detection from electroencephalographic signals},
  volume       = {119},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning features from covariance matrix of gabor wavelet
for face recognition under adverse conditions. <em>PR</em>,
<em>119</em>, 108085. (<a
href="https://doi.org/10.1016/j.patcog.2021.108085">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face recognition under adverse conditions , such as low-resolution, difficult illumination, blur and noise remains a challenging task. Among existing face recognition methods , Gabor wavelet plays a significant role and has robust performance under adverse conditions since it models the visual cortices of mammalian brain . It has been demonstrated the subbands of Gabor Wavelet (GW) can be efficiently represented by a covariance matrix . However, because covariance matrix does not belong to Euclidean space, Euclidean-based measure such as 2-norm cannot be directly applied to covariance matrix, and more importantly, it is difficult to incorporate learning techniques for the covariance matrix to promote the performance of face recognition. To address this issue, we propose two promising methods by learning the Covariance Matrix of Gabor Wavelet (LCMoG). The first method, called LCMoG-CNN, uses a shallow Convolutional Neural Network (CNN) to project the covariance matrices of GW into a feature vector of Euclidean space; the second method, called LCMoG-LWPZ, uses matrix-logarithm to embed the covariance matrix in the linear space and then uses Whitening Principal Component Analysis (WPCA) to learn the face features from the embedded covariance matrix. The proposed methods are effective to extract the fine features from the face image and have better performance than Deep CNN (DCNN) for small-varying face pose . For the large-varying face pose , LCMoG features combining with DCNN feature can enhance the performance of face recognition. In the experiments, the proposed methods yield promising recognition &amp; verification accuracies under adverse conditions.},
  archive      = {J_PR},
  author       = {Chaorong Li and Yuanyuan Huang and Wei Huang and Fengqing Qin},
  doi          = {10.1016/j.patcog.2021.108085},
  journal      = {Pattern Recognition},
  pages        = {108085},
  shortjournal = {Pattern Recognition},
  title        = {Learning features from covariance matrix of gabor wavelet for face recognition under adverse conditions},
  volume       = {119},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). COVID-index: A texture-based approach to classifying lung
lesions based on CT images. <em>PR</em>, <em>119</em>, 108083. (<a
href="https://doi.org/10.1016/j.patcog.2021.108083">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {COVID-19 is an infectious disease caused by a newly discovered type of coronavirus called SARS-CoV-2. Since the discovery of this disease in late 2019, COVID-19 has become a worldwide concern, mainly due to its high degree of contagion. As of April 2021, the number of confirmed cases of COVID-19 reported to the World Health Organization has already exceeded 135 million worldwide, while the number of deaths exceeds 2.9 million. Due to the impacts of the disease, efforts in the literature have intensified in terms of studying approaches aiming to detect COVID-19, with a focus on supporting and facilitating the process of disease diagnosis. This work proposes the application of texture descriptors based on phylogenetic relationships between species to characterize segmented CT volumes, and the subsequent classification of regions into COVID-19, solid lesion or healthy tissue. To evaluate our method, we use images from three different datasets. The results are promising, with an accuracy of 99.93\%, a recall of 99.93\%, a precision of 99.93\%, an F1-score of 99.93\%, and an AUC of 0.997. We present a robust, simple, and efficient method that can be easily applied to 2D and/or 3D images without limitations on their dimensionality.},
  archive      = {J_PR},
  author       = {Vitória de Carvalho Brito and Patrick Ryan Sales dos Santos and Nonato Rodrigues de Sales Carvalho and Antonio Oseas de Carvalho Filho},
  doi          = {10.1016/j.patcog.2021.108083},
  journal      = {Pattern Recognition},
  pages        = {108083},
  shortjournal = {Pattern Recognition},
  title        = {COVID-index: A texture-based approach to classifying lung lesions based on CT images},
  volume       = {119},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Detecting pulmonary diseases using deep features in x-ray
images. <em>PR</em>, <em>119</em>, 108081. (<a
href="https://doi.org/10.1016/j.patcog.2021.108081">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {COVID-19 leads to radiological evidence of lower respiratory tract lesions, which support analysis to screen this disease using chest X-ray. In this scenario, deep learning techniques are applied to detect COVID-19 pneumonia in X-ray images, aiding a fast and precise diagnosis. Here, we investigate seven deep learning architectures associated with data augmentation and transfer learning techniques to detect different pneumonia types. We also propose an image resizing method with the maximum window function that preserves anatomical structures of the chest. The results are promising, reaching an accuracy of 99.8\% considering COVID-19, normal, and viral and bacterial pneumonia classes. The differentiation between viral pneumonia and COVID-19 achieved an accuracy of 99.8\%, and 99.9\% of accuracy between COVID-19 and bacterial pneumonia. We also evaluated the impact of the proposed image resizing method on classification performance comparing with the bilinear interpolation ; this pre-processing increased the classification rate regardless of the deep learning architectures used. We c ompared our results with ten related works in the state-of-the-art using eight sets of experiments, which showed that the proposed method outperformed them in most cases. Therefore, we demonstrate that deep learning models trained with pre-processed X-ray images could precisely assist the specialist in COVID-19 detection.},
  archive      = {J_PR},
  author       = {Pablo Vieira and Orrana Sousa and Deborah Magalhães and Ricardo Rabêlo and Romuere Silva},
  doi          = {10.1016/j.patcog.2021.108081},
  journal      = {Pattern Recognition},
  pages        = {108081},
  shortjournal = {Pattern Recognition},
  title        = {Detecting pulmonary diseases using deep features in X-ray images},
  volume       = {119},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep compact polyhedral conic classifier for open and closed
set recognition. <em>PR</em>, <em>119</em>, 108080. (<a
href="https://doi.org/10.1016/j.patcog.2021.108080">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a new deep neural network classifier that simultaneously maximizes the inter-class separation and minimizes the intra-class variation by using the polyhedral conic classification function. The proposed method has one loss term that allows the margin maximization to maximize the inter-class separation and another loss term that controls the compactness of the class acceptance regions. Our proposed method has a nice geometric interpretation using polyhedral conic function geometry. We tested the proposed method on various visual classification problems including closed/open set recognition and anomaly detection . The experimental results show that the proposed method typically outperforms other state-of-the-art methods, and becomes a better choice compared to other tested methods especially for open set recognition type problems. The source code of the proposed method is available at https://github.com/bdrhn9/dc-epcc .},
  archive      = {J_PR},
  author       = {Hakan Cevikalp and Bedirhan Uzun and Okan Köpüklü and Gurkan Ozturk},
  doi          = {10.1016/j.patcog.2021.108080},
  journal      = {Pattern Recognition},
  pages        = {108080},
  shortjournal = {Pattern Recognition},
  title        = {Deep compact polyhedral conic classifier for open and closed set recognition},
  volume       = {119},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Metaheuristic search based feature selection methods for
classification of cancer. <em>PR</em>, <em>119</em>, 108079. (<a
href="https://doi.org/10.1016/j.patcog.2021.108079">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cancer is a cluster of diseases caused due to unusual cell growth. This paper aims to discover cancer prediction from the microarray gene expression data using the selected features. The metaheuristic search algorithms select the global and local optimal features using population and neighbourhood based algorithms. Although the ant colony optimization and genetic algorithm search for the global optimal features from the dataset entails enhanced classification, sometimes there occur some challenges in the selection of neighbourhood features. Against this background, two feature selection algorithms are proposed to hybridize tabu search, a neighbourhood based search algorithm with global optimal feature selection algorithm. Those are (1) Ant Colony Optimization and Tabu search with Fuzzy Rough set for Optimal feature selection (ACTFRO) algorithm, (2) Genetic algorithm and Tabu search with Fuzzy Rough set for Optimal feature selection (GATFRO) algorithm. The performance of proposed feature selection algorithms is assessed through a fuzzy rough nearest neighbour classifier using ten-fold cross validation. Four cancer medical datasets and one non-medical dataset are used to analyse the performance of the proposed algorithms in terms of classification accuracy, computation time, sensitivity, specificity, f-measure, receiver operation characteristics and positive predicted value. Results derived from the different performance metrics confirm that the proposed algorithms evidence effective global and local feature selection hybridization with improved results.},
  archive      = {J_PR},
  author       = {L. Meenachi and S. Ramakrishnan},
  doi          = {10.1016/j.patcog.2021.108079},
  journal      = {Pattern Recognition},
  pages        = {108079},
  shortjournal = {Pattern Recognition},
  title        = {Metaheuristic search based feature selection methods for classification of cancer},
  volume       = {119},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A novel eye center localization method for multiview faces.
<em>PR</em>, <em>119</em>, 108078. (<a
href="https://doi.org/10.1016/j.patcog.2021.108078">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing studies on eye center localization have mostly applied localization methods on faces viewed from frontal angles or faces with small yaw-rotation angles. This study proposed a novel eye center localization method that can effectively localize the eye centers under various situations on multiview faces with large yaw-rotation angles (from +67.5° to −67.5°). First, this study developed a multiview face detector that can be applied to large yaw angles and can flexibly detect and precisely capture the face region. The face detector can facilitate the generation of satisfactory results by a complete representation generative adversarial network (CR-GAN). Because a large yaw-rotation angle can substantially reduce the completeness of eye representation in an image or even cause the eyes to disappear within the face image, this study used a CR-GAN to produce frontal face images to solve the problem of incomplete representation in multiview face images. Furthermore, this study proposed a new iris-ripple filter to increase the accuracy and robustness of gradient localization. Finally, a new depth corresponding-points conversion method was proposed to automatically estimate the rotation variable between two faces and the conversion relationship between the eye centers. This method can effectively solve the problem of instability resulting from a CR-GAN during eye generation and can ensure localization accuracy for eyeballs with subtle changes. According to the experimental results, compared with other advanced methods, the proposed method exhibited higher accuracy and robustness in eye center localization when applied to images obtained from four databases that involved various challenging situations, including large yaw-rotation angle, illumination change, head pose change, gaze interaction, and complete occlusion of eyes.},
  archive      = {J_PR},
  author       = {Wei-Yen Hsu and Chi-Jui Chung},
  doi          = {10.1016/j.patcog.2021.108078},
  journal      = {Pattern Recognition},
  pages        = {108078},
  shortjournal = {Pattern Recognition},
  title        = {A novel eye center localization method for multiview faces},
  volume       = {119},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). IsGAN: Identity-sensitive generative adversarial network for
face photo-sketch synthesis. <em>PR</em>, <em>119</em>, 108077. (<a
href="https://doi.org/10.1016/j.patcog.2021.108077">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face photo-sketch synthesis aims to generate face sketches from real photos and vice versa. It can be abstracted as a constrained quantization problem. Although many efforts have been dedicated to this problem, it is still a challenging task to synthesize detail-preserving photos or sketches due to the significant differences between face sketch (drawn by people) and photo (taken by cameras) domains. In this paper, we propose a novel Identity-sensitive Generative Adversarial Network (IsGAN) to address it. Our key insight is to formalize face photo-sketch synthesis as a special case of image-to-image translation and propose to embed identity information through adversarial learning. In particular, an adversarial architecture is used to capture the differences between the two domains, and a new network loss, namely, identity recognition loss is introduced to preserve the detailed identifiable information, which is crucial for photo-sketch synthesis. In addition, to enforce structural consistency during generation, a cyclic-synthesized loss is applied between the generated image of one domain and cycled image of another. The experiments on the CUFS and CUFSF datasets suggest that our model achieves state-of-the-art performance in both qualitative and quantitative measures.},
  archive      = {J_PR},
  author       = {Lan Yan and Wenbo Zheng and Chao Gou and Fei-Yue Wang},
  doi          = {10.1016/j.patcog.2021.108077},
  journal      = {Pattern Recognition},
  pages        = {108077},
  shortjournal = {Pattern Recognition},
  title        = {IsGAN: Identity-sensitive generative adversarial network for face photo-sketch synthesis},
  volume       = {119},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A confidence prior for image dehazing. <em>PR</em>,
<em>119</em>, 108076. (<a
href="https://doi.org/10.1016/j.patcog.2021.108076">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {By sorting channel-minimized values in an ascending order, we individually put the values of several existing image dehazing priors on the curve of sorted values to propose a framework for unifying and understanding these priors. Then we propose a confidence ratio to specify the probability of each channel-minimized value within a range, and thus we can intuitively find a suitable point from the curve, which is actually defined as a novel prior. Although our novel prior and existing ones are perfectly unified under the same framework, our prior has an important advantage that it can freely control the suppression degree of outliers by directly adjusting the confidence ratio of channel-minimized values. In this way, we can remove influence of outliers in a controllable manner. To solve the problems caused by heterogeneity of pixel values and abrupt jumps of scene depths in hazy images, we adopt a regression method to adaptively learn the relationship between patch appearance and confidence ratios for all pixels. To further improve robustness, we use a Gaussian kernel to smooth the estimated confidence ratios for local consistency. Extensive experiments on both natural and synthetic images show that our confidence prior achieves significantly better performance than existing state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Feiniu Yuan and Yu Zhou and Xue Xia and Xueming Qian and Jian Huang},
  doi          = {10.1016/j.patcog.2021.108076},
  journal      = {Pattern Recognition},
  pages        = {108076},
  shortjournal = {Pattern Recognition},
  title        = {A confidence prior for image dehazing},
  volume       = {119},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Design of cancelable MCC-based fingerprint templates using
dyno-key model. <em>PR</em>, <em>119</em>, 108074. (<a
href="https://doi.org/10.1016/j.patcog.2021.108074">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Minutia Cylinder Code (MCC) is an effective, high-quality representation of local minutia structures. MCC templates demonstrate fast and excellent fingerprint matching performance, but if compromised, they can be reverse-engineered to retrieve minutia information. In this paper, we propose alignment-free cancelable MCC-based templates by exploiting the MCC feature extraction and representation. The core component of our design is a dynamic random key model, called Dyno-key model. The Dyno-key model dynamically extracts elements from MCC’s binary feature vectors based on randomly generated keys. Those extracted elements are discarded after the block-based logic operations so as to increase security. Leveling with the performance of the unprotected, reproduced MCC templates, the proposed method exhibits competitive performance in comparison with state-of-the-art cancelable fingerprint templates, as evaluated over seven public databases, FVC2002 DB1-DB3, FVC2004 DB1 and DB2, and FVC2006 DB2 and DB3. The proposed cancelable MCC-based templates satisfy all the requirements of biometric template protection.},
  archive      = {J_PR},
  author       = {Aseel Bedari and Song Wang and Wencheng Yang},
  doi          = {10.1016/j.patcog.2021.108074},
  journal      = {Pattern Recognition},
  pages        = {108074},
  shortjournal = {Pattern Recognition},
  title        = {Design of cancelable MCC-based fingerprint templates using dyno-key model},
  volume       = {119},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Simultaneous positive sequential vectors modeling and
unsupervised feature selection via continuous hidden markov models.
<em>PR</em>, <em>119</em>, 108073. (<a
href="https://doi.org/10.1016/j.patcog.2021.108073">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since positive data vectors are often naturally generated in various real-life applications, positive vectors modeling has become an important research topic. In this article, we tackle the problem of modeling positive sequential vectors through continuous hidden Markov models (HMMs). Motivated by several recent studies in which the generalized inverted Dirichlet (GID) distribution has provided better performance than the Gaussian distribution for modeling positive data, instead of adopting Gaussian mixture models (GMM) as the emission density for conventional continuous HMMs, we theoretically propose a novel HMM by considering the mixture of GID distributions as the emission density. Moreover, to cope with high-dimensional data which may contain irrelevant features, an unsupervised localized feature selection method is incorporated with our model, which results in a unified framework that can simultaneously perform positive sequential data modeling and feature selection. To learn the proposed model, we develop a convergence-guaranteed algorithm based on variational Bayes. The advantages of our model are demonstrated through both simulated data sets and a real-life application about human action recognition .},
  archive      = {J_PR},
  author       = {Wentao Fan and Ru Wang and Nizar Bouguila},
  doi          = {10.1016/j.patcog.2021.108073},
  journal      = {Pattern Recognition},
  pages        = {108073},
  shortjournal = {Pattern Recognition},
  title        = {Simultaneous positive sequential vectors modeling and unsupervised feature selection via continuous hidden markov models},
  volume       = {119},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Holistic word descriptor for lexicon reduction in
handwritten arabic documents. <em>PR</em>, <em>119</em>, 108072. (<a
href="https://doi.org/10.1016/j.patcog.2021.108072">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most of word recognition systems rely on a pre-defined lexicon in aims to achieve high performance. Recently, the availability of training /testing data allows to include a huge number of words in the lexicon to recognize. However, this leads to high computation cost as the lexicon is grown. In addition, including more and more word-classes may lead to increase the burden on classification methods and degrade the recognition rate. In this work, we propose a holistic word descriptor for word lexicon reduction in Arabic handwritten documents. The proposed descriptor represents geometrical features of word shape through three main feature sets, defined from multi-scale convexity concavity analysis. The first two sets are dedicated to defined the number of peaks and their intensity levels of convexity/concavity peaks, respectively. In contrast, the last set is dedicated to define a region codes of the peaks by analyzing their regions according to their spatial information. Given a query word and lexicon(reference dataset), the lexicon reduction system is applied by first defining the holistic word descriptor for both query word and each word in the lexicon. The lexicon is then indexed according to its distances to the query word descriptor. Finally, the reduced lexicon is formulated from the first k k th entries of the indexed lexicon. The proposed system has been evaluated under two well-known Arabic datasets, namely Ibn Sina and IFN/ENIT. Reported results show superior performance compared to prior art, with 93.7\% 93.7\% and 91.2\% 91.2\% reduction efficacy for Ibn Sina and IFN/ENIT, respectively.},
  archive      = {J_PR},
  author       = {Said Elaiwat},
  doi          = {10.1016/j.patcog.2021.108072},
  journal      = {Pattern Recognition},
  pages        = {108072},
  shortjournal = {Pattern Recognition},
  title        = {Holistic word descriptor for lexicon reduction in handwritten arabic documents},
  volume       = {119},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Lung segmentation and automatic detection of COVID-19 using
radiomic features from chest CT images. <em>PR</em>, <em>119</em>,
108071. (<a href="https://doi.org/10.1016/j.patcog.2021.108071">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper aims to develop an automatic method to segment pulmonary parenchyma in chest CT images and analyze texture features from the segmented pulmonary parenchyma regions to assist radiologists in COVID-19 diagnosis. A new segmentation method, which integrates a three-dimensional (3D) V-Net with a shape deformation module implemented using a spatial transform network (STN), was proposed to segment pulmonary parenchyma in chest CT images. The 3D V-Net was adopted to perform an end-to-end lung extraction while the deformation module was utilized to refine the V-Net output according to the prior shape knowledge. The proposed segmentation method was validated against the manual annotation generated by experienced operators. The radiomic features measured from our segmentation results were further analyzed by sophisticated statistical models with high interpretability to discover significant independent features and detect COVID-19 infection. Experimental results demonstrated that compared with the manual annotation, the proposed segmentation method achieved a Dice similarity coefficient of 0.9796, a sensitivity of 0.9840, a specificity of 0.9954, and a mean surface distance error of 0.0318 mm. Furthermore, our COVID-19 classification model achieved an area under curve (AUC) of 0.9470, a sensitivity of 0.9670, and a specificity of 0.9270 when discriminating lung infection with COVID-19 from community-acquired pneumonia and healthy controls using statistically significant radiomic features. The significant features measured from our segmentation results agreed well with those from the manual annotation. Our approach has great promise for clinical use in facilitating automatic diagnosis of COVID-19 infection on chest CT images.},
  archive      = {J_PR},
  author       = {Chen Zhao and Yan Xu and Zhuo He and Jinshan Tang and Yijun Zhang and Jungang Han and Yuxin Shi and Weihua Zhou},
  doi          = {10.1016/j.patcog.2021.108071},
  journal      = {Pattern Recognition},
  pages        = {108071},
  shortjournal = {Pattern Recognition},
  title        = {Lung segmentation and automatic detection of COVID-19 using radiomic features from chest CT images},
  volume       = {119},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Minimum variance-embedded kernelized extension of extreme
learning machine for imbalance learning. <em>PR</em>, <em>119</em>,
108069. (<a href="https://doi.org/10.1016/j.patcog.2021.108069">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In classification problems, detecting a skew class has extensively been studied in the machine learning community. Traditional extreme learning machine (ELM) algorithm becomes biased towards the majority class due to imbalance learning. To handle this problem, several extensions of ELM have been proposed such as variances-constrained weighted ELM (VW-ELM) and class-specific kernelized ELM (CSKELM). Kernelized ELM (KELM) has a better generalization capability than traditional ELM. This work proposes novel minimum variance embedded-kernelized weighted extreme learning machine (MVKWELM) and minimum variance-embedded class-specific kernelized extreme learning machine (MVCSKELM) methods for handling the imbalanced classification problems more effectively. These methods constitute novel extensions of the VW-ELM and CSKELM classifiers respectively. This minimum variance-embedding enhances the generalization capability of the algorithm by minimizing the intra-class variance. MVCSKELM uses the advantages of both the minimum variance-embedding framework and the class-specific regularization parameters . The proposed MVCSKELM also has comparable computational complexity compared to kernelized weighted ELM (KWELM). The proposed MVCSKELM adopted class-specific regularization parameters , which are determined by using class distribution. The proposed works are evaluated using benchmark real-world imbalanced datasets downloaded from the KEEL dataset repository. The experimental results demonstrate that MVKWELM and MVCSKELM achieve superior performance in contrast to KELM, KWELM, CCR-KELM, CSKELM, RUSBoost, WKSMOTE, VW-ELM, and EasyEnsemble for imbalance learning.},
  archive      = {J_PR},
  author       = {Bhagat Singh Raghuwanshi and Sanyam Shukla},
  doi          = {10.1016/j.patcog.2021.108069},
  journal      = {Pattern Recognition},
  pages        = {108069},
  shortjournal = {Pattern Recognition},
  title        = {Minimum variance-embedded kernelized extension of extreme learning machine for imbalance learning},
  volume       = {119},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Weakly-supervised temporal attention 3D network for human
action recognition. <em>PR</em>, <em>119</em>, 108068. (<a
href="https://doi.org/10.1016/j.patcog.2021.108068">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {From a series of observations, we have inferred that human actions in videos are defined by a set of significant frames. In this paper, we propose a weakly-supervised temporal attention 3D network for human action recognition , called as TA3DNet, to accelerate 3D convolutional neural networks (3D CNNs) by temporally assigning different importance to each frame. First, we obtain short-term frames with long-term connection by regularly or randomly skipping frames to avoid temporal redundancy, and apply 3D convolutional layers to extract features for action recognition. Then, we apply a temporal attention module to assign different weights to each frame. We train the temporal attention module in a weakly-supervised manner that updates weights based on only class labels without event information and extra labels. Thus, TA3DNet reduces the number of input frames and constructs a lightweight network for action recognition. Experimental results demonstrate that TA3DNet achieves high performance on two challenging datasets (UCF101 and HMDB51) and outperforms state-of-the-art methods for action recognition.},
  archive      = {J_PR},
  author       = {Jonghyun Kim and Gen Li and Inyong Yun and Cheolkon Jung and Joongkyu Kim},
  doi          = {10.1016/j.patcog.2021.108068},
  journal      = {Pattern Recognition},
  pages        = {108068},
  shortjournal = {Pattern Recognition},
  title        = {Weakly-supervised temporal attention 3D network for human action recognition},
  volume       = {119},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MaskCOV: A random mask covariance network for
ultra-fine-grained visual categorization. <em>PR</em>, <em>119</em>,
108067. (<a href="https://doi.org/10.1016/j.patcog.2021.108067">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ultra-fine-grained visual categorization (ultra-FGVC) categorizes objects with more similar patterns between classes than those in fine-grained visual categorization (FGVC), e.g. , where the spectrum of granularity significantly moves down from classifying species to classifying cultivars within the same species. It is considered as an open research problem mainly due to the following challenges. First, the inter-class differences among images are much smaller by level of orders ( e.g. , cultivars in the same species) than those in current FGVC tasks ( e.g. , species). Second, there is only a few samples per category, which is beyond the ability of most large training data favored convolutional neural network methods. To address these problems, we propose a novel random mask covariance network (MaskCOV), which integrates an auxiliary self-supervised learning module with a powerful in-image data augmentation scheme for the ultra-FGVC. Specifically, we first uniformly partition input images into patches and then augment data by randomly shuffling and masking these patches. On top of that, we introduce an auxiliary self-supervised learning module of predicting the spatial covariance context of these patches to increase discriminability of our network for classification. Very encouraging experimental results of the proposed method in comparison with the state-of-the-art benchmarks demonstrate its superiority and potential of MaskCOV concept, which pushes research boundary forward from the fine-grained to the ultra-fine-grained visual categorization.},
  archive      = {J_PR},
  author       = {Xiaohan Yu and Yang Zhao and Yongsheng Gao and Shengwu Xiong},
  doi          = {10.1016/j.patcog.2021.108067},
  journal      = {Pattern Recognition},
  pages        = {108067},
  shortjournal = {Pattern Recognition},
  title        = {MaskCOV: A random mask covariance network for ultra-fine-grained visual categorization},
  volume       = {119},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A deep fusion framework for unlabeled data-driven tumor
recognition. <em>PR</em>, <em>119</em>, 108066. (<a
href="https://doi.org/10.1016/j.patcog.2021.108066">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional pattern recognition problems are usually accomplished through two successive stages of representation and classification, the generalization ability and stability are difficult to guarantee for small samples and category imbalance. For tackling these problems, an unlabeled data-driven representation learning classification (RLC) fused model is constructed by integrating representation learning and classification into one model, rather than simple putting the two stages together. The RLC fused model mainly focuses on interactive iteratively optimizing representation learning and classification in a model, guiding and reinforcing each other. Under the framework of RLC, a deep nonnegative matrix factorization (NMF) is adopted for representation learning by complementing the advantages of NMF and deep learning, and avoiding complex network structure and parameter modulation. The framework is called deep NMF-RLC fusion model, which can achieve good performance for binary classification even the simplest linear regression classifier is used. The model explores useful information embedded in unlabeled data, and is suitable for small training samples and unbalanced classification. The performance of the proposed framework is verified on genetic-based tumor recognition, which contains all three stages of early diagnosis, tumor type recognition and postoperative metastasis. Experiments show that, compared with the published state-of-the-art methods and results, there are significant improvements in classification accuracy, specificity and sensitivity.},
  archive      = {J_PR},
  author       = {Xiaohui Yang and Wenming Wu and Licheng Jiao and Changzhe Jiao and Zhicheng Jiao},
  doi          = {10.1016/j.patcog.2021.108066},
  journal      = {Pattern Recognition},
  pages        = {108066},
  shortjournal = {Pattern Recognition},
  title        = {A deep fusion framework for unlabeled data-driven tumor recognition},
  volume       = {119},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Solving archaeological puzzles. <em>PR</em>, <em>119</em>,
108065. (<a href="https://doi.org/10.1016/j.patcog.2021.108065">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper focuses on the re-assembly of an archaeological artifact, given images of its fragments. This problem can be considered as a special challenging case of puzzle solving. The restricted case of re-assembly of a natural image from square pieces has been investigated extensively and was shown to be a difficult problem in its own right. Likewise, the case of matching “clean” 2D polygons/splines based solely on their geometric properties has been studied. But what if these ideal conditions do not hold? This is the problem addressed in the paper. Three unique characteristics of archaeological fragments make puzzle solving extremely difficult: (1) The fragments are of general shape; (2) They are abraded, especially at the boundaries (where the strongest cues for matching should exist); and (3) The domain of valid transformations between the pieces is continuous. The key contribution of this paper is a fully-automatic and general algorithm that addresses puzzle solving in this intriguing domain. We show that our approach manages to correctly reassemble dozens of broken artifacts and frescoes.},
  archive      = {J_PR},
  author       = {Niv Derech and Ayellet Tal and Ilan Shimshoni},
  doi          = {10.1016/j.patcog.2021.108065},
  journal      = {Pattern Recognition},
  pages        = {108065},
  shortjournal = {Pattern Recognition},
  title        = {Solving archaeological puzzles},
  volume       = {119},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Collaborative feature-weighted multi-view fuzzy c-means
clustering. <em>PR</em>, <em>119</em>, 108064. (<a
href="https://doi.org/10.1016/j.patcog.2021.108064">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fuzzy c-means (FCM) clustering had been extended for handling multi-view data with collaborative idea. However, these collaborative multi-view FCM treats multi-view data under equal importance of feature components. In general, different features should take different weights for clustering real multi-view data. In this paper, we propose a novel multi-view FCM (MVFCM) clustering algorithm with view and feature weights based on collaborative learning , called collaborative feature-weighted MVFCM (Co-FW-MVFCM). The Co-FW-MVFCM contains a two-step schema that includes a local step and a collaborative step. The local step is a single-view partition process to produce local partition clustering in each view, and the collaborative step is sharing information of their memberships between different views. These two steps are then continuing by an aggregation way to get a global result after collaboration. Furthermore, the embedded feature-weighted procedure in Co-FW-MVFCM can give feature reduction to exclude redundant/irrelevant feature components during clustering processes . Experiments with several data sets demonstrate that the proposed Co-FW-MVFCM algorithm can completely identify irrelevant feature components in each view and that, additionally, it can improve the performance of the algorithm. Comparisons of Co-FW-MVFCM with some existing MVFCM algorithms are made and also demonstrated the effectiveness and usefulness of the proposed Co-FW-MVFCM clustering algorithm.},
  archive      = {J_PR},
  author       = {Miin-Shen Yang and Kristina P. Sinaga},
  doi          = {10.1016/j.patcog.2021.108064},
  journal      = {Pattern Recognition},
  pages        = {108064},
  shortjournal = {Pattern Recognition},
  title        = {Collaborative feature-weighted multi-view fuzzy c-means clustering},
  volume       = {119},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Weak segmentation supervised deep neural networks for
pedestrian detection. <em>PR</em>, <em>119</em>, 108063. (<a
href="https://doi.org/10.1016/j.patcog.2021.108063">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic segmentation has been used successfully as a complementary information source in pedestrian detection. However, it requires accurate pixel-level semantic segmentation annotations for training, but it is extremely time-consuming to obtain these. In this work, we solve this problem by using weak segmentation masks automatically generated by depth images. This enables joint semantic segmentation and pedestrian detection with only ground truth bounding boxes for training. We show that this joint training boosts the performance of the pedestrian detector. Moreover, we show that fusing the outputs of the classification network and the generated segmentation masks leads to a further detection performance improvement. Extensive experiments have been conducted on three RGBD pedestrian datasets to demonstrate the effectiveness of our proposed method. As a byproduct, we also obtain pedestrian segmentation results of good quality, without using pixel-level segmentation annotations during training.},
  archive      = {J_PR},
  author       = {Zhixin Guo and Wenzhi Liao and Yifan Xiao and Peter Veelaert and Wilfried Philips},
  doi          = {10.1016/j.patcog.2021.108063},
  journal      = {Pattern Recognition},
  pages        = {108063},
  shortjournal = {Pattern Recognition},
  title        = {Weak segmentation supervised deep neural networks for pedestrian detection},
  volume       = {119},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Mining consistent correspondences using co-occurrence
statistics. <em>PR</em>, <em>119</em>, 108062. (<a
href="https://doi.org/10.1016/j.patcog.2021.108062">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a mismatch removal method, which mines consistent image feature correspondences using co-occurrence statistics. The proposed method relies on a co-occurrence matrix that counts the number of pixel value pairs co-occurring within the images. Specifically, we propose to integrate the co-occurrence statistics with local spatial information, to preserve the consensus of neighborhood elements. Then, a new measure based on co-occurrence statistics is defined for correspondence similarity, to preserve the consensus of neighborhood topology. After that, with the consensus of neighborhood elements and neighborhood topology, the mismatch removal problem is formulated into a mathematical model, which has a closed-form solution. Extensive experiments show that the proposed method is able to achieve superior or competitive performance on matching accuracy over several state-of-the-art competing methods. In addition, we further exploit the consensus of neighborhood elements and neighborhood topology to propose a novel guided sampling method, which can significantly improve the quality of sampling minimal subsets over state-of-the-arts for two-view geometric model fitting.},
  archive      = {J_PR},
  author       = {Guobao Xiao and Shiping Wang and Han Wang and Jiayi Ma},
  doi          = {10.1016/j.patcog.2021.108062},
  journal      = {Pattern Recognition},
  pages        = {108062},
  shortjournal = {Pattern Recognition},
  title        = {Mining consistent correspondences using co-occurrence statistics},
  volume       = {119},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Ensemble selection with joint spectral clustering and
structural sparsity. <em>PR</em>, <em>119</em>, 108061. (<a
href="https://doi.org/10.1016/j.patcog.2021.108061">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generally, ensemble selection techniques are split into two categories: dynamic and static. Static ensemble selection selects a fixed subset of the original ensemble which improves the space complexity but is not flexible to each test instance. Dynamic ensemble selection selects base learners on-the-fly according to each test instance but it does not significantly improve the complexity. Currently, there is no ensemble selection technique that is robust to the test instances as well as improves space complexity. To narrow this gap, we propose a novel static ensemble selection method, called Ensemble Selection with Joint Spectral Clustering and Structural Sparsity . This method integrates spectral clustering and structural sparsity into a joint framework whose ensemble selection result is robust to test instances and consumes less space. Using 25 datasets from KEEL and UCI, we demonstrate the effectiveness of our proposed algorithm and its promising performance compared to that of other state-of-the-art algorithms.},
  archive      = {J_PR},
  author       = {Zhenlei Wang and Suyun Zhao and Zheng Li and Hong Chen and Cuiping Li and Yufeng Shen},
  doi          = {10.1016/j.patcog.2021.108061},
  journal      = {Pattern Recognition},
  pages        = {108061},
  shortjournal = {Pattern Recognition},
  title        = {Ensemble selection with joint spectral clustering and structural sparsity},
  volume       = {119},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A triplet network framework based automatic assessment of
simulation quality for respiratory droplet propagation. <em>PR</em>,
<em>119</em>, 108060. (<a
href="https://doi.org/10.1016/j.patcog.2021.108060">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Respiratory droplet propagation has been extensively explored with simulation and experimental methods. However, there still exists a huge gap between these methods, making automatic assessment of simulation quality quantitatively being a challenge. To address above problem, in this work, a triplet neural network framework with multi-scale CNN-BiLSTM network is developed. Firstly, Conditional Variational Auto-Encoder (CVAE) is utilized to generate multi-view simulations. Secondly, YOLOv3 is adopted to extract droplet regions of real image and simulation results. Then, a multi-scale CNN-BiLSTM network with attentive temporal pooling is designed to extract and aggregate temporal information across consecutive frames. Finally, all above networks are constructed into a triplet structure with triplet loss, and a regularization constraint being denoted as reconstruction term and prediction term is proposed. To demonstrate the performance of our approach, a new dataset is established including real sequences of cough droplets and simulation results. We validate the effectiveness and feasibility of our proposed framework using our dataset and two benchmarks, the PSB dataset and the ETH dataset, for 3D object retrieval. Our approach outperforms state-of-the-arts on our dataset and achieves comparative performance on PSB and ETH for 3D object retrieval, given quantitative quality assessment of simulation for droplet respiratory propagation automatically.},
  archive      = {J_PR},
  author       = {Jinlong Hu and Songhua Xu and Xiangdong Ding},
  doi          = {10.1016/j.patcog.2021.108060},
  journal      = {Pattern Recognition},
  pages        = {108060},
  shortjournal = {Pattern Recognition},
  title        = {A triplet network framework based automatic assessment of simulation quality for respiratory droplet propagation},
  volume       = {119},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A branch and bound irredundant graph algorithm for
large-scale MLCS problems. <em>PR</em>, <em>119</em>, 108059. (<a
href="https://doi.org/10.1016/j.patcog.2021.108059">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Finding the multiple longest common subsequences (MLCS) among many long sequences (i.e., the large scale MLCS problem) has many important applications, such as gene alignment, disease diagnosis, and documents similarity check, etc. It is an NP-hard problem (Maier et al., 1978). The key bottle neck of this problem is that the existing state-of-the-art algorithms must construct a huge graph (called direct acyclic graph, briefly DAG), and the computer usually has no enough space to store and handle this graph. Thus the existing algorithms cannot solve the large scale MLCS problem. In order to quickly solve the large-scale MLCS problem within limited computer resources, this paper therefore proposes a branch and bound irredundant graph algorithm called Big-MLCS, which constructs a much smaller DAG (called Small-DAG) than the existing algorithms do by a branch and bound method, and designs a new data structure to efficiently store and handle Small-DAG. By these schemes, Big-MLCS is more efficient than the existing algorithms. Also, we compare the proposed algorithm with two state-of-the-art algorithms through the experiments, and the results show that the proposed algorithm outperforms the compared algorithms and is more suitable to large-scale MLCS problems.},
  archive      = {J_PR},
  author       = {Chunyang Wang and Yuping Wang and Yiuming Cheung},
  doi          = {10.1016/j.patcog.2021.108059},
  journal      = {Pattern Recognition},
  pages        = {108059},
  shortjournal = {Pattern Recognition},
  title        = {A branch and bound irredundant graph algorithm for large-scale MLCS problems},
  volume       = {119},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fused lasso for feature selection using structural
information. <em>PR</em>, <em>119</em>, 108058. (<a
href="https://doi.org/10.1016/j.patcog.2021.108058">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most state-of-the-art feature selection methods tend to overlook the structural relationship between a pair of samples associated with each feature dimension, which may encapsulate useful information for refining the performance of feature selection. Moreover, they usually consider candidate feature relevancy equivalent to selected feature relevancy , and therefore, some less relevant features may be misinterpreted as salient features . To overcome these issues, we propose a new feature selection method based on graph-based feature representations and the Fused Lasso framework in this paper. Unlike state-of-the-art feature selection approaches, our method has two main advantages. First, it can accommodate structural relationship between a pair of samples through a graph-based feature representation. Second, our method can enhance the trade-off between the relevancy of each individual feature on the one hand and its redundancy between pairwise features on the other. This is achieved through the use of a Fused Lasso framework applied to features reordered on the basis of their relevance with respect to the target feature. To effectively solve the optimization problem , an iterative algorithm is developed to identify the most discriminative features . Experiments demonstrate that our proposed approach can outperform its competitors on benchmark datasets.},
  archive      = {J_PR},
  author       = {Lixin Cui and Lu Bai and Yue Wang and Philip S. Yu and Edwin R. Hancock},
  doi          = {10.1016/j.patcog.2021.108058},
  journal      = {Pattern Recognition},
  pages        = {108058},
  shortjournal = {Pattern Recognition},
  title        = {Fused lasso for feature selection using structural information},
  volume       = {119},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Mitigating severe over-parameterization in deep
convolutional neural networks through forced feature abstraction and
compression with an entropy-based heuristic. <em>PR</em>, <em>119</em>,
108057. (<a href="https://doi.org/10.1016/j.patcog.2021.108057">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional Neural Networks (CNNs) such as ResNet-50, DenseNet-40 and ResNeXt-56 are severely over-parameterized, necessitating a consequent increase in the computational resources required for model training which scales exponentially for increments in model depth. In this paper, we propose an Entropy-Based Convolutional Layer Estimation (EBCLE) heuristic which is robust and simple, yet effective in resolving the problem of over-parameterization with regards to network depth of CNN model. The EBCLE heuristic employs a priori knowledge of the entropic data distribution of input datasets to determine an upper bound for convolutional network depth, beyond which identity transformations are prevalent offering insignificant contributions for enhancing model performance. Restricting depth redundancies by forcing feature compression and abstraction restricts over-parameterization while decreasing training time by 24.99\% - 78.59\% without degradation in model performance. We present empirical evidence to emphasize the relative effectiveness of broader, yet shallower models trained using the EBCLE heuristic, which maintains or outperforms baseline classification accuracies of narrower yet deeper models. The EBCLE heuristic is architecturally agnostic and EBCLE based CNN models restrict depth redundancies resulting in enhanced utilization of the available computational resources. The proposed EBCLE heuristic is a compelling technique for researchers to analytically justify their HyperParameter (HP) choices for CNNs. Empirical validation of the EBCLE heuristic in training CNN models was established on five benchmarking datasets (ImageNet32, CIFAR-10/100, STL-10, MNIST) and four network architectures (DenseNet, ResNet , ResNeXt and EfficientNet B0-B2) with appropriate statistical tests employed to infer any conclusive claims presented in this paper.},
  archive      = {J_PR},
  author       = {Nidhi Gowdra and Roopak Sinha and Stephen MacDonell and Wei Qi Yan},
  doi          = {10.1016/j.patcog.2021.108057},
  journal      = {Pattern Recognition},
  pages        = {108057},
  shortjournal = {Pattern Recognition},
  title        = {Mitigating severe over-parameterization in deep convolutional neural networks through forced feature abstraction and compression with an entropy-based heuristic},
  volume       = {119},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep neural network compression through
interpretability-based filter pruning. <em>PR</em>, <em>119</em>,
108056. (<a href="https://doi.org/10.1016/j.patcog.2021.108056">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a method to compress deep neural networks (DNNs) based on interpretability . For a trained DNN model, the activation maximization technique is first used to visualize every filter of the DNN model. Then, a single-layer filter pruning approach is introduced from what is learned by visualization. The entire DNN model is compressed layer by layer by using the single-layer filter pruning method in which the compression of the current layer is based on the compression of the preceding layers. Importantly, in addition to effective compression, the proposed method renders a better interpretation of the deep learning process. With a 60\% 60\% compression rate of the VGG-16, our method achieves 0.8429 Top-1 accuracy under CIFAR-10, with a slight accuracy drop of only 0.0322, and the storage space of the model can be compressed to 9.42 Mb. For a modern DNN model such as ResNet50 , our visualization-based filter pruning method is significantly better than other pruning strategies in different convolutional layers under different compression rates and the larger ImageNet dataset. After pruning, the computation cost and storage requirement of the DNN can be significantly reduced, which means that complex DNN models can be easily implemented in small mobile devices , thus enabling the efficient use of DNNs in the Internet of Things technologies.},
  archive      = {J_PR},
  author       = {Kaixuan Yao and Feilong Cao and Yee Leung and Jiye Liang},
  doi          = {10.1016/j.patcog.2021.108056},
  journal      = {Pattern Recognition},
  pages        = {108056},
  shortjournal = {Pattern Recognition},
  title        = {Deep neural network compression through interpretability-based filter pruning},
  volume       = {119},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). COVID-19 detection from x-ray images using multi-kernel-size
spatial-channel attention network. <em>PR</em>, <em>119</em>, 108055.
(<a href="https://doi.org/10.1016/j.patcog.2021.108055">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Novel coronavirus 2019 (COVID-19) has spread rapidly around the world and is threatening the health and lives of people worldwide. Early detection of COVID-19 positive patients and timely isolation of the patients are essential to prevent its spread. Chest X-ray images of COVID-19 patients often show the characteristics of multifocality, bilateral hairy glass turbidity, patchy network turbidity, etc. It is crucial to design a method to automatically identify COVID-19 from chest X-ray images to help diagnosis and prognosis. Existing studies for the classification of COVID-19 rarely consider the role of attention mechanisms on the classification of chest X-ray images and fail to capture the cross-channel and cross-spatial interrelationships in multiple scopes. This paper proposes a multi-kernel-size spatial-channel attention method to detect COVID-19 from chest X-ray images. Our proposed method consists of three stages. The first stage is feature extraction. The second stage contains two parallel multi-kernel-size attention modules: multi-kernel-size spatial attention and multi-kernel-size channel attention. The two modules capture the cross-channel and cross-spatial interrelationships in multiple scopes using multiple 1D and 2D convolutional kernels of different sizes to obtain channel and spatial attention feature maps. The third stage is the classification module. We integrate the chest X-ray images from three public datasets: COVID-19 Chest X-ray Dataset Initiative, ActualMed COVID-19 Chest X-ray Dataset Initiative, and COVID-19 radiography database for evaluation. Experimental results demonstrate that the proposed method improves the performance of COVID-19 detection and achieves an accuracy of 98.2\%.},
  archive      = {J_PR},
  author       = {Yuqi Fan and Jiahao Liu and Ruixuan Yao and Xiaohui Yuan},
  doi          = {10.1016/j.patcog.2021.108055},
  journal      = {Pattern Recognition},
  pages        = {108055},
  shortjournal = {Pattern Recognition},
  title        = {COVID-19 detection from X-ray images using multi-kernel-size spatial-channel attention network},
  volume       = {119},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Stroke constrained attention network for online handwritten
mathematical expression recognition. <em>PR</em>, <em>119</em>, 108047.
(<a href="https://doi.org/10.1016/j.patcog.2021.108047">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel stroke constrained attention network (SCAN) which treats stroke as the basic unit for encoder-decoder based online handwritten mathematical expression recognition (HMER). Unlike previous methods which use trace points or image pixels as basic units, SCAN makes full use of stroke-level information for better alignment and representation. The proposed SCAN can be adopted in both single-modal (online or offline) and multi-modal HMER. For single-modal HMER, SCAN first employs a CNN-GRU encoder to extract point-level features from input traces in online mode and employs a CNN encoder to extract pixel-level features from input images in offline mode, then use stroke constrained information to convert them into online and offline stroke-level features. Using stroke-level features can explicitly group points or pixels belonging to the same stroke, therefore reduces the difficulty of symbol segmentation and recognition via the decoder with attention mechanism. For multi-modal HMER, other than fusing multi-modal information in decoder, SCAN can also fuse multi-modal information in encoder by utilizing the stroke based alignments between online and offline modalities. The encoder fusion is a better way for combining multi-modal information as it implements the information interaction one step before the decoder fusion so that the advantages of multiple modalities can be exploited earlier and more adequately. Besides, we propose an approach combining the encoder fusion and decoder fusion, namely encoder-decoder fusion, which can further improve the performance. Evaluated on a benchmark published by CROHME competition, the proposed SCAN achieves the state-of-the-art performance. Furthermore, by conducting experiments on an additional task: online handwritten Chinese character recognition (HCCR), we demonstrate the generality of our proposed method.},
  archive      = {J_PR},
  author       = {Jiaming Wang and Jun Du and Jianshu Zhang and Bin Wang and Bo Ren},
  doi          = {10.1016/j.patcog.2021.108047},
  journal      = {Pattern Recognition},
  pages        = {108047},
  shortjournal = {Pattern Recognition},
  title        = {Stroke constrained attention network for online handwritten mathematical expression recognition},
  volume       = {119},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Non-linear and selective fusion of cross-modal images.
<em>PR</em>, <em>119</em>, 108042. (<a
href="https://doi.org/10.1016/j.patcog.2021.108042">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing image fusion methods pay little research attention to human visual characteristics. However, human visual characteristics play an important role in visual processing tasks. To solve this problem, we propose a cross-modal image fusion method that combines illuminance factors and attention mechanisms . Human visual characteristics are studied and simulated in cross-modal image fusion task. Firstly, in order to reject high and low-frequency mixing and reduce the halo effect , we perform cross-modal image multi-scale decomposition. Secondly, in order to remove highlights, the visual saliency map and the deep feature map are combined with the illuminance fusion factor to perform high-low frequency non-linear fusion. Thirdly, the feature maps are selected through a channel attention network to obtain the final fusion map. Finally, we validate our image fusion method on public datasets of infrared and visible images. The experimental results demonstrate the superiority of our fusion method under the complex illumination environment. In addition, the experimental results also demonstrate the effectiveness of our simulation of human visual characteristics.},
  archive      = {J_PR},
  author       = {Aiqing Fang and Xinbo Zhao and Jiaqi Yang and Yanning Zhang and Xiang Zheng},
  doi          = {10.1016/j.patcog.2021.108042},
  journal      = {Pattern Recognition},
  pages        = {108042},
  shortjournal = {Pattern Recognition},
  title        = {Non-linear and selective fusion of cross-modal images},
  volume       = {119},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Understanding crowd flow patterns using active-langevin
model. <em>PR</em>, <em>119</em>, 108037. (<a
href="https://doi.org/10.1016/j.patcog.2021.108037">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crowd flow describes the elementary group behavior. Dynamics behind group behavior can help to identify abnormalities in flows. Quantifying flow dynamics can be challenging. In this paper, an algorithm has been proposed to describe groups’ movements in crowded scenarios by analyzing videos. A force model has been proposed based on the active Langevin equation , where the motion points are assumed to behave similarly to active colloidal particles in fluids. The force model is further augmented with computer-vision techniques to segment linear and non-linear flows. The evaluation of the proposed spatio-temporal flow segmentation scheme has been carried out with public datasets. Experiments reveal that the proposed system can segment the flows with lesser errors than existing methods. The segmentation accuracy and Normalized Mutual Information (NMI) have improved by 10\% 10\% as compared to existing flow segmentation algorithms .},
  archive      = {J_PR},
  author       = {Shreetam Behera and Debi Prosad Dogra and Malay Kumar Bandyopadhyay and Partha Pratim Roy},
  doi          = {10.1016/j.patcog.2021.108037},
  journal      = {Pattern Recognition},
  pages        = {108037},
  shortjournal = {Pattern Recognition},
  title        = {Understanding crowd flow patterns using active-langevin model},
  volume       = {119},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Feature learning and patch matching for diverse image
inpainting. <em>PR</em>, <em>119</em>, 108036. (<a
href="https://doi.org/10.1016/j.patcog.2021.108036">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present an image inpainting approach to generate diverse high-quality inpainting results. Recent advances in deep adversarial networks have led to significant improvements in the challenging task of filling large holes in natural images. Although deep generative models can generate visually plausible structures and textures, most of them are not interpretable, making it difficult to control the inpainting output. In addition, deep generative models do not have capacity to produce diverse results for each input. To address such limitations, we design a novel free-form image inpainting framework with two sequential steps: the first step formulates the inpainting process as a regression problem and utilizes a U-Net-like convolutional neural network to map an input to a coarse inpainting output, and the second step utilizes the nearest neighbor based pixel-wise matching to map the coarse output to diverse high-quality outputs. The second step allows our approach to compose novel high-quality content by copy-pasting high-frequency missing information from different training exemplars. Experiments on multiple datasets, i.e., CelebA-HQ, AFHQ, and Paris StreetView, show that our approach is able to offer multiple natural outputs with higher diversity in a controllable manner.},
  archive      = {J_PR},
  author       = {Yuan Zeng and Yi Gong and Jin Zhang},
  doi          = {10.1016/j.patcog.2021.108036},
  journal      = {Pattern Recognition},
  pages        = {108036},
  shortjournal = {Pattern Recognition},
  title        = {Feature learning and patch matching for diverse image inpainting},
  volume       = {119},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). VSRNet: End-to-end video segment retrieval with text query.
<em>PR</em>, <em>119</em>, 108027. (<a
href="https://doi.org/10.1016/j.patcog.2021.108027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Users are sometimes interested in specific segments of an untrimmed video when using the video search engine. Targeting at this demand, we explore a novel research topic of text query based video segment retrieval (VSR). Different from the conventional video retrieval task or localizing text descriptions in a single video, it requires the retrieval of the most relevant video from a large collection as well as localizing the start and end timestamps of a segment that matches the text query best from the video. A direct solution is to perform video-level matching first, and then apply description localization among such video candidates. Such two-stage based methods are not able to utilize complementary information of each stage, and are time-consuming in inference. In this paper, We propose VSRNet, an end-to-end framework that efficiently retrieves video at segment granularity with two branches. In the first branch, individual videos and texts are mapped to a common space for stand-alone ranking. In the second branch, we propose a supervised text-aligned attention mechanism and calculate the response of every frame to the text query, from which the frames with high scores are aggregated as segment proposals. Extensive experiments conducted on ActivityNet Captions and DiDeMo verify the effectiveness of our method and show that our solution significantly outperforms the state of the art.},
  archive      = {J_PR},
  author       = {Xiao Sun and Xiang Long and Dongliang He and Shilei Wen and Zhouhui Lian},
  doi          = {10.1016/j.patcog.2021.108027},
  journal      = {Pattern Recognition},
  pages        = {108027},
  shortjournal = {Pattern Recognition},
  title        = {VSRNet: End-to-end video segment retrieval with text query},
  volume       = {119},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Reinforced SVM method and memorization mechanisms.
<em>PR</em>, <em>119</em>, 108018. (<a
href="https://doi.org/10.1016/j.patcog.2021.108018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper is devoted to two problems: (1) reinforcement of SVM algorithms, and (2) justification of memorization mechanisms for generalization. (1) Current SVM algorithm was designed for the case when the risk for the set of nonnegative slack variables is defined by l 1 l1 norm. In this paper, along with that classical l 1 l1 norm, we consider risks defined by l 2 l2 norm and l ∞ l∞ norm. Using these norms, we formulate several modifications of the existing SVM algorithm and show that the resulting modified SVM algorithms can improve (sometimes significantly) the classification performance. (2) Generalization ability of existing learning algorithms is usually explained by arguments involving uniform convergence of empirical losses to the corresponding expected losses over a given set of functions. However, along with bounds for uniform convergence of empirical losses to the expected losses, the VC theory also provides bounds for relative uniform convergence. These bounds lead to a more accurate estimate of the expected loss. Advanced methods of estimating of expected risk of error have to leverage these bounds, which also support mechanisms of training data memorization, which, as the paper demonstrates, can improve classification performance.},
  archive      = {J_PR},
  author       = {Vladimir Vapnik and Rauf Izmailov},
  doi          = {10.1016/j.patcog.2021.108018},
  journal      = {Pattern Recognition},
  pages        = {108018},
  shortjournal = {Pattern Recognition},
  title        = {Reinforced SVM method and memorization mechanisms},
  volume       = {119},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Monocular multi-person pose estimation: A survey.
<em>PR</em>, <em>118</em>, 108046. (<a
href="https://doi.org/10.1016/j.patcog.2021.108046">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-person pose estimation in unconstrained scenarios, with an unknown number of individuals, is a main step towards scene understanding and action recognition. Due to the recent advancements on the architecture of convolutional networks , body part detectors are now accurate and estimate poses in real-time for both single- and multi-person scenes. In contrast, assigning detected body parts to coherent human poses when there are multiple persons interacting is an arduous task. To name a few of the challenges that arise in such scenes: person-to-person occlusion, truncated body parts, and more sources for double counting. Recently, the community contributed towards solving most of them. Hence, it would be interesting to analyze and compile successful approaches from current literature into research trends, and identify possible gaps for future works. To the best of our knowledge, there is no up-to-date review on the main advancements in the field that target this particular set of challenges. This survey fills this gap by reviewing the main breakthroughs on multi-person pose estimation over the last decade and summarizing their impact on the state-of-the-art. Regarding scientific contributions, we propose a novel taxonomy that categorizes the reviewed methods according to their main contributions to the pose estimation pipeline, lists the main datasets and evaluation metrics to train new models, and provides insights on the best entries of publicly available benchmarks.},
  archive      = {J_PR},
  author       = {Eduardo Souza dos Reis and Lucas Adams Seewald and Rodolfo Stoffel Antunes and Vinicius Facco Rodrigues and Rodrigo da Rosa Righi and Cristiano André da Costa and Luiz Gonzaga da Silveira Jr. and Bjoern Eskofier and Andreas Maier and Tim Horz and Rebecca Fahrig},
  doi          = {10.1016/j.patcog.2021.108046},
  journal      = {Pattern Recognition},
  pages        = {108046},
  shortjournal = {Pattern Recognition},
  title        = {Monocular multi-person pose estimation: A survey},
  volume       = {118},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Polarization image fusion with self-learned fusion strategy.
<em>PR</em>, <em>118</em>, 108045. (<a
href="https://doi.org/10.1016/j.patcog.2021.108045">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Polarization image fusion aims to integrate intensity and degree of linear polarization images into one with more details, which is beneficial to improve the ability of targets detection under complex background. The fusion strategies in conventional methods are designed in a hand-crafted way and not robust to different fusion tasks. In this paper, we propose a novel and deep network to address the polarization image fusion issue with self-learned strategy. The network consists of Encoder, Fusion, and Decoder layers. Feature maps extracted by Encoder are fused, then fed into Decoder to generate fused images. Besides, a novel loss function is adopted to train the network in an unsupervised way, without ground truth of fused images. To verify the advantage, the network trained on polarization images is also used to infrared and visible images fusion, and multi-focus image fusion. Experimental results showed that our method outperforms several state-of-the-art methods in terms of visual quality and quantitative measurement . The proposed fused method can be applied in the military and civilian fields such as camouflage and hidden targets detection, medical diagnosis, and environmental monitoring.},
  archive      = {J_PR},
  author       = {Junchao Zhang and Jianbo Shao and Jianlai Chen and Degui Yang and Buge Liang},
  doi          = {10.1016/j.patcog.2021.108045},
  journal      = {Pattern Recognition},
  pages        = {108045},
  shortjournal = {Pattern Recognition},
  title        = {Polarization image fusion with self-learned fusion strategy},
  volume       = {118},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Normalized edge convolutional networks for skeleton-based
hand gesture recognition. <em>PR</em>, <em>118</em>, 108044. (<a
href="https://doi.org/10.1016/j.patcog.2021.108044">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic hand skeletons consisting of discrete spatial-temporal finger joint clouds effectively convey the intentions of communicators. Previous graph convolutional networks (GCNs) relying on human hand-crafted inductive biases have been quickly promoted for skeleton-based hand gesture recognition (SHGR). However, most existing graph constructions for GCN-based solutions are set manually, only considering the physical topology of the hand skeleton, and the fixed dependencies among hand joints may lead to suboptimal models. To enrich the local dependencies, we emphasize that hand skeletons can be seen from two views: explicit joint clouds and implicit skeleton topology. Starting from those two views of hand gestures, we attempt to introduce dynamics and diversities into the local neighborhood of the graph by dividing it into sets of physical neighbors, temporal neighbors and varying neighbors. Next, we systematically proceed with three innovations, including the novel edge-varying graph, normalized edge convolution operation , and zig-zag sampling strategy, to alleviate the challenges resulting from engineering practices. Finally, spatial-based GCNs called normalized edge convolutional networks are constructed for hand gesture recognition. Experiments on publicly available hand datasets show that our work is stable for performing state-of-the-art gesture recognition, and ablation experiments are also provided to validate each contribution.},
  archive      = {J_PR},
  author       = {Fangtai Guo and Zaixing He and Shuyou Zhang and Xinyue Zhao and Jinhui Fang and Jianrong Tan},
  doi          = {10.1016/j.patcog.2021.108044},
  journal      = {Pattern Recognition},
  pages        = {108044},
  shortjournal = {Pattern Recognition},
  title        = {Normalized edge convolutional networks for skeleton-based hand gesture recognition},
  volume       = {118},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Arbitrary-view human action recognition via novel-view
action generation. <em>PR</em>, <em>118</em>, 108043. (<a
href="https://doi.org/10.1016/j.patcog.2021.108043">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Arbitrary-view human action recognition is still a big challenge due to the view changes. A possible solution is to enlarge the view range of action samples in the training set. Therefore, we propose a Two-Branch Novel-View action Generation approach based on auxiliary conditional GAN, which generates a novel-view action sample for arbitrary-view human action recognition . The generated sample enlarge the view range of action samples for training. Furthermore, to narrow the representation of actions in different views, we propose a view-domain generalization model that improves the recognition performance of arbitrary-view human action recognition. Our approach is evaluated on three large-scale RGB+D skeleton datasets including UESTC varying-view RGB+D dataset, NTU RGB+D 60, and NTU RGB+D 120 datasets, with two types of view-invariant evaluations, i.e., the cross-view, and arbitrary-view recognition. The proposed approach achieves outstanding performance in human action recognition.},
  archive      = {J_PR},
  author       = {Kumie Gedamu and Yanli Ji and Yang Yang and LingLing Gao and Heng Tao Shen},
  doi          = {10.1016/j.patcog.2021.108043},
  journal      = {Pattern Recognition},
  pages        = {108043},
  shortjournal = {Pattern Recognition},
  title        = {Arbitrary-view human action recognition via novel-view action generation},
  volume       = {118},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep self-representative subspace clustering network.
<em>PR</em>, <em>118</em>, 108041. (<a
href="https://doi.org/10.1016/j.patcog.2021.108041">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning based subspace clustering networks have been a significant technique for motion segmentation , unsupervised image segmentation , image representation and compression, and face clustering by separating the high-dimensional data points into their representative low-dimensional linear subspaces. Effective feature selection is critical to remove redundant samples and select the representative feature subset from high-dimensional data space; hence deriving the number of subspaces, their dimensions, data segmentation, and a basis for each subspace. The effective self-representative feature selection and emphasis by scaling the feature map in the learned embedded space is required for deep learning based subspace clustering to reduce the number of parameters and dimension of the self-representative layer. In this paper, we propose a self-representative feature extraction deep neural network for unsupervised subspace clustering to improve representativeness and clustering ability. The extensive relevant results on various data demonstrate that deep subspace clustering employing self-representative features from high-dimensional data can effectively reduce the dimension of the self-representative layer while improving performance.},
  archive      = {J_PR},
  author       = {Sangwon Baek and Gangjoon Yoon and Jinjoo Song and Sang Min Yoon},
  doi          = {10.1016/j.patcog.2021.108041},
  journal      = {Pattern Recognition},
  pages        = {108041},
  shortjournal = {Pattern Recognition},
  title        = {Deep self-representative subspace clustering network},
  volume       = {118},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Network edge entropy decomposition with spin statistics.
<em>PR</em>, <em>118</em>, 108040. (<a
href="https://doi.org/10.1016/j.patcog.2021.108040">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a previous study, we have explored how to decompose the global entropy of a network into edge components using a graph-spectral decomposition technique. Here, we develop this work in more depth to understand the role of edge entropy as an efficient and effective tool in analysing network structure. We use the edge entropy distribution as a network feature or characterisation and combine it with linear discriminant analysis to distinguish different types of network model and structure. Interpreting the normalised Laplacian matrix as the network Hamiltonian (or energy) operator, the network is assumed to be in thermodynamic equilibrium with a heat bath where the energy states correspond to the normalised Laplacian eigenvalues. To model the way in which particles occupy the energy states, we explore the use of three different spin-dependent statistical models to determine the thermodynamic entropy of the network. These are a) the classical spinless Maxwell-Boltzmann distribution, and two models based on quantum mechanical spin-statistics, namely b) the Bose-Einstein model for particles with integer spin, and c) the Fermi-Dirac model for particles with half-integer spin. By using the spectral decomposition of the Laplacian, we illustrate how to project out the edge-entropy components from the global network entropy. In this way, the detailed distribution of entropy across the edges of a network can be constructed. Compared to our previous study of the von Neumann edge entropy, where the edge entropy just depends on the degrees of the nodes forming an edge, in the case of the new statistical mechanical model, there is a more subtle dependence of the edge entropy on the structure of a network. We illustrate how this new edge entropy distribution can be used to more effectively identify variations in network structure, in particular for edges incorporating nodes of large degree. Numerical experiments on synthetic and real-world data-sets are presented to evaluate the qualitative and quantitative differences in performance.},
  archive      = {J_PR},
  author       = {Jianjia Wang and Richard C. Wilson and Edwin R. Hancock},
  doi          = {10.1016/j.patcog.2021.108040},
  journal      = {Pattern Recognition},
  pages        = {108040},
  shortjournal = {Pattern Recognition},
  title        = {Network edge entropy decomposition with spin statistics},
  volume       = {118},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep graph learning for semi-supervised classification.
<em>PR</em>, <em>118</em>, 108039. (<a
href="https://doi.org/10.1016/j.patcog.2021.108039">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph learning (GL) can dynamically capture the distribution structure (graph structure) of data based on graph convolutional networks (GCN), and the learning quality of the graph structure directly influences GCN for semi-supervised classification. Most existing methods combine the computational layer and the related losses into GCN for exploring the global graph (measuring graph structure from all data samples) or local graph (measuring graph structure from local data samples). The global graph emphasizes the whole structure description of the inter-class data, while the local graph tends to the neighborhood structure representation of the intra-class data. However, it is difficult to simultaneously balance these learning process graphs for semi-supervised classification because of the interdependence of these graphs. To simulate the interdependence, deep graph learning (DGL) is proposed to find a better graph representation for semi-supervised classification. DGL can not only learn the global structure by the previous layer metric computation updating, but also mine the local structure by next layer local weight reassignment. Furthermore, DGL can fuse the different structures by dynamically encoding the interdependence of these structures, and deeply mine the relationship of the different structures by hierarchical progressive learning to improve the performance of semi-supervised classification. Experiments demonstrate that the DGL outperforms state-of-the-art methods on three benchmark datasets (Citeseer, Cora, and Pubmed) for citation networks and two benchmark datasets (MNIST and Cifar10) for images.},
  archive      = {J_PR},
  author       = {Guangfeng Lin and Xiaobing Kang and Kaiyang Liao and Fan Zhao and Yajun Chen},
  doi          = {10.1016/j.patcog.2021.108039},
  journal      = {Pattern Recognition},
  pages        = {108039},
  shortjournal = {Pattern Recognition},
  title        = {Deep graph learning for semi-supervised classification},
  volume       = {118},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Unsupervised domain-adaptive scene-specific pedestrian
detection for static video surveillance. <em>PR</em>, <em>118</em>,
108038. (<a href="https://doi.org/10.1016/j.patcog.2021.108038">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Objects from one category may be drawn from different distributions due to diverse illuminations, backgrounds, and camera viewpoints. Traditional object detection methods generally perform poorly due to the domain shift. To address this problem, we propose to train a domain-adaptive scene-specific pedestrian detector in an unsupervised manner . A generic detector is transferred to different target domains from one labeled source domain dataset without human-annotated target samples. Specifically, we first extend the generic detector to a dual-boundary classifier and collect hard samples as unlabeled target samples according to the detection confidence. Then, we propose a cycle semantic transfer network to align the instance-level and class-level distributions between the source domain and target domain and automatically label the hard samples. The initial generic detector is then re-trained by these labeled hard samples and specialized to a target scene. This process can be conveniently extended to different surveillance scenarios and generate specific detectors under various static camera viewpoints. Moreover, to reduce the impact of mislabeled hard samples on the generic detector, an online gradual optimization algorithm is proposed to iteratively update the generic model, thereby obtaining an optimized process that is insensitive to individual mislabeled target samples. Extensive experiments show that even if the target domain is not manually annotated, the proposed self-learning method demonstrates the effectiveness of pedestrian detection in various domain shift scenarios, and it outperforms existing scene-specific pedestrian detection methods and some classic supervised methods.},
  archive      = {J_PR},
  author       = {Quanzheng Mou and Longsheng Wei and Conghao Wang and Dapeng Luo and Songze He and Jing Zhang and Huimin Xu and Chen Luo and Changxin Gao},
  doi          = {10.1016/j.patcog.2021.108038},
  journal      = {Pattern Recognition},
  pages        = {108038},
  shortjournal = {Pattern Recognition},
  title        = {Unsupervised domain-adaptive scene-specific pedestrian detection for static video surveillance},
  volume       = {118},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Checklist for responsible deep learning modeling of medical
images based on COVID-19 detection studies. <em>PR</em>, <em>118</em>,
108035. (<a href="https://doi.org/10.1016/j.patcog.2021.108035">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The sudden outbreak and uncontrolled spread of COVID-19 disease is one of the most important global problems today. In a short period of time, it has led to the development of many deep neural network models for COVID-19 detection with modules for explainability. In this work, we carry out a systematic analysis of various aspects of proposed models. Our analysis revealed numerous mistakes made at different stages of data acquisition, model development, and explanation construction. In this work, we overview the approaches proposed in the surveyed Machine Learning articles and indicate typical errors emerging from the lack of deep understanding of the radiography domain. We present the perspective of both: experts in the field - radiologists and deep learning engineers dealing with model explanations. The final result is a proposed checklist with the minimum conditions to be met by a reliable COVID-19 diagnostic model.},
  archive      = {J_PR},
  author       = {Weronika Hryniewska and Przemysław Bombiński and Patryk Szatkowski and Paulina Tomaszewska and Artur Przelaskowski and Przemysław Biecek},
  doi          = {10.1016/j.patcog.2021.108035},
  journal      = {Pattern Recognition},
  pages        = {108035},
  shortjournal = {Pattern Recognition},
  title        = {Checklist for responsible deep learning modeling of medical images based on COVID-19 detection studies},
  volume       = {118},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Bayesian damage recognition in document images based on a
joint global and local homogeneity model. <em>PR</em>, <em>118</em>,
108034. (<a href="https://doi.org/10.1016/j.patcog.2021.108034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Physical damages (such as torn-offs and scratches) are commonly seen in historical documents. Recognition of such damages is currently absent in digitization-and-information-extraction (DIE) systems but crucial for automatic document comprehension and exploitation. In this paper we propose a generic damage recognition (DR) method based on a joint global and local modeling of the text homogeneity (TH) pattern exhibited in document images. More specifically, a connected component (CC) based formulation is developed as a global homogeneity measure, where TH is characterized using a probabilistic graph model for a coarse recognition of damaged regions. A multi-resolution analysis (MRA) of TH is further developed for a granular within-CC recognition of damage pixels, where the disparity between damage and text pixels is characterized by exploiting neighborhood transitions. This enables the formulation of a local homogeneity measure, where the neighborhood transition around an individual pixel is modeled using the propagation of the approximation coefficients of a stationary wavelet transform (SWT). The proposed global and local homogeneity measures are integrated as a joint likelihood in a Bayesian model with a Markov random field (MRF) prior, where DR is formulated as a maximum a posterior (MAP) inference which is addressed using Markov Chain Monte Carlo (MCMC) sampling. The resulting algorithm is tested on a set of real-life historical newspaper images containing damages of varying size and shape. The performance of the algorithm is evaluated using both F-measures and the Intersection-over-Union (IoU) metric, where test results demonstrate the promising potential of the proposed method.},
  archive      = {J_PR},
  author       = {Tan Lu and Ann Dooms},
  doi          = {10.1016/j.patcog.2021.108034},
  journal      = {Pattern Recognition},
  pages        = {108034},
  shortjournal = {Pattern Recognition},
  title        = {Bayesian damage recognition in document images based on a joint global and local homogeneity model},
  volume       = {118},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). PCLoss: Fashion landmark estimation with position constraint
loss. <em>PR</em>, <em>118</em>, 108028. (<a
href="https://doi.org/10.1016/j.patcog.2021.108028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fashion landmark estimation aims at locating functional key points of clothes, which has wide potential applications in electronic commerce . However, due to the occlusion and weak outline information, landmark estimation occurs outliers and duplicate detection problems. To alleviate these issues, we propose Position Constraint Loss (PCLoss) to constrain error landmark locations by utilizing the position relationship of landmarks. Specifically, PCLoss adds a regularization term for each landmark to regularize their relative positions, and it can be easily applied to both regression and heatmap based methods without extra computation during inference. Unlike existing approaches that propagate landmark information between feature layers by specific network structures, PCLoss introduces position relations of landmarks in the label space without modifying the network structure. In addition, we leverage the skeleton-like relation of clothing to further strengthen position constraints between landmarks. Extensive experimental results on DeepFashion, FLD and FashionAI demonstrate that our methods can effectively increase the performance of mainstream frameworks by a large margin. We also explore the effectiveness of PCLoss on human pose estimation task, and the experimental results on COCO 2017 prove the generality of our methods on other key point estimation tasks.},
  archive      = {J_PR},
  author       = {Meijia Song and Hong Liu and Wei Shi and Xia Li},
  doi          = {10.1016/j.patcog.2021.108028},
  journal      = {Pattern Recognition},
  pages        = {108028},
  shortjournal = {Pattern Recognition},
  title        = {PCLoss: Fashion landmark estimation with position constraint loss},
  volume       = {118},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A simple graph-based semi-supervised learning approach for
imbalanced classification. <em>PR</em>, <em>118</em>, 108026. (<a
href="https://doi.org/10.1016/j.patcog.2021.108026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph-based Semi-Supervised Learning (GSSL) methods aim to classify unlabeled data by learning the graph structure and labeled data jointly. In this work, we propose a simple GSSL approach, which can deal with various degrees of class imbalance in given datasets. The key idea is to estimate the class proportion of input data in order to enhance the discriminative power of learned smooth classification function on the graph. Moreover, it has interesting connections to the regularization framework, the Markov stability for graph partition and the group inverse of normalized Laplacain matrix. For classification problems, experimental results demonstrate our approach can achieve promising performance on several datasets with varying class imbalance.},
  archive      = {J_PR},
  author       = {Jianjin Deng and Jin-Gang Yu},
  doi          = {10.1016/j.patcog.2021.108026},
  journal      = {Pattern Recognition},
  pages        = {108026},
  shortjournal = {Pattern Recognition},
  title        = {A simple graph-based semi-supervised learning approach for imbalanced classification},
  volume       = {118},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Improving one-shot NAS with shrinking-and-expanding
supernet. <em>PR</em>, <em>118</em>, 108025. (<a
href="https://doi.org/10.1016/j.patcog.2021.108025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Training a supernet using a copy of shared weights has become a popular approach to speed up neural architecture search (NAS). However, it is difficult for supernet to accurately evaluate on a large-scale search space due to high weight coupling in weight-sharing setting. To address this, we present a shrinking-and-expanding supernet that decouples the shared parameters by reducing the degree of weight sharing, avoiding unstable and inaccurate performance estimation as in previous methods. Specifically, we propose a new shrinking strategy that progressively simplifies the original search space by discarding unpromising operators in a smart way. Based on this, we further present an expanding strategy by appropriately increasing parameters of the shrunk supernet. We provide comprehensive evidences showing that, in weight-sharing supernet, the proposed method SE-NAS brings more accurate and more stable performance estimation. Experimental results on ImageNet dataset indicate that SE-NAS achieves higher Top-1 accuracy than its counterparts under the same complexity constraint and search space. The ablation study is presented to further understand SE-NAS.},
  archive      = {J_PR},
  author       = {Yiming Hu and Xingang Wang and Lujun Li and Qingyi Gu},
  doi          = {10.1016/j.patcog.2021.108025},
  journal      = {Pattern Recognition},
  pages        = {108025},
  shortjournal = {Pattern Recognition},
  title        = {Improving one-shot NAS with shrinking-and-expanding supernet},
  volume       = {118},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Relation-based discriminative cooperation network for
zero-shot classification. <em>PR</em>, <em>118</em>, 108024. (<a
href="https://doi.org/10.1016/j.patcog.2021.108024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zero-shot learning (ZSL) aims to assign the category corresponding to the relevant semantic as the label of the unseen sample based on the relationship between the learned visual and semantic features . However, most typical ZSL models faced with the domain bias problem, which leads to unseen or test samples being easily misclassified into seen or training categories. To handle this problem, we propose a relation-based discriminative cooperation network (RDCN) model for ZSL in this work. The proposed model effectively utilize the robust metric space spanned by the cooperated semantics with the help of a set of relations. On the other hand, we devise the relation network to measure the relationship between the visual features and embedded semantics, and the validation information will guide the embedding module to learn more discriminative information. At last, the proposed RDCN model is validated on six benchmarks, and extensive experiments demonstrate the superiority of proposed method over most existing ZSL models on the traditional zero-shot setting and the more realistic generalized zero-shot setting.},
  archive      = {J_PR},
  author       = {Yang Liu and Xinbo Gao and Quanxue Gao and Jungong Han and Ling Shao},
  doi          = {10.1016/j.patcog.2021.108024},
  journal      = {Pattern Recognition},
  pages        = {108024},
  shortjournal = {Pattern Recognition},
  title        = {Relation-based discriminative cooperation network for zero-shot classification},
  volume       = {118},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). EFNet: Enhancement-fusion network for semantic
segmentation. <em>PR</em>, <em>118</em>, 108023. (<a
href="https://doi.org/10.1016/j.patcog.2021.108023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic segmentation is a challenging and important task in computer vision . Convolutional neural networks (CNNs) have demonstrated their outstanding performances on such dense classification tasks . Most recent segmentation networks mainly focus on feature extraction for one single input image, while paying little attention to facilitating the segmentation by image manipulation or enhancement. In this paper, we design an enhancement-fusion network (EFNet), which aims at enhancing an input image for more diversified features to boost the following task of pixel-wise labeling. Specifically, the enhancement modules are trained to produce multiple enhanced images. Afterwards, the fusion module selectively attends on such images and fuses them to yield one new image. The proposed EFNet can be directly and flexibly integrated as an auxiliary network with state-of-the-art semantic segmentation networks , while maintaining the end-to-end training manner. Extensive results on benchmark datasets corroborate that the combination of the EFNet and the CNN-based semantic segmentation networks significantly improves the segmentation performance compared with the original segmentation networks.},
  archive      = {J_PR},
  author       = {Zhijie Wang and Ran Song and Peng Duan and Xiaolei Li},
  doi          = {10.1016/j.patcog.2021.108023},
  journal      = {Pattern Recognition},
  pages        = {108023},
  shortjournal = {Pattern Recognition},
  title        = {EFNet: Enhancement-fusion network for semantic segmentation},
  volume       = {118},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Pan-sharpening based on multi-objective decision for
multi-band remote sensing images. <em>PR</em>, <em>118</em>, 108022. (<a
href="https://doi.org/10.1016/j.patcog.2021.108022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pan-sharpening applies details injection to fuse a multispectral (MS) image with its corresponding panchromatic (PAN) image to produce a synthetic image. Theoretically, the synthetic image&#39;s spectral resolution should equal that of the MS image and its spatial resolution is the same as that of the PAN image. However, for existing pan-sharpening methods, the trade-off between the spectral and intensity information in the process of details injection is insufficient, resulting in spatial or spectral distortion of the fused image. In this paper we propose a novel pan-sharpening algorithm based on multi-objective decision for multi-band remote sensing images to improve the quality of the fused image. The proposed method focuses on developing a parametric model from a multi-objective perspective to simultaneously maximize the quality of all the pixels in the fused image. We introduce a details injection approach to enhance the edge and texture of the MS image. We design an efficient spectral fidelity fusion model based on the injected details using spectral modulation to pan-sharpen the MS image. We provide an algorithm based on multi-objective decision to solve this model. The main advantage of the proposed method is that it can provide effective spectral modulation to eliminate the adverse effects of details injection. We conduct experiments on simulated and real satellite image datasets to evaluate the proposed method. The results show that our method achieves superior performance to other state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Lei Wu and Yunqiang Yin and Xunyan Jiang and T.C.E. Cheng},
  doi          = {10.1016/j.patcog.2021.108022},
  journal      = {Pattern Recognition},
  pages        = {108022},
  shortjournal = {Pattern Recognition},
  title        = {Pan-sharpening based on multi-objective decision for multi-band remote sensing images},
  volume       = {118},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Visual attention dehazing network with multi-level features
refinement and fusion. <em>PR</em>, <em>118</em>, 108021. (<a
href="https://doi.org/10.1016/j.patcog.2021.108021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image dehazing is very important for many computer vision tasks. However, typical CNN-based methods learn a direct mapping from a hazy image to a clear image, ignoring relevant haze priors and multi-level features. In this paper, a new Visual Attention Dehazing Network (VADN) with multi-level refinement and fusion is proposed, which leverages a haze attention map as a haze relevant prior and learns complementary haze information among multi-level features. The VADN contains a feature extraction network , a recurrent refinement network and an encoder-decoder network. The feature extraction network captures the multi-level features. The recurrent refinement network generates and refines the haze attention map by taking low-level features and high-level features as inputs alternatively. Then, the haze attention map is injected into the encoder-decoder network to obtain the clear image with the help of complementary information learned from informative multi-level features. The experimental results demonstrate that the average PSNR of VADN is 32.50 dB which outperforms most state-of-the-art methods by up to 5.14 dB. Besides, the run time of VADN is 0.067 s, only 55\% 55\% of the run time spent by the recent enhanced pix2pix dehazing network.},
  archive      = {J_PR},
  author       = {Shibai Yin and Xiaolong Yang and Yibin Wang and Yee-Hong Yang},
  doi          = {10.1016/j.patcog.2021.108021},
  journal      = {Pattern Recognition},
  pages        = {108021},
  shortjournal = {Pattern Recognition},
  title        = {Visual attention dehazing network with multi-level features refinement and fusion},
  volume       = {118},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021c). Channel attention in LiDAR-camera fusion for lane line
segmentation. <em>PR</em>, <em>118</em>, 108020. (<a
href="https://doi.org/10.1016/j.patcog.2021.108020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To assess the contributions of the different feature channels of sensors, we introduce a novel multimodal fusion method and demonstrate its practical utility using LiDAR-camera fusion networks. Specifically, a channel attention module that can be easily added to a fusion segmentation network is proposed. In this module, we use the channel attention mechanism to obtain the cross-channel local interaction information, and the weights of feature channels are assigned to represent the contributions of different feature channels. To verify the effectiveness of the proposed method, we conduct experiments on two types of feature fusion with the KITTI benchmark and A2D2 dataset. Our model achieves precise edge segmentation, with a 5.59\% gain in precision and a 2.12\% gain in F2-score compared to the values of the original fusion method. We believe that we have introduced a new optimization idea for multimodal fusion.},
  archive      = {J_PR},
  author       = {Xinyu Zhang and Zhiwei Li and Xin Gao and Dafeng Jin and Jun Li},
  doi          = {10.1016/j.patcog.2021.108020},
  journal      = {Pattern Recognition},
  pages        = {108020},
  shortjournal = {Pattern Recognition},
  title        = {Channel attention in LiDAR-camera fusion for lane line segmentation},
  volume       = {118},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Edge-guided composition network for image stitching.
<em>PR</em>, <em>118</em>, 108019. (<a
href="https://doi.org/10.1016/j.patcog.2021.108019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Panorama creation is still challenging in consumer-level photography because of varying conditions of image capturing. A long-standing problem is the presence of artifacts caused by structure inconsistent image transitions. Since it is difficult to achieve perfect alignment in unconstrained shooting environment especially with parallax and object movements, image composition becomes a crucial step to produce artifact-free stitching results. Current energy-based seam-cutting image composition approaches are limited by the hand-crafted features, which are not discriminative and adaptive enough to robustly create structure consistent image transitions. In this paper, we present the first end-to-end deep learning framework named Edge Guided Composition Network (EGCNet) for the composition stage in image stitching. We cast the whole composition stage as an image blending problem, and aims to regress the blending weights to seamlessly produce the stitched image. To better preserve the structure consistency, we exploit perceptual edges to guide the network with additional geometric prior. Specifically, we introduce a perceptual edge branch to integrate edge features into the model and propose two edge-aware losses for edge guidance. Meanwhile, we gathered a general-purpose dataset for image stitching training and evaluation (namely, RISD). Extensive experiments demonstrate that our EGCNet produces plausible results with less running time, and outperforms traditional methods especially under the circumstances of parallax and object motions.},
  archive      = {J_PR},
  author       = {Qinyan Dai and Faming Fang and Juncheng Li and Guixu Zhang and Aimin Zhou},
  doi          = {10.1016/j.patcog.2021.108019},
  journal      = {Pattern Recognition},
  pages        = {108019},
  shortjournal = {Pattern Recognition},
  title        = {Edge-guided composition network for image stitching},
  volume       = {118},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). CT-net: Cascade t-shape deep fusion networks for document
binarization. <em>PR</em>, <em>118</em>, 108010. (<a
href="https://doi.org/10.1016/j.patcog.2021.108010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Document binarization is a key step in most document analysis tasks. However, historical-document images usually suffer from various degradations, making this a very challenging processing stage. The performance of document image binarization has improved dramatically in recent years by the use of Convolutional Neural Networks (CNNs). In this paper, a dual-task, T-shaped neural network is proposed that has the main task of binarization and an auxiliary task of image enhancement. The neural network for enhancement learns the degradations in document images and the specific CNN-kernel features can be adapted towards the binarization task in the training process. In addition, the enhancement image can be considered as an improved version of the input image, which can be fed into the network for fine-tuning, making it possible to design a chained-cascade network (CT-Net). Experimental results on document binarization competition datasets (DIBCO datasets) and MCS dataset show that our proposed method outperforms competing state-of-the-art methods in most cases.},
  archive      = {J_PR},
  author       = {Sheng He and Lambert Schomaker},
  doi          = {10.1016/j.patcog.2021.108010},
  journal      = {Pattern Recognition},
  pages        = {108010},
  shortjournal = {Pattern Recognition},
  title        = {CT-net: Cascade T-shape deep fusion networks for document binarization},
  volume       = {118},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Offline signature verification using a region based deep
metric learning network. <em>PR</em>, <em>118</em>, 108009. (<a
href="https://doi.org/10.1016/j.patcog.2021.108009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Handwritten signature verification is a widely used biometric for person identity authentication in document forensics. Despite the tremendous efforts in past research, offline signature verification still remains a challenge, particularly in discriminating between genuine signatures and skilled forgeries, because the difference of appearance between genuine and skilled forgery may be smaller than that between genuine ones. This challenge is even more critical in writer-independent scenario, where each writer has very few samples for training. This paper proposes a region based Deep Convolutional Siamese Network using metric learning method, which is applicable to both writer-dependent (WD) and writer-independent (WI) scenario. For representing minute but discriminative details, a Mutual Signature DenseNet (MSDN) is designed to extract features and learn the similarity measure from local regions instead of whole signature images. Based on local regions comparison, the similarity scores of multiple regions are fused for final decision of verification. In experiments on public datasets CEDAR and GPDS, the proposed method achieved state-of-the-art performance of 6.74\% EER and 8.24\% EER in WI scenario, respectively, and 1.67\% EER and 1.65\% EER in WD scenario, respectively.},
  archive      = {J_PR},
  author       = {Li Liu and Linlin Huang and Fei Yin and Youbin Chen},
  doi          = {10.1016/j.patcog.2021.108009},
  journal      = {Pattern Recognition},
  pages        = {108009},
  shortjournal = {Pattern Recognition},
  title        = {Offline signature verification using a region based deep metric learning network},
  volume       = {118},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning the micro deformations by max-pooling for offline
signature verification. <em>PR</em>, <em>118</em>, 108008. (<a
href="https://doi.org/10.1016/j.patcog.2021.108008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For signature verification systems , micro deformations can be defined as the small differences in the same strokes of signatures or special writing habits of different signers. These micro deformations can reveal the core distinction between the genuine signatures and skilled forgeries. In this paper, we prove that Convolutional Neural Networks (CNNs) have the potential to extract those micro deformations by max-pooling. More specifically, the micro deformations can be determined by watching the location coordinates of the maximum values in pooling windows of max-pooling. Extensive analysis and experiments demonstrate that it is possible to achieve state-of-the-art performance by using this location information as a new feature for capturing micro deformations, along with convolutional features. The proposed method outperforms the state-of-the-art systems on four publicly available datasets of different languages, i.e., English (GPDSsynthetic, CEDAR), Persian (UTSig), and Hindi (BHSig260).},
  archive      = {J_PR},
  author       = {Yuchen Zheng and Brian Kenji Iwana and Muhammad Imran Malik and Sheraz Ahmed and Wataru Ohyama and Seiichi Uchida},
  doi          = {10.1016/j.patcog.2021.108008},
  journal      = {Pattern Recognition},
  pages        = {108008},
  shortjournal = {Pattern Recognition},
  title        = {Learning the micro deformations by max-pooling for offline signature verification},
  volume       = {118},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Online deep transferable dictionary learning. <em>PR</em>,
<em>118</em>, 108007. (<a
href="https://doi.org/10.1016/j.patcog.2021.108007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In real-world applications, large-scale unlabeled data usually becomes available gradually over time. Online learning is important to update models while preserving their historical knowledge. However, a time-varying distribution shift exists in incoming sequential data in online learning, resulting in a data cluster discrepancy between the incoming unlabeled data and older labeled data, which is a challenging situation for online learning. To address this issue, we propose an online deep transferable dictionary learning (ODTDL) method that simultaneously mitigates the data cluster discrepancy for incoming unlabeled data while preserving historical knowledge of older data in the dictionary. By forming a locally linear representation and association of incoming unlabeled data over a small amount of labeled data in a deep feature space, the proposed ODTDL method can reveal data cluster discrepancies. To implement this approach, we propose a two-level affiliation regularizer that both comprehensively reveals the local instance-level and global cluster-level affiliations and enables an off-the-shelf dictionary reconstruction error method to establish a knowledge transfer pipeline between the labeled and unlabeled data. For online learning, this approach further decomposes the knowledge transfer pipeline into batchwise transfer pipelines, thereby establishing batchwise transfer pipelines between labeled and unlabeled data. Finally, the proposed method is confirmed to be feasible in online semi-supervised learning (SSL) and online unsupervised domain adaptation (UDA) scenarios and demonstrates its superiority in the online setting.},
  archive      = {J_PR},
  author       = {Sheng Wu and Ancong Wu and Wei-Shi Zheng},
  doi          = {10.1016/j.patcog.2021.108007},
  journal      = {Pattern Recognition},
  pages        = {108007},
  shortjournal = {Pattern Recognition},
  title        = {Online deep transferable dictionary learning},
  volume       = {118},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Joint segmentation and detection of COVID-19 via a
sequential region generation network. <em>PR</em>, <em>118</em>, 108006.
(<a href="https://doi.org/10.1016/j.patcog.2021.108006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The fast pandemics of coronavirus disease (COVID-19) has led to a devastating influence on global public health. In order to treat the disease, medical imaging emerges as a useful tool for diagnosis. However, the computed tomography (CT) diagnosis of COVID-19 requires experts’ extensive clinical experience. Therefore, it is essential to achieve rapid and accurate segmentation and detection of COVID-19. This paper proposes a simple yet efficient and general-purpose network, called Sequential Region Generation Network (SRGNet), to jointly detect and segment the lesion areas of COVID-19. SRGNet can make full use of the supervised segmentation information and then outputs multi-scale segmentation predictions. Through this, high-quality lesion-areas suggestions can be generated on the predicted segmentation maps , reducing the diagnosis cost. Simultaneously, the detection results conversely refine the segmentation map by a post-processing procedure, which significantly improves the segmentation accuracy . The superiorities of our SRGNet over the state-of-the-art methods are validated through extensive experiments on the built COVID-19 database.},
  archive      = {J_PR},
  author       = {Jipeng Wu and Haibo Xu and Shengchuan Zhang and Xi Li and Jie Chen and Jiawen Zheng and Yue Gao and Yonghong Tian and Yongsheng Liang and Rongrong Ji},
  doi          = {10.1016/j.patcog.2021.108006},
  journal      = {Pattern Recognition},
  pages        = {108006},
  shortjournal = {Pattern Recognition},
  title        = {Joint segmentation and detection of COVID-19 via a sequential region generation network},
  volume       = {118},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Periphery-aware COVID-19 diagnosis with contrastive
representation enhancement. <em>PR</em>, <em>118</em>, 108005. (<a
href="https://doi.org/10.1016/j.patcog.2021.108005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computer-aided diagnosis has been extensively investigated for more rapid and accurate screening during the outbreak of COVID-19 epidemic. However, the challenge remains to distinguish COVID-19 in the complex scenario of multi-type pneumonia classification and improve the overall diagnostic performance. In this paper, we propose a novel periphery-aware COVID-19 diagnosis approach with contrastive representation enhancement to identify COVID-19 from influenza-A (H1N1) viral pneumonia, community acquired pneumonia (CAP), and healthy subjects using chest CT images. Our key contributions include: 1) an unsupervised Periphery-aware Spatial Prediction (PSP) task which is designed to introduce important spatial patterns into deep networks; 2) an adaptive Contrastive Representation Enhancement (CRE) mechanism which can effectively capture the intra-class similarity and inter-class difference of various types of pneumonia. We integrate PSP and CRE to obtain the representations which are highly discriminative in COVID-19 screening. We evaluate our approach comprehensively on our constructed large-scale dataset and two public datasets. Extensive experiments on both volume-level and slice-level CT images demonstrate the effectiveness of our proposed approach with PSP and CRE for COVID-19 diagnosis.},
  archive      = {J_PR},
  author       = {Junlin Hou and Jilan Xu and Longquan Jiang and Shanshan Du and Rui Feng and Yuejie Zhang and Fei Shan and Xiangyang Xue},
  doi          = {10.1016/j.patcog.2021.108005},
  journal      = {Pattern Recognition},
  pages        = {108005},
  shortjournal = {Pattern Recognition},
  title        = {Periphery-aware COVID-19 diagnosis with contrastive representation enhancement},
  volume       = {118},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Color edge detection by learning classification network with
anisotropic directional derivative matrices. <em>PR</em>, <em>118</em>,
108004. (<a href="https://doi.org/10.1016/j.patcog.2021.108004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a neural network-based color edge detector is constructed by learning a classifier using anisotropic directional derivative (ANDD) matrices of a color image as input. The training stage on a color edge dataset with ground truth (GT) edges includes calculation of ANDD matrices, generation of feature matrices, and training a classifier. For each training image, a set of ANDD matrices are calculated from the ANDDs with different parameter setups for training and from which a set of the color edge strength maps (CESMs) are extracted by the singular vector decomposition. The CESMs and the GTs on edges of the image are combined into a feature matrix for training. Using the feature matrices of all the training images as input, a classification neural network is trained and it outputs the probability of a pixel to be an edge pixel. In the detection stage, for a color image, its ANDD matrices, CESMs, and the color edge direction maps (CEDMs) are first computed and then the CESMs are input into the classification neural network to obtain the edge probability map (EPM) of the image. Finally, the non-maximum suppression and hysteresis thresholding are applied to the EPM and CEDMs to generate the binary edge map. The proposed detector attains better performance than the existing gradient-based detectors and is competitive with learning-based detectors on three commonly-used color image datasets for edge and contour detection.},
  archive      = {J_PR},
  author       = {Ou Li and Peng-Lang Shui},
  doi          = {10.1016/j.patcog.2021.108004},
  journal      = {Pattern Recognition},
  pages        = {108004},
  shortjournal = {Pattern Recognition},
  title        = {Color edge detection by learning classification network with anisotropic directional derivative matrices},
  volume       = {118},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Siamese network for object tracking with multi-granularity
appearance representations. <em>PR</em>, <em>118</em>, 108003. (<a
href="https://doi.org/10.1016/j.patcog.2021.108003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A reliable tracker has the ability to adapt to change of objects over time, and is robust and accurate. We build such a tracker by extracting semantic features using robust Siamese networks and multi-granularity color features . It incorporates a semantic model that can capture high quality semantic features and an appearance model that can describe object at pixel, local and global levels effectively. Furthermore, we propose a novel selective traverse algorithm to allocate weights to semantic models and appearance models dynamically for better tracking performance. During tracking, our tracker updates appearance representations for objects based on the recent tracking results. The proposed tracker operates at speeds that exceed the real-time requirement, and outperforms nearly all other state-of-the-art trackers on OTB-2013/2015 and VOT-2016/2017 benchmarks.},
  archive      = {J_PR},
  author       = {Zhuoyi Zhang and Yifeng Zhang and Xu Cheng and Guojun Lu},
  doi          = {10.1016/j.patcog.2021.108003},
  journal      = {Pattern Recognition},
  pages        = {108003},
  shortjournal = {Pattern Recognition},
  title        = {Siamese network for object tracking with multi-granularity appearance representations},
  volume       = {118},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). ScieNet: Deep learning with spike-assisted contextual
information extraction. <em>PR</em>, <em>118</em>, 108002. (<a
href="https://doi.org/10.1016/j.patcog.2021.108002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spiking neural network (SNN) is a type of artificial neural network that uses biologically inspired neuron models and learning rules to develop artificial intelligence with capability parallel to human brain. Deep neural networks (DNNs), on the other hand, uses less biologically plausible neurons and training methods such as gradient descent , and has shown good accuracy in computer vision tasks. However, human brain can still outperform DNN in certain scenarios. For example, DNN experiences significant performance degradation when perturbation from various sources is present in the input, which makes DNN less reliable for systems interacting with physical world. In this paper, we present a hybrid deep net work architecture with s pike-assisted c ontextual i nformation e xtraction ( ScieNet ) as a solution to the problem. ScieNet integrates a front-end SNN with a novel stochastic spike-timing-dependent plasticity (STDP) algorithm that extracts visual context from images. The back-end DNN is trained for classification given the contextual information. The integrated network demonstrates high resilience to input perturbations without relying on pre-training on perturbed inputs . We demonstrate ScieNet with various back-end DNNs for image classification using different datasets and considering both stochastic and structured input perturbations. Experimental results demonstrate significant improvement in accuracy on perturbed images, while maintaining state-of-the-art accuracy on clean images.},
  archive      = {J_PR},
  author       = {Xueyuan She and Yun Long and Daehyun Kim and Saibal Mukhopadhyay},
  doi          = {10.1016/j.patcog.2021.108002},
  journal      = {Pattern Recognition},
  pages        = {108002},
  shortjournal = {Pattern Recognition},
  title        = {ScieNet: Deep learning with spike-assisted contextual information extraction},
  volume       = {118},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Controllable image generation with semi-supervised deep
learning and deformable-mean-template based geometry-appearance
disentanglement. <em>PR</em>, <em>118</em>, 108001. (<a
href="https://doi.org/10.1016/j.patcog.2021.108001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Typical deep-neural-network (DNN) based generative image models often (i) show limited ability to learn a disentangled latent representation, (ii) show limited controllability leading to undesirable side effects when manipulating selected attributes during image generation , and (iii) require large attribute-annotated training sets. We propose a generative DNN model for face images by explicitly disentangling geometry and appearance modeling to achieve selective controllability of the desired attributes with less side effects. To learn geometric variability, we leverage grayscale sketch representations to learn (i) a deformable mean template representing the population-mean face geometry and (ii) a generative model of deformations to model individual face-geometry variations, using dense image registration. We learn the appearance variability in a (color-image) space that we explicitly design by factoring out the geometric variability. We propose a variational formulation to enable semi-supervised learning when manually-annotated attributes are severely limited in the training set. Results on large datasets show that, compared to schemes using deformation models or variational learning, our method significantly improves face-image model fits and facial-feature controllability even with semi-supervised learning.},
  archive      = {J_PR},
  author       = {Krishna Wadhwani and Suyash P. Awate},
  doi          = {10.1016/j.patcog.2021.108001},
  journal      = {Pattern Recognition},
  pages        = {108001},
  shortjournal = {Pattern Recognition},
  title        = {Controllable image generation with semi-supervised deep learning and deformable-mean-template based geometry-appearance disentanglement},
  volume       = {118},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Low-complexity arrays of contour signatures for exact shape
retrieval. <em>PR</em>, <em>118</em>, 108000. (<a
href="https://doi.org/10.1016/j.patcog.2021.108000">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a framework for a fast exact shape retrieval called Low-complexity Arrays of Contour Signatures. The purposes are to match a shape against a database in constant time and to retrieve correct shapes very close to the query, while the latter may have undergone rigid transformations and noise. We present a shape signature based on prior works as well as a compact characterization of such signatures, a system of associative arrays allowing a short search time for retrieval and a technique of pairwise alignment . This method shows a good resilience to perturbations and is performed in constant computational time.},
  archive      = {J_PR},
  author       = {Florian Lardeux and Sylvain Marchand and Petra Gomez-Krämer},
  doi          = {10.1016/j.patcog.2021.108000},
  journal      = {Pattern Recognition},
  pages        = {108000},
  shortjournal = {Pattern Recognition},
  title        = {Low-complexity arrays of contour signatures for exact shape retrieval},
  volume       = {118},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dual feature extraction network for hyperspectral image
analysis. <em>PR</em>, <em>118</em>, 107992. (<a
href="https://doi.org/10.1016/j.patcog.2021.107992">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperspectral anomaly detection (HAD) is a research endeavor of high practical relevance within remote sensing scene interpretation. In this work, we propose an unsupervised approach, dual feature extraction network (DFEN) for HAD, to gradually build up ever-greater discrimination between the original data and background. In particular, we impose an end-to-end discriminative learning loss on two networks. Among them, adversarial learning aims to keep the original spectrum while Gaussian constrained learning intends to learn the background distribution in the potential space. To extract the anomaly, we calculate spatial and spectral anomaly scores based on mean squared error (MSE) spatial distance and orthogonal projection divergence (OPD) spectral distance between two latent feature matrices. Finally, the comprehensive detection result is obtained by a simple dot product between two domains to further reduce the false alarm rate . Experiments have been conducted on eight real hyperspectral data sets captured by different sensors over different scenes, which show that the proposed DFEN method is superior to other compared methods in detection accuracy or false alarm rate.},
  archive      = {J_PR},
  author       = {Weiying Xie and Jie Lei and Shuo Fang and Yunsong Li and Xiuping Jia and Mingsuo Li},
  doi          = {10.1016/j.patcog.2021.107992},
  journal      = {Pattern Recognition},
  pages        = {107992},
  shortjournal = {Pattern Recognition},
  title        = {Dual feature extraction network for hyperspectral image analysis},
  volume       = {118},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Average localised proximity: A new data descriptor with good
default one-class classification performance. <em>PR</em>, <em>118</em>,
107991. (<a href="https://doi.org/10.1016/j.patcog.2021.107991">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One-class classification is a challenging subfield of machine learning in which so-called data descriptors are used to predict membership of a class based solely on positive examples of that class, and no counter-examples. A number of data descriptors that have been shown to perform well in previous studies of one-class classification, like the Support Vector Machine (SVM), require setting one or more hyperparameters. There has been no systematic attempt to date to determine optimal default values for these hyperparameters, which limits their ease of use, especially in comparison with hyperparameter-free proposals like the Isolation Forest (IF). We address this issue by determining optimal default hyperparameter values across a collection of 246 one-class classification problems derived from 50 different real-world datasets. In addition, we propose a new data descriptor, Average Localised Proximity (ALP) to address certain issues with existing approaches based on nearest neighbour distances. Finally, we evaluate classification performance using a leave-one-dataset-out procedure, and find strong evidence that ALP outperforms IF and a number of other data descriptors, as well as weak evidence that it outperforms SVM, making ALP a good default choice.},
  archive      = {J_PR},
  author       = {Oliver Urs Lenz and Daniel Peralta and Chris Cornelis},
  doi          = {10.1016/j.patcog.2021.107991},
  journal      = {Pattern Recognition},
  pages        = {107991},
  shortjournal = {Pattern Recognition},
  title        = {Average localised proximity: A new data descriptor with good default one-class classification performance},
  volume       = {118},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A review of methods for imbalanced multi-label
classification. <em>PR</em>, <em>118</em>, 107965. (<a
href="https://doi.org/10.1016/j.patcog.2021.107965">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-Label Classification (MLC) is an extension of the standard single-label classification where each data instance is associated with several labels simultaneously. MLC has gained much importance in recent years due to its wide range of application domains. However, the class imbalance problem has become an inherent characteristic of many multi-label datasets, where the samples and their corresponding labels are non-uniformly distributed over the data space. The imbalanced problem in MLC imposes challenges to multi-label data analytics which can be viewed from three perspectives: imbalance within labels, among labels, and label-sets. In this paper, we provide a review of the approaches for handling the imbalance problem in multi-label data by collecting the existing research work. As the first systematic study of approaches addressing an imbalanced problem in MLC, this paper provides a comprehensive survey of the state-of-the-art methods for imbalanced MLC, including the characteristics of imbalanced multi-label datasets, evaluation measures and comparative analysis of the proposed methods. The study also discusses important results reported so far in the literature and highlights some of their strengths and limitations to guide future research.},
  archive      = {J_PR},
  author       = {Adane Nega Tarekegn and Mario Giacobini and Krzysztof Michalak},
  doi          = {10.1016/j.patcog.2021.107965},
  journal      = {Pattern Recognition},
  pages        = {107965},
  shortjournal = {Pattern Recognition},
  title        = {A review of methods for imbalanced multi-label classification},
  volume       = {118},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Detection of COVID-19 from speech signal using bio-inspired
based cepstral features. <em>PR</em>, <em>117</em>, 107999. (<a
href="https://doi.org/10.1016/j.patcog.2021.107999">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The early detection of COVID-19 is a challenging task due to its deadly spreading nature and existing fear in minds of people. Speech-based detection can be one of the safest tools for this purpose as the voice of the suspected can be easily recorded. The Mel Frequency Cepstral Coefficient (MFCC) analysis of speech signal is one of the oldest but potential analysis tools. The performance of this analysis mainly depends on the use of conversion between normal frequency scale to perceptual frequency scale and the frequency range of the filters used. Traditionally, in speech recognition, these values are fixed. But the characteristics of speech signals vary from disease to disease. In the case of detection of COVID-19, mainly the coughing sounds are used whose bandwidth and properties are quite different from the complete speech signal. By exploiting these properties the efficiency of the COVID-19 detection can be improved. To achieve this objective the frequency range and the conversion scale of frequencies have been suitably optimized. Further to enhance the accuracy of detection performance, speech enhancement has been carried out before extraction of features. By implementing these two concepts a new feature called COVID-19 Coefficient (C-19CC) is developed in this paper. Finally, the performance of these features has been compared.},
  archive      = {J_PR},
  author       = {Tusar Kanti Dash and Soumya Mishra and Ganapati Panda and Suresh Chandra Satapathy},
  doi          = {10.1016/j.patcog.2021.107999},
  journal      = {Pattern Recognition},
  pages        = {107999},
  shortjournal = {Pattern Recognition},
  title        = {Detection of COVID-19 from speech signal using bio-inspired based cepstral features},
  volume       = {117},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Scale-balanced loss for object detection. <em>PR</em>,
<em>117</em>, 107997. (<a
href="https://doi.org/10.1016/j.patcog.2021.107997">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object detection is an important field in computer vision . Nevertheless, a research area that has so far not received much attention is the study into the effectiveness of anchor matching strategy and imbalance in anchor-based object detection, in particular small object detection. It is clear that the objects with larger sizes tend to match more anchors than smaller ones. This matching imbalance may result in poor performance in detecting small objects. It can be alleviated by paying more attention to the objects that match with fewer anchors. We propose an innovative flexible loss function for object detection, which is compatible with popular anchor-based detection methods. The proposed method, called the scale-balanced loss, does not add any extra computational cost to the original pipelines. By re-weighting strategy, the proposed method significantly improves the accuracy of multi-scale object detection, especially for small objects. Comprehensive experiments indicate that the scale-balanced loss achieved excellent generalization performance when separately applied to some popular detection methods. The scale-balanced loss attained up to 15\% improvements on recall rates of small and medium objects in both the PASCAL VOC and MS COCO dataset. It is also beneficial to the AP result on MS COCO with an improvement of more than 1.5\%.},
  archive      = {J_PR},
  author       = {Kai Shuang and Zhiheng Lyu and Jonathan Loo and Wentao Zhang},
  doi          = {10.1016/j.patcog.2021.107997},
  journal      = {Pattern Recognition},
  pages        = {107997},
  shortjournal = {Pattern Recognition},
  title        = {Scale-balanced loss for object detection},
  volume       = {117},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust deep k-means: An effective and simple method for data
clustering. <em>PR</em>, <em>117</em>, 107996. (<a
href="https://doi.org/10.1016/j.patcog.2021.107996">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering aims to partition an input dataset into distinct groups according to some distance or similarity measurements. One of the most widely used clustering method nowadays is the k k -means algorithm because of its simplicity and efficiency. In the last few decades, k k -means and its various extensions have been formulated to solve the practical clustering problems . However, existing clustering methods are often presented in a single-layer formulation (i.e., shallow formulation). As a result, the mapping between the obtained low-level representation and the original input data may contain rather complex hierarchical information. To overcome the drawbacks of low-level features, deep learning techniques are adopted to extract deep representations and improve the clustering performance. In this paper, we propose a robust deep k k -means model to learn the hidden representations associate with different implicit lower-level attributes. By using the deep structure to hierarchically perform k k -means, the hierarchical semantics of data can be exploited in a layerwise way. Data samples from the same class are forced to be closer layer by layer, which is beneficial for clustering task . The objective function of our model is derived to a more trackable form such that the optimization problem can be tackled more easily and the final robust results can be obtained. Experimental results over 12 benchmark data sets substantiate that the proposed model achieves a breakthrough in clustering performance, compared with both classical and state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Shudong Huang and Zhao Kang and Zenglin Xu and Quanhui Liu},
  doi          = {10.1016/j.patcog.2021.107996},
  journal      = {Pattern Recognition},
  pages        = {107996},
  shortjournal = {Pattern Recognition},
  title        = {Robust deep k-means: An effective and simple method for data clustering},
  volume       = {117},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Cross-scene foreground segmentation with supervised and
unsupervised model communication. <em>PR</em>, <em>117</em>, 107995. (<a
href="https://doi.org/10.1016/j.patcog.2021.107995">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper 1 , we investigate cross-scene video foreground segmentation via supervised and unsupervised model communication. Traditional unsupervised background subtraction methods often face the challenging problem of updating the statistical background model online. In contrast, supervised foreground segmentation methods , such as those that are based on deep learning , rely on large amounts of training data, thereby limiting their cross-scene performance. Our method leverages segmented masks from a cross-scene trained deep model (spatio-temporal attention model (STAM), pyramid scene parsing network (PSPNet), or DeepLabV3+) to seed online updates for the statistical background model (CPB), thereby refining the foreground segmentation. More flexible than methods that require scene-specific training and more data-efficient than unsupervised models, our method outperforms state-of-the-art approaches on CDNet2014, WallFlower, and LIMU according to our experimental results. The proposed framework can be integrated into a video surveillance system in a plug-and-play form to realize cross-scene foreground segmentation.},
  archive      = {J_PR},
  author       = {Dong Liang and Bin Kang and Xinyu Liu and Pan Gao and Xiaoyang Tan and Shun’ichi Kaneko},
  doi          = {10.1016/j.patcog.2021.107995},
  journal      = {Pattern Recognition},
  pages        = {107995},
  shortjournal = {Pattern Recognition},
  title        = {Cross-scene foreground segmentation with supervised and unsupervised model communication},
  volume       = {117},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Digital hair removal by deep learning for skin lesion
segmentation. <em>PR</em>, <em>117</em>, 107994. (<a
href="https://doi.org/10.1016/j.patcog.2021.107994">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Occlusion due to hair in dermoscopic images affects the diagnostic operation and the accuracy of its analysis of a skin lesion. Also, dermis hair has the following different characteristics: thin; overlapping; faded; of similar contrast or colour to the underlying skin; and obscuring/covering textured lesions. These make digital hair removal (DHR), which involves hair segmentation and hair gap inpainting, a challenging task. Thus, traditional hard-coded threshold-based hair removal methods are not effective, resulting in over-removal which loses important information of the skin lesion, or under-removal which cannot remove the hair effectively. In this paper, we propose a deep learning approach to DHR based on U-Net and a free-form image inpainting architecture. In hair segmentation, a well-labelled dataset is created and used to train U-Net in order to obtain accurate hair masks. In inpainting, a free-form image inpainting architecture (i.e., Gated convolution and SN-PatchGAN) which has been trained on millions of images is used to inpaint any hair gaps. We also propose an evaluation method to analyze the effect of hair removal based on a single dermoscopic image, named intra structural similarity (Intra-SSIM). The process of DHR is repeated until there is no change in the average value of Intra-SSIM. Using the ISIC 2018 dataset, the performance of the proposed method is shown to be better than other state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Wei Li and Alex Noel Joseph Raj and Tardi Tjahjadi and Zhemin Zhuang},
  doi          = {10.1016/j.patcog.2021.107994},
  journal      = {Pattern Recognition},
  pages        = {107994},
  shortjournal = {Pattern Recognition},
  title        = {Digital hair removal by deep learning for skin lesion segmentation},
  volume       = {117},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Image stitching based on angle-consistent warping.
<em>PR</em>, <em>117</em>, 107993. (<a
href="https://doi.org/10.1016/j.patcog.2021.107993">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many warping methods for image stitching have been proposed to construct panoramic image mosaics free of artifacts. Existing methods heavily rely on coordinate correspondences between keypoints in stitching, which may not provide adequate constraints for alignment. In this paper, we discover and employ a new constraint — angle correspondences to address the above problem. The angle of a feature point represents the local directional structure of the point, which is an extension to its position and customarily ignored in image stitching. We propose to jointly consider the coordinates as well as the angles in keypoint correspondences. Such a strategy helps to generate a correct warping in the overlapping regions of the stitched image. In addition, we propose a novel constraint — mesh angle preservation to prevent undesired distortion in non-overlapping areas. Experiments in several challenging cases demonstrate that our method yields more accurate results with significantly less artifacts in comparison with state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Yinqi Chen and Huicheng Zheng and Yiyan Ma and Zhiwei Yan},
  doi          = {10.1016/j.patcog.2021.107993},
  journal      = {Pattern Recognition},
  pages        = {107993},
  shortjournal = {Pattern Recognition},
  title        = {Image stitching based on angle-consistent warping},
  volume       = {117},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Two-stage adaptive random fourier sampling method for image
reconstruction. <em>PR</em>, <em>117</em>, 107990. (<a
href="https://doi.org/10.1016/j.patcog.2021.107990">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a random Fourier sampling scheme to enhance the accuracy of the high frequency pattern estimation for image reconstruction. This method is designed to work in a constrained ℓ 1 ℓ1 minimization based on the Fourier-Haar interplay revealing a column-wise maximum coherent structure that we provide. Essential in the scheme is to generate a data-driven density function by a small percentage of Fourier samples. The density function governs a random sampling procedure to acquire high frequency information, resulting in better reconstruction of the Haar wavelet coefficients . We also discuss a few examples of exact recovery of the Haar wavelet coefficients from which the proposed sampling scheme has emerged. Numerical experiments confirm superiority of the proposed sampling scheme to other conventional sampling schemes in the ℓ 1 ℓ1 framework.},
  archive      = {J_PR},
  author       = {Joo Dong Yun and Yunho Kim},
  doi          = {10.1016/j.patcog.2021.107990},
  journal      = {Pattern Recognition},
  pages        = {107990},
  shortjournal = {Pattern Recognition},
  title        = {Two-stage adaptive random fourier sampling method for image reconstruction},
  volume       = {117},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An efficient and locality-oriented hausdorff distance
algorithm: Proposal and analysis of paradigms and implementations.
<em>PR</em>, <em>117</em>, 107989. (<a
href="https://doi.org/10.1016/j.patcog.2021.107989">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hausdorff distance (HD) is a popular similarity metric used in the comparison of images or 3D volumes. Although popular, its main weakness is computing power consumption , being one of the slowest set distances. In this work, a novel, parallel and locality-oriented Hausdorff distance implementation is proposed. Novel as it is the first time in the literature that an actual algorithmic implementation using morphological dilations is proposed and thoroughly evaluated. Parallel, as it is more robust in terms of parallelization than the state-of-the-art algorithm and local as it has an intrinsic sensitivity to voxels that are closer in space. This proposal can be faster than the state-of-the-art in several practical cases such as in medical imaging registrations (up to 8 times faster on average in one of the CPU experiments) and is faster in the worst-case (up to 22337 times faster in one of the CPU experiments). Worst-case scenarios and high resolution volumes also favor the proposed approach. Throughout the work, several sequential and parallel CPU and GPU implementations are evaluated and compared.},
  archive      = {J_PR},
  author       = {Érick Oliveira Rodrigues},
  doi          = {10.1016/j.patcog.2021.107989},
  journal      = {Pattern Recognition},
  pages        = {107989},
  shortjournal = {Pattern Recognition},
  title        = {An efficient and locality-oriented hausdorff distance algorithm: Proposal and analysis of paradigms and implementations},
  volume       = {117},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Feature selection with kernelized multi-class support vector
machine. <em>PR</em>, <em>117</em>, 107988. (<a
href="https://doi.org/10.1016/j.patcog.2021.107988">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection is an important procedure in machine learning because it can reduce the complexity of the final learning model and simplify the interpretation. In this paper, we propose a novel non-linear feature selection method that targets multi-class classification problems in the framework of support vector machines . The proposed method is achieved using a kernelized multi-class support vector machine with a fast version of recursive feature elimination. The proposed method selects features that work well for all classes, as the involved classifier simultaneously constructs multiple decision functions that separates each class from the others. We formulate the classifier as a large optimisation problem , and iteratively solve one decision function at a time, leading to a lower computational time complexity than when solving the large optimisation problem directly. The coefficients of the classifier are then used as a ranking criterion in the accelerated recursive feature elimination by adding batch elimination and a rechecking process. Experimental results on several datasets demonstrate the superior performance of the proposed feature selection method.},
  archive      = {J_PR},
  author       = {Yinan Guo and Zirui Zhang and Fengzhen Tang},
  doi          = {10.1016/j.patcog.2021.107988},
  journal      = {Pattern Recognition},
  pages        = {107988},
  shortjournal = {Pattern Recognition},
  title        = {Feature selection with kernelized multi-class support vector machine},
  volume       = {117},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SRAGL-AWCL: A two-step multi-view clustering via sparse
representation and adaptive weighted cooperative learning. <em>PR</em>,
<em>117</em>, 107987. (<a
href="https://doi.org/10.1016/j.patcog.2021.107987">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sparse representation and cooperative learning are two representative technologies in the field of multi-view spectral clustering . The former can effectively extract features of multiple views by the removal of redundant information contained in each view. The latter can incorporate the diversity of each view. However, traditional sparse representation and cooperative learning algorithms are inadequate in preserving the internal geometric features of data by manifold regularization. In fact, general approaches rarely consider the similarities between the internal graph structures of individual views. Moreover, to achieve the optimal global feature learning, we present a novel two-step multi-view spectral clustering strategy, which combines the proposed sparse representation by adaptive graph learning with adaptive weighted cooperative learning. In the first step, the proposed matrix factorization by manifold regularization can strengthen the sparse features clustering discrimination of samples of each view. Specifically, the synchronization optimization method by introducing adaptive graph learning can better retain its internal complete structure of each view. This ensures the structure correlation of views through the usage of the sparse matrix and the optimal graph similarity matrix. In the second step, the adaptive weighted cooperative learning is performed on each view to get a global optimized matrix. In order to ensure that the global matrix is associated with various view features, graph learning is also performed on the global matrix. Experiment results on several multi-view datasets and single-view datasets show that the proposed method significantly outperformed the state-of-the-art algorithms.},
  archive      = {J_PR},
  author       = {Junpeng Tan and Zhijing Yang and Yongqiang Cheng and Jielin Ye and Bing Wang and Qingyun Dai},
  doi          = {10.1016/j.patcog.2021.107987},
  journal      = {Pattern Recognition},
  pages        = {107987},
  shortjournal = {Pattern Recognition},
  title        = {SRAGL-AWCL: A two-step multi-view clustering via sparse representation and adaptive weighted cooperative learning},
  volume       = {117},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust feature matching using guided local outlier factor.
<em>PR</em>, <em>117</em>, 107986. (<a
href="https://doi.org/10.1016/j.patcog.2021.107986">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Matching local features on two or more images is fundamental for many applications in the field of computer vision and pattern recognition. Identifying and rejecting mismatches is an important part in the framework of feature matching, due to the putative correspondences always contaminated by mismatches with the error-prone local feature detectors. In this paper, we introduce a novel method, namely Guided Local Outlier Factor (GLOF) for feature matching with gross mismatches under multi-granularity neighborhood structure-preserving. We first construct a tentative correspondence set by matching multi-features. Then, we identify and remove mismatches. Inspired by the anomaly detection technique, putative correspondences are assigned to a particular score, so abnormal instances, i.e. , mismatches can be classified by a user-defined threshold. More specially, the neighborhood preserving guides the local searching procedure. Moreover, to eliminate the fluctuation of the matching results with different sizes of local neighbors, we use the multi-granularity algorithm to average out the deviation. Experimental results demonstrate that the introduced approach is superior to several state-of-the-art methods in terms of mismatch rejection on publicly available datasets.},
  archive      = {J_PR},
  author       = {Gang Wang and Yufei Chen},
  doi          = {10.1016/j.patcog.2021.107986},
  journal      = {Pattern Recognition},
  pages        = {107986},
  shortjournal = {Pattern Recognition},
  title        = {Robust feature matching using guided local outlier factor},
  volume       = {117},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Level set framework with transcendental constraint for
robust and fast image segmentation. <em>PR</em>, <em>117</em>, 107985.
(<a href="https://doi.org/10.1016/j.patcog.2021.107985">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Though image segmentation models are plentiful and have many applications nowadays, it can be difficult to segment images with complex boundaries and serious intensity inhomogeneity . To some extent, the region-scalable fitting energy model can segment images suffering from intensity inhomogeneity since it considers image intensity as a function, but it relies on initial conditions dramatically. Nowadays, prior knowledge has been widely applied in image segmentation models, which can integrate automatic method and experts experience in one robust and fast segmentation model . In this paper we present a new model that can segment various images accurately by taking the advantages of the region-scalable fitting energy model and the advanced transcendental constraint from artificial experience. The proposed energy functional consists of a smooth length term, a target image data term and a transcendental constraint term. The transcendental constraint term plays a key role in the proposed model, which not only gives the accurate segmentation results but also provides us the chance to carry out the parallel computation. In the proposed-parallel model, the efficiency is improved a lot and the results become more precise compared with other methods. The split Bregman method is applied to minimize the energy functional. Furthermore, we present the convergence analysis and the time complexity analysis of our algorithm. Multiple experimental results and comparisons including parameters sensitivity discussion are shown to demonstrate the superiority of the proposed model such as high accuracy, robustness and efficiency.},
  archive      = {J_PR},
  author       = {Yunyun Yang and Ruofan Wang and Xiu Shu and Chong Feng and Ruicheng Xie and Wenjing Jia and Chunming Li},
  doi          = {10.1016/j.patcog.2021.107985},
  journal      = {Pattern Recognition},
  pages        = {107985},
  shortjournal = {Pattern Recognition},
  title        = {Level set framework with transcendental constraint for robust and fast image segmentation},
  volume       = {117},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Penalized -regression-based bicluster localization.
<em>PR</em>, <em>117</em>, 107984. (<a
href="https://doi.org/10.1016/j.patcog.2021.107984">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Biclustering (co-clustering, two-mode clustering), as one of the classical unsupervised learning methods, has been applied in many different fields in recent years. Different types of biclustering methods have been developed such as probabilistic methods , two-way clustering methods , variance minimization methods, and so on. However, few regression-based methods have been proposed to the best of our knowledge. Such methods have been applied in traditional clustering, which can improve both the computational efficiency and the clustering accuracy. In this paper, we present a penalized regression-based method for localizing the biclusters (PRbiclust). By imposing Truncated LASSO Penalty (TLP) and group TLP terms to penalize the column vectors and the row vectors in the regression model, the structure of biclusters in the data matrix is recovered. The model is formulated as an optimization problem with nonconvex penalties, and a computationally efficient algorithm is proposed to solve it. Convergence of the algorithm is proved. To extract the biclusters from the recovered data matrix, we propose a graph-based localization method. An evaluation criterion is also proposed to measure the efficiency of bicluster localization when noise entries exist. We apply the proposed method to both simulated datasets with different setups and a real dataset. Experiments show that this method can well capture the bicluster structure, and performs better than the existing works.},
  archive      = {J_PR},
  author       = {Hanjia Gao and Zhengjian Bai and Weiguo Gao and Shuqin Zhang},
  doi          = {10.1016/j.patcog.2021.107984},
  journal      = {Pattern Recognition},
  pages        = {107984},
  shortjournal = {Pattern Recognition},
  title        = {Penalized -regression-based bicluster localization},
  volume       = {117},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Integrating information theory and adversarial learning for
cross-modal retrieval. <em>PR</em>, <em>117</em>, 107983. (<a
href="https://doi.org/10.1016/j.patcog.2021.107983">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurately matching visual and textual data in cross-modal retrieval has been widely studied in the multimedia community. To address these challenges posited by the heterogeneity gap and the semantic gap , we propose integrating Shannon information theory and adversarial learning. In terms of the heterogeneity gap, we integrate modality classification and information entropy maximization adversarially. For this purpose, a modality classifier (as a discriminator) is built to distinguish the text and image modalities according to their different statistical properties. This discriminator uses its output probabilities to compute Shannon information entropy, which measures the uncertainty of the modality classification it performs. Moreover, feature encoders (as a generator) project uni-modal features into a commonly shared space and attempt to fool the discriminator by maximizing its output information entropy. Thus, maximizing information entropy gradually reduces the distribution discrepancy of cross-modal features, thereby achieving a domain confusion state where the discriminator cannot classify two modalities confidently. To reduce the semantic gap, Kullback-Leibler (KL) divergence and bi-directional triplet loss are used to associate the intra- and inter-modality similarity between features in the shared space. Furthermore, a regularization term based on KL-divergence with temperature scaling is used to calibrate the biased label classifier caused by the data imbalance issue. Extensive experiments with four deep models on four benchmarks are conducted to demonstrate the effectiveness of the proposed approach.},
  archive      = {J_PR},
  author       = {Wei Chen and Yu Liu and Erwin M. Bakker and Michael S. Lew},
  doi          = {10.1016/j.patcog.2021.107983},
  journal      = {Pattern Recognition},
  pages        = {107983},
  shortjournal = {Pattern Recognition},
  title        = {Integrating information theory and adversarial learning for cross-modal retrieval},
  volume       = {117},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Rough-bayesian approach to select class-pair specific
descriptors for HEp-2 cell staining pattern recognition. <em>PR</em>,
<em>117</em>, 107982. (<a
href="https://doi.org/10.1016/j.patcog.2021.107982">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the important problems in computer-aided diagnosis of connective tissue disease is automatic recognition of staining patterns present in HEp-2 cells. In this regard, the paper introduces a novel approach for the recognition of staining patterns by HEp-2 cell indirect immunofluorescence image analysis. The proposed method assumes that a fixed set of local texture descriptors or scales may not be effective for classifying staining patterns into multiple classes. A particular set of descriptors or scales may be significant for classifying a pair of classes, but may not be relevant for other pairs of classes. The proposed approach, therefore, first selects a set of local texture descriptors under appropriate scales for each class-pair, and then forms the final feature set for multiple classes from the relevant descriptors of all possible pairs of classes. A novel framework, termed as Rough-Bayesian model, is introduced to evaluate the relevance of a descriptor and/or a scale. It is based on the merits of rough sets and Bayes decision theory. During the selection of relevant descriptor and/or scale, the proposed method takes care of the presence of both noisy pixels in an HEp-2 cell image and noisy HEp-2 cell images in a staining pattern class. The support vector machine is used to predict the staining patterns present in HEp-2 cell images. The performance of the proposed method, along with a comparison with state-of-the-art methods, is demonstrated on several HEp-2 cell image databases. An important finding is that the accuracy for classifying HEp-2 cell images is significantly increased if class-pair specific descriptors under appropriate scales are considered, instead of selecting a uniform set of descriptors and scales for multiple classes.},
  archive      = {J_PR},
  author       = {Debamita Kumar and Pradipta Maji},
  doi          = {10.1016/j.patcog.2021.107982},
  journal      = {Pattern Recognition},
  pages        = {107982},
  shortjournal = {Pattern Recognition},
  title        = {Rough-bayesian approach to select class-pair specific descriptors for HEp-2 cell staining pattern recognition},
  volume       = {117},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust generalised quadratic discriminant analysis.
<em>PR</em>, <em>117</em>, 107981. (<a
href="https://doi.org/10.1016/j.patcog.2021.107981">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quadratic discriminant analysis (QDA) is a widely used statistical tool to classify observations from different multivariate Normal populations. The generalized quadratic discriminant analysis (GQDA) classification rule/classifier, which generalizes the QDA and the minimum Mahalanobis distance (MMD) classifiers to discriminate between populations with underlying elliptically symmetric distributions competes quite favorably with the QDA classifier when it is optimal and performs much better when QDA fails under non-Normal underlying distributions with heavy tail, e.g. Cauchy distribution . However, the classification rule in GQDA is still based on the sample mean vector and the sample dispersion matrix of a training set, which are extremely non-robust under data contamination. In real world, however, it is quite common to face data which are highly vulnerable to outliers and so the lack of robustness of the classical estimators of the mean vector and the dispersion matrix reduces the efficiency of the GQDA classifier significantly, increasing the misclassification errors. The present paper investigates the performance of the GQDA classifier when the classical estimators of the mean vector and the dispersion matrix used therein are replaced by various robust counterparts. Applications to various real data sets as well as simulation studies reveal far better performance of the proposed robust versions of the GQDA classifier. A comparative study has been made to advocate the appropriate choice of the robust estimators to be used in a specific situation.},
  archive      = {J_PR},
  author       = {Abhik Ghosh and Rita SahaRay and Sayan Chakrabarty and Sayan Bhadra},
  doi          = {10.1016/j.patcog.2021.107981},
  journal      = {Pattern Recognition},
  pages        = {107981},
  shortjournal = {Pattern Recognition},
  title        = {Robust generalised quadratic discriminant analysis},
  volume       = {117},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MASTER: Multi-aspect non-local network for scene text
recognition. <em>PR</em>, <em>117</em>, 107980. (<a
href="https://doi.org/10.1016/j.patcog.2021.107980">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attention-based scene text recognizers have gained huge success, which leverages a more compact intermediate representation to learn 1d- or 2d- attention by a RNN-based encoder-decoder architecture. However, such methods suffer from attention-drift problem because high similarity among encoded features leads to attention confusion under the RNN-based local attention mechanism . Moreover, RNN-based methods have low efficiency due to poor parallelization . To overcome these problems, we propose the MASTER, a self-attention based scene text recognizer that (1) not only encodes the input-output attention but also learns self-attention which encodes feature-feature and target-target relationships inside the encoder and decoder and (2) learns a more powerful and robust intermediate representation to spatial distortion, and (3) owns a great training efficiency because of high training parallelization and a high-speed inference because of an efficient memory-cache mechanism. Extensive experiments on various benchmarks demonstrate the superior performance of our MASTER on both regular and irregular scene text. Pytorch code can be found at https://github.com/wenwenyu/MASTER-pytorch, and Tensorflow code can be found at https://github.com/jiangxiluning/MASTER-TF .},
  archive      = {J_PR},
  author       = {Ning Lu and Wenwen Yu and Xianbiao Qi and Yihao Chen and Ping Gong and Rong Xiao and Xiang Bai},
  doi          = {10.1016/j.patcog.2021.107980},
  journal      = {Pattern Recognition},
  pages        = {107980},
  shortjournal = {Pattern Recognition},
  title        = {MASTER: Multi-aspect non-local network for scene text recognition},
  volume       = {117},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Random vector functional link neural network based ensemble
deep learning. <em>PR</em>, <em>117</em>, 107978. (<a
href="https://doi.org/10.1016/j.patcog.2021.107978">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose deep learning frameworks based on the randomized neural network . Inspired by the principles of Random Vector Functional Link (RVFL) network, we present a deep RVFL network (dRVFL) with stacked layers. The parameters of the hidden layers of the dRVFL are randomly generated within a suitable range and kept fixed while the output weights are computed using the closed-form solution as in a standard RVFL network. We also propose an ensemble deep network (edRVFL) that can be regarded as a marriage of ensemble learning with deep learning . Unlike traditional ensembling approaches that require training several models independently from scratch, edRVFL is obtained by training a single dRVFL network once. Both dRVFL and edRVFL frameworks are generic and can be used with any RVFL variant. To illustrate this, we integrate the deep learning RVFL networks with a recently proposed sparse pre-trained RVFL (SP-RVFL). Experiments on 46 tabular UCI classification datasets and 12 sparse datasets demonstrate that the proposed deep RVFL networks outperform state-of-the-art deep feed-forward neural networks (FNNs).},
  archive      = {J_PR},
  author       = {Qiushi Shi and Rakesh Katuwal and P.N. Suganthan and M. Tanveer},
  doi          = {10.1016/j.patcog.2021.107978},
  journal      = {Pattern Recognition},
  pages        = {107978},
  shortjournal = {Pattern Recognition},
  title        = {Random vector functional link neural network based ensemble deep learning},
  volume       = {117},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). CDF transform-and-shift: An effective way to deal with
datasets of inhomogeneous cluster densities. <em>PR</em>, <em>117</em>,
107977. (<a href="https://doi.org/10.1016/j.patcog.2021.107977">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of inhomogeneous cluster densities has been a long-standing issue for distance-based and density-based algorithms in clustering and anomaly detection . These algorithms implicitly assume that all clusters have approximately the same density. As a result, they often exhibit a bias towards dense clusters in the presence of sparse clusters. Many remedies have been suggested; yet, we show that they are partial solutions which do not address the issue satisfactorily. To match the implicit assumption, we propose to transform a given dataset such that the transformed clusters have approximately the same density while all regions of locally low density become globally low density—homogenising cluster density while preserving the cluster structure of the dataset. We show that this can be achieved by using a new multi-dimensional Cumulative Distribution Function in a transform-and-shift method. The method can be applied to every dataset, before the dataset is used in many existing algorithms to match their implicit assumption without algorithmic modification. We show that the proposed method performs better than existing remedies.},
  archive      = {J_PR},
  author       = {Ye Zhu and Kai Ming Ting and Mark J. Carman and Maia Angelova},
  doi          = {10.1016/j.patcog.2021.107977},
  journal      = {Pattern Recognition},
  pages        = {107977},
  shortjournal = {Pattern Recognition},
  title        = {CDF transform-and-shift: An effective way to deal with datasets of inhomogeneous cluster densities},
  volume       = {117},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep center-based dual-constrained hashing for
discriminative face image retrieval. <em>PR</em>, <em>117</em>, 107976.
(<a href="https://doi.org/10.1016/j.patcog.2021.107976">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advantages of low storage cost and extremely fast retrieval speed, deep hashing methods have attracted much attention for image retrieval recently. However, large-scale face image retrieval with significant intra-class variations is still challenging. Neither existing pairwise/triplet labels-based nor softmax classification loss-based deep hashing works can generate compact and discriminative binary codes . Considering these issues, we propose a center-based framework integrating end-to-end hashing learning and class centers learning simultaneously. The framework minimizes the intra-class variance by clustering intra-class samples into a learnable class center. To strengthen inter-class separability, it additionally imposes a novel regularization term to enlarge the Hamming distance between pairwise class centers. Moreover, a simple yet effective regression matrix is introduced to encourage intra-class samples to generate the same binary codes, which further enhances the hashing codes compactness. Experiments on four large-scale datasets show the proposed method outperforms state-of-the-art baselines under various code lengths and commonly-used evaluation metrics .},
  archive      = {J_PR},
  author       = {Ming Zhang and Xuefei Zhe and Shifeng Chen and Hong Yan},
  doi          = {10.1016/j.patcog.2021.107976},
  journal      = {Pattern Recognition},
  pages        = {107976},
  shortjournal = {Pattern Recognition},
  title        = {Deep center-based dual-constrained hashing for discriminative face image retrieval},
  volume       = {117},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). GR-RNN: Global-context residual recurrent neural networks
for writer identification. <em>PR</em>, <em>117</em>, 107975. (<a
href="https://doi.org/10.1016/j.patcog.2021.107975">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents an end-to-end neural network system to identify writers through handwritten word images, which jointly integrates global-context information and a sequence of local fragment-based features. The global-context information is extracted from the tail of the neural network by a global average pooling step. The sequence of local and fragment-based features is extracted from a low-level deep feature map which contains subtle information about the handwriting style. The spatial relationship between the sequence of fragments is modeled by the recurrent neural network (RNN) to strengthen the discriminative ability of the local fragment features. We leverage the complementary information between the global-context and local fragments, resulting in the proposed global-context residual recurrent neural network (GR-RNN) method. The proposed method is evaluated on four public data sets and experimental results demonstrate that it can provide state-of-the-art performance. In addition, the neural networks trained on gray-scale images provide better results than neural networks trained on binarized and contour images, indicating that texture information plays an important role for writer identification. The source code is available: https://github.com/shengfly/writer-identification .},
  archive      = {J_PR},
  author       = {Sheng He and Lambert Schomaker},
  doi          = {10.1016/j.patcog.2021.107975},
  journal      = {Pattern Recognition},
  pages        = {107975},
  shortjournal = {Pattern Recognition},
  title        = {GR-RNN: Global-context residual recurrent neural networks for writer identification},
  volume       = {117},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Optic disc segmentation by u-net and probability bubble in
abnormal fundus images. <em>PR</em>, <em>117</em>, 107971. (<a
href="https://doi.org/10.1016/j.patcog.2021.107971">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Segmenting optic disc (OD) in abnormal fundus images is a challenge task because of many distractions such as illumination variations , blurry boundary, occlusion of retinal vessels and big bright lesions. Data-driven deep learning is effective and robust to illumination variations , blurry boundary and occlusion in the normal fundus images but sensitive to big bright lesions in abnormal images. In this paper, an automatic OD segmentation method fusing U-net with model-driven probability bubble approach is proposed in abnormal fundus images. The probability bubble is conceived according to the position relationship between retinal vessels and OD, and the localization result is fused into the output layer of U-net through calculating the joint probability. The proposed method takes the advantage of the deep learning architecture and improves the architecture’s performance by including the model-driven position constraint when lack of sufficient training data. Experiments show that the proposed method successfully removes the distraction of bright lesions in abnormal fundus images and obtains a satisfying OD segmentation on three public databases: Kaggle, MESSIDOR and NIVE, and it outperforms existing methods with a very high accuracy.},
  archive      = {J_PR},
  author       = {Yinghua Fu and Jie Chen and Jiang Li and Dongyan Pan and Xuezheng Yue and Yiming Zhu},
  doi          = {10.1016/j.patcog.2021.107971},
  journal      = {Pattern Recognition},
  pages        = {107971},
  shortjournal = {Pattern Recognition},
  title        = {Optic disc segmentation by U-net and probability bubble in abnormal fundus images},
  volume       = {117},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deformed contour segment matching for multi-source images.
<em>PR</em>, <em>117</em>, 107968. (<a
href="https://doi.org/10.1016/j.patcog.2021.107968">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robust and accurate multi-source matching is a difficult task due to significant nonlinear radiometric differences, background clutter, and geometric deformation in corresponding regions. Motivated by these existing problems, a discriminating yet robust combined descriptor for multi-source image matching, called deformed contour segment similarity (DCSS), is proposed in this work. First, the proposed DCSS , which is constructed by histogram of the combined contour features rather than the commonly used corner point and gradient, presents the accurate correspondence between image pairs and improves the descriptive ability to radiometric differences. Second, the deformed curve is presented via a finite-dimensional matrix Lie group to determine the similarity metric with an explicit geodesic solution. The geodesic distance, which indicates the nearest distance between curves in fluid space, is defined as the weight coefficient of the constructed histogram to enhance the robustness of the descriptor. The proposed algorithm utilizes the holistic contour information for the scoring and ranking of the shape similarity hypothesis, which can effectively reduce the influence of partially missing contours. Finally, a precise bilateral matching rule is used to perform the matching between the corresponding contour segments. Some experiments are carried out on various infrared-visible image data sets. The results demonstrate that the proposed DCSS achieves more robust and accurate matching performance than many popular multi-source image matching methods.},
  archive      = {J_PR},
  author       = {Quan Wu and Guili Xu and Yuehua Cheng and Zhengsheng Wang and Zhenhua Li},
  doi          = {10.1016/j.patcog.2021.107968},
  journal      = {Pattern Recognition},
  pages        = {107968},
  shortjournal = {Pattern Recognition},
  title        = {Deformed contour segment matching for multi-source images},
  volume       = {117},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Mixed-order spectral clustering for complex networks.
<em>PR</em>, <em>117</em>, 107964. (<a
href="https://doi.org/10.1016/j.patcog.2021.107964">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spectral clustering (SC) is a popular approach for gaining insights from complex networks. Conventional SC focuses on second-order structures (e.g. edges) without direct consideration of higher-order structures (e.g. triangles). This has motivated SC extensions that directly consider higher-order structures. However, both approaches are limited to considering a single order. To address this issue, this paper proposes a novel Mixed-Order Spectral Clustering (MOSC) framework to model both second-order and third-order structures simultaneously. To model mixed-order structures, we propose two new methods based on Graph Laplacian (GL) and Random Walks (RW). MOSC-GL combines edge and triangle adjacency matrices , with theoretical performance guarantee. MOSC-RW combines first-order and second-order random walks for a probabilistic interpretation. Moreover, we design mixed-order cut criteria to enable existing SC methods to preserve mixed-order structures, and develop new mixed-order evaluation metrics for structure-level evaluation. Experiments on community detection and superpixel segmentation show (1) the superior performance of the MOSC methods over existing SC methods, (2) enhanced performance of conventional SC due to mixed-order cut criteria, and (3) new insights of output clusters offered by the mixed-order evaluation metrics.},
  archive      = {J_PR},
  author       = {Yan Ge and Pan Peng and Haiping Lu},
  doi          = {10.1016/j.patcog.2021.107964},
  journal      = {Pattern Recognition},
  pages        = {107964},
  shortjournal = {Pattern Recognition},
  title        = {Mixed-order spectral clustering for complex networks},
  volume       = {117},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dual self-attention with co-attention networks for visual
question answering. <em>PR</em>, <em>117</em>, 107956. (<a
href="https://doi.org/10.1016/j.patcog.2021.107956">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual Question Answering (VQA) as an important task in understanding vision and language has been proposed and aroused wide interests. In previous VQA methods, Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN) are generally used to extract visual and textual features respectively, and then the correlation between these two features is explored to infer the answer. However, CNN mainly focuses on extracting local spatial information and RNN pays more attention on exploiting sequential architecture and long-range dependencies. It is difficult for them to integrate the local features with their global dependencies to learn more effective representations of the image and question. To address this problem, we propose a novel model, i.e., Dual Self-Attention with Co-Attention networks (DSACA), for VQA. It aims to model the internal dependencies of both the spatial and sequential structure respectively by using the newly proposed self-attention mechanism. Specifically, DSACA mainly contains three submodules. The visual self-attention module selectively aggregates the visual features at each region by a weighted sum of the features at all positions. The textual self-attention module automatically emphasizes the interdependent word features by integrating associated features among the sentence words. Besides, the visual-textual co-attention module explores the close correlation between visual and textual features learned from self-attention modules. The three modules are integrated into an end-to-end framework to infer the answer. Extensive experiments performed on three generally used VQA datasets confirm the favorable performance of DSACA compared with state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Yun Liu and Xiaoming Zhang and Qianyun Zhang and Chaozhuo Li and Feiran Huang and Xianghong Tang and Zhoujun Li},
  doi          = {10.1016/j.patcog.2021.107956},
  journal      = {Pattern Recognition},
  pages        = {107956},
  shortjournal = {Pattern Recognition},
  title        = {Dual self-attention with co-attention networks for visual question answering},
  volume       = {117},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learn to abstract via concept graph for weakly-supervised
few-shot learning. <em>PR</em>, <em>117</em>, 107946. (<a
href="https://doi.org/10.1016/j.patcog.2021.107946">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, a large number of meta-learning methods have been proposed to address few-shot learning problems and have shown superior performance. However, the explicit prior knowledge (e.g., concept graph) and weakly-supervised information are rarely explored in existing methods, which are usually free or cheap to collect. In this paper, we introduce a concept graph for the weakly-supervised few-shot learning, and propose a novel meta-learning framework, namely, MetaConcept. Our key idea is to learn a universal meta-learner inferring any-level classifier, so as to boost the classification performance of meta-learning on the novel classes. Specifically, we firstly propose a novel regularization with multi-level conceptual abstraction to train a universal meta-learner to infer not only an entity classifier but also a concept classifier at different levels via the concept graph (i.e., learn to abstract). Then, we propose a meta concept inference network as the universal meta-learner for the base learner, aiming to quickly adapt to a novel task by the joint inference of the abstract concepts and a few annotated samples. We have conducted extensive experiments on two weakly-supervised few-shot learning benchmarks, namely, WS-ImageNet-Pure and WS-ImageNet-Mix. Our experimental results show that (1) the proposed MetaConcept outperforms state-of-the-art methods with an improvement of 2\% to 6\% in classification accuracy ; (2) the proposed MetaConcept is able to yield a good performance though merely training with weakly-labeled datasets.},
  archive      = {J_PR},
  author       = {Baoquan Zhang and Ka-Cheong Leung and Xutao Li and Yunming Ye},
  doi          = {10.1016/j.patcog.2021.107946},
  journal      = {Pattern Recognition},
  pages        = {107946},
  shortjournal = {Pattern Recognition},
  title        = {Learn to abstract via concept graph for weakly-supervised few-shot learning},
  volume       = {117},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Boundarymix: Generating pseudo-training images for improving
segmentation with scribble annotations. <em>PR</em>, <em>117</em>,
107924. (<a href="https://doi.org/10.1016/j.patcog.2021.107924">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weakly-supervised semantic segmentation, as a promising solution to alleviate the burden of collecting per-pixel annotations, aims to train a segmentation model from partial weak annotations. Scribble on the object is one of the commonly used weak annotations and has shown to be sufficient for learning a decent segmentation model . Despite being effective, scribble-based weakly-supervised learning methods often lead to imprecise segmentation on object boundaries. This is mainly because the scribble annotations usually locate inside the objects and the dataset lacks annotations close to the semantic boundaries. To alleviate this issue, this paper proposes a simple-but-effective solution, i.e. , BoundaryMix, which generates pseudo training image-annotation pairs from the original images to supplement the missing semantic boundaries. Specifically, given a prediction of segmentation, we cut off the regions around the estimated boundaries, which are error-prone and replace them with the contents from another image, which in effect creates new samples with less ambiguity around semantic boundaries. With training on scribbles and the on-the-fly generated pseudo annotations, the network acquires better prediction capability around the boundary region and thus improves the overall segmentation performance . By conducting experiments on PASCAL VOC 2012 dataset and POTSDAM dataset with only scribble annotations, we demonstrate the excellent performance of the proposed method and the almost closed gap between scribble-supervised and fully-supervised image segmentation.},
  archive      = {J_PR},
  author       = {Wanxuan Lu and Dong Gong and Kun Fu and Xian Sun and Wenhui Diao and Lingqiao Liu},
  doi          = {10.1016/j.patcog.2021.107924},
  journal      = {Pattern Recognition},
  pages        = {107924},
  shortjournal = {Pattern Recognition},
  title        = {Boundarymix: Generating pseudo-training images for improving segmentation with scribble annotations},
  volume       = {117},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Retraction notice. <em>PR</em>, <em>116</em>, 108015. (<a
href="https://doi.org/10.1016/S0031-3203(21)00202-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_PR},
  doi          = {10.1016/S0031-3203(21)00202-8},
  journal      = {Pattern Recognition},
  pages        = {108015},
  shortjournal = {Pattern Recognition},
  title        = {Retraction notice},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Single annotated pixel based weakly supervised semantic
segmentation under driving scenes. <em>PR</em>, <em>116</em>, 107979.
(<a href="https://doi.org/10.1016/j.patcog.2021.107979">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic segmentation tasks based on weakly supervised conditions have been put forward to achieve a lightweight labeling process. For simple images that only include a few categories, research based on image-level annotations has achieved acceptable performance. However, when facing complex scenes, since image contains a large number of classes, it becomes challenging to learn visual appearance based on image tags. In this case, image-level annotations are not useful in providing information. Therefore, we set up a new task in which a single annotated pixel is provided for each category in a whole dataset. Based on the more lightweight and informative condition, a three step process is built for pseudo labels generation, which progressively implements each class’ optimal feature representation, image inference, and context-location based refinement. In particular, since high-level semantics and low-level imaging features have different discriminative abilities for each class under driving scenes, we divide categories into “object” or “scene” and then provide different operations for the two types separately. Further, an alternate iterative structure is established to gradually improve segmentation performance , which combines CNN-based inter-image common semantic learning and imaging prior based intra-image modification process. Experiments on the Cityscapes dataset demonstrate that the proposed method provides a feasible way to solve weakly supervised semantic segmentation tasks under complex driving scenes.},
  archive      = {J_PR},
  author       = {Xi Li and Huimin Ma and Sheng Yi and Yanxian Chen and Hongbing Ma},
  doi          = {10.1016/j.patcog.2021.107979},
  journal      = {Pattern Recognition},
  pages        = {107979},
  shortjournal = {Pattern Recognition},
  title        = {Single annotated pixel based weakly supervised semantic segmentation under driving scenes},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Improved multi-scale dynamic feature encoding network for
image demoiréing. <em>PR</em>, <em>116</em>, 107970. (<a
href="https://doi.org/10.1016/j.patcog.2021.107970">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The popularity of smartphones with digital cameras makes photographing using smartphones an important daily activity. Moiré patterns can easily appear when shooting objects with rich textures, such as computer screens, and will severely degrade the image quality. Image demoiréing is an important image restoration task that aims to remove moiré patterns and reveal the underlying clean image. Two key properties of moiré patterns—the widely distributed frequency spectrum and the dynamic nature of moiré textures—challenge the image demoiréing task. In this paper, we propose an improved Multi-scale convolutional network with Dynamic feature encoding for image DeMoiréing (MDDM+). We design two schemes in our network to respectively attack the broad frequency spectrum and the dynamic texture of moiré: a multi-scale structure to process images at different spatial resolutions and a dynamic feature encoding module to encode the texture dynamically. To capture more moiré and texture information from different frequencies, we further propose a novel L 1 L1 wavelet loss used to train our model. Extensive experiments on two benchmarks show that our proposed image demoiréing network can outperform the state of the arts in terms of fidelity as well as perception.},
  archive      = {J_PR},
  author       = {Xi Cheng and Zhenyong Fu and Jian Yang},
  doi          = {10.1016/j.patcog.2021.107970},
  journal      = {Pattern Recognition},
  pages        = {107970},
  shortjournal = {Pattern Recognition},
  title        = {Improved multi-scale dynamic feature encoding network for image demoiréing},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). NM-GAN: Noise-modulated generative adversarial network for
video anomaly detection. <em>PR</em>, <em>116</em>, 107969. (<a
href="https://doi.org/10.1016/j.patcog.2021.107969">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an important and challenging task for intelligent video surveillance systems , video anomaly detection is generally referred to as automatic recognition of video frames that contain abnormal targets, behavior or events. Although it has been widely applied in real scenes, anomaly detection remains a challenging task because of the vague definition of anomaly and the lack of the anomaly samples. Inspired by the widespread application of Generative Adversarial Network (GAN), we propose an end-to-end pipeline called NM-GAN which assembles an encode-decoder reconstruction network and a CNN-based discrimination network in a GAN-like architecture. The generalization ability of the reconstruction network is properly modulated via the adversarial learning around reconstruction error maps and noise maps. Meanwhile, the discrimination network is trained to distinguish anomaly samples from normal samples based on the reconstruction error maps. Finally, the output of the discrimination network is transferred to evaluate anomaly score of the input frame. The thorough proof-of-principle experiments and ablation tests on several popular datasets reveal that the proposed model enhance the generalization ability of the reconstruction network and the distinguishability of the discrimination network significantly. The comparison with the state-of-the-art shows that the proposed NM-GAN model outperforms most competing models in precision and stability.},
  archive      = {J_PR},
  author       = {Dongyue Chen and Lingyi Yue and Xingya Chang and Ming Xu and Tong Jia},
  doi          = {10.1016/j.patcog.2021.107969},
  journal      = {Pattern Recognition},
  pages        = {107969},
  shortjournal = {Pattern Recognition},
  title        = {NM-GAN: Noise-modulated generative adversarial network for video anomaly detection},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Nonlocal graph theory based transductive learning for
hyperspectral image classification. <em>PR</em>, <em>116</em>, 107967.
(<a href="https://doi.org/10.1016/j.patcog.2021.107967">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperspectral Image classification plays an important role in the maintenance of remote image analysis, which has been attracting a lot of research interest. Although various approaches, including unsupervised and supervised methods, have been proposed, obtaining a satisfactory classification result is still a challenge. In this paper, an efficient transductive learning method using variational nonlocal graph theory for hyperspectral image classification is proposed. First, the nonlocal vector neighborhood similarity is employed to build sparse graph representation . Then the variational segmentation framework is extended to label space, and the vectorization nonlocal energy function is constructed. Next, a fast comprehensive alternating minimization iteration algorithm is designed to implement labels transductive learning . At the same time, the labeled sample constraints are doubled ensured with simplex projection. Finally, experiments on six widely used hyperspectral image datasets are implemented, compared with other state-of-the-art classification methods, the classification results demonstrate that the proposed method has higher classification performance. Benefiting from graph theory and transductive idea, the proposed classification method can propagate labels and overcome the very high dimensionality and limited labeling problem to some extent.},
  archive      = {J_PR},
  author       = {Baoxiang Huang and Linyao Ge and Ge Chen and Milena Radenkovic and Xiaopeng Wang and Jinming Duan and Zhenkuan Pan},
  doi          = {10.1016/j.patcog.2021.107967},
  journal      = {Pattern Recognition},
  pages        = {107967},
  shortjournal = {Pattern Recognition},
  title        = {Nonlocal graph theory based transductive learning for hyperspectral image classification},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Star-based learning correlation clustering. <em>PR</em>,
<em>116</em>, 107966. (<a
href="https://doi.org/10.1016/j.patcog.2021.107966">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Correlation clustering (CC) is a clustering method using a signed graph as input without specifying the number of clusters a priori. It had been widely used in real applications, such as social network and text mining. However, its exact optimization or approximate algorithms often give unsatisfactory results, especially for large-scale signed graphs. This paper tackles this problem and proposes a novel CC algorithm , termed star-based learning correlation clustering (SL-CC). The proposed SL-CC contains two phases. The first is a scale reduction for signed graphs. We propose a special motif, called a star structure, for reducing the scale of signed graphs. We assign the vertices within a star structure to have the same cluster label and then merge these vertices as a new vertex in the graph so we can shrink a large-scale graph to a much small-scale one. The second is to give a learning schema for the local search on the reduced graphs. It can discover some important stars as seeds of clusters according to the graph structure, and then justify whether the other stars need to be merged with seeds or not. We also construct a new integer linear programing (ILP) model based on cycle inequalities to perform the local search with final clustering results. The experiments and comparisons of the proposed SL-CC with some existing CC methods on synthetic and real data sets with variant scale structures of signed graphs demonstrate the efficiency and usefulness of the SL-CC algorithm.},
  archive      = {J_PR},
  author       = {Jialin Hua and Jian Yu and Miin-Shen Yang},
  doi          = {10.1016/j.patcog.2021.107966},
  journal      = {Pattern Recognition},
  pages        = {107966},
  shortjournal = {Pattern Recognition},
  title        = {Star-based learning correlation clustering},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). PILS: Exploring high-order neighborhoods by pattern mining
and injection. <em>PR</em>, <em>116</em>, 107957. (<a
href="https://doi.org/10.1016/j.patcog.2021.107957">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce pattern injection local search (PILS), an optimization strategy that uses pattern mining to explore high-order local-search neighborhoods, and illustrate its application on the vehicle routing problem. PILS operates by storing a limited number of frequent patterns from elite solutions. During the local search, each pattern is used to define one move in which 1) incompatible edges are disconnected, 2) the edges defined by the pattern are reconnected, and 3) the remaining solution fragments are optimally reconnected. Each such move is accepted only in case of solution improvement. As visible in our experiments, this strategy results in a new paradigm of local search, which complements and enhances classical search approaches in a controllable amount of computational time. We demonstrate that PILS identifies useful high-order moves that would otherwise not be found by enumeration, and that it significantly improves the performance of state-of-the-art population-based and neighborhood-centered metaheuristics .},
  archive      = {J_PR},
  author       = {Florian Arnold and Ítalo Santana and Kenneth Sörensen and Thibaut Vidal},
  doi          = {10.1016/j.patcog.2021.107957},
  journal      = {Pattern Recognition},
  pages        = {107957},
  shortjournal = {Pattern Recognition},
  title        = {PILS: Exploring high-order neighborhoods by pattern mining and injection},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Recognition of visual-related non-driving activities using a
dual-camera monitoring system. <em>PR</em>, <em>116</em>, 107955. (<a
href="https://doi.org/10.1016/j.patcog.2021.107955">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For a Level 3 automated vehicle, according to the SAE International Automation Levels definition (J3016), the identification of non-driving activities (NDAs) that the driver is engaging with is of great importance in the design of an intelligent take-over interface. Much of the existing literature focuses on the driver take-over strategy with associated Human-Machine Interaction design. This paper proposes a dual-camera based framework to identify and track NDAs that require visual attention. This is achieved by mapping the driver&#39;s gaze using a nonlinear system identification approach, on the object scene, recognised by a deep learning algorithm. A novel gaze-based region of interest (ROI) selection module is introduced and contributes about a 30\% improvement in average success rate and about a 60\% reduction in average processing time compared to the results without this module. This framework has been successfully demonstrated to identify five types of NDA required visual attention with an average success rate of 86.18\%. The outcome of this research could be applicable to the identification of other NDAs and the tracking of NDAs within a certain time window could potentially be used to evaluate the driver&#39;s attention level for both automated and human-driving vehicles.},
  archive      = {J_PR},
  author       = {Lichao Yang and Kuo Dong and Yan Ding and James Brighton and Zhenfei Zhan and Yifan Zhao},
  doi          = {10.1016/j.patcog.2021.107955},
  journal      = {Pattern Recognition},
  pages        = {107955},
  shortjournal = {Pattern Recognition},
  title        = {Recognition of visual-related non-driving activities using a dual-camera monitoring system},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Temporally smooth online action detection using
cycle-consistent future anticipation. <em>PR</em>, <em>116</em>, 107954.
(<a href="https://doi.org/10.1016/j.patcog.2021.107954">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many video understanding tasks work in the offline setting by assuming that the input video is given from the start to the end. However, many real-world problems require the online setting, making a decision immediately using only the current and the past frames of videos such as in autonomous driving and surveillance systems. In this paper, we present a novel solution for online action detection by using a simple yet effective RNN-based networks called the Future Anticipation and Temporally Smoothing network (FATSnet). The proposed network consists of a module for anticipating the future that can be trained in an unsupervised manner with the cycle-consistency loss, and another component for aggregating the past and the future for temporally smooth frame-by-frame predictions. We also propose a solution to relieve the performance loss when running RNN-based models on very long sequences. Evaluations on TVSeries, THUMOS’14, and BBDB show that our method achieve the state-of-the-art performances compared to the previous works on online action detection.},
  archive      = {J_PR},
  author       = {Young Hwi Kim and Seonghyeon Nam and Seon Joo Kim},
  doi          = {10.1016/j.patcog.2021.107954},
  journal      = {Pattern Recognition},
  pages        = {107954},
  shortjournal = {Pattern Recognition},
  title        = {Temporally smooth online action detection using cycle-consistent future anticipation},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Ventral &amp; dorsal stream theory based zero-shot action
recognition. <em>PR</em>, <em>116</em>, 107953. (<a
href="https://doi.org/10.1016/j.patcog.2021.107953">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most Zero-Shot Action Recognition (ZSAR) methods establish visual-semantic joint embedding space, which is based on commonly used visual features and semantic embeddings , to learn the correlation between actions. Nevertheless, extracting visual features without structural guidance would lead to sparse video features, which reflect the correlation of actions, fall into oblivion. Based on the Ventral &amp; Dorsal Stream Theory (VD), we propose a VD-ZSAR method to extract irredundant visual feature, which can relieve relation ambiguity caused by redundant visual feature. And a visual-semantic joint embedding space is learned by combining nonredundant visual space with semantic space. Specifically, visual space is constructed by the motion cues perceived by Dorsal Stream, and the object cues perceived by Ventral Stream. Semantic space is constructed by sentence-to-vector generator. The visual-semantic joint embedding space is built by a nonlinear similarity metric learning mechanism, which can better implicitly reflect the correlation between actions. Extensive experiments on the Olympic, HDMB51 and UCF101 datasets validate the favorable performance of our proposed approach.},
  archive      = {J_PR},
  author       = {Meng Xing and Zhiyong Feng and Yong Su and Weilong Peng and Jianhai Zhang},
  doi          = {10.1016/j.patcog.2021.107953},
  journal      = {Pattern Recognition},
  pages        = {107953},
  shortjournal = {Pattern Recognition},
  title        = {Ventral &amp; dorsal stream theory based zero-shot action recognition},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Vector of locally and adaptively aggregated descriptors for
image feature representation. <em>PR</em>, <em>116</em>, 107952. (<a
href="https://doi.org/10.1016/j.patcog.2021.107952">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {VLAD (Vector of Locally Aggregated Descriptors) has been widely adopted in image representation. However, the VLAD algorithm seeks for the algebraic sum of the residue vectors between the descriptors and the centroid of cluster they belong to, and this could decrease the discriminative power of feature representations. To this end, this paper originally proposes a VLAAD (Vector of Locally and Adaptively Aggregated Descriptors) framework to adaptively assign a weight to each residue vector. First, we compute the weights using the magnitude of each residue vector, and encapsulate the weighted VLAD block into ResNet to form an end-to-end Weighted NetVLAD method. To further enhance the discriminative power of the features, we subsequently replace the magnitude-based weight computation with a gating scheme to achieve automatic weight estimation. The enhanced version is named as Gated NetVLAD method. The experimental results on CIFAR-10, MNIST Digits, Pittsburgh Google street view and ImageNet-Dog datasets demonstrate the promotion in classification accuracy and retrieval mAP using VLAAD against several state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Jian Zhang and Yunyin Cao and Qun Wu},
  doi          = {10.1016/j.patcog.2021.107952},
  journal      = {Pattern Recognition},
  pages        = {107952},
  shortjournal = {Pattern Recognition},
  title        = {Vector of locally and adaptively aggregated descriptors for image feature representation},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Unsupervised meta-learning for few-shot learning.
<em>PR</em>, <em>116</em>, 107951. (<a
href="https://doi.org/10.1016/j.patcog.2021.107951">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Meta-learning is an effective tool to address the few-shot learning problem, which requires new data to be classified considering only a few training examples. However, when used for classification, it requires large labeled datasets, which are not always available in practice. In this paper, we propose an unsupervised meta-learning algorithm that learns from an unlabeled dataset and adapts to downstream human-specific tasks with few labeled data. The proposed algorithm constructs tasks using clustering embedding methods and data augmentation functions to satisfy two critical class distinction requirements. To alleviate the biases and the weak diversity problem introduced by data augmentation functions, the proposed algorithm uses two methods, which are shifting the feeding data between the inner-outer loops and a novel data augmentation function. We further provide theoretical analysis of the effect of augmentation data in the inner/outer loop. Experiments on the MiniImagenet and Omniglot datasets demonstrate that the proposed unsupervised meta-learning approach outperforms other tested unsupervised representation learning approaches and two recent unsupervised meta-learning baselines. Compared with supervised meta-learning approaches, certain results produced by our method are quite close to those produced by such methods trained on the human-designed labeled tasks.},
  archive      = {J_PR},
  author       = {Hui Xu and Jiaxing Wang and Hao Li and Deqiang Ouyang and Jie Shao},
  doi          = {10.1016/j.patcog.2021.107951},
  journal      = {Pattern Recognition},
  pages        = {107951},
  shortjournal = {Pattern Recognition},
  title        = {Unsupervised meta-learning for few-shot learning},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). PRRNet: Pixel-region relation network for face forgery
detection. <em>PR</em>, <em>116</em>, 107950. (<a
href="https://doi.org/10.1016/j.patcog.2021.107950">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As advanced facial manipulation technologies develop rapidly, one can easily modify an image by changing the identity or the facial expression of the target person, which threatens social security. To address this problem, face forgery detection becomes an important and challenging task. In this paper, we propose a novel network, called Pixel-Region Relation Network (PRRNet), to capture pixel-wise and region-wise relations respectively for face forgery detection. The main motivation is that a facial manipulated image is composed of two parts from different sources, and the inconsistencies between the two parts is a significant kind of evidence for manipulation detection. Specifically, PRRNet contains two serial relation modules, i.e. the Pixel-Wise Relation (PR) module and the Region-Wise Relation (RR) module. For each pixel in the feature map, the PR module captures its similarities with other pixels to exploit the local relations information. Then, the PR module employs a spatial attention mechanism to represent the manipulated region and the original region separately. With the representations of the two regions, the RR module compares them with multiple metrics to measure the inconsistency between these two regions. In particular, the final predictions are obtained totally based on whether the inconsistencies exist. PRRNet achieves the state-of-the-art detection performance on three recent proposed face forgery detection datasets. Besides, our PRRNet shows the robustness when trained and tested on different image qualities.},
  archive      = {J_PR},
  author       = {Zhihua Shang and Hongtao Xie and Zhengjun Zha and Lingyun Yu and Yan Li and Yongdong Zhang},
  doi          = {10.1016/j.patcog.2021.107950},
  journal      = {Pattern Recognition},
  pages        = {107950},
  shortjournal = {Pattern Recognition},
  title        = {PRRNet: Pixel-region relation network for face forgery detection},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Region-based dropout with attention prior for weakly
supervised object localization. <em>PR</em>, <em>116</em>, 107949. (<a
href="https://doi.org/10.1016/j.patcog.2021.107949">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weakly supervised object localization (WSOL) methods utilize the internal feature responses of a classifier trained only on image-level labels. Classifiers tend to focus on the most discriminative part of the target object, instead of considering its full extent. Adversarial erasing (AE) techniques have been proposed to ameliorate this problem. These techniques erase the most discriminative part during training, thereby encouraging the classifiers to learn the less discriminative parts of the object. Despite the success of AE-based methods, we have observed that the hyperparameters fail to generalize across model architectures and datasets. Therefore, new sets of hyperparameters must be determined for each architecture and dataset. The selection of hyperparameters frequently requires strong supervision ( e.g., pixel-level annotations or human inspection). Because WSOL is premised on the assumption that such strong supervision is absent, the applicability of AE-based methods is limited. In this paper, we propose the region-based dropout with attention prior (RDAP) algorithm, which features hyperparameter transferability. We combined AE with regional dropout algorithms that provide greater stability against the selection of hyperparameters. We empirically confirmed that the RDAP method achieved state-of-the-art localization accuracy on four architectures, namely VGG-GAP, InceptionV3, ResNet-50 SE, and PreResNet-18, and two datasets, namely CUB-200-2011 and ImageNet-1k, with a single set of hyperparameters.},
  archive      = {J_PR},
  author       = {Junsuk Choe and Dongyoon Han and Sangdoo Yun and Jung-Woo Ha and Seong Joon Oh and Hyunjung Shim},
  doi          = {10.1016/j.patcog.2021.107949},
  journal      = {Pattern Recognition},
  pages        = {107949},
  shortjournal = {Pattern Recognition},
  title        = {Region-based dropout with attention prior for weakly supervised object localization},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). New confocal hyperbola-based ellipse fitting with
applications to estimating parameters of mechanical pipes from point
clouds. <em>PR</em>, <em>116</em>, 107948. (<a
href="https://doi.org/10.1016/j.patcog.2021.107948">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This manuscript presents a new method for fitting ellipses to two-dimensional data using the confocal hyperbola approximation to the geometric distance of points to ellipses. The proposed method was evaluated and compared to established methods on simulated and real-world datasets. First, it was revealed that the confocal hyperbola distance considerably outperforms other distance approximations such as algebraic and Sampson. Next, the proposed ellipse fitting method was compared with five reliable and established methods proposed by Halir, Taubin, Kanatani, Ahn and Szpak. The performance of each method as a function of rotation, aspect ratio, noise, and arc-length were examined. It was observed that the proposed ellipse fitting method achieved almost identical results (and in some cases better) than the gold standard geometric method of Ahn and outperformed the remaining methods in all simulation experiments. Finally, the proposed method outperformed the considered ellipse fitting methods in estimating the geometric parameters of cylindrical mechanical pipes from point clouds. The results of the experiments show that the confocal hyperbola is an excellent approximation to the true geometric distance and produces reliable and accurate ellipse fitting in practical settings.},
  archive      = {J_PR},
  author       = {Reza Maalek and Derek D. Lichti},
  doi          = {10.1016/j.patcog.2021.107948},
  journal      = {Pattern Recognition},
  pages        = {107948},
  shortjournal = {Pattern Recognition},
  title        = {New confocal hyperbola-based ellipse fitting with applications to estimating parameters of mechanical pipes from point clouds},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Attention-shift based deep neural network for fine-grained
visual categorization. <em>PR</em>, <em>116</em>, 107947. (<a
href="https://doi.org/10.1016/j.patcog.2021.107947">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fine-grained visual categorization (FGVC) has attracted extensive attention in recent years. The general pipeline of current FGVC techniques is to 1) locate the discriminative regions; 2) extract features from each region independently; and 3) feed the integrated features to a classifier. In this paper, we re-investigate the pipeline from the view of human visual recognition mechanisms. The perceiving of discriminative regions is a temporal processing by the human visual system (HVS) via the attention-shift mechanism. However, the existing independent feature extracting and one-pass feeding strategy ignore the inherent semantic relationships among discriminative regions, and thus is improper to model the attention-shift process properly. Therefore, in this paper, we propose a novel end-to-end FGVC network structure named Attention-Shift based Deep Neural Network (AS-DNN) to locate the discriminative regions automatically and encode the semantic correlations iteratively. AS-DNN consists of two channels: 1) the global perception channel C glb Cglb and 2) the attention-shift channel C sft , Csft, simulating the global perception and the attention-shift mechanism, respectively. Experimental results show that AS-DNN achieves state-of-the-art performances by outperforming both the CNN-based weakly or strongly-supervised FGVC algorithms on several widely-used fine-grained datasets, and the visualization of attention regions exhibit that the proposed method can locate the discriminative regions robustly in complex backgrounds and postures.},
  archive      = {J_PR},
  author       = {Yi Niu and Yang Jiao and Guangming Shi},
  doi          = {10.1016/j.patcog.2021.107947},
  journal      = {Pattern Recognition},
  pages        = {107947},
  shortjournal = {Pattern Recognition},
  title        = {Attention-shift based deep neural network for fine-grained visual categorization},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). 2D wasserstein loss for robust facial landmark detection.
<em>PR</em>, <em>116</em>, 107945. (<a
href="https://doi.org/10.1016/j.patcog.2021.107945">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recent performance of facial landmark detection has been significantly improved by using deep Convolutional Neural Networks (CNNs), especially the Heatmap Regression Models (HRMs). Although their performance on common benchmark datasets has reached a high level, the robustness of these models still remains a challenging problem in the practical use under noisy conditions of realistic environments. Contrary to most existing work focusing on the design of new models, we argue that improving the robustness requires rethinking many other aspects, including the use of datasets, the format of landmark annotation, the evaluation metric as well as the training and detection algorithm itself. In this paper, we propose a novel method for robust facial landmark detection, using a loss function based on the 2D Wasserstein distance combined with a new landmark coordinate sampling relying on the barycenter of the individual probability distributions. Our method can be plugged-and-play on most state-of-the-art HRMs with neither additional complexity nor structural modifications of the models. Further, with the large performance increase, we found that current evaluation metrics can no longer fully reflect the robustness of these models. Therefore, we propose several improvements to the standard evaluation protocol. Extensive experimental results on both traditional evaluation metrics and our evaluation metrics demonstrate that our approach significantly improves the robustness of state-of-the-art facial landmark detection models.},
  archive      = {J_PR},
  author       = {Yongzhe Yan and Stefan Duffner and Priyanka Phutane and Anthony Berthelier and Christophe Blanc and Christophe Garcia and Thierry Chateau},
  doi          = {10.1016/j.patcog.2021.107945},
  journal      = {Pattern Recognition},
  pages        = {107945},
  shortjournal = {Pattern Recognition},
  title        = {2D wasserstein loss for robust facial landmark detection},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Iterative sparse and deep learning for accurate diagnosis of
alzheimer’s disease. <em>PR</em>, <em>116</em>, 107944. (<a
href="https://doi.org/10.1016/j.patcog.2021.107944">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning techniques have been increasingly applied to the diagnosis of Alzheimer’s disease (AD) and the conversion from mild cognitive impairment (MCI) to AD. Despite their prevalence, existing methods usually suffer from using either irrelevant brain regions or less-accurate landmarks. In this paper, we propose the iterative sparse and deep learning (ISDL) model for joint deep feature extraction and critical cortical region identification to diagnose AD and MCI. We first design a deep feature extraction (DFE) module to capture the local-to-global structural information derived from 62 cortical regions. Then we design a sparse regression module to identify the critical cortical regions and integrate it into the DFE module to exclude irrelevant cortical regions from the diagnosis process. The parameters of the two modules are updated alternatively and iteratively in an end-to-end manner. Our experimental results suggest the ISDL model provides a state-of-the-art solution to both AD-CN classification and MCI-to-AD prediction.},
  archive      = {J_PR},
  author       = {Yuanyuan Chen and Yong Xia},
  doi          = {10.1016/j.patcog.2021.107944},
  journal      = {Pattern Recognition},
  pages        = {107944},
  shortjournal = {Pattern Recognition},
  title        = {Iterative sparse and deep learning for accurate diagnosis of alzheimer’s disease},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Discriminative feature alignment: Improving transferability
of unsupervised domain adaptation by gaussian-guided latent alignment.
<em>PR</em>, <em>116</em>, 107943. (<a
href="https://doi.org/10.1016/j.patcog.2021.107943">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we focus on the unsupervised domain adaptation problem where an approximate inference model is to be learned from a labeled data domain and expected to generalize well to an unlabeled domain. The success of unsupervised domain adaptation largely relies on the cross-domain feature alignment. Previous work has attempted to directly align features by classifier-induced discrepancies. Nevertheless, a common feature space cannot always be learned via this direct feature alignment especially when large domain gaps exist. To solve this problem, we introduce a Gaussian-guided latent alignment approach to align the latent feature distributions of the two domains under the guidance of a prior. In such an indirect way, the distributions over the samples from the two domains will be constructed on a common feature space, i.e., the space of the prior, which promotes better feature alignment. To effectively align the target latent distribution with this prior distribution, we also propose a novel unpaired L1-distance by taking advantage of the formulation of the encoder-decoder. The extensive evaluations on nine benchmark datasets validate the superior knowledge transferability through outperforming state-of-the-art methods and the versatility of the proposed method by improving the existing work significantly.},
  archive      = {J_PR},
  author       = {Jing Wang and Jiahong Chen and Jianzhe Lin and Leonid Sigal and Clarence W. de Silva},
  doi          = {10.1016/j.patcog.2021.107943},
  journal      = {Pattern Recognition},
  pages        = {107943},
  shortjournal = {Pattern Recognition},
  title        = {Discriminative feature alignment: Improving transferability of unsupervised domain adaptation by gaussian-guided latent alignment},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MIXCAPS: A capsule network-based mixture of experts for lung
nodule malignancy prediction. <em>PR</em>, <em>116</em>, 107942. (<a
href="https://doi.org/10.1016/j.patcog.2021.107942">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lung cancer is among the most common and deadliest cancers with a low 5-year survival rate. Timely diagnosis of lung cancer is, therefore, of paramount importance as it can save countless lives. In this regard, Computed Tomography (CT) scan is widely used for early detection of lung cancer, where human judgment is currently considered as the gold standard approach. Recently, there has been a surge of interest on development of automatic solutions via radiomics, as human-centered diagnosis is subject to inter-observer variability and is highly burdensome. Hand-crafted radiomics, serving as a radiologist assistant, requires fine annotations and pre-defined features. Deep learning radiomics solutions, however, have the promise of extracting the most useful features on their own in an end-to-end fashion without having access to the annotated boundaries. Among different deep learning models, Capsule Networks are proposed to overcome shortcomings of the Convolutional Neural Networks (CNNs) such as their inability to recognize detailed spatial relations . Capsule networks have so far shown satisfying performance in medical imaging problems. Capitalizing on their success, in this study, we propose a novel capsule network-based mixture of experts, referred to as the MIXCAPS. The proposed MIXCAPS architecture takes advantage of not only the capsule network’s capabilities to handle small datasets, but also automatically splitting dataset through a convolutional gating network. MIXCAPS enables capsule network experts to specialize on different subsets of the data. Our results show that MIXCAPS outperforms a single capsule network, a single CNN, a mixture of CNNs, and an ensemble of capsule networks, with an average accuracy of 90.7\% , 90.7\%, average sensitivity of 89.5\% , 89.5\%, average specificity of 93.4\% 93.4\% and average area under the curve of 0.956. Our experiments also show that there is a relation between the gate outputs and a couple of hand-crafted features, illustrating explainable nature of the proposed MIXCAPS. To further evaluate generalization capabilities of the proposed MIXCAPS architecture, additional experiments on a brain tumor dataset are performed showing potentials of MIXCAPS for detection of tumors related to other organs.},
  archive      = {J_PR},
  author       = {Parnian Afshar and Farnoosh Naderkhani and Anastasia Oikonomou and Moezedin Javad Rafiee and Arash Mohammadi and Konstantinos N. Plataniotis},
  doi          = {10.1016/j.patcog.2021.107942},
  journal      = {Pattern Recognition},
  pages        = {107942},
  shortjournal = {Pattern Recognition},
  title        = {MIXCAPS: A capsule network-based mixture of experts for lung nodule malignancy prediction},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A kernel path algorithm for general parametric quadratic
programming problem. <em>PR</em>, <em>116</em>, 107941. (<a
href="https://doi.org/10.1016/j.patcog.2021.107941">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is well known that the performance of a kernel method highly depends on the choice of kernel parameter . A kernel path provides a compact representation of all optimal solutions, which can be used to choose the optimal value of kernel parameter along with cross validation (CV) method. However, none of these existing kernel path algorithms provides a unified implementation to various learning problems. To fill this gap, in this paper, we first study a general parametric quadratic programming (PQP) problem that can be instantiated to an extensive number of learning problems. Then we provide a generalized kernel path (GKP) for the general PQP problem. Furthermore, we analyze the iteration complexity and computational complexity of GKP. Extensive experimental results on various benchmark datasets not only confirm the identity of GKP with several existing kernel path algorithms, but also show that our GKP is superior to the existing kernel path algorithms in terms of generalization and robustness.},
  archive      = {J_PR},
  author       = {Bin Gu and Ziran Xiong and Shuyang Yu and Guansheng Zheng},
  doi          = {10.1016/j.patcog.2021.107941},
  journal      = {Pattern Recognition},
  pages        = {107941},
  shortjournal = {Pattern Recognition},
  title        = {A kernel path algorithm for general parametric quadratic programming problem},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A novel unsupervised domain adaptation method for
inertia-trajectory translation of in-air handwriting. <em>PR</em>,
<em>116</em>, 107939. (<a
href="https://doi.org/10.1016/j.patcog.2021.107939">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a new method of human-computer interaction, inertial sensor-based in-air handwriting can provide natural and unconstrained interaction to express more complex and rich information in 3D space. However, most of the existing literature is mainly focused on in-air handwriting recognition (IAHR), which makes these works suffer from the poor readability of inertial signals and the lack of labeled samples. To address these two problems, we use an unsupervised domain adaptation method to recover the trajectory of inertial signals and generate inertial samples using handwritten trajectories. In this paper, we propose an Air-Writing Translator model to learn the bi-directional translation between trajectory domain and inertial domain in the absence of paired inertial and trajectory samples. Through latent-level adversarial learning and latent classification loss, the proposed model learns to extract domain-invariant features between the inertial signal and the trajectory while preserving semantic consistency during the translation across the two domains. In addition, the proposed framework can accept inputs of arbitrary length and translate between different sampling rates. Experiments on two public datasets, 6DMG (in-air handwriting dataset) and CT (handwritten trajectory dataset), are conducted and the results demonstrate that the proposed model can achieve reliable translation between inertial domain and trajectory domain. Empirically, our method also yields the best results in comparison to the state-of-the-art methods for IAHR.},
  archive      = {J_PR},
  author       = {Songbin Xu and Yang Xue and Xin Zhang and Lianwen Jin},
  doi          = {10.1016/j.patcog.2021.107939},
  journal      = {Pattern Recognition},
  pages        = {107939},
  shortjournal = {Pattern Recognition},
  title        = {A novel unsupervised domain adaptation method for inertia-trajectory translation of in-air handwriting},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning deep part-aware embedding for person retrieval.
<em>PR</em>, <em>116</em>, 107938. (<a
href="https://doi.org/10.1016/j.patcog.2021.107938">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person retrieval is an important vision task, aiming at matching the images of the same person under various camera views. The key challenge of person retrieval lies in the large intra-class variations among the person images. Therefore, how to learn discriminative feature representations becomes the core problem. In this paper, we propose a deep part-aware representation learning method for person retrieval. First, an improved triplet loss is introduced such that the global feature representations from the same identity are closely clustered. Meanwhile, a localization branch is proposed to automatically localize those discriminative person-wise parts or regions, only using identity labels in a weakly supervised manner. Via the learning simultaneously guided by the global branch and the localization branch, the proposed method can further improve the performance for person retrieval. Through an extensive set of ablation studies, we verify that the localization branch and the improved triplet loss each contributes to the performance boosts of the proposed method. Our model obtains superior (or comparable) performance compared to state-of-the-art methods for person retrieval on the four public person retrieval datasets. On the CUHK03-labeled dataset, for instance, the performance increases from 73.0\% mAP and 77.9\% rank-1 accuracy to 80.8\% (+7.8\%) mAP and 83.9\% (+6.0\%) rank-1 accuracy.},
  archive      = {J_PR},
  author       = {Yang Zhao and Chunhua Shen and Xiaohan Yu and Hao Chen and Yongsheng Gao and Shengwu Xiong},
  doi          = {10.1016/j.patcog.2021.107938},
  journal      = {Pattern Recognition},
  pages        = {107938},
  shortjournal = {Pattern Recognition},
  title        = {Learning deep part-aware embedding for person retrieval},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MEMF: Multi-level-attention embedding and
multi-layer-feature fusion model for person re-identification.
<em>PR</em>, <em>116</em>, 107937. (<a
href="https://doi.org/10.1016/j.patcog.2021.107937">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person re-identification (re-ID) methods need to extract representative, rich and discriminative features in order to deal with the effect of imperfect pedestrian detectors, illumination changes, occlusions, and background confusion. In this paper, a multi-level-attention embedding and multi-layer-feature fusion (MEMF) model is proposed for person re-ID. Specifically, a novel backbone network is designed, in which multi-level-attention blocks are embedded into a multi-layer-feature fusion architecture. Multi-level-attention blocks can highlight representative features and assist global feature expression, and multi-layer-feature fusion can increase the fine granularity of feature expression and obtain richer features. Besides, a new eigenvalue difference orthogonality (EDO) loss is designed to reduce the correlation between features. The final loss is defined as the combination of the cross-entropy loss and the EDO loss, which improves re-ID results. The proposed method is evaluated on four popular and challenging datasets. Detailed experiments demonstrate that the application of various elements of the MEMF model can help improve person re-ID performance. Compared with start-of-the-art methods, the MEMF model gets a promising result.},
  archive      = {J_PR},
  author       = {Jia Sun and Yanfeng Li and Houjin Chen and Bin Zhang and Jinlei Zhu},
  doi          = {10.1016/j.patcog.2021.107937},
  journal      = {Pattern Recognition},
  pages        = {107937},
  shortjournal = {Pattern Recognition},
  title        = {MEMF: Multi-level-attention embedding and multi-layer-feature fusion model for person re-identification},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A survey on heterogeneous network representation learning.
<em>PR</em>, <em>116</em>, 107936. (<a
href="https://doi.org/10.1016/j.patcog.2021.107936">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heterogeneous information networks usually contain different kinds of nodes and distinguishing types of relations, which can preserve more information than homogeneous information networks. Heterogeneous network representation learning attempts to learn a low-dimensional representation for each node and capture rich semantic information of the given network. Most of existing surveys focus on heterogeneous information network analysis and homogeneous information network representation learning. Although considerable research efforts concentrate on heterogeneous network representation learning, there are few surveys that systematically review the state-of-the-art heterogeneous network representation learning techniques. Motivated by this, we propose a taxonomy of heterogeneous network representation learning algorithms according to different approaches of capturing semantic information in heterogeneous networks, including path based algorithms and semantic unit based algorithms. Moreover, we introduce the typical heterogeneous network representation learning techniques in detail and make a comparative analysis of these techniques. In addition, the research challenges in terms of semantics preserving, data sparsity and scalability are discussed. To tackle these challenges, several potential future research directions for heterogeneous network representation learning are pointed out, including semantic relations extraction, dynamic heterogeneous networks, very large heterogeneous networks and heterogeneous networks construction.},
  archive      = {J_PR},
  author       = {Yu Xie and Bin Yu and Shengze Lv and Chen Zhang and Guodong Wang and Maoguo Gong},
  doi          = {10.1016/j.patcog.2021.107936},
  journal      = {Pattern Recognition},
  pages        = {107936},
  shortjournal = {Pattern Recognition},
  title        = {A survey on heterogeneous network representation learning},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Local descriptor-based multi-prototype network for few-shot
learning. <em>PR</em>, <em>116</em>, 107935. (<a
href="https://doi.org/10.1016/j.patcog.2021.107935">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prototype-based few-shot learning methods are promising in that they are simple yet effective to handle any-shot problems, and many prototype associated works are raised since then. However, these traditional prototype-based methods generally use only one single prototype to represent a class, which essentially cannot effectively estimate the complicated distribution of a class. To tackle this problem, we propose a novel Local descriptor-based Multi-Prototype Network (LMPNet) in this paper, a well-designed framework that generates an embedding space with multiple prototypes. Specifically, the proposed LMPNet employs local descriptors to represent each image, which can capture more informative and subtler cues of an image than the normally adopted image-level features. Moreover, to alleviate the uncertainty introduced by the fixed construction (averaging over samples) of prototypes, we introduce a channel squeeze and spatial excitation (sSE) attention module to learn multiple local descriptor-based prototypes for each class through end-to-end learning. Extensive experiments on both few-shot and fine-grained few-shot image classification tasks have been conducted on various benchmark datasets, including mini ImageNet, tiered ImageNet, Stanford Dogs, Stanford Cars, and CUB-200-2010. The experimental results of our LMPNet on above datasets show tangibly learning performance improvements and distinguishable outcomes over the baseline models .},
  archive      = {J_PR},
  author       = {Hongwei Huang and Zhangkai Wu and Wenbin Li and Jing Huo and Yang Gao},
  doi          = {10.1016/j.patcog.2021.107935},
  journal      = {Pattern Recognition},
  pages        = {107935},
  shortjournal = {Pattern Recognition},
  title        = {Local descriptor-based multi-prototype network for few-shot learning},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A parameterized geometric fitting method for ellipse.
<em>PR</em>, <em>116</em>, 107934. (<a
href="https://doi.org/10.1016/j.patcog.2021.107934">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study presents an accurate and robust method for fitting noisy and occlusion elliptic data. The nonlinear issue of ellipse fitting is interpreted as a set of Levenberg–Marquardt iterations (LMIs) by minimizing the geometric distance . For each iteration of the geometric fitting, the representations of the parameterized ellipses are mapped to the geometric error distances, which are implemented latently by an orthogonal angle segmentation of the ellipses. Moreover, dimension reduction is utilized by the LMIs to avoid misconvergence and expensive computations. The method based on two recent representative algorithms is verified by simulation and real-world experiments. The results suggest that the saliency capability of the new method to fit ellipse-specific profiles with severe noise and occlusion, which is better than or equal to those of the reference approaches, has potential applications in quality monitoring, three-dimensional reconstruction, and instrument calibration .},
  archive      = {J_PR},
  author       = {Tao Wang and Zhaoyao Shi and Bo Yu},
  doi          = {10.1016/j.patcog.2021.107934},
  journal      = {Pattern Recognition},
  pages        = {107934},
  shortjournal = {Pattern Recognition},
  title        = {A parameterized geometric fitting method for ellipse},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A two-stage hybrid ant colony optimization for
high-dimensional feature selection. <em>PR</em>, <em>116</em>, 107933.
(<a href="https://doi.org/10.1016/j.patcog.2021.107933">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ant colony optimization (ACO) is widely used in feature selection owing to its excellent global/local search capabilities and flexible graph representation . However, the current ACO-based feature selection methods are mainly applied to low-dimensional datasets. For thousands of dimensional datasets, the search for the optimal feature subset (OFS) becomes extremely difficult due to the exponential increase of the search space. In this paper, we propose a two-stage hybrid ACO for high-dimensional feature selection (TSHFS-ACO). As an additional stage, it uses the interval strategy to determine the size of OFS for the following OFS search. Compared to the traditional one-stage methods that determine the size of OFS and search for OFS simultaneously, the stage of checking the performance of partial feature number endpoints in advance helps to reduce the complexity of the algorithm and alleviate the algorithm from getting into a local optimum. Moreover, the advanced ACO algorithm embeds the hybrid model, which uses the features’ inherent relevance attributes and the classification performance to guide OFS search. The test results on eleven high-dimensional public datasets show that TSHFS-ACO is suitable for high-dimensional feature selection. The obtained OFS has state-of-the-art performance on most datasets. And compared with other ACO-based feature selection methods, TSHFS-ACO has a shorter running time.},
  archive      = {J_PR},
  author       = {Wenping Ma and Xiaobo Zhou and Hao Zhu and Longwei Li and Licheng Jiao},
  doi          = {10.1016/j.patcog.2021.107933},
  journal      = {Pattern Recognition},
  pages        = {107933},
  shortjournal = {Pattern Recognition},
  title        = {A two-stage hybrid ant colony optimization for high-dimensional feature selection},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptive, hybrid feature selection (AHFS). <em>PR</em>,
<em>116</em>, 107932. (<a
href="https://doi.org/10.1016/j.patcog.2021.107932">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper deals with the problem of integrating the most suitable feature selection methods for a given problem in order to achieve the best feature order. A new, adaptive and hybrid feature selection approach is proposed, which combines and utilizes multiple individual methods in order to achieve a more generalized solution. Various state-of-the-art feature selection methods are presented in detail with examples of their applications and an exhaustive evaluation is conducted to measure and compare the their performance with the proposed approach. Results prove that while the individual feature selection methods may perform with high variety on the test cases, the combined algorithm steadily provides noticeably better solution.},
  archive      = {J_PR},
  author       = {Zsolt János Viharos and Krisztián Balázs Kis and Ádám Fodor and Máté István Büki},
  doi          = {10.1016/j.patcog.2021.107932},
  journal      = {Pattern Recognition},
  pages        = {107932},
  shortjournal = {Pattern Recognition},
  title        = {Adaptive, hybrid feature selection (AHFS)},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Single-image super-resolution - when model adaptation
matters. <em>PR</em>, <em>116</em>, 107931. (<a
href="https://doi.org/10.1016/j.patcog.2021.107931">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, impressive advances have been made in single-image super-resolution. Deep learning is behind much of this success. Deep(er) architecture design and external prior modeling are the key ingredients. The internal contents of the low-resolution input image are neglected with deep modeling, despite earlier works that show the power of using such internal priors. In this paper, we propose a variation of deep residual convolutional neural networks , which has been carefully designed for robustness and efficiency in both learning and testing. Moreover, we propose multiple strategies for model adaptation to the internal contents of the low-resolution input image and analyze their strong points and weaknesses. By trading runtime and using internal priors, we achieve improvements from 0.1 to 0.3 dB PSNR over the reported results on standard datasets. Our adaptation especially favors images with repetitive structures or high resolutions. It indicates a more practical usage when our adaption approach applies to sequences or videos in which adjacent frames are strongly correlated in their contents. Moreover, the approach can be combined with other simple techniques, such as back-projection and enhanced prediction, to realize further improvements.},
  archive      = {J_PR},
  author       = {Yudong Liang and Radu Timofte and Jinjun Wang and Sanping Zhou and Yihong Gong and Nanning Zheng},
  doi          = {10.1016/j.patcog.2021.107931},
  journal      = {Pattern Recognition},
  pages        = {107931},
  shortjournal = {Pattern Recognition},
  title        = {Single-image super-resolution - when model adaptation matters},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Compensating over- and underexposure in optical target pose
determination. <em>PR</em>, <em>116</em>, 107930. (<a
href="https://doi.org/10.1016/j.patcog.2021.107930">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optical coded targets allow to determine the relative pose of a camera, on a metric scale, from one image only. Furthermore, they are easily and efficiently detected, opening to a wide range of applications in robotics and computer vision . In this work we describe the effect of pixel saturation and non-ideal lens Point Spread Function , causing the apparent position of the corners and the edges of the target to change as a function of the camera exposure time. This effect, which we call exposure bias, is frequent in over- or underexposed images and introduces a systematic error in the estimated camera pose. We propose an algorithm that is able to estimate and correct for the exposure bias exploiting specific geometric features of a common target design based on concentric circles . Through rigorous laboratory experiments carried out in a highly controlled environment, we demonstrate that the proposed algorithm is seven times more precise and three times more accurate in the target distance estimation than the algorithms available in the literature.},
  archive      = {J_PR},
  author       = {E. Cledat and M. Rufener and D.A. Cucci},
  doi          = {10.1016/j.patcog.2021.107930},
  journal      = {Pattern Recognition},
  pages        = {107930},
  shortjournal = {Pattern Recognition},
  title        = {Compensating over- and underexposure in optical target pose determination},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). STDnet-ST: Spatio-temporal ConvNet for small object
detection. <em>PR</em>, <em>116</em>, 107929. (<a
href="https://doi.org/10.1016/j.patcog.2021.107929">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object detection through convolutional neural networks is reaching unprecedented levels of precision. However, a detailed analysis of the results shows that the accuracy in the detection of small objects is still far from being satisfactory. A recent trend that will likely improve the overall object detection success is to use the spatial information operating alongside temporal video information. This paper introduces STDnet-ST, an end-to-end spatio-temporal convolutional neural network for small object detection in video. We define small as those objects under 16 × 16 16×16 px, where the features become less distinctive. STDnet-ST is an architecture that detects small objects over time and correlates pairs of the top-ranked regions with the highest likelihood of containing those small objects. This permits to link the small objects across the time as tubelets. Furthermore, we propose a procedure to dismiss unprofitable object links in order to provide high quality tubelets, increasing the accuracy. STDnet-ST is evaluated on the publicly accessible USC-GRAD-STDdb, UAVDT and VisDrone2019-VID video datasets, where it achieves state-of-the-art results for small objects.},
  archive      = {J_PR},
  author       = {Brais Bosquet and Manuel Mucientes and Víctor M. Brea},
  doi          = {10.1016/j.patcog.2021.107929},
  journal      = {Pattern Recognition},
  pages        = {107929},
  shortjournal = {Pattern Recognition},
  title        = {STDnet-ST: Spatio-temporal ConvNet for small object detection},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep-seated features histogram: A novel image retrieval
method. <em>PR</em>, <em>116</em>, 107926. (<a
href="https://doi.org/10.1016/j.patcog.2021.107926">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-level features and deep features each have their own advantages and disadvantages in image representation. However, combining their advantages within a CBIR framework remains challenging. To address this problem, we propose a novel image-retrieval method: the deep-seated features histogram (DSFH). Its main highlights are: 1) Low-level features are extracted by simulating the human orientation selection and color perception mechanisms. This follows the human habit of looking at conspicuous regions and then less-conspicuous ones. 2) A novel method, ranking whitening , is proposed for extracting deep features via low-level features and combining them to obtain deep-seated features. 3) The proposed method is straightforward and reduces the vector dimensionality of the FC7 layer of a pre-trained VGG-16 network, and significantly improves image-retrieval precision. Comparative experiments demonstrate that the proposed method outperforms several state-of-the-art methods, including low-level feature-based, deep feature-based, and fused feature-based methods, in terms of precision/recall, area under the precision/recall curve metrics, and mean average precision. The proposed method provides efficient CBIR performance and not only has the power to discriminate low-level features, including color, texture, and shape, but can also match scenes of similar style.},
  archive      = {J_PR},
  author       = {Guang-Hai Liu and Jing-Yu Yang},
  doi          = {10.1016/j.patcog.2021.107926},
  journal      = {Pattern Recognition},
  pages        = {107926},
  shortjournal = {Pattern Recognition},
  title        = {Deep-seated features histogram: A novel image retrieval method},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Global motion estimation with iterative optimization-based
independent univariate model for action recognition. <em>PR</em>,
<em>116</em>, 107925. (<a
href="https://doi.org/10.1016/j.patcog.2021.107925">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motion information used in the existed video action recognition schemes is mixing of global motion(GM) and local motion(LM). In fact, GM &amp; LM have their respective semantic concepts . Thus, it is promising to decouple GM and LM from the mixed motions. Numerous efforts have been made on the design of global motion models for video encoding , video dejittering, video denoising , and so on. Nevertheless, some of the models are too basic to cover the camera motions in action recognition while others are over-complicated. In this paper, we focus on the characteristic of the action recognition and propose a novel independent univariate GM model. It ignores camera rotation, which appears rarely in action recognition videos, and represents the GM in x and y direction respectively. Furthermore, GM is position invariant because it is from the universal camera motion. Pixels with global motions are subjected to the same parametric model and pixels with mixed motion can be seen as outliers. Motivated by this, we develop an iterative optimization scheme for GM estimation which removes the outlier points step by step and estimates global motions in a coarse-to-fine manner. Finally, the LM is estimated through a Spatio-temporal threshold-based method. Experimental results demonstrate that the proposed GM model makes a better trade-off between the model complexity and the robustness. And the iterative optimization scheme is more effective than the existed algorithms. The compared experiments using four popular action recognition models on UCF-101 (for action recognition) and NCAA (for group activity recognition) demonstrate that local motions are more effective than the mixed motions.},
  archive      = {J_PR},
  author       = {Lifang Wu and Zhou Yang and Meng Jian and Jialie Shen and Yuchen Yang and Xianglong Lang},
  doi          = {10.1016/j.patcog.2021.107925},
  journal      = {Pattern Recognition},
  pages        = {107925},
  shortjournal = {Pattern Recognition},
  title        = {Global motion estimation with iterative optimization-based independent univariate model for action recognition},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SNAP: Shaping neural architectures progressively via
information density criterion. <em>PR</em>, <em>116</em>, 107923. (<a
href="https://doi.org/10.1016/j.patcog.2021.107923">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Excellent neural network architecture is built on the specific target task and device. As the target task or device is different, the neural architecture we need will be different, too. Rather than redesigning or searching a brand new one, adjusting the existing architecture automatically is an alternative yet efficient way. To this end, we propose a method to Shape the existing Neural Architectures Progressively (SNAP) to adapt the target task and device better. Inspired by the streamline of water drop shaped by air resistance , we define an information density criterion (play the role of resistance) to drive the network architecture reducing the size of the part with the lowest information density. Iteratively, a more adaptive architecture will be obtained progressively in a greedy way. Theoretically, we prove that the greedy strategy is reasonable and can shape a better architecture. Because of the small adjustment of architecture each time, new architecture can inherit the parameters in old architecture to avoid retraining it from scratch. So the proposed method is very efficient in no need of high computation cost. Experimental results show that proposed method can effectively improve the given network by adjusting its architecture. And it can generate different architectures for different tasks and devices to adapt them well. Compared with search-based auto-generated neural architectures, our approach can achieve comparable or even better performance in no need of tremendous computation resources.},
  archive      = {J_PR},
  author       = {Zhiqiang Chen and Ting-Bing Xu and Weijian Liao and Zhengcheng Li and Jinpeng Li and Cheng-Lin Liu and Huiguang He},
  doi          = {10.1016/j.patcog.2021.107923},
  journal      = {Pattern Recognition},
  pages        = {107923},
  shortjournal = {Pattern Recognition},
  title        = {SNAP: Shaping neural architectures progressively via information density criterion},
  volume       = {116},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). GPNet: Gated pyramid network for semantic segmentation.
<em>PR</em>, <em>115</em>, 107940. (<a
href="https://doi.org/10.1016/j.patcog.2021.107940">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic segmentation is a challenging task which requires both solid unanimous global context and rich spatial information. Recent methods ignore adaptively capturing of valid feature. The lack of useful multi-scale information filtering hinders further explicit feature generation. In this paper, we develop a novel network named GPNet, which can densely capture and filter the multi-scale information in a gated and pair-wise manner. Specifically, a Gated Pyramid Module (GPM) is designed to incorporate dense and growing receptive fields from both low-level and high-level features. In GPM we build a gated path to select useful context among multi-scale information. Moreover, a Cross-Layer Attention Module (CLAM) is proposed to reuse the context information from shallow layers to guide the deep features. Comprehensive experimental evaluations are conducted on popular semantic segmentation benchmarks including Cityscapes and ADE20K. Our GPNet achieves the mIoU score of 82.5\% and 45.81\% on Cityscapes test set and ADE20K validation set, respectively, which are the new state-of-the-art results using ResNet-101 as the backbone.},
  archive      = {J_PR},
  author       = {Yu Zhang and Xin Sun and Junyu Dong and Changrui Chen and Qingxuan Lv},
  doi          = {10.1016/j.patcog.2021.107940},
  journal      = {Pattern Recognition},
  pages        = {107940},
  shortjournal = {Pattern Recognition},
  title        = {GPNet: Gated pyramid network for semantic segmentation},
  volume       = {115},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Divergent-convergent attention for image captioning.
<em>PR</em>, <em>115</em>, 107928. (<a
href="https://doi.org/10.1016/j.patcog.2021.107928">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attention mechanism has made great progress in image captioning, where semantic words or local regions are selectively embedded into the language model . However, current attention-based image captioning methods ignore the fine-grained semantic information and their interaction with visual regions. Inspired by the activity of human in describing an image: divergent observation and convergent attention, we propose a novel divergent-convergent attention (DCA) model to tackle the problems of the current attention model in image captioning. In our DCA model, divergent observation is mainly reflected in the multi-perspective inputs: a visual collection coming from object detection and three semantic components of scene graph made of objects, attributes and relations respectively. Then the convergent attention merges these multi-perspective inputs by adaptively deciding which perspective is crucial and which element in the focused perspective dominates in the attention process through a hierarchical structure. Our model also makes use of the interaction between visual objects and semantic components to achieve complementary advantages. Above all, owing to the interaction between divergent visual and semantic components, and the gradual convergence of attention, our model can attend to the corresponding local region more precisely under the guidance of semantic components. Besides, with the assistance of the visual components, the DCA model can effectively utilize the fine-grained semantic components to generate more descriptive sentences. Experiments on the MS COCO dataset demonstrate the superiority of our proposed method.},
  archive      = {J_PR},
  author       = {Junzhong Ji and Zhuoran Du and Xiaodan Zhang},
  doi          = {10.1016/j.patcog.2021.107928},
  journal      = {Pattern Recognition},
  pages        = {107928},
  shortjournal = {Pattern Recognition},
  title        = {Divergent-convergent attention for image captioning},
  volume       = {115},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Joint feature extraction and classification in a unified
framework for cost-sensitive face recognition. <em>PR</em>,
<em>115</em>, 107927. (<a
href="https://doi.org/10.1016/j.patcog.2021.107927">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cost-sensitive face recognition is a challenging problem in pattern recognition. Due to the high-dimensional face features, cost-sensitive face recognition usually conducts feature extraction in advance, followed by the learning of classifier in reduced subspace. However, the pre-extracted face features are kept fixed and may suboptimal for subsequent classifier learning, which will degrade the final face recognition performance. Besides, most of face learners are cost in sensitive. Even the cost-sensitive methods proposed for face recognition, they only incorporate the cost information in feature extraction or classification phase as an alternative. There is no doubt that some cost-sensitive information will be lost in their cost insensitive steps. To deal with these issues, this paper proposes to incorporate feature extraction and classification in a unified cost-sensitive framework for face recognition. The experimental results on three public face benchmarks, including Extended Yale B, CMU PIE and LFW datasets, demonstrate that the proposed method can significantly reduce the overall misclassification loss of face recognition system as well as the classification errors associated with high costs, when comparing with eleven state-of-the-art face learners and nine cost-sensitive methods.},
  archive      = {J_PR},
  author       = {Jianwu Wan and Yinjuan Chen and Bing Bai},
  doi          = {10.1016/j.patcog.2021.107927},
  journal      = {Pattern Recognition},
  pages        = {107927},
  shortjournal = {Pattern Recognition},
  title        = {Joint feature extraction and classification in a unified framework for cost-sensitive face recognition},
  volume       = {115},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Aberrance suppressed spatio-temporal correlation filters for
visual object tracking. <em>PR</em>, <em>115</em>, 107922. (<a
href="https://doi.org/10.1016/j.patcog.2021.107922">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The objective of the present study is to design a correlation filter-based tracking method for robust visual object tracking. In the literature, numerous tracking methods have been proposed based on discriminative correlation filter (DCF) and obtained impressive performance. However, existing algorithms still face difficulties such as partial occlusion , clutter background, uncertainties, boundary effects (especially when the target search area is small) and other challenging visual factors. Furthermore, during the target detection process, the sudden changes in objects caused by illumination variations and partial/full occlusion degrade the performance. To tackle the drawbacks mentioned earlier, we propose a tracking algorithm concerning the aberrance suppressed correlation filters with spatio-temporal information for visual tracking. Specifically, we introduce a spatial regularization term into the correlation filter to suppresses the boundary effects. Following that, a temporal regularization is adopted into the DCF-based framework to achieve a more robust appearance model and further enhance the tracking performance. In addition, we introduce an approach to suppress the aberrance in response maps caused by the sudden changes. Technically, our proposed method can be directly solved by using the alternating direction method of multipliers (ADMM) technique with a low computational cost. Finally, extensive experimental results on OTB2013, OTB2015, TempleColor128 and UAV123 datasets demonstrate that the proposed method performs favorably against state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Dinesh Elayaperumal and Young Hoon Joo},
  doi          = {10.1016/j.patcog.2021.107922},
  journal      = {Pattern Recognition},
  pages        = {107922},
  shortjournal = {Pattern Recognition},
  title        = {Aberrance suppressed spatio-temporal correlation filters for visual object tracking},
  volume       = {115},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Tripool: Graph triplet pooling for 3D skeleton-based action
recognition. <em>PR</em>, <em>115</em>, 107921. (<a
href="https://doi.org/10.1016/j.patcog.2021.107921">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Convolutional Network (GCN) has already been successfully applied to skeleton-based action recognition. However, current GCNs in this task are lack of pooling operations such that the architectures are inherently flat, which not only increases the computational complexity but also requires larger memory space to keep the entire graph embedding. More seriously, a flat architecture forces the high-level semantic feature representations to have the same physical structure of the low-level input skeletons, which we argue is unreasonable and harmful for the final performance. To address these issues, we propose Tripool, a novel graph pooling method for 3D action recognition from skeleton data. Tripool provides to optimize a triplet pooling loss, in which both graph topology and global graph context are taken into consideration, to learn a hierarchical graph representation . The training process of graph pooling is efficient since it optimizes the graph topology by minimizing an upper bound of the pooling loss. Besides, Tripool also automatically generates an embedding matrix since the graph is changed after pooling. On one hand, Tripool reduces the computational cost by removing the redundant nodes. On the other hand it overcomes the limitation of the topology constrain for the high-level semantic representations , thus improves the final performance. Tripool can be combined with various graph neural networks in an end-to-end fashion. Comprehensive experiments on two current largest scale 3D datasets are conducted to evaluate our method. With our Tripool, we consistently get the best results in terms of various performance measures .},
  archive      = {J_PR},
  author       = {Wei Peng and Xiaopeng Hong and Guoying Zhao},
  doi          = {10.1016/j.patcog.2021.107921},
  journal      = {Pattern Recognition},
  pages        = {107921},
  shortjournal = {Pattern Recognition},
  title        = {Tripool: Graph triplet pooling for 3D skeleton-based action recognition},
  volume       = {115},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dyadic relational graph convolutional networks for
skeleton-based human interaction recognition. <em>PR</em>, <em>115</em>,
107920. (<a href="https://doi.org/10.1016/j.patcog.2021.107920">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Skeleton-based human interaction recognition is a challenging task requiring all abilities to recognize spatial, temporal, and interactive features. These abilities rarely co-exist in existing methods. Graph convolutional network (GCN) based methods fail to extract interactive features. Traditional interaction recognition methods cannot effectively capture spatial features from skeletons. Toward this end, we propose a novel Dyadic Relational Graph Convolutional Network (DR-GCN) for interaction recognition. Specifically, we make four contributions: (i) we design a Relational Adjacency Matrix (RAM) that represents dynamic relational graphs. These graphs are constructed combining both geometric features and relative attention from the two skeleton sequences; (ii) we propose a Dyadic Relational Graph Convolution Block (DR-GCB) that extracts spatial-temporal interactive features; (iii) we stack the proposed DR-GCBs to build DR-GCN and integrate our methods with an advanced model. (iv) Our models achieve state-of-the-art results on SBU and significant improvements on the mutual action sub-datasets of NTU-RGB+D and NTU-RGB+D 120.},
  archive      = {J_PR},
  author       = {Liping Zhu and Bohua Wan and Chengyang Li and Gangyi Tian and Yi Hou and Kun Yuan},
  doi          = {10.1016/j.patcog.2021.107920},
  journal      = {Pattern Recognition},
  pages        = {107920},
  shortjournal = {Pattern Recognition},
  title        = {Dyadic relational graph convolutional networks for skeleton-based human interaction recognition},
  volume       = {115},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multivariate time series clustering based on complex
network. <em>PR</em>, <em>115</em>, 107919. (<a
href="https://doi.org/10.1016/j.patcog.2021.107919">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years have seen an increase in research on time series data mining (especially time-series clustering) owing to the widespread existence of time series in various fields. Techniques such as clustering can extract valuable information and potential patterns from time-series data. In this regard, the clustering analysis of multivariate time series is challenging because of the high dimensionality . Our study led us to develop a novel method based on complex networks for multivariate time series clustering (BCNC). BCNC includes a new method for mapping multivariate time series into complex networks and a new method to visualize multivariate time series. The solution is innovatively based on a relationship network and relies on the use of community detection technology to achieve complete multivariate time series clustering. The detailed algorithm and the simulation experiments of the proposed BCNC method are reported. The experimental results on various datasets show that BCNC is superior to traditional multivariate time series clustering methods .},
  archive      = {J_PR},
  author       = {Hailin Li and Zechen Liu},
  doi          = {10.1016/j.patcog.2021.107919},
  journal      = {Pattern Recognition},
  pages        = {107919},
  shortjournal = {Pattern Recognition},
  title        = {Multivariate time series clustering based on complex network},
  volume       = {115},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Complex common spatial patterns on time-frequency decomposed
EEG for brain-computer interface. <em>PR</em>, <em>115</em>, 107918. (<a
href="https://doi.org/10.1016/j.patcog.2021.107918">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motor imagery brain-computer interface (MI-BCI) has many promising applications but there are problems such as poor classification accuracy and robustness which need to be addressed. We propose a novel approach called time-frequency common spatial patterns (TFCSP) to enhance the robustness and accuracy of the electroencephalogram (EEG) signal classification. The proposed approach decomposes the EEG signal into time stages and frequency components to find the most robust and discriminative features . Common spatial patterns (CSP) are extracted from every decomposed time-frequency cell and unreliable features are removed while remaining features are weighted and regularized for the classification. Comparison on three publicly available datasets from BCI competition III and IV shows that the proposed TFCSP outperforms state-of-the-art methods. This demonstrates that adopting subject reaction time paradigm is useful to enhance the classification performance. It also shows that the complex CSP in the frequency domain significantly effective than the commonly used bandpass-filters in time domain. Finally, this work proves that weighting and regularizing CSP features are better techniques than selecting the leading CSP features because the former alleviates information loss.},
  archive      = {J_PR},
  author       = {Vasilisa Mishuhina and Xudong Jiang},
  doi          = {10.1016/j.patcog.2021.107918},
  journal      = {Pattern Recognition},
  pages        = {107918},
  shortjournal = {Pattern Recognition},
  title        = {Complex common spatial patterns on time-frequency decomposed EEG for brain-computer interface},
  volume       = {115},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A new approach for optimal offline time-series segmentation
with error bound guarantee. <em>PR</em>, <em>115</em>, 107917. (<a
href="https://doi.org/10.1016/j.patcog.2021.107917">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Piecewise Linear Approximation is one of the most commonly used strategies to represent time series effectively and approximately. This approximation divides the time series into non-overlapping segments and approximates each segment with a straight line. Many suboptimal methods were proposed for this purpose. This paper proposes a new optimal approach, called OSFS, based on feasible space (FS) Liu et al. (2008)[1], that minimizes the number of segments of the approximation and guarantees the error bound using the L ∞ L∞ -norm. On the other hand, a new performance measure combined with the OSFS method has been used to evaluate the performance of some suboptimal methods and that of the optimal method that minimizes the holistic approximation error ( L 2 L2 -norm). The results have shown that the OSFS method is optimal and demonstrates the advantages of L ∞ L∞ -norm over L 2 L2 -norm.},
  archive      = {J_PR},
  author       = {Ángel Carmona-Poyato and Nicolás Luis Fernández-Garcia and Francisco José Madrid-Cuevas and Antonio Manuel Durán-Rosal},
  doi          = {10.1016/j.patcog.2021.107917},
  journal      = {Pattern Recognition},
  pages        = {107917},
  shortjournal = {Pattern Recognition},
  title        = {A new approach for optimal offline time-series segmentation with error bound guarantee},
  volume       = {115},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Vicinal and categorical domain adaptation. <em>PR</em>,
<em>115</em>, 107907. (<a
href="https://doi.org/10.1016/j.patcog.2021.107907">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised domain adaptation aims to learn a task classifier that performs well on the unlabeled target domain, by utilizing the labeled source domain. Inspiring results have been acquired by learning domain-invariant deep features via domain-adversarial training. However, its parallel design of task and domain classifiers limits the ability to achieve a finer category-level domain alignment. To promote categorical domain adaptation (CatDA) , based on a joint category-domain classifier, we propose novel losses of adversarial training at both domain and category levels. Since the joint classifier can be regarded as a concatenation of individual task classifiers respectively for the two domains, our design principle is to enforce consistency of category predictions between the two task classifiers. Moreover, we propose a concept of vicinal domains whose instances are produced by a convex combination of pairs of instances respectively from the two domains. Intuitively, alignment of the possibly infinite number of vicinal domains enhances that of original domains. We propose novel adversarial losses for vicinal domain adaptation (VicDA) based on CatDA, leading to Vicinal and Categorical Domain Adaptation (ViCatDA) . We also propose Target Discriminative Structure Recovery (TDSR) to recover the intrinsic target discrimination damaged by adversarial feature alignment. We also analyze the principles underlying the ability of our key designs to align the joint distributions. Extensive experiments on several benchmark datasets demonstrate that we achieve the new state of the art.},
  archive      = {J_PR},
  author       = {Hui Tang and Kui Jia},
  doi          = {10.1016/j.patcog.2021.107907},
  journal      = {Pattern Recognition},
  pages        = {107907},
  shortjournal = {Pattern Recognition},
  title        = {Vicinal and categorical domain adaptation},
  volume       = {115},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multimodal fusion for indoor sound source localization.
<em>PR</em>, <em>115</em>, 107906. (<a
href="https://doi.org/10.1016/j.patcog.2021.107906">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To identify the localization of indoor sound source, especially when attempted using only a single microphone, it is a challenging problem to machine learning . To address these issues, this paper presents a distinct novel solution based on fusing visual and acoustic models. Therefore, we propose two novel approaches. First, to estimate orientation of vocal object in a stable manner, we employ the visual approach as estimation model, where we develop a robust image feature representation method that adopts Fourier analysis to efficiently extract polar descriptors. Second the distance information is estimated by calculating the signal difference between transmit receive ends. To implement these, we use phoneme-level hidden Markov models (HMMs) extracted from clean speech sound, to estimate the acoustic transfer function (ATF), which can capture the speech signal as a network of phoneme HMMs. And using the separated frame sequences of the ATF, we can indicate the signal difference between two positions, which can be used to estimate the distance of sound source. Experimental results show that the proposed method can simultaneously extract the sound source parameters of direction and distance, and thus improves the verification task of sound source localization.},
  archive      = {J_PR},
  author       = {Jinhui Chen and Ryoichi Takashima and Xingchen Guo and Zhihong Zhang and Xuexin Xu and Tetsuya Takiguchi and Edwin R. Hancock},
  doi          = {10.1016/j.patcog.2021.107906},
  journal      = {Pattern Recognition},
  pages        = {107906},
  shortjournal = {Pattern Recognition},
  title        = {Multimodal fusion for indoor sound source localization},
  volume       = {115},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Scalable multi-label canonical correlation analysis for
cross-modal retrieval. <em>PR</em>, <em>115</em>, 107905. (<a
href="https://doi.org/10.1016/j.patcog.2021.107905">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-label canonical correlation analysis (ml-CCA) has been developed for cross-modal retrieval. However, the computation of ml-CCA involves dense matrices eigendecomposition, which can be computationally expensive. In addition, ml-CCA only takes semantic correlation into account which ignores the cross-modal feature correlation. In this paper, we propose a novel framework to simultaneously integrate the semantic correlation and feature correlation for cross-modal retrieval. By using the semantic transformation, we show that our model can avoid computing the covariance matrix explicitly which is a huge save of computational cost. Further analysis shows that our proposed method can be solved via singular value decomposition which has linear time complexity. Experimental results on three multi-label datasets have demonstrated the accuracy and efficiency of our proposed method.},
  archive      = {J_PR},
  author       = {Xin Shu and Guoying Zhao},
  doi          = {10.1016/j.patcog.2021.107905},
  journal      = {Pattern Recognition},
  pages        = {107905},
  shortjournal = {Pattern Recognition},
  title        = {Scalable multi-label canonical correlation analysis for cross-modal retrieval},
  volume       = {115},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Brain segmentation based on multi-atlas and diffeomorphism
guided 3D fully convolutional network ensembles. <em>PR</em>,
<em>115</em>, 107904. (<a
href="https://doi.org/10.1016/j.patcog.2021.107904">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we proposed and validated a multi-atlas and diffeomorphism guided 3D fully convolutional network (FCN) ensemble model (M-FCN) for segmenting brain anatomical regions of interest (ROIs) from structural magnetic resonance images (MRIs). A novel multi-atlas and diffeomorphism based encoding block and ROI patches with adaptive sizes were used. In the multi-atlas and diffeomorphism based encoding block, both MRI intensity profiles and expert priors from deformed atlases were encoded and fed to the proposed FCN. Utilizing patches with adaptive sizes enabled more efficient network training and testing. To incorporate both local and global contextual information of a specific ROI, we employed a long skip connection between the layer of the encoding block and the layer of the encoding-decoding block. To relieve over-fitting of the proposed FCN model on the training data, we adopted an ensemble strategy in the learning procedure. Systematic evaluations were performed on two brain MRI datasets, aiming respectively at segmenting 14 subcortical and ventricular structures and 54 whole-brain ROIs. Compared with two state-of-the-art segmentation methods including a multi-atlas based segmentation method and an existing 3D FCN segmentation model , the proposed method exhibited superior segmentation performance .},
  archive      = {J_PR},
  author       = {Jiong Wu and Xiaoying Tang},
  doi          = {10.1016/j.patcog.2021.107904},
  journal      = {Pattern Recognition},
  pages        = {107904},
  shortjournal = {Pattern Recognition},
  title        = {Brain segmentation based on multi-atlas and diffeomorphism guided 3D fully convolutional network ensembles},
  volume       = {115},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fooling deep neural detection networks with adaptive
object-oriented adversarial perturbation. <em>PR</em>, <em>115</em>,
107903. (<a href="https://doi.org/10.1016/j.patcog.2021.107903">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning has shown superiority in dealing with complicated and professional tasks (e.g., computer vision , audio, and language processing). However, research works have confirmed that Deep Neural Networks (DNNs) are vulnerable to carefully crafted adversarial perturbations, which cause DNNs confusion on specific tasks. In object detection domain, the background has little contributions to object classification, and the crafted adversarial perturbations added to the background do not improve the adversary effect in fooling deep neural detection models yet induce substantial distortions in generated examples. Based on such situation, we introduce an adversarial attack algorithm named Adaptive Object-oriented Adversarial Method (AO 2 2 AM). It aims to fool deep neural object detection networks with the adversarial examples by applying the adaptive cumulation of object-based gradients and adding the adaptive object-based adversarial perturbations merely onto objects rather than the whole frame of input images. AO 2 2 AM can effectively make the representations of generated adversarial samples close to the decision boundary in the latent space, and force deep neural detection networks to yield inaccurate locations and false classification in the process of object detection. Compared with existing adversarial attack methods which generate adversarial perturbations acting on the global scale of the original inputs, the adversarial examples produced by AO 2 2 AM can effectively fool deep neural object detection networks and maintain a high structural similarity with corresponding clean inputs. Performing adversarial attacks on Faster R-CNN, AO 2 2 AM gains attack success rate (ASR) over 98.00\% on pre-processed Pascal VOC 2007&amp;2012 (Val), and reaches SSIM over 0.870. In Fooling SSD, AO 2 2 AM receives SSIM exceeding 0.980 on L 2 L2 norm constraint. On SSIM and Mean Attack Ratio, AO 2 2 AM outperforms adversarial attack methods based on global scale perturbations.},
  archive      = {J_PR},
  author       = {Yatie Xiao and Chi-Man Pun and Bo Liu},
  doi          = {10.1016/j.patcog.2021.107903},
  journal      = {Pattern Recognition},
  pages        = {107903},
  shortjournal = {Pattern Recognition},
  title        = {Fooling deep neural detection networks with adaptive object-oriented adversarial perturbation},
  volume       = {115},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Skeletonisation algorithms with theoretical guarantees for
unorganised point clouds with high levels of noise. <em>PR</em>,
<em>115</em>, 107902. (<a
href="https://doi.org/10.1016/j.patcog.2021.107902">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data Science aims to extract meaningful knowledge from unorganised data. Real datasets usually come in the form of a cloud of points. It is a requirement of numerous applications to visualise an overall shape of a noisy cloud of points sampled from a non-linear object that is more complicated than a union of disjoint clusters. The skeletonisation problem in its hardest form is to find a 1-dimensional skeleton that correctly represents the shape of the cloud. This paper compares different algorithms that solve the above skeletonisation problem for any point cloud and guarantee a successful reconstruction. For example, given a highly noisy point sample of an unknown underlying graph, a reconstructed skeleton should be geometrically close and homotopy equivalent to (has the same number of independent cycles as) the underlying graph. One of these algorithms produces a Homologically Persistent Skeleton (HoPeS) for any cloud without extra parameters. This universal skeleton contains subgraphs that provably represent the 1-dimensional shape of the cloud at any scale. Other subgraphs of HoPeS reconstruct an unknown graph from its noisy point sample with a correct homotopy type and within a small offset of the sample. The extensive experiments on synthetic and real data reveal for the first time the maximum level of noise that allows successful graph reconstructions.},
  archive      = {J_PR},
  author       = {P. Smith and V. Kurlin},
  doi          = {10.1016/j.patcog.2021.107902},
  journal      = {Pattern Recognition},
  pages        = {107902},
  shortjournal = {Pattern Recognition},
  title        = {Skeletonisation algorithms with theoretical guarantees for unorganised point clouds with high levels of noise},
  volume       = {115},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Boundary-induced and scene-aggregated network for monocular
depth prediction. <em>PR</em>, <em>115</em>, 107901. (<a
href="https://doi.org/10.1016/j.patcog.2021.107901">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Monocular depth prediction is an important task in scene understanding. It aims to predict the dense depth of a single RGB image . With the development of deep learning , the performance of this task has made great improvements. However, two issues remain unresolved: (1) The deep feature encodes the wrong farthest region in a scene, which leads to a distorted 3D structure of the predicted depth; (2) The low-level features are insufficient utilized, which makes it even harder to estimate the depth near the edge with sudden depth change. To tackle these two issues, we propose the Boundary-induced and Scene-aggregated network (BS-Net). In this network, the Depth Correlation Encoder (DCE) is first designed to obtain the contextual correlations between the regions in an image, and perceive the farthest region by considering the correlations. Meanwhile, the Bottom-Up Boundary Fusion (BUBF) module is designed to extract accurate boundary that indicates depth change. Finally, the Stripe Refinement module (SRM) is designed to refine the dense depth induced by the boundary cue, which improves the boundary accuracy of the predicted depth. Several experimental results on the NYUD v2 dataset and the iBims-1 dataset illustrate the state-of-the-art performance of the proposed approach. And the SUN-RGBD dataset is employed to evaluate the generalization of our method. Code is available at https://github.com/XuefengBUPT/BS-Net .},
  archive      = {J_PR},
  author       = {Feng Xue and Junfeng Cao and Yu Zhou and Fei Sheng and Yankai Wang and Anlong Ming},
  doi          = {10.1016/j.patcog.2021.107901},
  journal      = {Pattern Recognition},
  pages        = {107901},
  shortjournal = {Pattern Recognition},
  title        = {Boundary-induced and scene-aggregated network for monocular depth prediction},
  volume       = {115},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). FleBiC: Learning classifiers from high-dimensional
biomedical data using discriminative biclusters with non-constant
patterns. <em>PR</em>, <em>115</em>, 107900. (<a
href="https://doi.org/10.1016/j.patcog.2021.107900">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The discovery of discriminative patterns from high-dimensional data offers the possibility to learn from informative subspaces and pattern-centric features, paving the way to associative classifiers. Despite the success achieved by associative classifiers, such as random forests or XGBoost , they generally neglect discriminative subspaces with non-constant coherencies . Research on biclustering has for two decades highlighted the role of non-constant patterns in biomedical domains, including additive and order-preserving patterns. Still, their relevance for classification remains unexplored. This work assesses the impact of discriminative patterns with varying coherence and quality on associative classification . A novel classifier, FleBiC, is proposed as a result. FleBiC extends pattern-based biclustering with principles to match observations against non-constant and noise-tolerant patterns, address generalization difficulties, minimize scarcity of matches, support class disjunctions, and offer statistical guarantees. Results on biological and clinical data highlight the role of non-constant patterns, specially order-preserving patterns, for improving the performance of state-of-the-art classifiers.},
  archive      = {J_PR},
  author       = {Rui Henriques and Sara C. Madeira},
  doi          = {10.1016/j.patcog.2021.107900},
  journal      = {Pattern Recognition},
  pages        = {107900},
  shortjournal = {Pattern Recognition},
  title        = {FleBiC: Learning classifiers from high-dimensional biomedical data using discriminative biclusters with non-constant patterns},
  volume       = {115},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Pruning by explaining: A novel criterion for deep neural
network pruning. <em>PR</em>, <em>115</em>, 107899. (<a
href="https://doi.org/10.1016/j.patcog.2021.107899">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The success of convolutional neural networks (CNNs) in various applications is accompanied by a significant increase in computation and parameter storage costs. Recent efforts to reduce these overheads involve pruning and compressing the weights of various layers while at the same time aiming to not sacrifice performance. In this paper, we propose a novel criterion for CNN pruning inspired by neural network interpretability: The most relevant units, i.e. weights or filters, are automatically found using their relevance scores obtained from concepts of explainable AI (XAI). By exploring this idea, we connect the lines of interpretability and model compression research. We show that our proposed method can efficiently prune CNN models in transfer-learning setups in which networks pre-trained on large corpora are adapted to specialized tasks. The method is evaluated on a broad range of computer vision datasets. Notably, our novel criterion is not only competitive or better compared to state-of-the-art pruning criteria when successive retraining is performed, but clearly outperforms these previous criteria in the resource-constrained application scenario in which the data of the task to be transferred to is very scarce and one chooses to refrain from fine-tuning. Our method is able to compress the model iteratively while maintaining or even improving accuracy. At the same time, it has a computational cost in the order of gradient computation and is comparatively simple to apply without the need for tuning hyperparameters for pruning.},
  archive      = {J_PR},
  author       = {Seul-Ki Yeom and Philipp Seegerer and Sebastian Lapuschkin and Alexander Binder and Simon Wiedemann and Klaus-Robert Müller and Wojciech Samek},
  doi          = {10.1016/j.patcog.2021.107899},
  journal      = {Pattern Recognition},
  pages        = {107899},
  shortjournal = {Pattern Recognition},
  title        = {Pruning by explaining: A novel criterion for deep neural network pruning},
  volume       = {115},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust and discriminative image representation:
Fractional-order jacobi-fourier moments. <em>PR</em>, <em>115</em>,
107898. (<a href="https://doi.org/10.1016/j.patcog.2021.107898">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robust and discriminative image representation is a long-lasting battle in the computer vision and pattern recognition. Moment-based image representation model is effective in satisfying the core conditions of semantic description, due to its geometric invariance and independence. However, moment-based descriptors suffer from a contradiction between the robustness and discriminability , which limits the further improvement of description quality. In this paper, a set of generic moments along with a novel representation framework are proposed to mitigate this troublesome contradiction. We first define a new set of orthogonal moments, named Fractional-order Jacobi-Fourier Moments (FJFM), which is characterized by the generic nature and time-frequency analysis capability. We then develop a new framework to improve both the robustness and discriminability of image representation, called Mixed Low-order Moment Feature (MLMF), by fully exploiting the time-frequency property of FJFM. Extensive experimental results and a real-world application are provided to demonstrate the superior performance of our FJFM-based MLMF, with respect to robustness and discriminability.},
  archive      = {J_PR},
  author       = {Hongying Yang and Shuren Qi and Jialin Tian and Panpan Niu and Xiangyang Wang},
  doi          = {10.1016/j.patcog.2021.107898},
  journal      = {Pattern Recognition},
  pages        = {107898},
  shortjournal = {Pattern Recognition},
  title        = {Robust and discriminative image representation: Fractional-order jacobi-fourier moments},
  volume       = {115},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Practical globally optimal consensus maximization by
branch-and-bound based on interval arithmetic. <em>PR</em>,
<em>115</em>, 107897. (<a
href="https://doi.org/10.1016/j.patcog.2021.107897">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Consensus maximization is widely used in robust model fitting, and it is usually solved by RANSAC-type methods in practice. However, these methods cannot guarantee global optimality and sometimes return the wrong solutions. A series of Branch-and-bound (BnB) based globally optimal methods have been proposed, most of which involve deriving a complex bound. Interval arithmetic was utilized to derive simple bounds for BnB in solving geometric matching problems in 2003. However, this idea was somewhat forgotten in the community because it seems natural that the simple interval arithmetic based bounds might be worse than those elaborate bounds. Recently, some new globally optimal algorithms without using BnB were developed for consensus maximization, but they can only work with a small number of data points and low outlier ratios . In this work, we draw the idea of simple bounds by interval arithmetic back on the map and demonstrate its practicability by making substantial extensions. Concretely, we give detailed derivation of solving robust model fitting problems with both linear and quasi-convex residuals and propose practical methods to use them under Unit-Norm constraint and in a high-dimensional problem. Extensive experiments show that the proposed method can handle practical problems with large number of data points and high outlier ratios. It outperforms state-of-the-art global, RANSAC-type, and deterministic methods in terms of both accuracy and efficiency in low-dimensional problems. The source code is publicly available. 2},
  archive      = {J_PR},
  author       = {Yiru Wang and Yinlong Liu and Xuechen Li and Chen Wang and Manning Wang and Zhijian Song},
  doi          = {10.1016/j.patcog.2021.107897},
  journal      = {Pattern Recognition},
  pages        = {107897},
  shortjournal = {Pattern Recognition},
  title        = {Practical globally optimal consensus maximization by branch-and-bound based on interval arithmetic},
  volume       = {115},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Time series cluster kernels to exploit informative
missingness and incomplete label information. <em>PR</em>, <em>115</em>,
107896. (<a href="https://doi.org/10.1016/j.patcog.2021.107896">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The time series cluster kernel (TCK) provides a powerful tool for analysing multivariate time series subject to missing data. TCK is designed using an ensemble learning approach in which Bayesian mixture models form the base models. Because of the Bayesian approach, TCK can naturally deal with missing values without resorting to imputation and the ensemble strategy ensures robustness to hyperparameters, making it particularly well suited for unsupervised learning. However, TCK assumes missing at random and that the underlying missingness mechanism is ignorable, i.e. uninformative, an assumption that does not hold in many real-world applications, such as e.g. medicine. To overcome this limitation, we present a kernel capable of exploiting the potentially rich information in the missing values and patterns, as well as the information from the observed data. In our approach, we create a representation of the missing pattern, which is incorporated into mixed mode mixture models in such a way that the information provided by the missing patterns is effectively exploited. Moreover, we also propose a semi-supervised kernel, capable of taking advantage of incomplete label information to learn more accurate similarities. Experiments on benchmark data, as well as a real-world case study of patients described by longitudinal electronic health record data who potentially suffer from hospital-acquired infections, demonstrate the effectiveness of the proposed methods.},
  archive      = {J_PR},
  author       = {Karl Øyvind Mikalsen and Cristina Soguero-Ruiz and Filippo Maria Bianchi and Arthur Revhaug and Robert Jenssen},
  doi          = {10.1016/j.patcog.2021.107896},
  journal      = {Pattern Recognition},
  pages        = {107896},
  shortjournal = {Pattern Recognition},
  title        = {Time series cluster kernels to exploit informative missingness and incomplete label information},
  volume       = {115},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Tight lower bounds for dynamic time warping. <em>PR</em>,
<em>115</em>, 107895. (<a
href="https://doi.org/10.1016/j.patcog.2021.107895">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic Time Warping ( DTW DTW ) is a popular similarity measure for aligning and comparing time series. Due to DTW DTW ’s high computation time, lower bounds are often employed to screen poor matches. Many alternative lower bounds have been proposed, providing a range of different trade-offs between tightness and computational efficiency. LB _ KEOGH LB_KEOGH provides a useful trade-off in many applications. Two recent lower bounds, LB _ IMPROVED LB_IMPROVED and LB _ ENHANCED , LB_ENHANCED, are substantially tighter than LB _ KEOGH LB_KEOGH . All three have the same worst case computational complexity—linear with respect to series length and constant with respect to window size. We present four new DTW DTW lower bounds in the same complexity class. LB _ PETITJEAN LB_PETITJEAN is substantially tighter than LB _ IMPROVED , LB_IMPROVED, with only modest additional computational overhead. LB _ WEBB LB_WEBB is more efficient than LB _ IMPROVED , LB_IMPROVED, while often providing a tighter bound. LB _ WEBB LB_WEBB is always tighter than LB _ KEOGH LB_KEOGH . The parameter free LB _ WEBB LB_WEBB is usually tighter than LB _ ENHANCED LB_ENHANCED . A parameterized variant, LB_Webb_Enhanced , is always tighter than LB _ ENHANCED LB_ENHANCED . A further variant, LB _ WEBB * , LB_WEBB*, is useful for some constrained distance functions. In extensive experiments, LB _ WEBB LB_WEBB proves to be very effective for nearest neighbor search.},
  archive      = {J_PR},
  author       = {Geoffrey I. Webb and François Petitjean},
  doi          = {10.1016/j.patcog.2021.107895},
  journal      = {Pattern Recognition},
  pages        = {107895},
  shortjournal = {Pattern Recognition},
  title        = {Tight lower bounds for dynamic time warping},
  volume       = {115},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Jointly learning compact multi-view hash codes for few-shot
FKP recognition. <em>PR</em>, <em>115</em>, 107894. (<a
href="https://doi.org/10.1016/j.patcog.2021.107894">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a relatively new biometric trait, Finger-Knuckle-Print (FKP) plays a vital role in establishing a personal authentication system in modern society due to its rich discriminative features, low time cost in image capture and user-friendliness. However, most existing KFP descriptors are hand-crafted and fail to work well with limited training samples. In this paper, we propose a feature learning method for few-shot FKP recognition by jointly learning compact multi-view hash codes (JLCMHC) of a FKP image. We first form the multi-view data vectors (MVDV) to exploit the multiple feature-specific information from a FKP image. Then, we learn a feature projection to encode the MVDV into compact binary codes in an unsupervised manner , where 1) the variance of the learned feature codes on each view is maximized and 2) the difference of the inter-view binary codes is enlarged, so that the redundant information in MVDV is reduced and more informative features can be obtained. Lastly, we pool the binary codes into block-wise statistics features as the final descriptor for FKP representation and recognition. Experimental results on the existing benchmark FKP databases clearly show that the JLCMHC method outperforms the state-of-the-art FKP descriptors.},
  archive      = {J_PR},
  author       = {Lunke Fei and Bob Zhang and Jie Wen and Shaohua Teng and Shuyi Li and David Zhang},
  doi          = {10.1016/j.patcog.2021.107894},
  journal      = {Pattern Recognition},
  pages        = {107894},
  shortjournal = {Pattern Recognition},
  title        = {Jointly learning compact multi-view hash codes for few-shot FKP recognition},
  volume       = {115},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Residual multi-task learning for facial landmark
localization and expression recognition. <em>PR</em>, <em>115</em>,
107893. (<a href="https://doi.org/10.1016/j.patcog.2021.107893">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial landmark localization and expression recognition are two important and highly relevant topics in facial analysis. However, few works focus on using the complementary information between the two tasks to improve the performance. In this paper, we propose a residual multi-task learning framework to predict the two tasks simultaneously. Different from previous multi-task learning methods which directly train a deep multi-task network with additional branches and losses, we propose a novel residual learning module to further strengthen the linkages between the two tasks. Benefit from the proposed residual learning module, one task can learn complementary information from the other task, leading to the performance promotion. Another problem for the multi-task learning is the lack of training data with multi-task labels. For example, there is no landmark localization annotation for the two widely-used FER dataset (AffectNet and RAF), vice versa. To solve this problem, we propose an association learning method to further enhance the connection between the two tasks. Based on this connection, the dataset with single-task labels can be used in the multi-task learning. Extensive experiments are conducted on four popular datasets ( i.e. 300-W, AFLW for landmark localization and AffectNet, RAF for expression recognition), demonstrating the effectiveness of the proposed algorithm.},
  archive      = {J_PR},
  author       = {Boyu Chen and Wenlong Guan and Peixia Li and Naoki Ikeda and Kosuke Hirasawa and Huchuan Lu},
  doi          = {10.1016/j.patcog.2021.107893},
  journal      = {Pattern Recognition},
  pages        = {107893},
  shortjournal = {Pattern Recognition},
  title        = {Residual multi-task learning for facial landmark localization and expression recognition},
  volume       = {115},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An end-to-end framework for unconstrained monocular 3D hand
pose estimation. <em>PR</em>, <em>115</em>, 107892. (<a
href="https://doi.org/10.1016/j.patcog.2021.107892">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work addresses the challenging problem of unconstrained 3D hand pose estimation using monocular RGB images . Most of the existing approaches assume some prior knowledge of hand (such as hand locations and side information) is available for 3D hand pose estimation. This restricts their use in unconstrained environments. Therefore, we present an end-to-end framework that robustly predicts hand prior information and accurately infers 3D hand pose by learning ConvNet models while only using keypoint annotations. To enhance the hand detector’s robustness, we propose a novel keypoint-based method to simultaneously predict hand regions and side labels, unlike existing methods that suffer from background color confusion caused by using segmentation or detection-based technology. Moreover, inspired by the human hand’s biological structure, we introduce two geometric constraints directly into the 3D coordinates prediction that further improves its performance. Experimental results show that our proposed framework outperforms the state-of-art methods on standard benchmark datasets while providing robust predictions.},
  archive      = {J_PR},
  author       = {Sanjeev Sharma and Shaoli Huang},
  doi          = {10.1016/j.patcog.2021.107892},
  journal      = {Pattern Recognition},
  pages        = {107892},
  shortjournal = {Pattern Recognition},
  title        = {An end-to-end framework for unconstrained monocular 3D hand pose estimation},
  volume       = {115},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Quaternionic extended local binary pattern with adaptive
structural pyramid pooling for color image representation. <em>PR</em>,
<em>115</em>, 107891. (<a
href="https://doi.org/10.1016/j.patcog.2021.107891">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a novel feature representation method for color images, namely quaternionic extended local binary pattern (QxLBP) with adaptive structural pyramid pooling (ASPP). First, we propose a QxLBP operator to encode local neighboring information and complementary modulus and phase information in the quaternion domain of color images. In QxLBP, an extended quaternionic representation (EQR) is proposed which introduces an information term into the real part of a quaternion. The resulting EQR enables us to flexibly encode discriminative features and handle multichannel image data. Second, we propose ASPP as a multiresolution pooling way to aggregate local features . Unlike the traditional spatial pyramid pooling which is sensitive to image rotation and spatial changes, ASPP is structure-oriented pooling which can adaptively aggregate the encoded features into multiresolution histogram representations. Experiments on four benchmark image datasets demonstrate that the proposed method achieves the state-of-the-art performance for color texture classification and scene categorization.},
  archive      = {J_PR},
  author       = {Tiecheng Song and Liangliang Xin and Chenqiang Gao and Tianqi Zhang and Yao Huang},
  doi          = {10.1016/j.patcog.2021.107891},
  journal      = {Pattern Recognition},
  pages        = {107891},
  shortjournal = {Pattern Recognition},
  title        = {Quaternionic extended local binary pattern with adaptive structural pyramid pooling for color image representation},
  volume       = {115},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A novel consensus learning approach to incomplete multi-view
clustering. <em>PR</em>, <em>115</em>, 107890. (<a
href="https://doi.org/10.1016/j.patcog.2021.107890">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view data may lose some instances in real applications. Most existing methods for clustering such incomplete multi-view data still have at least one of the following limitations: 1) The common relations among data points across all views are ignored. 2) The complementary multi-view information of original data representation is not well exploited. 3) Arbitrary incomplete scenarios or data with negative entries cannot be handled. To address these limitations, in this paper, we propose a novel Consensus Learning approach to Incomplete Multi-view Clustering (CLIMC). Specifically, a low-dimensional consensus representation is introduced to exploit complementary multi-view information from the original feature representation of available instances by integrating index matrices into matrix factorization . In addition, by combining self-representation, index matrices, and consensus term, a consensus similarity graph is leveraged to explore the underlying cross-view relations among data points. Further, the key of the proposed CLIMC is that the consensus representation is correlated with the similarity graph by a graph Laplacian regularization . Consequently, the compactness of the low-dimensional representation and the accuracy of similarity degree of the graph are reciprocally promoted. Extensive experiments on several multi-view datasets demonstrate the effectiveness of CLIMC over state-of-the-arts.},
  archive      = {J_PR},
  author       = {Jianlun Liu and Shaohua Teng and Lunke Fei and Wei Zhang and Xiaozhao Fang and Zhuxiu Zhang and Naiqi Wu},
  doi          = {10.1016/j.patcog.2021.107890},
  journal      = {Pattern Recognition},
  pages        = {107890},
  shortjournal = {Pattern Recognition},
  title        = {A novel consensus learning approach to incomplete multi-view clustering},
  volume       = {115},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A hierarchical sampling based triplet network for
fine-grained image classification. <em>PR</em>, <em>115</em>, 107889.
(<a href="https://doi.org/10.1016/j.patcog.2021.107889">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep metric learning leverages well-designed distance measurement and a sample selection strategy to learn a discriminative feature space. Among the various deep metric learning formulations, triplet loss is built based on a 3-tuple that can simultaneously minimise the distance between the items in the positive pair and maximise the distance between those in the negative pair. However, this endeavour requires a critical selection of triplet samples to guide the training process. In this paper, we propose a layered Triplet loss to solve the fine-grained image classification problem. Unlike the existing triplet loss, which selects samples from only a single criterion, we construct the loss function with the ’coarse to fine’ scheme. This scheme can separate the coarse-level classes while clustering the fine-level samples within a certain margin. An ontology-based sampling method is proposed to enable the network to mine more reasonable hard triplets. Semantic knowledge is employed to assign the visually similar classes to the same learning task, from which hard triplets can be generated. Finally, the softmax tree classifier is used to classify the hierarchical features. The experimental results on multiple datasets demonstrate the effectiveness of the proposed method.},
  archive      = {J_PR},
  author       = {Guiqing He and Feng Li and Qiyao Wang and Zongwen Bai and Yuelei Xu},
  doi          = {10.1016/j.patcog.2021.107889},
  journal      = {Pattern Recognition},
  pages        = {107889},
  shortjournal = {Pattern Recognition},
  title        = {A hierarchical sampling based triplet network for fine-grained image classification},
  volume       = {115},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Unified unsupervised and semi-supervised domain adaptation
network for cross-scenario face anti-spoofing. <em>PR</em>,
<em>115</em>, 107888. (<a
href="https://doi.org/10.1016/j.patcog.2021.107888">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the environmental differences, many face anti-spoofing methods fail to generalize to unseen scenarios. In light of this, we propose a unified unsupervised and semi-supervised domain adaptation network (USDAN) for cross-scenario face anti-spoofing, aiming at minimizing the distribution discrepancy between the source and the target domains. Specifically, two modules, i.e. , marginal distribution alignment module (MDA) and conditional distribution alignment module (CDA), are designed to seek a domain-invariant feature space via adversarial learning and make the features of the same class compact, respectively. By adding/removing the CDA module, the network can be easily switched for semi-supervised/unsupervised setting, in which sense our method is named with “unified”. Moreover, the adaptive cross-entropy loss and normalization techniques are further incorporated to improve the generalization. Extensive experimental results show that the proposed USDAN outperforms state-of-the-art methods on several public datasets.},
  archive      = {J_PR},
  author       = {Yunpei Jia and Jie Zhang and Shiguang Shan and Xilin Chen},
  doi          = {10.1016/j.patcog.2021.107888},
  journal      = {Pattern Recognition},
  pages        = {107888},
  shortjournal = {Pattern Recognition},
  title        = {Unified unsupervised and semi-supervised domain adaptation network for cross-scenario face anti-spoofing},
  volume       = {115},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Geometric moment invariants to spatial transform and n-fold
symmetric blur. <em>PR</em>, <em>115</em>, 107887. (<a
href="https://doi.org/10.1016/j.patcog.2021.107887">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we focus on the derivation of blur moment invariants. Blur moment invariants are image moment-based features, which preserve their values when the image is convolved by a point-spread function (PSF). Suppose a PSF has N N -fold rotational symmetry , we prove its geometric moments of the same order are linearly dependent . Depending on this property, a new approach is proposed to determine whether an existing similarity or affine moment invariant also has invariance to N N -fold symmetric blur. Unlike earlier work, this method is not based on complicated operators and construction formulas. We use it to analyse classical moment-based features, and surprisingly find that five of Hu moment invariants are naturally invariant to N N -fold symmetric blur. Meanwhile, we first prove the existence of moment invariants to both affine transform and N N -fold symmetric blur. The experiments using synthetic and real blur image datasets are carried out to test these expectations. And the results show that five Hu moment invariants outperform some widely used blur moment invariants and non-moment image features in image retrieval , classification and template matching .},
  archive      = {J_PR},
  author       = {Hanlin Mo and Hongxiang Hao and Hua Li},
  doi          = {10.1016/j.patcog.2021.107887},
  journal      = {Pattern Recognition},
  pages        = {107887},
  shortjournal = {Pattern Recognition},
  title        = {Geometric moment invariants to spatial transform and N-fold symmetric blur},
  volume       = {115},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). LCU-net: A novel low-cost u-net for environmental
microorganism image segmentation. <em>PR</em>, <em>115</em>, 107885. (<a
href="https://doi.org/10.1016/j.patcog.2021.107885">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel Low-cost U-Net (LCU-Net) for the Environmental Microorganism (EM) image segmentation task to assist microbiologists in detecting and identifying EMs more effectively. The LCU-Net is an improved Convolutional Neural Network (CNN) based on U-Net, Inception, and concatenate operations. It addresses the limitation of single receptive field setting and the relatively high memory cost of U-Net. Experimental results show the effectiveness and potential of the proposed LCU-Net in the practical EM image segmentation field.},
  archive      = {J_PR},
  author       = {Jinghua Zhang and Chen Li and Sergey Kosov and Marcin Grzegorzek and Kimiaki Shirahama and Tao Jiang and Changhao Sun and Zihan Li and Hong Li},
  doi          = {10.1016/j.patcog.2021.107885},
  journal      = {Pattern Recognition},
  pages        = {107885},
  shortjournal = {Pattern Recognition},
  title        = {LCU-net: A novel low-cost U-net for environmental microorganism image segmentation},
  volume       = {115},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). 3D-CenterNet: 3D object detection network for point clouds
with center estimation priority. <em>PR</em>, <em>115</em>, 107884. (<a
href="https://doi.org/10.1016/j.patcog.2021.107884">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a single-stage 3D object detection framework, 3D-CenterNet, is proposed for accurate 3D object detection from point clouds. We find that the center position is more critical for accurate bounding box detection than the other two parameters, the size and the orientation. Motivated by this discovery, we propose the center regression module (CRM) to regress the centers’ location from the point-wise features. In CRM, the representative points belonging to objects are sampled to regress the center locations of the corresponding objects. The semantic and geometric information related to the estimated centers is aggregated for the following location refinement and other parameters’ estimation. The 3D-CenterNet stacks the CRMs to improve the accuracy of the estimated centers gradually. The size and orientation of the bounding boxes are decoded from the high dimensional center-wise features. The experiments on the KITTI benchmark and the SUN RGB-D datasets show that our proposed 3D-CenterNet achieves high-quality results in real time.},
  archive      = {J_PR},
  author       = {Qi Wang and Jian Chen and Jianqiang Deng and Xinfang Zhang},
  doi          = {10.1016/j.patcog.2021.107884},
  journal      = {Pattern Recognition},
  pages        = {107884},
  shortjournal = {Pattern Recognition},
  title        = {3D-CenterNet: 3D object detection network for point clouds with center estimation priority},
  volume       = {115},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A survey on matching strategies for boundary image
comparison and evaluation. <em>PR</em>, <em>115</em>, 107883. (<a
href="https://doi.org/10.1016/j.patcog.2021.107883">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most of the strategies for boundary image evaluation involve the comparison of computer-generated images with ground truth solutions. While this can be done in different manners, recent years have seen a dominance of techniques based on the use of confusion matrices . That is, techniques that, at the evaluation stage, interpret boundary detection as a classification problem. These techniques require a correspondence between the boundary pixels in the candidate image and those in the ground truth; that correspondence is further used to create the confusion matrix, from which evaluation statistics can be computed. The correspondence between boundary images faces different challenges, mainly related to the matching of potentially displaced boundaries. Interestingly, boundary image comparison relates to many other fields of study in literature, from object tracking to biometrical identification. In this work, we survey all existing strategies for boundary matching, we propose a taxonomy to embrace them all, and perform a usability-driven quantitative analysis of their behaviour.},
  archive      = {J_PR},
  author       = {C. Lopez-Molina and C. Marco-Detchart and H. Bustince and B. De Baets},
  doi          = {10.1016/j.patcog.2021.107883},
  journal      = {Pattern Recognition},
  pages        = {107883},
  shortjournal = {Pattern Recognition},
  title        = {A survey on matching strategies for boundary image comparison and evaluation},
  volume       = {115},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Few-shot prototype alignment regularization network for
document image layout segementation. <em>PR</em>, <em>115</em>, 107882.
(<a href="https://doi.org/10.1016/j.patcog.2021.107882">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the great performance in layout analysis tasks made by semantic segmentation , they usually need a large number of annotated images for training and are difficult to learn a new category which is absent in the training categories. Meta-learning and few-shot segmentation have been developed to solve the above two difficulties. In this paper, we propose a novel method dubbed Few-Shot Prototype Alignment Regularization Network (FS-PARN). The FS-PARN method is inspired by recent studies in both metric learning and few-shot segmentation, which just need a few annotated images to solve the above two difficulties. Our FS-PARN method can make better use of the information of the support set by metric learning and have a better effect on image segmentation. It learns classification prototype within an embedding space and then completes pixel classification by matching each pixel on the query image with the learned prototype. In addition to obtaining high-quality prototypes through metric learning methods, our FS-PARN method also introduces prototype alignment regularization between support and query sets to make segmentation better. Notably, our FS-PARN model achieves the mean-IoU score of 28.8\% and 31.7\% on the practical document image datasets,  i.e.  PASCAL-5i, DSSE-200, and Layout Analysis Dataset, for 1-shot and 5-shot settings respectively.},
  archive      = {J_PR},
  author       = {Yujie Li and Pengfei Zhang and Xing Xu and Yi Lai and Fumin Shen and Lijiang Chen and Pengxiang Gao},
  doi          = {10.1016/j.patcog.2021.107882},
  journal      = {Pattern Recognition},
  pages        = {107882},
  shortjournal = {Pattern Recognition},
  title        = {Few-shot prototype alignment regularization network for document image layout segementation},
  volume       = {115},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Mean-shift outlier detection and filtering. <em>PR</em>,
<em>115</em>, 107874. (<a
href="https://doi.org/10.1016/j.patcog.2021.107874">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional outlier detection methods create a model for data and then label as outliers for objects that deviate significantly from this model. However, when dat has many outliers, outliers also pollute the model. The model then becomes unreliable, thus rendering most outlier detectors to become ineffective. To solve this problem, we propose a mean-shift outlier detector. This detector employs a mean-shift technique to modify data and cancel the bias caused by the outliers. The mean-shift technique replaces every object by the mean of its k -nearest neighbors which essentially removes the effect of outliers before clustering without the need to know the outliers. In addition, it also detects outliers based on the distance shifted. Our experiments show that the proposed method works well regardless of the number of outliers in the data. This method outperforms all state-of-the-art methods tested, with both real-world numeric datasets as well as generated numeric and string datasets.},
  archive      = {J_PR},
  author       = {Jiawei Yang and Susanto Rahardja and Pasi Fränti},
  doi          = {10.1016/j.patcog.2021.107874},
  journal      = {Pattern Recognition},
  pages        = {107874},
  shortjournal = {Pattern Recognition},
  title        = {Mean-shift outlier detection and filtering},
  volume       = {115},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Face alignment using two-stage cascaded pose regression and
mirror error correction. <em>PR</em>, <em>115</em>, 107866. (<a
href="https://doi.org/10.1016/j.patcog.2021.107866">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, a series of cascaded pose regression based facial landmark localization methods under occlusion have been proposed. However, partial occlusions and pose variations will break the entire structure of the face which poses obstacles to global regression. Moreover, there lack techniques to evaluate the reliability of the regression results during the regression process. In this paper, we propose a Two-Stage Cascaded Pose Regression(TSCPR) for facial landmark localization under occlusion. In the first stage, a global cascaded pose regression with robust initialization is performed to get localization results for the original face and its mirror image. The localization difference between the original image and the mirror image is used to determine whether the localization of each landmark is reliable, while unreliable localization can be adjusted. In the second stage, the global results are divided into multiple parts, which are further refined by local regressions. Finally, multiple refined local results are rated and adjusted to get the final output. We evaluated the proposed method on widely used datasets COFW, LFPW, HELEN, 300-W and Menpo-Semifrontal. The experimental results show that the proposed method can outperform the state-of-the-arts.},
  archive      = {J_PR},
  author       = {Ziye Tong and Junwei Zhou},
  doi          = {10.1016/j.patcog.2021.107866},
  journal      = {Pattern Recognition},
  pages        = {107866},
  shortjournal = {Pattern Recognition},
  title        = {Face alignment using two-stage cascaded pose regression and mirror error correction},
  volume       = {115},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An adversarial human pose estimation network injected with
graph structure. <em>PR</em>, <em>115</em>, 107863. (<a
href="https://doi.org/10.1016/j.patcog.2021.107863">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Because of the invisible human keypoints in images caused by illumination, occlusion and overlap, it is likely to produce unreasonable human pose prediction for most of the current human pose estimation methods. In this paper, we design a novel generative adversarial network (GAN) to improve the localization accuracy of visible joints when some joints are invisible. The network consists of two simple but efficient modules, i.e ., Cascade Feature Network (CFN) and Graph Structure Network (GSN). First, the CFN utilizes the prediction maps from the previous stages to guide the prediction maps in the next stage to produce accurate human pose. Second, the GSN is designed to contribute to the localization of invisible joints by passing message among different joints. According to GAN, if the prediction pose produced by the generator G cannot be distinguished by the discriminator D , the generator network G has successfully obtained the underlying dependence of human joints. We conduct experiments on three widely used human pose estimation benchmark datasets, i.e ., LSP, MPII and COCO, whose results show the effectiveness of our proposed framework.},
  archive      = {J_PR},
  author       = {Lei Tian and Peng Wang and Guoqiang Liang and Chunhua Shen},
  doi          = {10.1016/j.patcog.2021.107863},
  journal      = {Pattern Recognition},
  pages        = {107863},
  shortjournal = {Pattern Recognition},
  title        = {An adversarial human pose estimation network injected with graph structure},
  volume       = {115},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Weakly-supervised semantic segmentation with saliency and
incremental supervision updating. <em>PR</em>, <em>115</em>, 107858. (<a
href="https://doi.org/10.1016/j.patcog.2021.107858">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weakly-supervised semantic segmentation aims at tackling the dense labeling task using weak supervision so as to reduce human annotation efforts. For weakly-supervised semantic segmentation using only image-level annotation, we propose a novel model of Learning with Saliency and Incremental Supervision Updating (LSISU), in which both the guidances of saliency prior and class information are jointly used and the segmentation supervision is dynamically updated. In the proposed LSISU, we present an image saliency objective complementary to classification loss, by which the trained weakly-supervised deep network can effectively deal with object co-occurrence problem. Meanwhile, we make full use of the class-wise pooling strategy to generate initial mask estimation of high quality. Given an initial annotation, a segmentation network is learned along with incremental supervision updating, which plays a role of region expansion and corrects the falsely estimated supervision for training images. The incremental supervision updating is performed on the fly and involves repeated usage of a fully connected conditional random field algorithm. LSISU achieves superior segmentation performance in terms of mIoU metric on benchmark datasets, which are 62.5\% on the PASCAL VOC 2012 test set and 30.1\% on the COCO val set.},
  archive      = {J_PR},
  author       = {Wenfeng Luo and Meng Yang and Weishi Zheng},
  doi          = {10.1016/j.patcog.2021.107858},
  journal      = {Pattern Recognition},
  pages        = {107858},
  shortjournal = {Pattern Recognition},
  title        = {Weakly-supervised semantic segmentation with saliency and incremental supervision updating},
  volume       = {115},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Pedestrian detection with super-resolution reconstruction
for low-quality image. <em>PR</em>, <em>115</em>, 107846. (<a
href="https://doi.org/10.1016/j.patcog.2021.107846">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pedestrian detection has emerged as a fundamental technology for autonomous cars, robotics, pedestrian search, and other applications. Although many excellent object detection algorithms can be used for pedestrian detection, it is still a challenging problem due to the complicated real-world scenarios, e.g., the detection of pedestrians in low-quality surveillance videos . In this paper, we aim to study the challenging topic of pedestrian detection in low-quality images. Low-quality images are interpreted as those taken with a low-resolution camera, heavy weather or a blurred scene, making it difficult to distinguish pedestrians from the background. To solve this problem, we first introduce a dataset called playground (PG) for low-quality image detection. Images from PG are shot using two different camera views, and pedestrian images are taken at different periods, including day and night. The dataset contains a total of 5,752 images with 31,041 annotations. The average size of the pedestrian is 87 × 41 87×41 and the image size is 480 × × 640, indicating that these images are taken from very long distances. Then, we propose a super-resolution detection (SRD) network to enhance the resolution of low-quality images that can help distinguish pedestrians from the blurred background. Finally, based on these enhanced images, we adopt and improve the Faster R-CNN network to help relocate occluded pedestrians. Experimental results on this new dataset proved the efficiency and effectiveness of our algorithm on low-quality images.},
  archive      = {J_PR},
  author       = {Yi Jin and Yue Zhang and Yigang Cen and Yidong Li and Vladimir Mladenovic and Viacheslav Voronin},
  doi          = {10.1016/j.patcog.2021.107846},
  journal      = {Pattern Recognition},
  pages        = {107846},
  shortjournal = {Pattern Recognition},
  title        = {Pedestrian detection with super-resolution reconstruction for low-quality image},
  volume       = {115},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dual space latent representation learning for unsupervised
feature selection. <em>PR</em>, <em>114</em>, 107873. (<a
href="https://doi.org/10.1016/j.patcog.2021.107873">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In real-world applications, data instances are not only related to high-dimensional features, but also interconnected with each other. However, the interconnection information has not been fully exploited for feature selection. To address this issue, we propose a novel feature selection algorithm , called dual space latent representation learning for unsupervised feature selection (DSLRL), which exploits the internal association information of data space and feature space to guide feature selection. Firstly, based on latent representation learning in data space, DSLRL produces dual space latent representation learning, which characterizes the inherent structure of data space and feature space, respectively. Secondly, in order to overcome the problem of the lack of label information, DSLRL optimizes the low-dimensional latent representation matrix of data space as a pseudo-label matrix to provide clustering indicators. Moreover, the latent representation matrix of feature space is unified with the transformation matrix to benefit the matching of the data matrix and the clustering indicator matrix. In addition, DSLRL uses non-negative and orthogonal conditions to constrain the sparse transform matrix, making it more accurate for evaluating features. Finally, an alternating method is employed to optimize the objective function. Compared with seven state-of-the-art algorithms, experimental results on twelve datasets show the effectiveness of DSLRL.},
  archive      = {J_PR},
  author       = {Ronghua Shang and Lujuan Wang and Fanhua Shang and Licheng Jiao and Yangyang Li},
  doi          = {10.1016/j.patcog.2021.107873},
  journal      = {Pattern Recognition},
  pages        = {107873},
  shortjournal = {Pattern Recognition},
  title        = {Dual space latent representation learning for unsupervised feature selection},
  volume       = {114},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Thermodynamic motif analysis for directed stock market
networks. <em>PR</em>, <em>114</em>, 107872. (<a
href="https://doi.org/10.1016/j.patcog.2021.107872">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present a novel thermodynamically based analysis method for directed networks, and in particular for time-evolving networks in the finance domain . Based on an analogy with a dilute gas in statistical mechanics, we develop a partition function for a network composed of directed motifs. The method relies on the decomposition of directed networks into a series of frequently occurring graphlets, or motifs. According to the connection between a directed network and the dilute gas, the network motifs have the same topological structure as the low-order interactions between particles in the gas. This means that we can use the so-called cluster expansion from statistical mechanics to develop a partition function for the motif decomposition. In prior work, we have reported a detailed analysis of the cluster expansion for the case of undirected graphs , and showed how the resulting motif entropy can be used to analyse time evolving networks [1]. In this paper we extend this work to the case of directed graphs to compute thermodynamic quantities including energy, entropy and temperature for the directed network. The three thermodynamic quantities constitute the thermodynamic framework for the analysis of directed network evolution. We apply our thermodynamic framework to the financial and biological domains to represent real world complex systems as time-varying directed networks. Experimental results successfully demonstrate the effectiveness of the thermodynamic framework in representing the evolution of directed network structure and anomalous event detection.},
  archive      = {J_PR},
  author       = {Dongdong Chen and Xingchen Guo and Jianjia Wang and Jiatong Liu and Zhihong Zhang and Edwin R. Hancock},
  doi          = {10.1016/j.patcog.2021.107872},
  journal      = {Pattern Recognition},
  pages        = {107872},
  shortjournal = {Pattern Recognition},
  title        = {Thermodynamic motif analysis for directed stock market networks},
  volume       = {114},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep ancient roman republican coin classification via
feature fusion and attention. <em>PR</em>, <em>114</em>, 107871. (<a
href="https://doi.org/10.1016/j.patcog.2021.107871">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We perform the classification of ancient Roman Republican coins via recognizing their reverse motifs where various objects, faces, scenes, animals, and buildings are minted along with legends. Most of these coins are eroded due to their age and varying degrees of preservation, thereby affecting their informative attributes for visual recognition. Changes in the positions of principal symbols on the reverse motifs also cause huge variations among the coin types. Lastly, in-plane orientations, uneven illumination, and a moderate background clutter further make the classification task non-trivial and challenging. To this end, we present a novel network model, CoinNet, that employs compact bilinear pooling, residual groups, and feature attention layers. Furthermore, we gathered the largest and most diverse image dataset of the Roman Republican coins that contains more than 18,000 images belonging to 228 different reverse motifs. On this dataset, our model achieves a classification accuracy of more than 98\% and outperforms the conventional bag-of-visual-words based approaches and more recent state-of-the-art deep learning methods . We also provide a detailed ablation study of our network and its generalization capability.},
  archive      = {J_PR},
  author       = {Hafeez Anwar and Saeed Anwar and Sebastian Zambanini and Fatih Porikli},
  doi          = {10.1016/j.patcog.2021.107871},
  journal      = {Pattern Recognition},
  pages        = {107871},
  shortjournal = {Pattern Recognition},
  title        = {Deep ancient roman republican coin classification via feature fusion and attention},
  volume       = {114},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning robust feature transformation for domain
adaptation. <em>PR</em>, <em>114</em>, 107870. (<a
href="https://doi.org/10.1016/j.patcog.2021.107870">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is a growing importance of feature extraction in transferring valuable knowledge from a source domain to a different but related target domain. However, when the target data are contaminated by unpredictable and complex noises, the ability of most existing feature extraction methods would be limited. In this paper, we deeply investigate the robust property of Kernel Mean P-Power Error Loss (KMPE-Loss), and thus propose a novel Robust Transfer Feature Learning (RTFL) method to enhance the robustness of domain adaptation . The key idea of RTFL is to learn a shared transformation by: 1) detecting and neglecting the contaminated target points without any specific assumption on noises; 2) reconstructing the remaining clean target points using the corresponding source-domain neighborhood; 3) incorporating a relative entropy based regularization to reap theoretic advantages. Consequently, the distribution difference between two domains is accurately reduced for knowledge transfer. We propose an alternative procedure to optimize RTFL with explicitly guaranteed convergence. As an extension, the transformation based matrix in RTFL is restricted to a small dimension basis, admitting the highly reduced computation complexity. Extensive experiments in various domain adaptation tasks demonstrate the superiority of our methods.},
  archive      = {J_PR},
  author       = {Wei Wang and Hao Wang and Zhi-Yong Ran and Ran He},
  doi          = {10.1016/j.patcog.2021.107870},
  journal      = {Pattern Recognition},
  pages        = {107870},
  shortjournal = {Pattern Recognition},
  title        = {Learning robust feature transformation for domain adaptation},
  volume       = {114},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Refining a k-nearest neighbor graph for a computationally
efficient spectral clustering. <em>PR</em>, <em>114</em>, 107869. (<a
href="https://doi.org/10.1016/j.patcog.2021.107869">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spectral clustering became a popular choice for data clustering for its ability of uncovering clusters of different shapes. However, it is not always preferable over other clustering methods due to its computational demands. One of the effective ways to bypass these computational demands is to perform spectral clustering on a subset of points (data representatives) then generalize the clustering outcome, this is known as approximate spectral clustering (ASC). ASC uses sampling or quantization to select data representatives. This makes it vulnerable to 1) performance inconsistency (since these methods have a random step either in initialization or training), 2) local statistics loss (because the pairwise similarities are extracted from data representatives instead of data points). We proposed a refined version of k k -nearest neighbor graph, in which we keep data points and aggressively reduce number of edges for computational efficiency. Local statistics were exploited to keep the edges that do not violate the intra-cluster distances and nullify all other edges in the k k -nearest neighbor graph. We also introduced an optional step to automatically select the number of clusters C C . The proposed method was tested on synthetic and real datasets. Compared to ASC methods, the proposed method delivered a consistent performance despite significant reduction of edges.},
  archive      = {J_PR},
  author       = {Mashaan Alshammari and John Stavrakakis and Masahiro Takatsuka},
  doi          = {10.1016/j.patcog.2021.107869},
  journal      = {Pattern Recognition},
  pages        = {107869},
  shortjournal = {Pattern Recognition},
  title        = {Refining a k-nearest neighbor graph for a computationally efficient spectral clustering},
  volume       = {114},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-task learning for gait-based identity recognition and
emotion recognition using attention enhanced temporal graph
convolutional network. <em>PR</em>, <em>114</em>, 107868. (<a
href="https://doi.org/10.1016/j.patcog.2021.107868">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human gait conveys significant information that can be used for identity recognition and emotion recognition. Recent studies have focused more on gait identity recognition than emotion recognition and regarded these two recognition tasks as independent and unrelated. How to train a unified model to effectively recognize the identity and emotion from gait at the same time is a novel and challenging problem. In this paper, we propose a novel Attention Enhanced Temporal Graph Convolutional Network (AT-GCN) for gait-based recognition and motion prediction. Enhanced by spatial and temporal attention, the proposed model can capture discriminative features in spatial dependency and temporal dynamics. We also present a multi-task learning architecture, which can jointly learn representations for multiple tasks. It helps the emotion recognition task with limited data considerably benefit from the identity recognition task and helps the recognition tasks benefit from the auxiliary prediction task. Furthermore, we present a new dataset (EMOGAIT) that consists of 1, 440 real gaits, annotated with identity and emotion labels. Experimental results on two datasets demonstrate the effectiveness of our approach and show that our approach achieves substantial improvements over mainstream methods for identity recognition and emotion recognition.},
  archive      = {J_PR},
  author       = {Weijie Sheng and Xinde Li},
  doi          = {10.1016/j.patcog.2021.107868},
  journal      = {Pattern Recognition},
  pages        = {107868},
  shortjournal = {Pattern Recognition},
  title        = {Multi-task learning for gait-based identity recognition and emotion recognition using attention enhanced temporal graph convolutional network},
  volume       = {114},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Spatial context-aware network for salient object detection.
<em>PR</em>, <em>114</em>, 107867. (<a
href="https://doi.org/10.1016/j.patcog.2021.107867">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Salient Object Detection (SOD) is a fundamental problem in the field of computer vision . This paper presents a novel Spatial Context-Aware Network (SCA-Net) for SOD in images. Compared with other recent deep learning based SOD algorithms , SCA-Net can more effectively aggregate multi-level deep features. A Long-Path Context Module (LPCM) is employed to grant better discrimination ability to feature maps that incorporate coarse global information. Consequently, a more accurate initial saliency map can be obtained to facilitate subsequent predictions. SCA-Net also adopts a Short-Path Context Module (SPCM) to progressively enforce the interaction between local contextual cues and global features. Extensive experiments on five large-scale benchmarks demonstrate that SCA-Net achieves favorable performance against very recent state-of-the-art algorithms.},
  archive      = {J_PR},
  author       = {Yuqiu Kong and Mengyang Feng and Xin Li and Huchuan Lu and Xiuping Liu and Baocai Yin},
  doi          = {10.1016/j.patcog.2021.107867},
  journal      = {Pattern Recognition},
  pages        = {107867},
  shortjournal = {Pattern Recognition},
  title        = {Spatial context-aware network for salient object detection},
  volume       = {114},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Online anomaly detection in surveillance videos with
asymptotic bound on false alarm rate. <em>PR</em>, <em>114</em>, 107865.
(<a href="https://doi.org/10.1016/j.patcog.2021.107865">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomaly detection in surveillance videos is attracting an increasing amount of attention. Despite the competitive performance of recent methods, they lack theoretical performance analysis, particularly due to the complex deep neural network architectures used in decision making. Additionally, online decision making is an important but mostly neglected factor in this domain. Much of the existing methods that claim to be online, depend on batch or offline processing in practice. Motivated by these research gaps, we propose an online anomaly detection method in surveillance videos with asymptotic bounds on the false alarm rate , which in turn provides a clear procedure for selecting a proper decision threshold that satisfies the desired false alarm rate . Our proposed algorithm consists of a multi-objective deep learning module along with a statistical anomaly detection module, and its effectiveness is demonstrated on several publicly available data sets where we outperform the state-of-the-art algorithms. All codes are available at https://github.com/kevaldoshi17/Prediction-based-Video-Anomaly-Detection- .},
  archive      = {J_PR},
  author       = {Keval Doshi and Yasin Yilmaz},
  doi          = {10.1016/j.patcog.2021.107865},
  journal      = {Pattern Recognition},
  pages        = {107865},
  shortjournal = {Pattern Recognition},
  title        = {Online anomaly detection in surveillance videos with asymptotic bound on false alarm rate},
  volume       = {114},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Automatic analysis of artistic paintings using
information-based measures. <em>PR</em>, <em>114</em>, 107864. (<a
href="https://doi.org/10.1016/j.patcog.2021.107864">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The artistic community is increasingly relying on automatic computational analysis for authentication and classification of artistic paintings. In this paper, we identify hidden patterns and relationships present in artistic paintings by analysing their complexity, a measure that quantifies the sum of characteristics of an object. Specifically, we apply Normalized Compression (NC) and the Block Decomposition Method (BDM) to a dataset of 4,266 paintings from 91 authors and examine the potential of these information-based measures as descriptors of artistic paintings. Both measures consistently described the equivalent types of paintings, authors, and artistic movements. Moreover, combining the NC with a measure of the roughness of the paintings creates an efficient stylistic descriptor. Furthermore, by quantifying the local information of each painting, we define a fingerprint that describes critical information regarding the artists’ style, their artistic influences, and shared techniques. More fundamentally, this information describes how each author typically composes and distributes the elements across the canvas and, therefore, how their work is perceived. Finally, we demonstrate that regional complexity and two-point height difference correlation function are useful auxiliary features that improve current methodologies in style and author classification of artistic paintings. The whole study is supported by an extensive website ( http://panther.web.ua.pt ) for fast author characterization and authentication .},
  archive      = {J_PR},
  author       = {Jorge Miguel Silva and Diogo Pratas and Rui Antunes and Sérgio Matos and Armando J. Pinho},
  doi          = {10.1016/j.patcog.2021.107864},
  journal      = {Pattern Recognition},
  pages        = {107864},
  shortjournal = {Pattern Recognition},
  title        = {Automatic analysis of artistic paintings using information-based measures},
  volume       = {114},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hierarchical distillation learning for scalable person
search. <em>PR</em>, <em>114</em>, 107862. (<a
href="https://doi.org/10.1016/j.patcog.2021.107862">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing person search methods typically focus on improving person detection accuracy. This ignores the model inference efficiency, which however is fundamentally significant for real-world applications. In this work, we address this limitation by investigating the scalability problem of person search involving both model accuracy and inference efficiency simultaneously. Specifically, we formulate a Hierarchical Distillation Learning (HDL) approach. With HDL, we aim to comprehensively distil the knowledge of a strong teacher model with strong learning capability to a lightweight student model with weak learning capability. To facilitate the HDL process, we design a simple and powerful teacher model for joint learning of person detection and person re-identification matching in unconstrained scene images. Extensive experiments show the modelling advantages and cost-effectiveness superiority of HDL over the state-of-the-art person search methods on three large person search benchmarks: CUHK-SYSU, PRW, and DukeMTMC-PS.},
  archive      = {J_PR},
  author       = {Wei Li and Shaogang Gong and Xiatian Zhu},
  doi          = {10.1016/j.patcog.2021.107862},
  journal      = {Pattern Recognition},
  pages        = {107862},
  shortjournal = {Pattern Recognition},
  title        = {Hierarchical distillation learning for scalable person search},
  volume       = {114},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Optical flow and scene flow estimation: A survey.
<em>PR</em>, <em>114</em>, 107861. (<a
href="https://doi.org/10.1016/j.patcog.2021.107861">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motion analysis is one of the most fundamental and challenging problems in the field of computer vision , which can be widely applied in many areas, such as autonomous driving , action recognition, scene understanding, and robotics. In general, the displacement field between subsequent frames can be divided into two types: optical flow and scene flow. The optical flow represents the pixel motion of adjacent frames. In contrast, the scene flow is a 3D motion field of the dynamic scene between two frames. Traditional approaches for the estimation of optical flow and scene flow usually leverage the variational technique, which can be solved as an energy minimization process. In recent years, deep learning has emerged as a powerful technique for learning feature representations directly from data. It has led to remarkable progress in the field of optical flow and scene flow estimation. In this paper, we provide a comprehensive survey of optical flow and scene flow estimation. First, we briefly review the pioneering approaches that use variational technique and then we delve in detail into the deep learning-based approaches. Furthermore, we present insightful observations on evaluation issues, specifically benchmark datasets, evaluation metrics , and state-of-the-art performance. Finally, we give the promising directions for future research. To the best of our knowledge, we are the first to review both optical flow and scene flow estimation, and the first to cover both traditional and deep learning-based approaches.},
  archive      = {J_PR},
  author       = {Mingliang Zhai and Xuezhi Xiang and Ning Lv and Xiangdong Kong},
  doi          = {10.1016/j.patcog.2021.107861},
  journal      = {Pattern Recognition},
  pages        = {107861},
  shortjournal = {Pattern Recognition},
  title        = {Optical flow and scene flow estimation: A survey},
  volume       = {114},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bound estimation-based safe acceleration for maximum margin
of twin spheres machine with pinball loss. <em>PR</em>, <em>114</em>,
107860. (<a href="https://doi.org/10.1016/j.patcog.2021.107860">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Maximum margin of twin spheres support vector machine (MMTSSVM) is an efficient method for imbalanced data classification. As an extension to enhance noise insensitivity of MMTSSVM, MMTSSVM with pinball loss (Pin-MMTSM) has a good generalization performance . However, it is not efficient enough for large-scale data. Inspired by the sparse solution of SVMs, in this paper, we propose a safe accelerative approach to reduce the computational cost. Unlike the existing safe screening rules, where only one variable changes with the parameters. We utilize bound estimation-based to derive the upper and lower bounds of center and radius. With our approach, the inactive samples are discarded before solving the problem, thus it can reduce the computational cost. One important advantage of our approach is safety, i.e., we can obtain the same solution as solving original problem both in linear and non-linear cases. Moreover, it is obvious that our acceleration approach is independent of the solver. To further accelerate the computational speed, a decomposition method is employed. Experiments on three artificial datasets and twelve benchmark datasets clearly demonstrate the effectiveness of our approach. At last, we extend bound estimation-based method to ν ν -SVM, theoretical analysis and experimental results both verify its feasibility and effectiveness.},
  archive      = {J_PR},
  author       = {Min Yuan and Yitian Xu},
  doi          = {10.1016/j.patcog.2021.107860},
  journal      = {Pattern Recognition},
  pages        = {107860},
  shortjournal = {Pattern Recognition},
  title        = {Bound estimation-based safe acceleration for maximum margin of twin spheres machine with pinball loss},
  volume       = {114},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Joint stroke classification and text line grouping in online
handwritten documents with edge pooling attention networks. <em>PR</em>,
<em>114</em>, 107859. (<a
href="https://doi.org/10.1016/j.patcog.2021.107859">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stroke classification and text line grouping are important tasks in online handwritten document segmentation. In the past, the two tasks were usually performed using different models which are trained independently and perform sequentially. This cannot optimize the integration of contextual information and the system may suffer from error accumulation in stroke classification. In this paper, we propose a method for joint text/non-text stroke classification and text line grouping in online handwritten documents using attention based graph neural network . In our framework, the stroke classification and text line grouping problems are formulated as node classification and node clustering problems in a relational graph , which is constructed based on the temporal and spatial relationship between strokes. We propose a new graph network architecture , called edge pooling attention network (EPAT) to efficiently aggregate information between the features of neighboring nodes and edges. The proposed model is trained by multi-task learning with cross entropy loss for node classification and distance metric loss for node clustering. In experiments on two online handwritten document datasets IAMOnDo and Kondate, the proposed method is demonstrated effective, yielding superior performance in both stroke classification and text line grouping.},
  archive      = {J_PR},
  author       = {Jun-Yu Ye and Yan-Ming Zhang and Qing Yang and Cheng-Lin Liu},
  doi          = {10.1016/j.patcog.2021.107859},
  journal      = {Pattern Recognition},
  pages        = {107859},
  shortjournal = {Pattern Recognition},
  title        = {Joint stroke classification and text line grouping in online handwritten documents with edge pooling attention networks},
  volume       = {114},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An efficient computational algorithm for hausdorff distance
based on points-ruling-out and systematic random sampling. <em>PR</em>,
<em>114</em>, 107857. (<a
href="https://doi.org/10.1016/j.patcog.2021.107857">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a novel algorithm for fast and accurate Hausdorff distance (HD) computation. The Hausdorff distance is used to measure the similarity between two point sets in various applications. However, it is hard to compute the HD algorithm efficiently between very large-scale point sets while ensuring the accuracy of the HD. The directed HD algorithm has two loops (called the outer loop and the inner loop) for calculating MAX-MIN distance, and the state-of-the-art algorithms, such as the Early break method and the Diffusion search method, focused on reducing the iterations of the inner loop. Our algorithm, however, concentrates on reducing the iterations of the outer loop. The proposed method simultaneously computes the temporary HD and temporary minimum distances of points corresponding to the outer loop using the opposite HD computation with very small systematic samples. Thereafter, a strategy of ruling out is employed to exclude non-contributing points. The new approach reduces the problems of different grid sizes and highly overlapping point sets as well as the very large-scale point sets. 3-D point clouds and real brain tumor segmentation (MRI 3-D volumes) are used for comparing the performance of the proposed algorithm and the state-of-the-art HD algorithms. In experimental results with 3-D point clouds, the proposed method is more than at least 1.5 times as faster as the compared algorithms. And, in experimental results with MRI 3-D volumes, the proposed method achieves a better performance than the compared algorithms over all pairs regardless of the grid size. Thus, as a whole, the proposed algorithm outperforms the compared algorithms.},
  archive      = {J_PR},
  author       = {Jegoon Ryu and Sei-ichiro Kamata},
  doi          = {10.1016/j.patcog.2021.107857},
  journal      = {Pattern Recognition},
  pages        = {107857},
  shortjournal = {Pattern Recognition},
  title        = {An efficient computational algorithm for hausdorff distance based on points-ruling-out and systematic random sampling},
  volume       = {114},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Automatic medical image interpretation: State of the art and
future directions. <em>PR</em>, <em>114</em>, 107856. (<a
href="https://doi.org/10.1016/j.patcog.2021.107856">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic Natural language interpretation of medical images is an emerging field of Artificial Intelligence (AI). The task combines two fields of AI; computer vision and natural language processing. This is a challenging task that goes beyond object detection, segmentation, and classification because it also requires the understanding of the relationship between different objects of an image and the actions performed by these objects as visual representations. Image interpretation is helpful in many tasks like helping visually impaired persons, information retrieval, early childhood learning, producing human like natural interaction between robots, and many more applications. Recently this work fascinated researchers to use the same approach by using more complex biomedical images. It has been applied from generating single sentence captions to multi sentence paragraph descriptions. Medical image captioning can assist and speed up the diagnosis process of medical professionals and generated report can be used for many further tasks. This is a comprehensive review of recent years’ research of medical image captioning published in different international conferences and journals. Their common parameters are extracted to compare their methods, performance, strengths, limitations, and our recommendations are discussed. Further publicly available datasets and evaluation measures used for deep-learning based captioning of medical images are also discussed.},
  archive      = {J_PR},
  author       = {Hareem Ayesha and Sajid Iqbal and Mehreen Tariq and Muhammad Abrar and Muhammad Sanaullah and Ishaq Abbas and Amjad Rehman and Muhammad Farooq Khan Niazi and Shafiq Hussain},
  doi          = {10.1016/j.patcog.2021.107856},
  journal      = {Pattern Recognition},
  pages        = {107856},
  shortjournal = {Pattern Recognition},
  title        = {Automatic medical image interpretation: State of the art and future directions},
  volume       = {114},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning deep discriminative embeddings via joint rescaled
features and log-probability centers. <em>PR</em>, <em>114</em>, 107852.
(<a href="https://doi.org/10.1016/j.patcog.2021.107852">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently softmax based loss functions have surged to advance image classification and face verification . Most efforts boost discrimination of the softmax loss by using novel angular margins in varying ways, but few analyze where the discrimination truly comes from whilst considering the power of relieving the overfitting to enhance the softmax loss. In this paper, we firstly delve into such mainstream of softmax based loss functions in theory, and recognize the importance of easing overfitting to the softmax loss. In terms of such analysis, this paper intends to bring the softmax loss up to the competitive level with current well-behaved loss functions. We do this in two ways: (1) regularizing the softmax to relieve the overfitting by learning the log-probability centers, and (2) rescaling deep embeddings of the softmax with a constant scale to further enhance inter-class separability in Euclidean space. We call the resulting loss function rLogCenter loss for short. Simple and interpretable as our loss is, it guides CNNs to induce performance gains in the experiments of both image classification and face verification .},
  archive      = {J_PR},
  author       = {Huayue Cai and Xiang Zhang and Long Lan and Guohua Dong and Chuanfu Xu and Xinwang Liu and Zhigang Luo},
  doi          = {10.1016/j.patcog.2021.107852},
  journal      = {Pattern Recognition},
  pages        = {107852},
  shortjournal = {Pattern Recognition},
  title        = {Learning deep discriminative embeddings via joint rescaled features and log-probability centers},
  volume       = {114},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Assessing partially ordered clustering in a multicriteria
comparative context. <em>PR</em>, <em>114</em>, 107850. (<a
href="https://doi.org/10.1016/j.patcog.2021.107850">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study considers the task of clustering for data characterized by peculiar quantitative features in that they express performance according to different indicators or criteria. Performance is supposed to be optimized in one way or the other, i.e. maximized or minimized. This peculiar type of data introduces a comparative context that is not generally taken into account in the field of pattern recognition, in general, and clustering, in particular. In the present study, we introduce different concepts and develop tools that facilitate the evaluation of data partitions in this comparative context leading to the consideration of asymmetric preference relationships between objects and between clusters. We show their usefulness on the basis of artificial data and also by analyzing the results produced on real data by means of clustering methods .},
  archive      = {J_PR},
  author       = {Jean Rosenfeld and Yves De Smet and Olivier Debeir and Christine Decaestecker},
  doi          = {10.1016/j.patcog.2021.107850},
  journal      = {Pattern Recognition},
  pages        = {107850},
  shortjournal = {Pattern Recognition},
  title        = {Assessing partially ordered clustering in a multicriteria comparative context},
  volume       = {114},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Memetic differential evolution methods for clustering
problems. <em>PR</em>, <em>114</em>, 107849. (<a
href="https://doi.org/10.1016/j.patcog.2021.107849">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Euclidean Minimum Sum-of-Squares Clustering ( MSSC ) is one of the most important models for the clustering problem . Due to its NP-hardness, the problem continues to receive much attention in the scientific literature and several heuristic procedures have been proposed. Recent research has been devoted to the improvement of the classical K-MEANS algorithm, either by suitably selecting its starting configuration or by using it as a local search method within a global optimization algorithm . This paper follows this last approach by proposing a new implementation of a Memetic Differential Evolution ( MDE ) algorithm specifically designed for the MSSC problem and based on the repeated execution of K-MEANS from selected configurations. In this paper we describe how to adapt MDE to the clustering problem and we show, through a vast set of numerical experiments, that the proposed method has very good quality, measured in terms of the minimization of the objective function, as well as a very good efficiency, measured in the number of calls to the local optimization routine, with respect to state of the art methods .},
  archive      = {J_PR},
  author       = {Pierluigi Mansueto and Fabio Schoen},
  doi          = {10.1016/j.patcog.2021.107849},
  journal      = {Pattern Recognition},
  pages        = {107849},
  shortjournal = {Pattern Recognition},
  title        = {Memetic differential evolution methods for clustering problems},
  volume       = {114},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-task contrastive learning for automatic CT and x-ray
diagnosis of COVID-19. <em>PR</em>, <em>114</em>, 107848. (<a
href="https://doi.org/10.1016/j.patcog.2021.107848">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computed tomography (CT) and X-ray are effective methods for diagnosing COVID-19. Although several studies have demonstrated the potential of deep learning in the automatic diagnosis of COVID-19 using CT and X-ray, the generalization on unseen samples needs to be improved. To tackle this problem, we present the contrastive multi-task convolutional neural network (CMT-CNN), which is composed of two tasks. The main task is to diagnose COVID-19 from other pneumonia and normal control. The auxiliary task is to encourage local aggregation though a contrastive loss: first, each image is transformed by a series of augmentations (Poisson noise, rotation, etc.). Then, the model is optimized to embed representations of a same image similar while different images dissimilar in a latent space. In this way, CMT-CNN is capable of making transformation-invariant predictions and the spread-out properties of data are preserved. We demonstrate that the apparently simple auxiliary task provides powerful supervisions to enhance generalization. We conduct experiments on a CT dataset (4,758 samples) and an X-ray dataset (5,821 samples) assembled by open datasets and data collected in our hospital. Experimental results demonstrate that contrastive learning (as plugin module) brings solid accuracy improvement for deep learning models on both CT (5.49\%-6.45\%) and X-ray (0.96\%-2.42\%) without requiring additional annotations. Our codes are accessible online.},
  archive      = {J_PR},
  author       = {Jinpeng Li and Gangming Zhao and Yaling Tao and Penghua Zhai and Hao Chen and Huiguang He and Ting Cai},
  doi          = {10.1016/j.patcog.2021.107848},
  journal      = {Pattern Recognition},
  pages        = {107848},
  shortjournal = {Pattern Recognition},
  title        = {Multi-task contrastive learning for automatic CT and X-ray diagnosis of COVID-19},
  volume       = {114},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-task face analyses through adversarial learning.
<em>PR</em>, <em>114</em>, 107837. (<a
href="https://doi.org/10.1016/j.patcog.2021.107837">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The inherent relations among multiple face analysis tasks, such as landmark detection, head pose estimation , gender recognition and face attribute estimation are crucial to boost the performance of each task, but have not been thoroughly explored since typically these multiple face analysis tasks are handled as separate tasks. In this paper, we propose a novel deep multi-task adversarial learning method to localize facial landmark, estimate head pose and recognize gender jointly or estimate multiple face attributes simultaneously through exploring their dependencies from both image representation-level and label-level. Specifically, the proposed method consists of a deep recognition network R R and a discriminator D D . The deep recognition network is used to learn the shared middle-level image representation and conducts multiple face analysis tasks simultaneously. Through multi-task learning mechanism, the recognition network explores the dependencies among multiple face analysis tasks from image representation-level. The discriminator is introduced to enforce the distribution of the multiple face analysis tasks to converge to that inherent in the ground-truth labels. During training, the recognizer tries to confuse the discriminator, while the discriminator competes with the recognizer through distinguishing the predicted label combination from the ground-truth one. Though adversarial learning, we explore the dependencies among multiple face analysis tasks from label-level. Experimental results on benchmark databases demonstrate the effectiveness of the proposed method for multi-task face analyses.},
  archive      = {J_PR},
  author       = {Shangfei Wang and Shi Yin and Longfei Hao and Guang Liang},
  doi          = {10.1016/j.patcog.2021.107837},
  journal      = {Pattern Recognition},
  pages        = {107837},
  shortjournal = {Pattern Recognition},
  title        = {Multi-task face analyses through adversarial learning},
  volume       = {114},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A new EM algorithm for flexibly tied GMMs with large number
of components. <em>PR</em>, <em>114</em>, 107836. (<a
href="https://doi.org/10.1016/j.patcog.2021.107836">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gaussian mixture models (GMMs) are a family of generative models used extensively in many machine learning applications. The modeling power of GMMs is directly linked to the number of components. Memory, computational load and lack of enough data hinders using GMMs with large number of components. To tackle this problem, GMMs with a tying scheme that we call flexibly tied GMM was proposed in the literature of the speech recognition community. In the literature, a coordinate-descent EM algorithm was proposed for estimating the parameters of flexibly tied GMMs. In this paper, we aim at reintroducing flexibly tied GMMs to the pattern recognition community. We rigorously investigate various optimization methods and see none of the out-of-the-box optimization methods can solve the parameter estimation problem due to the complexity of the cost function. To this end, we develop a fast Newton EM algorithm that combined with the coordinate descent EM algorithm, it significantly outperforms pure coordinate descent EM and all other optimization algorithms . Furthermore, we propose a computation factorization technique to increase the speed and decrease memory requirement of both Newton and coordinate descent EM algorithms in the case of large number of components. Experimental results on many datasets verifies the efficacy of the proposed algorithm. It also verifies that flexibly tied GMM outperforms both basic GMM and other types of tied GMMs on the datasets in terms of the log-likelihood. We also evaluate the performance of flexibly tied GMM on a clustering problem , and show that it can outperform basic GMM and kmeans algorithm.},
  archive      = {J_PR},
  author       = {Hadi Asheri and Reshad Hosseini and Babak Nadjar Araabi},
  doi          = {10.1016/j.patcog.2021.107836},
  journal      = {Pattern Recognition},
  pages        = {107836},
  shortjournal = {Pattern Recognition},
  title        = {A new EM algorithm for flexibly tied GMMs with large number of components},
  volume       = {114},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Internet financing credit risk evaluation using multiple
structural interacting elastic net feature selection. <em>PR</em>,
<em>114</em>, 107835. (<a
href="https://doi.org/10.1016/j.patcog.2021.107835">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Internet financing is an important alternative to banks where individuals or SMEs borrow money using online trading platforms. A central problem for internet financing is how to identify the most influential factors that are closely related to the credit risks. This problem is inherently challenging because the raw data of internet financing is often associated with complex structural correlations and usually contains many irrelevant and redundant features. To effectively identify the most salient features for credit risk evaluation in internet financing, we develop a new multiple structural interacting elastic net model for feature selection (MSIEN). Our idea is based on converting the original vectorial features into structure-based feature graph representations to encapsulate structural relationship between pairwise samples, and defining two new information theoretic criteria. One criterion maximizes joint relevance of different pairwise feature combinations in relation to the target feature graph and the other minimizes the redundancy between pairwise features. Then two structural interaction matrices are obtained with the elements representing the proposed information theoretic measures . To identify the most informative features, we formulate a new optimization model which combines the interaction matrices and an elastic net regularization model for the feature subset selection problem. We exploit an efficient iterative optimization algorithm to solve the proposed problem and also provide the theoretical analyses on its convergence property and computational complexity . Finally, experimental results on datasets of internet financing demonstrate the effectiveness of the proposed MSIEN method.},
  archive      = {J_PR},
  author       = {Lixin Cui and Lu Bai and Yanchao Wang and Xin Jin and Edwin R. Hancock},
  doi          = {10.1016/j.patcog.2021.107835},
  journal      = {Pattern Recognition},
  pages        = {107835},
  shortjournal = {Pattern Recognition},
  title        = {Internet financing credit risk evaluation using multiple structural interacting elastic net feature selection},
  volume       = {114},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). GuessWhich? Visual dialog with attentive memory network.
<em>PR</em>, <em>114</em>, 107823. (<a
href="https://doi.org/10.1016/j.patcog.2021.107823">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual dialog is a task that two agents: Question-BOT (Q-BOT) and Answer-BOT (A-BOT), which communicate in natural language on the situation of information asymmetry . Q-BOT generates questions based on an image caption and a historical dialog. A-BOT answers the questions grounded on the image. Moreover, we play a cooperative ‘image guessing’ game between Q-BOT and A-BOT, so that Q-BOT can select an unseen image from a set of images. However, as the valid information of the image caption and the historical dialog fades along the interaction, existing methods usually generate irrelevant and homogenous questions, which are worthless to the visual dialog system . To tackle this issue, we propose an A ttentive M emory N etwork (AMN) to fully exploit the image caption and historical dialog information. Specifically, the attentive memory network mainly consists of a memory network and a fusion module. The memory network holds long term historical dialog information and gives each round of the dialog a different weight. Aside from the historical dialog information, the fusion module in Q-BOT and A-BOT further uses the image caption and the image feature , respectively. The caption information assists Q-BOT with the attentive generation of the questions, and the image feature helps A-BOT produce precise answers. With the AMN, the generated questions are diverse and concentrated, and the corresponding answers are accurate. The experimental results on VisDial v1.0 show the effectiveness of our proposed model, which outperforms the state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Lei Zhao and Xinyu Lyu and Jingkuan Song and Lianli Gao},
  doi          = {10.1016/j.patcog.2021.107823},
  journal      = {Pattern Recognition},
  pages        = {107823},
  shortjournal = {Pattern Recognition},
  title        = {GuessWhich? visual dialog with attentive memory network},
  volume       = {114},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Discriminative shared transform learning for sketch to image
matching. <em>PR</em>, <em>114</em>, 107815. (<a
href="https://doi.org/10.1016/j.patcog.2021.107815">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sketch to digital image matching refers to the problem of matching a sketch image (often drawn by hand or created by a software) against a gallery of digital images (captured via an acquisition device such as a digital camera). Automated sketch to digital image matching has applicability in several day to day tasks such as similar object image retrieval , forensic sketch matching in law enforcement scenarios, or profile linking using caricature face images on social media. As opposed to the digital images, sketch images are generally edge-drawings containing limited (or no) textural or colour based information. Further, there is no single technique for sketch generation, which often results in varying artistic or software styles, along with the interpretation bias of the individual creating the sketch. Beyond the variations observed across the two domains (sketch and digital image), automated sketch to digital image matching is further marred by the challenge of limited training data and wide intra-class variability. In order to address the above problems, this research proposes a novel Discriminative Shared Transform Learning (DSTL) algorithm for sketch to digital image matching. DSTL learns a shared transform for data belonging to the two domains, while modeling the class variations, resulting in discriminative feature learning . Two models have been presented under the proposed DSTL algorithm: (i) Contractive Model (C-Model) and (ii) Divergent Model (D-Model), which have been formulated with different supervision constraints. Experimental analysis on seven datasets for three case studies of sketch to digital image matching demonstrate the efficacy of the proposed approach, highlighting the importance of each component, its input-agnostic behavior, and improved matching performance.},
  archive      = {J_PR},
  author       = {Shruti Nagpal and Maneet Singh and Richa Singh and Mayank Vatsa},
  doi          = {10.1016/j.patcog.2021.107815},
  journal      = {Pattern Recognition},
  pages        = {107815},
  shortjournal = {Pattern Recognition},
  title        = {Discriminative shared transform learning for sketch to image matching},
  volume       = {114},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Automatic pancreas segmentation based on lightweight DCNN
modules and spatial prior propagation. <em>PR</em>, <em>114</em>,
107762. (<a href="https://doi.org/10.1016/j.patcog.2020.107762">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, pancreas segmentation in CT scans has gained more and more attention for computer-assisted diagnosis of inflammation (pancreatitis) or cancer. Despite the thrilling success of deep convolutional neural networks (DCNNs) in automatic pancreas segmentation, the heavy computational complexity of such networks impedes the deployment in clinical applications. To alleviate this issue, this paper establishes a novel end-to-end DCNN model for pursuing high-accurate automatic pancreas segmentation but with low computational cost. Specifically, built upon a simplified FCN architecture , we propose two novel network modules, named as the scale-transferrable feature fusion module (STFFM) and prior propagation module (PPM), respectively, for pancreas segmentation. Equipped with the scale-transferrable operation, STFFM can learn rich fusion features but with very lightweight network architecture. By dynamically adapting the spatial prior to the input slice data as well as the deep feature maps, PPM enables the network model to explore informative spatial priors for pancreas segmentation. Comprehensive experiments on the NIH dataset and the MSD dataset are conducted to evaluate the proposed approach. The obtained experimental results demonstrate that our approach can effectively reduce the computational cost and simultaneously archive the outperforming performance when compared to the state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Dingwen Zhang and Jiajia Zhang and Qiang Zhang and Jungong Han and Shu Zhang and Junwei Han},
  doi          = {10.1016/j.patcog.2020.107762},
  journal      = {Pattern Recognition},
  pages        = {107762},
  shortjournal = {Pattern Recognition},
  title        = {Automatic pancreas segmentation based on lightweight DCNN modules and spatial prior propagation},
  volume       = {114},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). ACN: Occlusion-tolerant face alignment by attentional
combination of heterogeneous regression networks. <em>PR</em>,
<em>114</em>, 107761. (<a
href="https://doi.org/10.1016/j.patcog.2020.107761">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents the Attentional Combination Network (ACN), which is a highly accurate face alignment method that is tolerant of occlusion. The method combines a coordinate regression network and a heatmap regression network with a spatial attention . The coordinate regression generates the coordinates of facial landmark points directly such that they are fitted to the input face on the whole. The heatmap regression generates the heatmap of facial landmark points such that each channel provides good localization of the detail of its facial landmark point. These independent regressions compensate for each other complementarily such that the overall fitting tendency of the coordinate regression compensates for the inaccurate alignment of the heatmap regression due to missing local information, and the detailed localization of the heatmap regression compensates for the relatively inaccurate alignment of the coordinate regression. The proposed ACN uses coordinate-to-heatmap and the heatmap-to-coordinate conversion networks to combine two heterogeneous regressions, and to generate the final coordinates of the facial landmark points. The ACN use the spatial attention mechanism to effectively reject impeditive local features that are caused by the occlusion. In experiments on several benchmarks, the proposed ACN achieved state-of-the-art accuracy},
  archive      = {J_PR},
  author       = {Hyunsung Park and Daijin Kim},
  doi          = {10.1016/j.patcog.2020.107761},
  journal      = {Pattern Recognition},
  pages        = {107761},
  shortjournal = {Pattern Recognition},
  title        = {ACN: Occlusion-tolerant face alignment by attentional combination of heterogeneous regression networks},
  volume       = {114},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Automatic COVID-19 lung infected region segmentation and
measurement using CT-scans images. <em>PR</em>, <em>114</em>, 107747.
(<a href="https://doi.org/10.1016/j.patcog.2020.107747">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {History shows that the infectious disease (COVID-19) can stun the world quickly, causing massive losses to health, resulting in a profound impact on the lives of billions of people, from both a safety and an economic perspective, for controlling the COVID-19 pandemic. The best strategy is to provide early intervention to stop the spread of the disease. In general, Computer Tomography (CT) is used to detect tumors in pneumonia , lungs, tuberculosis, emphysema, or other pleura (the membrane covering the lungs) diseases. Disadvantages of CT imaging system are: inferior soft tissue contrast compared to MRI as it is X-ray-based Radiation exposure. Lung CT image segmentation is a necessary initial step for lung image analysis. The main challenges of segmentation algorithms exaggerated due to intensity in-homogeneity, presence of artifacts, and closeness in the gray level of different soft tissue. The goal of this paper is to design and evaluate an automatic tool for automatic COVID-19 Lung Infection segmentation and measurement using chest CT images. The extensive computer simulations show better efficiency and flexibility of this end-to-end learning approach on CT image segmentation with image enhancement comparing to the state of the art segmentation approaches , namely GraphCut, Medical Image Segmentation (MIS), and Watershed. Experiments performed on COVID-CT-Dataset containing (275) CT scans that are positive for COVID-19 and new data acquired from the EL-BAYANE center for Radiology and Medical Imaging . The means of statistical measures obtained using the accuracy, sensitivity, F-measure, precision, MCC, Dice, Jacquard, and specificity are 0.98, 0.73, 0.71, 0.73, 0.71, 0.71, 0.57, 0.99 respectively; which is better than methods mentioned above. The achieved results prove that the proposed approach is more robust, accurate, and straightforward.},
  archive      = {J_PR},
  author       = {Adel Oulefki and Sos Agaian and Thaweesak Trongtirakul and Azzeddine Kassah Laouar},
  doi          = {10.1016/j.patcog.2020.107747},
  journal      = {Pattern Recognition},
  pages        = {107747},
  shortjournal = {Pattern Recognition},
  title        = {Automatic COVID-19 lung infected region segmentation and measurement using CT-scans images},
  volume       = {114},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). View-graph construction framework for robust and efficient
structure-from-motion. <em>PR</em>, <em>114</em>, 107712. (<a
href="https://doi.org/10.1016/j.patcog.2020.107712">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A view-graph is vital for both the accuracy and robustness of structure-from-motion (SfM). Conventional matrix decomposition techniques treat all edges of view-graph equally; hence, many edge outliers are produced in matching pairs with fewer feature matches. To address this problem, we propose an incremental framework for view-graph construction, where the robustness of matched pairs that have a larger number of feature matches is propagated to their connected images. Given pairwise feature matches, a verified maximum spanning tree (VMST) is first constructed; for each edge in the VMST, we perform a local reconstruction and register its visible cameras. Based on the local reconstruction, pairwise relative geometries are computed and some new epipolar edges are produced. In this way, these newly computed edges inherit the robustness and accuracy of VMST, and by embedding them into VMST, our view-graph is constructed. We feed our view-graph into a standard SfM pipeline and compare this newly formed system with many of state-of-the-art SfM methods. The experimental results demonstrate that our view-graph provides a better foundation for conventional SfM systems, and enables them to reconstruct both general and ambiguous images.},
  archive      = {J_PR},
  author       = {Hainan Cui and Tianxin Shi and Jun Zhang and Pengfei Xu and Yiping Meng and Shuhan Shen},
  doi          = {10.1016/j.patcog.2020.107712},
  journal      = {Pattern Recognition},
  pages        = {107712},
  shortjournal = {Pattern Recognition},
  title        = {View-graph construction framework for robust and efficient structure-from-motion},
  volume       = {114},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptive super-resolution for person re-identification with
low-resolution images. <em>PR</em>, <em>114</em>, 107682. (<a
href="https://doi.org/10.1016/j.patcog.2020.107682">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person re-identification is challenging with low-resolution query and high-resolution gallery images. To address the resolution mismatch, many methods perform super-resolution (SR) on low-resolution queries with specifying a single scale factor. However, using a single SR module, whichever scale factor is specified, always brings both advantages and drawbacks in recovering and identifying identity information. A larger scale factor recovers more details but produces excessive artifacts, while a smaller one is on the contrary. To exploit their complementary property for more robust recovery and identification, we propose the Adaptive Person Super-Resolution (APSR) model. APSR jointly trains and fuses multiple SR modules based on their generated visual contents, to fully compensate and learn the complementary identity features in an end-to-end manner. To improve the robustness to artifacts during fusion, our model further learns informative features by online dividing and integrating the generated body regions. Extensive experiments verify the effectiveness of our method with state-of-the-art performances.},
  archive      = {J_PR},
  author       = {Ke Han and Yan Huang and Chunfeng Song and Liang Wang and Tieniu Tan},
  doi          = {10.1016/j.patcog.2020.107682},
  journal      = {Pattern Recognition},
  pages        = {107682},
  shortjournal = {Pattern Recognition},
  title        = {Adaptive super-resolution for person re-identification with low-resolution images},
  volume       = {114},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A generalized weighted distance k-nearest neighbor for
multi-label problems. <em>PR</em>, <em>114</em>, 107526. (<a
href="https://doi.org/10.1016/j.patcog.2020.107526">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In multi-label classification, each instance is associated with a set of pre-specified labels. One common approach is to use Binary Relevance (BR) paradigm to learn each label by a base classifier separately. Use of k -Nearest Neighbor (kNN) as the base classifier (denoted as BRkNN) is a simple, descriptive and powerful approach. In binary relevance a highly imbalanced view of dataset is used. However, kNN is known to perform poorly on imbalanced data . One approach to deal with this is to define the distance function in a parametric form and use the training data to adjust the parameters (i.e. adjusting boundaries between classes) by optimizing a performance measure customized for imbalanced data e.g. F F -measure. Prototype Weighting (PW) scheme presented in the literature (Paredes &amp; Vidal, 2006) uses gradient descent to specify the parameters by minimizing the classification error-rate on training data. This paper presents a generalized version of PW. First, instead of minimizing the error-rate proposed in PW, the generalized PW supports also other objective functions that use elements of confusion matrix (including F F -measure). Second, PW originally presented for 1NN is extended to the general case of kNN (i.e., k &gt; = 1 k&amp;gt;=1 ). For problems having highly overlapped classes, it is expected to perform better since a value of k &gt; 1 k&amp;gt;1 produces smoother decision boundaries which in turn can improve generalization. In multi-label problems with many labels or problems with highly overlapped classes, the proposed generalized PW is expected to significantly improve the performance as it involves many decision boundaries. The performance of the proposed method has been compared with state-of-the-art methods in multi-label classification containing 6 lazy classifiers based on kNN. Experiments show that the proposed method significantly outperforms other methods.},
  archive      = {J_PR},
  author       = {Niloofar Rastin and Mansoor Zolghadri Jahromi and Mohammad Taheri},
  doi          = {10.1016/j.patcog.2020.107526},
  journal      = {Pattern Recognition},
  pages        = {107526},
  shortjournal = {Pattern Recognition},
  title        = {A generalized weighted distance k-nearest neighbor for multi-label problems},
  volume       = {114},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Inferring spatial relations from textual descriptions of
images. <em>PR</em>, <em>113</em>, 107847. (<a
href="https://doi.org/10.1016/j.patcog.2021.107847">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generating an image from its textual description requires both a certain level of language understanding and common sense knowledge about the spatial relations of the physical entities being described. In this work, we focus on inferring the spatial relation between entities, a key step in the process of composing scenes based on text. More specifically, given a caption containing a mention to a subject and the location and size of the bounding box of that subject, our goal is to predict the location and size of an object mentioned in the caption. Previous work did not use the caption text information, but a manually provided relation holding between the subject and the object. In fact, the used evaluation datasets contain manually annotated ontological triplets but no captions, making the exercise unrealistic: a manual step was required; and systems did not leverage the richer information in captions. Here we present a system that uses the full caption, and Relations in Captions (REC-COCO), a dataset derived from MS-COCO which allows to evaluate spatial relation inference from captions directly. Our experiments show that: (1) it is possible to infer the size and location of an object with respect to a given subject directly from the caption; (2) the use of full text allows to place the object better than using a manually annotated relation. Our work paves the way for systems that, given a caption, decide which entities need to be depicted and their respective location and sizes, in order to then generate the final image.},
  archive      = {J_PR},
  author       = {Aitzol Elu and Gorka Azkune and Oier Lopez de Lacalle and Ignacio Arganda-Carreras and Aitor Soroa and Eneko Agirre},
  doi          = {10.1016/j.patcog.2021.107847},
  journal      = {Pattern Recognition},
  pages        = {107847},
  shortjournal = {Pattern Recognition},
  title        = {Inferring spatial relations from textual descriptions of images},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). AG3line: Active grouping and geometry-gradient combined
validation for fast line segment extraction. <em>PR</em>, <em>113</em>,
107834. (<a href="https://doi.org/10.1016/j.patcog.2021.107834">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Line segment detectors based on local image domain passively fit a line segment from a set of pixels, but no constraint on line geometry is set in the grouping process. Therefore, unstable pixels, such as the pixels in grass, clouds, or weak gradient edges, may cause false positives and fractures. This paper proposes the detector named AG3line, which employs an efficient active grouping strategy. In AG3line, the pixel for the next grouping is calculated actively with the line geometry and it can even be accurate to one pixel. To reduce the fracture caused by unstable pixels, when the adjacent pixel cannot satisfy the grouping rules, the candidate pixels for the next grouping are expanded with the line geometry constraint. To furtherly control false positives, AG3line then validates and refines the line segments by exploiting both the line geometry and the alignment of gradient magnitude . When AG3line was evaluated utilizing the image dataset with the ground truth, it outperformed both the classical and the latest detectors.The implementation of AG3line is available at https://github.com/weidong-whu/AG3line .},
  archive      = {J_PR},
  author       = {Yongjun Zhang and Dong Wei and Yansheng Li},
  doi          = {10.1016/j.patcog.2021.107834},
  journal      = {Pattern Recognition},
  pages        = {107834},
  shortjournal = {Pattern Recognition},
  title        = {AG3line: Active grouping and geometry-gradient combined validation for fast line segment extraction},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Compact learning for multi-label classification.
<em>PR</em>, <em>113</em>, 107833. (<a
href="https://doi.org/10.1016/j.patcog.2021.107833">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-label classification (MLC) studies the problem where each instance is associated with multiple relevant labels, which leads to the exponential growth of output space. It confronts with the great challenge for the exploration of the latent label relationship and the intrinsic correlation between feature and label spaces. MLC gave rise to a framework named label compression (LC) to obtain a compact space for efficient learning. Nevertheless, most existing LC methods failed to consider the influence of the feature space or misguided by original problematic features, which may result in performance degradation instead. In this paper, we present a compact learning (CL) framework to embed the features and labels simultaneously and with mutual guidance . The proposal is a versatile concept that does not rigidly adhere to some specific embedding methods, and is independent of the subsequent learning process. Following its spirit, a simple yet effective implementation called compact multi-label learning (CMLL) is proposed to learn a compact low-dimensional representation for both spaces. CMLL maximizes the dependence between the embedded spaces of the labels and features, and minimizes the loss of label space recovery concurrently. Theoretically, we provide a general analysis for different embedding methods. Practically, we conduct extensive experiments to validate the effectiveness of the proposed method.},
  archive      = {J_PR},
  author       = {Jiaqi Lv and Tianran Wu and Chenglun Peng and Yunpeng Liu and Ning Xu and Xin Geng},
  doi          = {10.1016/j.patcog.2021.107833},
  journal      = {Pattern Recognition},
  pages        = {107833},
  shortjournal = {Pattern Recognition},
  title        = {Compact learning for multi-label classification},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-task learning for simultaneous script identification
and keyword spotting in document images. <em>PR</em>, <em>113</em>,
107832. (<a href="https://doi.org/10.1016/j.patcog.2021.107832">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, an end-to-end multi-task deep neural network was proposed for simultaneous script identification and Keyword Spotting (KWS) in multi-lingual hand-written and printed document images . We introduced a unified approach which addresses both challenges cohesively, by designing a novel CNN-BLSTM architecture. The script identification stage involves local and global features extraction to allow the network to cover more relevant information. Contrarily to the traditional feature fusion approaches which build a linear feature concatenation, we employed a compact bi-linear pooling to capture pairwise correlations between these features. The script identification result is, then, injected in the KWS module to eliminate characters of irrelevant scripts and perform the decoding stage using a single-script mode. All the network parameters were trained in an end-to-end fashion using a multi-task learning that jointly minimizes the NLL loss for the script identification and the CTC loss for the KWS. Our approach was evaluated on a variety of public datasets of different languages and writing types.. Experiments proved the efficacy of our deep multi-task representation learning compared to the state-of-the-art systems for both of keyword spotting and script identification tasks.},
  archive      = {J_PR},
  author       = {Ahmed Cheikhrouhou and Yousri Kessentini and Slim Kanoun},
  doi          = {10.1016/j.patcog.2021.107832},
  journal      = {Pattern Recognition},
  pages        = {107832},
  shortjournal = {Pattern Recognition},
  title        = {Multi-task learning for simultaneous script identification and keyword spotting in document images},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Weakly-supervised action localization via embedding-modeling
iterative optimization. <em>PR</em>, <em>113</em>, 107831. (<a
href="https://doi.org/10.1016/j.patcog.2021.107831">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Action recognition and localization in untrimmed videos in weakly supervised scenario is a challenging problem of great application prospects. Limited by the information available in video-level labels, it is a promising attempt to fully leverage the instructive knowledge learned on trimmed videos to facilitate analysis of untrimmed videos, considering that there are abundant trimmed videos which are publicly available and well segmented with semantic descriptions . In order to enforce effective trimmed-untrimmed augmentation, this paper presents a novel framework of embedding-modeling iterative optimization network, referred to as IONet. In the proposed method, action classification modeling and shared subspace embedding are learned jointly in an iterative way, so that robust cross-domain knowledge transfer is achieved. With a carefully designed two-stage self-attentive representation learning workflow for untrimmed videos, irrelevant backgrounds are eliminated and fine-grained temporal relevance can be robustly explored. Extensive experiments are conducted on two benchmark datasets, i.e., THUMOS14 and ActivityNet1.3, and experimental results clearly corroborate the efficacy of our method. Source code is available on GitHub GitHub . 2 .},
  archive      = {J_PR},
  author       = {Xiao-Yu Zhang and Haichao Shi and Changsheng Li and Peng Li and Zekun Li and Peng Ren},
  doi          = {10.1016/j.patcog.2021.107831},
  journal      = {Pattern Recognition},
  pages        = {107831},
  shortjournal = {Pattern Recognition},
  title        = {Weakly-supervised action localization via embedding-modeling iterative optimization},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Copycat CNN: Are random non-labeled data enough to steal
knowledge from black-box models? <em>PR</em>, <em>113</em>, 107830. (<a
href="https://doi.org/10.1016/j.patcog.2021.107830">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks have been successful lately enabling companies to develop neural-based products, which demand an expensive process, involving data acquisition and annotation; and model generation, usually requiring experts. With all these costs, companies are concerned about the security of their models against copies and deliver them as black-boxes accessed by APIs. Nonetheless, we argue that even black-box models still have some vulnerabilities. In a preliminary work, we presented a simple, yet powerful, method to copy black-box models by querying them with natural random images. In this work, we consolidate and extend the copycat method: (i) some constraints are waived; (ii) an extensive evaluation with several problems is performed; (iii) models are copied between different architectures; and, (iv) a deeper analysis is performed by looking at the copycat behavior. Results show that natural random images are effective to generate copycats for several problems.},
  archive      = {J_PR},
  author       = {Jacson Rodrigues Correia-Silva and Rodrigo F. Berriel and Claudine Badue and Alberto F. De Souza and Thiago Oliveira-Santos},
  doi          = {10.1016/j.patcog.2021.107830},
  journal      = {Pattern Recognition},
  pages        = {107830},
  shortjournal = {Pattern Recognition},
  title        = {Copycat CNN: Are random non-labeled data enough to steal knowledge from black-box models?},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Coupled-dynamic learning for vision and language: Exploring
interaction between different tasks. <em>PR</em>, <em>113</em>, 107829.
(<a href="https://doi.org/10.1016/j.patcog.2021.107829">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intensive research interests have been paid for the vision and language communities. Especially, image captioning task aims to generate natural language descriptions from the image content. Oppositely, image synthesis task aims to generate realistic images from natural language descriptions. Moreover, both of them can achieve promising results by using Long Short-Term Memory (LSTM), which models the sequence dynamics at each time step as hidden state. Nevertheless, the research on dynamics is often limited in the individual task, while there is no progress exploring the mutual relationship between dynamics in different tasks. In this work, we present a novel coupled-dynamic formulation that can iteratively reduce the distance between task-dependent dynamics in the training process. To embed adverse information into individual network, we construct dual-loss architectures to interactively align dynamics. We evaluate the proposed framework on Flickr8k, Flickr30k and MSCOCO datasets. Experimental results show that our approach can boost dual tasks together and achieve competing performances against state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Ning Xu and Hongshuo Tian and Yanhui Wang and Weizhi Nie and Dan Song and An-An Liu and Wu Liu},
  doi          = {10.1016/j.patcog.2021.107829},
  journal      = {Pattern Recognition},
  pages        = {107829},
  shortjournal = {Pattern Recognition},
  title        = {Coupled-dynamic learning for vision and language: Exploring interaction between different tasks},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Synergistic learning of lung lobe segmentation and
hierarchical multi-instance classification for automated severity
assessment of COVID-19 in CT images. <em>PR</em>, <em>113</em>, 107828.
(<a href="https://doi.org/10.1016/j.patcog.2021.107828">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding chest CT imaging of the coronavirus disease 2019 (COVID-19) will help detect infections early and assess the disease progression . Especially, automated severity assessment of COVID-19 in CT images plays an essential role in identifying cases that are in great need of intensive clinical care. However, it is often challenging to accurately assess the severity of this disease in CT images, due to variable infection regions in the lungs, similar imaging biomarkers, and large inter-case variations. To this end, we propose a synergistic learning framework for automated severity assessment of COVID-19 in 3D CT images, by jointly performing lung lobe segmentation and multi-instance classification. Considering that only a few infection regions in a CT image are related to the severity assessment, we first represent each input image by a bag that contains a set of 2D image patches (with each cropped from a specific slice). A multi-task multi-instance deep network (called M 2 2 UNet) is then developed to assess the severity of COVID-19 patients and also segment the lung lobe simultaneously. Our M 2 2 UNet consists of a patch-level encoder, a segmentation sub-network for lung lobe segmentation, and a classification sub-network for severity assessment (with a unique hierarchical multi-instance learning strategy). Here, the context information provided by segmentation can be implicitly employed to improve the performance of severity assessment. Extensive experiments were performed on a real COVID-19 CT image dataset consisting of 666 chest CT images, with results suggesting the effectiveness of our proposed method compared to several state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Kelei He and Wei Zhao and Xingzhi Xie and Wen Ji and Mingxia Liu and Zhenyu Tang and Yinghuan Shi and Feng Shi and Yang Gao and Jun Liu and Junfeng Zhang and Dinggang Shen},
  doi          = {10.1016/j.patcog.2021.107828},
  journal      = {Pattern Recognition},
  pages        = {107828},
  shortjournal = {Pattern Recognition},
  title        = {Synergistic learning of lung lobe segmentation and hierarchical multi-instance classification for automated severity assessment of COVID-19 in CT images},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Rethinking data collection for person re-identification:
Active redundancy reduction. <em>PR</em>, <em>113</em>, 107827. (<a
href="https://doi.org/10.1016/j.patcog.2021.107827">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Annotating a large-scale image dataset is very tedious, yet necessary for training person re-identification (re-ID) models. To alleviate such a problem, we present an active redundancy reduction (ARR) framework via training an effective re-ID model with the least labeling efforts. The proposed ARR framework actively selects informative and diverse samples for annotation by estimating their uncertainty and intra-diversity, thus it can significantly reduce the annotation workload. Moreover, we propose a computer-assisted identity recommendation module embedded in the ARR framework to help human annotators to rapidly and accurately label the selected samples. Extensive experiments were carried out on several public re-ID datasets to demonstrate the existence of data redundancy . Experimental results indicate that our method can reduce 57\%, 63\%, and 49\% annotation efforts on the Market1501, MSMT17, and CUHK03, respectively, while maximizing the performance of the re-ID model.},
  archive      = {J_PR},
  author       = {Xin Xu and Lei Liu and Xiaolong Zhang and Weili Guan and Ruimin Hu},
  doi          = {10.1016/j.patcog.2021.107827},
  journal      = {Pattern Recognition},
  pages        = {107827},
  shortjournal = {Pattern Recognition},
  title        = {Rethinking data collection for person re-identification: Active redundancy reduction},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Momentum contrastive learning for few-shot COVID-19
diagnosis from chest CT images. <em>PR</em>, <em>113</em>, 107826. (<a
href="https://doi.org/10.1016/j.patcog.2021.107826">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The current pandemic, caused by the outbreak of a novel coronavirus (COVID-19) in December 2019, has led to a global emergency that has significantly impacted economies, healthcare systems and personal wellbeing all around the world. Controlling the rapidly evolving disease requires highly sensitive and specific diagnostics. While RT-PCR is the most commonly used, it can take up to eight hours, and requires significant effort from healthcare professionals. As such, there is a critical need for a quick and automatic diagnostic system . Diagnosis from chest CT images is a promising direction. However, current studies are limited by the lack of sufficient training samples, as acquiring annotated CT images is time-consuming. To this end, we propose a new deep learning algorithm for the automated diagnosis of COVID-19, which only requires a few samples for training. Specifically, we use contrastive learning to train an encoder which can capture expressive feature representations on large and publicly available lung datasets and adopt the prototypical network for classification. We validate the efficacy of the proposed model in comparison with other competing methods on two publicly available and annotated COVID-19 CT datasets. Our results demonstrate the superior performance of our model for the accurate diagnosis of COVID-19 based on chest CT images.},
  archive      = {J_PR},
  author       = {Xiaocong Chen and Lina Yao and Tao Zhou and Jinming Dong and Yu Zhang},
  doi          = {10.1016/j.patcog.2021.107826},
  journal      = {Pattern Recognition},
  pages        = {107826},
  shortjournal = {Pattern Recognition},
  title        = {Momentum contrastive learning for few-shot COVID-19 diagnosis from chest CT images},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning efficient, explainable and discriminative
representations for pulmonary nodules classification. <em>PR</em>,
<em>113</em>, 107825. (<a
href="https://doi.org/10.1016/j.patcog.2021.107825">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic pulmonary nodules classification is significant for early diagnosis of lung cancers. Recently, deep learning techniques have enabled remarkable progress in this field. However, these deep models are typically of high computational complexity and work in a black-box manner. To combat these challenges, in this work, we aim to build an efficient and (partially) explainable classification model . Specially, we use neural architecture search (NAS) to automatically search 3D network architectures with excellent accuracy/speed trade-off. Besides, we use the convolutional block attention module (CBAM) in the networks, which helps us understand the reasoning process. During training, we use A-Softmax loss to learn angularly discriminative representations. In the inference stage, we employ an ensemble of diverse neural networks to improve the prediction accuracy and robustness. We conduct extensive experiments on the LIDC-IDRI database. Compared with previous state-of-the-art, our model shows highly comparable performance by using less than 1/40 parameters. Besides, empirical study shows that the reasoning process of learned networks is in conformity with physicians’ diagnosis. Related code and results have been released at: https://github.com/fei-hdu/NAS-Lung .},
  archive      = {J_PR},
  author       = {Hanliang Jiang and Fuhao Shen and Fei Gao and Weidong Han},
  doi          = {10.1016/j.patcog.2021.107825},
  journal      = {Pattern Recognition},
  pages        = {107825},
  shortjournal = {Pattern Recognition},
  title        = {Learning efficient, explainable and discriminative representations for pulmonary nodules classification},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Towards purchase prediction: A transaction-based setting and
a graph-based method leveraging price information. <em>PR</em>,
<em>113</em>, 107824. (<a
href="https://doi.org/10.1016/j.patcog.2021.107824">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Targeting at boosting business revenue, purchase prediction based on user behavior is crucial to e-commerce. However, it is not a well-explored topic due to a lack of relevant datasets. Specifically, no public dataset provides both price and discount information varying on time, which play an essential role in the user’s decision making. Besides, existing learn-to-rank methods cannot explicitly predict the purchase possibility for a specific user-item pair. In this paper, we propose a two-step graph-based model, where the graph model is applied in the first step to learn representations of both users and items over click-through data, and the second step is a classifier incorporating the price information of each transaction record. To evaluate the model performance, we propose a transaction-based framework focusing on the purchased items and their context clicks, which contain items that a user is interested in but fails to choose after comparison. Our experiments show that exploiting the price and discount information can significantly enhance prediction accuracy.},
  archive      = {J_PR},
  author       = {Zongxi Li and Haoran Xie and Guandong Xu and Qing Li and Mingming Leng and Chi Zhou},
  doi          = {10.1016/j.patcog.2021.107824},
  journal      = {Pattern Recognition},
  pages        = {107824},
  shortjournal = {Pattern Recognition},
  title        = {Towards purchase prediction: A transaction-based setting and a graph-based method leveraging price information},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Visual SLAM for robot navigation in healthcare facility.
<em>PR</em>, <em>113</em>, 107822. (<a
href="https://doi.org/10.1016/j.patcog.2021.107822">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The COVID-19 pandemic has affected many countries, posing a threat to human health and safety, and putting tremendous pressure on the medical system. This paper proposes a novel SLAM technology using RGB and depth images to improve hospital operation efficiency, reduce the risk of doctor-patient cross-infection, and curb the spread of the COVID-19. Most current visual SLAM researches assume that the environment is stationary, which makes handling real-world scenarios such as hospitals a challenge. This paper proposes a method that effectively deals with SLAM problems for scenarios with dynamic objects, e.g., people and movable objects, based on the semantic descriptor extracted from images with help of a knowledge graph. Specifically, our method leverages a knowledge graph to construct a priori movement relationship between entities and establishes high-level semantic information. Built upon this knowledge graph, a semantic descriptor is constructed to describe the semantic information around key points, which is rotation-invariant and robust to illumination. The seamless integration of the knowledge graph and semantic descriptor helps eliminate the dynamic objects and improves the accuracy of tracking and positioning of robots in dynamic environments. Experiments are conducted using data acquired from healthcare facilities , and semantic maps are established to meet the needs of robots for delivering medical services. In addition, to compare with the state-of-the-art methods, a publicly available dataset is used in our evaluation. Compared with the state-of-the-art methods, our proposed method demonstrated great improvement with respect to both accuracy and robustness in dynamic environments. The computational efficiency is also competitive.},
  archive      = {J_PR},
  author       = {Baofu Fang and Gaofei Mei and Xiaohui Yuan and Le Wang and Zaijun Wang and Junyang Wang},
  doi          = {10.1016/j.patcog.2021.107822},
  journal      = {Pattern Recognition},
  pages        = {107822},
  shortjournal = {Pattern Recognition},
  title        = {Visual SLAM for robot navigation in healthcare facility},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Interwoven texture-based description of interest points in
images. <em>PR</em>, <em>113</em>, 107821. (<a
href="https://doi.org/10.1016/j.patcog.2021.107821">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Local feature description is to assign a unique signature to a key-point such that it becomes distinctive from the others regardless of changes in viewpoint, illumination, rotation, scale as well as distortions and noise. This paper proposes a novel approach to construct such a descriptor. For preserving both homogeneous and heterogeneous features of a given support region, we interweave the texture information so that the key-point is more likely to be assigned a distinctive signature and neighboring key-points will be less likely to share the same texture information. The main idea behind our descriptor is to increase the areas of our observations in the given scene while the length of the local support region is fixed. Gradient magnitude and divergence, as measurement parameters of texture information, are applied to a group of pixels instead of employing a pixel-wise strategy that make the descriptor more resistant to noise, distortions and illumination variation . The required storage of the proposed descriptor is just 72 floats and its computational complexity is much lower than those of existing ones. A comparative study between the proposed method and the selected state-of-the-art ones over multiple publicly accessible datasets with different characteristics shows its superiority, robustness and computational efficiency under various geometric changes, illumination variation, distortions and noise. The code and supplementary materials can be found at https://github.com/mogvision/InterTex-Feature-Descriptor .},
  archive      = {J_PR},
  author       = {Morteza Ghahremani and Yitian Zhao and Bernard Tiddeman and Yonghuai Liu},
  doi          = {10.1016/j.patcog.2021.107821},
  journal      = {Pattern Recognition},
  pages        = {107821},
  shortjournal = {Pattern Recognition},
  title        = {Interwoven texture-based description of interest points in images},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Change-point detection in hierarchical circadian models.
<em>PR</em>, <em>113</em>, 107820. (<a
href="https://doi.org/10.1016/j.patcog.2021.107820">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the problem of change-point detection in sequences of high-dimensional and heterogeneous observations, which also possess a periodic temporal structure . Due to the dimensionality problem , when the time between change points is of the order of the dimension of the model parameters, drifts in the underlying distribution can be misidentified as changes. To overcome this limitation, we assume that the observations lie in a lower-dimensional manifold that admits a latent variable representation . In particular, we propose a hierarchical model that is computationally feasible, widely applicable to heterogeneous data and robust to missing instances. Additionally, the observations’ periodic dependencies are captured by non-stationary periodic covariance functions . The proposed technique is particularly well suited to (and motivated by) the problem of detecting changes in human behavior using smartphones and its application to relapse detection in psychiatric patients. Finally, we validate the technique on synthetic examples and we demonstrate its utility in the detection of behavioral changes using real data acquired by smartphones.},
  archive      = {J_PR},
  author       = {Pablo Moreno-Muñoz and David Ramírez and Antonio Artés-Rodríguez},
  doi          = {10.1016/j.patcog.2021.107820},
  journal      = {Pattern Recognition},
  pages        = {107820},
  shortjournal = {Pattern Recognition},
  title        = {Change-point detection in hierarchical circadian models},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). BCMM: A novel post-based augmentation representation for
early rumour detection on social media. <em>PR</em>, <em>113</em>,
107818. (<a href="https://doi.org/10.1016/j.patcog.2021.107818">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online social media (OSM) has become a hotbed for the rapid dissemination of disinformation or rumour. Therefore, rumour detection, especially early rumour detection (ERD), is very challenging given the limited, incomplete and noisy information. Although there are some researches on earlier rumour detection, most of their studies require a larger dataset or a longer detection time span, i.e., the rumour detection efficiency needs to be improved. In this paper, we focus on a shorter detection time span which also means fewer online posts to achieve the task of ERD. We proposed a novel post-based augmentation representation approach to process post content of rumour events in the early stages of their dissemination, i.e., backward compression mapping mechanism (BCMM). In addition, we combine BCMM with gated recurrent unit (GRU) to represent post content, topology network of posts and metadata extracted from post datasets. We apply a three-layers GRU to enhance the representation of dataset within one hour after the occurrence of a social media event, i.e., BCMM-GRU. The steps are as follows: (1) we input the first-hour data into the first layer; (2) the first 40 min of data are channelled into the second layer with the output of the first layer making a full mapping to the second layer simultaneously; (3) the first 20 min of data are sent to the third layer while the output of the second layer applies a full mapping to the third layer simultaneously. The evaluation of BCMM-GRU’s performance entails applying k-fold cross-validation (CV) set-up on four available real-life rumour event datasets. The experimental results are superior to the baselines and model variants and achieve a high accuracy of 80.09\% and F1-score of 80.18\%.},
  archive      = {J_PR},
  author       = {Yongcong Luo and Jing Ma and Chai Kiat Yeo},
  doi          = {10.1016/j.patcog.2021.107818},
  journal      = {Pattern Recognition},
  pages        = {107818},
  shortjournal = {Pattern Recognition},
  title        = {BCMM: A novel post-based augmentation representation for early rumour detection on social media},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Reconstruction regularized low-rank subspace learning for
cross-modal retrieval. <em>PR</em>, <em>113</em>, 107813. (<a
href="https://doi.org/10.1016/j.patcog.2020.107813">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid increase of multi-modal data through the internet, cross-modal matching or retrieval has received much attention recently. It aims to use one type of data as query and retrieve results from the database of another type. For this task, the most popular approach is the latent subspace learning, which learns a shared subspace for multi-modal data, so that we can efficiently measure cross-modal similarity. Instead of adopting traditional regularization terms, we hope that the latent representation could recover the multi-modal information, which works as a reconstruction regularization term. Besides, we assume that different view features for samples of the same category share the same representation in the latent space. Since the number of classes is generally smaller than the number of samples and the feature dimension, therefore the latent feature matrix of training instances should be low-rank. We try to learn the optimal latent representation, and propose a reconstruction based term to recover original multi-modal data and a low-rank term to regularize the learning of subspace. Our method can deal with both supervised and unsupervised cross-modal retrieval tasks. For those situations where the semantic labels are not easy to obtain, our proposed method can also work very well. We propose an efficient algorithm to optimize our framework. To evaluate the performance of our method, we conduct extensive experiments on various datasets. The experimental results show that our proposed method is very efficient and outperforms the state-of-the-art subspace learning approaches.},
  archive      = {J_PR},
  author       = {Jianlong Wu and Xingxu Xie and Liqiang Nie and Zhouchen Lin and Hongbin Zha},
  doi          = {10.1016/j.patcog.2020.107813},
  journal      = {Pattern Recognition},
  pages        = {107813},
  shortjournal = {Pattern Recognition},
  title        = {Reconstruction regularized low-rank subspace learning for cross-modal retrieval},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). View-invariant action recognition via unsupervised AttentioN
transfer (UANT). <em>PR</em>, <em>113</em>, 107807. (<a
href="https://doi.org/10.1016/j.patcog.2020.107807">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With wide applications in surveillance and human-robot interaction, view-invariant human action recognition is critical, however, challenging, due to the action occlusion and information loss caused by view change. Current methods mainly seek for a common feature space for different views. However, such solutions become invalid when there exist few common features, e.g. large view change. To tackle the problem, we propose an Unsupervised AttentioN Transfer (UANT) approach for view-invariant action recognition. Other than transferring feature knowledge, UANT transfers attention from one selected reference view to arbitrary views, which correctly emphasizes crucial body joints and their relations for view-invariant representation. In addition, the attention calculation method taking into account both recognition contribution and reliability of skeleton joints generates effective attention. Experiments showed its effectiveness for correctly locating crucial body joints in action sequences. We exhaustively evaluate our approach on the UESTC and the NTU dataset, performing unsupervised view-invariant evaluations, i.e. X-view and Arbitrary-view recognition. Experiment results demonstrate its superiority in view-invariant representation and recognition.},
  archive      = {J_PR},
  author       = {Yanli Ji and Yang Yang and Heng Tao Shen and Tatsuya Harada},
  doi          = {10.1016/j.patcog.2020.107807},
  journal      = {Pattern Recognition},
  pages        = {107807},
  shortjournal = {Pattern Recognition},
  title        = {View-invariant action recognition via unsupervised AttentioN transfer (UANT)},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Collaborative representation with curriculum classifier
boosting for unsupervised domain adaptation. <em>PR</em>, <em>113</em>,
107802. (<a href="https://doi.org/10.1016/j.patcog.2020.107802">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain adaptation aims at leveraging rich knowledge in the source domain to build an accurate classifier in the different but related target domain. Most prior methods attempt to align features or reduce domain discrepancy by means of statistical properties yet ignore the differences among samples. In this paper, we put forward a novel solution based on collaborative representation for classifier adaptation. Similar to instance re-weighting, we aim to learn an adaptive classifier by multi-stage inference and instance rearranging. Specifically, a curriculum learning based sample selection scheme is proposed, then the chosen samples are integrated into training set iteratively. Due to the distribution mismatch of two domains, we propose distance-aware sparsity regularization to learn more flexible representations. Extensive experiments verify that the proposed method is comparable or superior to the state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Chao Han and Deyun Zhou and Yu Xie and Maoguo Gong and Yu Lei and Jiao Shi},
  doi          = {10.1016/j.patcog.2020.107802},
  journal      = {Pattern Recognition},
  pages        = {107802},
  shortjournal = {Pattern Recognition},
  title        = {Collaborative representation with curriculum classifier boosting for unsupervised domain adaptation},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MPPCANet: A feedforward learning strategy for few-shot image
classification. <em>PR</em>, <em>113</em>, 107792. (<a
href="https://doi.org/10.1016/j.patcog.2020.107792">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The main learning strategy of the PCANet is using Principal Component Analysis (PCA) for learning the convolutional filters from the data. The assumption that all the image patches are sampled from a single Gaussian component is implicitly taken, which is too strong. In this paper, the image patches are modeled using mixtures of probabilistic principal component analysis (MPPCA) and the corresponding MPPCANet (PCANet constructed using mixtures of probabilistic principal component analysis) is proposed. The proposed model is applied to the few-shot learning scenario. In the proposed framework, the image patches are assumed to come from several suppositions of Gaussian components. In the process of estimating the parameters of the MPPCA model, the clustering of the training image patches and the principal components of each cluster are simultaneously obtained. The number of mixture components is automatically determined during the optimization procedure . The theoretical insights of the proposed MPPCANet is elaborated by comparing with our prior work, CPCANet (PCANet with clustering-based filters). The proposed MPPCANet is evaluated on several benchmarking visual data sets in the experiment. It is compared with the original PCANet, CPCANet and several state-of-the-art methods. The experimental results show that the proposed MPPCANet has improved significantly the recognition capability of the original PCANet under the few-shot learning scenario. The performance of the MPPCANet is also better than the CPCANet in most cases.},
  archive      = {J_PR},
  author       = {Yu Song and Changsheng Chen},
  doi          = {10.1016/j.patcog.2020.107792},
  journal      = {Pattern Recognition},
  pages        = {107792},
  shortjournal = {Pattern Recognition},
  title        = {MPPCANet: A feedforward learning strategy for few-shot image classification},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). End-to-end video text detection with online tracking.
<em>PR</em>, <em>113</em>, 107791. (<a
href="https://doi.org/10.1016/j.patcog.2020.107791">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text in videos usually acts as important semantic cues, which is helpful to video analysis. Video text detection is considered as one of the most difficult tasks in document analysis due to the following two challenges: 1) the difficulties caused by video scenes, i.e., motion blur , illumination changes, and occlusion; 2) the properties of text including variants of fonts, languages, orientations, and shapes. Most existing methods try to improve the video text detection through video text tracking, but treat these two tasks separately. This can significantly increase the amount of calculations and cannot take full advantage of the supervisory information of both tasks. In this work, we introduce explainable descriptor, combines appearance, geometry and PHOC features, to establish a bridge between detection and tracking and build an end-to-end video text detection model with online tracking to address these challenges together. By integrating these two branches into one trainable framework, they can promote each other and the computational cost is significantly reduced. Besides, the introduce explainable descriptor also make our end-to-end model have inherent interpretability . Experiments on existing video text benchmarks including ICDAR 2013 Video, DOST, Minetto and YVT verify the role of explainable descriptors in improving model expression ability and the proposed method significantly outperforms state-of-the-art methods. Our method improves F-score by more than 2\% on all datasets and achieves 81.52\% on the MOTA of the Minetto dataset.},
  archive      = {J_PR},
  author       = {Hongyuan Yu and Yan Huang and Lihong Pi and Chengquan Zhang and Xuan Li and Liang Wang},
  doi          = {10.1016/j.patcog.2020.107791},
  journal      = {Pattern Recognition},
  pages        = {107791},
  shortjournal = {Pattern Recognition},
  title        = {End-to-end video text detection with online tracking},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Visual place recognition: A survey from deep learning
perspective. <em>PR</em>, <em>113</em>, 107760. (<a
href="https://doi.org/10.1016/j.patcog.2020.107760">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual place recognition has attracted widespread research interest in multiple fields such as computer vision and robotics. Recently, researchers have employed advanced deep learning techniques to tackle this problem. While an increasing number of studies have proposed novel place recognition methods based on deep learning , few of them has provided a whole picture about how and to what extent deep learning has been utilized for this issue. In this paper, by delving into over 200 references, we present a comprehensive survey that covers various aspects of place recognition from deep learning perspective. We first present a brief introduction of deep learning and discuss its opportunities for recognizing places. After that, we focus on existing approaches built upon convolutional neural networks , including off-the-shelf and specifically designed models as well as novel image representations. We also discuss challenging problems in place recognition and present an extensive review of the corresponding datasets. To explore the future directions, we describe open issues and some new tools, for instance, generative adversarial networks , semantic scene understanding and multi-modality feature learning for this research topic. Finally, a conclusion is drawn for this paper.},
  archive      = {J_PR},
  author       = {Xiwu Zhang and Lei Wang and Yan Su},
  doi          = {10.1016/j.patcog.2020.107760},
  journal      = {Pattern Recognition},
  pages        = {107760},
  shortjournal = {Pattern Recognition},
  title        = {Visual place recognition: A survey from deep learning perspective},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Joint direct estimation of 3D geometry and 3D motion using
spatio temporal gradients. <em>PR</em>, <em>113</em>, 107759. (<a
href="https://doi.org/10.1016/j.patcog.2020.107759">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conventional image-motion based methods for structure from motion first compute optical flow, then solve for the 3D motion parameters based on the epipolar constraint , and finally recover the 3D geometry of the scene. However, errors in optical flow due to regularization can lead to large errors in 3D motion and structure. This paper investigates whether performance and consistency can be improved by avoiding optical flow estimation in the early stages of the structure-from-motion pipeline, and it proposes a new direct method based on image gradients (normal flow) only. Our main idea lies in a reformulation of the positive-depth constraint – the basis for estimating egomotion from normal flow – as a continuous piecewise differentiable function, which allows the use of well-known minimization techniques to solve for 3D motion. The 3D motion estimate is then refined and structure estimated adding a regularization based on depth. Experimental comparisons on standard synthetic datasets and the real-world driving benchmark dataset Kitti using three different optic flow algorithms show that the method achieves better accuracy in all but one case. Furthermore, it outperforms existing normal flow based 3D motion estimation techniques. Finally, the recovered 3D geometry is shown to be also very accurate.},
  archive      = {J_PR},
  author       = {Francisco Barranco and Cornelia Fermüller and Yiannis Aloimonos and Eduardo Ros},
  doi          = {10.1016/j.patcog.2020.107759},
  journal      = {Pattern Recognition},
  pages        = {107759},
  shortjournal = {Pattern Recognition},
  title        = {Joint direct estimation of 3D geometry and 3D motion using spatio temporal gradients},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Low-rank adaptive graph embedding for unsupervised feature
extraction. <em>PR</em>, <em>113</em>, 107758. (<a
href="https://doi.org/10.1016/j.patcog.2020.107758">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most of manifold learning based feature extraction methods are two-step methods, which first construct a weighted neighborhood graph and then use the pre-constructed graph to perform subspace learning. As a result, these methods fail to use the underlying correlation structure of data to learn an adaptive graph to preciously characterize the similarity relationship between samples. To address this problem, we propose a novel unsupervised feature extraction method called low-rank adaptive graph embedding (LRAGE), which can perform subspace learning and adaptive probabilistic neighborhood graph embedding simultaneously based on reconstruction error minimization. The proposed LRAGE is imposed with low-rank constraint for the sake of exploring the underlying correlation structure of data and learning more informative projection. Moreover, the L 2 , 1 L2,1 -norm penalty is imposed on the regularization to further enhance the robustness of LRAGE. Since the resulting objective function has no closed-form solutions, an iterative optimization algorithm is elaborately designed. The convergence of the proposed algorithm is proved and the corresponding computational complexity analysis is also presented. In addition, we explore the potential properties of the proposed LRAGE by comparing it with several similar models on both synthetic and real-world data sets. Extensive experiments on five well-known face data sets and three non-face data sets demonstrate the superiority of the proposed LRAGE.},
  archive      = {J_PR},
  author       = {Jianglin Lu and Hailing Wang and Jie Zhou and Yudong Chen and Zhihui Lai and Qinghua Hu},
  doi          = {10.1016/j.patcog.2020.107758},
  journal      = {Pattern Recognition},
  pages        = {107758},
  shortjournal = {Pattern Recognition},
  title        = {Low-rank adaptive graph embedding for unsupervised feature extraction},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Contrast-weighted dictionary learning based saliency
detection for VHR optical remote sensing images. <em>PR</em>,
<em>113</em>, 107757. (<a
href="https://doi.org/10.1016/j.patcog.2020.107757">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object detection in very high resolution (VHR) optical remote sensing (RS) images is one of the most fundamental but challenging tasks in the field of RS image analysis. To reduce the computational complexity of redundant information and improve the efficiency of image processing , visual saliency models have been widely applied in this field. In this paper, a novel saliency detection model based on Contrast-weighted Dictionary Learning (CDL) is proposed for VHR optical RS images. Specifically, the proposed CDL learns salient and non-salient atoms from positive and negative samples to construct a discriminant dictionary, in which a contrast-weighted term is proposed to encourage the contrast-weighted patterns to be present in the learned salient dictionary while discouraging them from being present in the non-salient dictionary. Then, we measure the saliency by combining the coefficients of the sparse representation (SR) and reconstruction errors. Furthermore, by using the proposed joint saliency measure , a variety of saliency maps are generated based on the discriminant dictionary. Finally, a fusion method based on global gradient optimization is proposed to integrate multiple saliency maps. Experimental results on four datasets demonstrate that the proposed model outperforms other state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Zhou Huang and Huai-Xin Chen and Tao Zhou and Yun-Zhi Yang and Chang-Yin Wang and Bi-Yuan Liu},
  doi          = {10.1016/j.patcog.2020.107757},
  journal      = {Pattern Recognition},
  pages        = {107757},
  shortjournal = {Pattern Recognition},
  title        = {Contrast-weighted dictionary learning based saliency detection for VHR optical remote sensing images},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Crossover-net: Leveraging vertical-horizontal crossover
relation for robust medical image segmentation. <em>PR</em>,
<em>113</em>, 107756. (<a
href="https://doi.org/10.1016/j.patcog.2020.107756">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate boundary segmentation in medical images is significant yet challenging due to large variation of shape, size and appearance within intra- and inter- samples. In this paper, we present a novel deep model termed as Crossover-Net for robust segmentation in medical images. The proposed model is inspired by an interesting observation – the features learned from horizontal and vertical directions can provide informative and complement contextual information to enhance discriminative ability between different tissues. Specifically, we first originally propose a cross-shaped patch, namely crossover-patch which consists of a pair of (orthogonal and overlapping) vertical and horizontal patches. Then, we develop our Crossover-Net to learn the vertical and horizontal crossover relation according to the proposed crossover-patches. To train our model end-to-end, we design a novel loss function to (1) impose the consistency on overlapping region of vertical and horizontal patches and (2) preserve the diversity on their non-overlapping regions. We have extensively evaluated our method on CT kidney tumor, MR cardiac, and X-ray breast mass segmentation tasks , showing promising results compared with the current state-of-the-art methods. The code is available at https://github.com/Qianyu1226/Crossover-Net .},
  archive      = {J_PR},
  author       = {Qian Yu and Yang Gao and Yefeng Zheng and Jianbing Zhu and Yakang Dai and Yinghuan Shi},
  doi          = {10.1016/j.patcog.2020.107756},
  journal      = {Pattern Recognition},
  pages        = {107756},
  shortjournal = {Pattern Recognition},
  title        = {Crossover-net: Leveraging vertical-horizontal crossover relation for robust medical image segmentation},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep neural network oriented evolutionary parametric eye
modeling. <em>PR</em>, <em>113</em>, 107755. (<a
href="https://doi.org/10.1016/j.patcog.2020.107755">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Comprehensive and accurate eye modeling is crucial to a variety of applications, including human-computer interaction, assistive technologies , and medical diagnosis. However, most studies focus on the localization of one or two components of eyes, such as pupil or iris, lacking a comprehensive eye model. We propose to model an eye image by a set of parametric curves . The set of curves are plotted on an eye image to form a Contour-Eye image. A deep neural network is trained to evaluate the fitness of the Contour-Eye image. Then an evolutionary process is conducted to search the best fitting curve set, guided by the trained deep neural network . Finally, an accurate eye model with optimized parametric curves is obtained. For the algorithm evaluation, a finely annotated eye dataset denoted as FAED-50 is established by us, which contains 2,498 eye images from 50 subjects. The experimental results on the FAED-50 and the relabeled CASIA datasets and comparison with the state-of-the-art methods demonstrate the effectiveness and accuracy of the proposed parametric model .},
  archive      = {J_PR},
  author       = {Yang Zheng and Hong Fu and Ruimin Li and Tai-Chiu Hsung and Zongxi Song and Desheng Wen},
  doi          = {10.1016/j.patcog.2020.107755},
  journal      = {Pattern Recognition},
  pages        = {107755},
  shortjournal = {Pattern Recognition},
  title        = {Deep neural network oriented evolutionary parametric eye modeling},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep video code for efficient face video retrieval.
<em>PR</em>, <em>113</em>, 107754. (<a
href="https://doi.org/10.1016/j.patcog.2020.107754">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we address one specific video retrieval problem in terms of human face. Given one query in forms of either a frame or a sequence from a person, we search the database and return the most relevant face videos, i.e., ones have the same class label with the query. Such problem is very challenging due to the large intra-class variations and the high request on the efficiency of video representations in terms of both time and space. To handle such challenges, this paper proposes a novel Deep Video Code ( DVC ) method which encodes video faces into compact binary codes . Specifically, we devise an end-to-end convolutional neural network (CNN) framework that takes face videos as training inputs, models each of them as a unified representation by temporal feature pooling operation, and finally projects the high-dimensional representations of both frames and videos into Hamming space to generate binary codes. In such Hamming space, distance of dissimilar pairs is larger than that of similar pairs by a margin. To this end, a novel bounded triplet hashing loss is elaborately designed, which takes all dissimilar pairs into consideration for each anchor point in a mini-batch, and the optimization of the loss function is smoother and more stable. Extensive experiments on challenging video face databases and general image/video datasets with comparison to the state-of-the-arts verify the effectiveness of our method in different kinds of retrieval scenarios.},
  archive      = {J_PR},
  author       = {Shishi Qiao and Ruiping Wang and Shiguang Shan and Xilin Chen},
  doi          = {10.1016/j.patcog.2020.107754},
  journal      = {Pattern Recognition},
  pages        = {107754},
  shortjournal = {Pattern Recognition},
  title        = {Deep video code for efficient face video retrieval},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Low-rank tensor ring learning for multi-linear regression.
<em>PR</em>, <em>113</em>, 107753. (<a
href="https://doi.org/10.1016/j.patcog.2020.107753">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The emergence of large-scale data demands new regression models with multi-dimensional coefficient arrays, known as tensor regression models. The recently proposed tensor ring decomposition has interesting properties of enhanced representation and compression capability , cyclic permutation invariance and balanced tensor ring rank, which may lead to efficient computation and fewer parameters in regression problems . In this paper, a generally multi-linear tensor-on-tensor regression model is proposed that the coefficient array has a low-rank tensor ring structure, which is termed tensor ring ridge regression (TRRR). Two optimization models are developed for the TRRR problem and solved by different algorithms: the tensor factorization based one is solved by alternating least squares algorithm, and accelerated by a fast network contraction, while the rank minimization based one is addressed by the alternating direction method of multipliers algorithm. Comparative experiments , including Spatio-temporal forecasting tasks and 3D reconstruction of human motion capture data from its temporally synchronized video sequences, demonstrate the enhanced performance of our algorithms over existing state-of-the-art ones, especially in terms of training time.},
  archive      = {J_PR},
  author       = {Jiani Liu and Ce Zhu and Zhen Long and Huyan Huang and Yipeng Liu},
  doi          = {10.1016/j.patcog.2020.107753},
  journal      = {Pattern Recognition},
  pages        = {107753},
  shortjournal = {Pattern Recognition},
  title        = {Low-rank tensor ring learning for multi-linear regression},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Exploring a unified low rank representation for multi-focus
image fusion. <em>PR</em>, <em>113</em>, 107752. (<a
href="https://doi.org/10.1016/j.patcog.2020.107752">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years have witnessed a trend that uses image representation models, including sparse representation (SR), low-rank representation (LRR) and their variants for multi-focus image fusion. Despite the thrilling preliminary results, existing methods conduct the fusion patch by patch, leading to insufficient consideration of the spatial consistency among the image patches within a local region or an object. As a result, not only the spatial artifacts are easily introduced to the fused image but also the “jagged” artifacts frequently arise on the boundaries between the focused regions and the de-focused regions, which is an inherent problem in these patch-based fusion methods.Aiming to address the above problems, we propose, in this paper,a new multi-focus image fusion method integrating super-pixel clustering and a unified LRR (ULRR) model. The entire algorithm is carried out in three steps. In the first step, the source image is segmented into a few super-pixels with irregular sizes, rather than patches with regular sizes, to diminish the “jagged” artifacts and meanwhile to preserve the boundaries of objects on the fused image. Secondly, a super-pixel clustering-based fusion strategy is employed to further reduce the spatial artifacts in the fused images. This is achieved by using a proposed ULRR model, which imposes the low-rank constraints onto each super-pixel cluster.Thisis apparently more reasonable for those images with complicated scenes. Moreover, a Laplacianregularization term is incorporated in the proposed ULRR model to ensure the spatial consistency among the super-pixels with the same cluster. Finally, a measure of focus for each super-pixel is defined to seek the focused as well as de-focused regions in thesource image via jointly using representation coefficients and sparse errors derived from the proposed ULRR model. Extensive experiments have been conducted and the results demonstrate the superiorities of the proposed fusion method in diminishing the spatial artifactsin the fused image and the “jagged” boundary artifacts between the focused and de-focused regions, compared to the state-of-the-art fusion algorithms.},
  archive      = {J_PR},
  author       = {Qiang Zhang and Fan Wang and Yongjiang Luo and Jungong Han},
  doi          = {10.1016/j.patcog.2020.107752},
  journal      = {Pattern Recognition},
  pages        = {107752},
  shortjournal = {Pattern Recognition},
  title        = {Exploring a unified low rank representation for multi-focus image fusion},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An evidential clustering algorithm by finding belief-peaks
and disjoint neighborhoods. <em>PR</em>, <em>113</em>, 107751. (<a
href="https://doi.org/10.1016/j.patcog.2020.107751">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we introduce a new evidential clustering algorithm based on finding the belief-peaks and disjoint neighborhoods, called BPDNEC. The basic idea of BPDNEC is that each cluster center has the highest possibility of becoming a cluster center among its neighborhood and neighborhoods of those cluster centers are disjoint in vector space. Such possibility is measured by the belief notion in framework of evidence theory. By solving an equation related to neighborhood size, the size of such disjoint neighborhoods is determined and those objects having highest belief among their neighborhoods are automatically detected as cluster centers. Finally, a credal partition is created by minimizing an objective function concerning dissimilarity matrix of data objects. Experimental results show that BPDNEC can automatically detect cluster centers and derive an appropriate credal partition for both object data and proximity data . Simulations on synthetic and real-world datasets validate the conclusions.},
  archive      = {J_PR},
  author       = {Chaoyu Gong and Zhi-gang Su and Pei-hong Wang and Qian Wang},
  doi          = {10.1016/j.patcog.2020.107751},
  journal      = {Pattern Recognition},
  pages        = {107751},
  shortjournal = {Pattern Recognition},
  title        = {An evidential clustering algorithm by finding belief-peaks and disjoint neighborhoods},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Kernel two-dimensional ridge regression for subspace
clustering. <em>PR</em>, <em>113</em>, 107749. (<a
href="https://doi.org/10.1016/j.patcog.2020.107749">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Subspace clustering methods have been extensively studied in recent years. For 2-dimensional (2D) data, existing subspace clustering methods usually convert 2D examples to vectors, which severely damages inherent structural information and relationships of the original data. In this paper, we propose a novel subspace clustering method, named KTRR, for 2D data. The KTRR provides us with a way to learn the most representative 2D features from 2D data in learning data representation. In particular, the KTRR performs 2D feature learning and low-dimensional representation construction simultaneously, which renders the two tasks to mutually enhance each other. 2D kernel is introduced to the KTRR, which renders the KTRR to have enhanced capability of capturing nonlinear relationships from data. An efficient algorithm is developed for its optimization with provable decreasing and convergent property in objective value. Extensive experimental results confirm the effectiveness and efficiency of our method.},
  archive      = {J_PR},
  author       = {Chong Peng and Qian Zhang and Zhao Kang and Chenglizhao Chen and Qiang Cheng},
  doi          = {10.1016/j.patcog.2020.107749},
  journal      = {Pattern Recognition},
  pages        = {107749},
  shortjournal = {Pattern Recognition},
  title        = {Kernel two-dimensional ridge regression for subspace clustering},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Projected fuzzy c-means clustering with locality
preservation. <em>PR</em>, <em>113</em>, 107748. (<a
href="https://doi.org/10.1016/j.patcog.2020.107748">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional partition-based clustering algorithms , hard or fuzzy version of C-means, could not deal with high-dimensional data sets effectively as redundant features may impact the computation of distances and local spatial structures among patterns are rarely considered. High dimensionality of space gives rise to so-called concentration effect that is detrimental. In this paper, a novel locality preserving based fuzzy C-means (LPFCM) clustering method and its optimization are presented. An orthogonally projected space, which preserves the locality of structural properties, can be generated in LPFCM, thus enhancing the capability of fuzzy C-means (FCM) for handling high-dimensional data. It is the first time to introduce projection techniques to the FCM optimization objective function directly, and the ideas of fuzzy clustering , geometric structure preservation and feature extraction are seamlessly integrated. LPFCM is also regarded as a unified model that combines two separate stages of spectral clustering . Experimental results on some benchmark data sets show the effectiveness of LPFCM in comparison with FCM and some state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Jie Zhou and Witold Pedrycz and Xiaodong Yue and Can Gao and Zhihui Lai and Jun Wan},
  doi          = {10.1016/j.patcog.2020.107748},
  journal      = {Pattern Recognition},
  pages        = {107748},
  shortjournal = {Pattern Recognition},
  title        = {Projected fuzzy C-means clustering with locality preservation},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MetaCOVID: A siamese neural network framework with
contrastive loss for n-shot diagnosis of COVID-19 patients. <em>PR</em>,
<em>113</em>, 107700. (<a
href="https://doi.org/10.1016/j.patcog.2020.107700">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Various AI functionalities such as pattern recognition and prediction can effectively be used to diagnose (recognize) and predict coronavirus disease 2019 (COVID-19) infections and propose timely response (remedial action) to minimize the spread and impact of the virus. Motivated by this, an AI system based on deep meta learning has been proposed in this research to accelerate analysis of chest X-ray (CXR) images in automatic detection of COVID-19 cases. We present a synergistic approach to integrate contrastive learning with a fine-tuned pre-trained ConvNet encoder to capture unbiased feature representations and leverage a Siamese network for final classification of COVID-19 cases. We validate the effectiveness of our proposed model using two publicly available datasets comprising images from normal, COVID-19 and other pneumonia infected categories. Our model achieves 95.6\% accuracy and AUC of 0.97 in diagnosing COVID-19 from CXR images even with a limited number of training samples.},
  archive      = {J_PR},
  author       = {Mohammad Shorfuzzaman and M. Shamim Hossain},
  doi          = {10.1016/j.patcog.2020.107700},
  journal      = {Pattern Recognition},
  pages        = {107700},
  shortjournal = {Pattern Recognition},
  title        = {MetaCOVID: A siamese neural network framework with contrastive loss for n-shot diagnosis of COVID-19 patients},
  volume       = {113},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Biennial pattern recognition journal awards 2017 and 2018.
<em>PR</em>, <em>112</em>, 107844. (<a
href="https://doi.org/10.1016/j.patcog.2021.107844">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_PR},
  doi          = {10.1016/j.patcog.2021.107844},
  journal      = {Pattern Recognition},
  pages        = {107844},
  shortjournal = {Pattern Recognition},
  title        = {Biennial pattern recognition journal awards 2017 and 2018},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multiple discriminant analysis for collaborative
representation-based classification. <em>PR</em>, <em>112</em>, 107819.
(<a href="https://doi.org/10.1016/j.patcog.2021.107819">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Collaborative Representation-based Classifier (CRC) has shown its advantages and impressive results in face recognition. To further imporve the performance of CRC, we propose a novel dimensionality reduction method termed Multiple Discriminant Analysis for Collaborative Representation-based Classification (MDA-CRC). Considering the labeling criterion of CRC is class-specific, MDA-CRC solves a group of binary classification problems where specific feature subspaces are learned for each class. In each binary classification problem, an orthogonal discriminant analysis method based on collaborative representation is adopted. Hence, MDA-CRC can improve the discriminant ability of collaborative representation and be consistent with the labeling criterion of CRC simultaneously. Further, the convergence of MDA-CRC is proven. Extensive experiments on several benchmark datasets demonstrate the effectiveness of MDA-CRC.},
  archive      = {J_PR},
  author       = {Zhichao Zheng and Huaijiang Sun and Ying Zhou},
  doi          = {10.1016/j.patcog.2021.107819},
  journal      = {Pattern Recognition},
  pages        = {107819},
  shortjournal = {Pattern Recognition},
  title        = {Multiple discriminant analysis for collaborative representation-based classification},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep residual pooling network for texture recognition.
<em>PR</em>, <em>112</em>, 107817. (<a
href="https://doi.org/10.1016/j.patcog.2021.107817">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current deep learning-based texture recognition methods extract spatial orderless features from pre-trained deep learning models that are trained on large-scale image datasets. These methods either produce high dimensional features or have multiple steps like dictionary learning, feature encoding and dimension reduction. In this paper, we propose a novel end-to-end learning framework that not only overcomes these limitations, but also demonstrates faster learning. The proposed framework incorporates a residual pooling layer consisting of a residual encoding module and an aggregation module. The residual encoder preserves the spatial information for improved feature learning and the aggregation module generates orderless feature for classification through a simple averaging. The feature has the lowest dimension among previous deep texture recognition approaches, yet it achieves state-of-the-art performance on benchmark texture recognition datasets such as FMD, DTD, 4D Light and one industry dataset used for metal surface anomaly detection. Additionally, the proposed method obtains comparable results on the MIT-Indoor scene recognition dataset. Our codes are available at https://github.com/maoshangbo/DRP-Texture-Recognition .},
  archive      = {J_PR},
  author       = {Shangbo Mao and Deepu Rajan and Liang Tien Chia},
  doi          = {10.1016/j.patcog.2021.107817},
  journal      = {Pattern Recognition},
  pages        = {107817},
  shortjournal = {Pattern Recognition},
  title        = {Deep residual pooling network for texture recognition},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). IoU-uniform r-CNN: Breaking through the limitations of RPN.
<em>PR</em>, <em>112</em>, 107816. (<a
href="https://doi.org/10.1016/j.patcog.2021.107816">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Region Proposal Network (RPN) is the cornerstone of two-stage object detectors. It generates a sparse set of object proposals and alleviates the extrem foreground-background class imbalance problem during training. However, we find that the potential of the detector has not been fully exploited due to the IoU distribution imbalance and inadequate quantity of the training samples generated by RPN. With the increasing intersection over union (IoU), the exponentially smaller numbers of positive samples would lead to the distribution skewed towards lower IoUs, which hinders the optimization of detector at high IoU levels. In this paper, to break through the limitations of RPN, we propose IoU-Uniform R-CNN, a simple but effective method that directly generates training samples with uniform IoU distribution for the regression branch as well as the IoU prediction branch. Besides, we improve the performance of IoU prediction branch by eliminating the feature offsets of RoIs at inference, which helps the NMS procedure by preserving accurately localized bounding box . Extensive experiments on the PASCAL VOC and MS COCO dataset show the effectiveness of our method, as well as its compatibility and adaptivity to many object detection architectures. The code is made publicly available at https://github.com/zl1994/IoU-Uniform-R-CNN .},
  archive      = {J_PR},
  author       = {Li Zhu and Zihao Xie and Liman Liu and Bo Tao and Wenbing Tao},
  doi          = {10.1016/j.patcog.2021.107816},
  journal      = {Pattern Recognition},
  pages        = {107816},
  shortjournal = {Pattern Recognition},
  title        = {IoU-uniform R-CNN: Breaking through the limitations of RPN},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). NMF with feature relationship preservation penalty term for
clustering problems. <em>PR</em>, <em>112</em>, 107814. (<a
href="https://doi.org/10.1016/j.patcog.2021.107814">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The method proposed in this paper belongs to the family of orthogonal non-negative matrix factorization (ONMF) methods designed to solve clustering problems . Unlike some existing ONMF methods that explicitly constrain the orthogonality of the coefficient matrix in the cost function to derive their clustering models, the proposed method integrates it implicitly, so that it results in a new optimization model with a penalty term. The latter is added to impose a scale relationship between the scatter of the cluster centroids and that of the data points. The solution of the new model involves deriving a new parametrized update scheme for the basis matrix , which makes it possible to improve the performance of the clustering by adjusting a parameter. The proposed clustering algorithm , which we call “pairwise Feature Relationship preservation-based NMF” (FR-NMF), is evaluated on several real-life and synthetic datasets and compared to eight existing NMF-based clustering models. The results obtained show the effectiveness of the proposed algorithm.},
  archive      = {J_PR},
  author       = {Rachid Hedjam and Abdelhamid Abdesselam and Farid Melgani},
  doi          = {10.1016/j.patcog.2021.107814},
  journal      = {Pattern Recognition},
  pages        = {107814},
  shortjournal = {Pattern Recognition},
  title        = {NMF with feature relationship preservation penalty term for clustering problems},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Linguistically-aware attention for reducing the semantic gap
in vision-language tasks. <em>PR</em>, <em>112</em>, 107812. (<a
href="https://doi.org/10.1016/j.patcog.2020.107812">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attention models are widely used in Vision-language (V-L) tasks to perform the visual-textual correlation. Humans perform such a correlation with a strong linguistic understanding of the visual world. However, even the best performing attention model in V-L tasks lacks such a high-level linguistic understanding, thus creating a semantic gap between the modalities. In this paper, we propose an attention mechanism - Linguistically-aware Attention (LAT) - that leverages object attributes obtained from generic object detectors along with pre-trained language models to reduce this semantic gap . LAT represents visual and textual modalities in a common linguistically-rich space, thus providing linguistic awareness to the attention process. We apply and demonstrate the effectiveness of LAT in three V-L tasks: Counting-VQA, VQA, and Image captioning. In Counting-VQA, we propose a novel counting-specific VQA model to predict an intuitive count and achieve state-of-the-art results on five datasets. In VQA and Captioning, we show the generic nature and effectiveness of LAT by adapting it into various baselines and consistently improving their performance.},
  archive      = {J_PR},
  author       = {Gouthaman KV and Athira Nambiar and Kancheti Sai Srinivas and Anurag Mittal},
  doi          = {10.1016/j.patcog.2020.107812},
  journal      = {Pattern Recognition},
  pages        = {107812},
  shortjournal = {Pattern Recognition},
  title        = {Linguistically-aware attention for reducing the semantic gap in vision-language tasks},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Training deep retrieval models with noisy datasets: Bag
exponential loss. <em>PR</em>, <em>112</em>, 107811. (<a
href="https://doi.org/10.1016/j.patcog.2020.107811">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although the CNNs are a very powerful tool for image retrieval , the need of training datasets properly adapted to the application at hand hinders the usefulness of such networks, specially since the datasets need to be free of noise to avoid spoiling the learning process. An ad hoc preprocessing of the dataset to mitigate the noise is a possible solution, but it is usually non-trivial and requires significant human intervention. In this paper, we pave the road for training CNNs for image retrieval with noisy datasets. In particular, we propose a novel Bag Exponential Loss function that, inspired by the Multiple Instance Learning framework, works with bags of matching images instead of single pairs, and allows a dynamical weighting of the relevance of each sample as the training progresses. The formulation of the proposed model is general enough and may serve to other purposes than dealing with noise if parameters are chosen appropriately. Extensive experimental results show the superior performance of the proposed loss with respect to the current state-of-the-art as well as its ability to cope with noisy training sets. Pytorch code available in https://github.com/tmcortes/BELoss},
  archive      = {J_PR},
  author       = {Tomás Martínez-Cortés and Iván González-Díaz and Fernando Díaz-de-María},
  doi          = {10.1016/j.patcog.2020.107811},
  journal      = {Pattern Recognition},
  pages        = {107811},
  shortjournal = {Pattern Recognition},
  title        = {Training deep retrieval models with noisy datasets: Bag exponential loss},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Automated segmentation of the optic disc from fundus images
using an asymmetric deep learning network. <em>PR</em>, <em>112</em>,
107810. (<a href="https://doi.org/10.1016/j.patcog.2020.107810">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate segmentation of the optic disc (OD) regions from colour fundus images is a critical procedure for computer-aided diagnosis of glaucoma. We present a novel deep learning network to automatically identify the OD regions. On the basis of the classical U-Net framework, we define a unique sub-network and a decoding convolutional block. The sub-network is used to preserve important textures and facilitate their detections, while the decoding block is used to improve the contrast of the regions-of-interest with their background. We integrate these two components into the classical U-Net framework to improve the accuracy and reliability of segmenting the OD regions depicted on colour fundus images. We train and evaluate the developed network using three publicly available datasets ( i.e. , MESSIDOR, ORIGA, and REFUGE). The results on an independent testing set ( n = 1,970 images) show a segmentation performance with an average Dice similarity coefficient (DSC), intersection over union (IOU), and Matthew&#39;s correlation coefficient (MCC) of 0.9377, 0.8854, and 0.9383 when trained on the global field-of-view images, respectively, and 0.9735, 0.9494, and 0.9594 when trained on the local disc region images. When compared with the other three classical networks ( i.e. , the U-Net, M-Net, and Deeplabv3) on the same testing datasets, the developed network demonstrates a relatively higher performance.},
  archive      = {J_PR},
  author       = {Lei Wang and Juan Gu and Yize Chen and Yuanbo Liang and Weijie Zhang and Jiantao Pu and Hao Chen},
  doi          = {10.1016/j.patcog.2020.107810},
  journal      = {Pattern Recognition},
  pages        = {107810},
  shortjournal = {Pattern Recognition},
  title        = {Automated segmentation of the optic disc from fundus images using an asymmetric deep learning network},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Plant leaf recognition by integrating shape and texture
features. <em>PR</em>, <em>112</em>, 107809. (<a
href="https://doi.org/10.1016/j.patcog.2020.107809">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Plant leaf identification is a significant challenge in the fields of computer vision and pattern recognition. This article presents a new approach to plant leaf identification, one that integrates shape and texture characteristics. First, we introduce the shape and texture features used by the proposed plant leaf recognition method. The proposed multiscale triangle descriptor (MTD) is employed to characterize the shape information of a plant leaf, and the local binary pattern histogram Fourier (LBP-HF) is used as the texture feature. Then, the shape and texture features of a leaf image are combined by weighted distance measurement, where L 1 L1 distance and chi-square distance are used for shape and texture features, respectively. The proposed approach provides a robust descriptor for the task of plant leaf recognition by combining the complementary MTD and LBP-HF features. The proposed approach has been thoroughly evaluated on three benchmark leaf datasets, including the Flavia, Swedish and MEW2012 leaf datasets. Our method achieves 77.6\%, 85.7\%, and 67.5\% retrieval accuracy on the Flavia, Swedish and MEW2012 leaf datasets, respectively, while the corresponding classification accuracy is 99.1\%, 98.4\%, 95.6\%. The recognition performance of our method is better or comparable to prior state-of-the-art plant leaf recognition method.},
  archive      = {J_PR},
  author       = {Chengzhuan Yang},
  doi          = {10.1016/j.patcog.2020.107809},
  journal      = {Pattern Recognition},
  pages        = {107809},
  shortjournal = {Pattern Recognition},
  title        = {Plant leaf recognition by integrating shape and texture features},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Unsupervised learning framework for interest point detection
and description via properties optimization. <em>PR</em>, <em>112</em>,
107808. (<a href="https://doi.org/10.1016/j.patcog.2020.107808">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents an unsupervised interest point detection and description method named Properties Optimization Point (POP), which provides a unified objective to optimize different properties of interest point. First, the proposed objective formulates the interest point set as a latent variable, which is flexible to integrate different properties. With the latent variable, the probability formulations are designed for four traditional properties (sparsity, repeatability, invariability, discriminability). Second, a novel property termed as informativeness indicating the information complexity of a local area is designed to determine the areas containing high information, from which interest points are encouraged to be extracted. Third, an efficient approximate Expectation Maximization is proposed to optimize the non-differentiable objective which integrates the above five properties. Finally, POP is instantiated with fully convolutional networks . Experimental results demonstrate that POP outperforms state-of-the-art methods on a number of image matching benchmarks containing both planar and non-planar scenes.},
  archive      = {J_PR},
  author       = {Pei Yan and Yihua Tan and Yuan Tai and Dongrui Wu and Hanbin Luo and Xiaolong Hao},
  doi          = {10.1016/j.patcog.2020.107808},
  journal      = {Pattern Recognition},
  pages        = {107808},
  shortjournal = {Pattern Recognition},
  title        = {Unsupervised learning framework for interest point detection and description via properties optimization},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Exploiting structured high-level knowledge for
domain-specific visual classification. <em>PR</em>, <em>112</em>,
107806. (<a href="https://doi.org/10.1016/j.patcog.2020.107806">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the last decade, deep learning models have yielded impressive performance on visual object recognition and image classification . However these methods still rely on learning visual data distributions and show difficulties in dealing with complex scenarios where visual appearance only is not enough to effectively tackle them. This is the case, for instance, of fine-grained image classification in domain-specific applications for which it is very complex to employ data-driven models because of the lack of large amounts of samples and that, instead, can be solved by resorting to specialized human knowledge. However, encoding this specialized knowledge and injecting it into deep models is not trivial. In this paper, we address this problem by: a) employing computational ontologies to model specialized knowledge in a structured representation and, b) building a hybrid visual-semantic classification framework. The classification method performs inference over a Bayesian Network graph, whose structure depends on the knowledge encoded in an ontology and evidences are built using the outputs of deep networks. We test our approach on a fine-grained classification task , employing an extremely complex dataset containing images from several fruit varieties as well as visual and semantic annotations . Since the classification is done at the variety level (e.g., discriminating between different cherry varieties), appearance changes slightly and expert domain knowledge — making using of contextual information — is required to perform classification accurately. Experimental results show that our approach significantly outperforms standard deep learning–based classification methods over the considered scenario as well as existing methods leveraging semantic information for classification. These results demonstrate, on one hand, the difficulty of purely-visual deep methods in tackling small and highly-specialized datasets and, on the other hard, the capabilities of our approach to effectively encode and use semantic knowledge for enhanced accuracy.},
  archive      = {J_PR},
  author       = {S. Palazzo and F. Murabito and C. Pino and F. Rundo and D. Giordano and M. Shah and C. Spampinato},
  doi          = {10.1016/j.patcog.2020.107806},
  journal      = {Pattern Recognition},
  pages        = {107806},
  shortjournal = {Pattern Recognition},
  title        = {Exploiting structured high-level knowledge for domain-specific visual classification},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Machine learning based feature selection and knowledge
reasoning for CBR system under big data. <em>PR</em>, <em>112</em>,
107805. (<a href="https://doi.org/10.1016/j.patcog.2020.107805">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Under big data, large number of features as well as their complex data types makes traditional feature selection and knowledge reasoning in CBR system not adapt to new condition. To solve these problems, first, this paper proposes Weighted Relative Probability Change of Solution Parameters (WRPCSP) algorithm to execute feature selection. Then, this paper integrates Bayesian network (BN) with CBR system for knowledge reasoning. Based on probability calculation and reasoning, WRPCSP algorithm together with BN allows the proposed CBR system to well work under big data. In addition, to overcome the efficiency problem caused by large number of features, this paper also proposes Group-Outside (GO) algorithm to assign the computing task of big data for parallel data processing. GO algorithm can make the computing capacity of Hadoop fully utilized to gain the least time costing for parallel data processing. Finally, lots of experiments are performed to validate the proposed method.},
  archive      = {J_PR},
  author       = {Yuan Guo and Bing Zhang and Y. Sun and K. Jiang and K. Wu},
  doi          = {10.1016/j.patcog.2020.107805},
  journal      = {Pattern Recognition},
  pages        = {107805},
  shortjournal = {Pattern Recognition},
  title        = {Machine learning based feature selection and knowledge reasoning for CBR system under big data},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Feature selection using bare-bones particle swarm
optimization with mutual information. <em>PR</em>, <em>112</em>, 107804.
(<a href="https://doi.org/10.1016/j.patcog.2020.107804">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection (FS) is an important data processing method in pattern recognition and data mining. Due to not considering characteristics of the FS problem itself, traditional particle update mechanisms and swarm initialization strategies adopted in most particle swarm optimization (PSO) limit their performance on dealing with high-dimensional FS problems. Focused on it, this paper proposes a novel feature selection algorithm based on bare bones PSO (BBPSO) with mutual information. Firstly, an effective swarm initialization strategy based on label correlation is developed, making full use of the correlation between features and class labels to accelerate the convergence of swarm. Then, in order to enhance the exploitation performance of the algorithm, two local search operators, i.e., the supplementary operator and the deletion operator, are developed based on feature relevance-redundancy. Furthermore, an adaptive flip mutation operator is designed to help particles jump out of local optimal solutions . We apply the proposed algorithm to typical datasets based on the K -Nearest Neighbor classifier ( K -NN), and compare it with eleven state-of-the-art algorithms, SFS, PTA, SGA , BPSO , PSO(4-2), HPSO-LS, Binary BPSO , NaFA, IBFA , KPLS-mRMR and SMBA-CSFS. The experimental results show that the proposed algorithm can achieve a feature subset with better performance, and is a highly competitive FS algorithm.},
  archive      = {J_PR},
  author       = {Xian-fang Song and Yong Zhang and Dun-wei Gong and Xiao-yan Sun},
  doi          = {10.1016/j.patcog.2020.107804},
  journal      = {Pattern Recognition},
  pages        = {107804},
  shortjournal = {Pattern Recognition},
  title        = {Feature selection using bare-bones particle swarm optimization with mutual information},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sparsely-labeled source assisted domain adaptation.
<em>PR</em>, <em>112</em>, 107803. (<a
href="https://doi.org/10.1016/j.patcog.2020.107803">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain Adaptation (DA) aims to generalize the classifier learned from a well-labeled source domain to an unlabeled target domain. Existing DA methods usually assume that rich labels could be available in the source domain. However, we usually confront the source domain with a large number of unlabeled data but only a few labeled data, and thus, how to transfer knowledge from this sparsely-labeled source domain to the target domain is still a challenge, which greatly limits its application in the wild. This paper proposes a novel Sparsely-Labeled Source Assisted Domain Adaptation (SLSA-DA) algorithm to address the challenge with limited labeled source domain samples. Specifically, due to the label scarcity problem, the projected clustering is first conducted on both the source and target domains, so that the discriminative structures of data could be exploited elegantly. Then label propagation is adopted to propagate the labels from those limited labeled source samples to the whole unlabeled data progressively, so that the cluster labels are revealed correctly. Finally, we jointly align the marginal and conditional distributions to mitigate the cross-domain mismatching problem, and optimize those three procedures iteratively. However, it is nontrivial to incorporate the above three procedures into a unified optimization framework seamlessly since some variables to be optimized are implicitly involved in their formulas, thus they could not benefit to each other. Remarkably, we prove that the projected clustering and conditional distribution alignment could be reformulated into other formulations, thus the implicit variables are embedded in different optimization steps. As such, the variables related to those three quantities could be optimized in a unified optimization framework and benefit to each other, and improve the recognition performance obviously. Extensive experiments have verified that our approach could deal with the challenge in the SLSA-DA setting, and achieve the best performances across different real-world cross-domain visual recognition tasks. Our preliminary Matlab code is available at https://github.com/WWLoveTransfer/SLSA-DA/ .},
  archive      = {J_PR},
  author       = {Wei Wang and Shenglun Chen and Yuankai Xiang and Jing Sun and Haojie Li and Zhihui Wang and Fuming Sun and Zhengming Ding and Baopu Li},
  doi          = {10.1016/j.patcog.2020.107803},
  journal      = {Pattern Recognition},
  pages        = {107803},
  shortjournal = {Pattern Recognition},
  title        = {Sparsely-labeled source assisted domain adaptation},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Neural random subspace. <em>PR</em>, <em>112</em>, 107801.
(<a href="https://doi.org/10.1016/j.patcog.2020.107801">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The random subspace method , also known as the pillar of random forests , is good at making precise and robust predictions. However, there is as yet no straightforward way to combine it with deep learning . In this paper, we therefore propose Neural Random Subspace (NRS), a novel deep learning based random subspace method. In contrast to previous forest methods, NRS enjoys the benefits of end-to-end, data-driven representation learning , as well as pervasive support from deep learning software and hardware platforms, hence achieving faster inference speed and higher accuracy. Furthermore, as a non-linear component to be encoded into Convolutional Neural Networks (CNNs), NRS learns non-linear feature representations in CNNs more efficiently than contemporary, higher-order pooling methods, producing excellent results with negligible increase in parameters, floating point operations (FLOPs) and real running time. Compared with random subspaces, random forests and gradient boosting decision trees (GBDTs), NRS demonstrates superior performance on 35 machine learning datasets. Moreover, on both 2D image and 3D point cloud recognition tasks, integration of NRS with CNN architectures achieves consistent improvements with only incremental cost .},
  archive      = {J_PR},
  author       = {Yun-Hao Cao and Jianxin Wu and Hanchen Wang and Joan Lasenby},
  doi          = {10.1016/j.patcog.2020.107801},
  journal      = {Pattern Recognition},
  pages        = {107801},
  shortjournal = {Pattern Recognition},
  title        = {Neural random subspace},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). LSTM based trajectory prediction model for cyclist utilizing
multiple interactions with environment. <em>PR</em>, <em>112</em>,
107800. (<a href="https://doi.org/10.1016/j.patcog.2020.107800">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The cyclist trajectory prediction is critical for the local path planning of autonomous vehicles. Based on the assumption that cyclist&#39;s movement is limited by its dynamics and subjected to interactions with environments, a novel LSTM based cyclist trajectory prediction model which utilizes multiple interactions with surroundings and motion feature in a unified framework is proposed. Road features describing road boundary and static obstacles are employed to address cyclist&#39;s interaction with the road. To address interactions with pedestrians, other cyclists and vehicles, object features including object attributes and relative positions are utilized. The focal attention mechanism is employed to reveal the importance of features at each time-steps. By feeding features into LSTM encoder, the movement in the next two seconds is predicted. Experiments were conducted on two datasets, and results show that the presented model outperforms the state-of-art models in most cases.},
  archive      = {J_PR},
  author       = {Zhi Huang and Jun Wang and Lei Pi and Xiaolin Song and Lingfang Yang},
  doi          = {10.1016/j.patcog.2020.107800},
  journal      = {Pattern Recognition},
  pages        = {107800},
  shortjournal = {Pattern Recognition},
  title        = {LSTM based trajectory prediction model for cyclist utilizing multiple interactions with environment},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). 3D-GAT: 3D-guided adversarial transform network for person
re-identification in unseen domains. <em>PR</em>, <em>112</em>, 107799.
(<a href="https://doi.org/10.1016/j.patcog.2020.107799">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person Re-identification (ReID) has witnessed remarkable improvements in the past couple of years. However, its applications in real-world scenarios are limited by the disparity among different cameras and datasets. In general, it remains challenging to generalize ReID algorithms from one domain to another, especially when the target domain is unknown. To solve this issue, we develop a 3D-guided adversarial transform (3D-GAT) network which explores the transfer ability of source training data to facilitate learning domain-independent knowledge. Being aware of a 3D model and human poses, 3D-GAT makes use of image-to-image translation to synthesize person images in different conditions whilst preserving features for identification as much as possible. With these augmented training data, it is easier for ReID approaches to perceive how a person can appear differently under varying viewpoints and poses, most of which are not seen in the training data, and thus achieve higher ReID accuracy especially in an unknown domain. Extensive experiments conducted on Market-1501, DukeMTMC-reID and CUHK03 demonstrate the effectiveness of our proposed approach, which is competitive to the baseline models in the original dataset and sets the new state-of-the-art in direct transfer to other datasets.},
  archive      = {J_PR},
  author       = {Hengheng Zhang and Ying Li and Zijie Zhuang and Lingxi Xie and Qi Tian},
  doi          = {10.1016/j.patcog.2020.107799},
  journal      = {Pattern Recognition},
  pages        = {107799},
  shortjournal = {Pattern Recognition},
  title        = {3D-GAT: 3D-guided adversarial transform network for person re-identification in unseen domains},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Image super-resolution via channel attention and spatial
graph convolutional network. <em>PR</em>, <em>112</em>, 107798. (<a
href="https://doi.org/10.1016/j.patcog.2020.107798">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, deep convolutional neural networks (CNNs) have been widely explored in single image super-resolution (SISR) and obtained remarkable performance. However, most of the existing CNN-based SR methods mainly focus on wider or deeper architecture design, neglecting to discover the latent relationship of features, hence limiting the representational ability of networks. To address this issue, we propose a channel attention and spatial graph convolutional network (CASGCN) for more powerful feature obtaining and feature correlations modeling. The CASGCN is formed by several channel attention and spatial graph (CASG) blocks that incorporate global spatial and channel inter-dependencies for rendering features of each pixel. Inside the CASG block, channel branch and spatial branch are first arranged in a paralleled way, and then are concatenated to effectively learn the representation of each image pixel. Specifically, we use attention mechanism to extract informative features in channel branch while the spatial-aware graph is used in spatial branch to model the global self-similar information. Furthermore, the adjacency matrix in spatial-aware graph is dynamically generated via the Gram matrix to model global correlations between pixels and is shared across the whole network without auxiliary parameters. Extensive experiments on SISR with different degradation models show the effectiveness of our CASGCN in terms of quantitative and visual results.},
  archive      = {J_PR},
  author       = {Yue Yang and Yong Qi},
  doi          = {10.1016/j.patcog.2020.107798},
  journal      = {Pattern Recognition},
  pages        = {107798},
  shortjournal = {Pattern Recognition},
  title        = {Image super-resolution via channel attention and spatial graph convolutional network},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Temperature network for few-shot learning with
distribution-aware large-margin metric. <em>PR</em>, <em>112</em>,
107797. (<a href="https://doi.org/10.1016/j.patcog.2020.107797">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot learning learns to classify unseen data with few training samples in hand and has attracted increasing attentions recently. In this paper, we propose a novel Temperature Network to tackle few-shot learning tasks motivated by three crucial factors that are seldom considered in the existing literature. First, to encourage compact intra-class distribution, a general improvement for prototype-based methods is proposed to ensure compact intra-class distribution and the effectiveness is theoretically and experimentally validated. Second, the proposed Temperature Network can implicitly generate query-specific prototypes and thus enjoys a more effective distribution-aware metric. Third, to further strengthen the generalization ability of the proposed model, a novel and simple large-margin based method is developed by leveraging the temperature function and we gradually tune the learning temperature to stabilize the training process. Moreover, we note that the commonly used datasets in few-shot learning are actually contrived from large-scale datasets, and thus may not represent a real few-shot problem. We propose a real-life few shot problem, i.e., Dermnet skin disease , to comprehensively evaluate the performance of few-shot learning methods. Experiments conducted on conventional datasets as well as the proposed skin disease dataset demonstrate the superiority of the proposed method over other state-of-the-art methods. The source code of our method is available. 1},
  archive      = {J_PR},
  author       = {Wei Zhu and Wenbin Li and Haofu Liao and Jiebo Luo},
  doi          = {10.1016/j.patcog.2020.107797},
  journal      = {Pattern Recognition},
  pages        = {107797},
  shortjournal = {Pattern Recognition},
  title        = {Temperature network for few-shot learning with distribution-aware large-margin metric},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep mutual learning for visual object tracking.
<em>PR</em>, <em>112</em>, 107796. (<a
href="https://doi.org/10.1016/j.patcog.2020.107796">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing deep trackers use deep convolutional neural networks to extract powerful features or directly predict the position of the target. For most deep trackers, it is hard to improve their performance by replacing the original backbone with a more powerfully heavyweight network directly. In this paper, we propose a novel mutual-learning-based training methodology for visual object tracking. By re-training the backbone network with this novel methodology, we can improve the tracking performance simply and effectively. We demonstrate this novel training methodology with two mainstream tracking approaches: correlation-filter-based approach and tracking-by-detection-based approach. First, we reformulate a correlation-filter-based tracker as a fully convolutional network and design an end-to-end tracking framework. With this framework, we can enhance the backbone network in a mutual learning way. Second, we integrate our training methodology into a typical tracking-by-detection-based tracker, and then we improve the tracking performance with a simple offline training process. Extensive experiments on the OTB2013, OTB2015, VOT2017 and LaSOT benchmarks demonstrate that the tracking performance can be improved effectively by using the proposed mutual-learning-based training methodology.},
  archive      = {J_PR},
  author       = {Haojie Zhao and Gang Yang and Dong Wang and Huchuan Lu},
  doi          = {10.1016/j.patcog.2020.107796},
  journal      = {Pattern Recognition},
  pages        = {107796},
  shortjournal = {Pattern Recognition},
  title        = {Deep mutual learning for visual object tracking},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fast hard negative mining for deep metric learning.
<em>PR</em>, <em>112</em>, 107795. (<a
href="https://doi.org/10.1016/j.patcog.2020.107795">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep metric learning methods aim to measure similarity of data points (e.g. images) by calculating their distance in a high dimensional embedding space. These methods are usually trained by optimizing a ranking loss function, which is designed to bring together samples from the same class while separating them from samples from all other classes. The most challenging part of these methods is the selection of samples that contribute to effective network training. In this paper we present Bag of Negatives (BoN), a fast hard negative mining method, that provides a set, triplet or pair of potentially relevant training samples. BoN is an efficient method that selects a bag of hard negatives based on a novel online hashing strategy. We show the superiority of BoN against state-of-the-art hard negative mining methods in terms of accuracy and training time over three large datasets.},
  archive      = {J_PR},
  author       = {Bojana Gajić and Ariel Amato and Carlo Gatta},
  doi          = {10.1016/j.patcog.2020.107795},
  journal      = {Pattern Recognition},
  pages        = {107795},
  shortjournal = {Pattern Recognition},
  title        = {Fast hard negative mining for deep metric learning},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Semi-supervised blockwisely architecture search for
efficient lightweight generative adversarial network. <em>PR</em>,
<em>112</em>, 107794. (<a
href="https://doi.org/10.1016/j.patcog.2020.107794">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of computer vision , methods that use fully supervised learning and fixed deep network structures need to be improved. Currently, many studies are devoted to designing neural architecture search methods to use neural networks in a more flexible way. However, most of these methods use fully supervised learning at the cost of extraordinary GPU training time. In view of the above problems, we propose a semi-supervised generative adversarial network and search network architecture based on block structure. Use real pictures and generated pictures with corresponding real tags and pseudo tags for training, to achieve the purpose of semi-supervised learning. By setting the layer’s hyperparameters to a variable and flexible stacking block structure, network architecture search is achieved. The proposed method realizes image generation and extends to image classification . In the experimental results in Section 4, the training time is greatly reduced and the model performance is improved, which illustrates the efficiency of our method. The code can be found in https://github.com/AICV-CUMT/STASGAN .},
  archive      = {J_PR},
  author       = {Man Zhang and Yong Zhou and Jiaqi Zhao and Shixiong Xia and Jiaqi Wang and Zizheng Huang},
  doi          = {10.1016/j.patcog.2020.107794},
  journal      = {Pattern Recognition},
  pages        = {107794},
  shortjournal = {Pattern Recognition},
  title        = {Semi-supervised blockwisely architecture search for efficient lightweight generative adversarial network},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A novel dimension reduction and dictionary learning
framework for high-dimensional data classification. <em>PR</em>,
<em>112</em>, 107793. (<a
href="https://doi.org/10.1016/j.patcog.2020.107793">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-dimensional problem poses significant challenges for dictionary learning based classification architecture. Joint Dimension Reduction and Dictionary Learning (JDRDL) framework shows great potential for overcoming the challenges caused by high dimensionality . However, most of the existing JDRDL approaches do not consider the complex nonlinear relationships within high-dimensional data, which limits their classification performance. To overcome this problem, a novel joint dimension reduction and dictionary learning framework is proposed in this paper for high-dimensional data classification. Firstly, at dimension reduction stage, an autoencoder is employed to learn a nonlinear mapping that reduces dimensionality and preserves nonlinear structure of the high-dimensional data. Then, at dictionary learning stage, the locality constraint with label embedding, which takes the locality and label information into account together, is incorporated into the learning process to preserve desirable nonlinear local structure and enhance class discrimination. Moreover, the mapping function and dictionary are optimized simultaneously to enhance the performance. Encouraging experimental results on multiple benchmark datasets confirm that the proposed framework is effective and efficient for high-dimensional data classification.},
  archive      = {J_PR},
  author       = {Yanxia Li and Yi Chai and Han Zhou and Hongpeng Yin},
  doi          = {10.1016/j.patcog.2020.107793},
  journal      = {Pattern Recognition},
  pages        = {107793},
  shortjournal = {Pattern Recognition},
  title        = {A novel dimension reduction and dictionary learning framework for high-dimensional data classification},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Candidate fusion: Integrating language modelling into a
sequence-to-sequence handwritten word recognition architecture.
<em>PR</em>, <em>112</em>, 107790. (<a
href="https://doi.org/10.1016/j.patcog.2020.107790">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sequence-to-sequence models have recently become very popular for tackling handwritten word recognition problems. However, how to effectively integrate an external language model into such recognizer is still a challenging problem. The main challenge while training a language model is to deal with the language model corpus which is usually different to the one used for training the handwritten word recognition system. Thus, the bias between both word corpora leads to incorrectness on the transcriptions, providing similar or even worse performances on the recognition task. In this work, we introduce Candidate Fusion, a novel way to integrate an external language model to a sequence-to-sequence architecture. Moreover, it provides suggestions from an external language knowledge, as a new input to the sequence-to-sequence recognizer. Hence, Candidate Fusion provides two improvements. On the one hand, the sequence-to-sequence recognizer has the flexibility to not only combine the information from itself and the language model, but also choose the importance of the information provided by the language model. On the other hand, the external language model has the ability to adapt itself to the training corpus and even learn the most common errors produced from the recognizer. Finally, by conducting comprehensive experiments, the Candidate Fusion proves to outperform the state-of-the-art language models for handwritten word recognition tasks.},
  archive      = {J_PR},
  author       = {Lei Kang and Pau Riba and Mauricio Villegas and Alicia Fornés and Marçal Rusiñol},
  doi          = {10.1016/j.patcog.2020.107790},
  journal      = {Pattern Recognition},
  pages        = {107790},
  shortjournal = {Pattern Recognition},
  title        = {Candidate fusion: Integrating language modelling into a sequence-to-sequence handwritten word recognition architecture},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Label group diffusion for image and image pair segmentation.
<em>PR</em>, <em>112</em>, 107789. (<a
href="https://doi.org/10.1016/j.patcog.2020.107789">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diffusion technique is powerful for semi-supervised image segmentation since the geometry of the data manifold can be captured by the affinity propagation. Conventional diffusion methods focus on single label, which however ignore interactions among labels. This workstudies a generalized diffusion framework that considers label group for diffusion (LGD). The proposed framework can effectively capture the interactions among image elements via tensor product graph (TPG). A multivariate affinity framework is proposed to learn on higher-order TPG . Label pair diffusion algorithm is naturally derivedfrom the framework by considering second-order affinity (LG (2) D). We theoretically show that conventional label diffusion is the simplest case of the proposed framework (LG (1) D). Extensive experiments on image segmentation and image pair co-segmentation datasets demonstrate the superior performance of the proposed framework.},
  archive      = {J_PR},
  author       = {Tao Wang and Zexuan Ji and Jian Yang and Quansen Sun and Xiaobo Shen and Zhenwen Ren and Qi Ge},
  doi          = {10.1016/j.patcog.2020.107789},
  journal      = {Pattern Recognition},
  pages        = {107789},
  shortjournal = {Pattern Recognition},
  title        = {Label group diffusion for image and image pair segmentation},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MobileGCN applied to low-dimensional node feature learning.
<em>PR</em>, <em>112</em>, 107788. (<a
href="https://doi.org/10.1016/j.patcog.2020.107788">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The idea of the paper concentrates on an iterative learning process in Graph Convolution Networks (GCNs) involved in two vital steps: one is a message propagation (message passing) step to aggregate neighboring node features via aggregators performed, and another is an encoding output step to encode node feature representations by using updaters. In our model, we propose a novel affinity-aware encoding as an updater in GCNs, which aggregates the neighboring nodes of a node while updating this node’s features. By utilizing affinity values of our encoding, we order the neighboring nodes to determine the correspondence between encoding functions and the neighboring nodes. Furthermore, to explicitly reduce the model size, we propose a lightweight variant of our updater that integrates Depth-wise Separable Convolution (DSC) into it, namely Depth-wise Separable Graph Convolution (DSGC). Comprehensive experiments conducted on graph data demonstrate that our models’ accuracy improved significantly for graphs of low-dimensional node features. Also, performed in the low-dimensional node feature space we provide state-of-the-art results on two metrics (Macro-f1 and Matthews correlation coefficient (MCC)). Besides, our models are robust when taking different low-dimensional feature selection strategies.},
  archive      = {J_PR},
  author       = {Wei Dong and Junsheng Wu and Zongwen Bai and Yaoqi Hu and Weigang Li and Wei Qiao and Marcin Woźniak},
  doi          = {10.1016/j.patcog.2020.107788},
  journal      = {Pattern Recognition},
  pages        = {107788},
  shortjournal = {Pattern Recognition},
  title        = {MobileGCN applied to low-dimensional node feature learning},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A CenterNet++ model for ship detection in SAR images.
<em>PR</em>, <em>112</em>, 107787. (<a
href="https://doi.org/10.1016/j.patcog.2020.107787">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ship detection in SAR images is a challenging task due to two difficulties. (1) Because of the long observation distance, ships in SAR images are small with low resolution, leading to high false negative . (2) Because of the complex onshore background, ships are easily confused with other objects with similar appearance. To solve these problems, we propose an effective and stable single-stage detector called CenterNet++. Our model mainly consists of three modules, i.e., feature refinement module, feature pyramids fusion module, and head enhancement module . Firstly, to address small objects detection problem, we design a feature refinement module for extracting multi-scale contextual information. Secondly, feature pyramids fusion module is developed for generating more powerful semantic information. Finally, to alleviate the impact of complex background, head enhancement module is proposed for a balance between foreground and background. To prove the effectiveness and robustness of the proposed method, we make extensive experiments on three popular SAR image datasets, i.e., AIR-SARShip, SSDD, SAR-Ship. The experimental results show that our CenterNet++ reaches state-of-the-art performance on all datasets. In addition, compared with the baseline CenterNet, the proposed method achieves a remarkable accuracy improvement with negligible increase in time cost.},
  archive      = {J_PR},
  author       = {Haoyuan Guo and Xi Yang and Nannan Wang and Xinbo Gao},
  doi          = {10.1016/j.patcog.2020.107787},
  journal      = {Pattern Recognition},
  pages        = {107787},
  shortjournal = {Pattern Recognition},
  title        = {A CenterNet++ model for ship detection in SAR images},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-view content-based mammogram retrieval using dynamic
similarity and locality sensitive hashing. <em>PR</em>, <em>112</em>,
107786. (<a href="https://doi.org/10.1016/j.patcog.2020.107786">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Content-Based Mammogram Retrieval (CBMR) methods using Multi-View Information Fusion (MVIF) have triggered a growing interest in the last years given their ability to help radiologists make the right breast-cancer related decision. To further improve the retrieval performance , this paper introduces an efficient MVIF-CBMR method based on late fusion that combines retrieval result-level of Medio-Lateral Oblique (MLO) and Cranio-Caudal (CC) views. The proposed method adopts a coupled multi-index with a dynamic distance to evaluate the similarity between mammograms, which allows to fully exert the discriminative power of the complementary of MLO-CC features. Furthermore, the ROI dataset signature indexing step uses a hashing technique to optimize the computational time for retrieving relevant images. Thus, the proposed method takes two query ROIs corresponding to two different views (MLO and CC) as input and displays the most similar ROIs to each view using a dynamic similarity assessment. The retrieved ROIs can therefore be analyzed according to their clinical cases for the final decision-making relative to the query ROIs. The experiments realized on the challenging Digital Database for Screening Mammography (DDSM) dataset have proved the effectiveness and the efficiency of the proposed method.},
  archive      = {J_PR},
  author       = {Amira Jouirou and Abir Baâzaoui and Walid Barhoumi},
  doi          = {10.1016/j.patcog.2020.107786},
  journal      = {Pattern Recognition},
  pages        = {107786},
  shortjournal = {Pattern Recognition},
  title        = {Multi-view content-based mammogram retrieval using dynamic similarity and locality sensitive hashing},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Unsupervised deep hashing with node representation for
image retrieval. <em>PR</em>, <em>112</em>, 107785. (<a
href="https://doi.org/10.1016/j.patcog.2020.107785">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Supervised graph convolution network (GCN) based hashing algorithms have achieved good results by recognizing images according to the relationships between objects, but they are hard to be applied to label-free scenarios. Besides, most existing unsupervised deep hashing algorithms neglect the relationships between different samples and thus fail to achieve high precision. To address this problem, we propose NRDH, an unsupervised D eep H ashing method with N ode R epresentation for image retrieval , which adopts unsupervised GCN to integrate the relationships between samples into image visual features. NRDH consists of node representation learning stage and hash function learning stage. In the first stage, we treat each image as a node of a graph and design GCN-based AutoEncoder , which can integrate the relationships between samples into node representation. In the second stage, we use above node representations to guide the network and help learn the hash function to fast achieve an end-to-end hash model to generate semantic hash codes. Extensive experiments on CIFAR-10, MS-COCO and FLICKR25K show NRDH can achieve higher performance and outperform the state-of-the-art unsupervised deep hashing methods .},
  archive      = {J_PR},
  author       = {Yangtao Wang and Jingkuan Song and Ke Zhou and Yu Liu},
  doi          = {10.1016/j.patcog.2020.107785},
  journal      = {Pattern Recognition},
  pages        = {107785},
  shortjournal = {Pattern Recognition},
  title        = {Unsupervised deep hashing with node representation for image retrieval},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Type-reduced vague possibilistic fuzzy clustering for
medical images. <em>PR</em>, <em>112</em>, 107784. (<a
href="https://doi.org/10.1016/j.patcog.2020.107784">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Soft computing provides the framework for dealing with the uncertainty and imprecision inherent in real-life applications. Soft computing has become a long-standing notable paradigm for medical image processing . A typical fuzzy clustering uses the fuzzy membership function . Nevertheless, there is an alternative membership representation, known as typicality or possibilistic membership. Unlike fuzzy membership that is probabilistic in nature, typicality represents an absolute membership and it is the degree of belonging of an object to a class that does not depend on its distances from the other classes. However, both fuzzy membership and typicality play important role in assigning membership to an object. This study proposes a novel clustering model that creates a vague environment enriched with the concept of fuzzy membership and typicality, while the use of type-reduction plays an essential role in capturing all the vagueness present in the data set. The proposed model is called type-reduced vague possibilistic fuzzy clustering (TVPFC), and we use MRI images to demonstrate its superior robustness over that of FCM (fuzzy c-means), PCM (possibilistic c-means), VCM (vague c-means) and IPFCM (interval-valued possibilistic fuzzy c-means).},
  archive      = {J_PR},
  author       = {Ankita Bose and Kalyani Mali},
  doi          = {10.1016/j.patcog.2020.107784},
  journal      = {Pattern Recognition},
  pages        = {107784},
  shortjournal = {Pattern Recognition},
  title        = {Type-reduced vague possibilistic fuzzy clustering for medical images},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Variational posterior approximation using stochastic
gradient ascent with adaptive stepsize. <em>PR</em>, <em>112</em>,
107783. (<a href="https://doi.org/10.1016/j.patcog.2020.107783">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scalable algorithms of variational posterior approximation allow Bayesian nonparametrics such as Dirichlet process mixture to scale up to larger dataset at fractional cost. Recent algorithms, notably the stochastic variational inference performs local learning from minibatch. The main problem with stochastic variational inference is that it relies on closed form solution . Stochastic gradient ascent is a modern approach to machine learning and is widely deployed in the training of deep neural networks . In this work, we explore using stochastic gradient ascent as a fast algorithm for the posterior approximation of Dirichlet process mixture. However, stochastic gradient ascent alone is not optimal for learning. In order to achieve both speed and performance, we turn our focus to stepsize optimization in stochastic gradient ascent. As as intermediate approach, we first optimize stepsize using the momentum method. Finally, we introduce Fisher information to allow adaptive stepsize in our posterior approximation. In the experiments, we justify that our approach using stochastic gradient ascent do not sacrifice performance for speed when compared to closed form coordinate ascent learning on these datasets. Lastly, our approach is also compatible with deep ConvNet features as well as scalable to large class datasets such as Caltech256 and SUN397.},
  archive      = {J_PR},
  author       = {Kart-Leong Lim and Xudong Jiang},
  doi          = {10.1016/j.patcog.2020.107783},
  journal      = {Pattern Recognition},
  pages        = {107783},
  shortjournal = {Pattern Recognition},
  title        = {Variational posterior approximation using stochastic gradient ascent with adaptive stepsize},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Real-time sufficient dimension reduction through principal
least squares support vector machines. <em>PR</em>, <em>112</em>,
107768. (<a href="https://doi.org/10.1016/j.patcog.2020.107768">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a real-time approach for sufficient dimension reduction. Compared with popular sufficient dimension reduction methods including sliced inverse regression and principal support vector machines , the proposed principal least squares support vector machines approach enjoys better estimation of the central subspace. Furthermore, this new proposal can be used in the presence of streamed data for quick real-time updates. It is demonstrated through simulations and real data applications that our proposal performs better and faster than existing algorithms in the literature.},
  archive      = {J_PR},
  author       = {Andreas Artemiou and Yuexiao Dong and Seung Jun Shin},
  doi          = {10.1016/j.patcog.2020.107768},
  journal      = {Pattern Recognition},
  pages        = {107768},
  shortjournal = {Pattern Recognition},
  title        = {Real-time sufficient dimension reduction through principal least squares support vector machines},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A plug-in attribute correction module for generalized
zero-shot learning. <em>PR</em>, <em>112</em>, 107767. (<a
href="https://doi.org/10.1016/j.patcog.2020.107767">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While Zero Shot Learning models can recognize new classes without training examples, they often fails to incorporate both seen and unseen classes together at the test time, which is known as the Generalized Zero-shot Learning (GZSL) problem. This paper identifies a bottleneck issue when attributes are not well-defined, reliable, inaccurate in quantitative representations, or suffering from the visual-semantic discrepancy. We propose a Generic Plug-in Attribute Correction (GPAC) module which can effectively accommodate conventional ZSL in GZSL tasks. Different from existing embedding-based approaches which often lose the favor of transparency in attributes, our key challenge is to fully preserve the original meaning of the attributes and make it complementary and interpretable to upgrade existing ZSL models. To this end, we propose a novel nonnegative constraint with iterative Stochastic Gradient Descent toolbox to effectively fit our GPAC module into previous ZSL models. Extensive experiments on five popular datasets show that our method can effectively correct attributes and make conventional ZSL can achieve state-of-the-art performance on GZSL tasks. It is also a good practice for future models when incorporating prior human knowledge.},
  archive      = {J_PR},
  author       = {Haofeng Zhang and Haoyue Bai and Yang Long and Li Liu and Ling Shao},
  doi          = {10.1016/j.patcog.2020.107767},
  journal      = {Pattern Recognition},
  pages        = {107767},
  shortjournal = {Pattern Recognition},
  title        = {A plug-in attribute correction module for generalized zero-shot learning},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). EACOFT: An energy-aware correlation filter for visual
tracking. <em>PR</em>, <em>112</em>, 107766. (<a
href="https://doi.org/10.1016/j.patcog.2020.107766">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Correlation filter based trackers attribute to its calculation in the frequency domain can efficiently locate targets in a relatively fast speed. This characteristic however also limits its generalization in some specific scenarios. The reasons that they still fail to achieve superior performance to state-of-the-art (SOTA) trackers are possibly due to two main aspects. The first is that while tracking the objects whose energy is lower than the background, the tracker may occur drift or even lose the target. The second is that the biased samples may be inevitably selected for model training, which can easily lead to inaccurate tracking. To tackle these shortcomings, a novel energy-aware correlation filter (EACOFT) based tracking method is proposed, in our approach the energy between the foreground and the background is adaptively balanced, which enables the target of interest always having a higher energy than its background. The samples’ qualities are also evaluated in real time, which ensures that the samples used for template training are always helpful with tracking. In addition, we also propose an optimal bottom-up and top-down combined strategy for template training, which plays an important role in improving both the effectiveness and robustness of tracking. As a result, our approach achieves a great improvement on the basis of the baseline tracker, especially under the background clutter and fast motion challenges. Extensive experiments over multiple tracking benchmarks demonstrate the superior performance of our proposed methodology in comparison to a number of the SOTA trackers.},
  archive      = {J_PR},
  author       = {Qiaoyuan Liu and Jinchang Ren and Yuru Wang and Yuanbo Wu and Haijiang Sun and Huimin Zhao},
  doi          = {10.1016/j.patcog.2020.107766},
  journal      = {Pattern Recognition},
  pages        = {107766},
  shortjournal = {Pattern Recognition},
  title        = {EACOFT: An energy-aware correlation filter for visual tracking},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Behavior regularized prototypical networks for
semi-supervised few-shot image classification. <em>PR</em>,
<em>112</em>, 107765. (<a
href="https://doi.org/10.1016/j.patcog.2020.107765">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a Behavior Regularized Prototypical Network (BR-ProtoNet) for few-shot image classification in semi-supervised scenarios. To learn a generalizable metric, we exploit readily-available unlabeled data and construct complementary constraints to regularize the model’s behavior. Specifically, we match the label spaces between each episode and the whole training set. The predictions on the unlabeled data over different episodes can be aggregated to capture more reliable category information. We further construct new instances via adversarial perturbation and interpolation. These instances regularize the model’s behavior over the neighborhoods of the original ones and along the interpolation paths among them. In addition, they ensure the learnt embedding space possesses the property of proximity preservation. The regularization of these aspects is incorporated into the optimization process of BR-ProtoNet on partially labeled data. We have conducted thorough experiments on multiple challenging benchmarks. The results suggest that the metric learning can significantly benefit from the proposed regularization, and thus leading to the state-of-the-art performance in semi-supervised few-shot image classification.},
  archive      = {J_PR},
  author       = {Shixin Huang and Xiangping Zeng and Si Wu and Zhiwen Yu and Mohamed Azzam and Hau-San Wong},
  doi          = {10.1016/j.patcog.2020.107765},
  journal      = {Pattern Recognition},
  pages        = {107765},
  shortjournal = {Pattern Recognition},
  title        = {Behavior regularized prototypical networks for semi-supervised few-shot image classification},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Scale variance minimization for unsupervised domain
adaptation in image segmentation. <em>PR</em>, <em>112</em>, 107764. (<a
href="https://doi.org/10.1016/j.patcog.2020.107764">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We focus on unsupervised domain adaptation (UDA) in image segmentation . Existing works address this challenge largely by aligning inter-domain representations, which may lead over-alignment that impairs the semantic structures of images and further target-domain segmentation performance . We design a scale variance minimization (SVMin) method by enforcing the intra-image semantic structure consistency in the target domain. Specifically, SVMin leverages an intrinsic property that simple scale transformation has little effect on the semantic structures of images. It thus introduces certain supervision in the target domain by imposing a scale-invariance constraint while learning to segment an image and its scale-transformation concurrently. Additionally, SVMin is complementary to most existing UDA techniques and can be easily incorporated with consistent performance boost but little extra parameters. Extensive experiments show that our method achieves superior domain adaptive segmentation performance as compared with the state-of-the-art. Preliminary studies show that SVMin can be easily adapted for UDA-based image classification .},
  archive      = {J_PR},
  author       = {Dayan Guan and Jiaxing Huang and Shijian Lu and Aoran Xiao},
  doi          = {10.1016/j.patcog.2020.107764},
  journal      = {Pattern Recognition},
  pages        = {107764},
  shortjournal = {Pattern Recognition},
  title        = {Scale variance minimization for unsupervised domain adaptation in image segmentation},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Compact class-conditional domain invariant learning for
multi-class domain adaptation. <em>PR</em>, <em>112</em>, 107763. (<a
href="https://doi.org/10.1016/j.patcog.2020.107763">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural network-based models have recently shown excellent performance in various kinds of tasks. However, a large amount of labeled data is required to train deep networks, and the cost of gathering labeled training data for every kind of domain is prohibitively expensive. Domain adaptation tries to solve this problem by transferring knowledge from labeled source domain data to unlabeled target domain data . Previous research tried to learn domain-invariant features of source and target domains to address this problem, and this approach has been used as a key concept in various methods. However, domain-invariant features do not mean that a classifier trained on source data can be directly applied to target data because it does not guarantee that data distribution of the same classes will be aligned across two domains. In this paper, we present novel generalization upper bounds for domain adaptation that motivates the need for class-conditional domain invariant learning. Based on this theoretical framework, we then propose a class-conditional domain invariant learning method that can learn a feature space in which features in the same class are expected to be mapped nearby. We empirically experimented that our model showed state-of-the-art performance on standard datasets and showed effectiveness by visualization of latent space.},
  archive      = {J_PR},
  author       = {Woojin Lee and Hoki Kim and Jaewook Lee},
  doi          = {10.1016/j.patcog.2020.107763},
  journal      = {Pattern Recognition},
  pages        = {107763},
  shortjournal = {Pattern Recognition},
  title        = {Compact class-conditional domain invariant learning for multi-class domain adaptation},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Semi-supervised kernel matrix learning using adaptive
constraint-based seed propagation. <em>PR</em>, <em>112</em>, 107750.
(<a href="https://doi.org/10.1016/j.patcog.2020.107750">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose semi-supervised kernel matrix learning (SS-KML) using adaptive constraint-based seed propagation (ACSP). Conventional SS-KML methods such as pairwise constraint propagation (PCP) and kernel propagation (KP) have achieved outstanding performance in data classification . However, they are likely to distort the global data structure because of using hard constraints in their semi-definite problems (SDPs) for constraint propagation. Moreover, given a large number of pairwise constraints and a large amount of samples, they tend to be incredibly complex, thus being hard to be applied to real-life complex problems such as internet-scale image categorization. To address this problem, we utilize adaptive constraints to effectively maintain the inherent coherence of samples and successfully propagate constraint information into all samples. Moreover, we adopt seed propagation to remarkably reduce the computational complexity of SS-KML. Experimental results demonstrate that ACSP achieves a significant improvement in performance over PCP and KP in terms of both effectiveness and efficiency.},
  archive      = {J_PR},
  author       = {Meng Jian and Cheolkon Jung},
  doi          = {10.1016/j.patcog.2020.107750},
  journal      = {Pattern Recognition},
  pages        = {107750},
  shortjournal = {Pattern Recognition},
  title        = {Semi-supervised kernel matrix learning using adaptive constraint-based seed propagation},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A simple graph embedding for anomaly detection in a stream
of heterogeneous labeled graphs. <em>PR</em>, <em>112</em>, 107746. (<a
href="https://doi.org/10.1016/j.patcog.2020.107746">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we propose a new approach to detect anomalous graphs in a stream of directed and labeled heterogeneous edges. The stream consists of a sequence of edges derived from different graphs. Each of these dynamic graphs represents the evolution of a specific activity in a monitored system whose events are acquired in real-time. Our approach is based on graph clustering and uses a simple graph embedding based on substructures and graph edit distance. Our graph representation is flexible and updates incrementally the graph vectors as soon as a new edge arrives. This allows the detection of anomalies in real-time which is an important requirement for sensitive applications such as cyber-security. Our implementation results prove the effectiveness of our approach in terms of accuracy of detection and time processing.},
  archive      = {J_PR},
  author       = {Abd Errahmane Kiouche and Sofiane Lagraa and Karima Amrouche and Hamida Seba},
  doi          = {10.1016/j.patcog.2020.107746},
  journal      = {Pattern Recognition},
  pages        = {107746},
  shortjournal = {Pattern Recognition},
  title        = {A simple graph embedding for anomaly detection in a stream of heterogeneous labeled graphs},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). GraphAIR: Graph representation learning with neighborhood
aggregation and interaction. <em>PR</em>, <em>112</em>, 107745. (<a
href="https://doi.org/10.1016/j.patcog.2020.107745">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph representation learning is of paramount importance for a variety of graph analytical tasks, ranging from node classification to community detection. Recently, graph convolutional networks (GCNs) have been successfully applied for graph representation learning. These GCNs generate node representation by aggregating features from the neighborhoods, which follows the “neighborhood aggregation” scheme. In spite of having achieved promising performance on various tasks, existing GCN-based models have difficulty in well capturing complicated non-linearity of graph data. In this paper, we first theoretically prove that coefficients of the neighborhood interacting terms are relatively small in current models, which explains why GCNs barely outperforms linear models. Then, in order to better capture the complicated non-linearity of graph data, we present a novel GraphAIR framework which models the neighborhood interaction in addition to neighborhood aggregation. Comprehensive experiments conducted on benchmark tasks including node classification and link prediction using public datasets demonstrate the effectiveness of the proposed method.},
  archive      = {J_PR},
  author       = {Fenyu Hu and Yanqiao Zhu and Shu Wu and Weiran Huang and Liang Wang and Tieniu Tan},
  doi          = {10.1016/j.patcog.2020.107745},
  journal      = {Pattern Recognition},
  pages        = {107745},
  shortjournal = {Pattern Recognition},
  title        = {GraphAIR: Graph representation learning with neighborhood aggregation and interaction},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hyperspectral image classification via discriminative
convolutional neural network with an improved triplet loss. <em>PR</em>,
<em>112</em>, 107744. (<a
href="https://doi.org/10.1016/j.patcog.2020.107744">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyper-Spectral Image (HSI) classification is an important task because of its wide range of applications. With the remarkable success from the Convolutional Neural Network (CNN), the performance of HSI classification has been significantly improved. However, two main challenges remained. One is that the samples of HSI have dramatic intra-class diversity and inter-class similarity, and the conventional cross-entropy loss is not good enough to learn discriminative features . The other is that the number of the training samples is so limited that the network is easy to overfit. To address the first challenge, we develop an improved triplet loss in order to make samples from the same class close to each other and make samples from different classes further apart. The proposed loss function considers all the possible positive pairs and negative pairs in a training batch, filters many trivial pairs, and prevents the impact of the outliers at the same time. To deal with the second challenge, we design an appropriate network architecture with less learnable parameters. We train the designed network based on the proposed loss with randomly initialized network weights using only hundreds of training samples, and attain quite good results. The experimental results show that the proposed method significantly surpasses other state-of-the-art methods, especially with less training samples. Furthermore, being less complex, the training process only takes a few minutes on a single GPU , which is faster than other state-of-the-art CNN-based methods.},
  archive      = {J_PR},
  author       = {Ke-Kun Huang and Chuan-Xian Ren and Hui Liu and Zhao-Rong Lai and Yu-Feng Yu and Dao-Qing Dai},
  doi          = {10.1016/j.patcog.2020.107744},
  journal      = {Pattern Recognition},
  pages        = {107744},
  shortjournal = {Pattern Recognition},
  title        = {Hyperspectral image classification via discriminative convolutional neural network with an improved triplet loss},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Heterogeneous ensemble selection for evolving data streams.
<em>PR</em>, <em>112</em>, 107743. (<a
href="https://doi.org/10.1016/j.patcog.2020.107743">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ensemble learning has been widely applied to both batch data classification and streaming data classification. For the latter setting, most existing ensemble systems are homogenous, which means they are generated from only one type of learning model. In contrast, by combining several types of different learning models, a heterogeneous ensemble system can achieve greater diversity among its members, which helps to improve its performance. Although heterogeneous ensemble systems have achieved many successes in the batch classification setting, it is not trivial to extend them directly to the data stream setting. In this study, we propose a novel HEterogeneous Ensemble Selection (HEES) method, which dynamically selects an appropriate subset of base classifiers to predict data under the stream setting. We are inspired by the observation that a well-chosen subset of good base classifiers may outperform the whole ensemble system. Here, we define a good candidate as one that expresses not only high predictive performance but also high confidence in its prediction. Our selection process is thus divided into two sub-processes: accurate-candidate selection and confident-candidate selection. We define an accurate candidate in the stream context as a base classifier with high accuracy over the current concept, while a confident candidate as one with a confidence score higher than a certain threshold. In the first sub-process, we employ the prequential accuracy to estimate the performance of a base classifier at a specific time, while in the latter sub-process, we propose a new measure to quantify the predictive confidence and provide a method to learn the threshold incrementally. The final ensemble is formed by taking the intersection of the sets of confident classifiers and accurate classifiers. Experiments on a wide range of data streams show that the proposed method achieves competitive performance with lower running time in comparison to the state-of-the-art online ensemble methods.},
  archive      = {J_PR},
  author       = {Anh Vu Luong and Tien Thanh Nguyen and Alan Wee-Chung Liew and Shilin Wang},
  doi          = {10.1016/j.patcog.2020.107743},
  journal      = {Pattern Recognition},
  pages        = {107743},
  shortjournal = {Pattern Recognition},
  title        = {Heterogeneous ensemble selection for evolving data streams},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Joint adaptive manifold and embedding learning for
unsupervised feature selection. <em>PR</em>, <em>112</em>, 107742. (<a
href="https://doi.org/10.1016/j.patcog.2020.107742">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As data always lie on a lower-dimensional space, feature selection has become an important step in computer vision , machine learning and data mining. Due to the lack of class information, the performance of unsupervised feature selection depends on how to characterize and preserve the manifold structure among data. In this paper, we propose a novel unsupervised feature selection framework, named as joint adaptive manifold and embedding learning for unsupervised feature selection (JAMEL). It iteratively and adaptively learns lower-dimensional embeddings for data to preserve the manifold structure among data, regresses data to embeddings to measure the importance of features, and learns the manifold structure among data according to the data density in the intrinsic space, where the redundant and noisy features are eliminated. In addition, we present an efficient algorithm to solve the proposed problem, together with the convergence analysis . Finally, the evaluation results with the tasks of k k -means, spectral clustering and nearest neighbor classification using the selected features on 12 datasets show the effectiveness and efficiency of our approach.},
  archive      = {J_PR},
  author       = {Jian-Sheng Wu and Meng-Xiao Song and Weidong Min and Jian-Huang Lai and Wei-Shi Zheng},
  doi          = {10.1016/j.patcog.2020.107742},
  journal      = {Pattern Recognition},
  pages        = {107742},
  shortjournal = {Pattern Recognition},
  title        = {Joint adaptive manifold and embedding learning for unsupervised feature selection},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). EF-net: A novel enhancement and fusion network for RGB-d
saliency detection. <em>PR</em>, <em>112</em>, 107740. (<a
href="https://doi.org/10.1016/j.patcog.2020.107740">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Salient object detection (SOD) has gained tremendous attention in the field of computer vision . Multi-modal SOD based on the complementary information from RGB images and depth maps has shown remarkable success, making RGB-D saliency detection an active research topic. In this paper, we propose a novel multi-modal enhancement and fusion network (EF-Net) for effective RGB-D saliency detection. Specifically, we first utilize a color hint map module with RGB images to predict a hint map, which encodes the coarse information of salient objects. The resulting hint map is then utilized to enhance the depth map with our depth enhancement module , which suppresses the noise and sharpens the object boundary. Finally, we propose an effective layer-wise aggregation module to fuse the features extracted from the enhanced depth maps and RGB images for the accurate detection of salient objects. Our EF-Net utilizes an enhancement-and-fusion framework for saliency detection, which makes full use of the information from RGB images and depth maps. In addition, our depth enhancement module effectively resolves the low-quality issue of depth maps, which boosts the saliency detection performance remarkably. Extensive experiments on five widely-used benchmark datasets demonstrate that our method outperforms 12 state-of-the-art RGB-D saliency detection approaches in terms of five key evaluation metrics .},
  archive      = {J_PR},
  author       = {Qian Chen and Keren Fu and Ze Liu and Geng Chen and Hongwei Du and Bensheng Qiu and Ling Shao},
  doi          = {10.1016/j.patcog.2020.107740},
  journal      = {Pattern Recognition},
  pages        = {107740},
  shortjournal = {Pattern Recognition},
  title        = {EF-net: A novel enhancement and fusion network for RGB-D saliency detection},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Imprecise gaussian discriminant classification. <em>PR</em>,
<em>112</em>, 107739. (<a
href="https://doi.org/10.1016/j.patcog.2020.107739">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gaussian discriminant analysis is a popular classification model , that in the precise case can produce unreliable predictions in case of high uncertainty (e.g., due to scarce or noisy data). While imprecise probability theory offers a nice theoretical framework to solve such issues, it has not been yet applied to Gaussian discriminant analysis. This work remedies this, by proposing a new Gaussian discriminant analysis based on robust Bayesian analysis and near-ignorance priors. The model delivers cautious predictions, in form of set-valued class, in case of limited or imperfect available information. We present and discuss results of experimentation on real and synthetic datasets , where for this latter we corrupt the test instance to see how our approach reacts to non i.i.d. samples. Experiments show that including an imprecise component in the Gaussian discriminant analysis produces reasonably cautious predictions, and that set-valued predictions correspond to instances for which the precise model performs poorly.},
  archive      = {J_PR},
  author       = {Yonatan Carlos Carranza Alarcón and Sébastien Destercke},
  doi          = {10.1016/j.patcog.2020.107739},
  journal      = {Pattern Recognition},
  pages        = {107739},
  shortjournal = {Pattern Recognition},
  title        = {Imprecise gaussian discriminant classification},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust visual tracking via spatio-temporal adaptive and
channel selective correlation filters. <em>PR</em>, <em>112</em>,
107738. (<a href="https://doi.org/10.1016/j.patcog.2020.107738">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, Discriminative Correlation Filter (DCF) based tracking methods have achieved impressive performance in visual tracking. However, their excellent performance usually comes at the cost of sacrificing the computational speed. Furthermore, training correlation filters using high dimensional raw features may introduce the risk of severe over-fitting. To address the above issues, we propose Spatio-Temporal adaptive and Channel selective Correlation Filters (STCCF) for robust tracking. Specifically, we first select a set of target-specific features from high dimensional features via an effective channel selective scheme based on the Taylor expansion . Then, we reformulate the filter learning problem from ridge regression to elastic net regression to adaptively select the discriminative features inside the target bounding box at the spatial level. Moreover, we constrain the filters to be adaptive across temporal frames by learning a transformation matrix from the initial filters to the previous filters. In particular, with a specific spatio-temporal-channel constraint, STCCF can not only alleviate the over-fitting problem and reduce the computational cost, but also enhance the discriminability and interpretability of the learned filters. The proposed STCCF can be optimized by using a few iterations of Alternating Direction Method of Multipliers (ADMM). Experiments on six challenging datasets show that STCCF can achieve promising performance with fast running speed.},
  archive      = {J_PR},
  author       = {Yanjie Liang and Yi Liu and Yan Yan and Liming Zhang and Hanzi Wang},
  doi          = {10.1016/j.patcog.2020.107738},
  journal      = {Pattern Recognition},
  pages        = {107738},
  shortjournal = {Pattern Recognition},
  title        = {Robust visual tracking via spatio-temporal adaptive and channel selective correlation filters},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A hierarchical weighted low-rank representation for image
clustering and classification. <em>PR</em>, <em>112</em>, 107736. (<a
href="https://doi.org/10.1016/j.patcog.2020.107736">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-rank representation (LRR), which is a powerful method to find the low-dimensional subspace structure embedded in high-dimensional data spaces, has been used in both unsupervised learning and semi-supervised classification. LRR aims at finding the lowest rank representation that can express each data sample as linear combination of other samples. However, this method doesn’t consider the geometrical structure of the data. Thus the similarity and local structure might be lost in the process of learning. Motivated by this, a novel hierarchical weighted low-rank representation (HWLRR) is proposed in this paper. In the new algorithm, a hierarchical weighted matrix is defined to find more samples that may belong to the same subspace using affinity propagation . By taking advantage of the affinity propagation, our proposed method can preserve both local and global structure of the whole dataset. The experimental results on both unsupervised learning and semi-supervised classification demonstrate the superiority of our proposed method.},
  archive      = {J_PR},
  author       = {Zhiqiang Fu and Yao Zhao and Dongxia Chang and Yiming Wang},
  doi          = {10.1016/j.patcog.2020.107736},
  journal      = {Pattern Recognition},
  pages        = {107736},
  shortjournal = {Pattern Recognition},
  title        = {A hierarchical weighted low-rank representation for image clustering and classification},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Cross-modal discriminant adversarial network. <em>PR</em>,
<em>112</em>, 107734. (<a
href="https://doi.org/10.1016/j.patcog.2020.107734">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-modal retrieval aims at retrieving relevant points across different modalities, such as retrieving images via texts. One key challenge of cross-modal retrieval is narrowing the heterogeneous gap across diverse modalities. To overcome this challenge, we propose a novel method termed as Cross-modal discriminant Adversarial Network (CAN). Taking bi-modal data as a showcase, CAN consists of two parallel modality-specific generators, two modality-specific discriminators , and a Cross-modal Discriminant Mechanism (CDM). To be specific, the generators project diverse modalities into a latent cross-modal discriminant space. Meanwhile, the discriminators compete against the generators to alleviate the heterogeneous discrepancy in this space, i.e. , the generators try to generate unified features to confuse the discriminators, and the discriminators aim to classify the generated results. To further remove the redundancy and preserve the discrimination, we propose CDM to project the generated results into a single common space, accompanying with a novel eigenvalue-based loss. Thanks to the eigenvalue-based loss, CDM could push as much discriminative power as possible into all latent directions. To demonstrate the effectiveness of our CAN, comprehensive experiments are conducted on four multimedia datasets comparing with 15 state-of-the-art approaches.},
  archive      = {J_PR},
  author       = {Peng Hu and Xi Peng and Hongyuan Zhu and Jie Lin and Liangli Zhen and Wei Wang and Dezhong Peng},
  doi          = {10.1016/j.patcog.2020.107734},
  journal      = {Pattern Recognition},
  pages        = {107734},
  shortjournal = {Pattern Recognition},
  title        = {Cross-modal discriminant adversarial network},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Enhancing in-tree-based clustering via distance ensemble and
kernelization. <em>PR</em>, <em>112</em>, 107731. (<a
href="https://doi.org/10.1016/j.patcog.2020.107731">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, we have proposed a novel physically-inspired method, called the Nearest Descent (ND), which plays the role of organizing all the samples into an effective Graph, called the in-tree. Due to its effective characteristics, this in-tree proves very suitable for data clustering . Nevertheless, this in-tree-based clustering still has some non-trivial limitations in terms of robustness, capability, etc. In this study, we first propose a distance-ensemble-based framework for the in-tree-based clustering, which proves a very convenient way to overcome the robustness limitation in our previous in-tree-based clustering. To enhance the capability of the in-tree-based clustering in handling extremely linearly-inseparable clusters, we kernelize the proposed ensemble-based clustering via the so-called kernel trick. As a result, the improved in-tree-based clustering method achieves high robustness and accuracy on diverse challenging synthetic and real-world datasets, showing a certain degree of practical value.},
  archive      = {J_PR},
  author       = {Teng Qiu and Yongjie Li},
  doi          = {10.1016/j.patcog.2020.107731},
  journal      = {Pattern Recognition},
  pages        = {107731},
  shortjournal = {Pattern Recognition},
  title        = {Enhancing in-tree-based clustering via distance ensemble and kernelization},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Averaging GPS segments competition 2019. <em>PR</em>,
<em>112</em>, 107730. (<a
href="https://doi.org/10.1016/j.patcog.2020.107730">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Averaging GPS trajectories is needed in applications such as automatic generation of road network and finding representative movement patterns. We organized a challenge where participants submitted proposals to solve the averaging problem. In this paper, we review the proposals and evaluate their performance. We present a synthesis of the submitted methods and develop a new baseline composed of the well-performing components. The new baseline outperforms all existing averaging methods. All datasets, submissions and evaluations can be accessed on the competition webpage: http://cs.uef.fi/sipu/segments .},
  archive      = {J_PR},
  author       = {Pasi Fränti and Radu Mariescu-Istodor},
  doi          = {10.1016/j.patcog.2020.107730},
  journal      = {Pattern Recognition},
  pages        = {107730},
  shortjournal = {Pattern Recognition},
  title        = {Averaging GPS segments competition 2019},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Infrared small target detection via adaptive m-estimator
ring top-hat transformation. <em>PR</em>, <em>112</em>, 107729. (<a
href="https://doi.org/10.1016/j.patcog.2020.107729">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Top-Hat transformation is an essential technology in the field of infrared small target detection. Many modified Top-Hat transformation methods have been proposed based on the different structure of structural elements. However, these methods are still hard to handle the dim targets and complex background. It can be summarized as two reasons, one is that the structural elements cannot suppress the background adaptively due to the fixed value of structural elements in image. Another is that simple structural element cannot utilize the local feature for target enhancement. To overcome these two limitations, a special ring Top-Hat transformation based on M-estimator and local entropy is proposed in this paper. First, an adaptive ring structural element based on M-estimator is used to suppress the complex background. Second, a novel local entropy is proposed to weight structural element for capturing local feature and target enhancement. Finally, a comparison experiment based on massive infrared image data (more than 500 infrared target images) is done. And the results demonstrate that the proposed algorithm acquires better performance compared with some recent methods.},
  archive      = {J_PR},
  author       = {Lizhen Deng and Jieke Zhang and Guoxia Xu and Hu Zhu},
  doi          = {10.1016/j.patcog.2020.107729},
  journal      = {Pattern Recognition},
  pages        = {107729},
  shortjournal = {Pattern Recognition},
  title        = {Infrared small target detection via adaptive M-estimator ring top-hat transformation},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A simulated annealing algorithm with a dual perturbation
method for clustering. <em>PR</em>, <em>112</em>, 107713. (<a
href="https://doi.org/10.1016/j.patcog.2020.107713">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering is a powerful tool in exploratory data analysis that partitions a set of objects into clusters with the goal of maximizing the similarity of objects within each cluster. Due to the tendency of clustering algorithms to find suboptimal partitions of data, the approximation method Simulated Annealing (SA) has been used to search for near-optimal partitions. However, existing SA-based partitional clustering algorithms still settle to local optima. We propose a new SA-based clustering algorithm, the Simulated Annealing with Gaussian Mutation and Distortion Equalization algorithm (SAGMDE), which uses two perturbation methods to allow for both large and small perturbations in solutions. Our experiments on a diverse collection of data sets show that SAGMDE performs more consistently and yields better results than existing SA clustering algorithms in terms of cluster quality while maintaining a reasonable runtime. Finally, we use generative art as a visualization tool to compare various partitional clustering algorithms.},
  archive      = {J_PR},
  author       = {Julian Lee and David Perkins},
  doi          = {10.1016/j.patcog.2020.107713},
  journal      = {Pattern Recognition},
  pages        = {107713},
  shortjournal = {Pattern Recognition},
  title        = {A simulated annealing algorithm with a dual perturbation method for clustering},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). CPM: A general feature dependency pattern mining framework
for contrast multivariate time series. <em>PR</em>, <em>112</em>,
107711. (<a href="https://doi.org/10.1016/j.patcog.2020.107711">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With recent advances in sensor technology, multivariate time series data are becoming extremely large with sophisticated but insightful inter-variable dependency patterns. Mining contrast dependency patterns in controlled experiments can help quantify the differences between control and experimental time series, however, overwhelms practitioners’ capability. Existing methods suffer from determining whether the differences are caused by the intervention or by different states. We propose a novel Contrast Pattern Mining (CPM) framework to find the intervention-related differences by jointly determining and characterizing the dynamic states in both time series via multivariate Gaussian distributions. Under the CPM framework, we not only propose a new covariance-based contrast pattern model, but also integrate our previous proposed partial correlation-based model as a special case. An efficient generic algorithm is developed to optimize various CPM models by adjusting one of the sub-routines. Comprehensive experiments are conducted to analyze the effectiveness, scalability, utility, and interpretability of the proposed framework.},
  archive      = {J_PR},
  author       = {Qingzhe Li and Liang Zhao and Yi-Ching Lee and Avesta Sassan and Jessica Lin},
  doi          = {10.1016/j.patcog.2020.107711},
  journal      = {Pattern Recognition},
  pages        = {107711},
  shortjournal = {Pattern Recognition},
  title        = {CPM: A general feature dependency pattern mining framework for contrast multivariate time series},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On parameterizing higher-order motion for behaviour
recognition. <em>PR</em>, <em>112</em>, 107710. (<a
href="https://doi.org/10.1016/j.patcog.2020.107710">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human behaviours consist different types of motion; we show how they can be disambiguated into their components in a richer way than that currently possible. Studies on optical flow have concentrated on motion alone without the higher order components: snap, jerk and acceleration. We are the first to show how the acceleration, jerk, snap and their constituent parts can be obtained from image sequences, and can be deployed for analysis, especially of behaviour. We demonstrate the estimation of acceleration in sport, human motion, traffic and in scenes of violent behaviour to demonstrate the wide potential for application of analysis of acceleration. Determining higher order components is suited to the analysis of scenes which contain them: higher order motion is innate to scenes containing acts of violent behaviour, but it is not just for behaviour which contains quickly changing movement: human gait contains acceleration though approaches have yet to consider radial and tangential acceleration, since they concentrate on motion alone. The analysis of synthetic and real-world images illustrates the ability of higher order motion to discriminate different objects under different motion. Then the new approaches are applied in heel strike detection in the analysis of human gait. These results demonstrate that the new approach is ready for developing new applications in behaviour recognition and provides a new basis for future research and applications of higher-order motion analysis.},
  archive      = {J_PR},
  author       = {Yan Sun and Jonathon S. Hare and Mark S. Nixon},
  doi          = {10.1016/j.patcog.2020.107710},
  journal      = {Pattern Recognition},
  pages        = {107710},
  shortjournal = {Pattern Recognition},
  title        = {On parameterizing higher-order motion for behaviour recognition},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Connectivity-based convolutional neural network for
classifying point clouds. <em>PR</em>, <em>112</em>, 107708. (<a
href="https://doi.org/10.1016/j.patcog.2020.107708">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The acquisition of point clouds with a 3D scanner often yields large-scale, irregular, and unordered raw data, which hinders the classification of objects from these data. Some studies have introduced a method of applying the point clouds to convolutional neural networks (CNNs). This is achieved after preprocessing the volume metrics or multi-view images. However, this method has a limited resolution and a low classification accuracy in comparison to heavy computation in object classification. In this paper, DenX-Conv is proposed to improve the accuracy of object classification while securing the connectivity of points from the raw point cloud. DenX-Conv can extract effective local geometric features by finding the neighbor connectivity based on the geometric topology information of the points. In addition, stable feature learning is made possible by applying a densely connected network to PointCNN&#39;s χ-Conv. Application of DenX-Conv to the ModelNet40 dataset resulted in a classification accuracy of 92.5\%.},
  archive      = {J_PR},
  author       = {Jinwon Lee and Sang-Uk Cheon and Jeongsam Yang},
  doi          = {10.1016/j.patcog.2020.107708},
  journal      = {Pattern Recognition},
  pages        = {107708},
  shortjournal = {Pattern Recognition},
  title        = {Connectivity-based convolutional neural network for classifying point clouds},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning to transfer focus of graph neural network for scene
graph parsing. <em>PR</em>, <em>112</em>, 107707. (<a
href="https://doi.org/10.1016/j.patcog.2020.107707">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scene graph parsing has become a new challenge in the field of image understanding and pattern recognition in recent years. It captures objects and their relationships, and provides a structured representation of the visual scene. Among the three types of high-level relationships of scene graphs, semantic relationships, which contain the global understanding of the scene, are the core and the most valuable, while geometric and possessive relationships contain local and limited information. However, semantic relationships have the characteristics of multiple types and fewer instances, leading to a low recognition rate of most semantic relationships by existing detectors. To address this issue, this paper proposes a new architecture, the graphical focal network, which uses a decision-level global detector to capture the dependencies between object and relationship local detectors. We construct a graphical focal loss, which overcomes the lack of semantic relationship instances by adjusting the proportion of relationship loss based on the degree of relationship rarity and learning difficulty, and improves the stability of key object recognition by adjusting the proportion of object loss based on the degree of node connectivity and the value of neighborhood relationships. The proposed relative depth encoding module and regional layout encoding module, respectively, introduce relative depth information and more effective geometric layout information between objects, thereby further improving the performance. Experiments using the Visual Genome benchmark show that our method outperforms the most advanced competitors in two types of performance metrics. For semantic types, the recognition rate of our method is 2.0 times that of the baseline.},
  archive      = {J_PR},
  author       = {Junjie Jiang and Zaixing He and Shuyou Zhang and Xinyue Zhao and Jianrong Tan},
  doi          = {10.1016/j.patcog.2020.107707},
  journal      = {Pattern Recognition},
  pages        = {107707},
  shortjournal = {Pattern Recognition},
  title        = {Learning to transfer focus of graph neural network for scene graph parsing},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Reconstruction by inpainting for visual anomaly detection.
<em>PR</em>, <em>112</em>, 107706. (<a
href="https://doi.org/10.1016/j.patcog.2020.107706">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual anomaly detection addresses the problem of classification or localization of regions in an image that deviate from their normal appearance. A popular approach trains an auto-encoder on anomaly-free images and performs anomaly detection by calculating the difference between the input and the reconstructed image. This approach assumes that the auto-encoder will be unable to accurately reconstruct anomalous regions. But in practice neural networks generalize well even to anomalies and reconstruct them sufficiently well, thus reducing the detection capabilities. Accurate reconstruction is far less likely if the anomaly pixels were not visible to the auto-encoder. We thus cast anomaly detection as a self-supervised reconstruction-by-inpainting problem. Our approach (RIAD) randomly removes partial image regions and reconstructs the image from partial inpaintings, thus addressing the drawbacks of auto-enocoding methods. RIAD is extensively evaluated on several benchmarks and sets a new state-of-the art on a recent highly challenging anomaly detection benchmark.},
  archive      = {J_PR},
  author       = {Vitjan Zavrtanik and Matej Kristan and Danijel Skočaj},
  doi          = {10.1016/j.patcog.2020.107706},
  journal      = {Pattern Recognition},
  pages        = {107706},
  shortjournal = {Pattern Recognition},
  title        = {Reconstruction by inpainting for visual anomaly detection},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dynamic spectral residual superpixels. <em>PR</em>,
<em>112</em>, 107705. (<a
href="https://doi.org/10.1016/j.patcog.2020.107705">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of segmenting an image into superpixels in the context of k k -means clustering, in which we wish to decompose an image into local, homogeneous regions corresponding to the underlying objects. Our novel approach builds upon the widely used Simple Linear Iterative Clustering (SLIC), and incorporate a measure of objects’ structure based on the spectral residual of an image. Based on this combination, we propose a modified initialisation scheme and search metric, which keeps fine-details. This combination leads to better adherence to object boundaries, while preventing unnecessary segmentation of large, uniform areas, and remaining computationally tractable in comparison to other methods. We demonstrate through numerical and visual experiments that our approach outperforms the state-of-the-art techniques.},
  archive      = {J_PR},
  author       = {Jianchao Zhang and Angelica I. Aviles-Rivero and Daniel Heydecker and Xiaosheng Zhuang and Raymond Chan and Carola-Bibiane Schönlieb},
  doi          = {10.1016/j.patcog.2020.107705},
  journal      = {Pattern Recognition},
  pages        = {107705},
  shortjournal = {Pattern Recognition},
  title        = {Dynamic spectral residual superpixels},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multicamera pedestrian detection using logic minimization.
<em>PR</em>, <em>112</em>, 107703. (<a
href="https://doi.org/10.1016/j.patcog.2020.107703">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper an algorithm for multicamera pedestrian detection is proposed. The first stage of this work is based on the probabilistic occupancy map framework, in which the ground plane is discretized into a grid and the likelihood of pedestrian presence at each location is estimated by comparing a rectangle, of the average size of the pedestrians standing there, with the foreground silhouettes in all camera views. In the second stage, where we borrowed the idea from the Quine-McCluskey method for logic function minimization, essential candidates are initially identified, each of which covers at least a significant part of the foreground that is not covered by the other candidates. Then non-essential candidates are selected to cover the remaining foregrounds by following an iterative process, which alternates between merging redundant candidates and finding emerging essential candidates. Experiments on benchmark video datasets have demonstrated the improved performance of this algorithm in comparison with some benchmark non-deep or deep multicamera/monocular algorithms for pedestrian detection.},
  archive      = {J_PR},
  author       = {Yuyao Yan and Ming Xu and Jeremy S. Smith and Mo Shen and Jin Xi},
  doi          = {10.1016/j.patcog.2020.107703},
  journal      = {Pattern Recognition},
  pages        = {107703},
  shortjournal = {Pattern Recognition},
  title        = {Multicamera pedestrian detection using logic minimization},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Time-series averaging and local stability-weighted dynamic
time warping for online signature verification. <em>PR</em>,
<em>112</em>, 107699. (<a
href="https://doi.org/10.1016/j.patcog.2020.107699">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To meet the recent demands for automated security systems, this study proposes a novel single-template strategy that uses mean templates and local stability-weighted dynamic time warping (LS-DTW) to simultaneously improve the speed and accuracy of online signature verification. Specifically, we adopt a recent time-series averaging method, called Euclidean barycenter-based DTW barycenter averaging (EB-DBA), to obtain an effective mean template set for each feature while preserving intra-user variability among reference samples. We then estimate the local stability of the mean template set by using direct matching points that represent stable signature regions in the DTW warping paths between the mean template set and the references. Subsequently, we boost the discriminative power in the verification phase using the LS-DTW distance measure that incorporates the local stability sequence as the weights for the DTW cost function between the mean template set and a query signature. Finally, we use the public SVC2004 Task2/MCYT-100 online signature datasets and the recent 3DAirSig in-air signature dataset to conduct experiments, whose results confirm the effectiveness of the proposed method in both the random- and skilled-forgery scenarios.},
  archive      = {J_PR},
  author       = {Manabu Okawa},
  doi          = {10.1016/j.patcog.2020.107699},
  journal      = {Pattern Recognition},
  pages        = {107699},
  shortjournal = {Pattern Recognition},
  title        = {Time-series averaging and local stability-weighted dynamic time warping for online signature verification},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Siamese networks with distractor-reduction method for
long-term visual object tracking. <em>PR</em>, <em>112</em>, 107698. (<a
href="https://doi.org/10.1016/j.patcog.2020.107698">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many trackers which divide the tracking process into two stages have recently been proposed to solve the problem of long-term tracking. Their outstanding performance makes them become one of the mainstream algorithms of long-term tracking. To further improve the performance of two-stage tracking algorithms, some improvements are proposed in this paper. (a) A hard negative mining method is proposed. It can optimize the training process of the verification network and bridge the gap between the two sub-networks. (b) The architecture of the verification network is designed as a Siamese structure; therefore, the semantic ambiguity in classification can be alleviated. Extensive experiments performed on benchmarks demonstrate that the proposed approach significantly outperforms the state-of-the-art methods, yielding 7\% relative gain in the VOT2018-LT dataset and 14.2\% relative gain in the OxUvA dataset.},
  archive      = {J_PR},
  author       = {Shiyu Xuan and Shengyang Li and Zifei Zhao and Longxuan Kou and Zhuang Zhou and Gui-Song Xia},
  doi          = {10.1016/j.patcog.2020.107698},
  journal      = {Pattern Recognition},
  pages        = {107698},
  shortjournal = {Pattern Recognition},
  title        = {Siamese networks with distractor-reduction method for long-term visual object tracking},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Stable feature selection using copula based mutual
information. <em>PR</em>, <em>112</em>, 107697. (<a
href="https://doi.org/10.1016/j.patcog.2020.107697">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection is a key step in many machine learning tasks. A majority of the existing methods of feature selection address the problem by devising some scoring function while treating the features independently, thereby overlooking their interdependencies . We leverage the scale invariance property of copula to construct a greedy, supervised feature selection algorithm that maximizes the feature relevance while minimizing the redundant information content. Multivariate copula is used in the proposed copula Based Feature Selection (CBFS) to discover the dependence structure between features. The incorporation of copula-based multivariate dependency in the formulation of mutual information helps avoid averaging over multiple instances of bivariate dependencies, thus eliminating the average estimation error introduced when bivariate dependency is used between a pair of feature variables. Under a controlled setting, our algorithm outperformed the existing best practice methods in warding off the noise in data. On several real and synthetic datasets , the proposed algorithm performed competitively in maximizing classification accuracy . CBFS also outperforms the other methods in terms of its noise tolerance property.},
  archive      = {J_PR},
  author       = {Snehalika Lall and Debajyoti Sinha and Abhik Ghosh and Debarka Sengupta and Sanghamitra Bandyopadhyay},
  doi          = {10.1016/j.patcog.2020.107697},
  journal      = {Pattern Recognition},
  pages        = {107697},
  shortjournal = {Pattern Recognition},
  title        = {Stable feature selection using copula based mutual information},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Client-specific anomaly detection for face presentation
attack detection. <em>PR</em>, <em>112</em>, 107696. (<a
href="https://doi.org/10.1016/j.patcog.2020.107696">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One-class anomaly detection approaches are particularly appealing for use in face presentation attack detection (PAD), especially in an unseen attack scenario, where the system is exposed to novel types of attacks. This work builds upon an anomaly-based formulation of the problem and analyses the merits of deploying client-specific information for face spoofing detection. We propose training one-class client-specific classifiers (both generative and discriminative) using representations obtained from pre-trained deep Convolutional Neural Networks (CNN). In order to incorporate client-specific information, a distinct threshold is set for each client based on subject-specific score distributions, which is then used for decision making at the test time. Through extensive experiments using different one-class systems, it is shown that the use of client-specific information in a one-class anomaly detection formulation (both in model construction as well as decision boundary selection) improves the performance significantly. We also show that anomaly-based solutions have the capacity to perform as well or better than two-class approaches in the unseen attack scenarios. Moreover, it is shown that CNN features obtained from models trained for face recognition appear to discard discriminative traits for spoofing detection and are less capable for PAD compared to the CNNs trained for a generic object recognition task .},
  archive      = {J_PR},
  author       = {Soroush Fatemifar and Shervin Rahimzadeh Arashloo and Muhammad Awais and Josef Kittler},
  doi          = {10.1016/j.patcog.2020.107696},
  journal      = {Pattern Recognition},
  pages        = {107696},
  shortjournal = {Pattern Recognition},
  title        = {Client-specific anomaly detection for face presentation attack detection},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). OAENet: Oriented attention ensemble for accurate facial
expression recognition. <em>PR</em>, <em>112</em>, 107694. (<a
href="https://doi.org/10.1016/j.patcog.2020.107694">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial Expression Recognition (FER) is a challenging yet important research topic owing to its significance with respect to its academic and commercial potentials. In this work, we propose an oriented attention pseudo-siamese network that takes advantage of global and local facial information for high accurate FER. Our network consists of two branches, a maintenance branch that consisted of several convolutional blocks to take advantage of high-level semantic features, and an attention branch that possesses a UNet-like architecture to obtain local highlight information. Specifically, we first input the face image into the maintenance branch. For the attention branch, we calculate the correlation coefficient between a face and its sub-regions. Next, we construct a weighted mask by correlating the facial landmarks and the correlation coefficients. Then, the weighted mask is sent to the attention branch. Finally, the two branches are fused to output the classification results . As such, a direction-dependent attention mechanism is established to remedy the limitation of insufficient utilization of local information. With the help of our attention mechanism, our network not only grabs a global picture but can also concentrate on important local areas. Experiments are carried out on 4 leading facial expression datasets. Our method has achieved a very appealing performance compared to other state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Zhengning Wang and Fanwei Zeng and Shuaicheng Liu and Bing Zeng},
  doi          = {10.1016/j.patcog.2020.107694},
  journal      = {Pattern Recognition},
  pages        = {107694},
  shortjournal = {Pattern Recognition},
  title        = {OAENet: Oriented attention ensemble for accurate facial expression recognition},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SVMs multi-class loss feedback based discriminative
dictionary learning for image classification. <em>PR</em>, <em>112</em>,
107690. (<a href="https://doi.org/10.1016/j.patcog.2020.107690">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The learning model has been popular recently due to its promising results in various image classification tasks. Many existing learning methods, especially the deep learning methods , need a large amount of training data to achieve a high accuracy of classification. Conversely, only provided with a small-size dataset, some dictionary learning (DL) methods can achieve a perfect performance on a image classification task and hence still get a lot of attention. Among these DL methods, DL based feature learning methods are the mainstream for image classification in recent years, however, most of these methods have trained a classifier independently from dictionary learning. Therefore, the features extracted by the learned dictionary may not be very proper to perform classification for the classifier. Inspired by the feedback mechanism in cybernetics, this paper proposes a novel discriminative DL framework, named support vector machines (SVMs) multi-class loss feedback based discriminative dictionary learning (SMLFDL) that learns a discriminative dictionary while training SVMs to make the features extracted by the learned dictionary and SVMs better matched with each other. Because of integrating dictionary learning and SVMs training into a unified learning framework and good exactness of the looped multi-class loss term formulated from the feedback viewpoint for the classification scheme , better classification performance can be achieved. Experimental results on several widely used image databases show that SMLFDL can achieve a competitive performance with other state-of-the-art dictionary learning methods.},
  archive      = {J_PR},
  author       = {Bao-Qing Yang and Xin-Ping Guan and Jun-Wu Zhu and Chao-Chen Gu and Kai-Jie Wu and Jia-Jie Xu},
  doi          = {10.1016/j.patcog.2020.107690},
  journal      = {Pattern Recognition},
  pages        = {107690},
  shortjournal = {Pattern Recognition},
  title        = {SVMs multi-class loss feedback based discriminative dictionary learning for image classification},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On finite mixture modeling and model-based clustering of
directed weighted multilayer networks. <em>PR</em>, <em>112</em>,
107641. (<a href="https://doi.org/10.1016/j.patcog.2020.107641">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel approach relying on the notion of mixture models is proposed for modeling and clustering directed weighted networks. The developed methodology can be used in a variety of settings including multilayer networks. Computational issues associated with the developed procedure are effectively addressed by the use of MCMC techniques. The utility of the methodology is illustrated on a set of experiments as well as applications to real-life data containing export trade amounts for European countries .},
  archive      = {J_PR},
  author       = {Volodymyr Melnykov and Shuchismita Sarkar and Yana Melnykov},
  doi          = {10.1016/j.patcog.2020.107641},
  journal      = {Pattern Recognition},
  pages        = {107641},
  shortjournal = {Pattern Recognition},
  title        = {On finite mixture modeling and model-based clustering of directed weighted multilayer networks},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Single-shot 3D multi-person pose estimation in complex
images. <em>PR</em>, <em>112</em>, 107534. (<a
href="https://doi.org/10.1016/j.patcog.2020.107534">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a new single shot method for multi-person 3D human pose estimation in complex images. The model jointly learns to locate the human joints in the image, to estimate their 3D coordinates and to group these predictions into full human skeletons. The proposed method deals with a variable number of people and does not need bounding boxes to estimate the 3D poses. It leverages and extends the Stacked Hourglass Network and its multi-scale feature learning to manage multi-person situations. Thus, we exploit a robust 3D human pose formulation to fully describe several 3D human poses even in case of strong occlusions or crops. Then, joint grouping and human pose estimation for an arbitrary number of people are performed using the associative embedding method. Our approach significantly outperforms the state of the art on the challenging CMU Panoptic and a previous single shot method on the MuPoTS-3D dataset. Furthermore, it leads to good results on the complex and synthetic images from the newly proposed JTA Dataset.},
  archive      = {J_PR},
  author       = {Abdallah Benzine and Bertrand Luvison and Quoc Cuong Pham and Catherine Achard},
  doi          = {10.1016/j.patcog.2020.107534},
  journal      = {Pattern Recognition},
  pages        = {107534},
  shortjournal = {Pattern Recognition},
  title        = {Single-shot 3D multi-person pose estimation in complex images},
  volume       = {112},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Fast high-precision ellipse detection method. <em>PR</em>,
<em>111</em>, 107741. (<a
href="https://doi.org/10.1016/j.patcog.2020.107741">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Obtaining an optimal tradeoff between accuracy and efficiency in ellipse detection is a significant challenge. In this paper, we propose a fast, high-precision ellipse detection method that utilizes arc selection and grouping strategies to significantly reduce the computation amount. A fast corner detection algorithm is also proposed. In the proposed method, to generate ellipse candidates comprehensively, both grouped and ungrouped-salient arcs are fitted. Further, the salient ellipse candidates are selected as final detections that are subject to the selection strategy, which realizes both validation and de-redundancy (clustering) functions. A complexity analysis of the method revealed that the detection time is linearly related to the number of edge points. The results of extensive experiments conducted on three public datasets demonstrate that the proposed method is approximately 75\% faster than state-of-the-art methods with comparable or higher precision, and its detection time is less than 30 ms in most cases.},
  archive      = {J_PR},
  author       = {Zepeng Wang and Derong Chen and Jiulu Gong and Changyuan Wang},
  doi          = {10.1016/j.patcog.2020.107741},
  journal      = {Pattern Recognition},
  pages        = {107741},
  shortjournal = {Pattern Recognition},
  title        = {Fast high-precision ellipse detection method},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep feature augmentation for occluded image classification.
<em>PR</em>, <em>111</em>, 107737. (<a
href="https://doi.org/10.1016/j.patcog.2020.107737">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the difficulty in acquiring massive task-specific occluded images, the classification of occluded images with deep convolutional neural networks (CNNs) remains highly challenging. To alleviate the dependency on large-scale occluded image datasets, we propose a novel approach to improve the classification accuracy of occluded images by fine-tuning the pre-trained models with a set of augmented deep feature vectors (DFVs). The set of augmented DFVs is composed of original DFVs and pseudo-DFVs. The pseudo-DFVs are generated by randomly adding difference vectors (DVs), extracted from a small set of clean and occluded image pairs, to the real DFVs. In the fine-tuning, the back-propagation is conducted on the DFV data flow to update the network parameters. The experiments on various datasets and network structures show that the deep feature augmentation significantly improves the classification accuracy of occluded images without a noticeable influence on the performance of clean images. Specifically, on the ILSVRC2012 dataset with synthetic occluded images, the proposed approach achieves 11.21\% and 9.14\% average increases in classification accuracy for the ResNet50 networks fine-tuned on the occlusion-exclusive and occlusion-inclusive training sets, respectively.},
  archive      = {J_PR},
  author       = {Feng Cen and Xiaoyu Zhao and Wuzhuang Li and Guanghui Wang},
  doi          = {10.1016/j.patcog.2020.107737},
  journal      = {Pattern Recognition},
  pages        = {107737},
  shortjournal = {Pattern Recognition},
  title        = {Deep feature augmentation for occluded image classification},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Alignment-free cancelable fingerprint templates with dual
protection. <em>PR</em>, <em>111</em>, 107735. (<a
href="https://doi.org/10.1016/j.patcog.2020.107735">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cancelable biometrics is an important biometric template protection technique. However, many existing cancelable fingerprint templates suffer post-transformation performance deterioration and the attacks via record multiplicity (ARM). In this paper, we design alignment-free cancelable fingerprint templates with dual protection, which is composed of the window-shift-XOR model and the partial discrete wavelet transform . The former defuses the ARM threat and is combined with the latter to provide dual protection and enhance matching performance. The designed cancelable templates meet the requirements of non-invertibility, diversity and revocability and demonstrate superior recognition accuracy, when evaluated over public databases; for example, the Equal Error Rate of the proposed method in the lost-key scenario under the 1vs1 protocol is 0\% for both FVC2002 DB1 and DB2, 1.63\% for FVC2002 DB3, 7.35\% for FVC2004 DB1 and 4.69\% for FVC2004DB2.},
  archive      = {J_PR},
  author       = {Muhammad Shahzad and Song Wang and Guang Deng and Wencheng Yang},
  doi          = {10.1016/j.patcog.2020.107735},
  journal      = {Pattern Recognition},
  pages        = {107735},
  shortjournal = {Pattern Recognition},
  title        = {Alignment-free cancelable fingerprint templates with dual protection},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Biometric key generation based on generated intervals and
two-layer error correcting technique. <em>PR</em>, <em>111</em>, 107733.
(<a href="https://doi.org/10.1016/j.patcog.2020.107733">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As for a biometric key, key management and biometric data security are both important. Existing bio-key generation methods are usually based on the biometric templates or features directly, it may expose user’s biometric data and will further make the biometric data permanently unusable for his secure identification recognitions. In this paper, a fingerprint bio-key generation approach using the feature distance is proposed. We utilize the relative distances among user’s fingerprint minutiae to generate a unique bio-key. Such bio-key is determinable and recoverable via the generation interval scheme. In addition, we use a two-layer error correcting technique to guarantee a better reliability during the data transmission. The experimental results positively show that our approach can ensure higher security of the bio-key and guarantee a good key regeneration rate. Besides, the storage of the original bio-key or any fingerprint template is unnecessary.},
  archive      = {J_PR},
  author       = {Peiyi Wang and Lin You and Gengran Hu and Liqin Hu and Zhihua Jian and Chaoping Xing},
  doi          = {10.1016/j.patcog.2020.107733},
  journal      = {Pattern Recognition},
  pages        = {107733},
  shortjournal = {Pattern Recognition},
  title        = {Biometric key generation based on generated intervals and two-layer error correcting technique},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Cluster-wise unsupervised hashing for cross-modal similarity
search. <em>PR</em>, <em>111</em>, 107732. (<a
href="https://doi.org/10.1016/j.patcog.2020.107732">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-modal hashing similarity retrieval plays dual roles across various applications including search engines and autopilot systems. More generally, these methods also known to reduce the computation and memory storage in a training scheme. The key limitation of current methods are that: (i) they relax the discrete constrains to solve the optimization problem which may defeat the model purpose, (ii) projecting heterogenous data into a latent space may encourage to loss the diverse representations in such data, (iii) transforming real-valued data point to the binary codes always resulting in a loss of information and producing the suboptimal continuous latent space. In this paper, we propose a novel framework to project the original data points from different modalities into its own low-dimensional latent space and finds the cluster centroid points in its a low-dimensional space, using Cluster-wise Unsupervised Hashing (CUH). In particular, the proposed clustering scheme aims to jointly learns the compact hash codes and the corresponding linear hash functions . A discrete optimization framework is developed to learn the unified binary codes across modalities under of the guidance cluster-wise code-prototypes. Extensive experiments over multiple datasets demonstrate the effectiveness of our proposed model in comparison with the state-of-the-art in unsupervised cross-modal hashing tasks.},
  archive      = {J_PR},
  author       = {Lu Wang and Jie Yang and Masoumeh Zareapoor and Zhonglong Zheng},
  doi          = {10.1016/j.patcog.2020.107732},
  journal      = {Pattern Recognition},
  pages        = {107732},
  shortjournal = {Pattern Recognition},
  title        = {Cluster-wise unsupervised hashing for cross-modal similarity search},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Face illumination recovery for the deep learning feature
under severe illumination variations. <em>PR</em>, <em>111</em>, 107724.
(<a href="https://doi.org/10.1016/j.patcog.2020.107724">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The deep learning feature is the best for face recognition nowadays, but its performance exhibits unsatisfactorily under severe illumination variations. The main reason is that the deep learning feature was trained by the internet face images with variations of large pose/expression and slight/moderate illumination, which cannot well tackle severe illumination variations. Inspired by the fact that the deep learning feature can cope well with slight/moderate varying illumination, this paper proposes an illumination recovery model to transform severe varying illumination to slight/moderate varying illumination. The illumination recovery model enables the illumination of the severe illumination variation image close to that of the reference image with slight/moderate varying illumination. The reference image generated from the severe illumination variation image is termed as the generated reference image (GRI), which is obtained by normalizing singular values of the logarithm version of the severe illumination variation image to have unit L2-norm. The gradient descent algorithm is employed to address the proposed illumination recovery model, to obtain the generated reference image based illumination recovery image (GRIR). GRIR preserves better face inherent information than GRI such as the face color. Experimental results indicate that the proposed GRIR can efficiently improve the performance of the deep learning feature under severe illumination variations.},
  archive      = {J_PR},
  author       = {Chang-Hui Hu and Jian Yu and Fei Wu and Yang Zhang and Xiao-Yuan Jing and Xiao-Bo Lu and Pan Liu},
  doi          = {10.1016/j.patcog.2020.107724},
  journal      = {Pattern Recognition},
  pages        = {107724},
  shortjournal = {Pattern Recognition},
  title        = {Face illumination recovery for the deep learning feature under severe illumination variations},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SI(FS)2: Fast simultaneous instance and feature selection
for datasets with many features. <em>PR</em>, <em>111</em>, 107723. (<a
href="https://doi.org/10.1016/j.patcog.2020.107723">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data reduction is becoming increasingly relevant due to the enormous amounts of data that are constantly being produced in many fields of research. Instance selection is one of the most widely used methods for this task. At the same time, most recent pattern recognition problems involve highly complex datasets with a large number of possible explanatory variables . For many reasons, this abundance of variables significantly hinders classification and recognition tasks. There are efficiency issues, too, because the speed of many classification algorithms is greatly improved when the complexity of the data is reduced. Thus, feature selection is also a widely used method for data reduction and for gaining an understanding of feature information. Although most methods address instance and feature selection separately, the two problems are interwoven, and benefits are expected from performing these two tasks jointly. However, few algorithms have been proposed for simultaneously addressing the tasks of instance and feature selection. Furthermore, most of those methods are based on complex heuristics that are very difficult to scale up even to moderately large datasets. This paper proposes a new algorithm for dealing with many instances and many features simultaneously by performing joint instance and feature selection using a simple heuristic search and several scaling-up mechanisms that can be successfully applied to datasets with millions of features and instances. In the proposed method, a forward selection search is performed in the feature space jointly with the application of standard instance selection in a constructive subspace built stepwise. Several simplifications are adopted in the search to obtain a scalable method . An extensive comparison using 95 large datasets shows the usefulness of our method and its ability to deal with millions of instances and features simultaneously. The method is able to obtain better classification performance results than state-of-the-art approaches while achieving considerable data reduction.},
  archive      = {J_PR},
  author       = {Nicolás García-Pedrajas and Juan A. Romero del Castillo and Gonzalo Cerruela-García},
  doi          = {10.1016/j.patcog.2020.107723},
  journal      = {Pattern Recognition},
  pages        = {107723},
  shortjournal = {Pattern Recognition},
  title        = {SI(FS)2: Fast simultaneous instance and feature selection for datasets with many features},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Joint architecture and knowledge distillation in CNN for
chinese text recognition. <em>PR</em>, <em>111</em>, 107722. (<a
href="https://doi.org/10.1016/j.patcog.2020.107722">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The distillation technique helps transform cumbersome neural networks into compact networks so that models can be deployed on alternative hardware devices. The main advantage of distillation-based approaches include a simple training process, supported by most off-the-shelf deep learning software and no special hardware requirements. In this paper, we propose a guideline for distilling the architecture and knowledge of pretrained standard CNNs . The proposed algorithm is first verified on a large-scale task: offline handwritten Chinese text recognition (HCTR). Compared with the CNN in the state-of-the-art system, the reconstructed compact CNN can reduce the computational cost by &gt; 10 × &amp;gt;10× and the model size by &gt; 8 × &amp;gt;8× with negligible accuracy loss. Then, by conducting experiments on two additional classification task datasets: Chinese Text in the Wild (CTW) and MNIST, we demonstrate that the proposed approach can also be successfully applied on mainstream backbone networks .},
  archive      = {J_PR},
  author       = {Zi-Rui Wang and Jun Du},
  doi          = {10.1016/j.patcog.2020.107722},
  journal      = {Pattern Recognition},
  pages        = {107722},
  shortjournal = {Pattern Recognition},
  title        = {Joint architecture and knowledge distillation in CNN for chinese text recognition},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A three-step classification framework to handle complex data
distribution for radar UAV detection. <em>PR</em>, <em>111</em>, 107709.
(<a href="https://doi.org/10.1016/j.patcog.2020.107709">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unmanned aerial vehicles (UAVs) have been used in a wide range of applications and become an increasingly important radar target. To better model radar data and to tackle the curse of dimensionality, a three-step classification framework is proposed for UAV detection. First we propose to utilize the greedy subspace clustering to handle potential outliers and the complex sample distribution of radar data. Parameters of the resulting multi-Gaussian model, especially the covariance matrices , could not be reliably estimated due to insufficient training samples and the high dimensionality . Thus, in the second step, a multi-Gaussian subspace reliability analysis is proposed to handle the unreliable feature dimensions of these covariance matrices . To address the challenges of classifying samples using the complex multi-Gaussian model and to fuse the distances of a sample to different clusters at different dimensionalities, a subspace-fusion scheme is proposed in the third step. The proposed approach is validated on a large benchmark dataset, which significantly outperforms the state-of-the-art approaches.},
  archive      = {J_PR},
  author       = {Jianfeng Ren and Xudong Jiang},
  doi          = {10.1016/j.patcog.2020.107709},
  journal      = {Pattern Recognition},
  pages        = {107709},
  shortjournal = {Pattern Recognition},
  title        = {A three-step classification framework to handle complex data distribution for radar UAV detection},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Joint discriminative feature learning for multimodal finger
recognition. <em>PR</em>, <em>111</em>, 107704. (<a
href="https://doi.org/10.1016/j.patcog.2020.107704">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, finger-based multimodal biometrics , due to its high security and stability, has received considerable attention compared with unimodal biometrics . However, existing multimodal finger feature extraction approaches separately extract the features of different modalities, at the same time ignoring correlations among these different modalities. Furthermore, most of the conventional finger feature representation approaches are hand-crafted by design, which require strong prior knowledge. It is therefore very important to explore and develop a suitable feature representation and fusion strategy for multimodal biometrics recognition. In this paper, we proposed a joint discriminative feature learning (JDFL) framework for multimodal finger recognition by combining finger vein (FV) and finger knuckle print (FKP) patterns. For the FV and FKP images, we first established the informative dominant direction vector by convoluting a bank of Gabor filters and the original finger image. Then, we developed a simple yet effective feature learning algorithm, which simultaneously maximized the distance of between-class samples and minimized the distance of within-class samples, as well as maximized the correlation among inter-modality samples of the within-class. Finally, we integrated the block-wise histograms of the learned feature maps together for multimodal finger fusion recognition. Experimental results demonstrated that the proposed approach has a better recognition performance than state-of-the-art finger recognition methods.},
  archive      = {J_PR},
  author       = {Shuyi Li and Bob Zhang and Lunke Fei and Shuping Zhao},
  doi          = {10.1016/j.patcog.2020.107704},
  journal      = {Pattern Recognition},
  pages        = {107704},
  shortjournal = {Pattern Recognition},
  title        = {Joint discriminative feature learning for multimodal finger recognition},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Enhancing the alignment between target words and
corresponding frames for video captioning. <em>PR</em>, <em>111</em>,
107702. (<a href="https://doi.org/10.1016/j.patcog.2020.107702">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video captioning aims at translating from a sequence of video frames into a sequence of words with the encoder-decoder framework. Hence, it is critical to align these two different sequences. Most existing methods exploit soft-attention (temporal attention) mechanism to align target words with corresponding frames, where the relevance of them merely depends on the previously generated words (i.e., language context). As we know, however, there is an inherent gap between vision and language, and most of the words in a caption belong to non-visual words (e.g. “a”, “is”, and “in”). Hence, merely with the guidance of the language context, existing temporal attention-based methods cannot exactly align target words with corresponding frames. In order to address this problem, we first introduce pre-detected visual tags from the video to bridge the gap between vision and language. The reason is that visual tags not only belong to textual modality, but also can convey visual information. Then, we present a Textual-Temporal Attention Model (TTA) to exactly align the target words with corresponding frames. The experimental results show that our proposed method outperforms the state-of-the-art methods on two well known datasets, i.e., MSVD and MSR-VTT. 1},
  archive      = {J_PR},
  author       = {Yunbin Tu and Chang Zhou and Junjun Guo and Shengxiang Gao and Zhengtao Yu},
  doi          = {10.1016/j.patcog.2020.107702},
  journal      = {Pattern Recognition},
  pages        = {107702},
  shortjournal = {Pattern Recognition},
  title        = {Enhancing the alignment between target words and corresponding frames for video captioning},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Surrogate network-based sparseness hyper-parameter
optimization for deep expression recognition. <em>PR</em>, <em>111</em>,
107701. (<a href="https://doi.org/10.1016/j.patcog.2020.107701">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For facial expression recognition, the sparseness constraints of the features or weights can improve the generalization ability of a deep network. However, the optimization of the hyper-parameters in fusing different sparseness strategies demands much computation, when the traditional gradient-based algorithms are used. In this work, an iterative framework with surrogate network is proposed for the optimization of hyper-parameters in fusing different sparseness strategies. In each iteration, a network with significantly smaller model complexity is fitted to the original large network based on four Euclidean losses, where the hyper-parameters are optimized with heuristic optimizers. Since the surrogate network uses the same deep metrics and embeds the same hyper-parameters as the original network, the optimized hyper-parameters are then used for the training of the original deep network in the next iteration. While the performance of the proposed algorithm is justified with a tiny model, i.e. LeNet on the FER2013 database, our approach achieved competitive performances on six publicly available expression datasets, i.e., FER2013, CK+, Oulu-CASIA, MMI, AFEW and AffectNet.},
  archive      = {J_PR},
  author       = {Weicheng Xie and Wenting Chen and Linlin Shen and Jinming Duan and Meng Yang},
  doi          = {10.1016/j.patcog.2020.107701},
  journal      = {Pattern Recognition},
  pages        = {107701},
  shortjournal = {Pattern Recognition},
  title        = {Surrogate network-based sparseness hyper-parameter optimization for deep expression recognition},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Temporal filtering networks for online action detection.
<em>PR</em>, <em>111</em>, 107695. (<a
href="https://doi.org/10.1016/j.patcog.2020.107695">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online action detection aims to detect a current action from an untrimmed, streaming video, where only current and past frames are available. Recent methods for online action detection have focused on how to model discriminative representations from temporally partial information. However, they overlook the fact that the input video contains background as well as actions. To overcome this problem, in this paper, we propose a novel approach, named Temporal Filtering Network, to distinguish between relevant and irrelevant information from a partially observed, untrimmed video. Specifically, we present a filtering module to learn relevance scores indicating how relevant the information is to a current action. Our filtering module emphasizes the relevant information to a current action, while it filters out the information of background and unrelated actions. We conduct extensive experiments on THUMOS-14 and TVSeries datasets. On these datasets, the proposed method outperforms state-of-the-art methods by a large margin. We also show the effectiveness of the filtering module through comprehensive ablation studies.},
  archive      = {J_PR},
  author       = {Hyunjun Eun and Jinyoung Moon and Jongyoul Park and Chanho Jung and Changick Kim},
  doi          = {10.1016/j.patcog.2020.107695},
  journal      = {Pattern Recognition},
  pages        = {107695},
  shortjournal = {Pattern Recognition},
  title        = {Temporal filtering networks for online action detection},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust line segment matching via reweighted random walks on
the homography graph. <em>PR</em>, <em>111</em>, 107693. (<a
href="https://doi.org/10.1016/j.patcog.2020.107693">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a novel method for matching line segments between stereo images . Given the fundamental matrix, the local homography can be over determined with pairwise line segment candidates. We exploit this constraint to initialize the candidate and construct the novel homography graph. Because the constraint between the node is based on the epipolar geometry, the homography graph is invariant to the local projective transformation . We employ the reweighted random walk on the graph to rank the candidate, then, we propose the constrained-greedy algorithm to obtain the reliable match. To the best of our knowledge, this is the first study to embed the epipolar geometry into the graph matching theory for the line segment matching. When evaluated on the 32 image patches, our method outperformed the state of the art methods , especially in the scenes of the wide baseline, steep viewpoint changes and dense line segments. The proposed algorithm is available at https://github.com/weidong-whu/line-match-RRW .},
  archive      = {J_PR},
  author       = {Dong Wei and Yongjun Zhang and Chang Li},
  doi          = {10.1016/j.patcog.2020.107693},
  journal      = {Pattern Recognition},
  pages        = {107693},
  shortjournal = {Pattern Recognition},
  title        = {Robust line segment matching via reweighted random walks on the homography graph},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). STAN: A sequential transformation attention-based network
for scene text recognition. <em>PR</em>, <em>111</em>, 107692. (<a
href="https://doi.org/10.1016/j.patcog.2020.107692">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scene text with an irregular layout is difficult to recognize. To this end, a S equential T ransformation A ttention-based N etwork (STAN), which comprises a sequential transformation network and an attention-based recognition network , is proposed for general scene text recognition. The sequential transformation network rectifies irregular text by decomposing the task into a series of patch-wise basic transformations, followed by a grid projection submodule to smooth the junction between neighboring patches. The entire rectification process is able to be trained in an end-to-end weakly supervised manner, requiring only images and their corresponding groundtruth text. Based on the rectified images, an attention-based recognition network is employed to predict a character sequence. Experiments on several benchmarks demonstrate the state-of-the-art performance of STAN on both regular and irregular text.},
  archive      = {J_PR},
  author       = {Qingxiang Lin and Canjie Luo and Lianwen Jin and Songxuan Lai},
  doi          = {10.1016/j.patcog.2020.107692},
  journal      = {Pattern Recognition},
  pages        = {107692},
  shortjournal = {Pattern Recognition},
  title        = {STAN: A sequential transformation attention-based network for scene text recognition},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021d). KDD: A kernel density based descriptor for 3D point clouds.
<em>PR</em>, <em>111</em>, 107691. (<a
href="https://doi.org/10.1016/j.patcog.2020.107691">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D feature description is one of the central techniques that rely on point clouds since a lot of point cloud processing techniques apply the point-to-point correspondences that are achieved via feature descriptors as input data. The feature descriptor encodes the information of the underlying surface around the feature point so as to make a local surface distinguished from another. The focus of the existing descriptors is accumulating the geometric or topological measurements into histograms or encoding the 2D images that are acquired by rotationally projecting the 3D local surfaces onto 2D planes. Histograms can hardly deal with three or more dimensional information, and the rotational projection operation does bring much unnecessary intermediate computations. To overcome these limitations, in this article, a descriptor named Kernel Density Descriptor (KDD) has been presented. One core contribution of this method is to encode the information of the whole 3D space around the feature point via kernel density estimation, and another is providing the strategy for selecting different matching metrics for datasets with diverse levels of resolution qualities. We compare KDD against several representative descriptors on publicly available datasets, the experimental results demonstrate that the KDD descriptor achieves a satisfactory and balanced performance in terms of descriptiveness, robustness, and compactness, furthermore, the comparisons validate the overall superiority of our method. The benefits and applicability on object registration and recognition and 3D object reconstruction are demonstrated by the favorable results that are obtained for both public datastes and the real-world point clouds of Terracotta fragments.},
  archive      = {J_PR},
  author       = {Yuhe Zhang and Chunhui Li and Bao Guo and Chenhao Guo and Shunli Zhang},
  doi          = {10.1016/j.patcog.2020.107691},
  journal      = {Pattern Recognition},
  pages        = {107691},
  shortjournal = {Pattern Recognition},
  title        = {KDD: A kernel density based descriptor for 3D point clouds},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Black-box attack against handwritten signature verification
with region-restricted adversarial perturbations. <em>PR</em>,
<em>111</em>, 107689. (<a
href="https://doi.org/10.1016/j.patcog.2020.107689">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Handwritten signature verification is used to verify the identity of individuals through recognizing their signatures. Adversarial examples can induce misclassification , hence posing a severe threat to signature verification. At present, a variety of adversarial example attacks have been developed for image classification , but they are not that useful for attacking signature verification due to two main reasons. First, adversarial perturbations are likely to be imposed on the background of signature images, making them perceptible to human eyes. Second, perfect knowledge about signature verification systems is actually unavailable to attackers. Therefore, how to generate effective and stealthy signature adversarial examples is still an open issue. To shed insights on this challenging problem, we propose the first black-box adversarial example attack against handwritten signature verification in this paper. Our method has two key designs. First, its perturbations are intentionally restricted to the foreground (i.e., strokes) of signature images, which reduces the risk of being recognized by humans. Second, a gradient-free method is developed to achieve the desired perturbations through iteratively updating their positions and optimizing their intensity. Extensive experiments confirm the three advantages of our method. First, the adversarial perturbations generated by our method are almost invisible, while those generated by existing methods are more well-marked. Second, our method defeats the state-of-the-art signature verification method with a surprisingly high success rate of 92.1\%. Last, our method breaks through the defense of background cleaning, although this defense can deactivate almost all the existing adversarial example attacks towards signature verification.},
  archive      = {J_PR},
  author       = {Haoyang Li and Heng Li and Hansong Zhang and Wei Yuan},
  doi          = {10.1016/j.patcog.2020.107689},
  journal      = {Pattern Recognition},
  pages        = {107689},
  shortjournal = {Pattern Recognition},
  title        = {Black-box attack against handwritten signature verification with region-restricted adversarial perturbations},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hypergraph video pedestrian re-identification based on
posture structure relationship and action constraints. <em>PR</em>,
<em>111</em>, 107688. (<a
href="https://doi.org/10.1016/j.patcog.2020.107688">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Discriminative feature learning is critical for pedestrian re-identification. Previous part-based methods mainly focus on the region of specific predefined semantics to learn local representations, ignoring the influence of posture changes, and the learning efficiency and robustness in complex scenes are poor. In this paper, a hypergraph video pedestrian re-identification method based on posture structure relationships and action constraint(PA-HVPReid) is proposed, which aims to make full use of pedestrian walking postures to obtain more discriminative features. The pose structure relationship feature solves the problem that the pooling operation destroys the feature structure relationship. This paper uses a graph convolution network (GCN) to preserve the structure relationship presented by the image feature map. The input of the GCN is the region at the joint points of the pedestrian to be detected, and the output is the color feature that retains the structural relationship. The structural relationship hypergraph is formed according to the structural relationship between the joint point regions. The action hypergraph can be constructed by constraining the action information between the joint point regions. The saliency score of the joint point region is calculated from the posture structure hypergraph and the action hypergraph. We convert the saliency score into a probability distribution problem and propose a relative entropy loss function based on regional saliency to measure the similarity of the two probability distributions. Experimental results show that the performance of our method is better than the existing method on three data sets.},
  archive      = {J_PR},
  author       = {Xiaoqiang Hu and Dan Wei and Ziyang Wang and Jianglin Shen and Hongjuan Ren},
  doi          = {10.1016/j.patcog.2020.107688},
  journal      = {Pattern Recognition},
  pages        = {107688},
  shortjournal = {Pattern Recognition},
  title        = {Hypergraph video pedestrian re-identification based on posture structure relationship and action constraints},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A new localization method for epileptic seizure onset zones
based on time-frequency and clustering analysis. <em>PR</em>,
<em>111</em>, 107687. (<a
href="https://doi.org/10.1016/j.patcog.2020.107687">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-frequency oscillations (HFOs) are spontaneous electroencephalogram patterns that have been regarded as potential biomarkers of epileptic seizure onset zones (SOZs). Accurately detected HFOs are used to localize SOZs, which is crucial for the presurgical assessment. Since the visual marking of HFOs is time-consuming, a method is desirable to automatically detect HFOs for localizing SOZs in clinical practice. However, the existing methods cannot obtain satisfactory performance, which are not suitable for clinical application. In order to solve this problem, we present a new localization method for epileptic SOZs in this study. Firstly, a threshold method is used to detect events of interest (EoIs). Secondly, a time-frequency analysis method is adopted to acquire channels of interest (CoIs) by calculating the average power of EoIs on each channel. Then, the k -medoids clustering method is employed to detect HFOs of CoIs. Finally, the concentrations of detected HFOs are used to localize SOZs. The superiority of our localization method is demonstrated by comparing its sensitivity and specificity with some existing methods.},
  archive      = {J_PR},
  author       = {Min Wu and Ting Wan and Xiongbo Wan and Zelin Fang and Yuxiao Du},
  doi          = {10.1016/j.patcog.2020.107687},
  journal      = {Pattern Recognition},
  pages        = {107687},
  shortjournal = {Pattern Recognition},
  title        = {A new localization method for epileptic seizure onset zones based on time-frequency and clustering analysis},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust sparse coding for one-class classification based on
correntropy and logarithmic penalty function. <em>PR</em>, <em>111</em>,
107685. (<a href="https://doi.org/10.1016/j.patcog.2020.107685">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Similar to binary and multi-class classifiers, one-class classifiers have to face the difficulty of ’curse of dimensionality’ when they are applied to deal with high-dimensional samples. As an efficient dimensionality reduction method, sparse coding tries to learn a set of over-complete bases to represent the given samples. It can effectively overcome the ’curse of dimensionality’ problem. However, the traditional sparse coding only fit for tackling Gaussian noise . When the noise within the given set of samples obey non-Gaussian distribution, the conventional sparse coding cannot obtain accurate coefficient vectors. To make sparse coding more fit for dealing with non-Gaussian noise and enhance the sparseness of the obtained coefficient vectors, correntropy is utilized to substitute its reconstruction error term and logarithmic penalty function is introduced as its regularization term. Furthermore, the obtained sparse coefficient vectors are used as the input vectors for one-class support vector machine (OCSVM). Experimental results on twenty UCI benchmark data sets and one handwritten digit data set demonstrate that the proposed method achieves better anti-noise and generalization abilities in comparison with its related approaches.},
  archive      = {J_PR},
  author       = {Hong-Jie Xing and Ya-Jie Liu and Zi-Chuan He},
  doi          = {10.1016/j.patcog.2020.107685},
  journal      = {Pattern Recognition},
  pages        = {107685},
  shortjournal = {Pattern Recognition},
  title        = {Robust sparse coding for one-class classification based on correntropy and logarithmic penalty function},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). ReLaText: Exploiting visual relationships for
arbitrary-shaped scene text detection with graph convolutional networks.
<em>PR</em>, <em>111</em>, 107684. (<a
href="https://doi.org/10.1016/j.patcog.2020.107684">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a new arbitrary-shaped text detection approach named ReLaText by formulating text detection as a visual relationship detection problem. To demonstrate the effectiveness of this new formulation, we start from using a “link” relationship to address the challenging text-line grouping problem firstly. The key idea is to decompose text detection into two subproblems , namely detection of text primitives and prediction of link relationships between nearby text primitive pairs. Specifically, an anchor-free region proposal network based text detector is first used to detect text primitives of different scales from different feature maps of a feature pyramid network, from which a text primitive graph is constructed by linking each pair of nearby text primitives detected from a same feature map with an edge. Then, a Graph Convolutional Network (GCN) based link relationship prediction module is used to prune wrongly-linked edges in the text primitive graph to generate a number of disjoint subgraphs, each representing a detected text instance. As GCN can effectively leverage context information to improve link prediction accuracy, our GCN based text-line grouping approach can achieve better text detection accuracy than previous text-line grouping methods, especially when dealing with text instances with large inter-character or very small inter-line spacing. Consequently, the proposed ReLaText achieves state-of-the-art performance on five public text detection benchmarks, namely RCTW-17, MSRA-TD500, Total-Text, CTW1500 and DAST1500.},
  archive      = {J_PR},
  author       = {Chixiang Ma and Lei Sun and Zhuoyao Zhong and Qiang Huo},
  doi          = {10.1016/j.patcog.2020.107684},
  journal      = {Pattern Recognition},
  pages        = {107684},
  shortjournal = {Pattern Recognition},
  title        = {ReLaText: Exploiting visual relationships for arbitrary-shaped scene text detection with graph convolutional networks},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust semi-supervised nonnegative matrix factorization for
image clustering. <em>PR</em>, <em>111</em>, 107683. (<a
href="https://doi.org/10.1016/j.patcog.2020.107683">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nonnegative matrix factorization (NMF) is a powerful dimension reduction method , and has received increasing attention in various practical applications. However, most traditional NMF based algorithms are sensitive to noisy data, or fail to fully utilize the limited supervised information . In this paper, a novel robust semi-supervised NMF method, namely correntropy based semi-supervised NMF (CSNMF), is proposed to solve these issues. Specifically, CSNMF adopts a correntropy based loss function instead of the squared Euclidean distance (SED) in constrained NMF to suppress the influence of non-Gaussian noise or outliers contaminated in real world data, and simultaneously uses two types of supervised information , i.e., the pointwise and pairwise constraints, to obtain the discriminative data representation. The proposed method is analyzed in terms of convergence, robustness and computational complexity . The relationships between CSNMF and several previous NMF based methods are also discussed. Extensive experimental results show the effectiveness and robustness of CSNMF in image clustering tasks , compared with several state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Siyuan Peng and Wee Ser and Badong Chen and Zhiping Lin},
  doi          = {10.1016/j.patcog.2020.107683},
  journal      = {Pattern Recognition},
  pages        = {107683},
  shortjournal = {Pattern Recognition},
  title        = {Robust semi-supervised nonnegative matrix factorization for image clustering},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Linear classifier combination via multiple potential
functions. <em>PR</em>, <em>111</em>, 107681. (<a
href="https://doi.org/10.1016/j.patcog.2020.107681">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A vital aspect of the classification based model construction process is the calibration of the scoring function. One of the weaknesses of the calibration process is that it does not take into account the information about the relative positions of the recognized objects in the feature space. To alleviate this limitation, in this paper, we propose a novel concept of calculating a scoring function based on the distance of the object from the decision boundary and its distance to the class centroid . An important property is that the proposed score function has the same nature for all linear base classifiers, which means that outputs of these classifiers are equally represented and have the same meaning. The proposed approach is compared with other ensemble algorithms and experiments on multiple Keel datasets demonstrate the effectiveness of our method. To discuss the results of our experiments, we use multiple classification performance measures and statistical analysis.},
  archive      = {J_PR},
  author       = {Pawel Trajdos and Robert Burduk},
  doi          = {10.1016/j.patcog.2020.107681},
  journal      = {Pattern Recognition},
  pages        = {107681},
  shortjournal = {Pattern Recognition},
  title        = {Linear classifier combination via multiple potential functions},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptive feature fusion for visual object tracking.
<em>PR</em>, <em>111</em>, 107679. (<a
href="https://doi.org/10.1016/j.patcog.2020.107679">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advanced trackers, consisting of discriminative classification component and dedicated bounding box estimation, have achieved improved performance in the visual tracking community. The most essential factor for the development is the utilization of different Convolutional Neural Networks (CNNs), which significantly improves the model capacity via offline trained deep feature representations. Though powerful deep structures emphasize more semantic appearance through high dimensional latent variables, how to achieve effective feature adaptation in the online tracking stage has not been sufficiently considered yet. To this end, we argue the necessity of exploring hierarchical and complementary appearance descriptors from different convolutional layers to achieve online tracking adaptation. Therefore, in this paper, we propose an adaptive feature fusion mechanism, which can balance the detection granularities from shallow to deep convolutional layers. To be specific, the correlation between template and instance is employed to generate adaptive weights to achieve advanced saliency and discrimination. In addition, considering temporal appearance variation, the projection matrix for the multi-channel inputs is jointly updated with the correlation classifier to further enhance the robustness. The experimental results on four recent benchmarks, i.e. , OTB-2015, VOT2018, LaSOT and TrackingNet, demonstrate the effectiveness and robustness of the proposed method, with superior performance compared to the state-of-the-art approaches.},
  archive      = {J_PR},
  author       = {Shaochuan Zhao and Tianyang Xu and Xiao-Jun Wu and Xue-Feng Zhu},
  doi          = {10.1016/j.patcog.2020.107679},
  journal      = {Pattern Recognition},
  pages        = {107679},
  shortjournal = {Pattern Recognition},
  title        = {Adaptive feature fusion for visual object tracking},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Enhanced low-rank constraint for temporal subspace
clustering and its acceleration scheme. <em>PR</em>, <em>111</em>,
107678. (<a href="https://doi.org/10.1016/j.patcog.2020.107678">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inspired by the temporal subspace clustering (TSC) method and low-rank matrix approximation constraint, a new model is proposed termed as temporal plus low-rank subspace clustering (TLRSC) by utilizing both the local and global structural information. On one hand, to solve the drawback that the nuclear norm-based constraint usually results in a suboptimal solution, we incorporate certain nonconvex surrogates into our model, which approximates the low-rank constraint closely and holds the potential for the convexity of the whole cost function. On the other hand, to ensure fast convergence, we propose an efficient iteratively reweighted singular value minimization (IRSVD) algorithm under the majorization-minimization framework. Moreover, we show that for the weighted low-rank constraint, a cutoff can be derived to automatically threshold the singular values computed from the proximal operator. This guarantees the thresholding operation can be reduced to that of two smaller matrices. Accordingly, an efficient singular value thresholding scheme is proposed for acceleration. Comprehensive experiments are conducted on several public available datasets for quantitative evaluation . Results demonstrate the efficacy and efficiency of TLRSC compared with several state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Jianwei Zheng and Ping Yang and Guojiang Shen and Shengyong Chen and Wei Zhang},
  doi          = {10.1016/j.patcog.2020.107678},
  journal      = {Pattern Recognition},
  pages        = {107678},
  shortjournal = {Pattern Recognition},
  title        = {Enhanced low-rank constraint for temporal subspace clustering and its acceleration scheme},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Exploring global diverse attention via pairwise temporal
relation for video summarization. <em>PR</em>, <em>111</em>, 107677. (<a
href="https://doi.org/10.1016/j.patcog.2020.107677">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video summarization is an effective way to facilitate video searching and browsing. Most of existing systems employ encoder-decoder based recurrent neural networks , which fail to explicitly diversify the system-generated summary frames while requiring intensive computations. In this paper, we propose an efficient convolutional neural network architecture for video SUM marization via G lobal D iverse A ttention called SUM-GDA , which adapts attention mechanism in a global perspective to consider pairwise temporal relations of video frames. Particularly, the GDA module has two advantages: (1) it models the relations within paired frames as well as the relations among all pairs, thus capturing the global attention across all frames of one video; (2) it reflects the importance of each frame to the whole video, leading to diverse attention on these frames. Thus, SUM-GDA is beneficial for generating diverse frames to form satisfactory video summary. Extensive experiments on three data sets, i.e., SumMe, TVSum, and VTW, have demonstrated that SUM-GDA and its extension outperform other competing state-of-the-art methods with remarkable improvements. In addition, the proposed models can be run in parallel with significantly less computational costs, which helps the deployment in highly demanding applications.},
  archive      = {J_PR},
  author       = {Ping Li and Qinghao Ye and Luming Zhang and Li Yuan and Xianghua Xu and Ling Shao},
  doi          = {10.1016/j.patcog.2020.107677},
  journal      = {Pattern Recognition},
  pages        = {107677},
  shortjournal = {Pattern Recognition},
  title        = {Exploring global diverse attention via pairwise temporal relation for video summarization},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Discriminative subspace matrix factorization for multiview
data clustering. <em>PR</em>, <em>111</em>, 107676. (<a
href="https://doi.org/10.1016/j.patcog.2020.107676">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a real-world scenario, an object is easily considered as features combined by multiple views in reality. Thus, multiview features can be encoded into a unified and discriminative framework to achieve satisfactory clustering performance. An increasing number of algorithms have been proposed for multiview data clustering . However, existing multiview methods have several drawbacks. First, most multiview algorithms focus only on origin data in high dimension directly without the intrinsic structure in the relative low-dimensional subspace. Spectral and manifold-based methods ignore pseudo-information that can be extracted from the optimization process. Thus, we design an unsupervised nonnegative matrix factorization (NMF)-based method called discriminative multiview subspace matrix factorization (DMSMF) for clustering. We provide the following contributions. (1) We extend linear discriminant analysis and NMF to a multiview version and connect them to a unified framework to learn in the discriminant subspace. (2) We propose a multiview manifold regularization term and discriminant multiview manifold regularization term that instruct the regularization term to discriminate different classes and obtain the geometry st ructure from the low-dimensional subspace. (3) We design an effective optimization algorithm with proven convergence to obtain an optimal solution procedure for the complex model. Adequate experiments are conducted on multiple benchmark datasets. Finally, we demonstrate that our model is superior to other comparable multiview data clustering algorithms .},
  archive      = {J_PR},
  author       = {Jiaqi Ma and Yipeng Zhang and Lefei Zhang},
  doi          = {10.1016/j.patcog.2020.107676},
  journal      = {Pattern Recognition},
  pages        = {107676},
  shortjournal = {Pattern Recognition},
  title        = {Discriminative subspace matrix factorization for multiview data clustering},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Expand globally, shrink locally: Discriminant multi-label
learning with missing labels. <em>PR</em>, <em>111</em>, 107675. (<a
href="https://doi.org/10.1016/j.patcog.2020.107675">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In multi-label learning, the issue of missing labels brings a major challenge. Many methods attempt to recovery missing labels by exploiting low-rank structure of label matrix. However, these methods just utilize global low-rank label structure, ignore both local low-rank label structures and label discriminant information to some extent, leaving room for further performance improvement. In this paper, we develop a simple yet effective discriminant multi-label learning (DM2L) method for multi-label learning with missing labels. Specifically, we impose the low-rank structures on all the predictions of instances from the same labels (local shrinking of rank), and a maximally separated structure (high-rank structure) on the predictions of instances from different labels (global expanding of rank). In this way, these imposed low-rank structures can help modeling both local and global low-rank label structures, while the imposed high-rank structure can help providing more underlying discriminability . Our subsequent theoretical analysis also supports these intuitions. In addition, we provide a nonlinear extension via using kernel trick to enhance DM2L and establish a concave-convex objective to learn these models. Compared to the other methods, our method involves the fewest assumptions and only one hyper-parameter. Even so, extensive experiments show that our method still outperforms the state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Zhongchen Ma and Songcan Chen},
  doi          = {10.1016/j.patcog.2020.107675},
  journal      = {Pattern Recognition},
  pages        = {107675},
  shortjournal = {Pattern Recognition},
  title        = {Expand globally, shrink locally: Discriminant multi-label learning with missing labels},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Attribute and instance weighted naive bayes. <em>PR</em>,
<em>111</em>, 107674. (<a
href="https://doi.org/10.1016/j.patcog.2020.107674">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Naive Bayes (NB) continues to be one of the top 10 data mining algorithms , but its conditional independence assumption rarely holds true in real-world applications. Therefore, many different categories of improved approaches, including attribute weighting and instance weighting, have been proposed to alleviate this assumption. However, few of these approaches simultaneously pay attention to attribute weighting and instance weighting. In this study, we propose a new improved model called attribute and instance weighted naive Bayes (AIWNB), which combines attribute weighting with instance weighting into one uniform framework. In AIWNB, the attribute weights are incorporated into the naive Bayesian classification formula, and then the prior and conditional probabilities are estimated using instance weighted training data. To learn instance weights, we single out an eager approach and a lazy approach, and thus two different versions are created, which we denote as AIWNB E and AIWNB L , respectively. Extensive experimental results show that both AIWNB E and AIWNB L significantly outperform NB and all the other existing state-of-the-art competitors.},
  archive      = {J_PR},
  author       = {Huan Zhang and Liangxiao Jiang and Liangjun Yu},
  doi          = {10.1016/j.patcog.2020.107674},
  journal      = {Pattern Recognition},
  pages        = {107674},
  shortjournal = {Pattern Recognition},
  title        = {Attribute and instance weighted naive bayes},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Local structured feature learning with dynamic maximum
entropy graph. <em>PR</em>, <em>111</em>, 107673. (<a
href="https://doi.org/10.1016/j.patcog.2020.107673">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, Linear Discriminant Analysis (LDA) has seen huge adoption in data mining applications . Due to its globality, it is incompetent to handle multimodal data. Besides, most of LDA’s variants learn the projection matrix based on the pre-defined similarity matrix, which is easily affected by noisy and irrelevant features. To address above two issues, a novel local structured feature learning with Dynamic Maximum Entropy Graph (DMEG) method is developed which firstly develops a more discriminative LDA with whitening constraint that can minimize the within-class scatter while keeping the total samples scatter unchanged simultaneously. Second, for exploring the local structure of data, the ℓ 0 -norm constraint is imposed on similarity matrix to ensure the k connectivity on graph. More importantly, proposed model learns the similarity and projection matrix simultaneously to ensure that the neighborships can be found in the optimal subspace where the noise have been removed already. Moreover, a maximum entropy regularization is employed to reinforce the discriminability of graph and avoid the trivial solution . Last but not least, an efficient iterative optimization algorithm is provided to optimize proposed model with a NP-hard constraint. Extensive experiments conducted on synthetic and several real-world data sets demonstrate the efficiency in classification task and robustness to noise of proposed method.},
  archive      = {J_PR},
  author       = {Zheng Wang and Feiping Nie and Rong Wang and Hui Yang and Xuelong Li},
  doi          = {10.1016/j.patcog.2020.107673},
  journal      = {Pattern Recognition},
  pages        = {107673},
  shortjournal = {Pattern Recognition},
  title        = {Local structured feature learning with dynamic maximum entropy graph},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Online incremental hierarchical classification resonance
network. <em>PR</em>, <em>111</em>, 107672. (<a
href="https://doi.org/10.1016/j.patcog.2020.107672">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hierarchical classification is imperative in that almost all objects are described in hierarchical semantics. If a classification method enables incremental class learning to learn new objects online, it will be practically used for real-time applications. In this sense, we propose online incremental hierarchical classification resonance network (OIHCRN) that enables online incremental class learning in hierarchical classification. OIHCRN has a structure that grows horizontally and vertically online according to object classes, so that a newly added object can be classified. By the proposed process of scale-preserving projection and prior label appending, OIHCRN reflects the class dependency between class levels and simultaneously normalizes the input vector online. Additionally, to reduce the model complexity and improve performance, two auxiliary strategies, named OIHCRN with class END and OIHCRN with differentiated class labels , are introduced. To demonstrate the effectiveness of OIHCRNs, experiments are carried out for benchmark datasets and then for a multimedia recommendation system.},
  archive      = {J_PR},
  author       = {Ju-Youn Park and Jong-Hwan Kim},
  doi          = {10.1016/j.patcog.2020.107672},
  journal      = {Pattern Recognition},
  pages        = {107672},
  shortjournal = {Pattern Recognition},
  title        = {Online incremental hierarchical classification resonance network},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). EKENet: Efficient knowledge enhanced network for real-time
scene parsing. <em>PR</em>, <em>111</em>, 107671. (<a
href="https://doi.org/10.1016/j.patcog.2020.107671">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scene parsing is essential for many high-level AI applications, such as intelligent vehicles and traffic surveillance. In this work, we propose a highly efficient and powerful deep convolutional neural network , namely Efficient Knowledge Enhanced Network (EKENet), for parsing scenes in real-time. Unlike most existing approaches that compromise efficiency for the sake of high accuracy, EKENet achieves an ideal trade-off between the two. Our EKENet is built upon a novel building block , namely Efficient Dual Abstraction (EDA) block, which employs an efficiently parallel convolution structure for extracting spatial features and modeling cross-channel correlations in a dual fashion. Additionally, a novel light-weight Encoding-Enhancing (EE) module is designed to enhance our EKENet, which can efficiently encode high-level knowledge extracted from top layers to guide the learning of low-level features from bottom layers. Extensive experiments on challenging benchmarks, Cityscapes and CamVid datasets, demonstrate that EKENet achieves the new state-of-the-art performance in terms of speed and accuracy tradeoff.},
  archive      = {J_PR},
  author       = {Ao Luo and Fan Yang and Xin Li and Rui Huang and Hong Cheng},
  doi          = {10.1016/j.patcog.2020.107671},
  journal      = {Pattern Recognition},
  pages        = {107671},
  shortjournal = {Pattern Recognition},
  title        = {EKENet: Efficient knowledge enhanced network for real-time scene parsing},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Guided image filtering in shape-from-focus: A comparative
analysis. <em>PR</em>, <em>111</em>, 107670. (<a
href="https://doi.org/10.1016/j.patcog.2020.107670">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mostly, shape from focus (SFF) methods do not consider any prior to extend the accuracy of the depth map. Ultimately, even the improved depth map might lack the accurate structure of the object. While reviewing the guided filters, it has been observed that SFF has not been considered as an application. In this study, we not only suggest to apply guided filtering for depth enhancement but also provide a comparative analysis of recently proposed guided filters for SFF framework in a systematic way. In addition, a set of potential guidance maps has been suggested and the performance of these guidance maps has been evaluated. The improved performance of guided filters has been ranked against the depth maps of synthetic and real image sequences where the corresponding scenes have diverse range of geometrical complexities. It has been observed that guided image filtering is effective in improving the initial depth maps in SFF.},
  archive      = {J_PR},
  author       = {Usman Ali and Ik Hyun Lee and Muhammad Tariq Mahmood},
  doi          = {10.1016/j.patcog.2020.107670},
  journal      = {Pattern Recognition},
  pages        = {107670},
  shortjournal = {Pattern Recognition},
  title        = {Guided image filtering in shape-from-focus: A comparative analysis},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). All-in-focus synthetic aperture imaging using generative
adversarial network-based semantic inpainting. <em>PR</em>,
<em>111</em>, 107669. (<a
href="https://doi.org/10.1016/j.patcog.2020.107669">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Occlusions handling poses a significant challenge to many computer vision and pattern recognition applications. Recently, Synthetic Aperture Imaging (SAI), which uses more than two cameras, is widely applied to reconstruct occluded objects in complex scenes. However, it usually fails in cases of heavy occlusions, in particular, when the occluded information is not captured by any of the camera views. Hence, it is a challenging task to generate a realistic all-in-focus synthetic aperture image which shows a completely occluded object. In this paper, semantic inpainting using a Generative Adversarial Network (GAN) is proposed to address the above-mentioned problem. The proposed method first computes a synthetic aperture image of the occluded objects using a labeling method, and an alpha matte of the partially occluded objects. Then, it uses energy minimization to reconstruct the background by focusing on the background depth of each camera. Finally, the occluded regions of the synthesized image are semantically inpainted using a GAN and the results are composited with the reconstructed background to generate a realistic all-in-focus image. The experimental results demonstrate that the proposed method can handle heavy occlusions and can produce better all-in-focus images than other state-of-the-art methods. Compared with traditional labeling methods, our method can quickly generate label for occlusion without introducing noise. To the best of our knowledge, our method is the first to address missing information caused by heavy occlusions in SAI using a GAN.},
  archive      = {J_PR},
  author       = {Zhao Pei and Min Jin and Yanning Zhang and Miao Ma and Yee-Hong Yang},
  doi          = {10.1016/j.patcog.2020.107669},
  journal      = {Pattern Recognition},
  pages        = {107669},
  shortjournal = {Pattern Recognition},
  title        = {All-in-focus synthetic aperture imaging using generative adversarial network-based semantic inpainting},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep rényi entropy graph kernel. <em>PR</em>, <em>111</em>,
107668. (<a href="https://doi.org/10.1016/j.patcog.2020.107668">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph kernels are applied heavily for the classification of structured data. In this paper, we propose a deep Rényi entropy graph kernel for this purpose. We gauge the deep information through a family of h -layer expansion subgraphs rooted at a vertex, and define a h -layer depth-based second-order Rényi entropy representation for each vertex. The second-order Rényi entropy representation is used together with Euclidean distance to build a deep second-order Rényi entropy graph kernel (SREGK). For graphs with n vertices, the time complexity for our kernel is O ( n 3 ). This low-order polynomial complexity enables our subgraph kernels to easily scale up to graphs of reasonably large sizes and thus overcome the size limits arising in state-of-the-art graph kernels. Experimental results on fourteen real world graph datasets are shown to demonstrate the overall superior performance of our approach over a number of state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Lixiang Xu and Lu Bai and Xiaoyi Jiang and Ming Tan and Daoqiang Zhang and Bin Luo},
  doi          = {10.1016/j.patcog.2020.107668},
  journal      = {Pattern Recognition},
  pages        = {107668},
  shortjournal = {Pattern Recognition},
  title        = {Deep rényi entropy graph kernel},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Supervised quality evaluation of binary partition trees for
object segmentation. <em>PR</em>, <em>111</em>, 107667. (<a
href="https://doi.org/10.1016/j.patcog.2020.107667">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The binary partition tree (BPT) allows for the hierarchical representation of images in a multiscale way, by providing a tree of nodes corresponding to image regions. In particular, cuts of a BPT can be interpreted as segmentations of the associated image. Building the BPT of an image then constitutes a relevant preliminary step for optimization-based segmentation methods . A wide literature has been devoted to the construction of BPTs, and their involvement in such segmentation tasks . Comparatively, there exist few works dedicated to evaluate the quality of BPTs, i.e. their ability to allow further segmentation methods to compute good results. We propose such a framework for evaluating the quality of a BPT with respect to the object segmentation problem, i.e. the segmentation of one or several objects from an image. This framework is supervised, since the notion of segmentation quality is not only depending on the application but also on the user’s objectives, expressed via the chosen ground-truth and quality metric. We develop two sides within this framework. First, we propose an intrinsic quality analysis, that relies on the structural coherence of the BPT with respect to ground-truth. More precisely, we evaluate to what extent the BPT structure is well-matching such examples, in a set / combinatorial fashion. Second, we propose an extrinsic analysis, by allowing the user to assess the quality of a BPT based on chosen metrics that correspond to the desired properties of the subsequent segmentation. In particular, we evaluate to what extent a BPT can provide good results with respect to such metrics whereas handling the trade-off with the cardinality of the cuts.},
  archive      = {J_PR},
  author       = {Jimmy Francky Randrianasoa and Pierre Cettour-Janet and Camille Kurtz and Éric Desjardin and Pierre Gançarski and Nathalie Bednarek and François Rousseau and Nicolas Passat},
  doi          = {10.1016/j.patcog.2020.107667},
  journal      = {Pattern Recognition},
  pages        = {107667},
  shortjournal = {Pattern Recognition},
  title        = {Supervised quality evaluation of binary partition trees for object segmentation},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A BFS-tree of ranking references for unsupervised manifold
learning. <em>PR</em>, <em>111</em>, 107666. (<a
href="https://doi.org/10.1016/j.patcog.2020.107666">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Contextual information, defined in terms of the proximity of feature vectors in a feature space, has been successfully used in the construction of search services. These search systems aim to exploit such information to effectively improve ranking results, by taking into account the manifold distribution of features usually encoded. In this paper, a novel unsupervised manifold learning is proposed through a similarity representation based on ranking references. A breadth-first tree is used to represent similarity information given by ranking references and is exploited to discovery underlying similarity relationships. As a result, a more effective similarity measure is computed, which leads to more relevant objects in the returned ranked lists of search sessions. Several experiments conducted on eight public datasets, commonly used for image retrieval benchmarking, demonstrated that the proposed method achieves very high effectiveness results, which are comparable or superior to the ones produced by state-of-the-art approaches.},
  archive      = {J_PR},
  author       = {Daniel Carlos Guimarães Pedronette and Lucas Pascotti Valem and Ricardo da S. Torres},
  doi          = {10.1016/j.patcog.2020.107666},
  journal      = {Pattern Recognition},
  pages        = {107666},
  shortjournal = {Pattern Recognition},
  title        = {A BFS-tree of ranking references for unsupervised manifold learning},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Ranking list preservation for feature matching. <em>PR</em>,
<em>111</em>, 107665. (<a
href="https://doi.org/10.1016/j.patcog.2020.107665">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature matching plays a very important role in many computer vision and pattern recognition tasks. The spatial neighborhood relationship (representing the topological structures of some key feature points of an image scene) is generally well preserved between two feature points of an image pair. Several mismatch-removing methods that maintain the local neighborhood structures of potential true matches have been proposed. Defining local neighborhood structures is a crucial issue in the feature matching problem. In this paper, we propose a robust and efficient topological structure measurement called top K rank preservation (TopKRP) for mismatch removal from given putative point set. We transform feature points from the feature space to the ranking list space. Thus, the topological structure similarity of two feature points can be simply calculated by comparing their ranking lists, which are measured by the top K ranking similarity based on the spatial Euclidean distance as well as the angle correlation. TopKRP is validated on 10 public image pairs with typical scenes and 2 artificially established datasets, namely, MI52 and RS153 . Experimental results demonstrate that the proposed approach outperforms several state-of-the-art feature matching methods, especially when the number of mismatches is large.},
  archive      = {J_PR},
  author       = {Junjun Jiang and Qing Ma and Xingyu Jiang and Jiayi Ma},
  doi          = {10.1016/j.patcog.2020.107665},
  journal      = {Pattern Recognition},
  pages        = {107665},
  shortjournal = {Pattern Recognition},
  title        = {Ranking list preservation for feature matching},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Representative null space LDA for discriminative
dimensionality reduction. <em>PR</em>, <em>111</em>, 107664. (<a
href="https://doi.org/10.1016/j.patcog.2020.107664">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Null space Linear Discriminant Analysis (NLDA) was proposed twenty years ago to overcome the singularity problem of LDA in practical applications. With two decades of technique development, many Discriminative Dimensionality Reduction (DDR) methods that outperform NLDA have been proposed. This paper provides new insight into NLDA and illustrates that NLDA is much more powerful after solving its inherent problem. The main problem of NLDA is the intrinsic overfitting problem. An ideal NLDA model is proposed to analyze its overfitting problem. Based on the ideal NLDA model, a more reasonable Representative NLDA (RNLDA) method is proposed to prevent overfitting. Two simple but efficient RNLDA algorithms are proposed to implement the RNLDA method with a theoretical proof. This study theoretically analyzed and indicated that applying the classical but simple hold-out pretraining method can automatically set the only parameter to achieve high performance. Extensive experiments with eight databases demonstrate the superior performance of the RNLDA method over state-of-the-art DDR methods.},
  archive      = {J_PR},
  author       = {Zaixing He and Mengtian Wu and Xinyue Zhao and Shuyou Zhang and Jianrong Tan},
  doi          = {10.1016/j.patcog.2020.107664},
  journal      = {Pattern Recognition},
  pages        = {107664},
  shortjournal = {Pattern Recognition},
  title        = {Representative null space LDA for discriminative dimensionality reduction},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Pairwise dependence-based unsupervised feature selection.
<em>PR</em>, <em>111</em>, 107663. (<a
href="https://doi.org/10.1016/j.patcog.2020.107663">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many research topics present very high dimensional data . Because of the heavy execution times and large memory requirements, many machine learning methods have difficulty in processing these data. In this paper, we propose a new unsupervised feature selection method considering the pairwise dependence of features (feature dependency-based unsupervised feature selection, or DUFS). To avoid selecting redundant features, the proposed method calculates the dependence among features and applies this information to a regression-based unsupervised feature selection process. We can select small feature set with the dependence among features by eliminating redundant features. To consider the dependence among features, we used mutual information widely used in supervised feature selection area. To our best knowledge, it is the first study to consider the pairwise dependence of features in the unsupervised feature selection method. Experimental results for six data sets demonstrate that the proposed method outperforms existing state-of-the-art unsupervised feature selection methods in most cases.},
  archive      = {J_PR},
  author       = {Hyunki Lim and Dae-Won Kim},
  doi          = {10.1016/j.patcog.2020.107663},
  journal      = {Pattern Recognition},
  pages        = {107663},
  shortjournal = {Pattern Recognition},
  title        = {Pairwise dependence-based unsupervised feature selection},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The trace kernel bandwidth criterion for support vector data
description. <em>PR</em>, <em>111</em>, 107662. (<a
href="https://doi.org/10.1016/j.patcog.2020.107662">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Support vector data description (SVDD) is a popular anomaly detection technique. The computation of the SVDD classifier requires a kernel function , for which the Gaussian kernel is a common choice. The Gaussian kernel has a bandwidth parameter, and it is important to set the value of this parameter correctly to ensure good results. A small bandwidth leads to overfitting, and the resulting SVDD classifier overestimates the number of anomalies, whereas a large bandwidth leads to underfitting and an inability to detect many anomalies. In this paper, we present a new, unsupervised method for selecting the Gaussian kernel bandwidth . Our method exploits a low-rank representation of the kernel matrix to suggest a kernel bandwidth value. Our new technique is competitive with the current state of the art for low-dimensional data and performs extremely well for many classes of high-dimensional data. This method is also applicable to one-class support vector machines (OCSVM).},
  archive      = {J_PR},
  author       = {Arin Chaudhuri and Carol Sadek and Deovrat Kakde and Haoyu Wang and Wenhao Hu and Hansi Jiang and Seunghyun Kong and Yuwei Liao and Sergiy Peredriy},
  doi          = {10.1016/j.patcog.2020.107662},
  journal      = {Pattern Recognition},
  pages        = {107662},
  shortjournal = {Pattern Recognition},
  title        = {The trace kernel bandwidth criterion for support vector data description},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bounded manifold completion. <em>PR</em>, <em>111</em>,
107661. (<a href="https://doi.org/10.1016/j.patcog.2020.107661">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nonlinear dimensionality reduction is an active area of research. In this paper, we present a thematically different approach to detect a low-dimensional manifold that lies within a set of bounds derived from a given point cloud. A matrix representing distances on a low-dimensional manifold is low-rank, and our method is based on current low-rank Matrix Completion (MC) techniques for recovering a partially observed matrix from fully observed entries. MC methods are currently used to solve challenging real-world problems such as image inpainting and recommender systems . Our MC scheme utilizes efficient optimization techniques that employ a nuclear norm convex relaxation as a surrogate for non-convex and discontinuous rank minimization. The method theoretically guarantees on detection of low-dimensional embeddings and is robust to non-uniformity in the sampling of the manifold. We validate the performance of this approach using both a theoretical analysis as well as synthetic and real-world benchmark datasets.},
  archive      = {J_PR},
  author       = {Kelum Gajamannage and Randy Paffenroth},
  doi          = {10.1016/j.patcog.2020.107661},
  journal      = {Pattern Recognition},
  pages        = {107661},
  shortjournal = {Pattern Recognition},
  title        = {Bounded manifold completion},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Speed-up and multi-view extensions to subclass discriminant
analysis. <em>PR</em>, <em>111</em>, 107660. (<a
href="https://doi.org/10.1016/j.patcog.2020.107660">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a speed-up approach for subclass discriminant analysis and formulate a novel efficient multi-view solution to it. The speed-up approach is developed based on graph embedding and spectral regression approaches that involve eigendecomposition of the corresponding Laplacian matrix and regression to its eigenvectors . We show that by exploiting the structure of the between-class Laplacian matrix, the eigendecomposition step can be substituted with a much faster process. Furthermore, we formulate a novel criterion for multi-view subclass discriminant analysis and show that an efficient solution to it can be obtained in a similar manner to the single-view case. We evaluate the proposed methods on nine single-view and nine multi-view datasets and compare them with related existing approaches. Experimental results show that the proposed solutions achieve competitive performance, often outperforming the existing methods. At the same time, they significantly decrease the training time.},
  archive      = {J_PR},
  author       = {Kateryna Chumachenko and Jenni Raitoharju and Alexandros Iosifidis and Moncef Gabbouj},
  doi          = {10.1016/j.patcog.2020.107660},
  journal      = {Pattern Recognition},
  pages        = {107660},
  shortjournal = {Pattern Recognition},
  title        = {Speed-up and multi-view extensions to subclass discriminant analysis},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adversarial co-distillation learning for image recognition.
<em>PR</em>, <em>111</em>, 107659. (<a
href="https://doi.org/10.1016/j.patcog.2020.107659">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge distillation is an effective way to transfer the knowledge from a pre-trained teacher model to a student model. Co-distillation, as an online variant of distillation, further accelerates the training process and paves a new way to explore the “dark knowledge” by training n models in parallel. In this paper, we explore the “divergent examples”, which can make the classifiers have different predictions and thus induce the “dark knowledge”, and we propose a novel approach named Adversarial Co-distillation Networks (ACNs) to enhance the “dark knowledge” by generating extra divergent examples. Note that we do not involve any extra dataset, and we only utilize the standard training set to train the entire framework. ACNs are end-to-end frameworks composed of two parts: an adversarial phase consisting of Generative Adversarial Networks (GANs) to generate the divergent examples and a co-distillation phase consisting of multiple classifiers to learn the divergent examples. These two phases are learned in an iterative and adversarial way. To guarantee the quality of the divergent examples and the stability of ACNs, we further design “Weakly Residual Connection” module and “Restricted Adversarial Search” module to assist in the training process. Extensive experiments with various deep architectures on different datasets well demonstrate the effectiveness of our approach.},
  archive      = {J_PR},
  author       = {Haoran Zhang and Zhenzhen Hu and Wei Qin and Mingliang Xu and Meng Wang},
  doi          = {10.1016/j.patcog.2020.107659},
  journal      = {Pattern Recognition},
  pages        = {107659},
  shortjournal = {Pattern Recognition},
  title        = {Adversarial co-distillation learning for image recognition},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Mixed-precision quantized neural networks with progressively
decreasing bitwidth. <em>PR</em>, <em>111</em>, 107647. (<a
href="https://doi.org/10.1016/j.patcog.2020.107647">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efficient model inference is an important and practical issue in the deployment of deep neural networks on resource constraint platforms. Network quantization addresses this problem effectively by leveraging low-bit representation and arithmetic that could be conducted on dedicated embedded systems . In the previous works, the parameter bitwidth is set homogeneously and there is a trade-off between superior performance and aggressive compression. Actually, the stacked network layers, which are generally regarded as hierarchical feature extractors, contribute diversely to the overall performance. For a well-trained neural network, the feature distributions of different categories are organized gradually as the network propagates forward. Hence the capability requirement on the subsequent feature extractors is reduced. It indicates that the neurons in posterior layers could be assigned with lower bitwidth for quantized neural networks . Based on this observation, a simple yet effective mixed-precision quantized neural network with progressively decreasing bitwidth is proposed to improve the trade-off between accuracy and compression. Extensive experiments on typical network architectures and benchmark datasets demonstrate that the proposed method could achieve better or comparable results while reducing the memory space for quantized parameters by more than 25\% in comparison with the homogeneous counterparts. In addition, the results also demonstrate that the higher-precision bottom layers could boost the 1-bit network performance appreciably due to a better preservation of the original image information while the lower-precision posterior layers contribute to the regularization of k − k− bit networks.},
  archive      = {J_PR},
  author       = {Tianshu Chu and Qin Luo and Jie Yang and Xiaolin Huang},
  doi          = {10.1016/j.patcog.2020.107647},
  journal      = {Pattern Recognition},
  pages        = {107647},
  shortjournal = {Pattern Recognition},
  title        = {Mixed-precision quantized neural networks with progressively decreasing bitwidth},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Semisupervised charting for spectral multimodal manifold
learning and alignment. <em>PR</em>, <em>111</em>, 107645. (<a
href="https://doi.org/10.1016/j.patcog.2020.107645">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For one given scene, multimodal data are acquired from multiple sensors. They share some similarities across the sensor types (redundant part of the information, also called coupling part) and they also provide modality-specific information (dissimilarities across the sensors, also called decoupling part). Additional critical knowledge about the scene can hence be extracted, which is not extractable from each modality alone. For the processing of multimodal data, we propose in this paper a model to simultaneously learn the underlying low-dimensional manifold in each modality, and locally align these manifolds across different modalities. For each pair of modalities we first build a common manifold that represents the corresponding (redundant) part of information, ignoring non-corresponding (modality specific) parts. We propose a semi-supervised learning model, using a limited amount of prior knowledge about the coupling and decoupling components of the different modalities. We propose a localized version of Laplacian eigenmaps technique specifically designed to handle multimodal manifold learning, in which the ideas of local patching of the manifolds, also known as manifold charting, is combined with the joint spectral analysis of the graph Laplacians of the different modalities. The limited given supervised information is then extending on the manifold of each modality. The idea of functional mapping is finally used to align the different manifolds across modalities. The evaluation of the proposed model using synthetic and real-world multimodal problems shows promising results, compared to several related techniques.},
  archive      = {J_PR},
  author       = {Ali Pournemat and Peyman Adibi and Jocelyn Chanussot},
  doi          = {10.1016/j.patcog.2020.107645},
  journal      = {Pattern Recognition},
  pages        = {107645},
  shortjournal = {Pattern Recognition},
  title        = {Semisupervised charting for spectral multimodal manifold learning and alignment},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Faster SVM training via conjugate SMO. <em>PR</em>,
<em>111</em>, 107644. (<a
href="https://doi.org/10.1016/j.patcog.2020.107644">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose an improved version of the SMO algorithm for training classification and regression SVMs , based on a Conjugate Descent procedure. This new approach only involves a modest increase on the computational cost of each iteration but, in turn, usually results in a substantial decrease in the number of iterations required to converge to a given precision. Besides, we prove convergence of the iterates of this new Conjugate SMO as well as a linear rate when the kernel matrix is positive definite. We have implemented Conjugate SMO within the LIBSVM library and show experimentally that it is faster for many hyper-parameter configurations, being often a better option than second order SMO when performing a grid-search for SVM tuning.},
  archive      = {J_PR},
  author       = {Alberto Torres-Barrán and Carlos M. Alaíz and José R. Dorronsoro},
  doi          = {10.1016/j.patcog.2020.107644},
  journal      = {Pattern Recognition},
  pages        = {107644},
  shortjournal = {Pattern Recognition},
  title        = {Faster SVM training via conjugate SMO},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Image denoising using complex-valued deep CNN. <em>PR</em>,
<em>111</em>, 107639. (<a
href="https://doi.org/10.1016/j.patcog.2020.107639">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While complex-valued transforms have been widely used in image processing and have their deep connections to biological vision systems, complex-valued convolutional neural networks (CNNs) have not seen their applications in image recovery. This paper aims at investigating the potentials of complex-valued CNNs for image denoising . A CNN is developed for image denoising with its key mathematical operations defined in the complex number field to exploit the merits of complex-valued operations, including the compactness of convolution given by the tensor product of 1D complex-valued filters, the nonlinear activation on phase, and the noise robustness of residual blocks. The experimental results show that, the proposed complex-valued denoising CNN performs competitively against existing state-of-the-art real-valued denoising CNNs, with better robustness to possible inconsistencies of noise models between training samples and test images. The results also suggest that complex-valued CNNs provide another promising deep-learning-based approach to image denoising and other image recovery tasks.},
  archive      = {J_PR},
  author       = {Yuhui Quan and Yixin Chen and Yizhen Shao and Huan Teng and Yong Xu and Hui Ji},
  doi          = {10.1016/j.patcog.2020.107639},
  journal      = {Pattern Recognition},
  pages        = {107639},
  shortjournal = {Pattern Recognition},
  title        = {Image denoising using complex-valued deep CNN},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hyperspectral remote sensing image classification based on
tighter random projection with minimal intra-class variance algorithm.
<em>PR</em>, <em>111</em>, 107635. (<a
href="https://doi.org/10.1016/j.patcog.2020.107635">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aiming at solving the problem of image size limiting in the traditional Random Projection (RP) algorithm, a novel Tighter Random Projection (TRP), which combines the scheme with Minimal Intra-class Variance (TRP-MIV) for hyperspectral remote sensing image classification is proposed. First, a new tighter dimensional boundary for expanding image size with the TRP-MIV matrix selected by multiple sampling for improving the class separability is defined to reduce dimension. Then the proposed algorithm is implemented, which integrates TRP-MIV for dimensionality reduction and Minimum Distance (MD) classifier for image classification . Finally, the image size and dimensionality reduction are evaluated by the number of spectral pixels under different theorems, and the spectral difference before and after dimensionality reduction, respectively. Classification performance is evaluated by kappa coefficient , Overall Accuracy (OA), Average Accuracy (AA), Average Precision Rate (APR) and running time. Classification results are obtained from the Airborne Visible/Infrared Imaging Spectrometer (AVIRIS) scanner and the Reflective Optics System Imaging Spectrometer (ROSIS) scanner, which indicate that the proposed algorithm is efficient and promising.},
  archive      = {J_PR},
  author       = {Quanhua Zhao and Shuhan Jia and Yu Li},
  doi          = {10.1016/j.patcog.2020.107635},
  journal      = {Pattern Recognition},
  pages        = {107635},
  shortjournal = {Pattern Recognition},
  title        = {Hyperspectral remote sensing image classification based on tighter random projection with minimal intra-class variance algorithm},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Spectrum-aware discriminative deep feature learning for
multi-spectral face recognition. <em>PR</em>, <em>111</em>, 107632. (<a
href="https://doi.org/10.1016/j.patcog.2020.107632">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One primary challenge of face recognition is that the performance is seriously affected by varying illumination . Multi-spectral imaging can capture face images in the visible spectrum and beyond, which is deemed to be an effective technology in response to this challenge. For current multi-spectral imaging-based face recognition methods , how to fully explore the discriminant and correlation features from both the intra-spectrum and inter-spectrum aspects with only a limited number of multi-spectral samples for model training has not been well studied. To address this problem, in this paper, we propose a novel face recognition approach named Spectrum-aware Discriminative Deep Learning (SDDL). To take full advantage of the multi-spectral training samples, we build a discriminative multi-spectral network (DMN) and take face sample pairs as the input of the network. By jointly considering the spectrum and the class label information, SDDL trains the network for projecting samples pairs into a discriminant feature subspace, on which the intrinsic relationship including the intra- and inter-spectrum discrimination and the inter-spectrum correlation among face samples is well discovered. The proposed approach is evaluated on three widely used datasets HK PolyU, CMU, and UWA. Extensive experimental results demonstrate the superiority of SDDL over state-of-the-art competing methods.},
  archive      = {J_PR},
  author       = {Fei Wu and Xiao-Yuan Jing and Yujian Feng and Yi-mu Ji and Ruchuan Wang},
  doi          = {10.1016/j.patcog.2020.107632},
  journal      = {Pattern Recognition},
  pages        = {107632},
  shortjournal = {Pattern Recognition},
  title        = {Spectrum-aware discriminative deep feature learning for multi-spectral face recognition},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Context-aware network for RGB-d salient object detection.
<em>PR</em>, <em>111</em>, 107630. (<a
href="https://doi.org/10.1016/j.patcog.2020.107630">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks (CNNs) have shown unprecedented success in object representation and detection. Nevertheless, CNNs lack the capability to model context dependencies among objects, which are crucial for salient object detection. As the long short-term memory (LSTM) is advantageous in propagating information, in this paper, we propose two variant LSTM units for the exploration of contextual dependencies. By incorporating these units, we present a context-aware network (CAN) to detect salient objects in RGB-D images. The proposed model consists of three components: feature extraction, context fusion of multiple modalities and context-dependent deconvolution . The first component is responsible for extracting hierarchical features in color and depth images using CNNs, respectively. The second component fuses high-level features by a variant LSTM to model multi-modal spatial dependencies in contexts. The third component, embedded with another variant LSTM, models local hierarchical context dependencies of the fused features at multi-scales. Experimental results on two public benchmark datasets show that the proposed CAN can achieve state-of-the-art performance for RGB-D stereoscopic salient object detection.},
  archive      = {J_PR},
  author       = {Fangfang Liang and Lijuan Duan and Wei Ma and Yuanhua Qiao and Jun Miao and Qixiang Ye},
  doi          = {10.1016/j.patcog.2020.107630},
  journal      = {Pattern Recognition},
  pages        = {107630},
  shortjournal = {Pattern Recognition},
  title        = {Context-aware network for RGB-D salient object detection},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A review of lane detection methods based on deep learning.
<em>PR</em>, <em>111</em>, 107623. (<a
href="https://doi.org/10.1016/j.patcog.2020.107623">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lane detection is an application of environmental perception, which aims to detect lane areas or lane lines by camera or lidar. In recent years, gratifying progress has been made in detection accuracy. To the best of our knowledge, this paper is the first attempt to make a comprehensive review of vision-based lane detection methods. First, we introduce the background of lane detection, including traditional lane detection methods and related deep learning methods. Second, we group the existing lane detection methods into two categories: two-step and one-step methods. Around the above summary, we introduce lane detection methods from the following two perspectives: (1) network architectures, including classification and object detection-based methods, end-to-end image-segmentation based methods, and some optimization strategies; (2) related loss functions. For each method, its contributions and weaknesses are introduced. Then, a brief comparison of representative methods is presented. Finally, we conclude this survey with some current challenges, such as expensive computation and the lack of generalization. And we point out some directions to be further explored in the future, that is, semi-supervised learning, meta-learning and neural architecture search, etc.},
  archive      = {J_PR},
  author       = {Jigang Tang and Songbin Li and Peng Liu},
  doi          = {10.1016/j.patcog.2020.107623},
  journal      = {Pattern Recognition},
  pages        = {107623},
  shortjournal = {Pattern Recognition},
  title        = {A review of lane detection methods based on deep learning},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dual subspace discriminative projection learning.
<em>PR</em>, <em>111</em>, 107581. (<a
href="https://doi.org/10.1016/j.patcog.2020.107581">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a dual subspace discriminative projection learning (DSDPL) framework for multi-category image classification . Our approach reflects the notion that images are composed of class-shared information, class-specific information, and sparse noise. Unlike traditional subspace learning methods, DSDPL serves to decompose original high dimensional data , via learned projection matrices , into class-shared and class-specific subspaces. The learned projection matrices are jointly constrained with l 2,1 sparse norm and LDA terms while the reconstructive properties of DSDPL reduce information loss, leading to greater stability within low dimensional subspaces . Regression-based terms are also included to facilitate a more robust classification approach , using extracted class-specific features for better classification. Our approach is examined on five different datasets for face, object and scene classifications. Experimental results demonstrate not only the superiority and versatility of DSDPL over current benchmark approaches, but also a more robust classification approach with low sample size training data.},
  archive      = {J_PR},
  author       = {Gregg Belous and Andrew Busch and Yongsheng Gao},
  doi          = {10.1016/j.patcog.2020.107581},
  journal      = {Pattern Recognition},
  pages        = {107581},
  shortjournal = {Pattern Recognition},
  title        = {Dual subspace discriminative projection learning},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). FeatFlow: Learning geometric features for 3D motion
estimation. <em>PR</em>, <em>111</em>, 107574. (<a
href="https://doi.org/10.1016/j.patcog.2020.107574">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D motion estimation is an important prerequisite for the autonomous operation of vehicles and robots in dynamic environments. This work presents FeatFlow, a novel neural network architecture to estimate 3D motions from unstructured point clouds. Specifically, we learn deep geometric features to estimate the dense scene flow and the ego-motion of the platform. We build a scene flow estimation pipeline by an encoder-decoder architecture which comprises three novel modules: feature extractor, motion embedder, and flow decoder. By using a point-score layer to assign scores to the extracted features in a learning procedure, the feature extractor effectively extracts keypoints and features that are most significant for estimating the relative transformation between two consecutive point clouds. The whole model adaptively learns the required robust descriptors to represent a variety of point motions at the object or scene level. We evaluated our approach on synthetic data from FlyingThings3D, and real-world LiDAR scans from KITTI and Oxford RobotCar. Our network successfully generalizes to datasets with different patterns, outperforming various baselines and achieving state-of-the-art performance.},
  archive      = {J_PR},
  author       = {Qing Li and Cheng Wang and Xin Li and Chenglu Wen},
  doi          = {10.1016/j.patcog.2020.107574},
  journal      = {Pattern Recognition},
  pages        = {107574},
  shortjournal = {Pattern Recognition},
  title        = {FeatFlow: Learning geometric features for 3D motion estimation},
  volume       = {111},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep snippet selective network for weakly supervised
temporal action localization. <em>PR</em>, <em>110</em>, 107686. (<a
href="https://doi.org/10.1016/j.patcog.2020.107686">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal action localization has been a hot topic in video analyzation. In this paper, we propose a novel method called deep snippet selective network (DSSN) to address two key problems in weak supervision for temporal action localization, which are separability and integrality. Specifically, we employ two erasing branches to ensure the integrality, which can force the network to select other complementary snippets by erasing the most discriminative snippets. It is worth mentioning that a ternary mask is utilized to provide erasing branches with a background prior to enhance the separability of the model. Besides, we design a background suppression branch to further reduce the effect of background snippets. Extensive experiments on dataset THUMOS’14 and ActivityNet show the effectiveness of our method.},
  archive      = {J_PR},
  author       = {Yongxin Ge and Xiaolei Qin and Dan Yang and Martin Jagersand},
  doi          = {10.1016/j.patcog.2020.107686},
  journal      = {Pattern Recognition},
  pages        = {107686},
  shortjournal = {Pattern Recognition},
  title        = {Deep snippet selective network for weakly supervised temporal action localization},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Place perception from the fusion of different image
representation. <em>PR</em>, <em>110</em>, 107680. (<a
href="https://doi.org/10.1016/j.patcog.2020.107680">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inspired by the human way of place understanding, we present a novel indoor place perception network to overcome: 1). the simplicity of existing methods that only use the image features of object regions to recognize the indoor place, 2). insufficient consideration of the semantic information about object attributes and states. By utilizing multi-modal information containing the image and natural language, the proposed method can comprehensively express the attributes, state, and relationships of objects which are beneficial for indoor place understanding and recognition. Specifically, we first present a natural language generation framework based on a Convolution Neural Network (CNN) and Long Short-Term Memory (LSTM) to imitate the process of place understanding. Next, a Convolutional Auto-Encoder (CAE) and a mixed CNN-LSTM are proposed to extract image features and semantic features , respectively. Then, two different fusion strategies, namely feature-level fusion and object-level fusion, are designed to integrate different types of features and features from different objects. The category of the indoor place is finally recognized based on fused information. Comprehensive experiments are conducted on public datasets, and the results verify the effectiveness of the proposed place perception method based on linguistic cues .},
  archive      = {J_PR},
  author       = {Pei Li and Xinde Li and Xianghui Li and Hong Pan and M.O. Khyam and Md. Noor-A-Rahim and Shuzhi Sam Ge},
  doi          = {10.1016/j.patcog.2020.107680},
  journal      = {Pattern Recognition},
  pages        = {107680},
  shortjournal = {Pattern Recognition},
  title        = {Place perception from the fusion of different image representation},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Unified cross-domain classification via geometric and
statistical adaptations. <em>PR</em>, <em>110</em>, 107658. (<a
href="https://doi.org/10.1016/j.patcog.2020.107658">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain adaptation aims to learn an adaptive classifier for target data using the labelled source data from a different distribution. Most proposed works construct cross-domain classifier by exploring one-sided property of the input data, i.e., either geometric or statistical property. Therefore they may ignore the complementarity between the two properties. Moreover, many previous methods implement knowledge transfer with two separated steps: divergence minimization and classifier construction, which degrades the adaptation robustness. In order to address such problems, we propose a u nified c ross-domain classification method via g eometric and s tatistical adaptations (UCGS). UCGS models the divergence minimization and classifier construction in a unified way based on structural risk minimization principle and coupled adaptations theory. Specifically, UCGS constructs an adaptive model by simultaneously minimizing the structural risk on labelled source data, using Maximum Mean Discrepancy (MMD) criterion to implement statistical adaptation, and flexibly employing the Nyström method to explore the geometric connections between domains. A domain-invariant graph is successfully constructed to link the two domains geometrically. The standard supervised methods can be used to instantiate UCGS to handle inter-domain classification problems. Comprehensive experiments show the superiority of UCGS on several real-world datasets.},
  archive      = {J_PR},
  author       = {Weifeng Liu and Jinfeng Li and Baodi Liu and Weili Guan and Yicong Zhou and Changsheng Xu},
  doi          = {10.1016/j.patcog.2020.107658},
  journal      = {Pattern Recognition},
  pages        = {107658},
  shortjournal = {Pattern Recognition},
  title        = {Unified cross-domain classification via geometric and statistical adaptations},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Application of binocular disparity and receptive field
dynamics: A biologically-inspired model for contour detection.
<em>PR</em>, <em>110</em>, 107657. (<a
href="https://doi.org/10.1016/j.patcog.2020.107657">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neurophysiological evidence demonstrates that classical receptive field responses in the primary visual cortex can be modulated by the non-classical receptive field. Although models based on the non-classical receptive field have been proposed, which has not employed the two following characteristics: dynamic regulation of the receptive field under external stimuli and depth determination by binocular cells. We propose a model that includes processing of gray-scale images and depth images. Luminance of grayscale image response was preprocessed using weighted least squares filtering; luminance of grayscale image and disparity information of depth image responses were obtained by V1 neuron responses. The receptive field size of each pixel used to calculate the luminance and disparity response is determined by the depth information. Luminance and disparity information responses were combined based on the optimal orientation, and robust contour maps were developed. Experimental results show that the proposed contour detection model outperforms biologically inspired models.},
  archive      = {J_PR},
  author       = {Qing Zhang and Chuan Lin and Fuzhang Li},
  doi          = {10.1016/j.patcog.2020.107657},
  journal      = {Pattern Recognition},
  pages        = {107657},
  shortjournal = {Pattern Recognition},
  title        = {Application of binocular disparity and receptive field dynamics: A biologically-inspired model for contour detection},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Real-time lexicon-free scene text retrieval. <em>PR</em>,
<em>110</em>, 107656. (<a
href="https://doi.org/10.1016/j.patcog.2020.107656">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we address the task of scene text retrieval: given a text query, the system returns all images containing the queried text. The proposed model uses a single shot CNN architecture that predicts bounding boxes and builds a compact representation of spotted words. In this way, this problem can be modeled as a nearest neighbor search of the textual representation of a query over the outputs of the CNN collected from the totality of an image database. Our experiments demonstrate that the proposed model outperforms previous state-of-the-art, while offering a significant increase in processing speed and unmatched expressiveness with samples never seen at training time. Several experiments to assess the generalization capability of the model are conducted in a multilingual dataset, as well as an application of real-time text spotting in videos.},
  archive      = {J_PR},
  author       = {Andrés Mafla and Rubèn Tito and Sounak Dey and Lluís Gómez and Marçal Rusiñol and Ernest Valveny and Dimosthenis Karatzas},
  doi          = {10.1016/j.patcog.2020.107656},
  journal      = {Pattern Recognition},
  pages        = {107656},
  shortjournal = {Pattern Recognition},
  title        = {Real-time lexicon-free scene text retrieval},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-objective adaptive differential evolution for SVM/SVR
hyperparameters selection. <em>PR</em>, <em>110</em>, 107649. (<a
href="https://doi.org/10.1016/j.patcog.2020.107649">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Parameters Selection Problem (PSP) is a relevant and complex optimization issue in Support Vector Machine (SVM) and Support Vector Regression (SVR), looking for obtaining an optimal set of hyperparameters. In our case, the optimization problem is addressed to obtain models that minimize the number of support vectors and maximize generalization capacity. However, to obtain accurate and low complexity solutions, defining an adequate kernel function and the SVM/SVR’s hyperparameters are necessary, which currently represents a relevant research topic. To tackle this problem, this work proposes a multi-objective metaheuristic named Adaptive Parameter control with Mutant Tournament Multi-Objective Differential Evolution (APMT-MODE). Its performance is first tested in a series of benchmarks for classification and regression problems using simple kernels such as Gaussian and polynomial kernels. In both cases, the APMT-MODE is able to yield more precise and more straightforward solutions using simple kernels. Then, the approach is used on a real case study to create a welding bead depth and width SVR models for a Gas Metal Arc Welding (GMAW) process. Additionally, a study on kernel functions was developed in terms of computational effort, aiming to assess its performance for embedded systems applications.},
  archive      = {J_PR},
  author       = {Carlos Eduardo da Silva Santos and Renato Coral Sampaio and Leandro dos Santos Coelho and Guillermo Alvarez Bestard and Carlos Humberto Llanos},
  doi          = {10.1016/j.patcog.2020.107649},
  journal      = {Pattern Recognition},
  pages        = {107649},
  shortjournal = {Pattern Recognition},
  title        = {Multi-objective adaptive differential evolution for SVM/SVR hyperparameters selection},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multimodal subspace support vector data description.
<em>PR</em>, <em>110</em>, 107648. (<a
href="https://doi.org/10.1016/j.patcog.2020.107648">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel method for projecting data from multiple modalities to a new subspace optimized for one-class classification. The proposed method iteratively transforms the data from the original feature space of each modality to a new common feature space along with finding a joint compact description of data coming from all the modalities. For data in each modality, we define a separate transformation to map the data from the corresponding feature space to the new optimized subspace by exploiting the available information from the class of interest only. We also propose different regularization strategies for the proposed method and provide both linear and non-linear formulations. The proposed Multimodal Subspace Support Vector Data Description outperforms all the competing methods using data from a single modality or fusing data from all modalities in four out of five datasets.},
  archive      = {J_PR},
  author       = {Fahad Sohrab and Jenni Raitoharju and Alexandros Iosifidis and Moncef Gabbouj},
  doi          = {10.1016/j.patcog.2020.107648},
  journal      = {Pattern Recognition},
  pages        = {107648},
  shortjournal = {Pattern Recognition},
  title        = {Multimodal subspace support vector data description},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Tackling mode collapse in multi-generator GANs with
orthogonal vectors. <em>PR</em>, <em>110</em>, 107646. (<a
href="https://doi.org/10.1016/j.patcog.2020.107646">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative Adversarial Networks (GANs) have been widely used to generate realistic-looking instances. However, training robust GAN is a non-trivial task due to the problem of mode collapse. Although many GAN variants are proposed to overcome this problem, they have limitations. Those existing studies either generate identical instances or result in negative gradients during training. In this paper, we propose a new approach to training GAN to overcome mode collapse by employing a set of generators, an encoder and a discriminator . A new minimax formula is proposed to simultaneously train all components in a similar spirit to vanilla GAN. The orthogonal vector strategy is employed to guide multiple generators to learn different information in a complementary manner. In this way, we term our approach Multi-Generator Orthogonal GAN (MGO-GAN). Specifically, the synthetic data produced by those generators are fed into the encoder to obtain feature vectors. The orthogonal value is calculated between any two feature vectors, which loyally reflects the correlation between vectors. Such a correlation indicates how different information has been learnt by generators. The lower the orthogonal value is, the more different information the generators learn. We minimize the orthogonal value along with minimizing the generator loss through back-propagation in the training of GAN. The orthogonal value is integrated with the original generator loss to jointly update the corresponding generator’s parameters. We conduct extensive experiments utilizing MNIST, CIFAR10 and CelebA datasets to demonstrate the significant performance improvement of MGO-GAN in terms of generated data quality and diversity at different resolutions.},
  archive      = {J_PR},
  author       = {Wei Li and Li Fan and Zhenyu Wang and Chao Ma and Xiaohui Cui},
  doi          = {10.1016/j.patcog.2020.107646},
  journal      = {Pattern Recognition},
  pages        = {107646},
  shortjournal = {Pattern Recognition},
  title        = {Tackling mode collapse in multi-generator GANs with orthogonal vectors},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Virtual sample-based deep metric learning using discriminant
analysis. <em>PR</em>, <em>110</em>, 107643. (<a
href="https://doi.org/10.1016/j.patcog.2020.107643">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep metric learning (DML) has been designed to maximize the inter-class variance that is the distance between embedding features belonging to different classes. Since conventional DML techniques do not consider the statistical characteristics of the embedding space, or they calculate similarity using only a given feature, they make it difficult to adaptively reflect the characteristics of the feature distribution during the learning process. This paper proposes a virtual metric loss (VML) incorporating with embedding features by using virtual samples produced through linear discriminant analysis (LDA). This study is valuable in that it proposes a new metric that can learn inter-class variance of embedding features by integrating discriminant analysis and metric learning which have a common purpose of inter-class variance analysis. In addition, we theoretically analyze the eigenvalue equation problem and the degree of stabilization in the embedding space. We have verified the performance of the proposed VML through extensive experiments on large and few-shot retrieval datasets. For example, in the CUB200-2011 dataset, the VML showed a recall rate about 0.7\% higher than a state-of-the-art method. We also explored a new similarity through virtual samples and adjusted the difficulty of embedding features, thereby confirming the possibility of expanding virtual samples into various fields of pattern recognition.},
  archive      = {J_PR},
  author       = {Dae Ha Kim and Byung Cheol Song},
  doi          = {10.1016/j.patcog.2020.107643},
  journal      = {Pattern Recognition},
  pages        = {107643},
  shortjournal = {Pattern Recognition},
  title        = {Virtual sample-based deep metric learning using discriminant analysis},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A novel error-correcting output codes based on genetic
programming and ternary digit operators. <em>PR</em>, <em>110</em>,
107642. (<a href="https://doi.org/10.1016/j.patcog.2020.107642">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The key to the success of an Error-Correcting Output Code (ECOC) algorithm is the effective codematrix, which represents a set of class reassignment schemes for decomposing a multiclass problem into a set of binary class problems. This paper proposes a new method, which uses Ternary digit Operators based Genetic Programming (GP) to generate effective ECOC codematrix (TOGP-ECOC for short). In our GP, each terminal node stores a ternary digit string, representing a column and a related feature subset; each non-terminal node represents a ternary digit operator, which produces a new column based on its child nodes. In this way, each individual is interpreted as an ECOC codematrix along with a set of corresponding feature subsets, serving the solution for the multiclass classification task. When a new individual is produced, a legality checking process is carried out to verify whether the transformed codematrix follows the ECOC constraints. The illegal one is corrected according to different strategies. Besides, a local optimization algorithm is designed to prune redundant columns and improve the performance of each individual. Our experiments compared TOGP-ECOC with some well known ECOC algorithms on various data sets, and the results confirm the superiority of our algorithm. Our source code is available at: https://github.com/MLDMXM2017/TOGP-ECOC .},
  archive      = {J_PR},
  author       = {Liang Yi-Fan and Liu Chang and Wang Han-Rui and Liu Kun-Hong and Yao Jun-Feng and She Ying-Ying and Dai Gui-Ming and Yuna Okina},
  doi          = {10.1016/j.patcog.2020.107642},
  journal      = {Pattern Recognition},
  pages        = {107642},
  shortjournal = {Pattern Recognition},
  title        = {A novel error-correcting output codes based on genetic programming and ternary digit operators},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning representation from multiple media domains for
enhanced event discovery. <em>PR</em>, <em>110</em>, 107640. (<a
href="https://doi.org/10.1016/j.patcog.2020.107640">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we focus on event discovery by utilizing data distributed in multiple media domains, such as news media and social media. To this end, we propose an in-domain and cross-domain Laplacian regularization (ICLR) model, which can learn effective data representation for both textual news reports contributed by journalists in news media domain, and image posts shared by amateur users in social media domain. The achieved data representation can be used by classification and clustering strategies for existing and new event discovery, respectively. More specifically, ICLR constructs respective Laplacian regularization terms considering the property of inter-domain and intra-domain label consistency, which can be optimized by employing an alternating optimization strategy with theoretical guarantee for convergence. In particular, we collect and release a multi-domain and multimodal dataset for evaluations and public use.},
  archive      = {J_PR},
  author       = {Zhenguo Yang and Qing Li and Haoran Xie and Qi Wang and Wenyin Liu},
  doi          = {10.1016/j.patcog.2020.107640},
  journal      = {Pattern Recognition},
  pages        = {107640},
  shortjournal = {Pattern Recognition},
  title        = {Learning representation from multiple media domains for enhanced event discovery},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning adaptive geometry for unsupervised domain
adaptation. <em>PR</em>, <em>110</em>, 107638. (<a
href="https://doi.org/10.1016/j.patcog.2020.107638">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised domain adaptation is an effective approach to solve the problem of dataset bias. However, most existing unsupervised domain adaptation methods assume that the geometry structures of data distributions are similar in the source and target domains. This assumption is invalid in many practical applications, because the training and test datasets usually differ in the variability modes and/or variation degrees. This paper handles the problem of inconsistent geometries by aligning both data representations and geometries. To overcome the lack of target labels in aligning geometries, this paper proposes learning the adaptive geometry that is derived from the domain-shared label space. Source and target geometries are aligned by constraining them with the unified criteria of the adaptive geometry. Combining the adaptive geometry learning and adversarial learning techniques, we develop a geometry-aware dual-stream network to learn the geometry-aligned representations. Experimental results show that our method achieves good performance on cross-dataset recognition tasks.},
  archive      = {J_PR},
  author       = {Baoyao Yang and Pong C. Yuen},
  doi          = {10.1016/j.patcog.2020.107638},
  journal      = {Pattern Recognition},
  pages        = {107638},
  shortjournal = {Pattern Recognition},
  title        = {Learning adaptive geometry for unsupervised domain adaptation},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hypergraph convolution and hypergraph attention.
<em>PR</em>, <em>110</em>, 107637. (<a
href="https://doi.org/10.1016/j.patcog.2020.107637">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, graph neural networks have attracted great attention and achieved prominent performance in various research fields. Most of those algorithms have assumed pairwise relationships of objects of interest. However, in many real applications, the relationships between objects are in higher-order, beyond a pairwise formulation. To efficiently learn deep embeddings on the high-order graph-structured data, we introduce two end-to-end trainable operators to the family of graph neural networks, i.e. , hypergraph convolution and hypergraph attention. Whilst hypergraph convolution defines the basic formulation of performing convolution on a hypergraph, hypergraph attention further enhances the capacity of representation learning by leveraging an attention module. With the two operators, a graph neural network is readily extended to a more flexible model and applied to diverse applications where non-pairwise relationships are observed. Extensive experimental results with semi-supervised node classification demonstrate the effectiveness of hypergraph convolution and hypergraph attention.},
  archive      = {J_PR},
  author       = {Song Bai and Feihu Zhang and Philip H.S. Torr},
  doi          = {10.1016/j.patcog.2020.107637},
  journal      = {Pattern Recognition},
  pages        = {107637},
  shortjournal = {Pattern Recognition},
  title        = {Hypergraph convolution and hypergraph attention},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Position-aware self-attention based neural sequence
labeling. <em>PR</em>, <em>110</em>, 107636. (<a
href="https://doi.org/10.1016/j.patcog.2020.107636">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sequence labeling is a fundamental task in natural language processing and has been widely studied. Recently, RNN-based sequence labeling models have increasingly gained attentions. Despite superior performance achieved by learning the long short-term ( i.e., successive ) dependencies, the way of sequentially processing inputs might limit the ability to capture the non-continuous relations over tokens within a sentence. To tackle the problem, we focus on how to effectively model successive and discrete dependencies of each token for enhancing the sequence labeling performance. Specifically, we propose an innovative attention-based model (called position-aware self-attention , i.e., PSA ) as well as a well-designed self-attentional context fusion layer within a neural network architecture , to explore the positional information of an input sequence for capturing the latent relations among tokens. Extensive experiments on three classical tasks in sequence labeling domain, i.e., part-of-speech ( POS ) tagging, named entity recognition ( NER ) and phrase chunking , demonstrate our proposed model outperforms the state-of-the-arts without any external knowledge, in terms of various metrics.},
  archive      = {J_PR},
  author       = {Wei Wei and Zanbo Wang and Xianling Mao and Guangyou Zhou and Pan Zhou and Sheng Jiang},
  doi          = {10.1016/j.patcog.2020.107636},
  journal      = {Pattern Recognition},
  pages        = {107636},
  shortjournal = {Pattern Recognition},
  title        = {Position-aware self-attention based neural sequence labeling},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Entropy based dictionary learning for image classification.
<em>PR</em>, <em>110</em>, 107634. (<a
href="https://doi.org/10.1016/j.patcog.2020.107634">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a new discriminative dictionary learning algorithm is introduced. An entropy based criterion is embedded into the objective function to enforce a proper structure for the dictionary items when decomposing signals of different classes. The proposed criterion influences the dictionary items to participate in the decomposition of a smaller number of classes as possible. Unlike the other methods, columns of the dictionary are not restricted to have pre-assigned labels and they are free to be representative of any class or to share features of several classes. The number of shared and discriminative items along with the number of dictionary items for each specific class is learned dynamically during the optimization process, depending on the complexity of the classification task and the distribution of different classes. The experimental results demonstrate that the proposed entropy based dictionary learning (EDL) algorithm outperforms other discriminative dictionary learning methods using several real-world image datasets.},
  archive      = {J_PR},
  author       = {Arash Abdi and Mohammad Rahmati and Mohammad M. Ebadzadeh},
  doi          = {10.1016/j.patcog.2020.107634},
  journal      = {Pattern Recognition},
  pages        = {107634},
  shortjournal = {Pattern Recognition},
  title        = {Entropy based dictionary learning for image classification},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Cross-view classification by joint adversarial learning and
class-specificity distribution. <em>PR</em>, <em>110</em>, 107633. (<a
href="https://doi.org/10.1016/j.patcog.2020.107633">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the promising preliminary results, none of existing deep learning based cross-view classification methods simultaneously takes into account both view consistency learning and class-specificity distribution of the extracted features, resulting in unstable classification performance. Moreover, most existing cross-view classification methods are sensitive to scale due to the scale issue of view representations, resulting in unstable view-consistent representations. In this paper, we propose a new deep adversarial network for cross-view classification that attempts to learn robust view-consistent representations by combing the thought of adversarial learning and metric learning in Fisher criterion. Meanwhile, a class-specificity distribution term, which is measured by ℓ 12 -norm, is employed to make the view-consistent representations with the same label to further have a common distribution in dimension space while view-representations with different labels have different distribution in the intrinsic dimension space. We formulate the aforementioned two concerns into a unified optimization framework. Extensive experiments on several real-world datasets indicate the effectiveness of our method over the other state-of-the-arts.},
  archive      = {J_PR},
  author       = {Siyang Deng and Wei Xia and Quanxue Gao and Xinbo Gao},
  doi          = {10.1016/j.patcog.2020.107633},
  journal      = {Pattern Recognition},
  pages        = {107633},
  shortjournal = {Pattern Recognition},
  title        = {Cross-view classification by joint adversarial learning and class-specificity distribution},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sparse motion fields for trajectory prediction. <em>PR</em>,
<em>110</em>, 107631. (<a
href="https://doi.org/10.1016/j.patcog.2020.107631">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Trajectory prediction is a crucial element of many automated tasks, such as autonomous navigation or video surveillance. To automatically predict the motion of an agent ( e.g. , pedestrian or car), the model needs to efficiently represent human motion and “understand” the external stimuli that may influence human behavior. In this work we propose a methodology to model the motion of agents in a video scene. Our method is based on space-varying sparse motion fields, which simultaneously characterize diverse motion patterns in the scene and implicitly learn contextual cues about the static environment , namely obstacles and semantic constraints. The sparse motion fields are applied to the task of long-term trajectory prediction using a probabilistic generative approach . Several benchmark data sets are used to demonstrate the potential of the proposed approach and show that our method achieves competitive state-of-the-art performances.},
  archive      = {J_PR},
  author       = {Catarina Barata and Jacinto C. Nascimento and João M. Lemos and Jorge S. Marques},
  doi          = {10.1016/j.patcog.2020.107631},
  journal      = {Pattern Recognition},
  pages        = {107631},
  shortjournal = {Pattern Recognition},
  title        = {Sparse motion fields for trajectory prediction},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Online feature selection system for big data classification
based on multi-objective automated negotiation. <em>PR</em>,
<em>110</em>, 107629. (<a
href="https://doi.org/10.1016/j.patcog.2020.107629">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature Selection (FS) plays an important role in learning and classification tasks . Its objective is to select the relevant and non-redundant features. Considering the huge number of features in real-world applications, FS methods using batch learning technique cannot resolve big data problems especially when data arrive sequentially. In this paper, we proposed an online feature selection system which resolves this problem. The proposed OFS system called MOANOFS (Multi-Objective Automated Negotiation based Online Feature Selection) explore the recent advances of online machine learning techniques and a conflict resolution technique (Automated Negotiation) for the purpose of enhancing the classification performance of ultra-high dimensional databases. MOANOFS uses two decision levels. In the first level, we decided which k(s) among the learners (or OFS methods) are the trustful ones (with high confidence or trust value). These elected k learners would participate in the second level where we integrated our proposed Multilateral Automated Negotiation based OFS (MANOFS) method. This would enable us to finally decide which features are the most relevant. We showed that MOANOFS system achieves high accuracy with several real text classification datasets as 20Newsgroups, RCV1.},
  archive      = {J_PR},
  author       = {Fatma BenSaid and Adel M. Alimi},
  doi          = {10.1016/j.patcog.2020.107629},
  journal      = {Pattern Recognition},
  pages        = {107629},
  shortjournal = {Pattern Recognition},
  title        = {Online feature selection system for big data classification based on multi-objective automated negotiation},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust kernelized graph-based learning. <em>PR</em>,
<em>110</em>, 107628. (<a
href="https://doi.org/10.1016/j.patcog.2020.107628">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The studies of hidden complex structures in data have popularized the use of graph-based learning methods in semi-supervised and unsupervised learning tasks. Kernelized graph-based methods are proven to perform better, but these methods suffer from the issue of appropriate kernel selection. Instead of using multiple views, these methods generally use a single view. But multi-view methods need a proper weight assignment technique to each view in proportion to their contribution to the learning task. To solve this, a novel Self-weighted Multi-view Multiple Kernel Learning (SMVMKL) framework is proposed using multiple kernels on multiple views that automatically assigns appropriate weight to each kernel of each view without introducing an additional parameter. But the real-world data that is either noisy or corrupt with outliers which may effect the performance of the proposed SMVMKL method. To deal with this, a Robust Self-weighted Multi-view Multiple Kernel Learning (RSMVMKL) framework using the l 2,1 -norm has also been proposed that reduces the effect of outliers present in the data set. Both the proposed methods have been evaluated on multiple benchmark data sets and result in a performance comparable with the other state-of-the-art multi-view methods considered in this paper.},
  archive      = {J_PR},
  author       = {Supratim Manna and Jessy Rimaya Khonglah and Anirban Mukherjee and Goutam Saha},
  doi          = {10.1016/j.patcog.2020.107628},
  journal      = {Pattern Recognition},
  pages        = {107628},
  shortjournal = {Pattern Recognition},
  title        = {Robust kernelized graph-based learning},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Structured graph learning for clustering and semi-supervised
classification. <em>PR</em>, <em>110</em>, 107627. (<a
href="https://doi.org/10.1016/j.patcog.2020.107627">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graphs have become increasingly popular in modeling structures and interactions in a wide variety of problems during the last decade. Graph-based clustering and semi-supervised classification techniques have shown impressive performance. This paper proposes a graph learning framework to preserve both the local and global structure of data. Specifically, our method uses the self-expressiveness of samples to capture the global structure and adaptive neighbor approach to respect the local structure. Furthermore, most existing graph-based methods conduct clustering and semi-supervised classification on the graph learned from the original data matrix, which doesn’t have explicit cluster structure, thus they might not achieve the optimal performance. By considering rank constraint, the achieved graph will have exactly c connected components if there are c clusters or classes. As a byproduct of this, graph learning and label inference are jointly and iteratively implemented in a principled way. Theoretically, we show that our model is equivalent to a combination of kernel k -means and k -means methods under certain condition. Extensive experiments on clustering and semi-supervised classification demonstrate that the proposed method outperforms other state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Zhao Kang and Chong Peng and Qiang Cheng and Xinwang Liu and Xi Peng and Zenglin Xu and Ling Tian},
  doi          = {10.1016/j.patcog.2020.107627},
  journal      = {Pattern Recognition},
  pages        = {107627},
  shortjournal = {Pattern Recognition},
  title        = {Structured graph learning for clustering and semi-supervised classification},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A prototype-based SPD matrix network for domain adaptation
EEG emotion recognition. <em>PR</em>, <em>110</em>, 107626. (<a
href="https://doi.org/10.1016/j.patcog.2020.107626">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emotion plays a vital role in human daily life, and EEG signals are widely used in emotion recognition. Due to individual variability, training a generic emotion recognition model across different subjects is difficult. The conventional method involves the collection of a large amount of calibration data to build subject-specific models. Recently, developing an effective brain-computer interface with a short calibration time has become a challenge. To solve this problem, we propose a domain adaptation SPD matrix network (daSPDnet) that can successfully capture an intrinsic emotional representation shared between different subjects. Our method jointly exploits feature adaptation with distribution confusion and sample adaptation with centroid alignment. We compute the SPD matrix based on the covariance as a feature and make a novel attempt to combine prototype learning with the Riemannian metric. Extensive experiments are conducted on the DREAMER and DEAP datasets, and the results show the superiority of our proposed method.},
  archive      = {J_PR},
  author       = {Yixin Wang and Shuang Qiu and Xuelin Ma and Huiguang He},
  doi          = {10.1016/j.patcog.2020.107626},
  journal      = {Pattern Recognition},
  pages        = {107626},
  shortjournal = {Pattern Recognition},
  title        = {A prototype-based SPD matrix network for domain adaptation EEG emotion recognition},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). CNAK: Cluster number assisted k-means. <em>PR</em>,
<em>110</em>, 107625. (<a
href="https://doi.org/10.1016/j.patcog.2020.107625">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The K-means clustering algorithm is well-known for its easy computational approach. In this algorithm, essential cluster-level information is captured by the K cluster centroids . However, how many such centroids can reveal the structure of the underlying data depends upon the choice of K . In this paper, we propose a clustering algorithm in which the number of cluster K can be learned as well as it performs the clustering. Our work revolves around two observations: i) a large-sized random sampled dataset may have a similar distribution as the original data, and ii) for the true number of clusters their centroids, generated from a sampled datasets, approximate the cluster centroids generated from the original dataset. The first observation has paved the way to provide a scalable solution, and the second one forms the key aspect of building the proposed algorithm. We have tested our method on several real and synthetic datasets . Our method can solve a few pertinent issues of clustering a dataset: 1) detection of a single cluster in the absence of any other cluster in a dataset, 2) the presence of hierarchy, 3) clustering of a high dimensional dataset, 4) robustness over dataset having cluster imbalance, and 5) robustness to noise. We have observed significant improvement in speed and quality for predicting cluster numbers as well as the composition of clusters in a large dataset.},
  archive      = {J_PR},
  author       = {Jayasree Saha and Jayanta Mukherjee},
  doi          = {10.1016/j.patcog.2020.107625},
  journal      = {Pattern Recognition},
  pages        = {107625},
  shortjournal = {Pattern Recognition},
  title        = {CNAK: Cluster number assisted K-means},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Cascaded hierarchical atrous spatial pyramid pooling module
for semantic segmentation. <em>PR</em>, <em>110</em>, 107622. (<a
href="https://doi.org/10.1016/j.patcog.2020.107622">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Atrous Spatial Pyramid Pooling (ASPP) is a module that can collect semantic information distributed in different scopes. However, because of the limited number of sampling ranges of ASPP, much valuable global features and contextual information cannot be sufficiently sampled, which degrades the representation ability of the segmentation network . Besides, due to the sparse distribution of the effective sampling points in the atrous convolution kernels of ASPP, large amount of local detail characteristics are easily discarded. To overcome the above two problems, a new Cascaded Hierarchical Atrous Pyramid Pooling (CHASPP) module, consisting of two cascaded components, is proposed. Each component is a hierarchical pyramid pooling structure containing two layers of atrous convolutions with the aim to densify the sampling distribution. On the foundation of such a hierarchical structure, another same structure is appended to form a cascaded module which can further enlarge the diversity of sampling ranges. Based on this cascaded module, not only rich local detail characteristics can be comprehensively presented, but also important global contextual information can be effectively exploited to improve the prediction accuracy. To demonstrate the performance of our CHASPP module, experiments on the benchmarks PASCAL VOC 2012 and Cityscape are conducted.},
  archive      = {J_PR},
  author       = {Xuhang Lian and Yanwei Pang and Jungong Han and Jing Pan},
  doi          = {10.1016/j.patcog.2020.107622},
  journal      = {Pattern Recognition},
  pages        = {107622},
  shortjournal = {Pattern Recognition},
  title        = {Cascaded hierarchical atrous spatial pyramid pooling module for semantic segmentation},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Exploiting textual queries for dynamically visual
disambiguation. <em>PR</em>, <em>110</em>, 107620. (<a
href="https://doi.org/10.1016/j.patcog.2020.107620">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the high cost of manual annotation, learning directly from the web has attracted broad attention. One issue that limits the performance of current webly supervised models is the problem of visual polysemy. In this work, we present a novel framework that resolves visual polysemy by dynamically matching candidate text queries with retrieved images. Specifically, our proposed framework includes three major steps: we first discover and then dynamically select the text queries according to the keyword-based image search results, we employ the proposed saliency-guided deep multi-instance learning (MIL) network to remove outliers and learn classification models for visual disambiguation. Compared to existing methods, our proposed approach can figure out the right visual senses, adapt to dynamic changes in the search results, remove outliers, and jointly learn the classification models . Extensive experiments and ablation studies on CMU-Poly-30 and MIT-ISD datasets demonstrate the effectiveness of our proposed approach.},
  archive      = {J_PR},
  author       = {Zeren Sun and Yazhou Yao and Jimin Xiao and Lei Zhang and Jian Zhang and Zhenmin Tang},
  doi          = {10.1016/j.patcog.2020.107620},
  journal      = {Pattern Recognition},
  pages        = {107620},
  shortjournal = {Pattern Recognition},
  title        = {Exploiting textual queries for dynamically visual disambiguation},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Video super-resolution based on a spatio-temporal matching
network. <em>PR</em>, <em>110</em>, 107619. (<a
href="https://doi.org/10.1016/j.patcog.2020.107619">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep spatio-temporal neural networks have shown promising performance for video super-resolution (VSR) in recent years. However, most of them heavily rely on accuracy motion estimations. In this paper, we propose a novel spatio-temporal matching network (STMN) for video super-resolution, which works on the wavelet domain to reduce dependence on motion estimations. Specifically, our STMN consists of three major components: a temporal fusion wavelet network (TFWN), a non-local matching network (NLMN), and a global wavelet domain residual connection (GWDRC). TFWN adaptively extracts temporal fusion wavelet maps via three 3d convolutional layers and a discrete wavelet transform (DWT) decomposition layer . The extracted temporal fusion wavelet maps are rich in spatial information and knowledge of different frequencies from consecutive frames, which are feed to NLMN for learning deep wavelet representations . NLMN integrates super-resolution and denoising into a unified module by pyramidally stacking non-local matching residual blocks (NLMRB). At last, GWDRC reconstructs the super-resolved frames from the deep wavelet representations by using global wavelet domain residual information. Consequently, our STMN can efficiently enhance reconstruction quality by capturing different frequencies wavelet representations in consecutive frames, and does not require any motion compensation. Extensive experiments conducted on benchmark datasets demonstrate the effectiveness of our method compared with state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Xiaobin Zhu and Zhuangzi Li and Jungang Lou and Qing Shen},
  doi          = {10.1016/j.patcog.2020.107619},
  journal      = {Pattern Recognition},
  pages        = {107619},
  shortjournal = {Pattern Recognition},
  title        = {Video super-resolution based on a spatio-temporal matching network},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Coupled adversarial learning for semi-supervised
heterogeneous face recognition. <em>PR</em>, <em>110</em>, 107618. (<a
href="https://doi.org/10.1016/j.patcog.2020.107618">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visible-near infrared (VIS-NIR) face matching is a challenging issue in heterogeneous face recognition due to the large spectrum domain discrepancy as well as the over-fitting on insufficient pairwise VIS and NIR images during training. This paper proposes a coupled adversarial learning (CAL) approach for the VIS-NIR face matching by performing adversarial learning on both image and feature levels. On the image level, we learn a transformation network from unpaired NIR-VIS images to transform a NIR image to VIS domain . Cycle loss, global intensity loss and local texture loss are employed to better capture the discrepancy between NIR and VIS domains. The synthesized NIR or VIS images can be further used to alleviate the over-fitting problem in a semi-supervised way. On the feature level, we seek a shared feature space in which the heterogeneous face matching problem can be approximately treated as a homogeneous face matching problem. An adversarial loss and an orthogonal constraint are employed to reduce the spectrum domain discrepancy and the over-fitting problem, respectively. Experimental results show that CAL not only synthesizes high-quality VIS or NIR images, but also obtains state-of-the-art recognition results.},
  archive      = {J_PR},
  author       = {Ran He and Yi Li and Xiang Wu and Lingxiao Song and Zhenhua Chai and Xiaolin Wei},
  doi          = {10.1016/j.patcog.2020.107618},
  journal      = {Pattern Recognition},
  pages        = {107618},
  shortjournal = {Pattern Recognition},
  title        = {Coupled adversarial learning for semi-supervised heterogeneous face recognition},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Progressive sample mining and representation learning for
one-shot person re-identification. <em>PR</em>, <em>110</em>, 107614.
(<a href="https://doi.org/10.1016/j.patcog.2020.107614">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we aim to tackle the one-shot person re-identification problem where only one image is labelled for each person, while other images are unlabelled. This task is challenging due to the lack of sufficient labelled training data. To tackle this problem, we propose to iteratively guess pseudo labels for the unlabelled image samples, which are later used to update the re-identification model together with the labelled samples. A new sampling mechanism is designed to select unlabelled samples to pseudo labelled samples based on the distance matrix , and to form a training triplet batch including both labelled samples and pseudo labelled samples. We also design an HSoften-Triplet-Loss to soften the negative impact of the incorrect pseudo label, considering the unreliable nature of pseudo labelled samples. Finally, we deploy an adversarial learning method to expand the image samples to different camera views. Our experiments show that our framework achieves a new state-of-the-art one-shot Re-ID performance on Market-1501 (mAP 42.7\%) and DukeMTMC-Reid dataset (mAP 40.3\%). Code is available on https://github.com/detectiveli/PSMA .},
  archive      = {J_PR},
  author       = {Hui Li and Jimin Xiao and Mingjie Sun and Eng Gee Lim and Yao Zhao},
  doi          = {10.1016/j.patcog.2020.107614},
  journal      = {Pattern Recognition},
  pages        = {107614},
  shortjournal = {Pattern Recognition},
  title        = {Progressive sample mining and representation learning for one-shot person re-identification},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Automatically discriminating and localizing COVID-19 from
community-acquired pneumonia on chest x-rays. <em>PR</em>, <em>110</em>,
107613. (<a href="https://doi.org/10.1016/j.patcog.2020.107613">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The COVID-19 outbreak continues to threaten the health and life of people worldwide. It is an immediate priority to develop and test a computer-aided detection (CAD) scheme based on deep learning (DL) to automatically localize and differentiate COVID-19 from community-acquired pneumonia (CAP) on chest X-rays. Therefore, this study aims to develop and test an efficient and accurate deep learning scheme that assists radiologists in automatically recognizing and localizing COVID-19. A retrospective chest X-ray image dataset was collected from open image data and the Xiangya Hospital, which was divided into a training group and a testing group. The proposed CAD framework is composed of two steps with DLs: the Discrimination-DL and the Localization-DL. The first DL was developed to extract lung features from chest X-ray radiographs for COVID-19 discrimination and trained using 3548 chest X-ray radiographs. The second DL was trained with 406-pixel patches and applied to the recognized X-ray radiographs to localize and assign them into the left lung, right lung or bipulmonary. X-ray radiographs of CAP and healthy controls were enrolled to evaluate the robustness of the model. Compared to the radiologists’ discrimination and localization results, the accuracy of COVID-19 discrimination using the Discrimination-DL yielded 98.71\%, while the accuracy of localization using the Localization-DL was 93.03\%. This work represents the feasibility of using a novel deep learning-based CAD scheme to efficiently and accurately distinguish COVID-19 from CAP and detect localization with high accuracy and agreement with radiologists.},
  archive      = {J_PR},
  author       = {Zheng Wang and Ying Xiao and Yong Li and Jie Zhang and Fanggen Lu and Muzhou Hou and Xiaowei Liu},
  doi          = {10.1016/j.patcog.2020.107613},
  journal      = {Pattern Recognition},
  pages        = {107613},
  shortjournal = {Pattern Recognition},
  title        = {Automatically discriminating and localizing COVID-19 from community-acquired pneumonia on chest X-rays},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Efficient semantic segmentation with pyramidal fusion.
<em>PR</em>, <em>110</em>, 107611. (<a
href="https://doi.org/10.1016/j.patcog.2020.107611">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emergence of large datasets and resilience of convolutional models have enabled successful training of very large semantic segmentation models. However, high capacity implies high computational complexity and therefore hinders real-time operation. We therefore study compact architectures which aim at high accuracy in spite of modest capacity. We propose a novel semantic segmentation approach based on shared pyramidal representation and fusion of heterogeneous features along the upsampling path. The proposed pyramidal fusion approach is especially effective for dense inference in images with large scale variance due to strong regularization effects induced by feature sharing across the resolution pyramid. Interpretation of the decision process suggests that our approach succeeds by acting as a large ensemble of relatively simple models, as well as due to large receptive range and strong gradient flow towards early layers. Our best model achieves 76.4\% mIoU on Cityscapes test and runs in real time on low-power embedded devices.},
  archive      = {J_PR},
  author       = {Marin Oršić and Siniša Šegvić},
  doi          = {10.1016/j.patcog.2020.107611},
  journal      = {Pattern Recognition},
  pages        = {107611},
  shortjournal = {Pattern Recognition},
  title        = {Efficient semantic segmentation with pyramidal fusion},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-scale structural kernel representation for object
detection. <em>PR</em>, <em>110</em>, 107593. (<a
href="https://doi.org/10.1016/j.patcog.2020.107593">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing high-performance object detection methods greatly benefit from the powerful representation ability of deep convolutional neural networks (CNNs). Recent researches show that integration of high-order statistics remarkably improves the representation ability of deep CNNs. However, high-order statistics for object detection lie in two challenges. Firstly, previous methods insert high-order statistics into deep CNNs as global representations, which lose spatial information of inputs, and so are not applicable to object detection. Furthermore, high-order statistics have special structures, which should be considered for proper use of high-order statistics. To overcome above challenges, this paper proposes a Multi-scale Structural Kernel Representation (MSKR) for improving performance of object detection. Our MSKR is developed based on the polynomial kernel approximation , which does not only draw into high-order statistics but also preserve the spatial information of input. To consider geometry structures of high-order representations, a feature power normalization method is introduced before computation of kernel representation. Comparing with the most commonly used first-order statistics in existing CNN-based detectors, our MSKR can generate more discriminative representations, and so be flexibly integrated into deep CNNs for improving performance of object detection. By adopting the proposed MSKR to existing object detection methods (i.e., Faster R-CNN, FPN, Mask R-CNN and RetinaNet), it achieves clear improvement on three widely used benchmarks, while obtaining very competitive performance with state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Hao Wang and Qilong Wang and Peihua Li and Wangmeng Zuo},
  doi          = {10.1016/j.patcog.2020.107593},
  journal      = {Pattern Recognition},
  pages        = {107593},
  shortjournal = {Pattern Recognition},
  title        = {Multi-scale structural kernel representation for object detection},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Probabilistic framework for solving visual dialog.
<em>PR</em>, <em>110</em>, 107586. (<a
href="https://doi.org/10.1016/j.patcog.2020.107586">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a probabilistic framework for solving the task of ‘Visual Dialog’. Solving this task requires reasoning and understanding of visual modality , language modality, and common sense knowledge to answer. Various architectures have been proposed to solve this task by variants of multi-modal deep learning techniques that combine visual and language representations. However, we believe that it is crucial to understand and analyze the sources of uncertainty for solving this task. Our approach allows for estimating uncertainty and also aids a diverse generation of answers. The proposed approach is obtained through a probabilistic representation module that provides us with representations for image, question and conversation history, a module that ensures that diverse latent representations for candidate answers are obtained given the probabilistic representations and an uncertainty representation module that chooses the appropriate answer that minimizes uncertainty. We thoroughly evaluate the model with a detailed ablation analysis, comparison with state of the art and visualization of the uncertainty that aids in the understanding of the method. Using the proposed probabilistic framework , we thus obtain an improved visual dialog system that is also more explainable.},
  archive      = {J_PR},
  author       = {Badri N. Patro and Anupriy and Vinay P. Namboodiri},
  doi          = {10.1016/j.patcog.2020.107586},
  journal      = {Pattern Recognition},
  pages        = {107586},
  shortjournal = {Pattern Recognition},
  title        = {Probabilistic framework for solving visual dialog},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). LOW: Training deep neural networks by learning optimal
sample weights. <em>PR</em>, <em>110</em>, 107585. (<a
href="https://doi.org/10.1016/j.patcog.2020.107585">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The performance of deep learning (DL) models is highly dependent on the quality and size of the training data, whose annotations are often expensive and hard to obtain. This work proposes a new strategy to train DL models by Learning Optimal samples Weights (LOW), making better use of the available data. LOW determines how much each sample in a batch should contribute to the training process, by automatically estimating its weight in the loss function. This effectively forces the model to focus on more relevant samples. Consequently, the models exhibit a faster convergence and better generalization, specially on imbalanced data sets where class distribution is long-tailed. LOW can be easily integrated to train any DL model and can be combined with any loss function, while adding marginal computational burden to the training process. Additionally, the analysis of how sample weights change during training provides insights on what the model is learning and which samples or classes are more challenging. Results on popular computer vision benchmarks and on medical data sets show that DL models trained with LOW perform better than with other state-of-the-art strategies. 1},
  archive      = {J_PR},
  author       = {Carlos Santiago and Catarina Barata and Michele Sasdelli and Gustavo Carneiro and Jacinto C. Nascimento},
  doi          = {10.1016/j.patcog.2020.107585},
  journal      = {Pattern Recognition},
  pages        = {107585},
  shortjournal = {Pattern Recognition},
  title        = {LOW: Training deep neural networks by learning optimal sample weights},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Universal adversarial perturbations against object
detection. <em>PR</em>, <em>110</em>, 107584. (<a
href="https://doi.org/10.1016/j.patcog.2020.107584">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the remarkable success of deep neural networks on many visual tasks, they have been proved to be vulnerable to adversarial examples . For visual tasks, adversarial examples are images added with visually imperceptible perturbations that result in failure for recognition. Previous works have demonstrated that adversarial perturbations can cause neural networks to fail on object detection. But these methods focus on generating an adversarial perturbation for a specific image, which is the image-specific perturbation. This paper tries to extend such image-level adversarial perturbations to detector-level, which are universal (image-agnostic) adversarial perturbations. Motivated by this, we propose a Universal Dense Object Suppression (U-DOS) algorithm to derive the universal adversarial perturbations against object detection and show that such perturbations with visual imperceptibility can lead the state-of-the-art detectors to fail in finding any objects in most images. Compared to image-specific perturbations, the results of image-agnostic perturbations are more interesting and also pose more challenges in AI security, because they are more convenient to be applied in the real physical world. We also analyze the generalization of such universal adversarial perturbations across different detectors and datasets under the black-box attack settings, showing it’s a simple but promising adversarial attack approach against object detection. Furthermore, we validate the class-specific universal perturbations, which can remove the detection results of the target class and keep others unchanged.},
  archive      = {J_PR},
  author       = {Debang Li and Junge Zhang and Kaiqi Huang},
  doi          = {10.1016/j.patcog.2020.107584},
  journal      = {Pattern Recognition},
  pages        = {107584},
  shortjournal = {Pattern Recognition},
  title        = {Universal adversarial perturbations against object detection},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). AGUnet: Annotation-guided u-net for fast one-shot video
object segmentation. <em>PR</em>, <em>110</em>, 107580. (<a
href="https://doi.org/10.1016/j.patcog.2020.107580">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of semi-supervised video object segmentation has been popularly tackled by fine-tuning a general-purpose segmentation deep network on the annotated frame using hundreds of iterations of gradient descent . The time-consuming fine-tuning process, however, makes these methods difficult to use in practical applications. We propose a novel architecture called Annotation Guided U-net (AGUnet) for fast one-shot video object segmentation (VOS). AGUnet can quickly adapt a model trained on static images to segmenting the given target in a video by only several iterations of gradient descent . Our AGUnet is inspired by interactive image segmentation , where the interested target is segmented by using user annotated foreground. However, in AGUnet we use a fully-convolutional Siamese network to automatically annotate the foreground and background regions and fuse such annotation information into the skip connection of a U-net for VOS. Our AGUnet can be trained end-to-end effectively on static images instead of video sequences as required by many previous methods. The experiments show that AGUnet runs much faster than current state-of-the-art one-shot VOS algorithms while achieving competitive accuracy, and it has high generalization capability.},
  archive      = {J_PR},
  author       = {Yingjie Yin and De Xu and Xingang Wang and Lei Zhang},
  doi          = {10.1016/j.patcog.2020.107580},
  journal      = {Pattern Recognition},
  pages        = {107580},
  shortjournal = {Pattern Recognition},
  title        = {AGUnet: Annotation-guided U-net for fast one-shot video object segmentation},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Unsupervised text-to-image synthesis. <em>PR</em>,
<em>110</em>, 107573. (<a
href="https://doi.org/10.1016/j.patcog.2020.107573">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, text-to-image synthesis has achieved great progresses with the advancement of the Generative Adversarial Network (GAN). However, training the GAN models requires a large amount of pairwise image-text data, which is extremely labor-intensive to collect. In this paper, we make the first attempt to train a text-to-image synthesis model in an unsupervised manner, which does not require any human labeled image-text pair data. Specifically, we first rely on the visual concepts to bridge two independent image and sentence sets and thereby yield the pseudo image-text pair data, based on which one GAN model can thereby be initialized. One novel visual concept discrimination loss is proposed to train both generator and discriminator, which not only encourages the image expressing the true local visual concepts but also ensures the noisy visual concepts contained in the pseudo sentence being suppressed. Afterwards, one global semantic consistency regarding to the real sentence is used to adapt the pretrained GAN model to real sentences. Experimental results demonstrate that our proposed unsupervised training strategy is able to generate favorable images for given sentences, which even outperforms some existing models trained in the supervised manner. The code of this paper is available at https://github.com/dylls/Unsupervised_Text-to-Image_Synthesis .},
  archive      = {J_PR},
  author       = {Yanlong Dong and Ying Zhang and Lin Ma and Zhi Wang and Jiebo Luo},
  doi          = {10.1016/j.patcog.2020.107573},
  journal      = {Pattern Recognition},
  pages        = {107573},
  shortjournal = {Pattern Recognition},
  title        = {Unsupervised text-to-image synthesis},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Cross-modality deep feature learning for brain tumor
segmentation. <em>PR</em>, <em>110</em>, 107562. (<a
href="https://doi.org/10.1016/j.patcog.2020.107562">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in machine learning and prevalence of digital medical images have opened up an opportunity to address the challenging brain tumor segmentation (BTS) task by using deep convolutional neural networks . However, different from the RGB image data that are very widespread, the medical image data used in brain tumor segmentation are relatively scarce in terms of the data scale but contain the richer information in terms of the modality property. To this end, this paper proposes a novel cross-modality deep feature learning framework to segment brain tumors from the multi-modality MRI data. The core idea is to mine rich patterns across the multi-modality data to make up for the insufficient data scale. The proposed cross-modality deep feature learning framework consists of two learning processes: the cross-modality feature transition (CMFT) process and the cross-modality feature fusion (CMFF) process, which aims at learning rich feature representations by transiting knowledge across different modality data and fusing knowledge from different modality data, respectively. Comprehensive experiments are conducted on the BraTS benchmarks, which show that the proposed cross-modality deep feature learning framework can effectively improve the brain tumor segmentation performance when compared with the baseline methods and state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Dingwen Zhang and Guohai Huang and Qiang Zhang and Jungong Han and Junwei Han and Yizhou Yu},
  doi          = {10.1016/j.patcog.2020.107562},
  journal      = {Pattern Recognition},
  pages        = {107562},
  shortjournal = {Pattern Recognition},
  title        = {Cross-modality deep feature learning for brain tumor segmentation},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multiple attentional pyramid networks for chinese herbal
recognition. <em>PR</em>, <em>110</em>, 107558. (<a
href="https://doi.org/10.1016/j.patcog.2020.107558">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Chinese herbs play a critical role in Traditional Chinese Medicine. Due to different recognition granularity , they can be recognized accurately only by professionals with much experience. It is expected that they can be recognized automatically using new techniques like machine learning . However, there is no Chinese herbal image dataset available. Simultaneously, there is no machine learning method which can deal with Chinese herbal image recognition well. Therefore, this paper begins with building a new standard Chinese-Herbs dataset. Subsequently, a new Attentional Pyramid Networks (APN) for Chinese herbal recognition is proposed, where both novel competitive attention and spatial collaborative attention are proposed and then applied. APN can adaptively model Chinese herbal images with different feature scales. Finally, a new framework for Chinese herbal recognition is proposed as a new application of APN. Experiments are conducted on our constructed dataset and validate the effectiveness of our methods.},
  archive      = {J_PR},
  author       = {Yingxue Xu and Guihua Wen and Yang Hu and Mingnan Luo and Dan Dai and Yishan Zhuang and Wendy Hall},
  doi          = {10.1016/j.patcog.2020.107558},
  journal      = {Pattern Recognition},
  pages        = {107558},
  shortjournal = {Pattern Recognition},
  title        = {Multiple attentional pyramid networks for chinese herbal recognition},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-scale deep relational reasoning for facial kinship
verification. <em>PR</em>, <em>110</em>, 107541. (<a
href="https://doi.org/10.1016/j.patcog.2020.107541">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a deep relational network which exploits multi-scale information of facial images for kinship verification. Unlike most existing deep learning based facial kinship verification methods which employ convolutional neural networks to extract holistic features, we present a deep model to exploit facial kinship relationship from local regions. For each given pair of face images, our method uses two convolutional neural networks which share parameters to extract different scales of features, which are expected to provide global contextual information of face images. We split a set of features at the same scale into multiple groups, where different groups capture information of different local regions. For each pair of local feature groups which are extracted from the same scale and position, we propose a relation network to reason their relationship, and use a verification network to infer the kin relation based on the results of local relations from different facial regions. We conduct experiments on two widely used facial kinship datasets: KinFaceW-I and KinFaceW-II, and our experimental results are presented to demonstrate the effectiveness of our approach.},
  archive      = {J_PR},
  author       = {Haibin Yan and Chaohui Song},
  doi          = {10.1016/j.patcog.2020.107541},
  journal      = {Pattern Recognition},
  pages        = {107541},
  shortjournal = {Pattern Recognition},
  title        = {Multi-scale deep relational reasoning for facial kinship verification},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Challenging tough samples in unsupervised domain adaptation.
<em>PR</em>, <em>110</em>, 107540. (<a
href="https://doi.org/10.1016/j.patcog.2020.107540">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing domain adaptation approaches focus on taking advantage of easy samples, i.e, target samples which are easier for adaptation. In previous work, tough, or hard, target samples are generally regarded as outliers or just being left to chance. As a result, the adaptation of tough target samples remains as a challenging problem in the community. In this paper, we report three novel ideas for domain adaptation: 1) splitting target samples into easy and tough ones; 2) deploying different strategies for samples with different adaptation difficulties; 3) leveraging easy samples to facilitate tough ones. Furthermore, we present a novel approach, named challenging tough sample networks (CTSN) , to practice the three ideas and tame tough samples. Specifically, in our approach, a CNN with domain adaptation layers is first used to rapidly handle the easy samples and identify the tough ones. Then, a GAN with two classifiers is tailored to adapt the tough samples. The GAN leverages classification discrepancy and easy samples to tame the tough ones. Extensive experiments on both classic and large-scale benchmarks verify that both easy and tough samples do exist in real-world datasets and our approach is able to handle them.},
  archive      = {J_PR},
  author       = {Lin Zuo and Mengmeng Jing and Jingjing Li and Lei Zhu and Ke Lu and Yang Yang},
  doi          = {10.1016/j.patcog.2020.107540},
  journal      = {Pattern Recognition},
  pages        = {107540},
  shortjournal = {Pattern Recognition},
  title        = {Challenging tough samples in unsupervised domain adaptation},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Constructing multilayer locality-constrained matrix
regression framework for noise robust face super-resolution.
<em>PR</em>, <em>110</em>, 107539. (<a
href="https://doi.org/10.1016/j.patcog.2020.107539">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Representation learning methods have attracted considerable attention for learning-based face super-resolution in recent years. Conventional methods perform local models learning on low-resolution (LR) manifold and face reconstruction on high-resolution (HR) manifold respectively, leading to unsatisfactory reconstruction performance when the acquired LR face images are severely degraded (e.g., noisy, blurred). To tackle this issue, this paper proposes an efficient multilayer locality-constrained matrix regression (MLCMR) framework to learn the representation of the input LR patch and meanwhile preserve the manifold of the original HR space. Particularly, MLCMR uses nuclear norm regularization to capture the structural characteristic of the representation residual and applies an adaptive neighborhood selection scheme to find the HR patches that are compatible with its neighbors. Also, MLCMR iteratively applies the manifold structure of the desired HR space to induce the representation weights learning in the LR space, aims at reducing the inconsistency gap between different manifolds. Experimental results on widely used FEI database and real-world faces have demonstrated that compared with several state-of-the-art face super-resolution approaches, our proposed approach has the capability of obtaining better results both in objective metrics and visual quality.},
  archive      = {J_PR},
  author       = {Guangwei Gao and Yi Yu and Jin Xie and Jian Yang and Meng Yang and Jian Zhang},
  doi          = {10.1016/j.patcog.2020.107539},
  journal      = {Pattern Recognition},
  pages        = {107539},
  shortjournal = {Pattern Recognition},
  title        = {Constructing multilayer locality-constrained matrix regression framework for noise robust face super-resolution},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DecomVQANet: Decomposing visual question answering deep
network via tensor decomposition and regression. <em>PR</em>,
<em>110</em>, 107538. (<a
href="https://doi.org/10.1016/j.patcog.2020.107538">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The model we developed is a novel comprehensive solution to compress and accelerate the Visual Question Answering systems . In our algorithm Convolutional Neural Network is compressed with Long Short Term Memory to accelerate processing simultaneously. We propose to conduct various decomposition methods and regression strategies on different layers, including Canonical Polyadic, Tucker, and Tensor Train to decompose Fully Connected layers in CNN and LSTM. The Flattening Layer and Fully Connected layer at the end of the model are replaced with Tensor Regression layers. In order to compress the parameter further, the feature flow between the layers is compressed by Tensor Contraction layer. The proposed tensor decomposition model was evaluated on VQA 2.0 dataset with Pythia as baseline model . Our proposed model achieved from 77\% to 91\% of compression ratio, and only from 1\% to 5\% accuracy drop.},
  archive      = {J_PR},
  author       = {Zongwen Bai and Ying Li and Marcin Woźniak and Meili Zhou and Di Li},
  doi          = {10.1016/j.patcog.2020.107538},
  journal      = {Pattern Recognition},
  pages        = {107538},
  shortjournal = {Pattern Recognition},
  title        = {DecomVQANet: Decomposing visual question answering deep network via tensor decomposition and regression},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep multi-task learning with relational attention for
business success prediction. <em>PR</em>, <em>110</em>, 107469. (<a
href="https://doi.org/10.1016/j.patcog.2020.107469">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-task learning is a promising machine learning branch, which aims to improve the generalization of the prediction models by sharing knowledge among tasks. Most of the existing multi-task learning methods rely on predefined task relationships and guide the learning process of models by linear regularization terms. On the one hand, improper setting of task relationships may result in negative knowledge transfer; on the other hand, these methods also suffer from the insufficiency of representation ability. To overcome these problems, this paper focuses on attention-based deep multi-task learning method, and provides a novel deep multi-task learning method, namely, Deep Multi-task Learning with Relational Attention (DMLRA). In particular, we first provide a task-specific attention module to specify features for different learning tasks, because different prediction tasks may rely on different parts of the shared feature set. Then, we design a relational attention module to learn relationships among multiple tasks automatically, and transfer positive and negative knowledge among multiple tasks accordingly. Moreover, we provide a joint deep multi-task learning framework to combine task-specific module and relational attention module. Finally, we apply our method on a multi-criteria business success assessment problem, both classical and the state-of-the-art multi-task learning methods are employed to provide baseline performance. The experiments are conducted on real-world datasets, results demonstrate the superiority of our method over the existing methods.},
  archive      = {J_PR},
  author       = {Jiejie Zhao and Bowen Du and Leilei Sun and Weifeng Lv and Yanchi Liu and Hui Xiong},
  doi          = {10.1016/j.patcog.2020.107469},
  journal      = {Pattern Recognition},
  pages        = {107469},
  shortjournal = {Pattern Recognition},
  title        = {Deep multi-task learning with relational attention for business success prediction},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Goal driven network pruning for object recognition.
<em>PR</em>, <em>110</em>, 107468. (<a
href="https://doi.org/10.1016/j.patcog.2020.107468">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pruning studies up to date focused on uncovering a smaller network by removing redundant units, and fine-tuning to compensate accuracy drop as a result. In this study, unlike the others, we propose an approach to uncover a smaller network that is competent only in a specific task, similar to top-down attention mechanism in human visual system . This approach doesn’t require fine-tuning and is proposed as a fast and effective alternative of training from scratch when the network focuses on a specific task in the dataset. Pruning starts from the output and proceeds towards the input by computing neuron importance scores in each layer and propagating them to the preceding layer. In the meantime, neurons determined as worthless are pruned. We applied our approach on three benchmark datasets: MNIST, CIFAR-10 and ImageNet. The results demonstrate that the proposed pruning method typically reduces computational units and storage without harming accuracy significantly.},
  archive      = {J_PR},
  author       = {Cagri Kaplan and Abdullah Bulbul},
  doi          = {10.1016/j.patcog.2020.107468},
  journal      = {Pattern Recognition},
  pages        = {107468},
  shortjournal = {Pattern Recognition},
  title        = {Goal driven network pruning for object recognition},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Explaining the semantics capturing capability of scene graph
generation models. <em>PR</em>, <em>110</em>, 107427. (<a
href="https://doi.org/10.1016/j.patcog.2020.107427">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural network is a effective way for scene graph generation tasks. However, it also makes the scene graph generation models difficult to explain. For instance, the current standard metric cannot explain how capable neural network models are of capturing the semantics of relations. In this paper, we try to understand the semantics capturing capability of scene graph generation models based on three types of metrics: conformance recall, violation recall, and non-violation recall, which measure semantic properties of relations that are reflected by triples in scene graph generated by models. Evaluation of these metrics on three representative state-of-the-art scene graph generation models based on deep neural network in Visual Genome dataset shows that the proposed metrics can effectively explain the capability of models to capture different semantic properties and identify design problems in models. By extending the Visual Genome dataset with different sets of additional annotations, these metrics can also explaining whether the semantics capturing capability of deep neural network models can be improved by data enhancement.},
  archive      = {J_PR},
  author       = {Jie Luo and Jia Zhao and Bin Wen and Yuhang Zhang},
  doi          = {10.1016/j.patcog.2020.107427},
  journal      = {Pattern Recognition},
  pages        = {107427},
  shortjournal = {Pattern Recognition},
  title        = {Explaining the semantics capturing capability of scene graph generation models},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep features for person re-identification on metric
learning. <em>PR</em>, <em>110</em>, 107424. (<a
href="https://doi.org/10.1016/j.patcog.2020.107424">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person re-identification, a branch of image retrieval , is an increasingly important public safety application. When monitoring larger areas, it is crucial to correctly match the same person in different camera views. With the emergence of deep learning and large-scale data, metric learning has significantly improved person re-identification performance, but the extent to which deep features affect metric learning performance is unknown. However, given the large number of approaches, datasets, evaluation indices, and experimental environments, comparing metric learning methods directly is difficult. To obtain a more comprehensive empirical evaluation of the person re-identification, here we summarize the different types of features and metric learning approaches from a label attributes perspective. Then, by combining advanced approaches to data enhancement and feature extraction, we conduct comprehensive experiments on metric learning methods with two datasets. For fairness, all methods use a unified code library that includes two data enhancement schemes, eight feature extraction algorithms, and eight metric learning methods. Our results show that, the relations of loss function with deep feature space and metric learning.},
  archive      = {J_PR},
  author       = {Wanyin Wu and Dapeng Tao and Hao Li and Zhao Yang and Jun Cheng},
  doi          = {10.1016/j.patcog.2020.107424},
  journal      = {Pattern Recognition},
  pages        = {107424},
  shortjournal = {Pattern Recognition},
  title        = {Deep features for person re-identification on metric learning},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Explainable skin lesion diagnosis using taxonomies.
<em>PR</em>, <em>110</em>, 107413. (<a
href="https://doi.org/10.1016/j.patcog.2020.107413">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks have rapidly become an indispensable tool in many classification applications. However, the inclusion of deep learning methods in medical diagnostic systems has come at the cost of diminishing their explainability. This significantly reduces the safety of a diagnostic system, since the physician is unable to interpret and validate the output. Therefore, in this work we aim to address this major limitation and improve the explainability of a skin cancer diagnostic system. We propose to leverage two sources of information: (i) medical knowledge, in particular the taxonomic organization of skin lesions, which will be used to develop a hierarchical neural network ; and (ii) recent advances in channel and spatial attention modules, which can identify interpretable features and regions in dermoscopy images. We demonstrate that the proposed approach achieves competitive results in two dermoscopy data sets (ISIC 2017 and 2018) and provides insightful information about its decisions, thus increasing the safety of the model.},
  archive      = {J_PR},
  author       = {Catarina Barata and M. Emre Celebi and Jorge S. Marques},
  doi          = {10.1016/j.patcog.2020.107413},
  journal      = {Pattern Recognition},
  pages        = {107413},
  shortjournal = {Pattern Recognition},
  title        = {Explainable skin lesion diagnosis using taxonomies},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Lightweight dynamic conditional GAN with pyramid attention
for text-to-image synthesis. <em>PR</em>, <em>110</em>, 107384. (<a
href="https://doi.org/10.1016/j.patcog.2020.107384">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The text-to-image synthesis task aims to generate photographic images conditioned on semantic text descriptions. To ensure the sharpness and fidelity of generated images, this task tends to generate high-resolution images (e.g., 128 2 or 256 2 ). However, as the resolution increases, the network parameters and complexity increases dramatically. Recent works introduce network structures with extensive parameters and heavy computations to guarantee the production of high-resolution images. As a result, these models come across problems of the unstable training process and high training cost. To tackle these issues, in this paper, we propose an effective information compensation based approach, namely Lightweight Dynamic Conditional GAN (LD-CGAN). LD-CGAN is a compact and structured single-stream network, and it consists of one generator and two independent discriminators to regularize and generate 64 2 and 128 2 images in one feed-forward process. Specifically, the generator of LD-CGAN is composed of three major components: (1) Conditional Embedding (CE), which is an automatically unsupervised learning process aiming at disentangling integrated semantic attributes in the text space; (2) Conditional Manipulating Modular (CM-M) in Conditional Manipulating Block (CM-B), which is designed to continuously provide the image features with the compensation information (i.e., the disentangled attribute); and (3) Pyramid Attention Refine Block (PAR-B), which is used to enrich multi-scale features by capturing spatial importance between multi-scale context. Consequently, experiments conducted under two benchmark datasets, CUB and Oxford-102, indicate that our generated 128 2 images can achieve comparable performance with 256 2 images generated by the state-of-the-arts on two evaluation metrics : Inception Score (IS) and Visual-semantic Similarity (VS). Compared with the current state-of-the-art HDGAN, our LD-CGAN significantly decreases the number of parameters and computation time by 86.8\% and 94.9\%, respectively.},
  archive      = {J_PR},
  author       = {Lianli Gao and Daiyuan Chen and Zhou Zhao and Jie Shao and Heng Tao Shen},
  doi          = {10.1016/j.patcog.2020.107384},
  journal      = {Pattern Recognition},
  pages        = {107384},
  shortjournal = {Pattern Recognition},
  title        = {Lightweight dynamic conditional GAN with pyramid attention for text-to-image synthesis},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Towards non-i.i.d. Image classification: A dataset and
baselines. <em>PR</em>, <em>110</em>, 107383. (<a
href="https://doi.org/10.1016/j.patcog.2020.107383">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {I.I.D. 2 hypothesis between training and testing data is the basis of numerous image classification methods. Such property can hardly be guaranteed in practice where the Non-IIDness is common, causing instable performances of these models. In literature, however, the Non-I.I.D. 3 image classification problem is largely understudied. A key reason is lacking of a well-designed dataset to support related research. In this paper, we construct and release a Non-I.I.D. image dataset called NICO 4 , which uses contexts to create Non-IIDness consciously. Compared to other datasets, extended analyses prove NICO can support various Non-I.I.D. situations with sufficient flexibility. Meanwhile, we propose a baseline model with ConvNet structure for General Non-I.I.D. image classification, where distribution of testing data is unknown but different from training data. The experimental results demonstrate that NICO can well support the training of ConvNet model from scratch, and a batch balancing module can help ConvNets to perform better in Non-I.I.D. settings.},
  archive      = {J_PR},
  author       = {Yue He and Zheyan Shen and Peng Cui},
  doi          = {10.1016/j.patcog.2020.107383},
  journal      = {Pattern Recognition},
  pages        = {107383},
  shortjournal = {Pattern Recognition},
  title        = {Towards non-I.I.D. image classification: A dataset and baselines},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). TextMountain: Accurate scene text detection via instance
segmentation. <em>PR</em>, <em>110</em>, 107336. (<a
href="https://doi.org/10.1016/j.patcog.2020.107336">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel scene text detection method named TextMountain. The key idea of TextMountain is making full use of border-center information. Different from previous works that treat center-border as a binary classification problem, we predict text center-border probability (TCBP) and text center-direction (TCD). The TCBP is just like a mountain whose top is text center and foot is text border. The mountaintop can separate text instances which cannot be easily achieved using semantic segmentation map and its rising direction can plan a road to top for each pixel on mountain foot at the group stage. The TCD helps TCBP learning better. Our label rules will not lead to the ambiguous problem with the transformation of angle, so the proposed method is robust to multi-oriented text and can also handle well curved text. In inference stage, each pixel at the mountain foot needs to search the path to the mountaintop and this process can be efficiently completed in parallel, yielding the efficiency of our method compared with others. The experiments on MLT, ICDAR2015, RCTW-17 and SCUT-CTW1500 datasets demonstrate that the proposed method achieves better or comparable performance in terms of both accuracy and efficiency. It is worth mentioning our method achieves an F-measure of 76.85\% on MLT which outperforms the previous methods by a large margin. Code will be made available.},
  archive      = {J_PR},
  author       = {Yixing Zhu and Jun Du},
  doi          = {10.1016/j.patcog.2020.107336},
  journal      = {Pattern Recognition},
  pages        = {107336},
  shortjournal = {Pattern Recognition},
  title        = {TextMountain: Accurate scene text detection via instance segmentation},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Understanding adversarial attacks on deep learning based
medical image analysis systems. <em>PR</em>, <em>110</em>, 107332. (<a
href="https://doi.org/10.1016/j.patcog.2020.107332">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks (DNNs) have become popular for medical image analysis tasks like cancer diagnosis and lesion detection. However, a recent study demonstrates that medical deep learning systems can be compromised by carefully-engineered adversarial examples/attacks with small imperceptible perturbations. This raises safety concerns about the deployment of these systems in clinical settings. In this paper, we provide a deeper understanding of adversarial examples in the context of medical images. We find that medical DNN models can be more vulnerable to adversarial attacks compared to models for natural images, according to two different viewpoints. Surprisingly, we also find that medical adversarial attacks can be easily detected, i.e., simple detectors can achieve over 98\% detection AUC against state-of-the-art attacks, due to fundamental feature differences compared to normal examples. We believe these findings may be a useful basis to approach the design of more explainable and secure medical deep learning systems.},
  archive      = {J_PR},
  author       = {Xingjun Ma and Yuhao Niu and Lin Gu and Yisen Wang and Yitian Zhao and James Bailey and Feng Lu},
  doi          = {10.1016/j.patcog.2020.107332},
  journal      = {Pattern Recognition},
  pages        = {107332},
  shortjournal = {Pattern Recognition},
  title        = {Understanding adversarial attacks on deep learning based medical image analysis systems},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). BLOCK-DBSCAN: Fast clustering for large scale data.
<em>PR</em>, <em>109</em>, 107624. (<a
href="https://doi.org/10.1016/j.patcog.2020.107624">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We analyze the drawbacks of DBSCAN and its variants, and find the grid technique, which is used in Fast-DBSCAN and ρ -approximate DBSCAN, is almost useless in high dimensional data space. Because it usually yields considerable redundant distance computations. In order to tame these problems, two techniques are proposed: one is to use ϵ 2 ϵ2 -norm ball to identify Inner Core Blocks within which all points are core points, it has higher efficiency than grid technique for finding more core points at one time; the other is a fast approximate algorithm for judging whether two Inner Core Blocks are density-reachable from each other. Besides, cover tree is also used to accelerate the process of density computations. Based on the three techniques, an approximate approach, namely BLOCK-DBSCAN, is proposed for large scale data, which runs in about O ( n log ( n )) expected time and obtains almost the same result as DBSCAN. BLOCK-DBSCAN has two versions, i.e., L 2 version can work well for relatively high dimensional data , and L ∞ version is suitable for high dimensional data. Experimental results show that BLOCK-DBSCAN is promising and outperforms NQDBSCAN, ρ -approximate DBSCAN and AnyDBC.},
  archive      = {J_PR},
  author       = {Yewang Chen and Lida Zhou and Nizar Bouguila and Cheng Wang and Yi Chen and Jixiang Du},
  doi          = {10.1016/j.patcog.2020.107624},
  journal      = {Pattern Recognition},
  pages        = {107624},
  shortjournal = {Pattern Recognition},
  title        = {BLOCK-DBSCAN: Fast clustering for large scale data},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A theoretical justification of warping generation for
dewarping using CNN. <em>PR</em>, <em>109</em>, 107621. (<a
href="https://doi.org/10.1016/j.patcog.2020.107621">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dewarping is a necessary preprocessing step to recognize text from a distorted camera captured document image . According to recent literature, deep learning-based approaches perform with higher accuracy in similar domains. The deep learning-based neural networks are not yet fully explored in the domain of dewarping. To fill this gap, we propose a dewarping approach based on the convolutional neural network . A large number of images are required to train such networks. However, it is a tedious job to capture such a large number of images. Hence, it is required to generate synthetic warped images for the training phase of the deep learning-based neural network. The existing synthetic warped image generation methods are heuristic-based. In this paper, we propose a novel mathematical model for the generation of warped images. The proposed model takes some parameters such as depth of the surface, camera angle, and camera position and generates the corresponding warped image. These parameters are the ground truth for that particular warped image. We use a Convolutional Neural Network (CNN) based model to estimate the warping parameters from a 2 D warped image for dewarping. In the training phase of CNN based model, the synthetic images and their corresponding ground truth are used. Next, the trained model is used to dewarp the unknown warped images. The performance of the proposed warping model is analyzed. Finally, the proposed dewarping method is compared with existing approaches. In both cases, the results are encouraging.},
  archive      = {J_PR},
  author       = {Arpan Garai and Samit Biswas and Sekhar Mandal},
  doi          = {10.1016/j.patcog.2020.107621},
  journal      = {Pattern Recognition},
  pages        = {107621},
  shortjournal = {Pattern Recognition},
  title        = {A theoretical justification of warping generation for dewarping using CNN},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Implementing transfer learning across different datasets for
time series forecasting. <em>PR</em>, <em>109</em>, 107617. (<a
href="https://doi.org/10.1016/j.patcog.2020.107617">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the extensive practical value of time series prediction, many excellent algorithms have been proposed. Most of these methods are developed assuming that massive labeled training data are available. However, this assumption might be invalid in some actual situations. To address this limitation, a transfer learning framework with deep architectures is proposed. Since convolutional neural network (CNN) owns favorable feature extraction capability and can implement parallelization more easily, we propose a deep transfer learning method resorting to the architecture of CNN , termed as DTr-CNN for short. It can effectively alleviate the available labeled data absence and leverage useful knowledge to the current prediction. Notably, in our method, transfer learning process is implemented across different datasets. For a given target domain, in real-world scenarios, relativity of truly available potential source datasets may not be obvious, which is challenging and rarely referred to in most existing time series prediction methods. Aiming at this problem, the incorporation of Dynamic Time Warping (DTW) and Jensen-Shannon (JS) divergence is adopted for the selection of the appropriate source domain. Effectiveness of the proposed method is empirically underpinned by the experiments conducted on one group of synthetic and two groups of practical datasets. Besides, an additional experiment on NN5 dataset is conducted.},
  archive      = {J_PR},
  author       = {Rui Ye and Qun Dai},
  doi          = {10.1016/j.patcog.2020.107617},
  journal      = {Pattern Recognition},
  pages        = {107617},
  shortjournal = {Pattern Recognition},
  title        = {Implementing transfer learning across different datasets for time series forecasting},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Towards using count-level weak supervision for crowd
counting. <em>PR</em>, <em>109</em>, 107616. (<a
href="https://doi.org/10.1016/j.patcog.2020.107616">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing crowd counting methods require object location-level annotation which is labor-intensive and time-consuming to obtain. In contrast, weaker annotations that only label the total count of objects can be easy to obtain in many practical scenarios. This paper focuses on the problem of weakly-supervised crowd counting which learns a model from a small amount of location-level annotations (fully-supervised) and a large amount of count-level annotations (weakly-supervised). Our study reveals that the most straightforward, that is, directly regressing the integral of density map to the object count, fails to provide satisfactory performance. As an alternative solution, we propose a method by taking advantage of the fact that the total count can be estimated via different-but-equivalent density maps. Our key idea is to enforce the consistency between those density maps and total object count on weakly labeled images as regularization terms. We realize this idea by using multiple density map estimation branches and a carefully devised asymmetry training strategy, called Multiple Auxiliary Tasks Training (MATT). Through extensive experiments on existing datasets and a newly proposed dataset, we validate the effectiveness of the proposed weakly-supervised method and demonstrate its superior performance over existing solutions.},
  archive      = {J_PR},
  author       = {Yinjie Lei and Yan Liu and Pingping Zhang and Lingqiao Liu},
  doi          = {10.1016/j.patcog.2020.107616},
  journal      = {Pattern Recognition},
  pages        = {107616},
  shortjournal = {Pattern Recognition},
  title        = {Towards using count-level weak supervision for crowd counting},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Video saliency prediction using enhanced spatiotemporal
alignment network. <em>PR</em>, <em>109</em>, 107615. (<a
href="https://doi.org/10.1016/j.patcog.2020.107615">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to a variety of motions across different frames, it is highly challenging to learn an effective spatiotemporal representation for accurate video saliency prediction (VSP). To address this issue, we develop an effective spatiotemporal feature alignment network tailored to VSP, mainly including two key sub-networks: a multi-scale deformable convolutional alignment network (MDAN) and a bidirectional convolutional Long Short-Term Memory (Bi-ConvLSTM) network. The MDAN learns to align the features of the neighboring frames to the reference one in a coarse-to-fine manner, which can well handle various motions. Specifically, the MDAN owns a pyramidal feature hierarchy structure that first leverages deformable convolution (Dconv) to align the lower-resolution features across frames, and then aggregates the aligned features to align the higher-resolution features, progressively enhancing the features from top to bottom. The output of MDAN is then fed into the Bi-ConvLSTM for further enhancement, which captures the useful long-time temporal information along forward and backward timing directions to effectively guide attention orientation shift prediction under complex scene transformation. Finally, the enhanced features are decoded to generate the predicted saliency map . The proposed model is trained end-to-end without any intricate post processing. Extensive evaluations on four VSP benchmark datasets demonstrate that the proposed method achieves favorable performance against state-of-the-art methods. The source codes and all the results will be released at https://github.com/cj4L/ESAN-VSP .},
  archive      = {J_PR},
  author       = {Jin Chen and Huihui Song and Kaihua Zhang and Bo Liu and Qingshan Liu},
  doi          = {10.1016/j.patcog.2020.107615},
  journal      = {Pattern Recognition},
  pages        = {107615},
  shortjournal = {Pattern Recognition},
  title        = {Video saliency prediction using enhanced spatiotemporal alignment network},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Semi-automatic data annotation guided by feature space
projection. <em>PR</em>, <em>109</em>, 107612. (<a
href="https://doi.org/10.1016/j.patcog.2020.107612">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data annotation using visual inspection (supervision) of each training sample can be laborious. Interactive solutions alleviate this by helping experts propagate labels from a few supervised samples to unlabeled ones based solely on the visual analysis of their feature space projection (with no further sample supervision). We present a semi-automatic data annotation approach based on suitable feature space projection and semi-supervised label estimation. We validate our method on the popular MNIST dataset and on images of human intestinal parasites with and without fecal impurities, a large and diverse dataset that makes classification very hard. We evaluate two approaches for semi-supervised learning from the latent and projection spaces, to choose the one that best reduces user annotation effort and also increases classification accuracy on unseen data. Our results demonstrate the added-value of visual analytics tools that combine complementary abilities of humans and machines for more effective machine learning .},
  archive      = {J_PR},
  author       = {Bárbara C. Benato and Jancarlo F. Gomes and Alexandru C. Telea and Alexandre X. Falcão},
  doi          = {10.1016/j.patcog.2020.107612},
  journal      = {Pattern Recognition},
  pages        = {107612},
  shortjournal = {Pattern Recognition},
  title        = {Semi-automatic data annotation guided by feature space projection},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Efficient densely connected convolutional neural networks.
<em>PR</em>, <em>109</em>, 107610. (<a
href="https://doi.org/10.1016/j.patcog.2020.107610">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent works have shown that convolutional neural networks (CNNs) are parameter redundant, which limits the application of CNNs in Mobile devices with limited memory and computational resources. In this paper, two novel and efficient lightweight CNNs architectures are proposed, which are called DenseDsc and Dense2Net. Two proposed CNNs are densely connected and the dense connectivity facilitates feature re-use in the networks. Dense2Net adopts efficient group convolution and DenseDsc adopts more efficient depthwise separable convolution. The novel dense blocks of DenseDsc and Dense2Net improve the parameter efficiency. The proposed DenseDsc and Dense2Net are evaluated on highly competitive classification benchmark datasets (CIFAR and ImageNet). The experimental results show that DenseDsc and Dense2Net have higher accuracy than DenseNet with similar parameters or FLOPs. Compared with other efficient CNNs with less than 0.5 M parameters for CIFAR, Dense2Net and DenseDsc achieve state-of-the-art results on CIFAR-10 and CIFAR-100, respectively. DenseDsc and Dense2Net are very competitive in efficient CNNs with less than 1.0 M parameters on CIFAR. Furthermore, Dense2Net achieves state-of-the-art results on ImageNet in manual CNNs with less than 10 M parameters.},
  archive      = {J_PR},
  author       = {Guoqing Li and Meng Zhang and Jiaojie Li and Feng Lv and Guodong Tong},
  doi          = {10.1016/j.patcog.2020.107610},
  journal      = {Pattern Recognition},
  pages        = {107610},
  shortjournal = {Pattern Recognition},
  title        = {Efficient densely connected convolutional neural networks},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A generic shift-norm-activation approach for deep learning.
<em>PR</em>, <em>109</em>, 107609. (<a
href="https://doi.org/10.1016/j.patcog.2020.107609">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning has received increasing attention in the last decade. Its amazing success, is partly attributed to the evolution of normalization and activation techniques. However, less works have devoted to explore both modules together. This work, therefore, aims at pushing for a deeper understanding on the effect of normalization and activation together analytically. We design a generic method which integrates both normalization and activation together as a whole, named as the Generic Shift-Normalization-Activation Approach (GSNA), in reserving richer information propagation in neural networks . A rigorous mathematical analysis was performed to investigate the benefits of the designed method, such as its computation complexity, performance potential as well as optimization over trainable parameter initialization. Further, extensive experiments are conducted to demonstrate the superiority and generality of the designed method in many computer vision benchmarking tasks, such as CIFAR-10/100, SVHN, ImageNet32 × 32, etc. To explore its generality, we also conduct some experiments on natural language understanding tasks like text classification , natural language inference, and some variational generative task as well. More interestingly, GSNA can be naturally incorporated into the existing neural networks with arbitrary architectures, demonstrating its generic effectiveness in deep learning field.},
  archive      = {J_PR},
  author       = {Zhi Chen and Pin-Han Ho},
  doi          = {10.1016/j.patcog.2020.107609},
  journal      = {Pattern Recognition},
  pages        = {107609},
  shortjournal = {Pattern Recognition},
  title        = {A generic shift-norm-activation approach for deep learning},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Topological optimization of the DenseNet with
pretrained-weights inheritance and genetic channel selection.
<em>PR</em>, <em>109</em>, 107608. (<a
href="https://doi.org/10.1016/j.patcog.2020.107608">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks (CNNs) have been successfully applied in many computer vision applications [1] , especially in image classification tasks, where most of the structures have been designed manually. With the aid of skip connection and dense connection, the depths of the models are becoming “deeper” and the filters of layers are getting “wider” in order to tackle the challenge of large-scale datasets. However, large-scale models in convolutional layers become inefficient due to the redundant channels from input feature maps . In this paper, we aim to automatically optimize the topology of the DenseNet, in which unnecessary convolutional kernels are reduced. To achieve this, we present a training pipeline that generates the network structure using a genetic algorithm . We first propose two encoding methods that can represent the structure of the model using a fixed-length binary string. A three-step based evolutionary process consisting of selection, crossover, and mutation is proposed to optimize the structure. We also present a pretrained weight inheritance method which can largely reduce the total time consumption of the genetic process. Experimental results have demonstrated that our proposed model can achieve comparable accuracy to the state-of-the-art models, across a wide range of image recognition and classification datasets, whilst significantly reducing the number of parameters.},
  archive      = {J_PR},
  author       = {Zhenyu Fang and Jinchang Ren and Stephen Marshall and Huimin Zhao and Song Wang and Xuelong Li},
  doi          = {10.1016/j.patcog.2020.107608},
  journal      = {Pattern Recognition},
  pages        = {107608},
  shortjournal = {Pattern Recognition},
  title        = {Topological optimization of the DenseNet with pretrained-weights inheritance and genetic channel selection},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Nonlocal patch similarity based heterogeneous remote sensing
change detection. <em>PR</em>, <em>109</em>, 107598. (<a
href="https://doi.org/10.1016/j.patcog.2020.107598">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Change detection of heterogeneous remote sensing images is an important and challenging topic, which has found a wide range of applications in many fields, especially in the emergency situation resulting from nature disaster. However, the difference in imaging mechanism of heterogeneous sensors makes it difficult to carry out a direct comparison of images. In this paper, we propose a new change detection method based on similarity measurement between heterogeneous images. The method constructs a graph for each patch based on the nonlocal patch similarity to establish a connection between heterogeneous data, and then measures the change level by measuring how much the graph structure of one image still conforms to that of the other image. The graph structures are compared in the same domain, so it can avoid the leakage of heterogeneous data and bring more robust change detection results. Experiments demonstrate the effective performance of the proposed nonlocal patch similarity based heterogeneous change detection method .},
  archive      = {J_PR},
  author       = {Yuli Sun and Lin Lei and Xiao Li and Hao Sun and Gangyao Kuang},
  doi          = {10.1016/j.patcog.2020.107598},
  journal      = {Pattern Recognition},
  pages        = {107598},
  shortjournal = {Pattern Recognition},
  title        = {Nonlocal patch similarity based heterogeneous remote sensing change detection},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Blind image deblurring based on the sparsity of patch
minimum information. <em>PR</em>, <em>109</em>, 107597. (<a
href="https://doi.org/10.1016/j.patcog.2020.107597">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blind image deblurring is a very challenging inverse problem due to the severe ill-posedness caused by the unknown kernel and the latent clear image. To tackle this problem, appropriate smoothing regularizations and image priors are usually employed and incorporated into the associated variational models to alleviate the inherent ill-posedness. In this paper, we first propose a strongly imposed zero patch minimum constraint for the latent image, which helps alleviate the ill-posedness of the inverse problem for blind image deblurring. Then, we retrieve important fine details by assigning the patch minimum information obtained from the blurred image back to the latent image to further enhance its structure. Finally, we introduce an adaptive regularizer which was shown to have significantly better edge-preserving property than the total variation regularizer for the image restoration of degraded images . Operator splitting techniques are used to accomplish an efficient numerical implementation of the proposed variational model. A number of numerical experiments and comparisons with some state-of-the-art methods are conducted to demonstrate the effective performance of the newly proposed method.},
  archive      = {J_PR},
  author       = {Po-Wen Hsieh and Pei-Chiang Shao},
  doi          = {10.1016/j.patcog.2020.107597},
  journal      = {Pattern Recognition},
  pages        = {107597},
  shortjournal = {Pattern Recognition},
  title        = {Blind image deblurring based on the sparsity of patch minimum information},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Weakly supervised image classification and pointwise
localization with graph convolutional networks. <em>PR</em>,
<em>109</em>, 107596. (<a
href="https://doi.org/10.1016/j.patcog.2020.107596">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In computer vision , the research community has been looking to how to benefit from weakly supervised learning that utilizes easily obtained image-level labels to train neural network models . The existing deep convolutional neural networks for weakly supervised learning, however, generally do not fully exploit the label dependencies in an image. To make full use of this information, in this paper, we propose a new framework for weakly supervised learning of deep convolutional neural networks , introducing graph convolutional networks to capture the semantic label co-occurrence in an image. Moreover, we propose a novel initialization method for label embedding in graph convolutional networks , which enables a smoother optimization for interrelationships learning. Extensive experiments and comparisons on four public benchmark datasets (PASCAL VOC 2007, PASCAL VOC 2012, Microsoft COCO, and NUS-WIDE) show the superior performance of our approach in both image classification and weakly supervised pointwise object localization . These results lead us to conclude that the label dependencies in the input image can provide valuable evidence for learning strongly localized features.},
  archive      = {J_PR},
  author       = {Yongsheng Liu and Wenyu Chen and Hong Qu and S.M. Hasan Mahmud and Kebin Miao},
  doi          = {10.1016/j.patcog.2020.107596},
  journal      = {Pattern Recognition},
  pages        = {107596},
  shortjournal = {Pattern Recognition},
  title        = {Weakly supervised image classification and pointwise localization with graph convolutional networks},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Two-phase dynamic routing for micro and macro-level
equivariance in multi-column capsule networks. <em>PR</em>,
<em>109</em>, 107595. (<a
href="https://doi.org/10.1016/j.patcog.2020.107595">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The capability of multi column convolutional networks in identifying local invariant features helps improve its performance on image classification tasks to a large extent. Suppression of non maximal activations in a convolutional network, however, can lead to loss of valuable information, as scalar activations typically only ,encode the presence (or absence) of a feature in an input image, providing no additional information. Capsule networks, on other hand, learn richer representations by propagating non-maximal activations to higher layers, encoding the agreement between neurons at various layers on the presence (or absence) of a feature into a fixed-length vector. Traditional capsule networks, however encodes agreements for micro and macro-level features of an input image with same precedence. Such an uniform agreement protocol can hinder the repsentation capability of a network, especially for datasets that contain objects with independently deformable components. To address this, we propose a novel two-phase dynamic routing protocol that computes agreements between neurons at various layers for micro and macro-level features, following a hierarchical learning paradigm. Experiments on seven publicly available datasets show that a multi-column capsule network that encodes an input image following our routing protocol performs competitively or better than contemporary multi-column convolutional architectures andtraditional capsule networks on a classification task.Implementations of the networks used in this paper have been made available at: github.com/DVLP-CMATERJU/TwoPhaseDynamicRouting.},
  archive      = {J_PR},
  author       = {Bodhisatwa Mandal and Ritesh Sarkhel and Swarnendu Ghosh and Nibaran Das and Mita Nasipuri},
  doi          = {10.1016/j.patcog.2020.107595},
  journal      = {Pattern Recognition},
  pages        = {107595},
  shortjournal = {Pattern Recognition},
  title        = {Two-phase dynamic routing for micro and macro-level equivariance in multi-column capsule networks},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). GridMix: Strong regularization through local context
mapping. <em>PR</em>, <em>109</em>, 107594. (<a
href="https://doi.org/10.1016/j.patcog.2020.107594">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently developed regularization techniques improve the networks generalization by only considering the global context. Therefore, the network tends to focus on a few most discriminative subregions of an image for prediction accuracy, leading the network being sensitive to unseen or noisy data. To address this disadvantage, we introduce the concept of local context mapping by predicting patch-level labels and combine it with a method of local data augmentation by grid-based mixing, called GridMix. Through our analysis of intermediate representations, we show that our GridMix can effectively regularize the network model. Finally, our evaluation results indicate that GridMix outperforms state-of-the-art techniques in classification and adversarial robustness, and it achieves a comparable performance in weakly supervised object localization .},
  archive      = {J_PR},
  author       = {Kyungjune Baek and Duhyeon Bang and Hyunjung Shim},
  doi          = {10.1016/j.patcog.2020.107594},
  journal      = {Pattern Recognition},
  pages        = {107594},
  shortjournal = {Pattern Recognition},
  title        = {GridMix: Strong regularization through local context mapping},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Solving large-scale support vector ordinal regression with
asynchronous parallel coordinate descent algorithms. <em>PR</em>,
<em>109</em>, 107592. (<a
href="https://doi.org/10.1016/j.patcog.2020.107592">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ordinal regression is one of the most influential tasks of supervised learning. Support vector ordinal regression (SVOR) is an appealing method to tackle ordinal regression problems . However, due to the complexity in the formulation of SVOR and the high cost of kernel computation, traditional SVOR solvers are inefficient for large-scale training. To address this problem, in this paper, we first highlight a special SVOR formulation whose thresholds are described implicitly, so that the dual formulation is concise to apply the state-of-the-art asynchronous parallel coordinate descent algorithm, such as AsyGCD. To further accelerate the training for SVOR, we propose two novel asynchronous parallel coordinate descent algorithms, called AsyACGD and AsyORGCD respectively. AsyACGD is an accelerated extension of AsyGCD using active set strategy. AsyORGCD is specifically designed for SVOR that it can keep the ordered thresholds when it is training so that it can obtain good performance with lower time. Experimental results on several large-scale ordinal regression datasets demonstrate the superiority of our proposed algorithms.},
  archive      = {J_PR},
  author       = {Bin Gu and Xiang Geng and Wanli Shi and Yingying Shan and Yufang Huang and Zhijie Wang and Guansheng Zheng},
  doi          = {10.1016/j.patcog.2020.107592},
  journal      = {Pattern Recognition},
  pages        = {107592},
  shortjournal = {Pattern Recognition},
  title        = {Solving large-scale support vector ordinal regression with asynchronous parallel coordinate descent algorithms},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Probabilistic homogeneity for document image segmentation.
<em>PR</em>, <em>109</em>, 107591. (<a
href="https://doi.org/10.1016/j.patcog.2020.107591">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we propose a novel probabilistic framework for document segmentation exploiting human perceptual recognition of text regions from complicated layouts. In particular, we conceptualize text homogeneity as the Gestalt pattern displayed in text regions, characterized by proximately and symmetrically arranged units with similar morphological and texture features. We model this pattern in the local region of a connected component (CC) using an hierarchical formulation, which simulates a random walk-and-check on a graph encoding the neighborhood of the CC. The proposed formulation allows an effective computation of what we call the probabilistic local text homogeneity (PLTH) using a weighted summation of the weights of the graph, which are derived from a probabilistic description of the homogeneity between neighboring CCs and computed through Bayesian cue integration. The proposed PLTH enables a multi-aspect analysis, where various primitives such as geometrical configuration , morphological features, texture characterization and location priors are integrated in one computational probabilistic model. This enables an effective text and non-text classification of CCs preceding any grouping process, which is currently absent in document segmentation. Experimental results show that our segmentation method based on the proposed PLTH model improves upon the state-of-the-art.},
  archive      = {J_PR},
  author       = {Tan Lu and Ann Dooms},
  doi          = {10.1016/j.patcog.2020.107591},
  journal      = {Pattern Recognition},
  pages        = {107591},
  shortjournal = {Pattern Recognition},
  title        = {Probabilistic homogeneity for document image segmentation},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning modulation filter networks for weak signal
detection in noise. <em>PR</em>, <em>109</em>, 107590. (<a
href="https://doi.org/10.1016/j.patcog.2020.107590">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weak signal detection is a challenging yet significant problem in the field of radio communication. Although hand-crafted filters are widely used in signal processing, they are challenged by the weak signal detection task with unknown background noise especially in the range of 0-5dB. In this paper, we propose the learning modulation filter networks (LMFNs) to improve the detection performance. The approach is based on a two-stage optimization scheme which addresses filter learning, attention mechanism and classification in a unified framework. Modulation filters are built to enhance the capacity of the learned filters, and the attention mechanism further characterizes the saliency properties of the input signal. LMFNs reduce the storage size of the network while achieving the state-of-the-art performance by a significant margin compared to traditional cognitive radio approaches. We establish a weak signal dataset that contains unmanned aerial vehicle (UAV) communication signals in a real-terrain environment. The source code and dataset will be made publicly available soon.},
  archive      = {J_PR},
  author       = {Duona Zhang and Wenrui Ding and Baochang Zhang and Chunhui Liu and Jungong Han and David Doermann},
  doi          = {10.1016/j.patcog.2020.107590},
  journal      = {Pattern Recognition},
  pages        = {107590},
  shortjournal = {Pattern Recognition},
  title        = {Learning modulation filter networks for weak signal detection in noise},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DenMune: Density peak based clustering using mutual nearest
neighbors. <em>PR</em>, <em>109</em>, 107589. (<a
href="https://doi.org/10.1016/j.patcog.2020.107589">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many clustering algorithms fail when clusters are of arbitrary shapes, of varying densities, or the data classes are unbalanced and close to each other, even in two dimensions. A novel clustering algorithm “DenMune” is presented to meet this challenge. It is based on identifying dense regions using mutual nearest neighborhoods of size K , where K is the only parameter required from the user, besides obeying the mutual nearest neighbor consistency principle. The algorithm is stable for a wide range of values of K . Moreover, it is able to automatically detect and remove noise from the clustering process as well as detecting the target clusters. It produces robust results on various low and high dimensional datasets relative to several known state of the art clustering algorithms .},
  archive      = {J_PR},
  author       = {Mohamed Abbas and Adel El-Zoghabi and Amin Shoukry},
  doi          = {10.1016/j.patcog.2020.107589},
  journal      = {Pattern Recognition},
  pages        = {107589},
  shortjournal = {Pattern Recognition},
  title        = {DenMune: Density peak based clustering using mutual nearest neighbors},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An occlusion-resistant circle detector using inscribed
triangles. <em>PR</em>, <em>109</em>, 107588. (<a
href="https://doi.org/10.1016/j.patcog.2020.107588">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Circle detection is a critical issue in pattern recognition and image analysis. Conventional geometry-based methods such as tangent or symmetry are sensitive to noise or occlusion. Area computation is more robust against noise, because it avoids differential calculations. Inspired by this characteristic, we present a novel method for fast circle detection using inscribed triangles. The proposed algorithm, which is robust to noise and resistant to occlusion, first extracts circular arcs by approximating line segments and identifying inflection points and sharp corners. To speed up the computation, irrelevant segments are filtered out through the triangle inequality . Arcs that belong to the same circle are then combined according to the position constraint and the inscribed triangle constraint. The circle parameters are further estimated by inscribed triangles based upon the Theil-Sen estimator and linear error refinement without the dependence of least-square fitting but still with the equivalent accuracy. Finally, candidate circles are verified to prune false positives through an inlier ratio rule, which jointly considers both distance and angle deviations. Extensive experiments are conducted on synthetic images including overlapping circles, and real images from four diverse datasets (three publicly available and one we built). Results are compared with those of representative state-of-the-art methods, and the proposed method is demonstrated to embraces several advantages: resistant to occlusion, more robust to noise, and better performance and efficiency.},
  archive      = {J_PR},
  author       = {Mingyang Zhao and Xiaohong Jia and Dong-Ming Yan},
  doi          = {10.1016/j.patcog.2020.107588},
  journal      = {Pattern Recognition},
  pages        = {107588},
  shortjournal = {Pattern Recognition},
  title        = {An occlusion-resistant circle detector using inscribed triangles},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SLiKER: Sparse loss induced kernel ensemble regression.
<em>PR</em>, <em>109</em>, 107587. (<a
href="https://doi.org/10.1016/j.patcog.2020.107587">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Kernel ridge regression (KRR) is an efficient method for regression task . However, KRR has a deficiency in finding appropriate type of kernel functions and their parameters. To overcome this shortcoming, a novel kernel ensemble framework is developed. In this ensemble framework, each kernel regressor is associated with a weight that can be adaptively determined according to its contribution to the regression result. By this way, more appropriate kernels and more accurate parameters can be learned directly from data without any manual intervention, which results in better performance in regression. In addition, to overcome the influence of existing outliers, the regressor loss is modeled as a sparse signal , thus a Sparse Loss induced Kernel Ensemble Regression (SLiKER) method is obtained. Experimental results on several UCI regression and computer vision datasets show that our proposed approach obtains best regression and classification performances among the state-of-art comparative methods .},
  archive      = {J_PR},
  author       = {Xiang-Jun Shen and ChengGong Ni and Liangjun Wang and Zheng-Jun Zha},
  doi          = {10.1016/j.patcog.2020.107587},
  journal      = {Pattern Recognition},
  pages        = {107587},
  shortjournal = {Pattern Recognition},
  title        = {SLiKER: Sparse loss induced kernel ensemble regression},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Active k-labelsets ensemble for multi-label classification.
<em>PR</em>, <em>109</em>, 107583. (<a
href="https://doi.org/10.1016/j.patcog.2020.107583">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The random k -labelsets ensemble (RA k EL) is a multi-label learning strategy that integrates many single-label learning models. Each single-label model is constructed using a label powerset (LP) technique based on a randomly generated size- k label subset. Although RA k EL can improve the generalization capability and reduce the complexity of the original LP method, the quality of the randomly generated label subsets could be low. On the one hand, the transformed classes may be difficult to separate in the feature space, negatively affecting the performance; on the other hand, the classes might be highly imbalanced, resulting in difficulties in using the existing single-label algorithms. To solve these problems, we propose an active k -labelsets ensemble (AC k EL) paradigm. Borrowing the idea of active learning, a label-selection criterion is proposed to evaluate the separability and balance level of the classes transformed from a label subset. Subsequently, by randomly selecting the first label or label subset, the remaining ones are iteratively chosen based on the proposed criterion. AC k EL can be realized in both the disjoint and overlapping modes, which adopt pool-based and stream-based frameworks, respectively. Experimental comparisons demonstrate the feasibility and effectiveness of the proposed methods.},
  archive      = {J_PR},
  author       = {Ran Wang and Sam Kwong and Xu Wang and Yuheng Jia},
  doi          = {10.1016/j.patcog.2020.107583},
  journal      = {Pattern Recognition},
  pages        = {107583},
  shortjournal = {Pattern Recognition},
  title        = {Active k-labelsets ensemble for multi-label classification},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Local minima found in the subparameter space can be
effective for ensembles of deep convolutional neural networks.
<em>PR</em>, <em>109</em>, 107582. (<a
href="https://doi.org/10.1016/j.patcog.2020.107582">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ensembles of deep convolutional neural networks (CNNs), which integrate multiple deep CNN models to achieve better generalization for an artificial intelligence application, now play an important role in ensemble learning due to the dominant position of deep learning. However, the usage of ensembles of deep CNNs is still not adequate because the increasing complexity of deep CNN architectures and the emerging data with large dimensionality have made the training stage and testing stage of ensembles of deep CNNs inevitably expensive. To alleviate this situation, we propose a new approach that finds multiple models converging to local minima in subparameter space for ensembles of deep CNNs. The subparameter space here refers to the space constructed by a partial selection of parameters, instead of the entire set of parameters, of a deep CNN architecture. We show that local minima found in the subparameter space of a deep CNN architecture can in fact be effective for ensembles of deep CNNs to achieve better generalization. Moreover, finding local minima in the subparameter space of a deep CNN architecture is more affordable at the training stage, and the multiple models at the found local minima can also be selectively fused to achieve better ensemble generalization while limiting the expense to a single deep CNN model at the testing stage. Demonstrations of MobilenetV2, Resnet50 and InceptionV4 (deep CNN architectures from lightweight to complex) on ImageNet, CIFAR-10 and CIFAR-100, respectively, lead us to believe that finding local minima in the subparameter space of a deep CNN architecture could be leveraged to broaden the usage of ensembles of deep CNNs.},
  archive      = {J_PR},
  author       = {Yongquan Yang and Haijun Lv and Ning Chen and Yang Wu and Jiayi Zheng and Zhongxi Zheng},
  doi          = {10.1016/j.patcog.2020.107582},
  journal      = {Pattern Recognition},
  pages        = {107582},
  shortjournal = {Pattern Recognition},
  title        = {Local minima found in the subparameter space can be effective for ensembles of deep convolutional neural networks},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Iterative local re-ranking with attribute guided synthesis
for face sketch recognition. <em>PR</em>, <em>109</em>, 107579. (<a
href="https://doi.org/10.1016/j.patcog.2020.107579">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Because of the large texture and spatial structure discrepancies between face sketches and photos, face sketch recognition becomes a challenging problem in face recognition community. For example, in law enforcement and security, the specific face sketch generation process could introduce some inevitable biases which results in poor face sketch recognition performance. In order to mimic the modality gap introduced by the biases during face sketch creation process, the novel iterative local re-ranking with attribute guided synthesis method is proposed for face sketch recognition, which does not require any extra manually annotation or human interaction. The clues of face attributes are utilized to generate images with varying local characteristic from probe sketches, which could help eliminate the unavoidable biases. Considering the special property of face sketches, the iterative local re-ranking algorithm is designed to encode the contextual information integrated with local invariant discriminative information for matching sketches with photos. Experimental results on multiple face sketch databases demonstrate that the proposed method achieves superior performances compared with state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Decheng Liu and Xinbo Gao and Nannan Wang and Chunlei Peng and Jie Li},
  doi          = {10.1016/j.patcog.2020.107579},
  journal      = {Pattern Recognition},
  pages        = {107579},
  shortjournal = {Pattern Recognition},
  title        = {Iterative local re-ranking with attribute guided synthesis for face sketch recognition},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DPNet: Detail-preserving network for high quality monocular
depth estimation. <em>PR</em>, <em>109</em>, 107578. (<a
href="https://doi.org/10.1016/j.patcog.2020.107578">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing monocular depth estimation methods are unsatisfactory due to the inaccurate inference of depth details and the loss of spatial information. In this paper, we present a novel detail-preserving network (DPNet), i.e., a dual-branch network architecture that fully addresses the above problems and facilitates the depth map inference. Specifically, in contextual branch (CB), we propose an effective and efficient nonlocal spatial attention module by introducing non-local filtering strategy to explicitly exploit the pixel relationship in spatial domain, which can bring significant promotion on depth details inference. Meanwhile, we design a spatial branch (SB) to preserve the spatial information and generate high-resolution features from input color image. A refinement module (RM) is then proposed to fuse the heterogeneous features from both spatial and contextual branches to obtain a high quality depth map. Experimental results show that the proposed method outperforms SOTA methods on benchmark RGB-D datasets.},
  archive      = {J_PR},
  author       = {Xinchen Ye and Shude Chen and Rui Xu},
  doi          = {10.1016/j.patcog.2020.107578},
  journal      = {Pattern Recognition},
  pages        = {107578},
  shortjournal = {Pattern Recognition},
  title        = {DPNet: Detail-preserving network for high quality monocular depth estimation},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Complex image processing with less data—document image
binarization by integrating multiple pre-trained u-net modules.
<em>PR</em>, <em>109</em>, 107577. (<a
href="https://doi.org/10.1016/j.patcog.2020.107577">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial neural networks have been shown significant performance in various image-to-image conversion tasks. However, complex conversions often require a large number of images for model training. Therefore, we propose a convolutional model for image-to-image conversions using a pipeline of simpler image processing modules. To verify our proposed approach, we use a document image binarization as the task. Document image binarization is an important process that affects the accuracy of document analysis and recognition. In this paper, we propose a novel document binarization method called Cascading Modular U-Nets (CMU-Nets). CMU-Nets consist of pre-trained modular modules useful for overcoming the problem of a shortage of training images. We also propose a novel cascading scheme for improving overall cascading model performance. We verify the proposed model on all available Document Image Binarization Competition (DIBCO) and the Handwritten-DIBCO (H-DIBCO) datasets.},
  archive      = {J_PR},
  author       = {Seokjun Kang and Brian Kenji Iwana and Seiichi Uchida},
  doi          = {10.1016/j.patcog.2020.107577},
  journal      = {Pattern Recognition},
  pages        = {107577},
  shortjournal = {Pattern Recognition},
  title        = {Complex image processing with less data—Document image binarization by integrating multiple pre-trained U-net modules},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Generalisations of stochastic supervision models.
<em>PR</em>, <em>109</em>, 107575. (<a
href="https://doi.org/10.1016/j.patcog.2020.107575">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When the labelling information is not deterministic, traditional supervised learning algorithms cannot be applied. In this case, stochastic supervision models provide a valuable alternative to classification. However, these models are restricted in several aspects, which critically limits their applicability. In this paper, we provide four generalisations of stochastic supervision models, extending them to asymmetric assessments, multiple classes, feature-dependent assessments and multi-modal classes, respectively. Corresponding to these generalisations, we derive four new EM algorithms . We show the effectiveness of our generalisations through illustrative examples of simulated datasets, as well as real-world examples of three famous datasets, the MNIST dataset, the CIFAR-10 dataset and the EMNIST dataset.},
  archive      = {J_PR},
  author       = {Xiaoou Lu and Yangqi Qiao and Rui Zhu and Guijin Wang and Zhanyu Ma and Jing-Hao Xue},
  doi          = {10.1016/j.patcog.2020.107575},
  journal      = {Pattern Recognition},
  pages        = {107575},
  shortjournal = {Pattern Recognition},
  title        = {Generalisations of stochastic supervision models},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Parametric recurrence quantification analysis of
autoregressive processes for pattern recognition in multichannel
electroencephalographic data. <em>PR</em>, <em>109</em>, 107572. (<a
href="https://doi.org/10.1016/j.patcog.2020.107572">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recurrence quantification analysis (RQA) is an acknowledged method for the characterization of experimental time series. We propose a parametric version of RQA, pRQA, allowing a fast processing of spatial arrays of time series, once each is modeled by an autoregressive stochastic process . This method relies on the analytical derivation of asymptotic expressions for five current RQA measures as a function of the model parameters. By avoiding the construction of the recurrence plot of the time series, pRQA is computationally efficient. As a proof of principle, we apply pRQA to pattern recognition in multichannel electroencephalographic (EEG) data from a patient with a brain tumor.},
  archive      = {J_PR},
  author       = {Sofiane Ramdani and Anthony Boyer and Stéphane Caron and François Bonnetblanc and Frédéric Bouchara and Hugues Duffau and Annick Lesne},
  doi          = {10.1016/j.patcog.2020.107572},
  journal      = {Pattern Recognition},
  pages        = {107572},
  shortjournal = {Pattern Recognition},
  title        = {Parametric recurrence quantification analysis of autoregressive processes for pattern recognition in multichannel electroencephalographic data},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). CADN: A weakly supervised learning-based category-aware
object detection network for surface defect detection. <em>PR</em>,
<em>109</em>, 107571. (<a
href="https://doi.org/10.1016/j.patcog.2020.107571">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large-scale data with human annotations is of crucial importance for training deep convolutional neural network (DCNN) to ensure stable and reliable performance. However, accurate annotations, such as bounding box and pixel-level annotations, demand expensive labeling efforts, which has prevented wide application of DCNN in industries. Focusing on the problem of surface defect detection , this paper proposes a weakly supervised learning method named Category-Aware object Detection network (CADN) to tackle the dilemma. CADN is trained with image tag annotations only and performs image classification and defect localization simultaneously. The weakly supervised learning is achieved by extracting category-aware spatial information in a classification pipeline. CADN could be equipped with either a lighter or a larger backbone network as the feature extractor resulting in better real-time performance or higher accuracy. To address the two conflicting objectives simultaneously, both of which are significant concerns in industrial applications, knowledge distillation strategy is adopted to force the learned features of a lighter CADN to mimic that of a larger CADN. Accordingly, the accuracy of the lighter CADN is improved while high real-time performance is maintained. The proposed approach is verified on our own defect dataset as well as on an open-source defect dataset. As demonstrated, satisfied performance is achieved by the proposed method, which could meet industrial requirements completely. Meanwhile, the method minimizes human efforts involved in image labelling, thus promoting the applications of DCNN in industries.},
  archive      = {J_PR},
  author       = {Jiabin Zhang and Hu Su and Wei Zou and Xinyi Gong and Zhengtao Zhang and Fei Shen},
  doi          = {10.1016/j.patcog.2020.107571},
  journal      = {Pattern Recognition},
  pages        = {107571},
  shortjournal = {Pattern Recognition},
  title        = {CADN: A weakly supervised learning-based category-aware object detection network for surface defect detection},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Convolutional kernels with an element-wise weighting
mechanism for identifying abnormal brain connectivity patterns.
<em>PR</em>, <em>109</em>, 107570. (<a
href="https://doi.org/10.1016/j.patcog.2020.107570">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning based human brain network classification has gained increasing attention in recent years. However, current methods remain limited in exploring the topological structure information of a brain network. In this paper, we propose a kind of new convolutional kernels with an element-wise weighting mechanism (CKEW) to extract hierarchical topological features of brain networks, in which each weight is assigned to an element with a unique neuroscientific meaning. In addition, a novel classification framework based on CKEW is presented to diagnose brain diseases and explore the most important original features by a tracing feature analysis method efficiently. Experimental results on two autism spectrum disorder (ASD) datasets and an attention deficit hyperactivity disorder (ADHD) dataset with functional magnetic resonance imaging (fMRI) data demonstrate that our method can more accurately distinguish subject groups compared to several state-of-the-art methods in cerebral disease classification, and abnormal connectivity patterns and brain regions identified are more likely to become biomarkers associated with a cerebral disease.},
  archive      = {J_PR},
  author       = {Junzhong Ji and Xinying Xing and Yao Yao and Junwei Li and Xiaodan Zhang},
  doi          = {10.1016/j.patcog.2020.107570},
  journal      = {Pattern Recognition},
  pages        = {107570},
  shortjournal = {Pattern Recognition},
  title        = {Convolutional kernels with an element-wise weighting mechanism for identifying abnormal brain connectivity patterns},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A comprehensive survey of multi-view video summarization.
<em>PR</em>, <em>109</em>, 107567. (<a
href="https://doi.org/10.1016/j.patcog.2020.107567">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There has been an exponential growth in the amount of visual data on a daily basis acquired from single or multi-view surveillance camera networks. This massive amount of data requires efficient mechanisms such as video summarization to ensure that only significant data are reported and the redundancy is reduced. Multi-view video summarization (MVS) is a less redundant and more concise way of providing information from the video content of all the cameras in the form of either keyframes or video segments. This paper presents an overview of the existing strategies proposed for MVS, including their advantages and drawbacks. Our survey covers the genericsteps in MVS, such as the pre-processing of video data, feature extraction, and post-processing followed by summary generation. We also describe the datasets that are available for the evaluation of MVS. Finally, we examine the major current issues related to MVS and put forward the recommendations for future research 1 .},
  archive      = {J_PR},
  author       = {Tanveer Hussain and Khan Muhammad and Weiping Ding and Jaime Lloret and Sung Wook Baik and Victor Hugo C. de Albuquerque},
  doi          = {10.1016/j.patcog.2020.107567},
  journal      = {Pattern Recognition},
  pages        = {107567},
  shortjournal = {Pattern Recognition},
  title        = {A comprehensive survey of multi-view video summarization},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Weighted bilateral k-means algorithm for fast co-clustering
and fast spectral clustering. <em>PR</em>, <em>109</em>, 107560. (<a
href="https://doi.org/10.1016/j.patcog.2020.107560">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bipartite spectral graph partition (BSGP) is a school of the most well-known algorithms designed for the bipartite graph partition problem. It is also a fundamental mathematical model widely used in the tasks of co-clustering and fast spectral clustering . In BSGP, the key is to find the minimal normalized cuts (Ncuts) of bipartite graph . However, the convolutional BSGP algorithms usually need to use the singular value decomposition (SVD) to find the minimal Ncuts, which is computational prohibitive. Under this circumstance, the application range of those methods would be limited when the volume of the dataset is huge or the dimension of features is high. To overcome this problem, this paper proposes a novel weighted bilateral k -means (WBKM) algorithm and applies it for co-clustering and fast spectral clustering . Specifically, WBKM is a relaxation of the problem of finding the minimal Ncuts of bipartite graph, so it can be seen as a new solution for the minimal-Ncuts problem in bipartite graph. Different from the conventional relaxation ways, WBKM relaxes the minimal-Ncuts problem to a Non-negative decomposition problem which can be solved by an efficient iterative method. Therefore, the running speed of the proposed method is much faster. Besides, as our model can directly output the clustering results without any help of post-procedures, its solution tends to be more close to the ideal solution of the minimal-Ncuts problem than that of the conventional BSGP algorithms. To demonstrate the effectiveness and efficiency of the proposed method, extensive experiments on various types of datasets are conducted. Compared with other state-of-the-art methods, the proposed WBKM not only has faster computational speed, but also achieves more promising clustering results .},
  archive      = {J_PR},
  author       = {Kun Song and Xiwen Yao and Feiping Nie and Xuelong Li and Mingliang Xu},
  doi          = {10.1016/j.patcog.2020.107560},
  journal      = {Pattern Recognition},
  pages        = {107560},
  shortjournal = {Pattern Recognition},
  title        = {Weighted bilateral K-means algorithm for fast co-clustering and fast spectral clustering},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
</ul>

</body>
</html>
