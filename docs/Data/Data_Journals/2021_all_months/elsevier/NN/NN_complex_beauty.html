<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>NN_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="nn---401">NN - 401</h2>
<ul>
<li><details>
<summary>
(2021a). INN/ENNS/JNNS - membership applic. form. <em>NN</em>,
<em>144</em>, II. (<a
href="https://doi.org/10.1016/S0893-6080(21)00421-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NN},
  doi          = {10.1016/S0893-6080(21)00421-4},
  journal      = {Neural Networks},
  pages        = {II},
  shortjournal = {Neural Netw.},
  title        = {INN/ENNS/JNNS - membership applic. form},
  volume       = {144},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Current events. <em>NN</em>, <em>144</em>, I. (<a
href="https://doi.org/10.1016/S0893-6080(21)00420-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NN},
  doi          = {10.1016/S0893-6080(21)00420-2},
  journal      = {Neural Networks},
  pages        = {I},
  shortjournal = {Neural Netw.},
  title        = {Current events},
  volume       = {144},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Theory of deep convolutional neural networks III:
Approximating radial functions. <em>NN</em>, <em>144</em>, 778–790. (<a
href="https://doi.org/10.1016/j.neunet.2021.09.027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a family of deep neural networks consisting of two groups of convolutional layers , a downsampling operator, and a fully connected layer. The network structure depends on two structural parameters which determine the numbers of convolutional layers and the width of the fully connected layer. We establish an approximation theory with explicit approximation rates when the approximated function takes a composite form f ∘ Q f∘Q with a feature polynomial Q Q and a univariate function f f . In particular, we prove that such a network can outperform fully connected shallow networks in approximating radial functions with Q ( x ) = | x | 2 Q(x)=|x|2 , when the dimension d d of data from R d Rd is large. This gives the first rigorous proof for the superiority of deep convolutional neural networks in approximating functions with special structures. Then we carry out generalization analysis for empirical risk minimization with such a deep network in a regression framework with the regression function of the form f ∘ Q f∘Q . Our network structure which does not use any composite information or the functions Q Q and f f can automatically extract features and make use of the composite nature of the regression function via tuning the structural parameters. Our analysis provides an error bound which decreases with the network depth to a minimum and then increases, verifying theoretically a trade-off phenomenon observed for network depths in many practical applications.},
  archive      = {J_NN},
  author       = {Tong Mao and Zhongjie Shi and Ding-Xuan Zhou},
  doi          = {10.1016/j.neunet.2021.09.027},
  journal      = {Neural Networks},
  pages        = {778-790},
  shortjournal = {Neural Netw.},
  title        = {Theory of deep convolutional neural networks III: Approximating radial functions},
  volume       = {144},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). TACN: A topical adversarial capsule network for textual
network embedding. <em>NN</em>, <em>144</em>, 766–777. (<a
href="https://doi.org/10.1016/j.neunet.2021.09.026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Combining topological information and attributed information of nodes in networks effectively is a valuable task in network embedding. Nevertheless, many prior network embedding methods regarded attributed information of nodes as simple attribute sets or ignored them totally. In some scenarios, the hidden information contained in vertex attributes are essential to network embedding. For instance, networks that contain vertexes with text information play an increasingly important role in our life, including citation networks, social networks, and entry networks. In these textual networks, the latent topic relevance information of different vertexes contained in textual attributes information are valuable in the network analysis process. Shared latent topics of nodes in networks may influence the interaction between them, which is critical to network embedding. However, much prior work for textual network embedding only regarded the text information as simple word sets while ignored the embedded topic information. In this paper, we develop a model named Topical Adversarial Capsule Network (TACN) for textual network embedding, which extracts a low-dimensional latent space of the original network from node structures, vertex attributes, and topic information contained in text of nodes. The proposed TACN contains three parts. The first part is an embedding model, which extracts the embedding representation from the topological structure , vertex attributes, and document-topic distributions. To ensure a consistent training process by back-propagation, we generate document-topic distributions by the neural topic model with Gaussian Softmax constructions. The second part is a prediction model, which is used to exploit labels of vertices. In the third part, an adversarial capsule model is used to help distinguish the latent representations from node structure domain, vertex attribute domain, or document-topic distribution domain. The latent representations, which may come from the three domains, are the output of the embedding model. We incorporate the adversarial idea into the adversarial capsule model to combine the information from these three domains, rather than to distinguish the representations conventionally. Experiments on seven real-world datasets validate the effectiveness of our method.},
  archive      = {J_NN},
  author       = {Xiaorui Qin and Yanghui Rao and Haoran Xie and Jiahai Wang and Fu Lee Wang},
  doi          = {10.1016/j.neunet.2021.09.026},
  journal      = {Neural Networks},
  pages        = {766-777},
  shortjournal = {Neural Netw.},
  title        = {TACN: A topical adversarial capsule network for textual network embedding},
  volume       = {144},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A novel meta-learning framework: Multi-features adaptive
aggregation method with information enhancer. <em>NN</em>, <em>144</em>,
755–765. (<a
href="https://doi.org/10.1016/j.neunet.2021.09.029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning has shown its great potential in the field of image classification due to its powerful feature extraction ability, which heavily depends on the number of available training samples. However, it is still a huge challenge on how to obtain an effective feature representation and further learn a promising classifier by deep networks when faced with few-shot classification tasks . This paper proposes a multi-features adaptive aggregation meta-learning method with an information enhancer for few-shot classification tasks , referred to as MFAML. It contains three main modules, including a feature extraction module, an information enhancer, and a multi-features adaptive aggregation classifier (MFAAC). During the meta-training stage, the information enhancer comprised of some deconvolutional layers is designed to promote the effective utilization of samples and thereby capturing more valuable information in the process of feature extraction. Simultaneously, the MFAAC module integrates the features from several convolutional layers of the feature extraction module. The obtained features then feed into the similarity module so that implementing the adaptive adjustment of the predicted label. The information enhancer and MFAAC are connected by a hybrid loss, providing an excellent feature representation. During the meta-test stage, the information enhancer is removed and we keep the remaining architecture for fast adaption on the final target task. The whole MFAML framework is solved by the optimization strategy of model-agnostic meta-learner (MAML) and can effectively improve generalization performance . Experimental results on several benchmark datasets demonstrate the superiority of the proposed method over other representative few-shot classification methods.},
  archive      = {J_NN},
  author       = {Hailiang Ye and Yi Wang and Feilong Cao},
  doi          = {10.1016/j.neunet.2021.09.029},
  journal      = {Neural Networks},
  pages        = {755-765},
  shortjournal = {Neural Netw.},
  title        = {A novel meta-learning framework: Multi-features adaptive aggregation method with information enhancer},
  volume       = {144},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Impact of axonal delay on structure development in a
multi-layered network. <em>NN</em>, <em>144</em>, 737–754. (<a
href="https://doi.org/10.1016/j.neunet.2021.08.023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The mechanisms underlying how activity in the visual pathway gives rise through neural plasticity to many features observed experimentally in early stages of visual processing was provided by Linsker in a seminal, three-paper series. Owing to the complexity of multi-layer models, an implicit assumption in Linsker’s and subsequent papers has been that propagation delay is homogeneous, playing little functional role in neural behavior . In this paper, we relax this assumption to examine the impact of distance-dependent axonal propagation delay on neural learning . We show that propagation delay induces low-pass filtering by dispersing arrival times of spikes from presynaptic neurons , providing a natural correlation cancellation mechanism for distal connections. The cut-off frequency decreases as radial propagation delay within a layer increases relative to propagation delay between layers, introducing an upper limit on temporal resolution. Given that the postsynaptic potential acts as a low-pass filter, we show that the effective time constant of each should enable processing of similar scales of temporal information. This has implications for the visual system, in which receptive field size and, thus, propagation delay, increases with eccentricity. Furthermore, network response is frequency dependent since higher frequencies require increased input amplitude to compensate for attenuation. This concords with frequency-dependent contrast sensitivity, which changes with eccentricity and receptive field size . We further show that the proportion of inhibition relative to excitation is larger where radial propagation delay is long relative to inter-laminar delay, and that delay reduces the range in on-center size, providing stability to variations in homeostatic parameters.},
  archive      = {J_NN},
  author       = {Catherine E. Davey and David B. Grayden and Anthony N. Burkitt},
  doi          = {10.1016/j.neunet.2021.08.023},
  journal      = {Neural Networks},
  pages        = {737-754},
  shortjournal = {Neural Netw.},
  title        = {Impact of axonal delay on structure development in a multi-layered network},
  volume       = {144},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). ARAE: Adversarially robust training of autoencoders improves
novelty detection. <em>NN</em>, <em>144</em>, 726–736. (<a
href="https://doi.org/10.1016/j.neunet.2021.09.014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autoencoders have recently been widely employed to approach the novelty detection problem. Trained only on the normal data, the AE is expected to reconstruct the normal data effectively while failing to regenerate the anomalous data. Based on this assumption, one could utilize the AE for novelty detection. However, it is known that this assumption does not always hold. Such an AE can often perfectly reconstruct the anomalous data due to modeling low-level and generic features in the input. We propose a novel training algorithm for the AE that facilitates learning more semantically meaningful features to address this problem. For this purpose, we exploit the fact that adversarial robustness promotes the learning of significant features. Therefore, we force the AE to learn such features by making its bottleneck layer more stable against adversarial perturbations. This idea is general and can be applied to other autoencoder-based approaches as well. We show that despite using a much simpler architecture than the prior methods, the proposed AE outperforms or is competitive to the state-of-the-art on four benchmark datasets and two medical datasets.},
  archive      = {J_NN},
  author       = {Mohammadreza Salehi and Atrin Arya and Barbod Pajoum and Mohammad Otoofi and Amirreza Shaeiri and Mohammad Hossein Rohban and Hamid R. Rabiee},
  doi          = {10.1016/j.neunet.2021.09.014},
  journal      = {Neural Networks},
  pages        = {726-736},
  shortjournal = {Neural Netw.},
  title        = {ARAE: Adversarially robust training of autoencoders improves novelty detection},
  volume       = {144},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Decentralized control and local information for robust and
adaptive decentralized deep reinforcement learning. <em>NN</em>,
<em>144</em>, 699–725. (<a
href="https://doi.org/10.1016/j.neunet.2021.09.017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decentralization is a central characteristic of biological motor control that allows for fast responses relying on local sensory information. In contrast, the current trend of Deep Reinforcement Learning (DRL) based approaches to motor control follows a centralized paradigm using a single, holistic controller that has to untangle the whole input information space. This motivates to ask whether decentralization as seen in biological control architectures might also be beneficial for embodied sensori-motor control systems when using DRL. To answer this question, we provide an analysis and comparison of eight control architectures for adaptive locomotion that were derived for a four-legged agent, but with their degree of decentralization varying systematically between the extremes of fully centralized and fully decentralized. Our comparison shows that learning speed is significantly enhanced in distributed architectures—while still reaching the same high performance level of centralized architectures—due to smaller search spaces and local costs providing more focused information for learning. Second, we find an increased robustness of the learning process in the decentralized cases—it is less demanding to hyperparameter selection and less prone to becoming trapped in poor local minima. Finally, when examining generalization to uneven terrains —not used during training—we find best performance for an intermediate architecture that is decentralized, but integrates only local information from both neighboring legs. Together, these findings demonstrate beneficial effects of distributing control into decentralized units and relying on local information. This appears as a promising approach towards more robust DRL and better generalization towards adaptive behavior .},
  archive      = {J_NN},
  author       = {Malte Schilling and Andrew Melnik and Frank W. Ohl and Helge J. Ritter and Barbara Hammer},
  doi          = {10.1016/j.neunet.2021.09.017},
  journal      = {Neural Networks},
  pages        = {699-725},
  shortjournal = {Neural Netw.},
  title        = {Decentralized control and local information for robust and adaptive decentralized deep reinforcement learning},
  volume       = {144},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Optimizing deeper spiking neural networks for dynamic vision
sensing. <em>NN</em>, <em>144</em>, 686–698. (<a
href="https://doi.org/10.1016/j.neunet.2021.09.022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spiking Neural Networks (SNNs) have recently emerged as a new generation of low-power deep neural networks due to sparse, asynchronous, and binary event-driven processing. Most previous deep SNN optimization methods focus on static datasets (e.g., MNIST) from a conventional frame-based camera. On the other hand, optimization techniques for event data from Dynamic Vision Sensor (DVS) cameras are still at infancy. Most prior SNN techniques handling DVS data are limited to shallow networks and thus, show low performance. Generally, we observe that the integrate-and-fire behavior of spiking neurons diminishes spike activity in deeper layers. The sparse spike activity results in a sub-optimal solution during training ( i . e ., performance degradation). To address this limitation, we propose novel algorithmic and architectural advances to accelerate the training of very deep SNNs on DVS data. Specifically, we propose Spike Activation Lift Training (SALT) which increases spike activity across all layers by optimizing both weights and thresholds in convolutional layers . After applying SALT, we train the weights based on the cross-entropy loss. SALT helps the networks to convey ample information across all layers during training and therefore improves the performance. Furthermore, we propose a simple and effective architecture, called Switched-BN, which exploits Batch Normalization (BN). Previous methods show that the standard BN is incompatible with the temporal dynamics of SNNs. Therefore, in Switched-BN architecture, we apply BN to the last layer of an SNN after accumulating all the spikes from previous layer with a spike voltage accumulator ( i . e ., converting temporal spike information to float value). Even though we apply BN in just one layer of SNNs, our results demonstrate a considerable performance gain without any significant computational overhead. Through extensive experiments, we show the effectiveness of SALT and Switched-BN for training very deep SNNs from scratch on various benchmarks including, DVS-Cifar10, N-Caltech, DHP19, CIFAR10, and CIFAR100. To the best of our knowledge, this is the first work showing state-of-the-art performance with deep SNNs on DVS data.},
  archive      = {J_NN},
  author       = {Youngeun Kim and Priyadarshini Panda},
  doi          = {10.1016/j.neunet.2021.09.022},
  journal      = {Neural Networks},
  pages        = {686-698},
  shortjournal = {Neural Netw.},
  title        = {Optimizing deeper spiking neural networks for dynamic vision sensing},
  volume       = {144},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep-hook: A trusted deep learning-based framework for
unknown malware detection and classification in linux cloud
environments. <em>NN</em>, <em>144</em>, 648–685. (<a
href="https://doi.org/10.1016/j.neunet.2021.09.019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since the beginning of the 21st century, the use of cloud computing has increased rapidly, and it currently plays a significant role among most organizations’ information technology (IT) infrastructure. Virtualization technologies, particularly virtual machines (VMs), are widely used and lie at the core of cloud computing . While different operating systems can run on top of VM instances , in public cloud environments the Linux operating system is used 90\% of the time. Because of their prevalence, organizational Linux-based virtual servers have become an attractive target for cyber-attacks, mainly launched by sophisticated malware designed at causing harm, sabotaging operations, obtaining data, or gaining financial profit. This has resulted in the need for an advanced and reliable unknown malware detection mechanism for Linux cloud-based environments. Antivirus software and today’s even more advanced malware detection solutions have limitations in detecting new, unseen, and evasive malware. Moreover, many existing solutions are considered untrusted, as they operate on the inspected machine and can be interfered with, and can even be detected by the malware itself, allowing malware to evade detection and cause damage. In this paper, we propose Deep-Hook, a trusted framework for unknown malware detection in Linux-based cloud environments. Deep-Hook hooks the VM’s volatile memory in a trusted manner and acquires the memory dump to discover malware footprints while the VM operates. The memory dumps are transformed into visual images which are analyzed using a convolutional neural network (CNN) based classifier. The proposed framework has some key advantages, such as its agility, its ability to eliminate the need for features defined by a cyber domain expert, and most importantly, its ability to analyze the entire memory dump and thus to better utilize the existing indication it conceals, thus allowing the induction of a more accurate detection model. Deep-Hook was evaluated on widely used Linux virtual servers ; four state-of-the-art CNN architectures; eight image resolutions; and a total of 22,400 volatile memory dumps representing the execution of a broad set of benign and malicious Linux applications. Our experimental evaluation results demonstrate Deep-Hook’s ability to effectively, efficiently, and accurately detect and classify unknown malware (even evasive malware like rootkits), with an AUC and accuracy of up to 99.9\%.},
  archive      = {J_NN},
  author       = {Tom Landman and Nir Nissim},
  doi          = {10.1016/j.neunet.2021.09.019},
  journal      = {Neural Networks},
  pages        = {648-685},
  shortjournal = {Neural Netw.},
  title        = {Deep-hook: A trusted deep learning-based framework for unknown malware detection and classification in linux cloud environments},
  volume       = {144},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Extreme neural machines. <em>NN</em>, <em>144</em>, 639–647.
(<a href="https://doi.org/10.1016/j.neunet.2021.09.021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recurrent neural networks can solve a variety of computational tasks and produce patterns of activity that capture key properties of brain circuits. However, learning rules designed to train these models are time-consuming and prone to inaccuracies when tuning connection weights located deep within the network. Here, we describe a rapid one-shot learning rule to train recurrent networks composed of biologically-grounded neurons. First, inputs to the model are compressed onto a smaller number of recurrent neurons. Then, a non-iterative rule adjusts the output weights of these neurons based on a target signal. The model learned to reproduce natural images, sequential patterns, as well as a high-resolution movie scene. Together, results provide a novel avenue for one-shot learning in biologically realistic recurrent networks and open a path to solving complex tasks by merging brain-inspired models with rapid optimization rules.},
  archive      = {J_NN},
  author       = {Megan Boucher-Routhier and Bill Ling Feng Zhang and Jean-Philippe Thivierge},
  doi          = {10.1016/j.neunet.2021.09.021},
  journal      = {Neural Networks},
  pages        = {639-647},
  shortjournal = {Neural Netw.},
  title        = {Extreme neural machines},
  volume       = {144},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A language modeling-like approach to sketching. <em>NN</em>,
<em>144</em>, 627–638. (<a
href="https://doi.org/10.1016/j.neunet.2021.09.020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sketching is a universal communication tool that, despite its simplicity, is able to efficiently express a large variety of concepts and, in some limited contexts, it can be even more immediate and effective than natural language. In this paper we explore the feasibility of using neural networks to approach sketching in the same way they are commonly used in Language Modeling . We propose a novel approach to what we refer to as “Sketch Modeling”, in which a neural network is exploited to learn a probabilistic model that estimates the probability of sketches. We focus on simple sketches and, in particular, on the case in which sketches are represented as sequences of segments. Segments and sequences can be either given – when the sketches are originally drawn in this format – or automatically generated from the input drawing by means of a procedure that we designed to create short sequences, loosely inspired by the human behavior . A Recurrent Neural Network is used to learn the sketch model and, afterward, the network is seeded with an incomplete sketch that it is asked to complete, generating one segment at each time step. We propose a set of measures to evaluate the outcome of a Beam Search-based generation procedure, showing how they can be used to identify the most promising generations. Our experimental analysis assesses the feasibility of this way of modeling sketches, also in the case in which several different categories of sketches are considered.},
  archive      = {J_NN},
  author       = {Lisa Graziani and Marco Gori and Stefano Melacci},
  doi          = {10.1016/j.neunet.2021.09.020},
  journal      = {Neural Networks},
  pages        = {627-638},
  shortjournal = {Neural Netw.},
  title        = {A language modeling-like approach to sketching},
  volume       = {144},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Weak sub-network pruning for strong and efficient neural
networks. <em>NN</em>, <em>144</em>, 614–626. (<a
href="https://doi.org/10.1016/j.neunet.2021.09.015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pruning methods to compress and accelerate deep convolutional neural networks (CNNs) have recently attracted growing attention, with the view of deploying pruned networks on resource-constrained hardware devices. However, most existing methods focus on small granularities , such as weight, kernel and filter, for the exploration of pruning. Thus, it will be bound to iteratively prune the whole neural networks based on those small granularities for high compression ratio with little performance loss. To address these issues, we theoretically analyze the relationship between the activation and gradient sparsity , and the channel saliency. Based on our findings, we propose a novel and effective method of weak sub-network pruning (WSP). Specifically, for a well-trained network model, we divide the whole compression process into two non-iterative stages. The first stage is to directly obtain a strong sub-network by pruning the weakest one. We first identify the less important channels from all the layers and determine the weakest sub-network, whereby each selected channel makes a minimal contribution to both the feed-forward and feed-backward processes. Then, a one-shot pruning strategy is executed to form a strong sub-network enabling fine tuning, while significantly reducing the impact of the network depth and width on the compression efficiency , especially for deep and wide network architectures . The second stage is to globally fine-tune the strong sub-network using several epochs to restore its original recognition accuracy. Furthermore, our proposed method impacts on the fully-connected layers as well as the convolutional layers for simultaneous compression and acceleration. Comprehensive experiments on VGG16 and ResNet-50 involving a variety of popular benchmarks, such as ImageNet-1K, CIFAR-10, CUB-200 and PASCAL VOC, demonstrate that our WSP method achieves superior performance on classification, domain adaption and object detection tasks with small model size. Our source code is available at https://github.com/QingbeiGuo/WSP.git .},
  archive      = {J_NN},
  author       = {Qingbei Guo and Xiao-Jun Wu and Josef Kittler and Zhiquan Feng},
  doi          = {10.1016/j.neunet.2021.09.015},
  journal      = {Neural Networks},
  pages        = {614-626},
  shortjournal = {Neural Netw.},
  title        = {Weak sub-network pruning for strong and efficient neural networks},
  volume       = {144},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Natural and artificial intelligence: A brief introduction to
the interplay between AI and neuroscience research. <em>NN</em>,
<em>144</em>, 603–613. (<a
href="https://doi.org/10.1016/j.neunet.2021.09.018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neuroscience and artificial intelligence (AI) share a long history of collaboration. Advances in neuroscience, alongside huge leaps in computer processing power over the last few decades, have given rise to a new generation of in silico neural networks inspired by the architecture of the brain. These AI systems are now capable of many of the advanced perceptual and cognitive abilities of biological systems, including object recognition and decision making. Moreover, AI is now increasingly being employed as a tool for neuroscience research and is transforming our understanding of brain functions. In particular, deep learning has been used to model how convolutional layers and recurrent connections in the brain’s cerebral cortex control important functions, including visual processing, memory, and motor control. Excitingly, the use of neuroscience-inspired AI also holds great promise for understanding how changes in brain networks result in psychopathologies, and could even be utilized in treatment regimes. Here we discuss recent advancements in four areas in which the relationship between neuroscience and AI has led to major advancements in the field; (1) AI models of working memory, (2) AI visual processing, (3) AI analysis of big neuroscience datasets, and (4) computational psychiatry .},
  archive      = {J_NN},
  author       = {Tom Macpherson and Anne Churchland and Terry Sejnowski and James DiCarlo and Yukiyasu Kamitani and Hidehiko Takahashi and Takatoshi Hikida},
  doi          = {10.1016/j.neunet.2021.09.018},
  journal      = {Neural Networks},
  pages        = {603-613},
  shortjournal = {Neural Netw.},
  title        = {Natural and artificial intelligence: A brief introduction to the interplay between AI and neuroscience research},
  volume       = {144},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The generalized extreme learning machines: Tuning
hyperparameters and limiting approach for the moore–penrose generalized
inverse. <em>NN</em>, <em>144</em>, 591–602. (<a
href="https://doi.org/10.1016/j.neunet.2021.09.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose the generalized extreme learning machine (GELM). GELM is an ELM that incorporates the analyzed hyperparameters of ELM, such as sizes and ranks of weight matrices , and a limiting approach for the Moore–Penrose generalized inverse (M–P GI) into the learning process. ELM overcomes shortcomings of traditional deep learning , such as time-consuming due to iterative executions, as it learns quickly by removing the adjustment time of hyperparameters. There are desirable numbers of hidden nodes in ELM for single hidden layer feedforward neural networks, minimizing prediction error. However, it is difficult to use the desired number because it is related to the number of data used and datasets tend to be large. We consider ELM for multiple hidden layer feedforward neural networks. We analyze matrices derived in the network and figure out the characteristics of weight matrices and biases considering accurate prediction and learning speed, based on mathematical theories and a limiting approach for the M–P GI. The final output matrix of GELM is formulated explicitly. Experiments are conducted to verify the analysis using network traffic data, including DDoS attacks. The performances of GLEM, such as accuracies and learning speed, are compared for the networks with single and multiple hidden layers. Numerical results show the advantages of GELM in the performance measures , and the use of multiple hidden layers in GELM does not significantly affect performance. The theory-based prediction performances obtained from GELM will be the criterion for the margin of deep learning performance.},
  archive      = {J_NN},
  author       = {Meejoung Kim},
  doi          = {10.1016/j.neunet.2021.09.008},
  journal      = {Neural Networks},
  pages        = {591-602},
  shortjournal = {Neural Netw.},
  title        = {The generalized extreme learning machines: Tuning hyperparameters and limiting approach for the Moore–Penrose generalized inverse},
  volume       = {144},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). World model learning and inference. <em>NN</em>,
<em>144</em>, 573–590. (<a
href="https://doi.org/10.1016/j.neunet.2021.09.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding information processing in the brain—and creating general-purpose artificial intelligence—are long-standing aspirations of scientists and engineers worldwide. The distinctive features of human intelligence are high-level cognition and control in various interactions with the world including the self, which are not defined in advance and are vary over time. The challenge of building human-like intelligent machines, as well as progress in brain science and behavioural analyses, robotics, and their associated theoretical formalisations , speaks to the importance of the world-model learning and inference. In this article, after briefly surveying the history and challenges of internal model learning and probabilistic learning, we introduce the free energy principle, which provides a useful framework within which to consider neuronal computation and probabilistic world models. Next, we showcase examples of human behaviour and cognition explained under that principle. We then describe symbol emergence in the context of probabilistic modelling , as a topic at the frontiers of cognitive robotics . Lastly, we review recent progress in creating human-like intelligence by using novel probabilistic programming languages . The striking consensus that emerges from these studies is that probabilistic descriptions of learning and inference are powerful and effective ways to create human-like artificial intelligent machines and to understand intelligence in the context of how humans interact with their world.},
  archive      = {J_NN},
  author       = {Karl Friston and Rosalyn J. Moran and Yukie Nagai and Tadahiro Taniguchi and Hiroaki Gomi and Josh Tenenbaum},
  doi          = {10.1016/j.neunet.2021.09.011},
  journal      = {Neural Networks},
  pages        = {573-590},
  shortjournal = {Neural Netw.},
  title        = {World model learning and inference},
  volume       = {144},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Highly parallelized memristive binary neural network.
<em>NN</em>, <em>144</em>, 565–572. (<a
href="https://doi.org/10.1016/j.neunet.2021.09.016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {At present, in the new hardware design work of deep learning , memristor as a non-volatile memory with computing power has become a research hotspot. The weights in the deep neural network are the floating-point number. Writing a floating-point value into a memristor will result in a loss of accuracy, and the writing process will take more time. The binarized neural network (BNN) binarizes the weights and activation values that were originally floating-point numbers to +1 and -1. This will greatly reduce the storage space consumption and time consumption of programming the resistance value of the memristor. Furthermore, this will help to simplify the programming of memristors in deep neural network circuits and speed up the inference process. This paper provides a complete solution for implementing memristive BNN. Furthermore, we improved the design of the memristor crossbar by converting the input feature map and kernel before performing the convolution operation that can ensure the sign of the input voltage of each port constant. Therefore, we do not need to determine the sign of the input voltage required by the port in advance which simplifies the process of inputting the feature map elements to each port of the crossbar in the form of voltage. At the same time, in order to ensure that the output of the current convolution layer can be directly used as the input of the next layer, we have added a corresponding processing circuit, which integrates batch-normalization and binarization operations.},
  archive      = {J_NN},
  author       = {Jiadong Chen and Shiping Wen and Kaibo Shi and Yin Yang},
  doi          = {10.1016/j.neunet.2021.09.016},
  journal      = {Neural Networks},
  pages        = {565-572},
  shortjournal = {Neural Netw.},
  title        = {Highly parallelized memristive binary neural network},
  volume       = {144},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Disturbance-immune weight sharing for neural architecture
search. <em>NN</em>, <em>144</em>, 553–564. (<a
href="https://doi.org/10.1016/j.neunet.2021.09.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural architecture search (NAS) has gained increasing attention in the community of architecture design. One of the key factors behind the success lies in the training efficiency brought by the weight sharing (WS) technique. However, WS-based NAS methods often suffer from a performance disturbance (PD) issue. That is, the training of subsequent architectures inevitably disturbs the performance of previously trained architectures due to the partially shared weights. This leads to inaccurate performance estimation for the previous architectures, which makes it hard to learn a good search strategy. To alleviate the performance disturbance issue, we propose a new disturbance-immune update strategy for model updating. Specifically, to preserve the knowledge learned by previous architectures, we constrain the training of subsequent architectures in an orthogonal space via orthogonal gradient descent . Equipped with this strategy, we propose a novel disturbance-immune training scheme for NAS. We theoretically analyze the effectiveness of our strategy in alleviating the PD risk. Extensive experiments on CIFAR-10 and ImageNet verify the superiority of our method.},
  archive      = {J_NN},
  author       = {Shuaicheng Niu and Jiaxiang Wu and Yifan Zhang and Yong Guo and Peilin Zhao and Junzhou Huang and Mingkui Tan},
  doi          = {10.1016/j.neunet.2021.09.002},
  journal      = {Neural Networks},
  pages        = {553-564},
  shortjournal = {Neural Netw.},
  title        = {Disturbance-immune weight sharing for neural architecture search},
  volume       = {144},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). A conversational model for eliciting new chatting topics in
open-domain conversation. <em>NN</em>, <em>144</em>, 540–552. (<a
href="https://doi.org/10.1016/j.neunet.2021.08.021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In human conversations, the emergence of new topics is a key factor in enabling dialogues to last longer. Additional information brought by new topics can make the conversation more diverse and interesting. Chat-bots also need to be equipped with this ability to proactively elicit new chatting topics. However, previous studies have neglected the elicitation of new topics in open-domain conversations. At the same time, previous works have represented topics with word-level keywords or entities. However, a topic is open to multiple keywords and a keyword can reflect multiple potential topics. To move towards a fine-grained topic representation, we represent topic with topically related words. In this paper, we design a novel model, named CMTE, which focuses not only on coherence with context, but also brings up new chatting topics. In order to extract topic information from conversational utterances, a Topic Fetcher module is designed to fetch semantic-coherent topics with the help of topic model. To equip model with the ability to elicit new topics, a Topic Manager module is designed to associate the new topic with context. Finally, responses are generated by a well-designed fusion decoding mechanism to explicitly distinguish between topic words and general words. Experiment results show that our model is better than state of the art in automatic metrics and manual evaluations.},
  archive      = {J_NN},
  author       = {Weizhao Li and Feng Ge and Yi Cai and Da Ren},
  doi          = {10.1016/j.neunet.2021.08.021},
  journal      = {Neural Networks},
  pages        = {540-552},
  shortjournal = {Neural Netw.},
  title        = {A conversational model for eliciting new chatting topics in open-domain conversation},
  volume       = {144},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Personalised predictive modelling with brain-inspired
spiking neural networks of longitudinal MRI neuroimaging data and the
case study of dementia. <em>NN</em>, <em>144</em>, 522–539. (<a
href="https://doi.org/10.1016/j.neunet.2021.09.013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Longitudinal neuroimaging provides spatiotemporal brain data (STBD) measurement that can be utilised to understand dynamic changes in brain structure and/or function underpinning cognitive activities. Making sense of such highly interactive information is challenging, given that the features manifest intricate temporal, causal relations between the spatially distributed neural sources in the brain. The current paper argues for the advancement of deep learning algorithms in brain-inspired spiking neural networks (SNN), capable of modelling structural data across time (longitudinal measurement) and space (anatomical components). The paper proposes a methodology and a computational architecture based on SNN for building personalised predictive models from longitudinal brain data to accurately detect, understand, and predict the dynamics of an individual’s functional brain state. The methodology includes finding clusters of similar data to each individual, data interpolation, deep learning in a 3-dimensional brain-template structured SNN model, classification and prediction of individual outcome, visualisation of structural brain changes related to the predicted outcomes, interpretation of results, and individual and group predictive marker discovery. To demonstrate the functionality of the proposed methodology, the paper presents experimental results on a longitudinal magnetic resonance imaging (MRI) dataset derived from 175 older adults of the internationally recognised community-based cohort Sydney Memory and Ageing Study (MAS) spanning 6 years of follow-up. The models were able to accurately classify and predict 2 years ahead of cognitive decline, such as mild cognitive impairment (MCI) and dementia with 95\% and 91\% accuracy, respectively. The proposed methodology also offers a 3-dimensional visualisation of the MRI models reflecting the dynamic patterns of regional changes in white matter hyperintensity (WMH) and brain volume over 6 years. The method is efficient for personalised predictive modelling on a wide range of neuroimaging longitudinal data , including also demographic, genetic, and clinical data. As a case study, it resulted in finding predictive markers for MCI and dementia as dynamic brain patterns using MRI data.},
  archive      = {J_NN},
  author       = {Maryam Doborjeh and Zohreh Doborjeh and Alexander Merkin and Helena Bahrami and Alexander Sumich and Rita Krishnamurthi and Oleg N. Medvedev and Mark Crook-Rumsey and Catherine Morgan and Ian Kirk and Perminder S. Sachdev and Henry Brodaty and Kristan Kang and Wei Wen and Valery Feigin and Nikola Kasabov},
  doi          = {10.1016/j.neunet.2021.09.013},
  journal      = {Neural Networks},
  pages        = {522-539},
  shortjournal = {Neural Netw.},
  title        = {Personalised predictive modelling with brain-inspired spiking neural networks of longitudinal MRI neuroimaging data and the case study of dementia},
  volume       = {144},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Parallel and hierarchical neural mechanisms for adaptive and
predictive behavioral control. <em>NN</em>, <em>144</em>, 507–521. (<a
href="https://doi.org/10.1016/j.neunet.2021.09.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Our brain can be recognized as a network of largely hierarchically organized neural circuits that operate to control specific functions, but when acting in parallel, enable the performance of complex and simultaneous behaviors . Indeed, many of our daily actions require concurrent information processing in sensorimotor, associative, and limbic circuits that are dynamically and hierarchically modulated by sensory information and previous learning. This organization of information processing in biological organisms has served as a major inspiration for artificial intelligence and has helped to create in silico systems capable of matching or even outperforming humans in several specific tasks, including visual recognition and strategy-based games. However, the development of human-like robots that are able to move as quickly as humans and respond flexibly in various situations remains a major challenge and indicates an area where further use of parallel and hierarchical architectures may hold promise. In this article we review several important neural and behavioral mechanisms organizing hierarchical and predictive processing for the acquisition and realization of flexible behavioral control. Then, inspired by the organizational features of brain circuits, we introduce a multi-timescale parallel and hierarchical learning framework for the realization of versatile and agile movement in humanoid robots .},
  archive      = {J_NN},
  author       = {Tom Macpherson and Masayuki Matsumoto and Hiroaki Gomi and Jun Morimoto and Eiji Uchibe and Takatoshi Hikida},
  doi          = {10.1016/j.neunet.2021.09.009},
  journal      = {Neural Networks},
  pages        = {507-521},
  shortjournal = {Neural Netw.},
  title        = {Parallel and hierarchical neural mechanisms for adaptive and predictive behavioral control},
  volume       = {144},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Combining STDP and binary networks for reinforcement
learning from images and sparse rewards. <em>NN</em>, <em>144</em>,
496–506. (<a
href="https://doi.org/10.1016/j.neunet.2021.09.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spiking neural networks (SNNs) aim to replicate energy efficiency, learning speed and temporal processing of biological brains. However, accuracy and learning speed of such networks is still behind reinforcement learning (RL) models based on traditional neural models . This work combines a pre-trained binary convolutional neural network with an SNN trained online through reward-modulated STDP in order to leverage advantages of both models. The spiking network is an extension of its previous version, with improvements in architecture and dynamics to address a more challenging task. We focus on extensive experimental evaluation of the proposed model with optimized state-of-the-art baselines, namely proximal policy optimization (PPO) and deep Q network (DQN). The models are compared on a grid-world environment with high dimensional observations, consisting of RGB images with up to 256 × 256 pixels. The experimental results show that the proposed architecture can be a competitive alternative to deep reinforcement learning (DRL) in the evaluated environment and provide a foundation for more complex future applications of spiking networks.},
  archive      = {J_NN},
  author       = {Sérgio F. Chevtchenko and Teresa B. Ludermir},
  doi          = {10.1016/j.neunet.2021.09.010},
  journal      = {Neural Networks},
  pages        = {496-506},
  shortjournal = {Neural Netw.},
  title        = {Combining STDP and binary networks for reinforcement learning from images and sparse rewards},
  volume       = {144},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The whole brain architecture approach: Accelerating the
development of artificial general intelligence by referring to the
brain. <em>NN</em>, <em>144</em>, 478–495. (<a
href="https://doi.org/10.1016/j.neunet.2021.09.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The vastness of the design space that is created by the combination of numerous computational mechanisms, including machine learning , is an obstacle to creating artificial general intelligence (AGI). Brain-inspired AGI development; that is, the reduction of the design space to resemble a biological brain more closely, is a promising approach for solving this problem. However, it is difficult for an individual to design a software program that corresponds to the entire brain as the neuroscientific data that are required to understand the architecture of the brain are extensive and complicated. The whole-brain architecture approach divides the brain-inspired AGI development process into the task of designing the brain reference architecture (BRA), which provides the flow of information and a diagram of the corresponding components, and the task of developing each component using the BRA. This is known as BRA-driven development. Another difficulty lies in the extraction of the operating principles that are necessary for reproducing the cognitive–behavioral function of the brain from neuroscience data. Therefore, this study proposes structure-constrained interface decomposition (SCID), which is a hypothesis-building method for creating a hypothetical component diagram that is consistent with neuroscientific findings. The application of this approach has been initiated for constructing various regions of the brain. In the future, we will examine methods for evaluating the biological plausibility of brain-inspired software. This evaluation will also be used to prioritize different computational mechanisms, which should be integrated and associated with the same regions of the brain.},
  archive      = {J_NN},
  author       = {Hiroshi Yamakawa},
  doi          = {10.1016/j.neunet.2021.09.004},
  journal      = {Neural Networks},
  pages        = {478-495},
  shortjournal = {Neural Netw.},
  title        = {The whole brain architecture approach: Accelerating the development of artificial general intelligence by referring to the brain},
  volume       = {144},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). ACSL: Adaptive correlation-driven sparsity learning for deep
neural network compression. <em>NN</em>, <em>144</em>, 465–477. (<a
href="https://doi.org/10.1016/j.neunet.2021.09.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep convolutional neural network compression has attracted lots of attention due to the need to deploy accurate models on resource-constrained edge devices. Existing techniques mostly focus on compressing networks for image-level classification, and it is not clear if they generalize well on network architectures for more challenging pixel-level tasks, e.g., dense crowd counting or semantic segmentation . In this paper, we propose an adaptive correlation-driven sparsity learning (ACSL) framework for channel pruning that outperforms state-of-the-art methods on both image-level and pixel-level tasks. In our ACSL framework, we first quantify the data-dependent channel correlation information with a channel affinity matrix . Next, we leverage these inter-dependencies to induce sparsity into the channels with the introduced adaptive penalty strength. After removing the redundant channels, we obtain compact and efficient models, which have significantly less number of parameters while maintaining comparable performance with the original models. We demonstrate the advantages of our proposed approach on three popular vision tasks, i.e., dense crowd counting, semantic segmentation , and image-level classification. The experimental results demonstrate the superiority of our framework. In particular, for crowd counting on the Mall dataset, the proposed ACSL framework is able to reduce up to 94\% parameters (VGG16-Decoder) and 84\% FLOPs (ResNet101), while maintaining the same performance of (at times outperforming) the original model.},
  archive      = {J_NN},
  author       = {Wei He and Meiqing Wu and Siew-Kei Lam},
  doi          = {10.1016/j.neunet.2021.09.012},
  journal      = {Neural Networks},
  pages        = {465-477},
  shortjournal = {Neural Netw.},
  title        = {ACSL: Adaptive correlation-driven sparsity learning for deep neural network compression},
  volume       = {144},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Detection of pancreatic cancer by
convolutional-neural-network-assisted spontaneous raman spectroscopy
with critical feature visualization. <em>NN</em>, <em>144</em>, 455–464.
(<a href="https://doi.org/10.1016/j.neunet.2021.09.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pancreatic cancer is the deadliest cancer type with a five-year survival rate of less than 9\%. Detection of tumor margins plays an essential role in the success of surgical resection. However, histopathological assessment is time-consuming, expensive, and labor-intensive. We constructed a lab-designed, hand-held Raman spectroscopic system that could enable intraoperative tissue diagnosis using convolutional neural network (CNN) models to efficiently distinguish between cancerous and normal pancreatic tissue. To our best knowledge, this is the first reported effort to diagnose pancreatic cancer by CNN-aided spontaneous Raman scattering with a lab-developed system designed for intraoperative applications. Classification based on the original one-dimensional (1D) Raman, two-dimensional (2D) Raman images, and the first principal component (PC1) from the principal component analysis on the 2D image, could all achieve high performance: the testing sensitivity, specificity, and accuracy were over 95\%, and the area under the curve approached 0.99. Although CNN models often show great success in classification, it has always been challenging to visualize the CNN features in these models, which has never been achieved in the Raman spectroscopy application in cancer diagnosis. By studying individual Raman regions and by extracting and visualizing CNN features from max-pooling layers, we identified critical Raman peaks that could aid in the classification of cancerous and noncancerous tissues. 2D Raman PC1 yielded more critical peaks for pancreatic cancer identification than that of 1D Raman, as the Raman intensity was amplified by 2D Raman PC1. To our best knowledge, the feature visualization was achieved for the first time in the field of CNN-aided spontaneous Raman spectroscopy for cancer diagnosis. Based on these CNN feature peaks and their frequency at specific wavenumbers, pancreatic cancerous tissue was found to contain more biochemical components related to the protein contents (particularly collagen), whereas normal pancreatic tissue was found to contain more lipids and nucleic acid (particularly deoxyribonucleic acid/ribonucleic acid). Overall, the CNN model in combination with Raman spectroscopy could serve as a useful tool for the extraction of key features that can help differentiate pancreatic cancer from a normal pancreas.},
  archive      = {J_NN},
  author       = {Zhongqiang Li and Zheng Li and Qing Chen and Alexandra Ramos and Jian Zhang and J. Philip Boudreaux and Ramcharan Thiagarajan and Yvette Bren-Mattison and Michael E. Dunham and Andrew J. McWhorter and Xin Li and Ji-Ming Feng and Yanping Li and Shaomian Yao and Jian Xu},
  doi          = {10.1016/j.neunet.2021.09.006},
  journal      = {Neural Networks},
  pages        = {455-464},
  shortjournal = {Neural Netw.},
  title        = {Detection of pancreatic cancer by convolutional-neural-network-assisted spontaneous raman spectroscopy with critical feature visualization},
  volume       = {144},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Combination of certainty and uncertainty: Using FusionGAN to
create abstract paintings. <em>NN</em>, <em>144</em>, 443–454. (<a
href="https://doi.org/10.1016/j.neunet.2021.09.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the study of generative art, it is relatively easy at present to achieve a high degree of certainty or uncertainty. However, the combination of certainty and uncertainty has always been an area of difficulty in generative art. In this paper, we present a novel FusionGAN system to automate the generation of abstract paintings. These generated abstract paintings combine the factors of certainty and uncertainty. First, we collect an APdataset consisting of three parts: abstract paintings drawn by artists, sketches, and abstract paintings generated by other neural network methods. We then train the proposed FusionGAN system on the collected dataset to learn the expression of abstract paintings. Corresponding to the two-step operation of the combination of certainty and uncertainty in the artist’s creation, the proposed FusionGAN system is also divided into two steps for the generation of abstract paintings. More specifically, the first step is the basic structure establishment, which corresponds to the fundamental certainty element in the painting creation. The second step is the realization of details, which integrates the uncertain details based on the basic structure. The experimental results achieved by our system in abstract painting generation enrich the diversity of artistic creation and have been recognized by art institutions, with some results displayed on their websites.},
  archive      = {J_NN},
  author       = {Mao Li and Jiancheng Lv and Chenwei Tang and Jian Wang and Zhichen Lai and Youcheng Huang},
  doi          = {10.1016/j.neunet.2021.09.001},
  journal      = {Neural Networks},
  pages        = {443-454},
  shortjournal = {Neural Netw.},
  title        = {Combination of certainty and uncertainty: Using FusionGAN to create abstract paintings},
  volume       = {144},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A note on computing with kolmogorov superpositions without
iterations. <em>NN</em>, <em>144</em>, 438–442. (<a
href="https://doi.org/10.1016/j.neunet.2021.07.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We extend Kolmogorov’s Superpositions to approximating arbitrary continuous functions with a noniterative approach that can be used by any neural network that uses these superpositions. Our approximation algorithm uses a modified dimension reducing function that allows for an increased number of summands to achieve an error bound commensurate with that of r iterations for any r . This new variant of Kolmogorov’s Superpositions improves upon the original parallelism inherent in them by performing highly distributed parallel computations without synchronization. We note that this approach makes implementation much easier and more efficient on networks of modern parallel hardware, and thus makes it a more practical tool.},
  archive      = {J_NN},
  author       = {Robert Demb and David Sprecher},
  doi          = {10.1016/j.neunet.2021.07.006},
  journal      = {Neural Networks},
  pages        = {438-442},
  shortjournal = {Neural Netw.},
  title        = {A note on computing with kolmogorov superpositions without iterations},
  volume       = {144},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Active inference through whiskers. <em>NN</em>,
<em>144</em>, 428–437. (<a
href="https://doi.org/10.1016/j.neunet.2021.08.037">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rodents use whisking to probe actively their environment and to locate objects in space, hence providing a paradigmatic biological example of active sensing. Numerous studies show that the control of whisking has anticipatory aspects. For example, rodents target their whisker protraction to the distance at which they expect objects, rather than just reacting fast to contacts with unexpected objects. Here we characterize the anticipatory control of whisking in rodents as an active inference process. In this perspective, the rodent is endowed with a prior belief that it will touch something at the end of the whisker protraction, and it continuously modulates its whisking amplitude to minimize (proprioceptive and somatosensory) prediction errors arising from an unexpected whisker–object contact, or from a lack of an expected contact. We will use the model to qualitatively reproduce key empirical findings about the ways rodents modulate their whisker amplitude during exploration and the scanning of (expected or unexpected) objects. Furthermore, we will discuss how the components of active inference model can in principle map to the neurobiological circuits of rodent whisking.},
  archive      = {J_NN},
  author       = {Francesco Mannella and Federico Maggiore and Manuel Baltieri and Giovanni Pezzulo},
  doi          = {10.1016/j.neunet.2021.08.037},
  journal      = {Neural Networks},
  pages        = {428-437},
  shortjournal = {Neural Netw.},
  title        = {Active inference through whiskers},
  volume       = {144},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Broad-UNet: Multi-scale feature learning for nowcasting
tasks. <em>NN</em>, <em>144</em>, 419–427. (<a
href="https://doi.org/10.1016/j.neunet.2021.08.036">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weather nowcasting consists of predicting meteorological components in the short term at high spatial resolutions. Due to its influence in many human activities , accurate nowcasting has recently gained plenty of attention. In this paper, we treat the nowcasting problem as an image-to-image translation problem using satellite imagery. We introduce Broad-UNet, a novel architecture based on the core UNet model, to efficiently address this problem. In particular, the proposed Broad-UNet is equipped with asymmetric parallel convolutions as well as Atrous Spatial Pyramid Pooling (ASPP) module. In this way, the Broad-UNet model learns more complex patterns by combining multi-scale features while using fewer parameters than the core UNet model. The proposed model is applied on two different nowcasting tasks, i.e. precipitation maps and cloud cover nowcasting. The obtained numerical results show that the introduced Broad-UNet model performs more accurate predictions compared to the other examined architectures.},
  archive      = {J_NN},
  author       = {Jesús García Fernández and Siamak Mehrkanoon},
  doi          = {10.1016/j.neunet.2021.08.036},
  journal      = {Neural Networks},
  pages        = {419-427},
  shortjournal = {Neural Netw.},
  title        = {Broad-UNet: Multi-scale feature learning for nowcasting tasks},
  volume       = {144},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Structured ensembles: An approach to reduce the memory
footprint of ensemble methods. <em>NN</em>, <em>144</em>, 407–418. (<a
href="https://doi.org/10.1016/j.neunet.2021.09.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel ensembling technique for deep neural networks , which is able to drastically reduce the required memory compared to alternative approaches. In particular, we propose to extract multiple sub-networks from a single, untrained neural network by solving an end-to-end optimization task combining differentiable scaling over the original architecture, with multiple regularization terms favouring the diversity of the ensemble. Since our proposal aims to detect and extract sub-structures, we call it Structured Ensemble . On a large experimental evaluation, we show that our method can achieve higher or comparable accuracy to competing methods while requiring significantly less storage. In addition, we evaluate our ensembles in terms of predictive calibration and uncertainty, showing they compare favourably with the state-of-the-art. Finally, we draw a link with the continual learning literature, and we propose a modification of our framework to handle continuous streams of tasks with a sub-linear memory cost. We compare with a number of alternative strategies to mitigate catastrophic forgetting, highlighting advantages in terms of average accuracy and memory.},
  archive      = {J_NN},
  author       = {Jary Pomponi and Simone Scardapane and Aurelio Uncini},
  doi          = {10.1016/j.neunet.2021.09.007},
  journal      = {Neural Networks},
  pages        = {407-418},
  shortjournal = {Neural Netw.},
  title        = {Structured ensembles: An approach to reduce the memory footprint of ensemble methods},
  volume       = {144},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Uncertainty propagation for dropout-based bayesian neural
networks. <em>NN</em>, <em>144</em>, 394–406. (<a
href="https://doi.org/10.1016/j.neunet.2021.09.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Uncertainty evaluation is a core technique when deep neural networks (DNNs) are used in real-world problems. In practical applications, we often encounter unexpected samples that have not seen in the training process. Not only achieving the high-prediction accuracy but also detecting uncertain data is significant for safety-critical systems. In statistics and machine learning , Bayesian inference has been exploited for uncertainty evaluation. The Bayesian neural networks (BNNs) have recently attracted considerable attention in this context, as the DNN trained using dropout is interpreted as a Bayesian method. Based on this interpretation, several methods to calculate the Bayes predictive distribution for DNNs have been developed. Though the Monte-Carlo method called MC dropout is a popular method for uncertainty evaluation, it requires a number of repeated feed-forward calculations of DNNs with randomly sampled weight parameters. To overcome the computational issue, we propose a sampling-free method to evaluate uncertainty. Our method converts a neural network trained using dropout to the corresponding Bayesian neural network with variance propagation . Our method is available not only to feed-forward NNs but also to recurrent NNs such as LSTM . We report the computational efficiency and statistical reliability of our method in numerical experiments of language modeling using RNNs , and the out-of-distribution detection with DNNs.},
  archive      = {J_NN},
  author       = {Yuki Mae and Wataru Kumagai and Takafumi Kanamori},
  doi          = {10.1016/j.neunet.2021.09.005},
  journal      = {Neural Networks},
  pages        = {394-406},
  shortjournal = {Neural Netw.},
  title        = {Uncertainty propagation for dropout-based bayesian neural networks},
  volume       = {144},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Neural network surgery: Combining training with topology
optimization. <em>NN</em>, <em>144</em>, 384–393. (<a
href="https://doi.org/10.1016/j.neunet.2021.08.034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With ever increasing computational capacities, neural networks become more and more proficient at solving complex tasks. However, picking a sufficiently good network topology usually relies on expert human knowledge. Neural architecture search aims to reduce the extent of expertise that is needed. Modern architecture search techniques often rely on immense computational power, or apply trained meta-controllers for decision making. We develop a framework for a genetic algorithm that is both computationally cheap and makes decisions based on mathematical criteria rather than trained parameters. It is a hybrid approach that fuses training and topology optimization together into one process. Structural modifications that are performed include adding or removing layers of neurons, with some re-training applied to make up for any incurred change in input–output behaviour . Our ansatz is tested on several benchmark datasets with limited computational overhead compared to training only the baseline. This algorithm can achieve a significant increase in accuracy (as compared to a fully trained baseline), rescue insufficient topologies that in their current state are only able to learn to a limited extent, and dynamically reduce network size without loss in achieved accuracy. On standard ML datasets, accuracy improvements compared to baseline performance can range from 20\% for well performing starting topologies to more than 40\% in case of insufficient baselines, or reduce network size by almost 15\%.},
  archive      = {J_NN},
  author       = {Elisabeth J. Schiessler and Roland C. Aydin and Kevin Linka and Christian J. Cyron},
  doi          = {10.1016/j.neunet.2021.08.034},
  journal      = {Neural Networks},
  pages        = {384-393},
  shortjournal = {Neural Netw.},
  title        = {Neural network surgery: Combining training with topology optimization},
  volume       = {144},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Pinning multisynchronization of delayed fractional-order
memristor-based neural networks with nonlinear coupling and
almost-periodic perturbations. <em>NN</em>, <em>144</em>, 372–383. (<a
href="https://doi.org/10.1016/j.neunet.2021.08.029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper concerns the multisynchronization issue for delayed fractional-order memristor-based neural networks with nonlinear coupling and almost-periodic perturbations. First, the coexistence of multiple equilibrium states for isolated subnetwork is analyzed. By means of state-space decomposition, fractional-order Halanay inequality and Caputo derivative properties, the novel algebraic sufficient conditions are derived to ensure that the addressed networks with arbitrary activation functions have multiple locally stable almost periodic orbits or equilibrium points. Then, based on the obtained multistability results, a pinning control strategy is designed to realize the multisynchronization of the N N coupled networks. By the aid of graph theory, depth first search method and pinning control law, some sufficient conditions are formulated such that the considered neural networks can possess multiple synchronization manifolds. Finally, the multistability and multisynchronization performance of the considered neural networks with different activation functions are illustrated by numerical examples.},
  archive      = {J_NN},
  author       = {Libiao Peng and Xifeng Li and Dongjie Bi and Xuan Xie and Yongle Xie},
  doi          = {10.1016/j.neunet.2021.08.029},
  journal      = {Neural Networks},
  pages        = {372-383},
  shortjournal = {Neural Netw.},
  title        = {Pinning multisynchronization of delayed fractional-order memristor-based neural networks with nonlinear coupling and almost-periodic perturbations},
  volume       = {144},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Physics-incorporated convolutional recurrent neural networks
for source identification and forecasting of dynamical systems.
<em>NN</em>, <em>144</em>, 359–371. (<a
href="https://doi.org/10.1016/j.neunet.2021.08.033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatio-temporal dynamics of physical processes are generally modeled using partial differential equations (PDEs). Though the core dynamics follows some principles of physics, real-world physical processes are often driven by unknown external sources. In such cases, developing a purely analytical model becomes very difficult and data-driven modeling can be of assistance. In this paper, we present a hybrid framework combining physics-based numerical models with deep learning for source identification and forecasting of spatio-temporal dynamical systems with unobservable time-varying external sources. We formulate our model PhICNet as a convolutional recurrent neural network (RNN) which is end-to-end trainable for spatio-temporal evolution prediction of dynamical systems and learns the source behavior as an internal state of the RNN. Experimental results show that the proposed model can forecast the dynamics for a relatively long time and identify the sources as well.},
  archive      = {J_NN},
  author       = {Priyabrata Saha and Saurabh Dash and Saibal Mukhopadhyay},
  doi          = {10.1016/j.neunet.2021.08.033},
  journal      = {Neural Networks},
  pages        = {359-371},
  shortjournal = {Neural Netw.},
  title        = {Physics-incorporated convolutional recurrent neural networks for source identification and forecasting of dynamical systems},
  volume       = {144},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep learning based spectral CT imaging. <em>NN</em>,
<em>144</em>, 342–358. (<a
href="https://doi.org/10.1016/j.neunet.2021.08.026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spectral computed tomography (CT) has attracted much attention in radiation dose reduction, metal artifacts removal, tissue quantification and material discrimination. The x-ray energy spectrum is divided into several bins, each energy-bin-specific projection has a low signal-noise-ratio (SNR) than the current-integrating counterpart, which makes image reconstruction a unique challenge. Traditional wisdom is to use prior knowledge based iterative methods. However, this kind of methods demands a great computational cost. Inspired by deep learning , here we first develop a deep learning based reconstruction method; i.e., U -net with L p p Lpp -norm, T otal variation, R esidual learning, and A nisotropic adaption (ULTRA). Specifically, we emphasize the various multi-scale feature fusion and multichannel filtering enhancement with a denser connection encoding architecture for residual learning and feature fusion . To address the image deblurring problem associated with the L 2 2 L22 - loss, we propose a general L p p Lpp -loss, p &gt; 0 p&amp;gt;0 . Furthermore, the images from different energy bins share similar structures of the same object, the regularization characterizing correlations of different energy bins is incorporated into the L p p Lpp - loss function, which helps unify the deep learning based methods with traditional compressed sensing based methods. Finally, the anisotropically weighted total variation is employed to characterize the sparsity in the spatial–spectral domain to regularize the proposed network In particular, we validate our ULTRA networks on three large-scale spectral CT datasets, and obtain excellent results relative to the competing algorithms. In conclusion, our quantitative and qualitative results in numerical simulation and preclinical experiments demonstrate that our proposed approach is accurate, efficient and robust for high-quality spectral CT image reconstruction.},
  archive      = {J_NN},
  author       = {Weiwen Wu and Dianlin Hu and Chuang Niu and Lieza Vanden Broeke and Anthony P.H. Butler and Peng Cao and James Atlas and Alexander Chernoglazov and Varut Vardhanabhuti and Ge Wang},
  doi          = {10.1016/j.neunet.2021.08.026},
  journal      = {Neural Networks},
  pages        = {342-358},
  shortjournal = {Neural Netw.},
  title        = {Deep learning based spectral CT imaging},
  volume       = {144},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Working memory connections for LSTM. <em>NN</em>,
<em>144</em>, 334–341. (<a
href="https://doi.org/10.1016/j.neunet.2021.08.030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recurrent Neural Networks with Long Short-Term Memory (LSTM) make use of gating mechanisms to mitigate exploding and vanishing gradients when learning long-term dependencies. For this reason, LSTMs and other gated RNNs are widely adopted, being the standard de facto for many sequence modeling tasks. Although the memory cell inside the LSTM contains essential information, it is not allowed to influence the gating mechanism directly. In this work, we improve the gate potential by including information coming from the internal cell state. The proposed modification, named Working Memory Connection, consists in adding a learnable nonlinear projection of the cell content into the network gates. This modification can fit into the classical LSTM gates without any assumption on the underlying task, being particularly effective when dealing with longer sequences. Previous research effort in this direction, which goes back to the early 2000s, could not bring a consistent improvement over vanilla LSTM. As part of this paper, we identify a key issue tied to previous connections that heavily limits their effectiveness, hence preventing a successful integration of the knowledge coming from the internal cell state. We show through extensive experimental evaluation that Working Memory Connections constantly improve the performance of LSTMs on a variety of tasks. Numerical results suggest that the cell state contains useful information that is worth including in the gate structure.},
  archive      = {J_NN},
  author       = {Federico Landi and Lorenzo Baraldi and Marcella Cornia and Rita Cucchiara},
  doi          = {10.1016/j.neunet.2021.08.030},
  journal      = {Neural Networks},
  pages        = {334-341},
  shortjournal = {Neural Netw.},
  title        = {Working memory connections for LSTM},
  volume       = {144},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Nonlinear tensor train format for deep neural network
compression. <em>NN</em>, <em>144</em>, 320–333. (<a
href="https://doi.org/10.1016/j.neunet.2021.08.028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural network (DNN) compression has become a hot topic in the research of deep learning since the scale of modern DNNs turns into too huge to implement on practical resource constrained platforms such as embedded devices. Among variant compression methods , tensor decomposition appears to be a relatively simple and efficient strategy owing to its solid mathematical foundations and regular data structure . Generally, tensorizing neural weights into higher-order tensors for better decomposition, and directly mapping efficient tensor structure to neural architecture with nonlinear activation functions , are the two most common ways. However, the considerable accuracy loss is still a fly in the ointment for the tensorizing way especially for convolutional neural networks (CNNs), while the number of studies in the mapping way is comparatively limited and corresponding compression ratio appears to be not considerable. Therefore, in this work, by researching multiple types of tensor decompositions, we realize that tensor train (TT), which has specific and efficient sequenced contractions, is potential to take into account both of tensorizing and mapping ways. Then we propose a novel nonlinear tensor train (NTT) format, which contains extra nonlinear activation functions embedded in sequenced contractions and convolutions on the top of the normal TT decomposition and the proposed TT format connected by convolutions , to compensate the accuracy loss that normal TT cannot give. Further than just shrinking the space complexity of original weight matrices and convolutional kernels, we prove that NTT can afford an efficient inference time as well. Extensive experiments and discussions demonstrate that the compressed DNNs in our NTT format can almost maintain the accuracy at least on MNIST, UCF11 and CIFAR-10 datasets, and the accuracy loss caused by normal TT could be compensated significantly on large-scale datasets such as ImageNet.},
  archive      = {J_NN},
  author       = {Dingheng Wang and Guangshe Zhao and Hengnu Chen and Zhexian Liu and Lei Deng and Guoqi Li},
  doi          = {10.1016/j.neunet.2021.08.028},
  journal      = {Neural Networks},
  pages        = {320-333},
  shortjournal = {Neural Netw.},
  title        = {Nonlinear tensor train format for deep neural network compression},
  volume       = {144},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bipartite synchronization of signed networks via
aperiodically intermittent control based on discrete-time state
observations. <em>NN</em>, <em>144</em>, 307–319. (<a
href="https://doi.org/10.1016/j.neunet.2021.08.035">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, bipartite synchronization of signed networks with stochastic disturbances via aperiodically intermittent control is investigated. The aperiodically intermittent control presented is based on discrete-time state observations rather than continuous-time ones. To formulate signed networks and exhibit the competitive relation, a structurally balanced signed network is built and all the units are divided into two subcommunities. By employing Lyapunov method and graph theory, some sufficient conditions on bipartite synchronization are given. Meanwhile, when aperiodically intermittent control degenerates into periodically intermittent control and feedback control respectively, two corollaries are also provided to ensure the bipartite synchronization of the signed networks. Ultimately, two applications to coupled single-link robot arms and coupled oscillators are presented and corresponding numerical examples are respectively provided to verify the feasibility and effectiveness of the theoretical results.},
  archive      = {J_NN},
  author       = {Dongsheng Xu and Jiahuan Pang and Huan Su},
  doi          = {10.1016/j.neunet.2021.08.035},
  journal      = {Neural Networks},
  pages        = {307-319},
  shortjournal = {Neural Netw.},
  title        = {Bipartite synchronization of signed networks via aperiodically intermittent control based on discrete-time state observations},
  volume       = {144},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Recurrent neural network from adder’s perspective:
Carry-lookahead RNN. <em>NN</em>, <em>144</em>, 297–306. (<a
href="https://doi.org/10.1016/j.neunet.2021.08.032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recurrent network architecture is a widely used model in sequence modeling, but its serial dependency hinders the computation parallelization , which makes the operation inefficient. The same problem was encountered in serial adder at the early stage of digital electronics. In this paper, we discuss the similarities between recurrent neural network (RNN) and serial adder . Inspired by carry-lookahead adder, we introduce carry-lookahead module to RNN, which makes it possible for RNN to run in parallel. Then, we design the method of parallel RNN computation, and finally Carry-lookahead RNN (CL-RNN) is proposed. CL-RNN takes advantages in parallelism and flexible receptive field. Through a comprehensive set of tests, we verify that CL-RNN can perform better than existing typical RNNs in sequence modeling tasks which are specially designed for RNNs. Code and models are available at: https://github.com/WinnieJiangHW/Carry-lookahead_RNN .},
  archive      = {J_NN},
  author       = {Haowei Jiang and Feiwei Qin and Jin Cao and Yong Peng and Yanli Shao},
  doi          = {10.1016/j.neunet.2021.08.032},
  journal      = {Neural Networks},
  pages        = {297-306},
  shortjournal = {Neural Netw.},
  title        = {Recurrent neural network from adder’s perspective: Carry-lookahead RNN},
  volume       = {144},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep tobit networks: A novel machine learning approach to
microeconometrics. <em>NN</em>, <em>144</em>, 279–296. (<a
href="https://doi.org/10.1016/j.neunet.2021.09.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tobit models (also called as “censored regression models” or classified as “sample selection models” in microeconometrics) have been widely applied to microeconometric problems with censored outcomes. However, due to their linear parametric settings and restrictive normality assumptions, the traditional Tobit models fail to capture the pervading nonlinearities and thus may be inadequate for microeconometric analysis with large-scale datasets. This paper proposes two novel deep neural networks for Tobit problems and explores machine learning approaches in the context of microeconometric modeling. We connect the censored outputs in Tobit models with some deep learning techniques, which are thought to be unrelated to microeconometrics, and use the rectified linear unit activation and a particularly designed network structure to implement the censored output mechanisms and realize the underlying econometric conceptions. The benchmark Tobit-I and Tobit-II models are then reformulated as two carefully designed deep feedforward neural networks named deep Tobit-I network and deep Tobit-II network, respectively. A novel significance testing method is developed based on the proposed networks. Compared with the traditional models, our networks with deep structures can effectively describe the underlying highly nonlinear relationships and achieve considerable improvements in fitting and prediction. With the novel testing method, the proposed networks enable highly accurate and sophisticated econometric analysis with minimal random assumptions. The encouraging numerical experiments on synthetic and realistic datasets demonstrate the utility and advantages of the proposed method.},
  archive      = {J_NN},
  author       = {Jiaming Zhang and Zhanfeng Li and Xinyuan Song and Hanwen Ning},
  doi          = {10.1016/j.neunet.2021.09.003},
  journal      = {Neural Networks},
  pages        = {279-296},
  shortjournal = {Neural Netw.},
  title        = {Deep tobit networks: A novel machine learning approach to microeconometrics},
  volume       = {144},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Biologically motivated learning method for deep neural
networks using hierarchical competitive learning. <em>NN</em>,
<em>144</em>, 271–278. (<a
href="https://doi.org/10.1016/j.neunet.2021.08.027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study proposes a novel biologically motivated learning method for deep convolutional neural networks (CNNs). The combination of CNNs and backpropagation learning is the most powerful method in recent machine learning regimes. However, it requires a large amount of labeled data for training, and this requirement can occasionally become a barrier for real world applications . To address this problem and use unlabeled data , we introduce unsupervised competitive learning, which only requires forward propagating signals for CNNs. The method was evaluated on image discrimination tasks using the MNIST, CIFAR-10, and ImageNet datasets , and it achieved state-of-the-art performance with respect to other biologically motivated methods in the ImageNet benchmark. The results suggest that the method enables higher-level learning representations solely based on the forward propagating signals without the need for a backward error signal for training convolutional layers . The proposed method could be useful for a variety of poorly labeled data, for example, time series or medical data.},
  archive      = {J_NN},
  author       = {Takashi Shinozaki},
  doi          = {10.1016/j.neunet.2021.08.027},
  journal      = {Neural Networks},
  pages        = {271-278},
  shortjournal = {Neural Netw.},
  title        = {Biologically motivated learning method for deep neural networks using hierarchical competitive learning},
  volume       = {144},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Incremental multi-view spectral clustering with sparse and
connected graph learning. <em>NN</em>, <em>144</em>, 260–270. (<a
href="https://doi.org/10.1016/j.neunet.2021.08.031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, a lot of excellent multi-view clustering methods have been proposed. Because most of them need to fuse all views at one time, they are infeasible as the number of views increases over time. If the present multi-view clustering methods are employed directly to re-fuse all views at each time, it is too expensive to store all historical views. In this paper, we proposed an efficient incremental multi-view spectral clustering method with sparse and connected graph learning (SCGL). In our method, only one consensus similarity matrix is stored to represent the structural information of all historical views. Once the newly collected view is available, the consensus similarity matrix is reconstructed by learning from its previous version and the current new view. To further improve the incremental multi-view clustering performance, the sparse graph learning and the connected graph learning are integrated into our model, which can not only reduce the noises, but also preserve the correct connections within clusters. Experiments on several multi-view datasets demonstrate that our method is superior to traditional methods in clustering accuracy, and is more suitable to deal with the multi-view clustering with the number of views increasing over time.},
  archive      = {J_NN},
  author       = {Hongwei Yin and Wenjun Hu and Zhao Zhang and Jungang Lou and Minmin Miao},
  doi          = {10.1016/j.neunet.2021.08.031},
  journal      = {Neural Networks},
  pages        = {260-270},
  shortjournal = {Neural Netw.},
  title        = {Incremental multi-view spectral clustering with sparse and connected graph learning},
  volume       = {144},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). One-stage CNN detector-based benthonic organisms detection
with limited training dataset. <em>NN</em>, <em>144</em>, 247–259. (<a
href="https://doi.org/10.1016/j.neunet.2021.08.014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, focusing on the challenges in unique shape dimension and limited training dataset of benthonic organisms, an one-stage CNN detector-based benthonic organisms detection (OSCD-BOD) scheme is proposed. Main contributions are as follows: (1) The regression loss between the predicted bounding box and ground truth box is innovatively measured by the generalized intersection over union (GIoU), such that localization accuracy of benthonic organisms is dramatically enhanced. (2) By devising K-means-based dimension clustering, multiple benthonic organisms anchor boxes (BOAB) sufficiently exploring a priori dimension information can be finely derived from limited training dataset, and thereby significantly promoting the recall ability. (3) Geometric and color transformations (GCT)-based data augmentation technique is further resorted to not only efficiently prevent over-fitting training but also to significantly enhance detection generalization in complex and changeable underwater environments. (4) The OSCD-BOD scheme is eventually established in a modular manner by integrating GIoU, BOAB and GCT functionals. Comprehensive experiments and comparisons sufficiently demonstrate that the proposed OSCD-BOD scheme outperforms typical approaches including Faster R-CNN, SSD, YOLOv2, YOLOv3 and CenterNet in terms of mean average precision by 6.88\%, 10.92\%, 12.44\%, 3.05\% and 1.09\%, respectively.},
  archive      = {J_NN},
  author       = {Tingkai Chen and Ning Wang and Rongfeng Wang and Hong Zhao and Guichen Zhang},
  doi          = {10.1016/j.neunet.2021.08.014},
  journal      = {Neural Networks},
  pages        = {247-259},
  shortjournal = {Neural Netw.},
  title        = {One-stage CNN detector-based benthonic organisms detection with limited training dataset},
  volume       = {144},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An empirical evaluation of active inference in multi-armed
bandits. <em>NN</em>, <em>144</em>, 229–246. (<a
href="https://doi.org/10.1016/j.neunet.2021.08.018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A key feature of sequential decision making under uncertainty is a need to balance between exploiting—choosing the best action according to the current knowledge, and exploring—obtaining information about values of other actions. The multi-armed bandit problem, a classical task that captures this trade-off, served as a vehicle in machine learning for developing bandit algorithms that proved to be useful in numerous industrial applications. The active inference framework, an approach to sequential decision making recently developed in neuroscience for understanding human and animal behaviour , is distinguished by its sophisticated strategy for resolving the exploration–exploitation trade-off. This makes active inference an exciting alternative to already established bandit algorithms. Here we derive an efficient and scalable approximate active inference algorithm and compare it to two state-of-the-art bandit algorithms: Bayesian upper confidence bound and optimistic Thompson sampling. This comparison is done on two types of bandit problems: a stationary and a dynamic switching bandit. Our empirical evaluation shows that the active inference algorithm does not produce efficient long-term behaviour in stationary bandits. However, in the more challenging switching bandit problem active inference performs substantially better than the two state-of-the-art bandit algorithms. The results open exciting venues for further research in theoretical and applied machine learning , as well as lend additional credibility to active inference as a general framework for studying human and animal behaviour.},
  archive      = {J_NN},
  author       = {Dimitrije Marković and Hrvoje Stojić and Sarah Schwöbel and Stefan J. Kiebel},
  doi          = {10.1016/j.neunet.2021.08.018},
  journal      = {Neural Networks},
  pages        = {229-246},
  shortjournal = {Neural Netw.},
  title        = {An empirical evaluation of active inference in multi-armed bandits},
  volume       = {144},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Performance of biologically grounded models of the early
visual system on standard object recognition tasks. <em>NN</em>,
<em>144</em>, 210–228. (<a
href="https://doi.org/10.1016/j.neunet.2021.08.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computational neuroscience models of vision and neural network models for object recognition are often framed by different research agendas. Computational neuroscience mainly aims at replicating experimental data , while (artificial) neural networks target high performance on classification tasks . However, we propose that models of vision should be validated on object recognition tasks . At some point, mechanisms of realistic neuro-computational models of the visual cortex have to convince in object recognition as well. In order to foster this idea, we report the recognition accuracy for two different neuro-computational models of the visual cortex on several object recognition datasets. The models were trained using unsupervised Hebbian learning rules on natural scene inputs for the emergence of receptive fields comparable to their biological counterpart. We assume that the emerged receptive fields result in a general codebook of features, which should be applicable to a variety of visual scenes. We report the performances on datasets with different levels of difficulty, ranging from the simple MNIST to the more complex CIFAR-10 or ETH-80. We found that both networks show good results on simple digit recognition, comparable with previously published biologically plausible models. We also observed that our deeper layer neurons provide for naturalistic datasets a better recognition codebook. As for most datasets, recognition results of biologically grounded models are not available yet, our results provide a broad basis of performance values to compare methodologically similar models.},
  archive      = {J_NN},
  author       = {Michael Teichmann and René Larisch and Fred H. Hamker},
  doi          = {10.1016/j.neunet.2021.08.009},
  journal      = {Neural Networks},
  pages        = {210-228},
  shortjournal = {Neural Netw.},
  title        = {Performance of biologically grounded models of the early visual system on standard object recognition tasks},
  volume       = {144},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adversarial text-to-image synthesis: A review. <em>NN</em>,
<em>144</em>, 187–209. (<a
href="https://doi.org/10.1016/j.neunet.2021.07.019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advent of generative adversarial networks , synthesizing images from text descriptions has recently become an active research area. It is a flexible and intuitive way for conditional image generation with significant progress in the last years regarding visual realism, diversity, and semantic alignment. However, the field still faces several challenges that require further research efforts such as enabling the generation of high-resolution images with multiple objects, and developing suitable and reliable evaluation metrics that correlate with human judgement. In this review, we contextualize the state of the art of adversarial text-to-image synthesis models, their development since their inception five years ago, and propose a taxonomy based on the level of supervision. We critically examine current strategies to evaluate text-to-image synthesis models, highlight shortcomings, and identify new areas of research, ranging from the development of better datasets and evaluation metrics to possible improvements in architectural design and model training. This review complements previous surveys on generative adversarial networks with a focus on text-to-image synthesis which we believe will help researchers to further advance the field.},
  archive      = {J_NN},
  author       = {Stanislav Frolov and Tobias Hinz and Federico Raue and Jörn Hees and Andreas Dengel},
  doi          = {10.1016/j.neunet.2021.07.019},
  journal      = {Neural Networks},
  pages        = {187-209},
  shortjournal = {Neural Netw.},
  title        = {Adversarial text-to-image synthesis: A review},
  volume       = {144},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Neural-network-based discounted optimal control via an
integrated value iteration with accuracy guarantee. <em>NN</em>,
<em>144</em>, 176–186. (<a
href="https://doi.org/10.1016/j.neunet.2021.08.025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A data-based value iteration algorithm with the bidirectional approximation feature is developed for discounted optimal control . The unknown nonlinear system dynamics is first identified by establishing a model neural network . To improve the identification precision, biases are introduced to the model network. The model network with biases is trained by the gradient descent algorithm, where the weights and biases across all layers are updated. The uniform ultimate boundedness stability with a proper learning rate is analyzed, by using the Lyapunov approach. Moreover, an integrated value iteration with the discounted cost is developed to fully guarantee the approximation accuracy of the optimal value function. Then, the effectiveness of the proposed algorithm is demonstrated by carrying out two simulation examples with physical backgrounds.},
  archive      = {J_NN},
  author       = {Mingming Ha and Ding Wang and Derong Liu},
  doi          = {10.1016/j.neunet.2021.08.025},
  journal      = {Neural Networks},
  pages        = {176-186},
  shortjournal = {Neural Netw.},
  title        = {Neural-network-based discounted optimal control via an integrated value iteration with accuracy guarantee},
  volume       = {144},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Predictive coding feedback results in perceived illusory
contours in a recurrent neural network. <em>NN</em>, <em>144</em>,
164–175. (<a
href="https://doi.org/10.1016/j.neunet.2021.08.024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern feedforward convolutional neural networks (CNNs) can now solve some computer vision tasks at super-human levels. However, these networks only roughly mimic human visual perception. One difference from human vision is that they do not appear to perceive illusory contours (e.g. Kanizsa squares) in the same way humans do. Physiological evidence from visual cortex suggests that the perception of illusory contours could involve feedback connections . Would recurrent feedback neural networks perceive illusory contours like humans? In this work we equip a deep feedforward convolutional network with brain-inspired recurrent dynamics. The network was first pretrained with an unsupervised reconstruction objective on a natural image dataset, to expose it to natural object contour statistics . Then, a classification decision head was added and the model was finetuned on a form discrimination task: squares vs. randomly oriented inducer shapes (no illusory contour). Finally, the model was tested with the unfamiliar “illusory contour” configuration: inducer shapes oriented to form an illusory square. Compared with feedforward baselines, the iterative “predictive coding” feedback resulted in more illusory contours being classified as physical squares. The perception of the illusory contour was measurable in the luminance profile of the image reconstructions produced by the model, demonstrating that the model really “sees” the illusion. Ablation studies revealed that natural image pretraining and feedback error correction are both critical to the perception of the illusion. Finally we validated our conclusions in a deeper network (VGG): adding the same predictive coding feedback dynamics again leads to the perception of illusory contours.},
  archive      = {J_NN},
  author       = {Zhaoyang Pang and Callum Biggs O’May and Bhavin Choksi and Rufin VanRullen},
  doi          = {10.1016/j.neunet.2021.08.024},
  journal      = {Neural Networks},
  pages        = {164-175},
  shortjournal = {Neural Netw.},
  title        = {Predictive coding feedback results in perceived illusory contours in a recurrent neural network},
  volume       = {144},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adversarial parameter defense by multi-step risk
minimization. <em>NN</em>, <em>144</em>, 154–163. (<a
href="https://doi.org/10.1016/j.neunet.2021.08.022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Previous studies demonstrate DNNs’ vulnerability to adversarial examples and adversarial training can establish a defense to adversarial examples . In addition, recent studies show that deep neural networks also exhibit vulnerability to parameter corruptions. The vulnerability of model parameters is of crucial value to the study of model robustness and generalization. In this work, we introduce the concept of parameter corruption and propose to leverage the loss change indicators for measuring the flatness of the loss basin and the parameter robustness of neural network parameters. On such basis, we analyze parameter corruptions and propose the multi-step adversarial corruption algorithm. To enhance neural networks, we propose the adversarial parameter defense algorithm that minimizes the average risk of multiple adversarial parameter corruptions. Experimental results show that the proposed algorithm can improve both the parameter robustness and accuracy of neural networks.},
  archive      = {J_NN},
  author       = {Zhiyuan Zhang and Ruixuan Luo and Xuancheng Ren and Qi Su and Liangyou Li and Xu Sun},
  doi          = {10.1016/j.neunet.2021.08.022},
  journal      = {Neural Networks},
  pages        = {154-163},
  shortjournal = {Neural Netw.},
  title        = {Adversarial parameter defense by multi-step risk minimization},
  volume       = {144},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Forward and inverse reinforcement learning sharing network
weights and hyperparameters. <em>NN</em>, <em>144</em>, 138–153. (<a
href="https://doi.org/10.1016/j.neunet.2021.08.017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes model-free imitation learning named Entropy-Regularized Imitation Learning (ERIL) that minimizes the reverse Kullback–Leibler (KL) divergence. ERIL combines forward and inverse reinforcement learning (RL) under the framework of an entropy-regularized Markov decision process . An inverse RL step computes the log-ratio between two distributions by evaluating two binary discriminators . The first discriminator distinguishes the state generated by the forward RL step from the expert’s state. The second discriminator, which is structured by the theory of entropy regularization , distinguishes the state–action–next-state tuples generated by the learner from the expert ones. One notable feature is that the second discriminator shares hyperparameters with the forward RL, which can be used to control the discriminator’s ability. A forward RL step minimizes the reverse KL estimated by the inverse RL step. We show that minimizing the reverse KL divergence is equivalent to finding an optimal policy . Our experimental results on MuJoCo-simulated environments and vision-based reaching tasks with a robotic arm show that ERIL is more sample-efficient than the baseline methods . We apply the method to human behaviors that perform a pole-balancing task and describe how the estimated reward functions show how every subject achieves her goal.},
  archive      = {J_NN},
  author       = {Eiji Uchibe and Kenji Doya},
  doi          = {10.1016/j.neunet.2021.08.017},
  journal      = {Neural Networks},
  pages        = {138-153},
  shortjournal = {Neural Netw.},
  title        = {Forward and inverse reinforcement learning sharing network weights and hyperparameters},
  volume       = {144},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An end-to-end 3D convolutional neural network for decoding
attentive mental state. <em>NN</em>, <em>144</em>, 129–137. (<a
href="https://doi.org/10.1016/j.neunet.2021.08.019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The detection of attentive mental state plays an essential role in the neurofeedback process and the treatment of Attention Deficit and Hyperactivity Disorder (ADHD). However, the performance of the detection methods is still not satisfactory. One of the challenges is to find a proper representation for the electroencephalogram (EEG) data, which could preserve the temporal information and maintain the spatial topological characteristics. Inspired by the deep learning (DL) methods in the research of brain–computer interface (BCI) field, a 3D representation of EEG signal was introduced into attention detection task, and a 3D convolutional neural network model with cascade and parallel convolution operations was proposed. The model utilized three cascade blocks, each consisting of two parallel 3D convolution branches, to simultaneously extract the multi-scale features. Evaluated on a public dataset containing twenty-six subjects, the proposed model achieved better performance compared with the baseline methods under the intra-subject, inter-subject and subject-adaptive classification scenarios. This study demonstrated the promising potential of the 3D CNN model for detecting attentive mental state.},
  archive      = {J_NN},
  author       = {Yangsong Zhang and Huan Cai and Li Nie and Peng Xu and Sirui Zhao and Cuntai Guan},
  doi          = {10.1016/j.neunet.2021.08.019},
  journal      = {Neural Networks},
  pages        = {129-137},
  shortjournal = {Neural Netw.},
  title        = {An end-to-end 3D convolutional neural network for decoding attentive mental state},
  volume       = {144},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Extremely randomized neural networks for constructing
prediction intervals. <em>NN</em>, <em>144</em>, 113–128. (<a
href="https://doi.org/10.1016/j.neunet.2021.08.020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The aim of this paper is to propose a novel prediction model based on an ensemble of deep neural networks adapting the extremely randomized trees method originally developed for random forests . The extra-randomness introduced in the ensemble reduces the variance of the predictions and improves out-of-sample accuracy. As a byproduct, we are able to compute the uncertainty about our model predictions and construct interval forecasts. Some of the limitations associated with bootstrap-based algorithms can be overcome by not performing data resampling and thus, by ensuring the suitability of the methodology in low and mid-dimensional settings, or when the i . i . d . i.i.d. assumption does not hold. An extensive Monte Carlo simulation exercise shows the good performance of this novel prediction method in terms of mean square prediction error and the accuracy of the prediction intervals in terms of out-of-sample prediction interval coverage probabilities . The advanced approach delivers better out-of-sample accuracy in experimental settings, improving upon state-of-the-art methods like MC dropout and bootstrap procedures.},
  archive      = {J_NN},
  author       = {Tullio Mancini and Hector Calvo-Pardo and Jose Olmo},
  doi          = {10.1016/j.neunet.2021.08.020},
  journal      = {Neural Networks},
  pages        = {113-128},
  shortjournal = {Neural Netw.},
  title        = {Extremely randomized neural networks for constructing prediction intervals},
  volume       = {144},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Observer-based event-triggered control for zero-sum games of
input constrained multi-player nonlinear systems. <em>NN</em>,
<em>144</em>, 101–112. (<a
href="https://doi.org/10.1016/j.neunet.2021.08.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, an event-triggered control (ETC) method is investigated to solve zero-sum game (ZSG) problems of unknown multi-player continuous-time nonlinear systems with input constraints by using adaptive dynamic programming (ADP). To relax the requirement of system dynamics , a neural network (NN) observer is constructed to identify the dynamics of multi-player system via the input and output data. Then, the event-triggered Hamilton–Jacobi–Isaacs (HJI) equation of the ZSG can be solved by constructing a critic NN, and the approximated optimal control law and the worst disturbance law can be obtained directly. A triggering scheme which determines the updating time instants of the control law and the disturbance law is developed. Thus, the proposed ADP-based ETC method cannot only reduce the computational burden, but also save communication resource and bandwidths. Furthermore, we prove that the signals of the closed-loop system and the approximate errors of the critic NN weights are uniformly ultimately bounded by using Lyapunov’s direct method, and the Zeno behavior is excluded. Finally, two simulation examples are provided to demonstrate the effectiveness of the proposed ETC scheme.},
  archive      = {J_NN},
  author       = {Shunchao Zhang and Bo Zhao and Derong Liu and Yongwei Zhang},
  doi          = {10.1016/j.neunet.2021.08.012},
  journal      = {Neural Networks},
  pages        = {101-112},
  shortjournal = {Neural Netw.},
  title        = {Observer-based event-triggered control for zero-sum games of input constrained multi-player nonlinear systems},
  volume       = {144},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A neural decoding algorithm that generates language from
visual activity evoked by natural images. <em>NN</em>, <em>144</em>,
90–100. (<a href="https://doi.org/10.1016/j.neunet.2021.08.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transforming neural activities into language is revolutionary for human–computer interaction as well as functional restoration of aphasia . Present rapid development of artificial intelligence makes it feasible to decode the neural signals of human visual activities. In this paper, a novel Progressive Transfer Language Decoding Model (PT-LDM) is proposed to decode visual fMRI signals into phrases or sentences when natural images are being watched. The PT-LDM consists of an image-encoder, a fMRI encoder and a language-decoder. The results showed that phrases and sentences were successfully generated from visual activities. Similarity analysis showed that three often-used evaluation indexes BLEU, ROUGE and CIDEr reached 0.182, 0.197 and 0.680 averagely between the generated texts and the corresponding annotated texts in the testing set respectively, significantly higher than the baseline. Moreover, we found that higher visual areas usually had better performance than lower visual areas and the contribution curve of visual response patterns in language decoding varied at successively different time points . Our findings demonstrate that the neural representations elicited in visual cortices when scenes are being viewed have already contained semantic information that can be utilized to generate human language. Our study shows potential application of language-based brain–machine interfaces in the future, especially for assisting aphasics in communicating more efficiently with fMRI signals .},
  archive      = {J_NN},
  author       = {Wei Huang and Hongmei Yan and Kaiwen Cheng and Chong Wang and Jiyi Li and Yuting Wang and Chen Li and Chaorong Li and Yunhan Li and Zhentao Zuo and Huafu Chen},
  doi          = {10.1016/j.neunet.2021.08.006},
  journal      = {Neural Networks},
  pages        = {90-100},
  shortjournal = {Neural Netw.},
  title        = {A neural decoding algorithm that generates language from visual activity evoked by natural images},
  volume       = {144},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Non-differentiable saddle points and sub-optimal local
minima exist for deep ReLU networks. <em>NN</em>, <em>144</em>, 75–89.
(<a href="https://doi.org/10.1016/j.neunet.2021.08.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Whether sub-optimal local minima and saddle points exist in the highly non-convex loss landscape of deep neural networks has a great impact on the performance of optimization algorithms . Theoretically, we study in this paper the existence of non-differentiable sub-optimal local minima and saddle points for deep ReLU networks with arbitrary depth. We prove that there always exist non-differentiable saddle points in the loss surface of deep ReLU networks with squared loss or cross-entropy loss under reasonable assumptions. We also prove that deep ReLU networks with cross-entropy loss will have non-differentiable sub-optimal local minima if some outermost samples do not belong to a certain class. Experimental results on real and synthetic datasets verify our theoretical findings.},
  archive      = {J_NN},
  author       = {Bo Liu and Zhaoying Liu and Ting Zhang and Tongtong Yuan},
  doi          = {10.1016/j.neunet.2021.08.005},
  journal      = {Neural Networks},
  pages        = {75-89},
  shortjournal = {Neural Netw.},
  title        = {Non-differentiable saddle points and sub-optimal local minima exist for deep ReLU networks},
  volume       = {144},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). TGAN: A simple model update strategy for visual tracking via
template-guidance attention network. <em>NN</em>, <em>144</em>, 61–74.
(<a href="https://doi.org/10.1016/j.neunet.2021.08.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual attention has been widely used in various fields of visual tasks in recent years. Recently, visual trackers based on probabilistic discriminative model prediction (PrDiMP) and Siamese box adaptive network (SiamBAN) have attracted much attention due to their excellent performance and high efficiency. However, the target template of the model in both the PrDiMP and SiamBAN is not updated online, and feature vectors of the template image and the search image are independent of each other in the IoU-Net and Siamese frameworks. In this research, we proposed a template-guidance attention network in both the IoU-Net (denoted as TGAN-I) and Siamese (denoted as TGAN-S) frameworks for visual tracking. TGAN-I and TGAN-S can comprehensively utilize the feature information of the template image and search image, and provide an implicit way to update the template. By utilizing a simple template update strategy, the TGAN-I and TGAN-S trackers can be more robust under certain challenging conditions such as occlusion and deformation. Besides, we introduce a channel and spatial attention module in feature maps of the template image and search image for adaptive feature refinement. Deformable convolutional networks are further used to enhance the model generalization capability in various transformations aspect ratios and scales of tracking targets. To verify the effectiveness of the proposed method, we evaluate the TGAN-I and TGAN-S trackers on six benchmarks and achieve state-of-the-art results. In particular, the TGAN-I method outperforms the strong baseline, PrDiMP, by 0.323 → → 0.355 and 0.471 → → 0.501 of EAO score on VOT2019 and VOT2016, respectively.},
  archive      = {J_NN},
  author       = {Kai Yang and Haijun Zhang and Dongliang Zhou and Linlin Liu},
  doi          = {10.1016/j.neunet.2021.08.010},
  journal      = {Neural Networks},
  pages        = {61-74},
  shortjournal = {Neural Netw.},
  title        = {TGAN: A simple model update strategy for visual tracking via template-guidance attention network},
  volume       = {144},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Schematic memory persistence and transience for efficient
and robust continual learning. <em>NN</em>, <em>144</em>, 49–60. (<a
href="https://doi.org/10.1016/j.neunet.2021.08.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Continual learning is considered a promising step toward next-generation Artificial Intelligence (AI), where deep neural networks (DNNs) make decisions by continuously learning a sequence of different tasks akin to human learning processes. It is still quite primitive, with existing works focusing primarily on avoiding (catastrophic) forgetting. However, since forgetting is inevitable given bounded memory and unbounded task loads, ‘how to reasonably forget’ is a problem continual learning must address in order to reduce the performance gap between AIs and humans, in terms of (1) memory efficiency, (2) generalizability , and (3) robustness when dealing with noisy data. To address this, we propose a novel ScheMAtic memory peRsistence and Transience (SMART) 1 framework for continual learning with external memory that builds on recent advances in neuroscience . The efficiency and generalizability are enhanced by a novel long-term forgetting mechanism and schematic memory, using sparsity and ‘backward positive transfer’ constraints with theoretical guarantees on the error bound. Robust enhancement is achieved using a novel short-term forgetting mechanism inspired by background information-gated learning. Finally, an extensive experimental analysis on both benchmark and real-world datasets demonstrates the effectiveness and efficiency of our model.},
  archive      = {J_NN},
  author       = {Yuyang Gao and Giorgio A. Ascoli and Liang Zhao},
  doi          = {10.1016/j.neunet.2021.08.011},
  journal      = {Neural Networks},
  pages        = {49-60},
  shortjournal = {Neural Netw.},
  title        = {Schematic memory persistence and transience for efficient and robust continual learning},
  volume       = {144},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Distributed associative memory network with memory
refreshing loss. <em>NN</em>, <em>144</em>, 33–48. (<a
href="https://doi.org/10.1016/j.neunet.2021.07.030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite recent progress in memory augmented neural network (MANN) research, associative memory networks with a single external memory still show limited performance on complex relational reasoning tasks. Especially the content-based addressable memory networks often fail to encode input data into rich enough representation for relational reasoning and this limits the relation modeling performance of MANN for long temporal sequence data. To address these problems, here we introduce a novel Distributed Associative Memory architecture (DAM) with Memory Refreshing Loss (MRL) which enhances the relation reasoning performance of MANN. Inspired by how the human brain works, our framework encodes data with distributed representation across multiple memory blocks and repeatedly refreshes the contents for enhanced memorization similar to the rehearsal process of the brain. For this procedure, we replace a single external memory with a set of multiple smaller associative memory blocks and update these sub-memory blocks simultaneously and independently for the distributed representation of input data. Moreover, we propose MRL which assists a task’s target objective while learning relational information existing in data. MRL enables MANN to reinforce an association between input data and task objective by reproducing stochastically sampled input data from stored memory contents. With this procedure, MANN further enriches the stored representations with relational information. In experiments, we apply our approaches to Differential Neural Computer (DNC), which is one of the representative content-based addressing memory models and achieves the state-of-the-art performance on both memorization and relational reasoning tasks.},
  archive      = {J_NN},
  author       = {Taewon Park and Inchul Choi and Minho Lee},
  doi          = {10.1016/j.neunet.2021.07.030},
  journal      = {Neural Networks},
  pages        = {33-48},
  shortjournal = {Neural Netw.},
  title        = {Distributed associative memory network with memory refreshing loss},
  volume       = {144},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning lightweight super-resolution networks with weight
pruning. <em>NN</em>, <em>144</em>, 21–32. (<a
href="https://doi.org/10.1016/j.neunet.2021.08.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single image super-resolution (SISR) has achieved significant performance improvements due to the deep convolutional neural networks (CNN). However, the deep learning-based method is computationally intensive and memory demanding, which limit its practical deployment, especially for mobile devices . Focusing on this issue, in this paper, we present a novel approach to compress SR networks by weight pruning. To achieve this goal, firstly, we explore a progressive optimization method to gradually zero out the redundant parameters. Then, we construct a sparse-aware attention module by exploring a pruning-based well-suited attention strategy. Finally, we propose an information multi-slicing network which extracts and integrates multi-scale features at a granular level to acquire a more lightweight and accurate SR network. Extensive experiments reflect the pruning method could reduce the model size without a noticeable drop in performance, making it possible to apply the start-of-the-art SR models in the real-world applications. Furthermore, our proposed pruning versions could achieve better accuracy and visual improvements than state-of-the-art methods.},
  archive      = {J_NN},
  author       = {Xinrui Jiang and Nannan Wang and Jingwei Xin and Xiaobo Xia and Xi Yang and Xinbo Gao},
  doi          = {10.1016/j.neunet.2021.08.002},
  journal      = {Neural Networks},
  pages        = {21-32},
  shortjournal = {Neural Netw.},
  title        = {Learning lightweight super-resolution networks with weight pruning},
  volume       = {144},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Intermittent control for finite-time synchronization of
fractional-order complex networks. <em>NN</em>, <em>144</em>, 11–20. (<a
href="https://doi.org/10.1016/j.neunet.2021.08.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper is concerned with the finite-time synchronization problem for fractional-order complex dynamical networks (FCDNs) with intermittent control. Using the definition of Caputo’s fractional derivative and the properties of Beta function , the Caputo fractional-order derivative of the power function is evaluated. A general fractional-order intermittent differential inequality is obtained with fewer additional constraints. Then, the criteria are established for the finite-time convergence of FCDNs under intermittent feedback control , intermittent adaptive control and intermittent pinning control indicate that the setting time is related to order of FCDNs and initial conditions. Finally, these theoretical results are illustrated by numerical examples.},
  archive      = {J_NN},
  author       = {Lingzhong Zhang and Jie Zhong and Jianquan Lu},
  doi          = {10.1016/j.neunet.2021.08.004},
  journal      = {Neural Networks},
  pages        = {11-20},
  shortjournal = {Neural Netw.},
  title        = {Intermittent control for finite-time synchronization of fractional-order complex networks},
  volume       = {144},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). P-DIFF+: Improving learning classifier with noisy labels by
noisy negative learning loss. <em>NN</em>, <em>144</em>, 1–10. (<a
href="https://doi.org/10.1016/j.neunet.2021.07.024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning deep neural network (DNN) classifier with noisy labels is a challenging task because the DNN can easily over-fit on these noisy labels due to its high capability. In this paper, we present a very simple but effective training paradigm called P-DIFF+ , which can train DNN classifiers but obviously alleviate the adverse impact of noisy labels. Our proposed probability difference distribution implicitly reflects the probability of a training sample to be clean, then this probability is employed to re-weight the corresponding sample during the training process. Moreover, Noisy Negative Learning(NNL) loss can be further employed to re-weight samples. P-DIFF+ can achieve good performance even without prior-knowledge on the noise rate of training samples. Experiments on benchmark datasets demonstrate that P-DIFF+ is superior to the state-of-the-art sample selection methods.},
  archive      = {J_NN},
  author       = {QiHao Zhao and Wei Hu and Yangyu Huang and Fan Zhang},
  doi          = {10.1016/j.neunet.2021.07.024},
  journal      = {Neural Networks},
  pages        = {1-10},
  shortjournal = {Neural Netw.},
  title        = {P-DIFF+: Improving learning classifier with noisy labels by noisy negative learning loss},
  volume       = {144},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). INN/ENNS/JNNS - membership applic. form. <em>NN</em>,
<em>143</em>, II. (<a
href="https://doi.org/10.1016/S0893-6080(21)00371-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NN},
  doi          = {10.1016/S0893-6080(21)00371-3},
  journal      = {Neural Networks},
  pages        = {II},
  shortjournal = {Neural Netw.},
  title        = {INN/ENNS/JNNS - membership applic. form},
  volume       = {143},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Current events. <em>NN</em>, <em>143</em>, I. (<a
href="https://doi.org/10.1016/S0893-6080(21)00370-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NN},
  doi          = {10.1016/S0893-6080(21)00370-1},
  journal      = {Neural Networks},
  pages        = {I},
  shortjournal = {Neural Netw.},
  title        = {Current events},
  volume       = {143},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A full-parallel implementation of self-organizing maps on
hardware. <em>NN</em>, <em>143</em>, 818–827. (<a
href="https://doi.org/10.1016/j.neunet.2021.05.021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-Organizing Maps (SOMs) are extensively used for data clustering and dimensionality reduction. However, if applications are to fully benefit from SOM based techniques, high-speed processing is demanding, given that data tends to be both highly dimensional and yet “big”. Hence, a fully parallel architecture for the SOM is introduced to optimize the system’s data processing time. Unlike most literature approaches, the architecture proposed here does not contain sequential steps — a common limiting factor for processing speed. The architecture was validated on FPGA and evaluated concerning hardware throughput and the use of resources. Comparisons to the state of the art show a speedup of 8 . 91 × 8.91× over a partially serial implementation , using less than 15\% of hardware resources available. Thus, the method proposed here points to a hardware architecture that will not be obsolete quickly.},
  archive      = {J_NN},
  author       = {Leonardo A. Dias and Augusto M.P. Damasceno and Elena Gaura and Marcelo A.C. Fernandes},
  doi          = {10.1016/j.neunet.2021.05.021},
  journal      = {Neural Networks},
  pages        = {818-827},
  shortjournal = {Neural Netw.},
  title        = {A full-parallel implementation of self-organizing maps on hardware},
  volume       = {143},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning hierarchically-structured concepts. <em>NN</em>,
<em>143</em>, 798–817. (<a
href="https://doi.org/10.1016/j.neunet.2021.07.033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We use a recently developed synchronous Spiking Neural Network (SNN) model to study the problem of learning hierarchically-structured concepts. We introduce an abstract data model that describes simple hierarchical concepts. We define a feed-forward layered SNN model, with learning modeled using Oja’s local learning rule , a well known biologically-plausible rule for adjusting synapse weights . We define what it means for such a network to recognize hierarchical concepts; our notion of recognition is robust, in that it tolerates a bounded amount of noise. Then, we present a learning algorithm by which a layered network may learn to recognize hierarchical concepts according to our robust definition. We analyze correctness and performance rigorously; the amount of time required to learn each concept, after learning all of the sub-concepts, is approximately O 1 η k ℓmax log ( k ) + 1 ɛ + b log ( k ) O1ηkℓmaxlog(k)+1ɛ+blog(k) , where k k is the number of sub-concepts per concept, ℓmax ℓmax is the maximum hierarchical depth, η η is the learning rate , ɛ ɛ describes the amount of uncertainty allowed in robust recognition, and b b describes the amount of weight decrease for “irrelevant” edges. An interesting feature of this algorithm is that it allows the network to learn sub-concepts in a highly interleaved manner. This algorithm assumes that the concepts are presented in a noise-free way; we also extend these results to accommodate noise in the learning process. Finally, we give a simple lower bound saying that, in order to recognize concepts with hierarchical depth two with noise-tolerance, a neural network should have at least two layers. The results in this paper represent first steps in the theoretical study of hierarchical concepts using SNNs. The cases studied here are basic, but they suggest many directions for extensions to more elaborate and realistic cases.},
  archive      = {J_NN},
  author       = {Nancy Lynch and Frederik Mallmann-Trenn},
  doi          = {10.1016/j.neunet.2021.07.033},
  journal      = {Neural Networks},
  pages        = {798-817},
  shortjournal = {Neural Netw.},
  title        = {Learning hierarchically-structured concepts},
  volume       = {143},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Constrained plasticity reserve as a natural way to control
frequency and weights in spiking neural networks. <em>NN</em>,
<em>143</em>, 783–797. (<a
href="https://doi.org/10.1016/j.neunet.2021.08.016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Biological neurons have adaptive nature and perform complex computations involving the filtering of redundant information. However, most common neural cell models, including biologically plausible, such as Hodgkin–Huxley or Izhikevich, do not possess predictive dynamics on a single-cell level. Moreover, the modern rules of synaptic plasticity or interconnections weights adaptation also do not provide grounding for the ability of neurons to adapt to the ever-changing input signal intensity. While natural neuron synaptic growth is precisely controlled and restricted by protein supply and recycling, weight correction rules such as widely used STDP are efficiently unlimited in change rate and scale. The present article introduces new mechanics of interconnection between neuron firing rate homeostasis and weight change through STDP growth bounded by abstract protein reserve, controlled by the intracellular optimization algorithm . We show how these cellular dynamics help neurons filter out the intense noise signals to help neurons keep a stable firing rate. We also examine that such filtering does not affect the ability of neurons to recognize the correlated inputs in unsupervised mode. Such an approach might be used in the machine learning domain to improve the robustness of AI systems.},
  archive      = {J_NN},
  author       = {Oleg Nikitin and Olga Lukyanova and Alex Kunin},
  doi          = {10.1016/j.neunet.2021.08.016},
  journal      = {Neural Networks},
  pages        = {783-797},
  shortjournal = {Neural Netw.},
  title        = {Constrained plasticity reserve as a natural way to control frequency and weights in spiking neural networks},
  volume       = {143},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Refined UNet v3: Efficient end-to-end patch-wise network for
cloud and shadow segmentation with multi-channel spectral features.
<em>NN</em>, <em>143</em>, 767–782. (<a
href="https://doi.org/10.1016/j.neunet.2021.08.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic segmentation is one of the essential prerequisites for computer vision tasks, but edge-precise segmentation stays challenging due to the potential lack of a proper model indicating the low-level relation between pixels. We have presented Refined UNet v2, a concatenation of a network backbone and a subsequent embedded conditional random field (CRF) layer, which coarsely performs pixel-wise classification and refines edges of segmentation regions in a one-stage way. However, the CRF layer of v2 employs a gray-scale global observation (image) to construct contrast-sensitive bilateral features, which is not able to achieve the desired performance on ambiguous edges. In addition, the naïve depth-wise Gaussian filter cannot always compute efficiently, especially for a longer-range message-passing step. To address the aforementioned issues, we upgrade the bilateral message-passing kernel and the efficient implementation of Gaussian filtering in the CRF layer in this paper, referred to as Refined UNet v3, which is able to effectively capture ambiguous edges and accelerate the message-passing procedure. Specifically, the inherited UNet is employed to coarsely locate cloud and shadow regions and the embedded CRF layer refines the edges of the forthcoming segmentation proposals. The multi-channel guided Gaussian filter is applied to the bilateral message-passing step, which improves detecting ambiguous edges that are hard for the gray-scale counterpart to identify, and fast Fourier transform-based (FFT-based) Gaussian filtering facilitates an efficient and potentially range-agnostic implementation. Furthermore, Refined UNet v3 is able to be extended to segmentation on multi-spectral datasets, and the corresponding refinement examination confirms the development of shadow retrieval. Experiments and corresponding results demonstrate that the proposed update can outperform its counterpart in terms of the detection of vague edges, shadow retrieval, and isolated redundant regions, and it is practically efficient in our TensorFlow implementation. The demo source code is available at https://github.com/92xianshen/refined-unet-v3 .},
  archive      = {J_NN},
  author       = {Libin Jiao and Lianzhi Huo and Changmiao Hu and Ping Tang},
  doi          = {10.1016/j.neunet.2021.08.008},
  journal      = {Neural Networks},
  pages        = {767-782},
  shortjournal = {Neural Netw.},
  title        = {Refined UNet v3: Efficient end-to-end patch-wise network for cloud and shadow segmentation with multi-channel spectral features},
  volume       = {143},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Reliable impulsive synchronization for fuzzy neural networks
with mixed controllers. <em>NN</em>, <em>143</em>, 759–766. (<a
href="https://doi.org/10.1016/j.neunet.2021.08.013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work studies the synchronization of the master–slave (MS) fuzzy neural networks (FNNs) with random actuator failure , where the state information of the master FNNs can not be obtained directly. To reduce the loads of the communication channel and the controller, the simultaneously impulsive driven strategy of the communication channel and the controller is proposed. On the basis of the received measurements of the master FNNs, the mixed controller consisting of observer based controller and the static controller is designed. The randomly occurred actuator failure is also considered. According to the Lyapunov method , the sufficient conditions are achieved to ensure the synchronization of the MS FNNs, and the controller gains are designed by using the obtained results. The validity of the derived results is illustrated by a numerical example.},
  archive      = {J_NN},
  author       = {Fen Liu and Chang Liu and Hongxia Rao and Yong Xu and Tingwen Huang},
  doi          = {10.1016/j.neunet.2021.08.013},
  journal      = {Neural Networks},
  pages        = {759-766},
  shortjournal = {Neural Netw.},
  title        = {Reliable impulsive synchronization for fuzzy neural networks with mixed controllers},
  volume       = {143},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Active sensing with artificial neural networks. <em>NN</em>,
<em>143</em>, 751–758. (<a
href="https://doi.org/10.1016/j.neunet.2021.08.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The fitness of behaving agents depends on their knowledge of the environment, which demands efficient exploration strategies. Active sensing formalizes exploration as reduction of uncertainty about the current state of the environment. Despite strong theoretical justifications, active sensing has had limited applicability due to difficulty in estimating information gain. Here we address this issue by proposing a linear approximation to information gain and by implementing efficient gradient-based action selection within an artificial neural network setting. We compare information gain estimation with state of the art, and validate our model on an active sensing task based on MNIST dataset. We also propose an approximation that exploits the amortized inference network, and performs equally well in certain contexts.},
  archive      = {J_NN},
  author       = {Oleg Solopchuk and Alexandre Zénon},
  doi          = {10.1016/j.neunet.2021.08.007},
  journal      = {Neural Networks},
  pages        = {751-758},
  shortjournal = {Neural Netw.},
  title        = {Active sensing with artificial neural networks},
  volume       = {143},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On the approximation of functions by tanh neural networks.
<em>NN</em>, <em>143</em>, 732–750. (<a
href="https://doi.org/10.1016/j.neunet.2021.08.015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We derive bounds on the error, in high-order Sobolev norms, incurred in the approximation of Sobolev-regular as well as analytic functions by neural networks with the hyperbolic tangent activation function . These bounds provide explicit estimates on the approximation error with respect to the size of the neural networks. We show that tanh neural networks with only two hidden layers suffice to approximate functions at comparable or better rates than much deeper ReLU neural networks.},
  archive      = {J_NN},
  author       = {Tim De Ryck and Samuel Lanthaler and Siddhartha Mishra},
  doi          = {10.1016/j.neunet.2021.08.015},
  journal      = {Neural Networks},
  pages        = {732-750},
  shortjournal = {Neural Netw.},
  title        = {On the approximation of functions by tanh neural networks},
  volume       = {143},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hebbian semi-supervised learning in a sample efficiency
setting. <em>NN</em>, <em>143</em>, 719–731. (<a
href="https://doi.org/10.1016/j.neunet.2021.08.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose to address the issue of sample efficiency , in Deep Convolutional Neural Networks (DCNN), with a semi-supervised training strategy that combines Hebbian learning with gradient descent : all internal layers (both convolutional and fully connected) are pre-trained using an unsupervised approach based on Hebbian learning , and the last fully connected layer (the classification layer) is trained using Stochastic Gradient Descent (SGD). In fact, as Hebbian learning is an unsupervised learning method, its potential lies in the possibility of training the internal layers of a DCNN without labels. Only the final fully connected layer has to be trained with labeled examples. We performed experiments on various object recognition datasets, in different regimes of sample efficiency , comparing our semi-supervised (Hebbian for internal layers + SGD for the final fully connected layer) approach with end-to-end supervised backprop training, and with semi-supervised learning based on Variational Auto-Encoder (VAE). The results show that, in regimes where the number of available labeled samples is low, our semi-supervised approach outperforms the other approaches in almost all the cases.},
  archive      = {J_NN},
  author       = {Gabriele Lagani and Fabrizio Falchi and Claudio Gennaro and Giuseppe Amato},
  doi          = {10.1016/j.neunet.2021.08.003},
  journal      = {Neural Networks},
  pages        = {719-731},
  shortjournal = {Neural Netw.},
  title        = {Hebbian semi-supervised learning in a sample efficiency setting},
  volume       = {143},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Visual-guided attentive attributes embedding for zero-shot
learning. <em>NN</em>, <em>143</em>, 709–718. (<a
href="https://doi.org/10.1016/j.neunet.2021.07.031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zero-shot learning (ZSL) aims to learn a classifier for unseen classes by exploiting both training data from seen classes and external knowledge. In many visual tasks such as image classification , a set of high-level attributes that describe the semantic properties of classes are used as the external knowledge to bridge seen and unseen classes. While the attributes are usually treated equally by previous ZSL studies, we observe that the contribution of different attributes varies significantly over model training. To adaptively exploit the discriminative information embedded in different attributes, we propose a novel encoder–decoder framework with attention mechanism on the attribute level for zero-shot learning. Specifically, by mapping the visual features into a semantic space, the more discriminative attributes are emphasized with larger attention weights. Further, the attentive attributes and the class prototypes are simultaneously decoded to the visual space so that the hubness problem can be eased. Finally, the labels are predicted in the visual space. Extensive experiments on multiple benchmark datasets demonstrate that our proposed model achieves a significant boost over several state-of-the-art methods for ZSL task and comparative results for generalized ZSL task.},
  archive      = {J_NN},
  author       = {Rui Zhang and Qi Zhu and Xiangyu Xu and Daoqiang Zhang and Sheng-Jun Huang},
  doi          = {10.1016/j.neunet.2021.07.031},
  journal      = {Neural Networks},
  pages        = {709-718},
  shortjournal = {Neural Netw.},
  title        = {Visual-guided attentive attributes embedding for zero-shot learning},
  volume       = {143},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Quantum neuron with real weights. <em>NN</em>, <em>143</em>,
698–708. (<a
href="https://doi.org/10.1016/j.neunet.2021.07.034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a new model of a real weights quantum neuron exploiting the so-called quantum parallelism which allows for an exponential speedup of computations. The quantum neurons were trained in a classical–quantum approach, considering the delta rule to update the values of the weights in an image database of three distinct patterns. We performed classical simulations and also executed experiments in an actual small-scale quantum processor. The results of the experiments show that the proposed quantum real neuron model has a good generalisation capacity, demonstrating better accuracy than the traditional binary quantum perceptron model.},
  archive      = {J_NN},
  author       = {Cláudio A. Monteiro and Gustavo I.S. Filho and Matheus Hopper J. Costa and Fernando M. de Paula Neto and Wilson R. de Oliveira},
  doi          = {10.1016/j.neunet.2021.07.034},
  journal      = {Neural Networks},
  pages        = {698-708},
  shortjournal = {Neural Netw.},
  title        = {Quantum neuron with real weights},
  volume       = {143},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Capsule networks with non-iterative cluster routing.
<em>NN</em>, <em>143</em>, 690–697. (<a
href="https://doi.org/10.1016/j.neunet.2021.07.032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Capsule networks use routing algorithms to flow information between consecutive layers. In the existing routing procedures , capsules produce predictions (termed votes) for capsules of the next layer. In a nutshell, the next-layer capsule’s input is a weighted sum over all the votes it receives. In this paper, we propose non-iterative cluster routing for capsule networks. In the proposed cluster routing, capsules produce vote clusters instead of individual votes for next-layer capsules, and each vote cluster sends its centroid to a next-layer capsule. Generally speaking, the next-layer capsule’s input is a weighted sum over the centroid of each vote cluster it receives. The centroid that comes from a cluster with a smaller variance is assigned a larger weight in the weighted sum process. Compared with the state-of-the-art capsule networks, the proposed capsule networks achieve the best accuracy on the Fashion-MNIST and SVHN datasets with fewer parameters, and achieve the best accuracy on the smallNORB and CIFAR-10 datasets with a moderate number of parameters. The proposed capsule networks also produce capsules with disentangled representation and generalize well to images captured at novel viewpoints. The proposed capsule networks also preserve 2D spatial information of an input image in the capsule channels: if the capsule channels are rotated, the object reconstructed from these channels will be rotated by the same transformation.},
  archive      = {J_NN},
  author       = {Zhihao Zhao and Samuel Cheng},
  doi          = {10.1016/j.neunet.2021.07.032},
  journal      = {Neural Networks},
  pages        = {690-697},
  shortjournal = {Neural Netw.},
  title        = {Capsule networks with non-iterative cluster routing},
  volume       = {143},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Smoothing neural network for l0 regularized optimization
problem with general convex constraints. <em>NN</em>, <em>143</em>,
678–689. (<a
href="https://doi.org/10.1016/j.neunet.2021.08.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a neural network modeled by a differential inclusion to solve a class of discontinuous and nonconvex sparse regression problems with general convex constraints , whose objective function is the sum of a convex but not necessarily differentiable loss function and L 0 L0 regularization . We construct a smoothing relaxation function of L 0 L0 regularization and propose a neural network to solve the considered problem. We prove that the solution of proposed neural network with any initial point satisfying linear equality constraints is global existent, bounded and reaches the feasible region in finite time and remains there thereafter. Moreover, the solution of proposed neural network is its slow solution and any accumulation point of it is a Clarke stationary point of the brought forward nonconvex smoothing approximation problem. In the box-constrained case, all accumulation points of the solution own a unified lower bound property and have a common support set. Except for a special case, any accumulation point of the solution is a local minimizer of the considered problem. In particular, the proposed neural network has a simple structure than most existing neural networks for solving the locally Lipschitz continuous but nonsmooth nonconvex problems . Finally, we give some numerical experiments to show the efficiency of proposed neural network.},
  archive      = {J_NN},
  author       = {Wenjing Li and Wei Bian},
  doi          = {10.1016/j.neunet.2021.08.001},
  journal      = {Neural Networks},
  pages        = {678-689},
  shortjournal = {Neural Netw.},
  title        = {Smoothing neural network for l0 regularized optimization problem with general convex constraints},
  volume       = {143},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). CommPOOL: An interpretable graph pooling framework for
hierarchical graph representation learning. <em>NN</em>, <em>143</em>,
669–677. (<a
href="https://doi.org/10.1016/j.neunet.2021.07.028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years have witnessed the emergence and flourishing of hierarchical graph pooling neural networks (HGPNNs) which are effective graph representation learning approaches for graph level tasks such as graph classification. However, current HGPNNs do not take full advantage of the graph’s intrinsic structures (e.g., community structure). Moreover, the pooling operations in existing HGPNNs are difficult to be interpreted. In this paper, we propose a new interpretable graph pooling framework — CommPOOL, that can capture and preserve the hierarchical community structure of graphs in the graph representation learning process. Specifically, the proposed community pooling mechanism in CommPOOL utilizes an unsupervised approach for capturing the inherent community structure of graphs in an interpretable manner. CommPOOL is a general and flexible framework for hierarchical graph representation learning that can further facilitate various graph-level tasks. Evaluations on five public benchmark datasets and one synthetic dataset demonstrate the superior performance of CommPOOL in graph representation learning for graph classification compared to the state-of-the-art baseline methods , and its effectiveness in capturing and preserving the community structure of graphs.},
  archive      = {J_NN},
  author       = {Haoteng Tang and Guixiang Ma and Lifang He and Heng Huang and Liang Zhan},
  doi          = {10.1016/j.neunet.2021.07.028},
  journal      = {Neural Networks},
  pages        = {669-677},
  shortjournal = {Neural Netw.},
  title        = {CommPOOL: An interpretable graph pooling framework for hierarchical graph representation learning},
  volume       = {143},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Content-aware convolutional neural networks. <em>NN</em>,
<em>143</em>, 657–668. (<a
href="https://doi.org/10.1016/j.neunet.2021.06.030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional Neural Networks (CNNs) have achieved great success due to the powerful feature learning ability of convolution layers. Specifically, the standard convolution traverses the input images/features using a sliding window scheme to extract features. However, not all the windows contribute equally to the prediction results of CNNs. In practice, the convolutional operation on some of the windows ( e.g. , smooth windows that contain very similar pixels) can be very redundant and may introduce noises into the computation. Such redundancy may not only deteriorate the performance but also incur the unnecessary computational cost. Thus, it is important to reduce the computational redundancy of convolution to improve the performance. To this end, we propose a Content-aware Convolution (CAC) that automatically detects the smooth windows and applies a 1 ×1 convolutional kernel to replace the original large kernel. In this sense, we are able to effectively avoid the redundant computation on similar pixels. By replacing the standard convolution in CNNs with our CAC, the resultant models yield significantly better performance and lower computational cost than the baseline models with the standard convolution. More critically, we are able to dynamically allocate suitable computation resources according to the data smoothness of different images, making it possible for content-aware computation. Extensive experiments on various computer vision tasks demonstrate the superiority of our method over existing methods.},
  archive      = {J_NN},
  author       = {Yong Guo and Yaofo Chen and Mingkui Tan and Kui Jia and Jian Chen and Jingdong Wang},
  doi          = {10.1016/j.neunet.2021.06.030},
  journal      = {Neural Networks},
  pages        = {657-668},
  shortjournal = {Neural Netw.},
  title        = {Content-aware convolutional neural networks},
  volume       = {143},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bidirectional interaction between visual and motor
generative models using predictive coding and active inference.
<em>NN</em>, <em>143</em>, 638–656. (<a
href="https://doi.org/10.1016/j.neunet.2021.07.016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we build upon the Active Inference (AIF) and Predictive Coding (PC) frameworks to propose a neural architecture comprising a generative model for sensory prediction, and a distinct generative model for motor trajectories. We highlight how sequences of sensory predictions can act as rails guiding learning, control and online adaptation of motor trajectories. We furthermore inquire the effects of bidirectional interactions between the motor and the visual modules. The architecture is tested on the control of a simulated robotic arm learning to reproduce handwritten letters.},
  archive      = {J_NN},
  author       = {Louis Annabi and Alexandre Pitti and Mathias Quoy},
  doi          = {10.1016/j.neunet.2021.07.016},
  journal      = {Neural Networks},
  pages        = {638-656},
  shortjournal = {Neural Netw.},
  title        = {Bidirectional interaction between visual and motor generative models using predictive coding and active inference},
  volume       = {143},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A computational model of familiarity detection for natural
pictures, abstract images, and random patterns: Combination of deep
learning and anti-hebbian training. <em>NN</em>, <em>143</em>, 628–637.
(<a href="https://doi.org/10.1016/j.neunet.2021.07.022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a neural network model for familiarity recognition of different types of images in the perirhinal cortex ( the FaRe model ). The model is designed as a two-stage system. At the first stage, the parameters of an image are extracted by a pretrained deep learning convolutional neural network . At the second stage, a two-layer feed forward neural network with anti-Hebbian learning is used to make the decision about the familiarity of the image. FaRe model simulations demonstrate high capacity of familiarity recognition memory for natural pictures and low capacity for both abstract images and random patterns. These findings are in agreement with psychological experiments.},
  archive      = {J_NN},
  author       = {Yakov Kazanovich and Roman Borisyuk},
  doi          = {10.1016/j.neunet.2021.07.022},
  journal      = {Neural Networks},
  pages        = {628-637},
  shortjournal = {Neural Netw.},
  title        = {A computational model of familiarity detection for natural pictures, abstract images, and random patterns: Combination of deep learning and anti-hebbian training},
  volume       = {143},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Continual learning for recurrent neural networks: An
empirical evaluation. <em>NN</em>, <em>143</em>, 607–627. (<a
href="https://doi.org/10.1016/j.neunet.2021.07.021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning continuously during all model lifetime is fundamental to deploy machine learning solutions robust to drifts in the data distribution. Advances in Continual Learning (CL) with recurrent neural networks could pave the way to a large number of applications where incoming data is non stationary, like natural language processing and robotics. However, the existing body of work on the topic is still fragmented, with approaches which are application-specific and whose assessment is based on heterogeneous learning protocols and datasets. In this paper, we organize the literature on CL for sequential data processing by providing a categorization of the contributions and a review of the benchmarks. We propose two new benchmarks for CL with sequential data based on existing datasets, whose characteristics resemble real-world applications. We also provide a broad empirical evaluation of CL and Recurrent Neural Networks in class-incremental scenario, by testing their ability to mitigate forgetting with a number of different strategies which are not specific to sequential data processing. Our results highlight the key role played by the sequence length and the importance of a clear specification of the CL scenario.},
  archive      = {J_NN},
  author       = {Andrea Cossu and Antonio Carta and Vincenzo Lomonaco and Davide Bacciu},
  doi          = {10.1016/j.neunet.2021.07.021},
  journal      = {Neural Networks},
  pages        = {607-627},
  shortjournal = {Neural Netw.},
  title        = {Continual learning for recurrent neural networks: An empirical evaluation},
  volume       = {143},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-view spectral clustering via common structure
maximization of local and global representations. <em>NN</em>,
<em>143</em>, 595–606. (<a
href="https://doi.org/10.1016/j.neunet.2021.07.020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The essential problem of multi-view spectral clustering is to learn a good common representation by effectively utilizing multi-view information. A popular strategy for improving the quality of the common representation is utilizing global and local information jointly. Most existing methods capture local manifold information by graph regularization . However, once local graphs are constructed, they do not change during the whole optimization process. This may lead to a degenerated common representation in the case of existing unreliable graphs. To address this problem, rather than directly using fixed local representations, we propose a dynamic strategy to construct a common local representation. Then, we impose a fusion term to maximize the common structure of the local and global representations so that they can boost each other in a mutually reinforcing manner. With this fusion term, we integrate local and global representation learning in a unified framework and design an alternative iteration based optimization procedure to solve it. Extensive experiments conducted on a number of benchmark datasets support the superiority of our algorithm over several state-of-the-art methods.},
  archive      = {J_NN},
  author       = {Wenyu Hao and Shanmin Pang and Zhikai Chen},
  doi          = {10.1016/j.neunet.2021.07.020},
  journal      = {Neural Networks},
  pages        = {595-606},
  shortjournal = {Neural Netw.},
  title        = {Multi-view spectral clustering via common structure maximization of local and global representations},
  volume       = {143},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Internal manipulation of perceptual representations in human
flexible cognition: A computational model. <em>NN</em>, <em>143</em>,
572–594. (<a
href="https://doi.org/10.1016/j.neunet.2021.07.013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Executive functions represent a set of processes in goal-directed cognition that depend on integrated cortical-basal ganglia brain systems and form the basis of flexible human behaviour . Several computational models have been proposed for studying cognitive flexibility as a key executive function and the Wisconsin card sorting test (WCST) that represents an important neuropsychological tool to investigate it. These models clarify important aspects that underlie cognitive flexibility, particularly decision-making, motor response, and feedback-dependent learning processes. However, several studies suggest that the categorisation processes involved in the solution of the WCST include an additional computational stage of category representation that supports the other processes. Surprisingly, all models of the WCST ignore this fundamental stage and they assume that decision making directly triggers actions. Thus, we propose a novel hypothesis where the key mechanisms of cognitive flexibility and goal-directed behaviour rely on the acquisition of suitable representations of percepts and their top-down internal manipulation. Moreover, we propose a neuro-inspired computational model to operationalise this hypothesis. The capacity of the model to support cognitive flexibility was validated by systematically reproducing and interpreting the behaviour exhibited in the WCST by young and old healthy adults, and by frontal and Parkinson patients. The results corroborate and further articulate the hypothesis that the internal manipulation of representations is a core process in goal-directed flexible cognition.},
  archive      = {J_NN},
  author       = {Giovanni Granato and Gianluca Baldassarre},
  doi          = {10.1016/j.neunet.2021.07.013},
  journal      = {Neural Networks},
  pages        = {572-594},
  shortjournal = {Neural Netw.},
  title        = {Internal manipulation of perceptual representations in human flexible cognition: A computational model},
  volume       = {143},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Levenberg–marquardt multi-classification using hinge loss
function. <em>NN</em>, <em>143</em>, 564–571. (<a
href="https://doi.org/10.1016/j.neunet.2021.07.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Incorporating higher-order optimization functions , such as Levenberg–Marquardt (LM) have revealed better generalizable solutions for deep learning problems. However, these higher-order optimization functions suffer from very large processing time and training complexity especially as training datasets become large, such as in multi-view classification problems, where finding global optima is a very costly problem. To solve this issue, we develop a solution for LM-enabled classification with, to the best of knowledge first-time implementation of hinge loss, for multiview classification. Hinge loss allows the neural network to converge faster and perform better than other loss functions such as logistic or square loss rates. We prove our method by experimenting with various multiclass classification challenges of varying complexity and training data size. The empirical results show the training time and accuracy rates achieved, highlighting how our method outperforms in all cases, especially when training time is limited. Our paper presents important results in the relationship between optimization and loss functions and how these can impact deep learning problems.},
  archive      = {J_NN},
  author       = {Buse Melis Ozyildirim and Mariam Kiran},
  doi          = {10.1016/j.neunet.2021.07.010},
  journal      = {Neural Networks},
  pages        = {564-571},
  shortjournal = {Neural Netw.},
  title        = {Levenberg–Marquardt multi-classification using hinge loss function},
  volume       = {143},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Transfer-RLS method and transfer-FORCE learning for simple
and fast training of reservoir computing models. <em>NN</em>,
<em>143</em>, 550–563. (<a
href="https://doi.org/10.1016/j.neunet.2021.06.031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reservoir computing is a machine learning framework derived from a special type of recurrent neural network . Following recent advances in physical reservoir computing, some reservoir computing devices are thought to be promising as energy-efficient machine learning hardware for real-time information processing . To realize efficient online learning with low-power reservoir computing devices, it is beneficial to develop fast convergence learning methods with simpler operations. This study proposes a training method located in the middle between the recursive least squares (RLS) method and the least mean squares (LMS) method, which are standard online learning methods for reservoir computing models. The RLS method converges fast but requires updates of a huge matrix called a gain matrix , whereas the LMS method does not use a gain matrix but converges very slow. On the other hand, the proposed method called a transfer-RLS method does not require updates of the gain matrix in the main-training phase by updating that in advance (i.e., in a pre-training phase). As a result, the transfer-RLS method can work with simpler operations than the original RLS method without sacrificing much convergence speed. We numerically and analytically show that the transfer-RLS method converges much faster than the LMS method. Furthermore, we show that a modified version of the transfer-RLS method (called transfer-FORCE learning) can be applied to the first-order reduced and controlled error (FORCE) learning for a reservoir computing model with a closed-loop, which is challenging to train.},
  archive      = {J_NN},
  author       = {Hiroto Tamura and Gouhei Tanaka},
  doi          = {10.1016/j.neunet.2021.06.031},
  journal      = {Neural Networks},
  pages        = {550-563},
  shortjournal = {Neural Netw.},
  title        = {Transfer-RLS method and transfer-FORCE learning for simple and fast training of reservoir computing models},
  volume       = {143},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A distributed optimisation framework combining natural
gradient with hessian-free for discriminative sequence training.
<em>NN</em>, <em>143</em>, 537–549. (<a
href="https://doi.org/10.1016/j.neunet.2021.05.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a novel natural gradient and Hessian-free (NGHF) optimisation framework for neural network training that can operate efficiently in a distributed manner. It relies on the linear conjugate gradient (CG) algorithm to combine the natural gradient (NG) method with local curvature information from Hessian-free (HF). A solution to a numerical issue in CG allows effective parameter updates to be generated with far fewer CG iterations than usually used ( e.g. 5-8 instead of 200). This work also presents a novel preconditioning approach to improve the progress made by individual CG iterations for models with shared parameters. Although applicable to other training losses and model structures, NGHF is investigated in this paper for lattice-based discriminative sequence training for hybrid hidden Markov model acoustic models using a standard recurrent neural network , long short-term memory, and time delay neural network models for output probability calculation. Automatic speech recognition experiments are reported on the multi-genre broadcast data set for a range of different acoustic model types. These experiments show that NGHF achieves larger word error rate reductions than standard stochastic gradient descent or Adam, while requiring orders of magnitude fewer parameter updates.},
  archive      = {J_NN},
  author       = {Adnan Haider and Chao Zhang and Florian L. Kreyssig and Philip C. Woodland},
  doi          = {10.1016/j.neunet.2021.05.011},
  journal      = {Neural Networks},
  pages        = {537-549},
  shortjournal = {Neural Netw.},
  title        = {A distributed optimisation framework combining natural gradient with hessian-free for discriminative sequence training},
  volume       = {143},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Online sensorimotor learning and adaptation for inverse
dynamics control. <em>NN</em>, <em>143</em>, 525–536. (<a
href="https://doi.org/10.1016/j.neunet.2021.06.029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a micro-data ( &amp;lt; 10 trials) sensorimotor learning and adaptation (SEED) model for human-like arm inverse dynamics control. The SEED model consists of a feedforward Gaussian motor primitive (GATE) neural network and an adaptive feedback impedance (AIM) mechanism. Sensorimotor weights over trials are learned in the GATE network, while the AIM mechanism is used to online tune impedance gains in a trial. The model was validated by periodic and non-periodic tracking tasks on a two-joint robot arm. As a result, the proposed model enables the arm to stably learn the tasks within 10 trials, compared to thousands of trials required by state-of-art deep learning . This model facilitates the exploration of unknown arm dynamics, in which the elbow joint requires much less active control compared to the shoulder. This control goes below 3\% of the overall effort. This finding complies with a proximal–distal control gradient in human arm control. Taken together, the proposed SEED model paves a way for implementing data-efficient sensorimotor learning and adaptation of human-like arm movement.},
  archive      = {J_NN},
  author       = {Xiaofeng Xiong and Poramate Manoonpong},
  doi          = {10.1016/j.neunet.2021.06.029},
  journal      = {Neural Networks},
  pages        = {525-536},
  shortjournal = {Neural Netw.},
  title        = {Online sensorimotor learning and adaptation for inverse dynamics control},
  volume       = {143},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dynamical and static multisynchronization analysis for
coupled multistable memristive neural networks with hybrid control.
<em>NN</em>, <em>143</em>, 515–524. (<a
href="https://doi.org/10.1016/j.neunet.2021.07.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the dynamical multisynchronization (DMS) and static multisynchronization (SMS) problems for a class of delayed coupled multistable memristive neural networks (DCMMNNs) via a novel hybrid controller which includes delayed impulsive control and state feedback control . Based on the state–space partition method and the geometrical properties of the activation function , each subnetwork has multiple locally exponential stable equilibrium states. By employing a new Halanay-type inequality and the impulsive control theory , some new linear matrix inequalities (LMIs)-based sufficient conditions are proposed. It is shown that the delayed impulsive control with suitable impulsive interval and allowable time-varying delay can still guarantee the DMS and SMS of DCMMNNs. Finally, a numerical example is presented to illustrate the effectiveness of the hybrid controller .},
  archive      = {J_NN},
  author       = {Xiaoxiao Lv and Jinde Cao and Leszek Rutkowski},
  doi          = {10.1016/j.neunet.2021.07.004},
  journal      = {Neural Networks},
  pages        = {515-524},
  shortjournal = {Neural Netw.},
  title        = {Dynamical and static multisynchronization analysis for coupled multistable memristive neural networks with hybrid control},
  volume       = {143},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sparse random feature maps for the item-multiset kernel.
<em>NN</em>, <em>143</em>, 500–514. (<a
href="https://doi.org/10.1016/j.neunet.2021.06.024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Random feature maps are a promising tool for large-scale kernel methods . Since most random feature maps generate dense random features causing memory explosion, it is hard to apply them to very-large-scale sparse datasets. The factorization machines and related models, which use feature combinations efficiently, scale well for large-scale sparse datasets and have been used in many applications. However, their optimization problems are typically non-convex. Therefore, although they are optimized by using gradient-based iterative methods, such methods cannot find global optimum solutions in general and require a large number of iterations for convergence. In this paper, we define the item-multiset kernel, which is a generalization of the itemset kernel and dot product kernels. Unfortunately, random feature maps for the itemset kernel and dot product kernels cannot approximate the item-multiset kernel. We thus develop a method that converts an item-multiset kernel into an itemset kernel, enabling the item-multiset kernel to be approximated by using a random feature map for the itemset kernel. We propose two random feature maps for the itemset kernel, which run faster and are more memory efficient than the existing feature map for the itemset kernel. They also generate sparse random features when the original (input) feature vector is sparse and thus linear models using proposed methods . Experiments using real-world datasets demonstrated the effectiveness of the proposed methodology: linear models using the proposed random feature maps ran from 10 to 100 times faster than ones based on existing methods.},
  archive      = {J_NN},
  author       = {Kyohei Atarashi and Satoshi Oyama and Masahito Kurihara},
  doi          = {10.1016/j.neunet.2021.06.024},
  journal      = {Neural Networks},
  pages        = {500-514},
  shortjournal = {Neural Netw.},
  title        = {Sparse random feature maps for the item-multiset kernel},
  volume       = {143},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Low-shot transfer with attention for highly imbalanced
cursive character recognition. <em>NN</em>, <em>143</em>, 489–499. (<a
href="https://doi.org/10.1016/j.neunet.2021.07.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recognition of ancient Korean–Chinese cursive character (Hanja) is a challenging problem mainly because of large number of classes, damaged cursive characters, various hand-writing styles, and similar confusable characters. They also suffer from lack of training data and class imbalance issues. To address these problems, we propose a unified Regularized Low-shot Attention Transfer with Imbalance τ τ -Normalizing ( R E L A T I N ) (RELATIN) framework. This handles the problem with instance-poor classes using a novel low-shot regularizer that encourages the norm of the weight vectors for classes with few samples to be aligned to those of many-shot classes. To overcome the class imbalance problem , we incorporate a decoupled classifier to rectify the decision boundaries via classifier weight-scaling into the proposed low-shot regularizer framework. To address the limited training data issue, the proposed framework performs Jensen–Shannon divergence based data augmentation and incorporate an attention module that aligns the most attentive features of the pretrained network to a target network. We verify the proposed RELATIN framework using highly-imbalanced ancient cursive handwritten character datasets. The results suggest that (i) the extreme class imbalance has a detrimental effect on classification performance; (ii) the proposed low-shot regularizer aligns the norm of the classifier in favor of classes with few samples; (iii) weight-scaling of decoupled classifier for addressing class imbalance appeared to be dominant in all the other baseline conditions ; (iv) further addition of the attention module attempts to select more representative features maps from base pretrained model; (v) the proposed ( R E L A T I N ) (RELATIN) framework results in superior representations to address extreme class imbalance issue.},
  archive      = {J_NN},
  author       = {Amin Jalali and Swathi Kavuri and Minho Lee},
  doi          = {10.1016/j.neunet.2021.07.003},
  journal      = {Neural Networks},
  pages        = {489-499},
  shortjournal = {Neural Netw.},
  title        = {Low-shot transfer with attention for highly imbalanced cursive character recognition},
  volume       = {143},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Recurrent neural network pruning using dynamical systems and
iterative fine-tuning. <em>NN</em>, <em>143</em>, 475–488. (<a
href="https://doi.org/10.1016/j.neunet.2021.07.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network pruning techniques are widely employed to reduce the memory requirements and increase the inference speed of neural networks . This work proposes a novel RNN pruning method that considers the RNN weight matrices as collections of time-evolving signals. Such signals that represent weight vectors can be modelled using Linear Dynamical Systems (LDSs). In this way, weight vectors with similar temporal dynamics can be pruned as they have limited effect on the performance of the model. Additionally, during the fine-tuning of the pruned model, a novel discrimination-aware variation of the L2 regularization is introduced to penalize network weights (i.e., reduce the magnitude), whose impact on the output of an RNN network is minimal. Finally, an iterative fine-tuning approach is proposed that employs a bigger model to guide an increasingly smaller pruned one, as a steep decrease of the network parameters can irreversibly harm the performance of the pruned model. Extensive experimentation with different network architectures demonstrates the potential of the proposed method to create pruned models with significantly improved perplexity by at least 0.62\% on the PTB dataset and improved F1-score by 1.39\% on the SQuAD dataset, contrary to other state-of-the-art approaches that slightly improve or even deteriorate models’ performance.},
  archive      = {J_NN},
  author       = {Christos Chatzikonstantinou and Dimitrios Konstantinidis and Kosmas Dimitropoulos and Petros Daras},
  doi          = {10.1016/j.neunet.2021.07.001},
  journal      = {Neural Networks},
  pages        = {475-488},
  shortjournal = {Neural Netw.},
  title        = {Recurrent neural network pruning using dynamical systems and iterative fine-tuning},
  volume       = {143},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On the characterization of cognitive tasks using
activity-specific short-lived synchronization between
electroencephalography channels. <em>NN</em>, <em>143</em>, 452–474. (<a
href="https://doi.org/10.1016/j.neunet.2021.06.022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate characterization of brain activity during a cognitive task is challenging due to the dynamically changing and the complex nature of the brain. The majority of the proposed approaches assume stationarity in brain activity and disregard the systematic timing organization among brain regions during cognitive tasks. In this study, we propose a novel cognitive activity recognition method that captures the activity-specific timing parameters from training data that elicits maximal average short-lived pairwise synchronization between electroencephalography signals. We evaluated the characterization power of the activity-specific timing parameter triplets in a motor imagery activity recognition framework. The activity-specific timing parameter triplets consist of latency of the maximally synchronized signal segments from activity onset Δ t Δt , the time lag between maximally synchronized signal segments τ τ , and the duration of the maximally synchronized signal segments w w . We used cosine-based similarity, wavelet bi-coherence, phase-locking value, phase coherence value, linearized mutual information, and cross-correntropy to calculate the channel synchronizations at the specific timing parameters. Recognition performances as well as statistical analyses on both BCI Competition-III dataset IVa and PhysioNet Motor Movement/Imagery dataset, indicate that the inter-channel short-lived synchronization calculated using activity-specific timing parameter triplets elicit significantly distinct synchronization profiles for different motor imagery tasks and can thus reliably be used for cognitive task recognition purposes.},
  archive      = {J_NN},
  author       = {B. Orkan Olcay and Murat Özgören and Bilge Karaçalı},
  doi          = {10.1016/j.neunet.2021.06.022},
  journal      = {Neural Networks},
  pages        = {452-474},
  shortjournal = {Neural Netw.},
  title        = {On the characterization of cognitive tasks using activity-specific short-lived synchronization between electroencephalography channels},
  volume       = {143},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sensitivity – local index to control chaoticity or gradient
globally –. <em>NN</em>, <em>143</em>, 436–451. (<a
href="https://doi.org/10.1016/j.neunet.2021.06.015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Here, we introduce a fully local index named “sensitivity” for each neuron to control chaoticity or gradient globally in a neural network (NN). We also propose a learning method to adjust it named “sensitivity adjustment learning (SAL)”. The index is the gradient magnitude of its output with respect to its inputs. By adjusting its time average to 1.0 in each neuron, information transmission in the neuron changes to be moderate without shrinking or expanding for both forward and backward computations. That results in moderate information transmission through a layer of neurons when the weights and inputs are random. Therefore, SAL can control the chaoticity of the network dynamics in a recurrent NN (RNN). It can also solve the vanishing gradient problem in error backpropagation (BP) learning in a deep feedforward NN or an RNN . We demonstrate that when applying SAL to an RNN with small and random initial weights, log-sensitivity, which is the logarithm of RMS (root mean square) sensitivity over all the neurons, is equivalent to the maximum Lyapunov exponent until it reaches 0.0. We also show that SAL works with BP or BPTT (BP through time) to avoid the vanishing gradient problem in a 300-layer NN or an RNN that learns a problem with a lag of 300 steps between the first input and the output. Compared with manually fine-tuning the spectral radius of the weight matrix before learning, SAL’s continuous nonlinear learning nature prevents loss of sensitivities during learning, resulting in a significant improvement in learning performance.},
  archive      = {J_NN},
  author       = {Katsunari Shibata and Takuya Ejima and Yuki Tokumaru and Toshitaka Matsuki},
  doi          = {10.1016/j.neunet.2021.06.015},
  journal      = {Neural Networks},
  pages        = {436-451},
  shortjournal = {Neural Netw.},
  title        = {Sensitivity – local index to control chaoticity or gradient globally –},
  volume       = {143},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). When noise meets chaos: Stochastic resonance in neurochaos
learning. <em>NN</em>, <em>143</em>, 425–435. (<a
href="https://doi.org/10.1016/j.neunet.2021.06.025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Chaos and Noise are ubiquitous in the Brain. Inspired by the chaotic firing of neurons and the constructive role of noise in neuronal models, we for the first time connect chaos, noise and learning. In this paper, we demonstrate Stochastic Resonance (SR) phenomenon in Neurochaos Learning (NL). SR manifests at the level of a single neuron of NL and enables efficient subthreshold signal detection. Furthermore, SR is shown to occur in single and multiple neuronal NL architecture for classification tasks — both on simulated and real-world spoken digit datasets, and in architectures with 1 D 1D chaotic maps as well as Hindmarsh–Rose spiking neurons . Intermediate levels of noise in neurochaos learning enable peak performance in classification tasks thus highlighting the role of SR in AI applications , especially in brain inspired learning architectures.},
  archive      = {J_NN},
  author       = {Harikrishnan N.B. and Nithin Nagaraj},
  doi          = {10.1016/j.neunet.2021.06.025},
  journal      = {Neural Networks},
  pages        = {425-435},
  shortjournal = {Neural Netw.},
  title        = {When noise meets chaos: Stochastic resonance in neurochaos learning},
  volume       = {143},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Equivalent-input-disturbance estimator-based event-triggered
control design for master–slave neural networks. <em>NN</em>,
<em>143</em>, 413–424. (<a
href="https://doi.org/10.1016/j.neunet.2021.06.023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the robust synchronization problem for a class of master–slave neural networks (MSNNs) subject to network-induced delays, unknown time-varying uncertainty, and exogenous disturbances . An equivalent-input-disturbance (EID) estimation technique is applied to compensate for the effects of unknown uncertainty and disturbances in the system output. In addition, to reduce the burden of the communication channel in the addressed MSNNs and improve the utilization of bandwidth an event-triggered control protocol is developed to obtain the synchronization of MSNNs. In particular, event-triggering conditions are verified periodically at every sampling instant in both sensors and actuators to avoid the Zeno behavior in the networks. By designing an appropriate low-pass filter in the EID estimator block, the accuracy of disturbance estimation performance is improved. Moreover, by concatenating the synchronization error, observer, and filter states as a single state vector, an augmented system is formulated. Then the tangible delay-dependent stability condition for that augmented system is established by employing the Lyapunov stability theory and reciprocally convex approach. Based on the feasible solutions of the derived stability conditions, the event-triggering parameters, controller, and observer gains are co-designed. Finally, two toy examples are given to illustrate the established theoretical findings.},
  archive      = {J_NN},
  author       = {P. Selvaraj and O.M. Kwon and S.H. Lee and R. Sakthivel},
  doi          = {10.1016/j.neunet.2021.06.023},
  journal      = {Neural Networks},
  pages        = {413-424},
  shortjournal = {Neural Netw.},
  title        = {Equivalent-input-disturbance estimator-based event-triggered control design for master–slave neural networks},
  volume       = {143},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Enhanced image prior for unsupervised remoting sensing
super-resolution. <em>NN</em>, <em>143</em>, 400–412. (<a
href="https://doi.org/10.1016/j.neunet.2021.06.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Numerous approaches based on training low-high resolution image pairs have been proposed to address the super-resolution (SR) task. Despite their success, low-high resolution image pairs are usually difficult to obtain in certain scenarios, and these methods are limited in the actual scene (unknown or non-ideal image acquisition process). In this paper, we proposed a novel unsupervised learning framework, termed Enhanced Image Prior (EIP), which achieves SR tasks without low/high resolution image pairs. We first feed random noise maps into a designed generative adversarial network (GAN) for satellite image SR reconstruction. Then, we convert the reference image to latent space as the enhanced image prior. Finally, we update the input noise in the latent space with a recurrent updating strategy, and further transfer the texture and structured information from the reference image. Results on extensive experiments on the Draper dataset show that EIP achieves significant improvements over state-of-the-art unsupervised SR methods both quantitatively and qualitatively. Our experiments on satellite (SuperView-1) images reveal the potential of the proposed approach in improving the resolution of remote sensing imagery compared with the supervised algorithms. Source code is available at https://github.com/jiaming-wang/EIP .},
  archive      = {J_NN},
  author       = {Jiaming Wang and Zhenfeng Shao and Xiao Huang and Tao Lu and Ruiqian Zhang and Jiayi Ma},
  doi          = {10.1016/j.neunet.2021.06.005},
  journal      = {Neural Networks},
  pages        = {400-412},
  shortjournal = {Neural Netw.},
  title        = {Enhanced image prior for unsupervised remoting sensing super-resolution},
  volume       = {143},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A global neural network learning machine: Coupled integer
and fractional calculus operator with an adaptive learning scheme.
<em>NN</em>, <em>143</em>, 386–399. (<a
href="https://doi.org/10.1016/j.neunet.2021.06.021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Find the global optimal solution of the model is one promising research topic in computational intelligent community. Dependent on analogies to natural processes, the evolutionary swarm intelligent algorithms are widely used for solving global optimization problems which directed by the fitness values. In this paper, we propose one efficient fractional global learning machine (Fragmachine) which includes two stages (descending and ascending) to determine the optimal search path. The neural network model is used to approach the given fitness value. Specifically, for the descending stage, the integer gradient of the network output with respect the current location is employed to find the next descending point, while for the ascending stage, the fractional gradient is implemented to climb and escape from the local optimal point. We further propose one adaptive learning rate during training which relies on both the current gradient (integer or fractional) information and the fitness value. Finally, a series of numerical experiments verify the effectiveness of the proposed algorithm, Fragmachine.},
  archive      = {J_NN},
  author       = {Huaqing Zhang and Yi-Fei Pu and Xuetao Xie and Bingran Zhang and Jian Wang and Tingwen Huang},
  doi          = {10.1016/j.neunet.2021.06.021},
  journal      = {Neural Networks},
  pages        = {386-399},
  shortjournal = {Neural Netw.},
  title        = {A global neural network learning machine: Coupled integer and fractional calculus operator with an adaptive learning scheme},
  volume       = {143},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Neural adaptive fault-tolerant finite-time control for
nonstrict feedback systems: An event-triggered mechanism. <em>NN</em>,
<em>143</em>, 377–385. (<a
href="https://doi.org/10.1016/j.neunet.2021.06.019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of event-triggered neural adaptive fault-tolerant finite-time control is investigated for a class of nonstrict feedback nonlinear systems in the presence of nonaffine nonlinear faults. The event-triggered signal is designed by using a relative-threshold to reduce communication burden. The dynamic surface control method is used to relax the assumption of the reference signal and deal with the computational complexity issue. Based on the finite-time stability, a new neural adaptive backstepping design method is developed. The event-triggered neural adaptive fault-tolerant control law is developed for the closed-loop system so that not only the semi-global practical finite-time stability is ensured, but also the tracking performance with a small residual set is guaranteed. Finally, the effectiveness of the proposed control law is verified via simulation results.},
  archive      = {J_NN},
  author       = {K. Sun and J. Qiu and H.R. Karimi},
  doi          = {10.1016/j.neunet.2021.06.019},
  journal      = {Neural Networks},
  pages        = {377-385},
  shortjournal = {Neural Netw.},
  title        = {Neural adaptive fault-tolerant finite-time control for nonstrict feedback systems: An event-triggered mechanism},
  volume       = {143},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Distributed learning for sketched kernel regression.
<em>NN</em>, <em>143</em>, 368–376. (<a
href="https://doi.org/10.1016/j.neunet.2021.06.020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study distributed learning for regularized least squares regression in a reproducing kernel Hilbert space (RKHS). The divide-and-conquer strategy is a frequently used approach for dealing with very large data sets, which computes an estimate on each subset and then takes an average of the estimators. Existing theoretical constraint on the number of subsets implies the size of each subset can still be large. Random sketching can thus be used to produce the local estimators on each subset to further reduce the computation compared to vanilla divide-and-conquer. In this setting, sketching and divide-and-conquer are complementary to each other in dealing with the large sample size. We show that optimal learning rates can be retained. Simulations are performed to compare sketched and non-standard divide-and-conquer methods.},
  archive      = {J_NN},
  author       = {Heng Lian and Jiamin Liu and Zengyan Fan},
  doi          = {10.1016/j.neunet.2021.06.020},
  journal      = {Neural Networks},
  pages        = {368-376},
  shortjournal = {Neural Netw.},
  title        = {Distributed learning for sketched kernel regression},
  volume       = {143},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). IGAGCN: Information geometry and attention-based
spatiotemporal graph convolutional networks for traffic flow prediction.
<em>NN</em>, <em>143</em>, 355–367. (<a
href="https://doi.org/10.1016/j.neunet.2021.05.035">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, a novel spatiotemporal graph convolutional networks model is proposed for traffic flow prediction in urban road networks by fully considering an information geometry approach and attention-based mechanism. Accurate traffic flow prediction in real urban road networks is challenging due to the presence of dynamic spatiotemporal data and external factors in the urban environment. Moreover, the dynamic spatial and temporal dependencies of urban traffic flow data are very important for predicting traffic flow, and it has been shown that a recent attention mechanism has a relatively good ability to capture these dynamic dependencies, which are not fully considered by most existing algorithms. Therefore, in the novel model abbreviated as IGAGCN, the information geometry method is utilized to determine the dynamic data distribution difference between different sensors. The attention mechanism is employed with the information geometry method, in which a matrix is derived by analyzing the distributions of sensor data, and the spatiotemporal dynamic connections in traffic flow data features are better at capturing the spatial dependencies of traffic between different sensors in urban road networks. Furthermore, a parallel sub-model architecture is proposed to consider long time spans, where each dilated causal convolution sub-model is applied to short time spans. Two well-known data sets were employed to demonstrate that our proposed method obtains better performance and is better at capturing the dynamic spatial dependencies of traffic than the existing only-attention-based models. In addition a real-world urban road network in Shenzhen, China, was studied to test and verify the proposed model.},
  archive      = {J_NN},
  author       = {Jiyao An and Liang Guo and Wei Liu and Zhiqiang Fu and Ping Ren and Xinzhi Liu and Tao Li},
  doi          = {10.1016/j.neunet.2021.05.035},
  journal      = {Neural Networks},
  pages        = {355-367},
  shortjournal = {Neural Netw.},
  title        = {IGAGCN: Information geometry and attention-based spatiotemporal graph convolutional networks for traffic flow prediction},
  volume       = {143},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Graph routing between capsules. <em>NN</em>, <em>143</em>,
345–354. (<a
href="https://doi.org/10.1016/j.neunet.2021.06.018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Routing methods in capsule networks often learn a hierarchical relationship for capsules in successive layers, but the intra-relation between capsules in the same layer is less studied, while this intra-relation is a key factor for the semantic understanding in text data. Therefore, in this paper, we introduce a new capsule network with graph routing to learn both relationships, where capsules in each layer are treated as the nodes of a graph. We investigate strategies to yield adjacency and degree matrix with three different distances from a layer of capsules, and propose the graph routing mechanism between those capsules. We validate our approach on five text classification datasets, and our findings suggest that the approach combining bottom-up routing and top-down attention performs the best. Such an approach demonstrates generalization capability across datasets. Compared to the state-of-the-art routing methods, the improvements in accuracy in the five datasets we used were 0.82, 0.39, 0.07, 1.01, and 0.02, respectively.},
  archive      = {J_NN},
  author       = {Yang Li and Wei Zhao and Erik Cambria and Suhang Wang and Steffen Eger},
  doi          = {10.1016/j.neunet.2021.06.018},
  journal      = {Neural Networks},
  pages        = {345-354},
  shortjournal = {Neural Netw.},
  title        = {Graph routing between capsules},
  volume       = {143},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust cost-sensitive kernel method with blinex loss and its
applications in credit risk evaluation. <em>NN</em>, <em>143</em>,
327–344. (<a
href="https://doi.org/10.1016/j.neunet.2021.06.016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Credit risk evaluation is a crucial yet challenging problem in financial analysis. It can not only help institutions reduce risk and ensure profitability, but also improve consumers’ fair practices. The data-driven algorithms such as artificial intelligence techniques regard the evaluation as a classification problem and aim to classify transactions as default or non-default. Since non-default samples greatly outnumber default samples, it is a typical imbalanced learning problem and each class or each sample needs special treatment. Numerous data-level, algorithm-level and hybrid methods are presented, and cost-sensitive support vector machines (CSSVMs) are representative algorithm-level methods. Based on the minimization of symmetric and unbounded loss functions, CSSVMs impose higher penalties on the misclassification costs of minority instances using domain specific parameters. However, such loss functions as error measurement cannot have an obvious cost-sensitive generalization. In this paper, we propose a robust cost-sensitive kernel method with Blinex loss (CSKB), which can be applied in credit risk evaluation. By inheriting the elegant merits of Blinex loss function, i.e., asymmetry and boundedness , CSKB not only flexibly controls distinct costs for both classes, but also enjoys noise robustness. As a data-driven decision-making paradigm of credit risk evaluation, CSKB can achieve the “win-win” situation for both the financial institutions and consumers. We solve linear and nonlinear CSKB by Nesterov accelerated gradient algorithm and Pegasos algorithm respectively. Moreover, the generalization capability of CSKB is theoretically analyzed. Comprehensive experiments on synthetic, UCI and credit risk evaluation datasets demonstrate that CSKB compares more favorably than other benchmark methods in terms of various measures.},
  archive      = {J_NN},
  author       = {Jingjing Tang and Jiahui Li and Weiqi Xu and Yingjie Tian and Xuchan Ju and Jie Zhang},
  doi          = {10.1016/j.neunet.2021.06.016},
  journal      = {Neural Networks},
  pages        = {327-344},
  shortjournal = {Neural Netw.},
  title        = {Robust cost-sensitive kernel method with blinex loss and its applications in credit risk evaluation},
  volume       = {143},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On the effective initialisation for restricted boltzmann
machines via duality with hopfield model. <em>NN</em>, <em>143</em>,
314–326. (<a
href="https://doi.org/10.1016/j.neunet.2021.06.017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Restricted Boltzmann machines (RBMs) with a binary visible layer of size N N and a Gaussian hidden layer of size P P have been proved to be equivalent to a Hopfield neural network (HNN) made of N N binary neurons and storing P P patterns ξ ξ , as long as the weights w w in the former are identified with the patterns. Here we aim to leverage this equivalence to find effective initialisations for weights in the RBM when what is available is a set of noisy examples of each pattern, aiming to translate statistical mechanics background available for HNN to the study of RBM’s learning and retrieval abilities. In particular, given a set of definite, structureless patterns we build a sample of blurred examples and prove that the initialisation where w w corresponds to the empirical average ξ ¯ ξ¯ over the sample is a fixed point under stochastic gradient descent . Further, as a toy application of the duality between HNN and RBM, we consider the simplest random auto-encoder (a three layer network made of two RBMs coupled by their hidden layer) and evidence that, as long as the parameter setting corresponds to the retrieval region of the dual HNN, reconstruction and denoising can be accomplished trivially, while when the system is in the spin-glass phase inference algorithms are necessary. This questions the need for larger retrieval regions which we obtain by applying a Gram–Schmidt orthogonalisation to the patterns: in fact, this procedure yields to a set of patterns devoid of correlations and for which the largest retrieval region can be accomplished. Finally we consider an application of duality also in a structured case: we test this approach on the MNIST dataset, and obtain that the network performs already ∼ 67\% ∼67\% of successful classifications, suggesting it can be exploited as a computationally-cheap pre-training.},
  archive      = {J_NN},
  author       = {Francesca Elisa Leonelli and Elena Agliari and Linda Albanese and Adriano Barra},
  doi          = {10.1016/j.neunet.2021.06.017},
  journal      = {Neural Networks},
  pages        = {314-326},
  shortjournal = {Neural Netw.},
  title        = {On the effective initialisation for restricted boltzmann machines via duality with hopfield model},
  volume       = {143},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Label propagation via local geometry preserving for deep
semi-supervised image recognition. <em>NN</em>, <em>143</em>, 303–313.
(<a href="https://doi.org/10.1016/j.neunet.2021.06.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel transductive pseudo-labeling based method for deep semi-supervised image recognition. Inspired from the superiority of pseudo labels inferred by label propagation compared with those inferred from network, we argue that information flow from labeled data to unlabeled data should be kept noiseless and with minimum loss. Previous research works use scarce labeled data for feature learning and solely consider the relationship between two feature vectors to construct the similarity graph in feature space, which causes two problems that ultimately lead to noisy and incomplete information flow from labeled data to unlabeled data . The first problem is that the learned feature mapping is highly likely to be biased and can easily over-fit noise. The second problem is the loss of local geometry information in feature space during label propagation. Accordingly, we firstly propose to incorporate self-supervised learning into feature learning for cleaner information flow in feature space during subsequent label propagation. Secondly, we propose to use reconstruction concept to measure pairwise similarity in feature space, such that local geometry information can be preserved. Ablation study confirms synergistic effects from features learned with self-supervision and similarity graph with local geometry preserving. Extensive experiments conducted on benchmark datasets have verified the effectiveness of our proposed method.},
  archive      = {J_NN},
  author       = {Yuanyuan Qing and Yijie Zeng and Guang-Bin Huang},
  doi          = {10.1016/j.neunet.2021.06.007},
  journal      = {Neural Networks},
  pages        = {303-313},
  shortjournal = {Neural Netw.},
  title        = {Label propagation via local geometry preserving for deep semi-supervised image recognition},
  volume       = {143},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Variational policy search using sparse gaussian process
priors for learning multimodal optimal actions. <em>NN</em>,
<em>143</em>, 291–302. (<a
href="https://doi.org/10.1016/j.neunet.2021.06.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Policy search reinforcement learning has been drawing much attention as a method of learning a robot control policy. In particular, policy search using such non-parametric policies as Gaussian process regression can learn optimal actions with high-dimensional and redundant sensors as input. However, previous methods implicitly assume that the optimal action becomes unique for each state. This assumption can severely limit such practical applications as robot manipulations since designing a reward function that appears in only one optimal action for complex tasks is difficult. The previous methods might have caused critical performance deterioration because the typical non-parametric policies cannot capture the optimal actions due to their unimodality. We propose novel approaches in non-parametric policy searches with multiple optimal actions and offer two different algorithms commonly based on a sparse Gaussian process prior and variational Bayesian inference . The following are the key ideas: (1) multimodality for capturing multiple optimal actions and (2) mode-seeking for capturing one optimal action by ignoring the others. First, we propose a multimodal sparse Gaussian process policy search that uses multiple overlapped GPs as a prior. Second, we propose a mode-seeking sparse Gaussian process policy search that uses the student-t distribution for a likelihood function. The effectiveness of those algorithms is demonstrated through applications to object manipulation tasks with multiple optimal actions in simulations.},
  archive      = {J_NN},
  author       = {Hikaru Sasaki and Takamitsu Matsubara},
  doi          = {10.1016/j.neunet.2021.06.010},
  journal      = {Neural Networks},
  pages        = {291-302},
  shortjournal = {Neural Netw.},
  title        = {Variational policy search using sparse gaussian process priors for learning multimodal optimal actions},
  volume       = {143},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptive neural network asymptotic tracking control for
nonstrict feedback stochastic nonlinear systems. <em>NN</em>,
<em>143</em>, 283–290. (<a
href="https://doi.org/10.1016/j.neunet.2021.06.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The adaptive neural network asymptotic tracking control issue of nonstrict feedback stochastic nonlinear systems is studied in our article by adopting backstepping algorithm. Compared with the existing research, the hypothesis about unknown virtual control coefficients (UVCC) is overcome in the control design. By using the bound estimation scheme and some smooth functions, associating with approximation-based neural network , the asymptotic tracking controller is recursively constructed. With the aid of Lyapunov function and beneficial inequalities, the asymptotic convergence character and stability with stochastic disturbance and unknown UVCC can be ensured. Finally, the theoretical finding is verified via a simulation example.},
  archive      = {J_NN},
  author       = {Yongchao Liu and Qidan Zhu},
  doi          = {10.1016/j.neunet.2021.06.011},
  journal      = {Neural Networks},
  pages        = {283-290},
  shortjournal = {Neural Netw.},
  title        = {Adaptive neural network asymptotic tracking control for nonstrict feedback stochastic nonlinear systems},
  volume       = {143},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Exponential passivity of discrete-time switched neural
networks with transmission delays via an event-triggered sliding mode
control. <em>NN</em>, <em>143</em>, 271–282. (<a
href="https://doi.org/10.1016/j.neunet.2021.06.014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the exponential passivity of discrete-time switched neural networks (DSNNs) with transmission delays via an event-triggered sliding mode control (SMC). Firstly, a novel discrete-time switched SMC scheme is constructed on the basis of sliding mode control method and event-triggered mechanism. Next, a state observer with transmission delays is designed to estimate the system state. Moreover, some new weighted summation inequalities are further proposed to effectively evaluate the exponential passivity criteria for the closed-loop system. Finally, the effectiveness of theoretical results is showed through a simulative analysis on a multi-area power system .},
  archive      = {J_NN},
  author       = {Jinling Wang and Haijun Jiang and Cheng Hu and Tianlong Ma},
  doi          = {10.1016/j.neunet.2021.06.014},
  journal      = {Neural Networks},
  pages        = {271-282},
  shortjournal = {Neural Netw.},
  title        = {Exponential passivity of discrete-time switched neural networks with transmission delays via an event-triggered sliding mode control},
  volume       = {143},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). HiAM: A hierarchical attention based model for knowledge
graph multi-hop reasoning. <em>NN</em>, <em>143</em>, 261–270. (<a
href="https://doi.org/10.1016/j.neunet.2021.06.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning to reason in large-scale knowledge graphs has attracted much attention from research communities recently. This paper targets a practical task of multi-hop reasoning in knowledge graphs, which can be applied in various downstream tasks such as question answering, and recommender systems . A key challenge in multi-hop reasoning is to synthesize structural information (e.g., paths) in knowledge graphs to perform deeper reasoning. Existing methods usually focus on connection paths between each entity pair. However, these methods ignore predecessor paths before connection paths and regard entities and relations within every single path as equally important. With our observations, predecessor paths before connection paths can provide more accurate semantic representations . Furthermore, entities and relations in a single path contribute variously to the right answers. To this end, we propose a novel model HiAM ( Hi erarchical A ttention based M odel) for knowledge graph multi-hop reasoning. HiAM makes use of predecessor paths to provide more accurate semantics for entities and explores the effects of different granularities . Firstly, we extract predecessor paths of head entities and connection paths between each entity pair. Then, a hierarchical attention mechanism is designed to capture the information of different granularities , including entity/relation-level and path-level features. Finally, multi-granularity features are fused together to predict the right answers. We go one step further to select the most significant path as the explanation for predicted answers. Comprehensive experimental results demonstrate that our method achieves competitive performance compared with the baselines on three benchmark datasets.},
  archive      = {J_NN},
  author       = {Ting Ma and Shangwen Lv and Longtao Huang and Songlin Hu},
  doi          = {10.1016/j.neunet.2021.06.008},
  journal      = {Neural Networks},
  pages        = {261-270},
  shortjournal = {Neural Netw.},
  title        = {HiAM: A hierarchical attention based model for knowledge graph multi-hop reasoning},
  volume       = {143},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Effective and direct control of neural TTS prosody by
removing interactions between different attributes. <em>NN</em>,
<em>143</em>, 250–260. (<a
href="https://doi.org/10.1016/j.neunet.2021.06.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {End-to-end TTS advancement has shown that synthesized speech prosody can be controlled by conditioning the decoder with speech prosody attribute labels. However, to annotate quantitatively the prosody patterns of a large set of training data is both time consuming and expensive. To use unannotated data, variational autoencoder (VAE) has been proposed to model individual prosody attribute as a random variable in the latent space. The VAE is an unsupervised approach and the corresponding latent variables are in general correlated with each other. For more effective and direct control of speech prosody along each attribute dimension, it is highly desirable to disentangle the correlated latent variables. Additionally, being able to interpret the disentangled attributes as speech perceptual cues is useful for designing more efficient prosody control of TTS. In this paper, we propose two attribute separation schemes: (1) using 3 separate VAEs to model the real-valued, different prosodic features, i.e., F 0 F0 , energy and duration; (2) minimizing mutual information between different prosody attributes to remove their mutual correlations, for facilitating more direct prosody control. Experimental results confirm that the two proposed schemes can indeed make individual prosody attributes more interpretable and direct TTS prosody control more effective. The improvements are measured objectively by F 0 F0 Frame Error (FFE) and subjectively with MOS and A/B comparison listening tests, respectively. The scatter diagrams of t-SNE also demonstrate the correlations between prosody attributes, which are well disentangled by minimizing their mutual information. Synthesized TTS samples can be found at https://xiaochunan.github.io/prosody/index.html .},
  archive      = {J_NN},
  author       = {Xiaochun An and Frank K. Soong and Shan Yang and Lei Xie},
  doi          = {10.1016/j.neunet.2021.06.006},
  journal      = {Neural Networks},
  pages        = {250-260},
  shortjournal = {Neural Netw.},
  title        = {Effective and direct control of neural TTS prosody by removing interactions between different attributes},
  volume       = {143},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Finite time convergence of pinning synchronization with a
single nonlinear controller. <em>NN</em>, <em>143</em>, 246–249. (<a
href="https://doi.org/10.1016/j.neunet.2021.05.036">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we discuss distributive synchronization of complex networks in finite time, with a single nonlinear pinning controller. The results apply to heterogeneous dynamic networks, too. Different from many models, which assume the coupling matrix being symmetric (or the connecting graph is undirected), here, the coupling matrix is asymmetric (or the connecting graph is directed).},
  archive      = {J_NN},
  author       = {Tianping Chen and Wenlian Lu and Xiwei Liu},
  doi          = {10.1016/j.neunet.2021.05.036},
  journal      = {Neural Networks},
  pages        = {246-249},
  shortjournal = {Neural Netw.},
  title        = {Finite time convergence of pinning synchronization with a single nonlinear controller},
  volume       = {143},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Periodicity and multi-periodicity generated by impulses
control in delayed cohen–grossberg-type neural networks with
discontinuous activations. <em>NN</em>, <em>143</em>, 230–245. (<a
href="https://doi.org/10.1016/j.neunet.2021.06.013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper discusses the periodicity and multi-periodicity in delayed Cohen–Grossberg-type neural networks (CGNNs) possessing impulsive effects, whose activation functions possess discontinuities and are allowed to be unbounded or nonmonotonic. Based on differential inclusion and cone expansion–compression fixed-point theory of set-valued mapping, several improved criteria are given to derive the positive solution with ω ω -periodicity and ω ω -multi-periodicity for delayed CGNNs under impulsive control. These ω ω -periodicity/ ω ω -multi-periodicity orbits are produced by impulses control. The analytical method and theoretical results presented in this paper are of certain significance to the design of neural network models or circuits possessing discontinuous neuron activation and impulsive effects in periodic environment.},
  archive      = {J_NN},
  author       = {Zuowei Cai and Lihong Huang and Zengyun Wang and Xianmin Pan and Shukun Liu},
  doi          = {10.1016/j.neunet.2021.06.013},
  journal      = {Neural Networks},
  pages        = {230-245},
  shortjournal = {Neural Netw.},
  title        = {Periodicity and multi-periodicity generated by impulses control in delayed Cohen–Grossberg-type neural networks with discontinuous activations},
  volume       = {143},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The asymmetric learning rates of murine exploratory behavior
in sparse reward environments. <em>NN</em>, <em>143</em>, 218–229. (<a
href="https://doi.org/10.1016/j.neunet.2021.05.030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Goal-oriented behaviors of animals can be modeled by reinforcement learning algorithms. Such algorithms predict future outcomes of selected actions utilizing action values and updating those values in response to the positive and negative outcomes. In many models of animal behavior, the action values are updated symmetrically based on a common learning rate , that is, in the same way for both positive and negative outcomes. However, animals in environments with scarce rewards may have uneven learning rates. To investigate the asymmetry in learning rates in reward and non-reward, we analyzed the exploration behavior of mice in five-armed bandit tasks using a Q-learning model with differential learning rates for positive and negative outcomes. The positive learning rate was significantly higher in a scarce reward environment than in a rich reward environment, and conversely, the negative learning rate was significantly lower in the scarce environment. The positive to negative learning rate ratio was about 10 in the scarce environment and about 2 in the rich environment. This result suggests that when the reward probability was low, the mice tend to ignore failures and exploit the rare rewards. Computational modeling analysis revealed that the increased learning rates ratio could cause an overestimation of and perseveration on rare-rewarding events, increasing total reward acquisition in the scarce environment but disadvantaging impartial exploration.},
  archive      = {J_NN},
  author       = {Hiroyuki Ohta and Kuniaki Satori and Yu Takarada and Masashi Arake and Toshiaki Ishizuka and Yuji Morimoto and Tatsuji Takahashi},
  doi          = {10.1016/j.neunet.2021.05.030},
  journal      = {Neural Networks},
  pages        = {218-229},
  shortjournal = {Neural Netw.},
  title        = {The asymmetric learning rates of murine exploratory behavior in sparse reward environments},
  volume       = {143},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). How to handle noisy labels for robust learning from
uncertainty. <em>NN</em>, <em>143</em>, 209–217. (<a
href="https://doi.org/10.1016/j.neunet.2021.06.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most deep neural networks (DNNs) are trained with large amounts of noisy labels when they are applied. As DNNs have the high capacity to fit any noisy labels, it is known to be difficult to train DNNs robustly with noisy labels. These noisy labels cause the performance degradation of DNNs due to the memorization effect by over-fitting. Earlier state-of-the-art methods used small loss tricks to efficiently resolve the robust training problem with noisy labels. In this paper, relationship between the uncertainties and the clean labels is analyzed. We present novel training method to use not only small loss trick but also labels that are likely to be clean labels selected from uncertainty called “Uncertain Aware Co-Training (UACT)”. Our robust learning techniques (UACT) avoid over-fitting the DNNs by extremely noisy labels. By making better use of the uncertainty acquired from the network itself, we achieve good generalization performance . We compare the proposed method to the current state-of-the-art algorithms for noisy versions of MNIST, CIFAR-10, CIFAR-100, T-ImageNet and News to demonstrate its excellence.},
  archive      = {J_NN},
  author       = {Daehyun Ji and Dokwan Oh and Yoonsuk Hyun and Oh-Min Kwon and Myeong-Jin Park},
  doi          = {10.1016/j.neunet.2021.06.012},
  journal      = {Neural Networks},
  pages        = {209-217},
  shortjournal = {Neural Netw.},
  title        = {How to handle noisy labels for robust learning from uncertainty},
  volume       = {143},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fast mesh data augmentation via chebyshev polynomial of
spectral filtering. <em>NN</em>, <em>143</em>, 198–208. (<a
href="https://doi.org/10.1016/j.neunet.2021.05.025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks have recently been recognized as one of the powerful learning techniques in computer vision and medical image analysis. Trained deep neural networks need to be generalizable to new data that are not seen before. In practice, there is often insufficient training data available, which can be solved via data augmentation . Nevertheless, there is a lack of augmentation methods to generate data on graphs or surfaces, even though graph convolutional neural network (graph-CNN) has been widely used in deep learning . This study proposed two unbiased augmentation methods, Laplace–Beltrami eigenfunction Data Augmentation (LB-eigDA) and Chebyshev polynomial Data Augmentation (C-pDA), to generate new data on surfaces, whose mean was the same as that of observed data. LB-eigDA augmented data via the resampling of the LB coefficients. In parallel with LB-eigDA, we introduced a fast augmentation approach, C-pDA, that employed a polynomial approximation of LB spectral filters on surfaces. We designed LB spectral bandpass filters by Chebyshev polynomial approximation and resampled signals filtered via these filters in order to generate new data on surfaces. We first validated LB-eigDA and C-pDA via simulated data and demonstrated their use for improving classification accuracy . We then employed brain images of the Alzheimer’s Disease Neuroimaging Initiative (ADNI) and extracted cortical thickness that was represented on the cortical surface to illustrate the use of the two augmentation methods. We demonstrated that augmented cortical thickness had a similar pattern to observed data. We also showed that C-pDA was faster than LB-eigDA and can improve the AD classification accuracy of graph-CNN.},
  archive      = {J_NN},
  author       = {Shih-Gu Huang and Moo K. Chung and Anqi Qiu and Alzheimer’s Disease Neuroimaging Initiative},
  doi          = {10.1016/j.neunet.2021.05.025},
  journal      = {Neural Networks},
  pages        = {198-208},
  shortjournal = {Neural Netw.},
  title        = {Fast mesh data augmentation via chebyshev polynomial of spectral filtering},
  volume       = {143},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A novel density-based neural mass model for simulating
neuronal network dynamics with conductance-based synapses and membrane
current adaptation. <em>NN</em>, <em>143</em>, 183–197. (<a
href="https://doi.org/10.1016/j.neunet.2021.06.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite its success in understanding brain rhythms, the neural mass model, as a low-dimensional mean-field network model, is phenomenological in nature, so that it cannot replicate some of rich repertoire of responses seen in real neuronal tissues. Here, using a colored-synapse population density method, we derived a novel neural mass model, termed density-based neural mass model (dNMM), as the mean-field description of network dynamics of adaptive exponential integrate-and-fire (aEIF) neurons, in which two critical neuronal features, i.e., voltage-dependent conductance-based synaptic interactions and adaptation of firing rate responses, were included. Our results showed that the dNMM was capable of correctly estimating firing rate responses of a neuronal population of aEIF neurons receiving stationary or time-varying excitatory and inhibitory inputs. Finally, it was also able to quantitatively describe the effect of spike-frequency adaptation in the generation of asynchronous irregular activity of excitatory–inhibitory cortical networks . We conclude that in terms of its biological reality and calculation efficiency, the dNMM is a suitable candidate to build significantly large-scale network models involving multiple brain areas, where the neuronal population is the smallest dynamic unit.},
  archive      = {J_NN},
  author       = {Chih-Hsu Huang and Chou-Ching K. Lin},
  doi          = {10.1016/j.neunet.2021.06.009},
  journal      = {Neural Networks},
  pages        = {183-197},
  shortjournal = {Neural Netw.},
  title        = {A novel density-based neural mass model for simulating neuronal network dynamics with conductance-based synapses and membrane current adaptation},
  volume       = {143},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Correlating subword articulation with lip shapes for
embedding aware audio-visual speech enhancement. <em>NN</em>,
<em>143</em>, 171–182. (<a
href="https://doi.org/10.1016/j.neunet.2021.06.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a visual embedding approach to improve embedding aware speech enhancement (EASE) by synchronizing visual lip frames at the phone and place of articulation levels. We first extract visual embedding from lip frames using a pre-trained phone or articulation place recognizer for visual-only EASE (VEASE). Next, we extract audio-visual embedding from noisy speech and lip frames in an information intersection manner, utilizing a complementarity of audio and visual features for multi-modal EASE (MEASE). Experiments on the TCD-TIMIT corpus corrupted by simulated additive noises show that our proposed subword based VEASE approach is more effective than conventional embedding at the word level. Moreover, visual embedding at the articulation place level, leveraging upon a high correlation between place of articulation and lip shapes, demonstrates an even better performance than that at the phone level. Finally the experiments establish that the proposed MEASE framework, incorporating both audio and visual embeddings, yields significantly better speech quality and intelligibility than those obtained with the best visual-only and audio-only EASE systems.},
  archive      = {J_NN},
  author       = {Hang Chen and Jun Du and Yu Hu and Li-Rong Dai and Bao-Cai Yin and Chin-Hui Lee},
  doi          = {10.1016/j.neunet.2021.06.003},
  journal      = {Neural Networks},
  pages        = {171-182},
  shortjournal = {Neural Netw.},
  title        = {Correlating subword articulation with lip shapes for embedding aware audio-visual speech enhancement},
  volume       = {143},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Synchronization of recurrent neural networks with unbounded
delays and time-varying coefficients via generalized differential
inequalities. <em>NN</em>, <em>143</em>, 161–170. (<a
href="https://doi.org/10.1016/j.neunet.2021.05.022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we revisit the drive-response synchronization of a class of recurrent neural networks with unbounded delays and time-varying coefficients, contrary to usual in the literature about time-varying neural networks , the signs of self-feedback coefficients are permitted to be indefinite or the time-varying coefficients can be unbounded. A generalized scalar delay differential inequality considering indefinite self-feedback coefficient and unbounded delay simultaneously is established, which covers the existing result with bounded delay, the applicabilities of the sufficient conditions are discussed. Some novel criteria for network synchronization are then derived by constructing different candidate functions. These results have been improved in some aspects compared with the existing ones. Differential inequality in vector form is also derived to obtain a more refined synchronization criterion which removes some strong assumptions. Three examples are presented to verify the effectiveness and show the superiorities of our theoretical results.},
  archive      = {J_NN},
  author       = {Hao Zhang and Zhigang Zeng},
  doi          = {10.1016/j.neunet.2021.05.022},
  journal      = {Neural Networks},
  pages        = {161-170},
  shortjournal = {Neural Netw.},
  title        = {Synchronization of recurrent neural networks with unbounded delays and time-varying coefficients via generalized differential inequalities},
  volume       = {143},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Periodic clustering of simple and complex cells in visual
cortex. <em>NN</em>, <em>143</em>, 148–160. (<a
href="https://doi.org/10.1016/j.neunet.2021.06.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neurons in the primary visual cortex (V1) are often classified as simple or complex cells, but it is debated whether they are discrete hierarchical classes of neurons or if they represent a continuum of variation within a single class of cells. Herein, we show that simple and complex cells may arise commonly from the feedforward projections from the retina. From analysis of the cortical receptive fields in cats, we show evidence that simple and complex cells originate from the periodic variation of ON–OFF segregation in the feedforward projection of retinal mosaics, by which they organize into periodic clusters in V1. From data in cats, we observed that clusters of simple and complex receptive fields correlate topographically with orientation maps, which supports our model prediction. Our results suggest that simple and complex cells are not two distinct neural populations but arise from common retinal afferents, simultaneous with orientation tuning.},
  archive      = {J_NN},
  author       = {Gwangsu Kim and Jaeson Jang and Se-Bum Paik},
  doi          = {10.1016/j.neunet.2021.06.002},
  journal      = {Neural Networks},
  pages        = {148-160},
  shortjournal = {Neural Netw.},
  title        = {Periodic clustering of simple and complex cells in visual cortex},
  volume       = {143},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A hybrid quantum–classical neural network with deep residual
learning. <em>NN</em>, <em>143</em>, 133–147. (<a
href="https://doi.org/10.1016/j.neunet.2021.05.028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inspired by the success of classical neural networks , there has been tremendous effort to develop classical effective neural networks into quantum concept. In this paper, a novel hybrid quantum–classical neural network with deep residual learning (Res-HQCNN) is proposed. We firstly analyse how to connect residual block structure with a quantum neural network, and give the corresponding training algorithm . At the same time, the advantages and disadvantages of transforming deep residual learning into quantum concept are provided. As a result, the model can be trained in an end-to-end fashion, analogue to the backpropagation in classical neural networks. To explore the effectiveness of Res-HQCNN , we perform extensive experiments for quantum data with or without noisy on classical computer. The experimental results show the Res-HQCNN performs better to learn an unknown unitary transformation and has stronger robustness for noisy data, when compared to state of the arts. Moreover, the possible methods of combining residual learning with quantum neural networks are also discussed.},
  archive      = {J_NN},
  author       = {Yanying Liang and Wei Peng and Zhu-Jun Zheng and Olli Silvén and Guoying Zhao},
  doi          = {10.1016/j.neunet.2021.05.028},
  journal      = {Neural Networks},
  pages        = {133-147},
  shortjournal = {Neural Netw.},
  title        = {A hybrid quantum–classical neural network with deep residual learning},
  volume       = {143},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Neural optimal tracking control of constrained nonaffine
systems with a wastewater treatment application. <em>NN</em>,
<em>143</em>, 121–132. (<a
href="https://doi.org/10.1016/j.neunet.2021.05.027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we aim to solve the optimal tracking control problem for a class of nonaffine discrete-time systems with actuator saturation. First, a data-based neural identifier is constructed to learn the unknown system dynamics . Then, according to the expression of the trained neural identifier, we can obtain the steady control corresponding to the reference trajectory . Next, by involving the iterative dual heuristic dynamic programming algorithm , the new costate function and the tracking control law are developed. Two other neural networks are used to estimate the costate function and approximate the tracking control law. Considering approximation errors of neural networks , the stability analysis of the proposed algorithm for the specific systems is provided by introducing the Lyapunov approach. Finally, via conducting simulation and comparison, the superiority of the developed optimal tracking method is confirmed. Moreover, the trajectory tracking performance of the wastewater treatment application is also involved for further verifying the proposed approach.},
  archive      = {J_NN},
  author       = {Ding Wang and Mingming Zhao and Mingming Ha and Jin Ren},
  doi          = {10.1016/j.neunet.2021.05.027},
  journal      = {Neural Networks},
  pages        = {121-132},
  shortjournal = {Neural Netw.},
  title        = {Neural optimal tracking control of constrained nonaffine systems with a wastewater treatment application},
  volume       = {143},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Locality preserving dense graph convolutional networks with
graph context-aware node representations. <em>NN</em>, <em>143</em>,
108–120. (<a
href="https://doi.org/10.1016/j.neunet.2021.05.031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph convolutional networks (GCNs) have been widely used for representation learning on graph data, which can capture structural patterns on a graph via specifically designed convolution and readout operations. In many graph classification applications, GCN-based approaches have outperformed traditional methods. However, most of the existing GCNs are inefficient to preserve local information of graphs — a limitation that is especially problematic for graph classification. In this work, we propose a locality-preserving dense GCN with graph context-aware node representations. Specifically, our proposed model incorporates a local node feature reconstruction module to preserve initial node features into node representations, which is realized via a simple but effective encoder–decoder mechanism. To capture local structural patterns in neighborhoods representing different ranges of locality, dense connectivity is introduced to connect each convolutional layer and its corresponding readout with all previous convolutional layers. To enhance node representativeness, the output of each convolutional layer is concatenated with the output of the previous layer’s readout to form a global context-aware node representation. In addition, a self-attention module is introduced to aggregate layer-wise representations to form the final graph-level representation. Experiments on benchmark datasets demonstrate the superiority of the proposed model over state-of-the-art methods in terms of classification accuracy .},
  archive      = {J_NN},
  author       = {Wenfeng Liu and Maoguo Gong and Zedong Tang and A.K. Qin and Kai Sheng and Mingliang Xu},
  doi          = {10.1016/j.neunet.2021.05.031},
  journal      = {Neural Networks},
  pages        = {108-120},
  shortjournal = {Neural Netw.},
  title        = {Locality preserving dense graph convolutional networks with graph context-aware node representations},
  volume       = {143},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Convolutional fusion network for monaural speech
enhancement. <em>NN</em>, <em>143</em>, 97–107. (<a
href="https://doi.org/10.1016/j.neunet.2021.05.017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural network (CNN) based methods, such as the convolutional encoder–decoder network, offer state-of-the-art results in monaural speech enhancement. In the conventional encoder–decoder network, large kernel size is often used to enhance the model capacity, which, however, results in low parameter efficiency. This could be addressed by using group convolution , as in AlexNet, where group convolutions are performed in parallel in each layer, before their outputs are concatenated. However, with the simple concatenation, the inter-channel dependency information may be lost. To address this, the Shuffle network re-arranges the outputs of each group before concatenating them, by taking part of the whole input sequence as the input to each group of convolution . In this work, we propose a new convolutional fusion network (CFN) for monaural speech enhancement by improving model performance, inter-channel dependency, information reuse and parameter efficiency. First, a new group convolutional fusion unit (GCFU) consisting of the standard and depth-wise separable CNN is used to reconstruct the signal. Second, the whole input sequence (full information) is fed simultaneously to two convolution networks in parallel, and their outputs are re-arranged (shuffled) and then concatenated, in order to exploit the inter-channel dependency within the network. Third, the intra skip connection mechanism is used to connect different layers inside the encoder as well as decoder to further improve the model performance. Extensive experiments are performed to show the improved performance of the proposed method as compared with three recent baseline methods .},
  archive      = {J_NN},
  author       = {Yang Xian and Yang Sun and Wenwu Wang and Syed Mohsen Naqvi},
  doi          = {10.1016/j.neunet.2021.05.017},
  journal      = {Neural Networks},
  pages        = {97-107},
  shortjournal = {Neural Netw.},
  title        = {Convolutional fusion network for monaural speech enhancement},
  volume       = {143},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A semi-supervised zero-shot image classification method
based on soft-target. <em>NN</em>, <em>143</em>, 88–96. (<a
href="https://doi.org/10.1016/j.neunet.2021.05.019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zero-shot learning (ZSL) aims at training a classification model with data only from seen categories to recognize data from disjoint unseen categories. Domain shift and generalization capability are two fundamental challenges in ZSL. In this paper, we address them with a novel Soft-Target Semi-supervised Classification (STSC) model. Specifically, an autoencoder network is leveraged, where both labeled seen data from the seen categories and unlabeled ancillary data collected from Internet or other datasets are employed as two branches, respectively. For the branch of labeled seen data, side information are employed as the latent vectors to separately connect the input of encoder and the output of decoder. In this way, visual and side information are implicitly aligned. For the branch of unlabeled ancillary data, it explicitly strengthens the reconstruction ability of the network. Meanwhile, these ancillary data can be viewed as a smooth to the domain distribution, which contributes to the alleviation of the domain shift problem. To further guarantee the generation ability, a Softmax-T loss function is proposed by making full use of the soft target. Extensive experiments on three benchmark datasets show the superiority of the proposed approach under tasks of both traditional zero-shot learning and generalized zero-shot learning.},
  archive      = {J_NN},
  author       = {Zhong Ji and Qiang Wang and Biying Cui and Yanwei Pang and Xianbin Cao and Xuelong Li},
  doi          = {10.1016/j.neunet.2021.05.019},
  journal      = {Neural Networks},
  pages        = {88-96},
  shortjournal = {Neural Netw.},
  title        = {A semi-supervised zero-shot image classification method based on soft-target},
  volume       = {143},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A brain-inspired computational model for spatio-temporal
information processing. <em>NN</em>, <em>143</em>, 74–87. (<a
href="https://doi.org/10.1016/j.neunet.2021.05.015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatio-temporal information processing is fundamental in both brain functions and AI applications. Current strategies for spatio-temporal pattern recognition usually involve explicit feature extraction followed by feature aggregation, which requires a large amount of labeled data. In the present study, motivated by the subcortical visual pathway and early stages of the auditory pathway for motion and sound processing, we propose a novel brain-inspired computational model for generic spatio-temporal pattern recognition. The model consists of two modules, a reservoir module and a decision-making module. The former projects complex spatio-temporal patterns into spatially separated neural representations via its recurrent dynamics, the latter reads out neural representations via integrating information over time, and the two modules are linked together using known examples. Using synthetic data, we demonstrate that the model can extract the frequency and order information of temporal inputs. We apply the model to reproduce the looming pattern discrimination behavior as observed in experiments successfully. Furthermore, we apply the model to the gait recognition task, and demonstrate that our model accomplishes the recognition in an event-based manner and outperforms deep learning counterparts when training data is limited.},
  archive      = {J_NN},
  author       = {Xiaohan Lin and Xiaolong Zou and Zilong Ji and Tiejun Huang and Si Wu and Yuanyuan Mi},
  doi          = {10.1016/j.neunet.2021.05.015},
  journal      = {Neural Networks},
  pages        = {74-87},
  shortjournal = {Neural Netw.},
  title        = {A brain-inspired computational model for spatio-temporal information processing},
  volume       = {143},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adversarial orthogonal regression: Two non-linear
regressions for causal inference. <em>NN</em>, <em>143</em>, 66–73. (<a
href="https://doi.org/10.1016/j.neunet.2021.05.018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose two nonlinear regression methods , namely, Adversarial Orthogonal Regression (AdOR) for additive noise models and Adversarial Orthogonal Structural Equation Model (AdOSE) for the general case of structural equation models. Both methods try to make the residual of regression independent from regressors , while putting no assumption on noise distribution. In both methods, two adversarial networks are trained simultaneously where a regression network outputs predictions and a loss network that estimates mutual information (in AdOR) and KL-divergence (in AdOSE). These methods can be formulated as a minimax two-player game; at equilibrium, AdOR finds a deterministic map between inputs and output and estimates mutual information between residual and inputs, while AdOSE estimates a conditional probability distribution of output given inputs. The proposed methods can be used as subroutines to address several learning problems in causality, such as causal direction determination (or more generally, causal structure learning) and causal model estimation. Experimental results on both synthetic and real-world data demonstrate that the proposed methods have remarkable performance with respect to previous solutions.},
  archive      = {J_NN},
  author       = {M. Reza Heydari and Saber Salehkaleybar and Kun Zhang},
  doi          = {10.1016/j.neunet.2021.05.018},
  journal      = {Neural Networks},
  pages        = {66-73},
  shortjournal = {Neural Netw.},
  title        = {Adversarial orthogonal regression: Two non-linear regressions for causal inference},
  volume       = {143},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A continuous-time neurodynamic approach and its
discretization for distributed convex optimization over multi-agent
systems. <em>NN</em>, <em>143</em>, 52–65. (<a
href="https://doi.org/10.1016/j.neunet.2021.05.020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributed optimization problem (DOP) over multi-agent systems, which can be described by minimizing the sum of agents’ local objective functions, has recently attracted widespread attention owing to its applications in diverse domains. In this paper, inspired by penalty method and subgradient descent method, a continuous-time neurodynamic approach is proposed for solving a DOP with inequality and set constraints. The state of continuous-time neurodynamic approach exists globally and converges to an optimal solution of the considered DOP. Comparisons reveal that the proposed neurodynamic approach can not only resolve more general convex DOPs, but also has lower dimension of solution space. Additionally, the discretization of the neurodynamic approach is also introduced for the convenience of implementation in practice. The iteration sequence of discrete-time method is also convergent to an optimal solution of DOP from any initial point. The effectiveness of the neurodynamic approach is verified by simulation examples and an application in L 1 L1 -norm minimization problem in the end.},
  archive      = {J_NN},
  author       = {Xingnan Wen and Linhua Luan and Sitian Qin},
  doi          = {10.1016/j.neunet.2021.05.020},
  journal      = {Neural Networks},
  pages        = {52-65},
  shortjournal = {Neural Netw.},
  title        = {A continuous-time neurodynamic approach and its discretization for distributed convex optimization over multi-agent systems},
  volume       = {143},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Experimental stability analysis of neural networks in
classification problems with confidence sets for persistence diagrams.
<em>NN</em>, <em>143</em>, 42–51. (<a
href="https://doi.org/10.1016/j.neunet.2021.05.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate classification performance of neural networks (NNs) based on topological insight in an attempt to guarantee stability of their inference. NNs which can accurately classify a dataset map it into a hidden space while disentangling intertwined data. NNs sometimes acquire forcible mapping to disentangle the data, and this forcible mapping generates outliers. The mapping around the outliers is unstable because the outputs change drastically. Hence, we define stable NNs to mean that they do not generate outliers. To investigate the possibility of the existence of outliers, we use persistent homology and a method to estimate the confidence set for persistence diagrams. The combined use enables us to test whether the focused geometry is topologically simple, that is, no outliers. In this work, we use the MNIST and CIFAR-10 datasets and investigate the relationship between the classification performance and the topological characteristics with several NNs. Investigation results with the MNIST dataset show that the test accuracy of all the networks is superior, exceeding 98\%, even though the transformed dataset is not topologically simple. Results with the CIFAR-10 dataset also show that the possibility of the existence of outliers is shown in the mapping by the accurate convolutional NNs. Therefore, we conclude that the presented investigation is necessary to guarantee that the NNs, in particular deep NNs, do not acquire unstable mapping for forcible classification.},
  archive      = {J_NN},
  author       = {Naoki Akai and Takatsugu Hirayama and Hiroshi Murase},
  doi          = {10.1016/j.neunet.2021.05.007},
  journal      = {Neural Networks},
  pages        = {42-51},
  shortjournal = {Neural Netw.},
  title        = {Experimental stability analysis of neural networks in classification problems with confidence sets for persistence diagrams},
  volume       = {143},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning to recognize while learning to speak:
Self-supervision and developing a speaking motor. <em>NN</em>,
<em>143</em>, 28–41. (<a
href="https://doi.org/10.1016/j.neunet.2021.05.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditionally, learning speech synthesis and speech recognition were investigated as two separate tasks. This separation hinders incremental development for concurrent synthesis and recognition, where partially-learned synthesis and partially-learned recognition must help each other throughout lifelong learning . This work is a paradigm shift—we treat synthesis and recognition as two intertwined aspects of a lifelong learning agent. Furthermore, in contrast to existing recognition or synthesis systems, babies do not need their mothers to directly supervise their vocal tracts at every moment during the learning. We argue that self-generated non-symbolic states/actions at fine-grained time level help such a learner as necessary temporal contexts. Here, we approach a new and challenging problem—how to enable an autonomous learning system to develop an artificial speaking motor for generating temporally-dense (e.g., frame-wise) actions on the fly without human handcrafting a set of symbolic states. The self-generated states/actions are Muscles-like, High-dimensional, Temporally-dense and Globally-smooth (MHTG), so that these states/actions are directly attended for concurrent synthesis and recognition for each time frame. Human teachers are relieved from supervising learner’s motor ends. The Candid Covariance-free Incremental (CCI) Principal Component Analysis (PCA) is applied to develop such an artificial speaking motor where PCA features drive the motor. Since each life must develop normally, each Developmental Network-2 (DN-2) reaches the same network (maximum likelihood, ML) regardless of randomly initialized weights, where ML is not just for a function approximator but rather an emergent Turing Machine . The machine-synthesized sounds are evaluated by both the neural network and humans with recognition experiments. Our experimental results showed learning-to-synthesize and learning-to-recognize-through-synthesis for phonemes. This work corresponds to a key step toward our goal to close a great gap toward fully autonomous machine learning directly from the physical world.},
  archive      = {J_NN},
  author       = {Xiang Wu and Juyang Weng},
  doi          = {10.1016/j.neunet.2021.05.006},
  journal      = {Neural Networks},
  pages        = {28-41},
  shortjournal = {Neural Netw.},
  title        = {Learning to recognize while learning to speak: Self-supervision and developing a speaking motor},
  volume       = {143},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A theory of capacity and sparse neural encoding.
<em>NN</em>, <em>143</em>, 12–27. (<a
href="https://doi.org/10.1016/j.neunet.2021.05.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by biological considerations, we study sparse neural maps from an input layer to a target layer with sparse activity, and specifically the problem of storing K K input-target associations ( x , y ) (x,y) , or memories, when the target vectors y y are sparse. We mathematically prove that K K undergoes a phase transition and that in general, and somewhat paradoxically, sparsity in the target layers increases the storage capacity of the map. The target vectors can be chosen arbitrarily, including in random fashion, and the memories can be both encoded and decoded by networks trained using local learning rules , including the simple Hebb rule. These results are robust under a variety of statistical assumptions on the data. The proofs rely on elegant properties of random polytopes and sub-gaussian random vector variables. Open problems and connections to capacity theories and polynomial threshold maps are discussed.},
  archive      = {J_NN},
  author       = {Pierre Baldi and Roman Vershynin},
  doi          = {10.1016/j.neunet.2021.05.005},
  journal      = {Neural Networks},
  pages        = {12-27},
  shortjournal = {Neural Netw.},
  title        = {A theory of capacity and sparse neural encoding},
  volume       = {143},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Augmented semantic feature based generative network for
generalized zero-shot learning. <em>NN</em>, <em>143</em>, 1–11. (<a
href="https://doi.org/10.1016/j.neunet.2021.04.014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zero-shot learning (ZSL) aims to recognize objects in images when no training data is available for the object classes. Under generalized zero-shot learning (GZSL) setting, the test objects belong to seen or unseen categories. In many recent studies, zero-shot learning is performed by leveraging generative networks to synthesize visual features for unseen class from class-specific semantic features . The user-defined semantic information is incomplete and lack of discriminability . However, most generative methods use user-defined semantic information directly as constraints of the generative model , which makes the visual features synthesized by the models lack of diversity and separability . In this paper, we propose a novel method to improve the semantic feature by utilizing discriminative visual features. Furthermore, a novel Augmented Semantic Feature Based Generative Network (ASFGN) is built to synthesize the separable visual representations for unseen classes. Since GAN-based generative model may suffer from mode collapse, we propose a novel collapse-alleviate loss to improve the training stability and generalization performance of our generative network. Extensive experiments on four benchmark datasets prove that our method outperforms the state-of-art approaches in both ZSL and GZSL settings.},
  archive      = {J_NN},
  author       = {Zhiqun Li and Qiong Chen and Qingfa Liu},
  doi          = {10.1016/j.neunet.2021.04.014},
  journal      = {Neural Networks},
  pages        = {1-11},
  shortjournal = {Neural Netw.},
  title        = {Augmented semantic feature based generative network for generalized zero-shot learning},
  volume       = {143},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021c). INN/ENNS/JNNS - membership applic. form. <em>NN</em>,
<em>142</em>, II. (<a
href="https://doi.org/10.1016/S0893-6080(21)00330-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NN},
  doi          = {10.1016/S0893-6080(21)00330-0},
  journal      = {Neural Networks},
  pages        = {II},
  shortjournal = {Neural Netw.},
  title        = {INN/ENNS/JNNS - membership applic. form},
  volume       = {142},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021c). Current events. <em>NN</em>, <em>142</em>, I. (<a
href="https://doi.org/10.1016/S0893-6080(21)00329-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NN},
  doi          = {10.1016/S0893-6080(21)00329-4},
  journal      = {Neural Networks},
  pages        = {I},
  shortjournal = {Neural Netw.},
  title        = {Current events},
  volume       = {142},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Exact coexistence and locally asymptotic stability of
multiple equilibria for fractional-order delayed hopfield neural
networks with gaussian activation function. <em>NN</em>, <em>142</em>,
690–700. (<a
href="https://doi.org/10.1016/j.neunet.2021.07.029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper explores the multistability issue for fractional-order Hopfield neural networks with Gaussian activation function and multiple time delays . First, several sufficient criteria are presented for ensuring the exact coexistence of 3 n 3n equilibria, based on the geometric characteristics of Gaussian function , the fixed point theorem and the contraction mapping principle. Then, different from the existing methods used in the multistability analysis of fractional-order neural networks without time delays , it is shown that 2 n 2n of 3 n 3n total equilibria are locally asymptotically stable, by applying the theory of fractional-order linear delayed system and constructing suitable Lyapunov function . The obtained results improve and extend some existing multistability works for classical integer-order neural networks and fractional-order neural networks without time delays. Finally, an illustrative example with comprehensive computer simulations is given to demonstrate the theoretical results.},
  archive      = {J_NN},
  author       = {Xiaobing Nie and Pingping Liu and Jinling Liang and Jinde Cao},
  doi          = {10.1016/j.neunet.2021.07.029},
  journal      = {Neural Networks},
  pages        = {690-700},
  shortjournal = {Neural Netw.},
  title        = {Exact coexistence and locally asymptotic stability of multiple equilibria for fractional-order delayed hopfield neural networks with gaussian activation function},
  volume       = {142},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Subspace-based predictive control of parkinson’s disease: A
model-based study. <em>NN</em>, <em>142</em>, 680–689. (<a
href="https://doi.org/10.1016/j.neunet.2021.07.025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep brain stimulation (DBS) of the Basal Ganglia (BG) is an effective treatment to suppress the symptoms of Parkinson’s disease (PD). Using a closed-loop scheme in DBS can not only improve its therapeutic effects but it can also reduce its energy consumption and possible side effects. In this paper, a predictive closed loop control strategy is employed to suppress the PD in real-time. A linear multi-input multi-output (MIMO) state-delayed system is considered as a simplified model of the BG neuronal network relating the stimulation signals as inputs to the beta power of local field potentials as PD biomarkers. The effect of time delay in different areas of the BG is incorporated into this model and a real-time subspace-based identification is implemented to continuously model the state of the BG neuronal network and drive the predictive control strategy. Simulation results show that the proposed MIMO subspace based predictive controller can suppress PD symptoms more effectively and with less power consumption compared to the conventional open-loop DBS and a recently proposed single-input single-output closed loop controller .},
  archive      = {J_NN},
  author       = {Mahboubeh Ahmadipour and Mojtaba Barkhordari-Yazdi and Saeid R. Seydnejad},
  doi          = {10.1016/j.neunet.2021.07.025},
  journal      = {Neural Networks},
  pages        = {680-689},
  shortjournal = {Neural Netw.},
  title        = {Subspace-based predictive control of parkinson’s disease: A model-based study},
  volume       = {142},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Unsupervised learning of disentangled representations in
deep restricted kernel machines with orthogonality constraints.
<em>NN</em>, <em>142</em>, 661–679. (<a
href="https://doi.org/10.1016/j.neunet.2021.07.023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce Constr-DRKM, a deep kernel method for the unsupervised learning of disentangled data representations. We propose augmenting the original deep restricted kernel machine formulation for kernel PCA by orthogonality constraints on the latent variables to promote disentanglement and to make it possible to carry out optimization without first defining a stabilized objective. After discussing a number of algorithms for end-to-end training, we quantitatively evaluate the proposed method’s effectiveness in disentangled feature learning . We demonstrate on four benchmark datasets that this approach performs similarly overall to β β -VAE on several disentanglement metrics when few training points are available while being less sensitive to randomness and hyperparameter selection than β β -VAE. We also present a deterministic initialization of Constr-DRKM’s training algorithm that significantly improves the reproducibility of the results. Finally, we empirically evaluate and discuss the role of the number of layers in the proposed methodology, examining the influence of each principal component in every layer and showing that components in lower layers act as local feature detectors capturing the broad trends of the data distribution, while components in deeper layers use the representation learned by previous layers and more accurately reproduce higher-level features.},
  archive      = {J_NN},
  author       = {Francesco Tonin and Panagiotis Patrinos and Johan A.K. Suykens},
  doi          = {10.1016/j.neunet.2021.07.023},
  journal      = {Neural Networks},
  pages        = {661-679},
  shortjournal = {Neural Netw.},
  title        = {Unsupervised learning of disentangled representations in deep restricted kernel machines with orthogonality constraints},
  volume       = {142},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Spatial information transfer in hippocampal place cells
depends on trial-to-trial variability, symmetry of place-field firing,
and biophysical heterogeneities. <em>NN</em>, <em>142</em>, 636–660. (<a
href="https://doi.org/10.1016/j.neunet.2021.07.026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The relationship between the feature-tuning curve and information transfer profile of individual neurons provides vital insights about neural encoding. However, the relationship between the spatial tuning curve and spatial information transfer of hippocampal place cells remains unexplored. Here, employing a stochastic search procedure spanning thousands of models, we arrived at 127 conductance-based place-cell models that exhibited signature electrophysiological characteristics and sharp spatial tuning, with parametric values that exhibited neither clustering nor strong pairwise correlations. We introduced trial-to-trial variability in responses and computed model tuning curves and information transfer profiles, using stimulus-specific (SSI) and mutual (MI) information metrics, across locations within the place field. We found spatial information transfer to be heterogeneous across models, but to reduce consistently with increasing levels of variability. Importantly, whereas reliable low-variability responses implied that maximal information transfer occurred at high-slope regions of the tuning curve, increase in variability resulted in maximal transfer occurring at the peak-firing location in a subset of models. Moreover, experience-dependent asymmetry in place-field firing introduced asymmetries in the information transfer computed through MI, but not SSI, and the impact of activity-dependent variability on information transfer was minimal compared to activity-independent variability. We unveiled ion-channel degeneracy in the regulation of spatial information transfer, and demonstrated critical roles for N -methyl- d -aspartate receptors, transient potassium and dendritic sodium channels in regulating information transfer. Our results demonstrate that trial-to-trial variability, tuning-curve shape and biological heterogeneities critically regulate the relationship between the spatial tuning curve and spatial information transfer in hippocampal place cells.},
  archive      = {J_NN},
  author       = {Ankit Roy and Rishikesh Narayanan},
  doi          = {10.1016/j.neunet.2021.07.026},
  journal      = {Neural Networks},
  pages        = {636-660},
  shortjournal = {Neural Netw.},
  title        = {Spatial information transfer in hippocampal place cells depends on trial-to-trial variability, symmetry of place-field firing, and biophysical heterogeneities},
  volume       = {142},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep ReLU neural networks in high-dimensional approximation.
<em>NN</em>, <em>142</em>, 619–635. (<a
href="https://doi.org/10.1016/j.neunet.2021.07.027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the computation complexity of deep ReLU (Rectified Linear Unit) neural networks for the approximation of functions from the Hölder–Zygmund space of mixed smoothness defined on the d -dimensional unit cube when the dimension d may be very large. The approximation error is measured in the norm of isotropic Sobolev space . For every function f from the Hölder–Zygmund space of mixed smoothness, we explicitly construct a deep ReLU neural network having an output that approximates f with a prescribed accuracy ɛ ɛ , and prove tight dimension-dependent upper and lower bounds of the computation complexity of the approximation, characterized as the size and depth of this deep ReLU neural network, explicitly in d and ɛ ɛ . The proof of these results in particular, relies on the approximation by sparse-grid sampling recovery based on the Faber series.},
  archive      = {J_NN},
  author       = {Dinh Dũng and Van Kien Nguyen},
  doi          = {10.1016/j.neunet.2021.07.027},
  journal      = {Neural Networks},
  pages        = {619-635},
  shortjournal = {Neural Netw.},
  title        = {Deep ReLU neural networks in high-dimensional approximation},
  volume       = {142},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bio-instantiated recurrent neural networks: Integrating
neurobiology-based network topology in artificial networks. <em>NN</em>,
<em>142</em>, 608–618. (<a
href="https://doi.org/10.1016/j.neunet.2021.07.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Biological neuronal networks (BNNs) are a source of inspiration and analogy making for researchers that focus on artificial neuronal networks (ANNs). Moreover, neuroscientists increasingly use ANNs as a model for the brain. Despite certain similarities between these two types of networks, important differences can be discerned. First, biological neural networks are sculpted by evolution and the constraints that it entails, whereas artificial neural networks are engineered to solve particular tasks. Second, the network topology of these systems, apart from some analogies that can be drawn, exhibits pronounced differences. Here, we examine strategies to construct recurrent neural networks (RNNs) that instantiate the network topology of brains of different species. We refer to such RNNs as bio-instantiated. We investigate the performance of bio-instantiated RNNs in terms of: (i) the prediction performance itself, that is, the capacity of the network to minimize the cost function at hand in test data, and (ii) speed of training, that is, how fast during training the network reaches its optimal performance. We examine bio-instantiated RNNs in working memory tasks where task-relevant information must be tracked as a sequence of events unfolds in time. We highlight the strategies that can be used to construct RNNs with the network topology found in BNNs, without sacrificing performance. Despite that we observe no enhancement of performance when compared to randomly wired RNNs, our approach demonstrates how empirical neural network data can be used for constructing RNNs, thus, facilitating further experimentation with biologically realistic network topologies, in contexts where such aspect is desired.},
  archive      = {J_NN},
  author       = {Alexandros Goulas and Fabrizio Damicelli and Claus C. Hilgetag},
  doi          = {10.1016/j.neunet.2021.07.011},
  journal      = {Neural Networks},
  pages        = {608-618},
  shortjournal = {Neural Netw.},
  title        = {Bio-instantiated recurrent neural networks: Integrating neurobiology-based network topology in artificial networks},
  volume       = {142},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Non-native acoustic modeling for mispronunciation
verification based on language adversarial representation learning.
<em>NN</em>, <em>142</em>, 597–607. (<a
href="https://doi.org/10.1016/j.neunet.2021.07.017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Non-native mispronunciation verification is designed to provide feedback to guide language learners to correct their pronunciation errors in their further learning and it plays an important role in the computer-aided pronunciation training (CAPT) system. Most existing approaches focus on establishing the acoustic model directly using non-native corpus thus they are suffering the data sparsity problem due to time-consuming non-native speech data collection and annotation tasks. In this work, to address this problem, we propose a pre-trained approach to utilize the speech data of two native languages (the learner’s native and target languages) for non-native mispronunciation verification. We set up an unsupervised model to extract knowledge from a large scale of unlabeled raw speech of the target language by making predictions about future observations in the speech signal, then the model is trained with language adversarial training using the learner’s native language to align the feature distribution of two languages by confusing a language discriminator . In addition, sinc filter is incorporated at the first convolutional layer to capture the formant-like feature. Formant is relevant to the place and manner of articulation. Therefore, it is useful not only for pronunciation error detection but also for providing instructive feedback. Then the pre-trained model serves as the feature extractor in the downstream mispronunciation verification task. Through the experiments on the Japanese part of the BLCU inter-Chinese speech corpus , the experimental results demonstrate that for the non-native phone recognition and mispronunciation verification tasks (1) the knowledge learned from two native languages speech with the proposed unsupervised approach is useful for these two tasks (2) our proposed language adversarial representation learning is effective to improve the performance (3) formant-like feature can be incorporated by introducing sinc filter to further improve the performance of mispronunciation verification.},
  archive      = {J_NN},
  author       = {Longfei Yang and Kaiqi Fu and Jinsong Zhang and Takahiro Shinozaki},
  doi          = {10.1016/j.neunet.2021.07.017},
  journal      = {Neural Networks},
  pages        = {597-607},
  shortjournal = {Neural Netw.},
  title        = {Non-native acoustic modeling for mispronunciation verification based on language adversarial representation learning},
  volume       = {142},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Task-relevant and task-irrelevant variability causally shape
error-based motor learning. <em>NN</em>, <em>142</em>, 583–596. (<a
href="https://doi.org/10.1016/j.neunet.2021.07.015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent studies of motor learning show dissociable roles of reward- and sensory-prediction errors in updating motor commands by using typical adaptation paradigms where force or visual perturbations are imposed on hand movements. Such classic adaptation paradigms ignore a problem of redundancy inherently embedded in the motor pathways where the central nervous system has to find a unique solution in the high-dimensional motor command space. Computationally, a possible way of solving such a redundancy problem is exploring and updating motor commands based on the learned knowledge of the structures of both the motor pathways and the tasks. However, the effects of task-irrelevant motor command exploration in structure learning and its effects on reward-based and error-based learning have yet to be examined. Here, we used a redundant motor task where participants manipulated a cursor on a monitor screen with their hand gesture movements and then analyzed single-trial motor learning by fitting models consisting of reward-based and error-based learning contributions. We found that the error-based learning rate positively correlated with both task-relevant and task-irrelevant variability, likely reflecting the effect of motor exploration in structure learning. Further modeling results show that the effects of both task-relevant and task-irrelevant variability are simultaneous, and not mediated by one another. In contrast, the reward-based learning rate correlated with neither task-relevant nor task-irrelevant variability. Thus, although not having a direct influence on the task outcome, exploration in the task-irrelevant space late in training has a significant effect on the learning of a task structure used for error-based learning. This suggests that motor exploration, in both task-relevant and task-irrelevant spaces, has an essential role in error-based motor learning in a redundant motor mechanism.},
  archive      = {J_NN},
  author       = {Lucas Rebelo Dal’Bello and Jun Izawa},
  doi          = {10.1016/j.neunet.2021.07.015},
  journal      = {Neural Networks},
  pages        = {583-596},
  shortjournal = {Neural Netw.},
  title        = {Task-relevant and task-irrelevant variability causally shape error-based motor learning},
  volume       = {142},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An interaction-modeling mechanism for context-dependent
text-to-SQL translation based on heterogeneous graph aggregation.
<em>NN</em>, <em>142</em>, 573–582. (<a
href="https://doi.org/10.1016/j.neunet.2021.07.014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For the context-dependent Text-to-SQL task, the generation of SQL query is placed in a multi-turn interaction scenario. Each turn of Text-to-SQL must take historical interactive information and database schema into account. Accordingly, how to encode and integrate these different types of texts (the question sentence, the corresponding SQL query, and database schema) is a tough problem. In previous work, these series of texts are usually concatenated into sequences and encoded by various variants of recurrent neural networks (RNN). However, the RNNs cannot model the intrinsic relationship of the text directly. To this end, we propose an interaction-modeling mechanism to represent and aggregate these texts. Firstly, different types of texts are represented as individual graphs. Then, heterogeneous graph aggregation is used to capture the interactions and aggregate graphs into a holistic representation. Finally, the corresponding SQL query is generated based on the current question and the aggregated information. We evaluate our model on the SparC and CoSQL dataset to demonstrate the benefits of interaction-modeling. Experimentally, our model has a competitive performance and space–time cost.},
  archive      = {J_NN},
  author       = {Wei Yu and Tao Chang and Xiaoting Guo and Mengzhu Wang and Xiaodong Wang},
  doi          = {10.1016/j.neunet.2021.07.014},
  journal      = {Neural Networks},
  pages        = {573-582},
  shortjournal = {Neural Netw.},
  title        = {An interaction-modeling mechanism for context-dependent text-to-SQL translation based on heterogeneous graph aggregation},
  volume       = {142},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A CNN model embedded with local feature knowledge and its
application to time-varying signal classification. <em>NN</em>,
<em>142</em>, 564–572. (<a
href="https://doi.org/10.1016/j.neunet.2021.07.018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel convolutional neural network is proposed for local prior feature embedding and imbalanced dataset modeling for multi-channel time-varying signal classification. This model consists of a single-channel signal feature parallel extraction unit, a multi-channel signal feature integration unit, a local feature embedding and feature similarity measurement unit , a full connection layer, and a Softmax classifier. An algorithm combining dynamic clustering and sliding window was used to select segments signals with typical local features in each pattern class, forming a typical local feature set. The one-dimensional CNNs were used to extract features from the single-channel signal in parallel, a comprehensive feature matrix of the multi-channel signal and the local feature matrix templates were produced. Using the method of external embedding, based on the sliding window and dynamic time warping (DTW) algorithm, the local feature similarities between the local feature template of each pattern class and the comprehensive feature sub-matrix of the input signal were measured, and the maximum values were selected to construct a local feature similarity vector in order. The information fusion was realized through a full connection layer. The proposed methodology can extract and represent both global and local signals features, strengthen the role of prior local feature in classification and improve the modeling properties of imbalanced datasets. A comprehensive learning algorithm is presented in this paper. The classification diagnosis of cardiovascular disease based on 12-lead ECG signals was used as a verification experiment. Results showed that the accuracy and generalization for the proposed technique were significantly improved.},
  archive      = {J_NN},
  author       = {Ruiping Yang and Xianyu Zha and Kun Liu and Shaohua Xu},
  doi          = {10.1016/j.neunet.2021.07.018},
  journal      = {Neural Networks},
  pages        = {564-572},
  shortjournal = {Neural Netw.},
  title        = {A CNN model embedded with local feature knowledge and its application to time-varying signal classification},
  volume       = {142},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A neuromimetic realization of hippocampal CA1 for theta wave
generation. <em>NN</em>, <em>142</em>, 548–563. (<a
href="https://doi.org/10.1016/j.neunet.2021.07.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in neural engineering allowed the development of neuroprostheses which facilitate functionality in people with neurological problems. In this research, a real-time neuromorphic system is proposed to artificially reproduce the theta wave and firing patterns of different neuronal populations in the CA1, a sub-region of the hippocampus . The hippocampal theta oscillations (4–12 Hz) are an important electrophysiological rhythm that contributes in various cognitive functions, including navigation, memory, and novelty detection. The proposed CA1 neuromimetic circuit includes 100 linearized Pinsky–Rinzel neurons and 668 excitatory and inhibitory synapses on a field programmable gate array (FPGA). The implemented spiking neural network of the CA1 includes the main neuronal populations for the theta rhythm generation: excitatory pyramidal cells , PV+ basket cells, and Oriens Lacunosum-Moleculare (OLM) cells which are inhibitory interneurons . Moreover, the main inputs to the CA1 region from the entorhinal cortex via the perforant pathway , the CA3 via Schaffer collaterals , and the medial septum via fimbria–fornix are also implemented on the FPGA using a bursting leaky-integrate and fire (LIF) neuron model. The results of hardware realization show that the proposed CA1 neuromimetic circuit successfully reconstructs the theta oscillations and functionally illustrates the phase relations between firing responses of the different neuronal populations. It is also evaluated the impact of medial septum elimination on the firing patterns of the CA1 neuronal population and the theta wave’s characteristics. This neuromorphic system can be considered as a potential platform that opens opportunities for neuroprosthetic applications in future works.},
  archive      = {J_NN},
  author       = {Nima Salimi-Nezhad and Mohammad Hasanlou and Mahmood Amiri and Georgios A. Keliris},
  doi          = {10.1016/j.neunet.2021.07.002},
  journal      = {Neural Networks},
  pages        = {548-563},
  shortjournal = {Neural Netw.},
  title        = {A neuromimetic realization of hippocampal CA1 for theta wave generation},
  volume       = {142},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Optimal attention tuning in a neuro-computational model of
the visual cortex–basal ganglia–prefrontal cortex loop. <em>NN</em>,
<em>142</em>, 534–547. (<a
href="https://doi.org/10.1016/j.neunet.2021.07.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual attention is widely considered a vital factor in the perception and analysis of a visual scene. Several studies explored the effects and mechanisms of top-down attention, but the mechanisms that determine the attentional signal are less explored. By developing a neuro-computational model of visual attention including the visual cortex–basal ganglia loop, we demonstrate how attentional alignment can evolve based on dopaminergic reward during a visual search task . Unlike most previous modeling studies of feature-based attention, we do not implement a manually predefined attention template. Dopamine-modulated covariance learning enable the basal ganglia to learn rewarded associations between the visual input and the attentional gain represented in the PFC of the model. Hence, the model shows human-like performance on a visual search task by optimally tuning the attention signal. In particular, similar as in humans, this reward-based tuning in the model leads to an attentional template that is not centered on the target feature, but a relevant feature deviating away from the target due to the presence of highly similar distractors . Further analyses of the model shows, attention is mainly guided by the signal-to-noise ratio between target and distractors .},
  archive      = {J_NN},
  author       = {Oliver Maith and Alex Schwarz and Fred H. Hamker},
  doi          = {10.1016/j.neunet.2021.07.008},
  journal      = {Neural Networks},
  pages        = {534-547},
  shortjournal = {Neural Netw.},
  title        = {Optimal attention tuning in a neuro-computational model of the visual cortex–basal ganglia–prefrontal cortex loop},
  volume       = {142},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep attributed graph clustering with self-separation
regularization and parameter-free cluster estimation. <em>NN</em>,
<em>142</em>, 522–533. (<a
href="https://doi.org/10.1016/j.neunet.2021.07.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting clusters over attributed graphs is a fundamental task in the graph analysis field. The goal is to partition nodes into dense clusters based on both their attributes and structures. Modern graph neural networks provide facilitation to jointly capture the above information in attributed graphs with a feature aggregation manner, and have achieved great success in attributed graph clustering . However, existing methods mainly focus on capturing the proximity information in graphs and often fail to learn cluster-friendly features during the training of models. Besides, similar to many deep clustering frameworks, current methods based on graph neural networks require a preassigned cluster number before estimating the clusters. To address these limitations, we propose in this paper a deep attributed clustering method based on self-separated graph neural networks and parameter-free cluster estimation. First, to learn cluster-friendly features, we jointly optimize a jumping graph convolutional auto-encoder with a self-separation regularizer, which learns clusters with changing sizes while keeping dense intra-cluster structures and sparse inter structures. Second, an additional softmax auto-encoder is trained to determine the natural cluster number from the data. The hidden units capture cluster structures and can be used to estimate the number of clusters. Extensive experiments show the effectiveness of the proposed model.},
  archive      = {J_NN},
  author       = {Junzhong Ji and Ye Liang and Minglong Lei},
  doi          = {10.1016/j.neunet.2021.07.012},
  journal      = {Neural Networks},
  pages        = {522-533},
  shortjournal = {Neural Netw.},
  title        = {Deep attributed graph clustering with self-separation regularization and parameter-free cluster estimation},
  volume       = {142},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Instance elimination strategy for non-convex
multiple-instance learning using sparse positive bags. <em>NN</em>,
<em>142</em>, 509–521. (<a
href="https://doi.org/10.1016/j.neunet.2021.07.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In some multiple instance learning (MIL) applications, positive bags are sparse (i.e. containing only a small fraction of positive instances). To deal with the imbalanced data caused by these situations, we present a novel MIL method based on a small sphere and large margin approach (SSLM-MIL). Due to the introduction of a large margin, SSLM-MIL enforces the desired constraint that for all positive bags, there is at least one positive instance in each bag. Moreover, our framework is flexible to incorporate the non-convex optimization problem . Therefore, we can solve it using the concave–convex procedure (CCCP). Still, CCCP may be computationally inefficient for the number of external iterations. Inspired by the existing safe screening rules, which can effectively reduce computational time by discarding some inactive instances. In this paper, we propose a strategy to reduce the scale of the optimization problem. Specifically, we construct a screening rule in the inner solver and another rule for propagating screened instances between iterations of CCCP. To the best of our knowledge, this is the first attempt to introduce safe instance screening to a non-convex hypersphere support vector machine . Experiments on thirty-one benchmark datasets demonstrate the safety and effectiveness of our approach.},
  archive      = {J_NN},
  author       = {Min Yuan and Yitian Xu and Renxiu Feng and Zongmin Liu},
  doi          = {10.1016/j.neunet.2021.07.009},
  journal      = {Neural Networks},
  pages        = {509-521},
  shortjournal = {Neural Netw.},
  title        = {Instance elimination strategy for non-convex multiple-instance learning using sparse positive bags},
  volume       = {142},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Novel methods to global mittag-leffler stability of delayed
fractional-order quaternion-valued neural networks. <em>NN</em>,
<em>142</em>, 500–508. (<a
href="https://doi.org/10.1016/j.neunet.2021.07.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a type of fractional-order quaternion-valued neural networks (FOQVNNs) with leakage and time-varying delays is established to simulate real-world situations, and the global Mittag-Leffler stability of the system is investigated by using the non-decomposition method. First, to avoid decomposing the system into two complex-valued systems or four real-valued systems, a new sign function for quaternion numbers is introduced based on the ones for real and complex numbers. And two novel lemmas for quaternion-valued sign function and Caputo fractional derivative are established in quaternion domain, which are used to investigate the stability of FOQVNNs. Second, a concise and flexible quaternion-valued state feedback controller is directly designed and a novel 1-norm Lyapunov function composed of the absolute values of real and imaginary parts is established. Then, based on the designed quaternion-valued state feedback controller and the proposed lemmas, some sufficient conditions are given to ensure the global Mittag-Leffler stability of the system. Finally, a numerical simulation is given to verify the theoretical results.},
  archive      = {J_NN},
  author       = {Hongyun Yan and Yuanhua Qiao and Lijuan Duan and Jun Miao},
  doi          = {10.1016/j.neunet.2021.07.005},
  journal      = {Neural Networks},
  pages        = {500-508},
  shortjournal = {Neural Netw.},
  title        = {Novel methods to global mittag-leffler stability of delayed fractional-order quaternion-valued neural networks},
  volume       = {142},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Predefined-time synchronization of competitive neural
networks. <em>NN</em>, <em>142</em>, 492–499. (<a
href="https://doi.org/10.1016/j.neunet.2021.06.026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, the predefined-time synchronization of competitive neural networks (CNNs) is researched based on two different predefined-time stability theorems. In view of the bilayer structure of CNNs, we design two bilayer predefined-time controllers. The first controller utilizes sign function, while the second controller utilizes exponential function and Lyapunov function . In these two controllers, the predefined time is set as a controller parameter , and it can be an arbitrary positive constant. Under these two controllers, the considered CNNs can achieve synchronization within the predefined time regardless of the initial values. A specific example is presented to validate the theoretical results.},
  archive      = {J_NN},
  author       = {Chuan Chen and Ling Mi and Zhongqiang Liu and Baolin Qiu and Hui Zhao and Lijuan Xu},
  doi          = {10.1016/j.neunet.2021.06.026},
  journal      = {Neural Networks},
  pages        = {492-499},
  shortjournal = {Neural Netw.},
  title        = {Predefined-time synchronization of competitive neural networks},
  volume       = {142},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An efficient encoder–decoder model for portrait depth
estimation from single images trained on pixel-accurate synthetic data.
<em>NN</em>, <em>142</em>, 479–491. (<a
href="https://doi.org/10.1016/j.neunet.2021.07.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Depth estimation from a single image frame is a fundamental challenge in computer vision , with many applications such as augmented reality , action recognition, image understanding, and autonomous driving . Large and diverse training sets are required for accurate depth estimation from a single image frame. Due to challenges in obtaining dense ground-truth depth, a new 3D pipeline of 100 synthetic virtual human models is presented to generate multiple 2D facial images and corresponding ground truth depth data, allowing complete control over image variations. To validate the synthetic facial depth data, we propose an evaluation of state-of-the-art depth estimation algorithms based on single image frames on the generated synthetic dataset . Furthermore, an improved encoder–decoder based neural network is presented. This network is computationally efficient and shows better performance than current state-of-the-art when tested and evaluated across 4 public datasets. Our training methodology relies on the use of synthetic data samples which provides a more reliable ground truth for depth estimation. Additionally, using a combination of appropriate loss functions leads to improved performance than the current state-of-the-art network performances. Our approach clearly outperforms competing methods across different test datasets , setting a new state-of-the-art for facial depth estimation from synthetic data.},
  archive      = {J_NN},
  author       = {Faisal Khan and Shahid Hussain and Shubhajit Basak and Joseph Lemley and Peter Corcoran},
  doi          = {10.1016/j.neunet.2021.07.007},
  journal      = {Neural Networks},
  pages        = {479-491},
  shortjournal = {Neural Netw.},
  title        = {An efficient encoder–decoder model for portrait depth estimation from single images trained on pixel-accurate synthetic data},
  volume       = {142},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Capped l2,p-norm metric based robust least squares twin
support vector machine for pattern classification. <em>NN</em>,
<em>142</em>, 457–478. (<a
href="https://doi.org/10.1016/j.neunet.2021.06.028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Least squares twin support vector machine (LSTSVM) is an effective and efficient learning algorithm for pattern classification. However, the distance in LSTSVM is measured by squared L2 -norm metric that may magnify the influence of outliers. In this paper, a novel robust least squares twin support vector machine framework is proposed for binary classification , termed as C L2,p -LSTSVM, which utilizes capped L2,p -norm distance metric to reduce the influence of noise and outliers. The goal of C L2,p -LSTSVM is to minimize the capped L2,p -norm intra-class distance dispersion, and eliminate the influence of outliers during training process, where the value of the metric is controlled by the capped parameter, which can ensure better robustness. The proposed metric includes and extends the traditional metrics by setting appropriate values of p and capped parameter. This strategy not only retains the advantages of LSTSVM, but also improves the robustness in solving a binary classification problem with outliers. However, the nonconvexity of metric makes it difficult to optimize. We design an effective iterative algorithm to solve the C L2,p -LSTSVM. In each iteration, two systems of linear equations are solved. Simultaneously, we present some insightful analyses on the computational complexity and convergence of algorithm. Moreover, we extend the C L2,p -LSTSVM to nonlinear classifier and semi-supervised classification. Experiments are conducted on artificial datasets, UCI benchmark datasets, and image datasets to evaluate our method. Under different noise settings and different evaluation criteria, the experiment results show that the C L2,p -LSTSVM has better robustness than state-of-the-art approaches in most cases, which demonstrates the feasibility and effectiveness of the proposed method.},
  archive      = {J_NN},
  author       = {Chao Yuan and Liming Yang},
  doi          = {10.1016/j.neunet.2021.06.028},
  journal      = {Neural Networks},
  pages        = {457-478},
  shortjournal = {Neural Netw.},
  title        = {Capped l2,p-norm metric based robust least squares twin support vector machine for pattern classification},
  volume       = {142},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Selective ensemble-based online adaptive deep neural
networks for streaming data with concept drift. <em>NN</em>,
<em>142</em>, 437–456. (<a
href="https://doi.org/10.1016/j.neunet.2021.06.027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Concept drift is an important issue in the field of streaming data mining. However, how to maintain real-time model convergence in a dynamic environment is an important and difficult problem. In addition, the current methods have limited ability to deal with the problem of streaming data classification for complex nonlinear problems . To solve these problems, a selective ensemble-based online adaptive deep neural network (SEOA) is proposed to address concept drift. First, the adaptive depth unit is constructed by combining shallow features with deep features and adaptively controls the information flow in the neural network according to changes in streaming data at adjacent moments, which improves the convergence of the online deep learning model. Then, the adaptive depth units of different layers are regarded as base classifiers for ensemble and weighted dynamically according to the loss of each classifier. In addition, a dynamic selection of base classifiers is adopted according to the fluctuation of the streaming data to achieve a balance between stability and adaptability. The experimental results show that the SEOA can effectively contend with different types of concept drift and has good robustness and generalization.},
  archive      = {J_NN},
  author       = {Husheng Guo and Shuai Zhang and Wenjian Wang},
  doi          = {10.1016/j.neunet.2021.06.027},
  journal      = {Neural Networks},
  pages        = {437-456},
  shortjournal = {Neural Netw.},
  title        = {Selective ensemble-based online adaptive deep neural networks for streaming data with concept drift},
  volume       = {142},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning to reweight examples in multi-label classification.
<em>NN</em>, <em>142</em>, 428–436. (<a
href="https://doi.org/10.1016/j.neunet.2021.03.022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a new method for reweighting examples in the multi-label classification problem. Existing weighting functions in self-paced learning simply determine the weights of examples according to their loss values given the current multi-label model, but neglect the unique properties of multi-label examples. It is inaccurate to treat two distinct examples as equal even if their loss values are the same. Therefore, we upgrade the classical weight functions by considering instance complexities, which are described by the distances between instance features and their corresponding labels. The distance metric can be easily optimized during training. Experimental results on real-world datasets demonstrate the significance of investigating both the dynamic and static complexities of multi-label examples, as well as the advantages of the proposed example reweighting algorithm in multi-label classification problems.},
  archive      = {J_NN},
  author       = {Yongjian Zhong and Bo Du and Chang Xu},
  doi          = {10.1016/j.neunet.2021.03.022},
  journal      = {Neural Networks},
  pages        = {428-436},
  shortjournal = {Neural Netw.},
  title        = {Learning to reweight examples in multi-label classification},
  volume       = {142},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Distributed-force-feedback-based reflex with online learning
for adaptive quadruped motor control. <em>NN</em>, <em>142</em>,
410–427. (<a
href="https://doi.org/10.1016/j.neunet.2021.06.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Biological motor control mechanisms (e.g., central pattern generators (CPGs), sensory feedback, reflexes, and motor learning) play a crucial role in the adaptive locomotion of animals. However, the interaction and integration of these mechanisms – necessary for generating the efficient, adaptive locomotion responses of legged robots to diverse terrains – have not yet been fully realized. One issue is that of achieving adaptive motor control for fast postural adaptation across various terrains. To address this issue, this study proposes a novel distributed-force-feedback-based reflex with online learning (DFRL). It integrates force-sensory feedback, reflexes, and learning to cooperate with CPGs in producing adaptive motor commands. The DFRL is based on a simple neural network that uses plastic synapses modulated online by a fast dual integral learner. Experimental results on different quadruped robots show that the DFRL can (1) automatically and rapidly adapt the CPG patterns (motor commands) of the robots, enabling them to realize appropriate body postures during locomotion and (2) enable the robots to effectively accommodate themselves to various slope terrains, including steep ones. Consequently, the DFRL-controlled robots can achieve efficient adaptive locomotion, to tackle complex terrains with diverse slopes.},
  archive      = {J_NN},
  author       = {Tao Sun and Zhendong Dai and Poramate Manoonpong},
  doi          = {10.1016/j.neunet.2021.06.001},
  journal      = {Neural Networks},
  pages        = {410-427},
  shortjournal = {Neural Netw.},
  title        = {Distributed-force-feedback-based reflex with online learning for adaptive quadruped motor control},
  volume       = {142},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Unsupervised multi-sense language models for natural
language processing tasks. <em>NN</em>, <em>142</em>, 397–409. (<a
href="https://doi.org/10.1016/j.neunet.2021.05.023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing language models (LMs) represent each word with only a single representation, which is unsuitable for processing words with multiple meanings. This issue has often been compounded by the lack of availability of large-scale data annotated with word meanings. In this paper, we propose a sense-aware framework that can process multi-sense word information without relying on annotated data. In contrast to the existing multi-sense representation models, which handle information in a restricted context, our framework provides context representations encoded without ignoring word order information or long-term dependency. The proposed framework consists of a context representation stage to encode the variable-size context, a sense-labeling stage that involves unsupervised clustering to infer a probable sense for a word in each context, and a multi-sense LM (MSLM) learning stage to learn the multi-sense representations. Particularly for the evaluation of MSLMs with different vocabulary sizes, we propose a new metric, i.e., unigram-normalized perplexity ( PPLu ), which is also understood as the negated mutual information between a word and its context information. Additionally, there is a theoretical verification of PPLu on the change of vocabulary size. Also, we adopt a method of estimating the number of senses, which does not require further hyperparameter search for an LM performance. For the LMs in our framework, both unidirectional and bidirectional architectures based on long short-term memory (LSTM) and Transformers are adopted. We conduct comprehensive experiments on three language modeling datasets to perform quantitative and qualitative comparisons of various LMs. Our MSLM outperforms single-sense LMs (SSLMs) with the same network architecture and parameters. It also shows better performance on several downstream natural language processing tasks in the General Language Understanding Evaluation (GLUE) and SuperGLUE benchmarks.},
  archive      = {J_NN},
  author       = {Jihyeon Roh and Sungjin Park and Bo-Kyeong Kim and Sang-Hoon Oh and Soo-Young Lee},
  doi          = {10.1016/j.neunet.2021.05.023},
  journal      = {Neural Networks},
  pages        = {397-409},
  shortjournal = {Neural Netw.},
  title        = {Unsupervised multi-sense language models for natural language processing tasks},
  volume       = {142},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Spectral embedding network for attributed graph clustering.
<em>NN</em>, <em>142</em>, 388–396. (<a
href="https://doi.org/10.1016/j.neunet.2021.05.026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attributed graph clustering aims to discover node groups by utilizing both graph structure and node features. Recent studies mostly adopt graph neural networks to learn node embeddings , then apply traditional clustering methods to obtain clusters. However, they usually suffer from the following issues: (1) they adopt original graph structure which is unfavorable for clustering due to its noise and sparsity problems; (2) they mainly utilize non-clustering driven losses that cannot well capture the global cluster structure, thus the learned embeddings are not sufficient for the downstream clustering task . In this paper, we propose a spectral embedding network for attributed graph clustering (SENet), which improves graph structure by leveraging the information of shared neighbors, and learns node embeddings with the help of a spectral clustering loss. By combining the original graph structure and shared neighbor based similarity, both the first-order and second-order proximities are encoded into the improved graph structure, thus alleviating the noise and sparsity issues. To make the spectral loss well adapt to attributed graphs, we integrate both structure and feature information into kernel matrix via a higher-order graph convolution . Experiments on benchmark attributed graphs show that SENet achieves superior performance over state-of-the-art methods.},
  archive      = {J_NN},
  author       = {Xiaotong Zhang and Han Liu and Xiao-Ming Wu and Xianchao Zhang and Xinyue Liu},
  doi          = {10.1016/j.neunet.2021.05.026},
  journal      = {Neural Networks},
  pages        = {388-396},
  shortjournal = {Neural Netw.},
  title        = {Spectral embedding network for attributed graph clustering},
  volume       = {142},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Self-spectral learning with GAN based spectral–spatial
target detection for hyperspectral image. <em>NN</em>, <em>142</em>,
375–387. (<a
href="https://doi.org/10.1016/j.neunet.2021.05.029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To alleviate the shortcomings of target detection in only one aspect and reduce redundant information among adjacent bands, we propose a spectral–spatial target detection (SSTD) framework in deep latent space based on self-spectral learning (SSL) with a spectral generative adversarial network (GAN). The concept of SSL is introduced into hyperspectral feature extraction in an unsupervised fashion with the purpose of background suppression and target saliency. In particular, a novel structure-to-structure selection rule that takes full account of the structure, contrast, and luminance similarity is established to interpret the mapping relationship between the latent spectral feature space and the original spectral band space, to generate the optimal spectral band subset without any prior knowledge. Finally, the comprehensive result is achieved by nonlinearly combining the spatial detection on the fused latent features with the spectral detection on the selected band subset and the corresponding selected target signature. This paper paves a novel self-spectral learning way for hyperspectral target detection and identifies sensitive bands for specific targets in practice. Comparative analyses demonstrate that the proposed SSTD method presents superior detection performance compared with CSCR , ACE, CEM , hCEM, and ECEM.},
  archive      = {J_NN},
  author       = {Weiying Xie and Jiaqing Zhang and Jie Lei and Yunsong Li and Xiuping Jia},
  doi          = {10.1016/j.neunet.2021.05.029},
  journal      = {Neural Networks},
  pages        = {375-387},
  shortjournal = {Neural Netw.},
  title        = {Self-spectral learning with GAN based spectral–spatial target detection for hyperspectral image},
  volume       = {142},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Classification of visuomotor tasks based on
electroencephalographic data depends on age-related differences in brain
activity patterns. <em>NN</em>, <em>142</em>, 363–374. (<a
href="https://doi.org/10.1016/j.neunet.2021.04.029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classification of physiological data provides a data driven approach to study central aspects of motor control, which changes with age. To implement such results in real-life applications for elderly it is important to identify age-specific characteristics of movement classification. We compared task-classification based on EEG derived activity patterns related to brain network characteristics between older and younger adults performing force tracking with two task characteristics (sinusoidal; constant) with the right or left hand. We extracted brain network patterns with dynamic mode decomposition (DMD) and classified the tasks on an individual level using linear discriminant analysis (LDA). Next, we compared the models’ performance between the groups. Studying brain activity patterns, we identified signatures of altered motor network function reflecting dedifferentiated and compensational brain activation in older adults. We found that the classification performance of the body side was lower in older adults. However, classification performance with respect to task characteristics was better in older adults. This may indicate a higher susceptibility of brain network mechanisms to task difficulty in elderly. Signatures of dedifferentiation and compensation refer to an age-related reorganization of functional brain networks, which suggests that classification of visuomotor tracking tasks is influenced by age-specific characteristics of brain activity patterns. In addition to insights into central aspects of fine motor control, the results presented here are relevant in application-oriented areas such as brain computer interfaces .},
  archive      = {J_NN},
  author       = {C. Goelz and K. Mora and J. Rudisch and R. Gaidai and E. Reuter and B. Godde and C. Reinsberger and C. Voelcker-Rehage and S. Vieluf},
  doi          = {10.1016/j.neunet.2021.04.029},
  journal      = {Neural Networks},
  pages        = {363-374},
  shortjournal = {Neural Netw.},
  title        = {Classification of visuomotor tasks based on electroencephalographic data depends on age-related differences in brain activity patterns},
  volume       = {142},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Capturing the grouping and compactness of high-level
semantic feature for saliency detection. <em>NN</em>, <em>142</em>,
351–362. (<a
href="https://doi.org/10.1016/j.neunet.2021.04.028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Saliency detection is an important and challenging research topic due to the variety and complexity of the background and saliency regions. In this paper, we present a novel unsupervised saliency detection approach by exploiting the grouping and compactness characteristics of the high-level semantic features. First, for the high-level semantic feature, the elastic net based hypergraph model is adopted to discover the group structure relationships of salient regional points, and the calculation of the spatial distribution is constructed to detect the compactness of the saliency regions. Next, the grouping-based and compactness-based saliency maps are improved by a propagation algorithm . The propagation process uses an enhanced similarity matrix , which fuses the low-level deep feature and the high-level semantic feature through cross diffusion. Results on four benchmark datasets with pixel-wise accurate labeling demonstrate the effectiveness of the proposed method. Particularly, the proposed unsupervised method achieves competitive performance with deep learning-based methods.},
  archive      = {J_NN},
  author       = {Ying Ying Zhang and HongJuan Wang and XiaoDong Lv and Ping Zhang},
  doi          = {10.1016/j.neunet.2021.04.028},
  journal      = {Neural Networks},
  pages        = {351-362},
  shortjournal = {Neural Netw.},
  title        = {Capturing the grouping and compactness of high-level semantic feature for saliency detection},
  volume       = {142},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Candidate region aware nested named entity recognition.
<em>NN</em>, <em>142</em>, 340–350. (<a
href="https://doi.org/10.1016/j.neunet.2021.02.019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Named entity recognition (NER) is crucial in various natural language processing (NLP) tasks. However, the nested entities which are common in practical corpus are often ignored in most of current NER models. To extract the nested entities, two categories of models (i.e., feature-based and neural network-based approaches) are proposed. However, the feature-based models suffer from the complicated feature engineering and often heavily rely on the external resources. Discarding the heavy feature engineering, recent neural network-based methods which treat the nested NER as a classification task are designed but still suffer from the heavy class imbalance issue and the high computational cost. To solve these problems, we propose a neural multi-task model with two modules: Binary Sequence Labeling and Candidate Region Classification to extract the nested entities. Extensive experiments are conducted on the public datasets. Comparing with recent neural network-based approaches, our proposed model achieves the better performance and obtains the higher efficiency.},
  archive      = {J_NN},
  author       = {Deng Jiang and Haopeng Ren and Yi Cai and Jingyun Xu and Yanxia Liu and Ho-fung Leung},
  doi          = {10.1016/j.neunet.2021.02.019},
  journal      = {Neural Networks},
  pages        = {340-350},
  shortjournal = {Neural Netw.},
  title        = {Candidate region aware nested named entity recognition},
  volume       = {142},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). H-VECTORS: Improving the robustness in utterance-level
speaker embeddings using a hierarchical attention model. <em>NN</em>,
<em>142</em>, 329–339. (<a
href="https://doi.org/10.1016/j.neunet.2021.05.024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a hierarchical attention network is proposed to generate robust utterance-level embeddings (H-vectors) for speaker identification and verification. Since different parts of an utterance may have different contributions to speaker identities, the use of hierarchical structure aims to learn speaker related information locally and globally. In the proposed approach, frame-level encoder and attention are applied on segments of an input utterance and generate individual segment vectors. Then, segment level attention is applied on the segment vectors to construct an utterance representation. To evaluate the quality of the learned utterance-level speaker embeddings on speaker identification and verification, the proposed approach is tested on several benchmark datasets, such as the NIST SRE2008 Part1, the Switchboard Cellular (Part1), the CallHome American English Speech ,the Voxceleb1 and Voxceleb2 datasets. In comparison with some strong baselines, the obtained results show that the use of H-vectors can achieve better identification and verification performances in various acoustic conditions.},
  archive      = {J_NN},
  author       = {Yanpei Shi and Qiang Huang and Thomas Hain},
  doi          = {10.1016/j.neunet.2021.05.024},
  journal      = {Neural Networks},
  pages        = {329-339},
  shortjournal = {Neural Netw.},
  title        = {H-VECTORS: Improving the robustness in utterance-level speaker embeddings using a hierarchical attention model},
  volume       = {142},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptive ensemble perception tracking. <em>NN</em>,
<em>142</em>, 316–328. (<a
href="https://doi.org/10.1016/j.neunet.2021.05.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, tracking models based on bounding box regression (such as region proposal networks), built on the Siamese network , have attracted much attention. Despite their promising performance, these trackers are less effective in perceiving the target information in the following two aspects. First, existing regression models cannot take a global view of a large-scale target since the effective receptive field of a neuron is too small to cover the target with a large scale. Second, the neurons with a fixed receptive field (RF) size in these models cannot adapt to the scale and aspect ratio changes of the target. In this paper, we propose an adaptive ensemble perception tracking framework to address these issues. Specifically, we first construct a per-pixel prediction model, which predicts the target state at each pixel of the correlated feature. On top of the per-pixel prediction model, we then develop a confidence-guided ensemble prediction mechanism. The ensemble mechanism adaptively fuses the predictions of multiple pixels with the guidance of confidence maps, which enlarges the perception range and enhances the adaptive perception ability at the object-level. In addition, we introduce a receptive field adaption model to enhance the adaptive perception ability at the neuron-level, which adjusts the RF by adaptively integrating the features with different RFs. Extensive experimental results on the VOT2018, VOT2016, UAV123, LaSOT, and TC128 datasets demonstrate that the proposed algorithm performs favorably against the state-of-the-art methods in terms of accuracy and speed.},
  archive      = {J_NN},
  author       = {Zikun Zhou and Nana Fan and Kai Yang and Hongpeng Wang and Zhenyu He},
  doi          = {10.1016/j.neunet.2021.05.003},
  journal      = {Neural Networks},
  pages        = {316-328},
  shortjournal = {Neural Netw.},
  title        = {Adaptive ensemble perception tracking},
  volume       = {142},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Streaming cascade-based speech translation leveraged by a
direct segmentation model. <em>NN</em>, <em>142</em>, 303–315. (<a
href="https://doi.org/10.1016/j.neunet.2021.05.013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The cascade approach to Speech Translation (ST) is based on a pipeline that concatenates an Automatic Speech Recognition (ASR) system followed by a Machine Translation (MT) system. Nowadays, state-of-the-art ST systems are populated with deep neural networks that are conceived to work in an offline setup in which the audio input to be translated is fully available in advance. However, a streaming setup defines a completely different picture, in which an unbounded audio input gradually becomes available and at the same time the translation needs to be generated under real-time constraints. In this work, we present a state-of-the-art streaming ST system in which neural-based models integrated in the ASR and MT components are carefully adapted in terms of their training and decoding procedures in order to run under a streaming setup. In addition, a direct segmentation model that adapts the continuous ASR output to the capacity of simultaneous MT systems trained at the sentence level is introduced to guarantee low latency while preserving the translation quality of the complete ST system. The resulting ST system is thoroughly evaluated on the real-life streaming Europarl-ST benchmark to gauge the trade-off between quality and latency for each component individually as well as for the complete ST system.},
  archive      = {J_NN},
  author       = {Javier Iranzo-Sánchez and Javier Jorge and Pau Baquero-Arnal and Joan Albert Silvestre-Cerdà and Adrià Giménez and Jorge Civera and Albert Sanchis and Alfons Juan},
  doi          = {10.1016/j.neunet.2021.05.013},
  journal      = {Neural Networks},
  pages        = {303-315},
  shortjournal = {Neural Netw.},
  title        = {Streaming cascade-based speech translation leveraged by a direct segmentation model},
  volume       = {142},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Event-triggered adaptive neural networks control for
fractional-order nonstrict-feedback nonlinear systems with unmodeled
dynamics and input saturation. <em>NN</em>, <em>142</em>, 288–302. (<a
href="https://doi.org/10.1016/j.neunet.2021.05.014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The event-triggered adaptive neural networks control is investigated in this paper for a class of fractional-order systems (FOSs) with unmodeled dynamics and input saturation. Firstly, in order to obtain an auxiliary signal and then avoid the state variables of unmodeled dynamics directly appearing in the designed controller, the notion of exponential input-to-state practical stability (ISpS) and some related lemmas for integer-order systems are extended to the ones for FOSs. Then, based on the traditional event-triggered mechanism, we propose a novel adaptive event-triggered mechanism (AETM) in this paper, in which the threshold parameters can be adjusted dynamically according to the tracking performance. Besides, different from the previous works where the derivative of hyperbolic tangent function tanh ( ⋅ ) tanh(⋅) needs to have positive lower bound, a new type of auxiliary signal is introduced in this paper to handle the effect of input saturation and thus this limitation is released. Finally, two numerical examples and some comparisons are provided to illustrate our proposed controllers.},
  archive      = {J_NN},
  author       = {Boqiang Cao and Xiaobing Nie},
  doi          = {10.1016/j.neunet.2021.05.014},
  journal      = {Neural Networks},
  pages        = {288-302},
  shortjournal = {Neural Netw.},
  title        = {Event-triggered adaptive neural networks control for fractional-order nonstrict-feedback nonlinear systems with unmodeled dynamics and input saturation},
  volume       = {142},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multiple clustering for identifying subject clusters and
brain sub-networks using functional connectivity matrices without
vectorization. <em>NN</em>, <em>142</em>, 269–287. (<a
href="https://doi.org/10.1016/j.neunet.2021.05.016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In neuroscience , the functional magnetic resonance imaging (fMRI) is a vital tool to non-invasively access brain activity . Using fMRI, the functional connectivity (FC) between brain regions can be inferred, which has contributed to a number of findings of the fundamental properties of the brain. As an important clinical application of FC, clustering of subjects based on FC recently draws much attention, which can potentially reveal important heterogeneity in subjects such as subtypes of psychiatric disorders. In particular, a multiple clustering method is a powerful analytical tool, which identifies clustering patterns of subjects depending on their FC in specific brain areas. However, when one applies an existing multiple clustering method to fMRI data, there is a need to simplify the data structure , independently dealing with elements in a FC matrix, i.e., vectorizing a correlation matrix . Such a simplification may distort the clustering results . To overcome this problem, we propose a novel multiple clustering method based on Wishart mixture models, which preserves the correlation matrix structure without vectorization . The uniqueness of this method is that the multiple clustering of subjects is based on particular networks of nodes (or regions of interest, ROIs), optimized in a data-driven manner. Hence, it can identify multiple underlying pairs of associations between a subject cluster solution and a ROI sub-network. The key assumption of the method is independence among sub-networks, which is effectively addressed by whitening correlation matrices. We applied the proposed method to synthetic and fMRI data, demonstrating the usefulness and power of the proposed method.},
  archive      = {J_NN},
  author       = {Tomoki Tokuda and Okito Yamashita and Junichiro Yoshimoto},
  doi          = {10.1016/j.neunet.2021.05.016},
  journal      = {Neural Networks},
  pages        = {269-287},
  shortjournal = {Neural Netw.},
  title        = {Multiple clustering for identifying subject clusters and brain sub-networks using functional connectivity matrices without vectorization},
  volume       = {142},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust optimization and validation of echo state networks
for learning chaotic dynamics. <em>NN</em>, <em>142</em>, 252–268. (<a
href="https://doi.org/10.1016/j.neunet.2021.05.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An approach to the time-accurate prediction of chaotic solutions is by learning temporal patterns from data. Echo State Networks (ESNs), which are a class of Reservoir Computing , can accurately predict the chaotic dynamics well beyond the predictability time. Existing studies, however, also showed that small changes in the hyperparameters may markedly affect the network’s performance. The overarching aim of this paper is to improve the robustness in the selection of hyperparameters in Echo State Networks for the time-accurate prediction of chaotic solutions. We define the robustness of a validation strategy as its ability to select hyperparameters that perform consistently between validation and test sets. The goal is three-fold. First, we investigate routinely used validation strategies. Second, we propose the Recycle Validation , and the chaotic versions of existing validation strategies, to specifically tackle the forecasting of chaotic systems. Third, we compare Bayesian optimization with the traditional grid search for optimal hyperparameter selection. Numerical tests are performed on prototypical nonlinear systems that have chaotic and quasiperiodic solutions, such as the Lorenz and Lorenz-96 systems, and the Kuznetsov oscillator. Both model-free and model-informed Echo State Networks are analysed. By comparing the networks’ performance in learning chaotic (unpredictable) versus quasiperiodic (predictable) solutions, we highlight fundamental challenges in learning chaotic solutions. The proposed validation strategies, which are based on the dynamical systems properties of chaotic time series, are shown to outperform the state-of-the-art validation strategies. Because the strategies are principled – they are based on chaos theory such as the Lyapunov time – they can be applied to other Recurrent Neural Networks architectures with little modification. This work opens up new possibilities for the robust design and application of Echo State Networks, and Recurrent Neural Networks , to the time-accurate prediction of chaotic systems.},
  archive      = {J_NN},
  author       = {Alberto Racca and Luca Magri},
  doi          = {10.1016/j.neunet.2021.05.004},
  journal      = {Neural Networks},
  pages        = {252-268},
  shortjournal = {Neural Netw.},
  title        = {Robust optimization and validation of echo state networks for learning chaotic dynamics},
  volume       = {142},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Anti-transfer learning for task invariance in convolutional
neural networks for speech processing. <em>NN</em>, <em>142</em>,
238–251. (<a
href="https://doi.org/10.1016/j.neunet.2021.05.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce the novel concept of anti-transfer learning for speech processing with convolutional neural networks . While transfer learning assumes that the learning process for a target task will benefit from re-using representations learned for another task, anti-transfer avoids the learning of representations that have been learned for an orthogonal task, i.e., one that is not relevant and potentially confounding for the target task, such as speaker identity for speech recognition or speech content for emotion recognition. This extends the potential use of pre-trained models that have become increasingly available. In anti-transfer learning, we penalize similarity between activations of a network being trained on a target task and another one previously trained on an orthogonal task, which yields more suitable representations. This leads to better generalization and provides a degree of control over correlations that are spurious or undesirable, e.g. to avoid social bias. We have implemented anti-transfer for convolutional neural networks in different configurations with several similarity metrics and aggregation functions, which we evaluate and analyze with several speech and audio tasks and settings, using six datasets. We show that anti-transfer actually leads to the intended invariance to the orthogonal task and to more appropriate features for the target task at hand. Anti-transfer learning consistently improves classification accuracy in all test cases. While anti-transfer creates computation and memory cost at training time, there is relatively little computation cost when using pre-trained models for orthogonal tasks. Anti-transfer is widely applicable and particularly useful where a specific invariance is desirable or where labeled data for orthogonal tasks are difficult to obtain on a given dataset but pre-trained models are available.},
  archive      = {J_NN},
  author       = {Eric Guizzo and Tillman Weyde and Giacomo Tarroni},
  doi          = {10.1016/j.neunet.2021.05.012},
  journal      = {Neural Networks},
  pages        = {238-251},
  shortjournal = {Neural Netw.},
  title        = {Anti-transfer learning for task invariance in convolutional neural networks for speech processing},
  volume       = {142},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). H∞ synchronization of delayed neural networks via
event-triggered dynamic output control. <em>NN</em>, <em>142</em>,
231–237. (<a
href="https://doi.org/10.1016/j.neunet.2021.05.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates H ∞ H∞ exponential synchronization (ES) of neural networks (NNs) with delay by designing an event-triggered dynamic output feedback controller (ETDOFC). The ETDOFC is flexible in practice since it is applicable to both full order and reduced order dynamic output techniques. Moreover, the event generator reduces the computational burden for the zero-order-hold (ZOH) operator and does not induce sampling delay as many existing event generators do. To obtain less conservative results, the delay-partitioning method is utilized in the Lyapunov–Krasovskii functional (LKF). Synchronization criteria formulated by linear matrix inequalities (LMIs) are established. A simple algorithm is provided to design the control gains of the ETDOFC, which overcomes the difficulty induced by different dimensions of the system parameters. One numerical example is provided to demonstrate the merits of the theoretical analysis.},
  archive      = {J_NN},
  author       = {Yachun Yang and Zhengwen Tu and Liangwei Wang and Jinde Cao and Lei Shi and Wenhua Qian},
  doi          = {10.1016/j.neunet.2021.05.009},
  journal      = {Neural Networks},
  pages        = {231-237},
  shortjournal = {Neural Netw.},
  title        = {H∞ synchronization of delayed neural networks via event-triggered dynamic output control},
  volume       = {142},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Graph embedding clustering: Graph attention auto-encoder
with cluster-specificity distribution. <em>NN</em>, <em>142</em>,
221–230. (<a
href="https://doi.org/10.1016/j.neunet.2021.05.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Towards exploring the topological structure of data, numerous graph embedding clustering methods have been developed in recent years, none of them takes into account the cluster-specificity distribution of the nodes representations, resulting in suboptimal clustering performance. Moreover, most existing graph embedding clustering methods execute the nodes representations learning and clustering in two separated steps, which increases the instability of its original performance. Additionally, rare of them simultaneously takes node attributes reconstruction and graph structure reconstruction into account, resulting in degrading the capability of graph learning. In this work, we integrate the nodes representations learning and clustering into a unified framework, and propose a new deep graph attention auto-encoder for nodes clustering that attempts to learn more favorable nodes representations by leveraging self-attention mechanism and node attributes reconstruction. Meanwhile, a cluster-specificity distribution constraint, which is measured by ℓ 1 , 2 ℓ1,2 -norm, is employed to make the nodes representations within the same cluster end up with a common distribution in the dimension space while representations with different clusters have different distributions in the intrinsic dimensions. Extensive experiment results reveal that our proposed method is superior to several state-of-the-art methods in terms of performance.},
  archive      = {J_NN},
  author       = {Huiling Xu and Wei Xia and Quanxue Gao and Jungong Han and Xinbo Gao},
  doi          = {10.1016/j.neunet.2021.05.008},
  journal      = {Neural Networks},
  pages        = {221-230},
  shortjournal = {Neural Netw.},
  title        = {Graph embedding clustering: Graph attention auto-encoder with cluster-specificity distribution},
  volume       = {142},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Self-selective attention using correlation between instances
for distant supervision relation extraction. <em>NN</em>, <em>142</em>,
213–220. (<a
href="https://doi.org/10.1016/j.neunet.2021.04.032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distant supervision relation extraction methods are widely used to extract relational facts in text. The traditional selective attention model regards instances in the bag as independent of each other, which makes insufficient use of correlation information between instances and supervision information of all correctly labeled instances, affecting the performance of relation extractor. Aiming at this problem, a distant supervision relation extraction method with self-selective attention is proposed. The method uses a layer of convolution and self-attention mechanism to encode instances to learn the better semantic vector representation of instances. The correlation between instances in the bag is used to assign a higher weight to all correctly labeled instances, and the weighted summation of instances in the bag is used to obtain a bag vector representation . Experiments on the NYT dataset show that the method can make full use of the information of all correctly labeled instances in the bag. The method can achieve better results as compared with baselines.},
  archive      = {J_NN},
  author       = {Yanru Zhou and Limin Pan and Chongyou Bai and Senlin Luo and Zhouting Wu},
  doi          = {10.1016/j.neunet.2021.04.032},
  journal      = {Neural Networks},
  pages        = {213-220},
  shortjournal = {Neural Netw.},
  title        = {Self-selective attention using correlation between instances for distant supervision relation extraction},
  volume       = {142},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Efficient learning with augmented spikes: A case study with
image classification. <em>NN</em>, <em>142</em>, 205–212. (<a
href="https://doi.org/10.1016/j.neunet.2021.05.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efficient learning of spikes plays a valuable role in training spiking neural networks (SNNs) to have desired responses to input stimuli. However, current learning rules are limited to a binary form of spikes. The seemingly ubiquitous phenomenon of burst in nervous systems suggests a new way to carry more information with spike bursts in addition to times. Based on this, we introduce an advanced form, the augmented spikes, where spike coefficients are used to carry additional information. How could neurons learn and benefit from augmented spikes remains unclear. In this paper, we propose two new efficient learning rules to process spatiotemporal patterns composed of augmented spikes. Moreover, we examine the learning abilities of our methods with a synthetic recognition task of augmented spike patterns and two practical ones for image classification . Experimental results demonstrate that our rules are capable of extracting information carried by both the timing and coefficient of spikes. Our proposed approaches achieve remarkable performance and good robustness under various noise conditions, as compared to benchmarks. The improved performance indicates the merits of augmented spikes and our learning rules, which could be beneficial and generalized to a broad range of spike-based platforms.},
  archive      = {J_NN},
  author       = {Shiming Song and Chenxiang Ma and Wei Sun and Junhai Xu and Jianwu Dang and Qiang Yu},
  doi          = {10.1016/j.neunet.2021.05.002},
  journal      = {Neural Networks},
  pages        = {205-212},
  shortjournal = {Neural Netw.},
  title        = {Efficient learning with augmented spikes: A case study with image classification},
  volume       = {142},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robot navigation as hierarchical active inference.
<em>NN</em>, <em>142</em>, 192–204. (<a
href="https://doi.org/10.1016/j.neunet.2021.05.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Localization and mapping has been a long standing area of research, both in neuroscience , to understand how mammals navigate their environment, as well as in robotics, to enable autonomous mobile robots . In this paper, we treat navigation as inferring actions that minimize (expected) variational free energy under a hierarchical generative model . We find that familiar concepts like perception, path integration, localization and mapping naturally emerge from this active inference formulation. Moreover, we show that this model is consistent with models of hippocampal functions, and can be implemented in silico on a real-world robot. Our experiments illustrate that a robot equipped with our hierarchical model is able to generate topologically consistent maps, and correct navigation behaviour is inferred when a goal location is provided to the system.},
  archive      = {J_NN},
  author       = {Ozan Çatal and Tim Verbelen and Toon Van de Maele and Bart Dhoedt and Adam Safron},
  doi          = {10.1016/j.neunet.2021.05.010},
  journal      = {Neural Networks},
  pages        = {192-204},
  shortjournal = {Neural Netw.},
  title        = {Robot navigation as hierarchical active inference},
  volume       = {142},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Two-timescale neurodynamic approaches to supervised feature
selection based on alternative problem formulations. <em>NN</em>,
<em>142</em>, 180–191. (<a
href="https://doi.org/10.1016/j.neunet.2021.04.038">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection is a crucial step in data processing and machine learning . While many greedy and sequential feature selection approaches are available, a holistic neurodynamics approach to supervised feature selection is recently developed via fractional programming by minimizing feature redundancy and maximizing relevance simultaneously. In view that the gradient of the fractional objective function is also fractional, alternative problem formulations are desirable to obviate the fractional complexity. In this paper, the fractional programming problem formulation is equivalently reformulated as bilevel and bilinear programming problems without using any fractional function. Two two-timescale projection neural networks are adapted for solving the reformulated problems. Experimental results on six benchmark datasets are elaborated to demonstrate the global convergence and high classification performance of the proposed neurodynamic approaches in comparison with six mainstream feature selection approaches.},
  archive      = {J_NN},
  author       = {Yadi Wang and Jun Wang and Hangjun Che},
  doi          = {10.1016/j.neunet.2021.04.038},
  journal      = {Neural Networks},
  pages        = {180-191},
  shortjournal = {Neural Netw.},
  title        = {Two-timescale neurodynamic approaches to supervised feature selection based on alternative problem formulations},
  volume       = {142},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Arguments for the unsuitability of convolutional neural
networks for non-local tasks. <em>NN</em>, <em>142</em>, 171–179. (<a
href="https://doi.org/10.1016/j.neunet.2021.05.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks have established themselves over the past years as the state of the art method for image classification, and for many datasets, they even surpass humans in categorizing images. Unfortunately, the same architectures perform much worse when they have to compare parts of an image to each other to correctly classify this image. Until now, no well-formed theoretical argument has been presented to explain this deficiency. In this paper, we will argue that convolutional layers are of little use for such problems, since comparison tasks are global by nature, but convolutional layers are local by design. We will use this insight to reformulate a comparison task into a sorting task and use findings on sorting networks to propose a lower bound for the number of parameters a neural network needs to solve comparison tasks in a generalizable way. We will use this lower bound to argue that attention, as well as iterative/recurrent processing, is needed to prevent a combinatorial explosion.},
  archive      = {J_NN},
  author       = {Sebastian Stabinger and David Peer and Antonio Rodríguez-Sánchez},
  doi          = {10.1016/j.neunet.2021.05.001},
  journal      = {Neural Networks},
  pages        = {171-179},
  shortjournal = {Neural Netw.},
  title        = {Arguments for the unsuitability of convolutional neural networks for non-local tasks},
  volume       = {142},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multistability and associative memory of neural networks
with morita-like activation functions. <em>NN</em>, <em>142</em>,
162–170. (<a
href="https://doi.org/10.1016/j.neunet.2021.04.035">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents the multistability analysis and associative memory of neural networks (NNs) with Morita-like activation functions . In order to seek larger memory capacity, this paper proposes Morita-like activation functions . In a weakened condition, this paper shows that the NNs with n n -neurons have ( 2 m + 1 ) n (2m+1)n equilibrium points (Eps) and ( m + 1 ) n (m+1)n of them are locally exponentially stable, where the parameter m m depends on the Morita-like activation functions, called Morita parameter. Also the attraction basins are estimated based on the state space partition. Moreover, this paper applies these NNs into associative memories (AMs). Compared with the previous related works, the number of Eps and AM’s memory capacity are extensively increased. The simulation results are illustrated and some reliable associative memories examples are shown at the end of this paper.},
  archive      = {J_NN},
  author       = {Yuanchu Shen and Song Zhu and Xiaoyang Liu and Shiping Wen},
  doi          = {10.1016/j.neunet.2021.04.035},
  journal      = {Neural Networks},
  pages        = {162-170},
  shortjournal = {Neural Netw.},
  title        = {Multistability and associative memory of neural networks with morita-like activation functions},
  volume       = {142},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Statistical guarantees for regularized neural networks.
<em>NN</em>, <em>142</em>, 148–161. (<a
href="https://doi.org/10.1016/j.neunet.2021.04.034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural networks have become standard tools in the analysis of data , but they lack comprehensive mathematical theories . For example, there are very few statistical guarantees for learning neural networks from data, especially for classes of estimators that are used in practice or at least similar to such. In this paper, we develop a general statistical guarantee for estimators that consist of a least-squares term and a regularizer. We then exemplify this guarantee with ℓ 1 ℓ1 -regularization, showing that the corresponding prediction error increases at most logarithmically in the total number of parameters and can even decrease in the number of layers. Our results establish a mathematical basis for regularized estimation of neural networks, and they deepen our mathematical understanding of neural networks and deep learning more generally.},
  archive      = {J_NN},
  author       = {Mahsa Taheri and Fang Xie and Johannes Lederer},
  doi          = {10.1016/j.neunet.2021.04.034},
  journal      = {Neural Networks},
  pages        = {148-161},
  shortjournal = {Neural Netw.},
  title        = {Statistical guarantees for regularized neural networks},
  volume       = {142},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Probabilistic robustness estimates for feed-forward neural
networks. <em>NN</em>, <em>142</em>, 138–147. (<a
href="https://doi.org/10.1016/j.neunet.2021.04.037">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robustness of deep neural networks is a critical issue in practical applications. In the general case of feed-forward neural networks (including convolutional deep neural network architectures), under random noise attacks, we propose to study the probability that the output of the network deviates from its nominal value by a given threshold. We derive a simple concentration inequality for the propagation of the input uncertainty through the network using the Cramer–Chernoff method and estimates of the local variation of the neural network mapping computed at the training points. We further discuss and exploit the resulting condition on the network to regularize the loss function during training. Finally, we assess the proposed tail probability estimates empirically on various public datasets and show that the observed robustness is very well estimated by the proposed method.},
  archive      = {J_NN},
  author       = {Nicolas Couellan},
  doi          = {10.1016/j.neunet.2021.04.037},
  journal      = {Neural Networks},
  pages        = {138-147},
  shortjournal = {Neural Netw.},
  title        = {Probabilistic robustness estimates for feed-forward neural networks},
  volume       = {142},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Why grid cells function as a metric for space. <em>NN</em>,
<em>142</em>, 128–137. (<a
href="https://doi.org/10.1016/j.neunet.2021.04.031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The brain is able to calculate the distance and direction to the desired position based on grid cells. Extensive neurophysiological studies of rodent navigation have postulated the grid cells function as a metric for space, and have inspired many computational studies to develop innovative navigation approaches. Furthermore, grid cells may provide a general encoding scheme for high-order nonspatial information. Built upon existing neuroscience and machine learning work, this paper provides theoretical clarity on that the grid cell population codes can be taken as a metric for space. The metric is generated by a shift-invariant positive definite kernel via kernel distance method and embeds isometrically in a Euclidean space , and the inner product of the grid cell population code exponentially converges to the kernel. We also provide a method to learn the distribution of grid cell population efficiently. Grid cells, as a scalable position encoding method, can encode the spatial relationships of places and enable grid cells to outperform place cells in navigation. Further, we extend the grid cell to images encoding and find that grid cells embed images into a mental map, where geometric relationships are conceptual relationships of images. The theoretical model and analysis would contribute to establishing the grid cell code as a generic coding scheme for both spatial and conceptual spaces, and is promising for a multitude of problems across spatial cognition, machine learning and semantic cognition.},
  archive      = {J_NN},
  author       = {Suogui Dang and Yining Wu and Rui Yan and Huajin Tang},
  doi          = {10.1016/j.neunet.2021.04.031},
  journal      = {Neural Networks},
  pages        = {128-137},
  shortjournal = {Neural Netw.},
  title        = {Why grid cells function as a metric for space},
  volume       = {142},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Novel criteria for global robust stability of dynamical
neural networks with multiple time delays. <em>NN</em>, <em>142</em>,
119–127. (<a
href="https://doi.org/10.1016/j.neunet.2021.04.039">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This research article considers the problem regarding global robust asymptotic stability of the general type of dynamical neural networks involving multiple constant time delays . Some new sufficient criteria are proposed for the existence, uniqueness and global asymptotic stability of the equilibrium point of this neural network model whose network parameters possess uncertainties. This paper will first address the existence and uniqueness problem for equilibrium points by utilizing the Homomorphic transformation theory. Secondly, by exploiting a novel Lyapunov functional candidate, the sufficient conditions for asymptotic stability of equilibrium points of this class of delayed neural networks will be established. The derived robust stability conditions are expressed independently of the constant time delay parameters and define some novel relationships among network parameters of the considered neural network. Thus, the applicability and validity of the obtained robust stability conditions for delayed-type neural networks can be easily tested. The comprehensive comparisons among the results of the current article and some of previously derived corresponding results will also be made by giving an illustrative numerical example.},
  archive      = {J_NN},
  author       = {Emel Arslan},
  doi          = {10.1016/j.neunet.2021.04.039},
  journal      = {Neural Networks},
  pages        = {119-127},
  shortjournal = {Neural Netw.},
  title        = {Novel criteria for global robust stability of dynamical neural networks with multiple time delays},
  volume       = {142},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Probabilistic learning vector quantization on manifold of
symmetric positive definite matrices. <em>NN</em>, <em>142</em>,
105–118. (<a
href="https://doi.org/10.1016/j.neunet.2021.04.024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we develop a new classification method for manifold-valued data in the framework of probabilistic learning vector quantization . In many classification scenarios, the data can be naturally represented by symmetric positive definite matrices, which are inherently points that live on a curved Riemannian manifold . Due to the non-Euclidean geometry of Riemannian manifolds, traditional Euclidean machine learning algorithms yield poor results on such data. In this paper, we generalize the probabilistic learning vector quantization algorithm for data points living on the manifold of symmetric positive definite matrices equipped with Riemannian natural metric (affine-invariant metric). By exploiting the induced Riemannian distance , we derive the probabilistic learning Riemannian space quantization algorithm, obtaining the learning rule through Riemannian gradient descent . Empirical investigations on synthetic data, image data , and motor imagery electroencephalogram (EEG) data demonstrate the superior performance of the proposed method.},
  archive      = {J_NN},
  author       = {Fengzhen Tang and Haifeng Feng and Peter Tino and Bailu Si and Daxiong Ji},
  doi          = {10.1016/j.neunet.2021.04.024},
  journal      = {Neural Networks},
  pages        = {105-118},
  shortjournal = {Neural Netw.},
  title        = {Probabilistic learning vector quantization on manifold of symmetric positive definite matrices},
  volume       = {142},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Flexible multi-view semi-supervised learning with unified
graph. <em>NN</em>, <em>142</em>, 92–104. (<a
href="https://doi.org/10.1016/j.neunet.2021.04.033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {At present, the diversity of data acquisition boosts the growth of multi-view data and the lack of label information. Since manually labeling is expensive and impractical, it is practical to enhance learning performance with a small amount of labeled data and a large amount of unlabeled data . In this study, we propose a novel multi-view semi-supervised learning (MSEL) framework termed flexible MSEL (FMSEL) with unified graph. In this framework, two flexible regression residual terms are introduced. One is a linear penalty term, which adaptively weighs the diverse contributions of different views and properly learns a well structured unified graph. The other is a relaxation regularization term, which finds the optimal prediction and the linear regression function . Both the prediction of samples in the database and new-coming data are supported. Moreover, during the process, the unified graph learns depending on the data structure and dynamically updated label information. Further, we provide an alternating optimization algorithm to iteratively solve the resultant objective problem and theoretically analyze the corresponding complexities. Extensive experiments conducted on synthetic and public datasets demonstrate the superiority of FMSEL.},
  archive      = {J_NN},
  author       = {Zhongheng Li and Qianyao Qiang and Bin Zhang and Fei Wang and Feiping Nie},
  doi          = {10.1016/j.neunet.2021.04.033},
  journal      = {Neural Networks},
  pages        = {92-104},
  shortjournal = {Neural Netw.},
  title        = {Flexible multi-view semi-supervised learning with unified graph},
  volume       = {142},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Generalized two-dimensional linear discriminant analysis
with regularization. <em>NN</em>, <em>142</em>, 73–91. (<a
href="https://doi.org/10.1016/j.neunet.2021.04.030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances show that two-dimensional linear discriminant analysis (2DLDA) is a successful matrix based dimensionality reduction method. However, 2DLDA may encounter the singularity issue theoretically, and also is sensitive to outliers. In this paper, a generalized Lp-norm 2DLDA framework with regularization for an arbitrary p &gt; 0 p&amp;gt;0 is proposed, named G2DLDA. There are mainly two contributions of G2DLDA: one is G2DLDA model uses an arbitrary Lp-norm to measure the between-class and within-class scatter, and hence a proper p p can be selected to achieve robustness. The other one is that the introduced regularization term makes G2DLDA enjoy better generalization performance and avoid singularity . In addition, an effective learning algorithm is designed for G2LDA, which can be solved through a series of convex problems with closed-form solutions. Its convergence can be guaranteed theoretically when 1 ≤ p ≤ 2 1≤p≤2 . Preliminary experimental results on three contaminated human face databases show the effectiveness of the proposed G2DLDA.},
  archive      = {J_NN},
  author       = {Chun-Na Li and Yuan-Hai Shao and Wei-Jie Chen and Zhen Wang and Nai-Yang Deng},
  doi          = {10.1016/j.neunet.2021.04.030},
  journal      = {Neural Networks},
  pages        = {73-91},
  shortjournal = {Neural Netw.},
  title        = {Generalized two-dimensional linear discriminant analysis with regularization},
  volume       = {142},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Towards a mathematical framework to inform neural network
modelling via polynomial regression. <em>NN</em>, <em>142</em>, 57–72.
(<a href="https://doi.org/10.1016/j.neunet.2021.04.036">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Even when neural networks are widely used in a large number of applications, they are still considered as black boxes and present some difficulties for dimensioning or evaluating their prediction error. This has led to an increasing interest in the overlapping area between neural networks and more traditional statistical methods, which can help overcome those problems. In this article, a mathematical framework relating neural networks and polynomial regression is explored by building an explicit expression for the coefficients of a polynomial regression from the weights of a given neural network, using a Taylor expansion approach. This is achieved for single hidden layer neural networks in regression problems . The validity of the proposed method depends on different factors like the distribution of the synaptic potentials or the chosen activation function . The performance of this method is empirically tested via simulation of synthetic data generated from polynomials to train neural networks with different structures and hyperparameters, showing that almost identical predictions can be obtained when certain conditions are met. Lastly, when learning from polynomial generated data, the proposed method produces polynomials that approximate correctly the data locally.},
  archive      = {J_NN},
  author       = {Pablo Morala and Jenny Alexandra Cifuentes and Rosa E. Lillo and Iñaki Ucar},
  doi          = {10.1016/j.neunet.2021.04.036},
  journal      = {Neural Networks},
  pages        = {57-72},
  shortjournal = {Neural Netw.},
  title        = {Towards a mathematical framework to inform neural network modelling via polynomial regression},
  volume       = {142},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Growth strategy determines the memory and structural
properties of brain networks. <em>NN</em>, <em>142</em>, 44–56. (<a
href="https://doi.org/10.1016/j.neunet.2021.04.027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The interplay between structure and function affects the emerging properties of many natural systems. Here we use an adaptive neural network model that couples activity and topological dynamics and reproduces the experimental temporal profiles of synaptic density observed in the brain. We prove that the existence of a transient period of relatively high synaptic connectivity is critical for the development of the system under noise circumstances, such that the resulting network can recover stored memories. Moreover, we show that intermediate synaptic densities provide optimal developmental paths with minimum energy consumption, and that ultimately it is the transient heterogeneity in the network that determines its evolution. These results could explain why the pruning curves observed in actual brain areas present their characteristic temporal profiles and they also suggest new design strategies to build biologically inspired neural networks with particular information processing capabilities.},
  archive      = {J_NN},
  author       = {Ana P. Millán and Joaquín J. Torres and Samuel Johnson and J. Marro},
  doi          = {10.1016/j.neunet.2021.04.027},
  journal      = {Neural Networks},
  pages        = {44-56},
  shortjournal = {Neural Netw.},
  title        = {Growth strategy determines the memory and structural properties of brain networks},
  volume       = {142},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Cluster synchronization of delayed coupled neural networks:
Delay-dependent distributed impulsive control. <em>NN</em>,
<em>142</em>, 34–43. (<a
href="https://doi.org/10.1016/j.neunet.2021.04.026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the issue of cluster synchronization (CS) for the coupled neural networks (CNNs) with time-varying delays via the delay-dependent distributed impulsive control. A new Halanay-like inequality, where delayed impulses are taken into consideration, is proposed. Based on the Lyapunov theory and the new differential inequality , sufficient conditions of CS for delayed CNNs with fixed and switching coupling topology are obtained, respectively. Moreover, delay-dependent distributed impulsive controllers with fixed or switching topology are designed thereby. Finally, we present a numerical example of CNNs with fixed or switching coupling to verify the effectiveness of our results, respectively.},
  archive      = {J_NN},
  author       = {Xiaoyu Zhang and Chuandong Li and Zhilong He},
  doi          = {10.1016/j.neunet.2021.04.026},
  journal      = {Neural Networks},
  pages        = {34-43},
  shortjournal = {Neural Netw.},
  title        = {Cluster synchronization of delayed coupled neural networks: Delay-dependent distributed impulsive control},
  volume       = {142},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Enhancing graph neural networks by a high-quality
aggregation of beneficial information. <em>NN</em>, <em>142</em>, 20–33.
(<a href="https://doi.org/10.1016/j.neunet.2021.04.025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Neural Networks (GNNs), such as GCN , GraphSAGE, GAT , and SGC, have achieved state-of-the-art performance on a wide range of graph-based tasks. These models all use a technique called neighborhood aggregation, in which the embedding of each node is updated by aggregating the embeddings of its neighbors. However, not all information aggregated from neighbors is beneficial. In some cases, a portion of the neighbor information may be harmful to the downstream tasks. For the high-quality aggregation of beneficial information, we propose a flexible method EGAI (Enhancing Graph neural networks by a high-quality Aggregation of beneficial Information). The core concept of this method is to filter out the redundant and harmful information by removing specific edges during each training epoch. The practical and theoretical motivations, considerations, and strategies related to this method are discussed in detail. EGAI is a general method that can be combined with many backbone models (e.g., GCN, GraphSAGE, GAT, and SGC) to enhance their performance in the node classification task. In addition, EGAI reduces the convergence speed of over-smoothing that occurs when models are deepened. Extensive experiments on three real-world networks demonstrate that EGAI indeed improves the performance for both shallow and deep GNN models, and to some extent, mitigates over-smoothing. The code is available at https://github.com/liucoo/egai .},
  archive      = {J_NN},
  author       = {Chuang Liu and Jia Wu and Weiwei Liu and Wenbin Hu},
  doi          = {10.1016/j.neunet.2021.04.025},
  journal      = {Neural Networks},
  pages        = {20-33},
  shortjournal = {Neural Netw.},
  title        = {Enhancing graph neural networks by a high-quality aggregation of beneficial information},
  volume       = {142},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). FastGAE: Scalable graph autoencoders with stochastic
subgraph decoding. <em>NN</em>, <em>142</em>, 1–19. (<a
href="https://doi.org/10.1016/j.neunet.2021.04.015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph autoencoders (AE) and variational autoencoders (VAE) are powerful node embedding methods, but suffer from scalability issues. In this paper, we introduce FastGAE, a general framework to scale graph AE and VAE to large graphs with millions of nodes and edges. Our strategy, based on an effective stochastic subgraph decoding scheme, significantly speeds up the training of graph AE and VAE while preserving or even improving performances. We demonstrate the effectiveness of FastGAE on various real-world graphs, outperforming the few existing approaches to scale graph AE and VAE by a wide margin.},
  archive      = {J_NN},
  author       = {Guillaume Salha and Romain Hennequin and Jean-Baptiste Remy and Manuel Moussallam and Michalis Vazirgiannis},
  doi          = {10.1016/j.neunet.2021.04.015},
  journal      = {Neural Networks},
  pages        = {1-19},
  shortjournal = {Neural Netw.},
  title        = {FastGAE: Scalable graph autoencoders with stochastic subgraph decoding},
  volume       = {142},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021d). INN/ENNS/JNNS - membership applic. form. <em>NN</em>,
<em>141</em>, II. (<a
href="https://doi.org/10.1016/S0893-6080(21)00289-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NN},
  doi          = {10.1016/S0893-6080(21)00289-6},
  journal      = {Neural Networks},
  pages        = {II},
  shortjournal = {Neural Netw.},
  title        = {INN/ENNS/JNNS - membership applic. form},
  volume       = {141},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021d). Current events. <em>NN</em>, <em>141</em>, I. (<a
href="https://doi.org/10.1016/S0893-6080(21)00288-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NN},
  doi          = {10.1016/S0893-6080(21)00288-4},
  journal      = {Neural Networks},
  pages        = {I},
  shortjournal = {Neural Netw.},
  title        = {Current events},
  volume       = {141},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). CutCat: An augmentation method for EEG classification.
<em>NN</em>, <em>141</em>, 433–443. (<a
href="https://doi.org/10.1016/j.neunet.2021.05.032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The non-invasive electroencephalogram (EEG) signals enable humans to communicate with devices and have control over them, this process requires precise classification and identification of those signals. The recent revolution of deep learning has empowered both feature extraction and classification in a joint manner of different data types . However, deep learning is a data learning approach that demands a large number of training samples. Whilst, the EEG research field lacks a large amount of data which restricts the use of deep learning within this field. This paper proposes a novel augmentation method for enlarging EEG datasets. Our CutCat augmentation method generates trials from inter- and intra-subjects and trials. The method relies on cutting a specific period from an EEG trial and concatenating it with a period from another trial from the same subject or different subjects. The method has been tested on shallow and deep convolutional neural networks (CNN) for the classification of motor imagery (MI) EEG data. Two input formulation types images and time-series have been used as input to the neural networks . Short-time Fourier transform (STFT) is used for generating training images from the time-series signals. The experimental results demonstrate that the proposed augmentation method is a promising strategy for handling the classification of small-scale datasets. Classification results on two EEG datasets show advancement in comparison with the results of state-of-the-art researches.},
  archive      = {J_NN},
  author       = {Ali Al-Saegh and Shefa A. Dawwd and Jassim M. Abdul-Jabbar},
  doi          = {10.1016/j.neunet.2021.05.032},
  journal      = {Neural Networks},
  pages        = {433-443},
  shortjournal = {Neural Netw.},
  title        = {CutCat: An augmentation method for EEG classification},
  volume       = {141},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). QTTNet: Quantized tensor train neural networks for 3D object
and video recognition. <em>NN</em>, <em>141</em>, 420–432. (<a
href="https://doi.org/10.1016/j.neunet.2021.05.034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Relying on the rapidly increasing capacity of computing clusters and hardware, convolutional neural networks (CNNs) have been successfully applied in various fields and achieved state-of-the-art results. Despite these exciting developments, the huge memory cost is still involved in training and inferring a large-scale CNN model and makes it hard to be widely used in resource-limited portable devices. To address this problem, we establish a training framework for three-dimensional convolutional neural networks (3DCNNs) named QTTNet that combines tensor train (TT) decomposition and data quantization together for further shrinking the model size and decreasing the memory and time cost. Through this framework, we can fully explore the superiority of TT in reducing the number of trainable parameters and the advantage of quantization in decreasing the bit-width of data, particularly compressing 3DCNN model greatly with little accuracy degradation. In addition, due to the low bit quantization to all parameters during the inference process including TT-cores, activations, and batch normalizations , the proposed method naturally takes advantage in memory and time cost. Experimental results of compressing 3DCNNs for 3D object and video recognition on ModelNet40, UCF11, and UCF50 datasets verify the effectiveness of the proposed method. The best compression ratio we have obtained is up to nearly 180 × × with competitive performance compared with other state-of-the-art researches. Moreover, the total bytes of our QTTNet models on ModelNet40 and UCF11 datasets can be 1000 × × lower than some typical practices such as MVCNN.},
  archive      = {J_NN},
  author       = {Donghyun Lee and Dingheng Wang and Yukuan Yang and Lei Deng and Guangshe Zhao and Guoqi Li},
  doi          = {10.1016/j.neunet.2021.05.034},
  journal      = {Neural Networks},
  pages        = {420-432},
  shortjournal = {Neural Netw.},
  title        = {QTTNet: Quantized tensor train neural networks for 3D object and video recognition},
  volume       = {141},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A deep network construction that adapts to intrinsic
dimensionality beyond the domain. <em>NN</em>, <em>141</em>, 404–419.
(<a href="https://doi.org/10.1016/j.neunet.2021.06.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the approximation of two-layer compositions f ( x ) = g ( ϕ ( x ) ) f(x)=g(ϕ(x)) via deep networks with ReLU activation, where ϕ ϕ is a geometrically intuitive, dimensionality reducing feature map. We focus on two intuitive and practically relevant choices for ϕ ϕ : the projection onto a low-dimensional embedded submanifold and a distance to a collection of low-dimensional sets. We achieve near optimal approximation rates , which depend only on the complexity of the dimensionality reducing map ϕ ϕ rather than the ambient dimension. Since ϕ ϕ encapsulates all nonlinear features that are material to the function f f , this suggests that deep nets are faithful to an intrinsic dimension governed by f f rather than the complexity of the domain of f f . In particular, the prevalent assumption of approximating functions on low-dimensional manifolds can be significantly relaxed using functions of type f ( x ) = g ( ϕ ( x ) ) f(x)=g(ϕ(x)) with ϕ ϕ representing an orthogonal projection onto the same manifold.},
  archive      = {J_NN},
  author       = {Alexander Cloninger and Timo Klock},
  doi          = {10.1016/j.neunet.2021.06.004},
  journal      = {Neural Networks},
  pages        = {404-419},
  shortjournal = {Neural Netw.},
  title        = {A deep network construction that adapts to intrinsic dimensionality beyond the domain},
  volume       = {141},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). PC-GAIN: Pseudo-label conditional generative adversarial
imputation networks for incomplete data. <em>NN</em>, <em>141</em>,
395–403. (<a
href="https://doi.org/10.1016/j.neunet.2021.05.033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Datasets with missing values are very common in real world applications . GAIN, a recently proposed deep generative model for missing data imputation , has been proved to outperform many state-of-the-art methods. But GAIN only uses a reconstruction loss in the generator to minimize the imputation error of the non-missing part, ignoring the potential category information which can reflect the relationship between samples. In this paper, we propose a novel unsupervised missing data imputation method named PC-GAIN, which utilizes potential category information to further enhance the imputation power. Specifically, we first propose a pre-training procedure to learn potential category information contained in a subset of low-missing-rate data. Then an auxiliary classifier is determined using the synthetic pseudo-labels. Further, this classifier is incorporated into the generative adversarial framework to help the generator to yield higher quality imputation results. The proposed method can improve the imputation quality of GAIN significantly. Experimental results on various benchmark datasets show that our method is also superior to other baseline approaches. Our code is available at https://github.com/WYu-Feng/pc-gain .},
  archive      = {J_NN},
  author       = {Yufeng Wang and Dan Li and Xiang Li and Min Yang},
  doi          = {10.1016/j.neunet.2021.05.033},
  journal      = {Neural Networks},
  pages        = {395-403},
  shortjournal = {Neural Netw.},
  title        = {PC-GAIN: Pseudo-label conditional generative adversarial imputation networks for incomplete data},
  volume       = {141},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). CRaDLe: Deep code retrieval based on semantic dependency
learning. <em>NN</em>, <em>141</em>, 385–394. (<a
href="https://doi.org/10.1016/j.neunet.2021.04.019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Code retrieval is a common practice for programmers to reuse existing code snippets in the open-source repositories. Given a user query (i.e., a natural language description), code retrieval aims at searching the most relevant ones from a set of code snippets. The main challenge of effective code retrieval lies in mitigating the semantic gap between natural language descriptions and code snippets. With the ever-increasing amount of available open-source code, recent studies resort to neural networks to learn the semantic matching relationships between the two sources. The statement-level dependency information, which highlights the dependency relations among the program statements during the execution, reflects the structural importance of one statement in the code, which is favorable for accurately capturing the code semantics but has never been explored for the code retrieval task. In this paper, we propose CRaDLe, a novel approach for C ode R etrieval based on statement-level sem a ntic D ependency Le arning. Specifically, CRaDLe distills code representations through fusing both the dependency and semantic information at the statement level, and then learns a unified vector representation for each code and description pair for modeling the matching relationship. Comprehensive experiments and analysis on real-world datasets show that the proposed approach can accurately retrieve code snippets for a given query and significantly outperform the state-of-the-art approaches on the task.},
  archive      = {J_NN},
  author       = {Wenchao Gu and Zongjie Li and Cuiyun Gao and Chaozheng Wang and Hongyu Zhang and Zenglin Xu and Michael R. Lyu},
  doi          = {10.1016/j.neunet.2021.04.019},
  journal      = {Neural Networks},
  pages        = {385-394},
  shortjournal = {Neural Netw.},
  title        = {CRaDLe: Deep code retrieval based on semantic dependency learning},
  volume       = {141},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Combination of deep speaker embeddings for diarisation.
<em>NN</em>, <em>141</em>, 372–384. (<a
href="https://doi.org/10.1016/j.neunet.2021.04.020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Significant progress has recently been made in speaker diarisation after the introduction of d-vectors as speaker embeddings extracted from neural network (NN) speaker classifiers for clustering speech segments. To extract better-performing and more robust speaker embeddings, this paper proposes a c-vector method by combining multiple sets of complementary d-vectors derived from systems with different NN components. Three structures are used to implement the c-vectors, namely 2D self-attentive, gated additive, and bilinear pooling structures, relying on attention mechanisms , a gating mechanism, and a low-rank bilinear pooling mechanism respectively. Furthermore, a neural-based single-pass speaker diarisation pipeline is also proposed in this paper, which uses NNs to achieve voice activity detection , speaker change point detection, and speaker embedding extraction. Experiments and detailed analyses are conducted on the challenging AMI and NIST RT05 datasets which consist of real meetings with 4–10 speakers and a wide range of acoustic conditions. For systems trained on the AMI training set, relative speaker error rate (SER) reductions of 13\% and 29\% are obtained by using c-vectors instead of d-vectors on the AMI dev and eval sets respectively, and a relative SER reduction of 15\% in SER is observed on RT05, which shows the robustness of the proposed methods. By incorporating VoxCeleb data into the training set, the best c-vector system achieved 7\%, 17\% and 16\% relative SER reduction compared to the d-vector on the AMI dev, eval and RT05 sets respectively.},
  archive      = {J_NN},
  author       = {Guangzhi Sun and Chao Zhang and Philip C. Woodland},
  doi          = {10.1016/j.neunet.2021.04.020},
  journal      = {Neural Networks},
  pages        = {372-384},
  shortjournal = {Neural Netw.},
  title        = {Combination of deep speaker embeddings for diarisation},
  volume       = {141},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Generative adversarial network with multi-branch
discriminator for imbalanced cross-species image-to-image translation.
<em>NN</em>, <em>141</em>, 355–371. (<a
href="https://doi.org/10.1016/j.neunet.2021.04.013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There has been an increased interest in high-level image-to-image translation to achieve semantic matching. Through a powerful translation model , we can efficiently synthesize high-quality images with diverse appearances while retaining semantic matching. In this paper, we address an imbalanced learning problem using a cross-species image-to-image translation. We aim to perform the data augmentation through the image translation to boost the recognition performance of imbalanced learning. It requires a strong ability of the model to perform a biomorphic transformation on a semantic level . To tackle this problem, we propose a novel, simple, and effective structure of Multi-Branch Discriminator (termed as MBD) based on Generative Adversarial Networks (GANs). We demonstrate the effectiveness of the proposed MBD through theoretical analysis as well as empirical evaluation. We provide theoretical proof of why the proposed MBD is an effective and optimal case to achieve remarkable performance. Comprehensive experiments on various cross-species image translation tasks illustrate that our MBD can dramatically promote the performance of popular GANs with state-of-the-art results in terms of both objective and subjective assessments. Extensive downstream image recognition evaluations at a few-shot setting have also been conducted to demonstrate that the proposed method can effectively boost the performance of imbalanced learning.},
  archive      = {J_NN},
  author       = {Ziqiang Zheng and Zhibin Yu and Yang Wu and Haiyong Zheng and Bing Zheng and Minho Lee},
  doi          = {10.1016/j.neunet.2021.04.013},
  journal      = {Neural Networks},
  pages        = {355-371},
  shortjournal = {Neural Netw.},
  title        = {Generative adversarial network with multi-branch discriminator for imbalanced cross-species image-to-image translation},
  volume       = {141},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bifurcations in a fractional-order BAM neural network with
four different delays. <em>NN</em>, <em>141</em>, 344–354. (<a
href="https://doi.org/10.1016/j.neunet.2021.04.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper illuminates the issue of bifurcations for a fractional-order bidirectional associative memory neural network(FOBAMNN) with four different delays. On account of the affirmatory presumption, the developed FOBAMNN is firstly transformed into the one with two nonidentical delays. Then the critical values of Hopf bifurcations with respect to disparate delays are calculated quantitatively by establishing one delay and selecting remaining delay as a bifurcation parameter in the transformed model. It detects that the stability of the developed FOBAMNN with multiple delays can be fairly preserved if selecting lesser control delays, and Hopf bifurcation emerges once the control delays outnumber their critical values. The derived bifurcation results are numerically testified via the bifurcation graphs. The feasibility of theoretical analysis is ultimately corroborated in the light of simulation experiments. The analytic results available in this paper are beneficial to give impetus to resolve the issues of bifurcations of high-order FONNs with multiple delays.},
  archive      = {J_NN},
  author       = {Chengdai Huang and Juan Wang and Xiaoping Chen and Jinde Cao},
  doi          = {10.1016/j.neunet.2021.04.005},
  journal      = {Neural Networks},
  pages        = {344-354},
  shortjournal = {Neural Netw.},
  title        = {Bifurcations in a fractional-order BAM neural network with four different delays},
  volume       = {141},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Autoencoder networks extract latent variables and encode
these variables in their connectomes. <em>NN</em>, <em>141</em>,
330–343. (<a
href="https://doi.org/10.1016/j.neunet.2021.03.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advances in electron microscopy and data processing techniques are leading to increasingly large and complete microscale connectomes. At the same time, advances in artificial neural networks have produced model systems that perform comparably rich computations with perfectly specified connectivity. This raises an exciting scientific opportunity for the study of both biological and artificial neural networks : to infer the underlying circuit function from the structure of its connectivity. A potential roadblock, however, is that – even with well constrained neural dynamics – there are in principle many different connectomes that could support a given computation. Here, we define a tractable setting in which the problem of inferring circuit function from circuit connectivity can be analyzed in detail: the function of input compression and reconstruction, in an autoencoder network with a single hidden layer. Here, in general there is substantial ambiguity in the weights that can produce the same circuit function, because largely arbitrary changes to input weights can be undone by applying the inverse modifications to the output weights. However, we use mathematical arguments and simulations to show that adding simple, biologically motivated regularization of connectivity resolves this ambiguity in an interesting way: weights are constrained such that the latent variable structure underlying the inputs can be extracted from the weights by using nonlinear dimensionality reduction methods.},
  archive      = {J_NN},
  author       = {Matthew Farrell and Stefano Recanatesi and R. Clay Reid and Stefan Mihalas and Eric Shea-Brown},
  doi          = {10.1016/j.neunet.2021.03.010},
  journal      = {Neural Networks},
  pages        = {330-343},
  shortjournal = {Neural Netw.},
  title        = {Autoencoder networks extract latent variables and encode these variables in their connectomes},
  volume       = {141},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning emotions latent representation with CVAE for
text-driven expressive audiovisual speech synthesis. <em>NN</em>,
<em>141</em>, 315–329. (<a
href="https://doi.org/10.1016/j.neunet.2021.04.021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Great improvement has been made in the field of expressive audiovisual Text-to-Speech synthesis (EAVTTS) thanks to deep learning techniques. However, generating realistic speech is still an open issue and researchers in this area have been focusing lately on controlling the speech variability. In this paper, we use different neural architectures to synthesize emotional speech. We study the application of unsupervised learning techniques for emotional speech modeling as well as methods for restructuring emotions representation to make it continuous and more flexible. This manipulation of the emotional representation should allow us to generate new styles of speech by mixing emotions. We first present our expressive audiovisual corpus. We validate the emotional content of this corpus with three perceptual experiments using acoustic only, visual only and audiovisual stimuli. After that, we analyze the performance of a fully connected neural network in learning characteristics specific to different emotions for the phone duration aspect and the acoustic and visual modalities . We also study the contribution of a joint and separate training of the acoustic and visual modalities in the quality of the generated synthetic speech. In the second part of this paper, we use a conditional variational auto-encoder (CVAE) architecture to learn a latent representation of emotions. We applied this method in an unsupervised manner to generate features of expressive speech. We used a probabilistic metric to compute the overlapping degree between emotions latent clusters to choose the best parameters for the CVAE. By manipulating the latent vectors, we were able to generate nuances of a given emotion and to generate new emotions that do not exist in our database. For these new emotions, we obtain a coherent articulation. We conducted four perceptual experiments to evaluate our findings.},
  archive      = {J_NN},
  author       = {Sara Dahmani and Vincent Colotte and Valérian Girard and Slim Ouni},
  doi          = {10.1016/j.neunet.2021.04.021},
  journal      = {Neural Networks},
  pages        = {315-329},
  shortjournal = {Neural Netw.},
  title        = {Learning emotions latent representation with CVAE for text-driven expressive audiovisual speech synthesis},
  volume       = {141},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). FastTalker: A neural text-to-speech architecture with
shallow and group autoregression. <em>NN</em>, <em>141</em>, 306–314.
(<a href="https://doi.org/10.1016/j.neunet.2021.04.016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Non-autoregressive architecture for neural text-to-speech (TTS) allows for parallel implementation, thus reduces inference time over its autoregressive counterpart. However, such system architecture does not explicitly model temporal dependency of acoustic signal as it generates individual acoustic frames independently. The lack of temporal modeling often adversely impacts speech continuity, thus voice quality. In this paper, we propose a novel neural TTS model that is denoted as FastTalker . We study two strategies for high-quality speech synthesis at low computational cost. First, we add a shallow autoregressive acoustic decoder on top of the non-autoregressive context decoder to retrieve the temporal information of the acoustic signal. Second, we further implement group autoregression to accelerate the inference of the autoregressive acoustic decoder. The group-based autoregression acoustic decoder generates acoustic features as a sequence of groups instead of frames, each group having multiple consecutive frames. Within a group, the acoustic features are generated in parallel. With the shallow and group autoregression, FastTalker retrieves the temporal information of the acoustic signal, while keeping the fast-decoding property. The proposed FastTalker achieves a good balance between speech quality and inference speed. Experiments show that, in terms of voice quality and naturalness , FastTalker outperforms the non-autoregressive FastSpeech baseline significantly, and is on par with the autoregressive baselines. It also shows a considerable inference speedup over Tacotron2 and Transformer TTS.},
  archive      = {J_NN},
  author       = {Rui Liu and Berrak Sisman and Yixing Lin and Haizhou Li},
  doi          = {10.1016/j.neunet.2021.04.016},
  journal      = {Neural Networks},
  pages        = {306-314},
  shortjournal = {Neural Netw.},
  title        = {FastTalker: A neural text-to-speech architecture with shallow and group autoregression},
  volume       = {141},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Radon–sobolev variational auto-encoders. <em>NN</em>,
<em>141</em>, 294–305. (<a
href="https://doi.org/10.1016/j.neunet.2021.04.018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The quality of generative models (such as Generative adversarial networks and Variational Auto-Encoders) depends heavily on the choice of a good probability distance. However some popular metrics like the Wasserstein or the Sliced Wasserstein distances, the Jensen–Shannon divergence, the Kullback–Leibler divergence, lack convenient properties such as (geodesic) convexity, fast evaluation and so on. To address these shortcomings, we introduce a class of distances that have built-in convexity. We investigate the relationship with some known paradigms (sliced distances – a synonym for Radon distances – reproducing kernel Hilbert spaces , energy distances). The distances are shown to possess fast implementations and are included in an adapted Variational Auto-Encoder termed Radon–Sobolev Variational Auto-Encoder (RS-VAE) which produces high quality results on standard generative datasets.},
  archive      = {J_NN},
  author       = {Gabriel Turinici},
  doi          = {10.1016/j.neunet.2021.04.018},
  journal      = {Neural Networks},
  pages        = {294-305},
  shortjournal = {Neural Netw.},
  title        = {Radon–Sobolev variational auto-encoders},
  volume       = {141},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). H∞ estimation for stochastic semi-markovian switching CVNNs
with missing measurements and mode-dependent delays. <em>NN</em>,
<em>141</em>, 281–293. (<a
href="https://doi.org/10.1016/j.neunet.2021.04.022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article is devoted to the H ∞ estimation problem for stochastic semi-Markovian switching complex-valued neural networks subject to incomplete measurement outputs, where the time-varying delay also depends on another semi-Markov process. A sequence of random variables with known statistical property is introduced to depict the missing measurement phenomenon. Based on the generalized It o ˆ ’s formula in complex form concerning with the semi-Markovian systems, complex-valued reciprocal convex inequality as well as intensive stochastic analysis method, some mode-dependent sufficient conditions are presented guaranteeing the estimation error system to be exponentially mean-square stable with a prespecified H ∞ disturbance attenuation level. In addition, the mode-dependent estimator gain matrices are appropriately designed according to the feasible solutions of certain complex matrix inequalities. In the end, one numerical example is provided to illustrate effectiveness of the theoretical results.},
  archive      = {J_NN},
  author       = {Qiang Li and Jinling Liang and Hong Qu},
  doi          = {10.1016/j.neunet.2021.04.022},
  journal      = {Neural Networks},
  pages        = {281-293},
  shortjournal = {Neural Netw.},
  title        = {H∞ estimation for stochastic semi-markovian switching CVNNs with missing measurements and mode-dependent delays},
  volume       = {141},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learnable heterogeneous convolution: Learning both topology
and strength. <em>NN</em>, <em>141</em>, 270–280. (<a
href="https://doi.org/10.1016/j.neunet.2021.03.038">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing convolution techniques in artificial neural networks suffer from huge computation complexity, while the biological neural network works in a much more powerful yet efficient way. Inspired by the biological plasticity of dendritic topology and synaptic strength , our method, Learnable Heterogeneous Convolution, realizes joint learning of kernel shape and weights, which unifies existing handcrafted convolution techniques in a data-driven way. A model based on our method can converge with structural sparse weights and then be accelerated by devices of high parallelism . In the experiments, our method either reduces VGG16/19 and ResNet34/50 computation by nearly 5 × 5× on CIFAR10 and 2 × 2× on ImageNet without harming the performance, where the weights are compressed by 10 × 10× and 4 × 4× respectively; or improves the accuracy by up to 1.0\% on CIFAR10 and 0.5\% on ImageNet with slightly higher efficiency. The code will be available on www.github.com/Genera1Z/LearnableHeterogeneousConvolution .},
  archive      = {J_NN},
  author       = {Rongzhen Zhao and Zhenzhi Wu and Qikun Zhang},
  doi          = {10.1016/j.neunet.2021.03.038},
  journal      = {Neural Networks},
  pages        = {270-280},
  shortjournal = {Neural Netw.},
  title        = {Learnable heterogeneous convolution: Learning both topology and strength},
  volume       = {141},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Saturated impulsive control for synchronization of coupled
delayed neural networks. <em>NN</em>, <em>141</em>, 261–269. (<a
href="https://doi.org/10.1016/j.neunet.2021.04.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper focuses on the synchronization problem for a class of coupled neural networks with impulsive control, where the saturation structure of impulse action is fully considered. The coupled neural networks under consideration are subject to mixed delays including transmission delay and coupled delay. The sector condition in virtue of a new constraint of set inclusion is given for a addressed network, based on which a sufficient condition for exponential synchronization problem is obtained by replacing saturation nonlinearity with a dead-zone function. In the framework of saturated impulses, our results relying on the domain of attraction can still achieve the synchronization of coupled delayed neural networks. In addition, the estimating domain of attraction is proposed as large as possible by solving an optimization problem . Finally, a numerical simulation example is presented to demonstrate the effectiveness of the proposed results.},
  archive      = {J_NN},
  author       = {Shuchen Wu and Xiaodi Li and Yanhui Ding},
  doi          = {10.1016/j.neunet.2021.04.012},
  journal      = {Neural Networks},
  pages        = {261-269},
  shortjournal = {Neural Netw.},
  title        = {Saturated impulsive control for synchronization of coupled delayed neural networks},
  volume       = {141},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A neuralized feature engineering method for entity relation
extraction. <em>NN</em>, <em>141</em>, 249–260. (<a
href="https://doi.org/10.1016/j.neunet.2021.04.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Making full use of semantic and structure information in a sentence is critical to support entity relation extraction. Neural networks use stacked neural layers to perform designated feature transformations and can automatically extract high-order abstract feature representations from raw inputs. However, because a sentence usually contains several pairs of named entities, the networks are weak when encoding semantic and structure information of a relation instance. In this paper, we propose a neuralized feature engineering approach for entity relation extraction. This approach enhances the neural network by manually designed features, which have the advantage of using prior knowledge and experience developed in feature-based models. Neuralized feature engineering encodes manually designed features into distributed representations to increase the discriminability of a neural network. Experiments show that this approach considerably improves the performance compared to that of neural networks or feature-based models alone, exceeding state-of-the-art performance by more than 8\% and 16.5\% in terms of F1-score on the ACE corpus and the Chinese literature text corpus, respectively.},
  archive      = {J_NN},
  author       = {Yanping Chen and Weizhe Yang and Kai Wang and Yongbin Qin and Ruizhang Huang and Qinghua Zheng},
  doi          = {10.1016/j.neunet.2021.04.010},
  journal      = {Neural Networks},
  pages        = {249-260},
  shortjournal = {Neural Netw.},
  title        = {A neuralized feature engineering method for entity relation extraction},
  volume       = {141},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A dual-stream deep attractor network with multi-domain
learning for speech dereverberation and separation. <em>NN</em>,
<em>141</em>, 238–248. (<a
href="https://doi.org/10.1016/j.neunet.2021.04.023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep attractor networks (DANs) perform speech separation with discriminative embeddings and speaker attractors. Compared with methods based on the permutation invariant training (PIT), DANs define a deep embedding space and deliver a more elaborate representation on each time–frequency (T–F) bin. However, it has been observed that the DANs achieve limited improvement on the signal quality if directly deployed in a reverberant environment. Following the success of time-domain separation networks on the clean mixture speech, we propose a dual-stream DAN with multi-domain learning to efficiently perform both dereverberation and separation tasks under the condition of variable numbers of speakers. The speaker encoding stream (SES) of the dual-stream DAN is trained to model the speaker information in the embedding space defined with the Fourier transform kernels. The speech decoding stream (SDS) accepts speaker attractors from the SES and learns to estimate the early component of the sound in the time domain. Meanwhile, additional clustering losses are used to bridge the gap between the oracle and the estimated attractors. Experiments were conducted on the Spatialized Multi-Speaker Wall Street Journal (SMS-WSJ) dataset. After comparing with the anechoic and reverberant signals, the early component was chosen as the learning targets. The experimental results demonstrated that the dual-stream DAN achieved scale-invariant source-to-distortion ratio (SI-SDR) improvement of 9 . 8 ∕ 7 . 5 9.8∕7.5 dB on the reverberant 2-/3-speaker evaluation set, exceeding the baseline DAN and convolutional time-domain audio separation network (Conv-TasNet) by 2 . 0 ∕ 0 . 7 2.0∕0.7 dB and 1 . 0 ∕ 0 . 5 1.0∕0.5 dB, respectively.},
  archive      = {J_NN},
  author       = {Hangting Chen and Pengyuan Zhang},
  doi          = {10.1016/j.neunet.2021.04.023},
  journal      = {Neural Networks},
  pages        = {238-248},
  shortjournal = {Neural Netw.},
  title        = {A dual-stream deep attractor network with multi-domain learning for speech dereverberation and separation},
  volume       = {141},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep neural network-based generalized sidelobe canceller for
dual-channel far-field speech recognition. <em>NN</em>, <em>141</em>,
225–237. (<a
href="https://doi.org/10.1016/j.neunet.2021.04.017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The traditional generalized sidelobe canceller (GSC) is a common speech enhancement front end to improve the noise robustness of automatic speech recognition (ASR) systems in the far-field cases. However, the traditional GSC is optimized based on the signal level criteria, causing it not to guarantee the optimal ASR performance. To address this issue, we propose a novel dual-channel deep neural network (DNN)-based GSC structure, called nnGSC, which is optimized by using the objective of maximizing the ASR performance. Our key idea is to make each module of the traditional GSC fully learnable and use the acoustic model to perform joint optimization with GSC. We use the coefficients of the traditional GSC to initialize nnGSC, so that both traditional signal processing knowledge and large amounts of data can be used to guide the network learning. In addition, nnGSC can automatically track the target direction-of-arrival (DOA) frame-by-frame without the need for additional localization algorithms. In the experiments, nnGSC achieves a relative character error rate (CER) improvement of 23.7\% compared to the microphone observation, 13.5\% compared to the oracle direction-based super-directive beamformer , 12.2\% compared to the oracle direction-based traditional GSC and 5.9\% compared to the oracle mask-based minimum variance distortionless response (MVDR) beamformer . Moreover, we can improve the robustness of nnGSC against array geometry mismatches by training with multi-geometry data.},
  archive      = {J_NN},
  author       = {Guanjun Li and Shan Liang and Shuai Nie and Wenju Liu and Zhanlei Yang},
  doi          = {10.1016/j.neunet.2021.04.017},
  journal      = {Neural Networks},
  pages        = {225-237},
  shortjournal = {Neural Netw.},
  title        = {Deep neural network-based generalized sidelobe canceller for dual-channel far-field speech recognition},
  volume       = {141},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep learning architectures for estimating breathing signal
and respiratory parameters from speech recordings. <em>NN</em>,
<em>141</em>, 211–224. (<a
href="https://doi.org/10.1016/j.neunet.2021.03.029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Respiration is an essential and primary mechanism for speech production. We first inhale and then produce speech while exhaling. When we run out of breath, we stop speaking and inhale. Though this process is involuntary, speech production involves a systematic outflow of air during exhalation characterized by linguistic content and prosodic factors of the utterance. Thus speech and respiration are closely related, and modeling this relationship makes sensing respiratory dynamics directly from the speech plausible, however is not well explored. In this article, we conduct a comprehensive study to explore techniques for sensing breathing signal and breathing parameters from speech using deep learning architectures and address the challenges involved in establishing the practical purpose of this technology. Estimating the breathing pattern from the speech would give us information about the respiratory parameters, thus enabling us to understand the respiratory health using one’s speech.},
  archive      = {J_NN},
  author       = {Venkata Srikanth Nallanthighal and Zohreh Mostaani and Aki Härmä and Helmer Strik and Mathew Magimai-Doss},
  doi          = {10.1016/j.neunet.2021.03.029},
  journal      = {Neural Networks},
  pages        = {211-224},
  shortjournal = {Neural Netw.},
  title        = {Deep learning architectures for estimating breathing signal and respiratory parameters from speech recordings},
  volume       = {141},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Modeling the grid cell activity on non-horizontal surfaces
based on oscillatory interference modulated by gravity. <em>NN</em>,
<em>141</em>, 199–210. (<a
href="https://doi.org/10.1016/j.neunet.2021.04.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Internal representation of the space is a fundamental and crucial function of the animal’s brain. Grid cells in the medial entorhinal cortex are thought to provide an environment-invariant metric system for the navigation of the animal. Most experimental and theoretical studies have focused on the horizontal planar codes of grid cell, while how this metric coordinate system is configured in the actual three-dimensional space remains unclear. Evidence has implied the spatial cognition may not be fully volumetric . We proposed an oscillatory interference model with a novel gravity and body plane modulation to simulate grid cell activity in complex space for rodents. The animal can perceive the rotation of its body plane along the local surface by sensing the gravity, causing the modulation to the dendritic oscillations. The results not only reproduce the firing patterns of the grid cell recorded from known experiments, but also predict the grid codes in novel environments. It further demonstrates that the gravity signal is indispensable for the animal’s navigation, and supports the hypothesis that the periodic firing of the grid cell is intrinsically not a volumetric code in three-dimensional space. This will provide new insights to understand the spatial representation of the actual world in the brain.},
  archive      = {J_NN},
  author       = {Yihong Wang and Xuying Xu and Rubin Wang},
  doi          = {10.1016/j.neunet.2021.04.009},
  journal      = {Neural Networks},
  pages        = {199-210},
  shortjournal = {Neural Netw.},
  title        = {Modeling the grid cell activity on non-horizontal surfaces based on oscillatory interference modulated by gravity},
  volume       = {141},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Emulation of wildland fire spread simulation using deep
learning. <em>NN</em>, <em>141</em>, 184–198. (<a
href="https://doi.org/10.1016/j.neunet.2021.04.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Numerical simulation of wildland fire spread is useful to predict the locations that are likely to burn and to support decision in an operational context , notably for crisis situations and long-term planning. For short-term, the computational time of traditional simulators is too high to be tractable over large zones like a country or part of a country, especially for fire danger mapping. This issue is tackled by emulating the area of the burned surface returned after simulation of a fire igniting anywhere in Corsica island and spreading freely during one hour, with a wide range of possible environmental input conditions. A deep neural network with a hybrid architecture is used to account for two types of inputs: the spatial fields describing the surrounding landscape and the remaining scalar inputs. After training on a large simulation dataset, the network shows a satisfactory approximation error on a complementary test dataset with a MAPE of 32.8\%. The convolutional part is pre-computed and the emulator is defined as the remaining part of the network, saving significant computational time. On a 32-core machine, the emulator has a speed-up factor of several thousands compared to the simulator and the overall relationship between its inputs and output is consistent with the expected physical behavior of fire spread. This reduction in computational time allows the computation of one-hour burned area map for the whole island of Corsica in less than a minute, opening new application in short-term fire danger mapping.},
  archive      = {J_NN},
  author       = {Frédéric Allaire and Vivien Mallet and Jean-Baptiste Filippi},
  doi          = {10.1016/j.neunet.2021.04.006},
  journal      = {Neural Networks},
  pages        = {184-198},
  shortjournal = {Neural Netw.},
  title        = {Emulation of wildland fire spread simulation using deep learning},
  volume       = {141},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Pinning bipartite synchronization for coupled
reaction–diffusion neural networks with antagonistic interactions and
switching topologies. <em>NN</em>, <em>141</em>, 174–183. (<a
href="https://doi.org/10.1016/j.neunet.2021.04.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, the bipartite synchronization issue for a class of coupled reaction–diffusion networks with antagonistic interactions and switching topologies is investigated. First of all, by virtue of Lyapunov functional method and pinning control technique, we obtain some sufficient conditions which can guarantee that networks with signed graph topologies realize bipartite synchronization under any initial conditions and arbitrary switching signals. Secondly, for the general switching signal and periodic switching signal, a pinning controller that can ensure bipartite synchronization of reaction–diffusions networks is designed based on the obtained conditions. Meanwhile, a directed relationship between coupling strength and control gains is presented. Thirdly, numerical simulation is provided to demonstrate the correctness and validity of the derived theoretical results for reaction–diffusion systems. We briefly conclude our findings and future work.},
  archive      = {J_NN},
  author       = {Baojun Miao and Xuechen Li and Jungang Lou and Jianquan Lu},
  doi          = {10.1016/j.neunet.2021.04.007},
  journal      = {Neural Networks},
  pages        = {174-183},
  shortjournal = {Neural Netw.},
  title        = {Pinning bipartite synchronization for coupled reaction–diffusion neural networks with antagonistic interactions and switching topologies},
  volume       = {141},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Neural network approximation: Three hidden layers are
enough. <em>NN</em>, <em>141</em>, 160–173. (<a
href="https://doi.org/10.1016/j.neunet.2021.04.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A three-hidden-layer neural network with super approximation power is introduced. This network is built with the floor function ( ⌊x⌋ ), the exponential function ( 2x ), the step function ( 1x≥0 ), or their compositions as the activation function in each neuron and hence we call such networks as Floor-Exponential-Step (FLES) networks. For any width hyper-parameter N∈N+ , it is shown that FLES networks with width max{d,N} and three hidden layers can uniformly approximate a Hölder continuous function f on [0,1]d with an exponential approximation rate 3λ(2d)α2−αN , where α∈(0,1] and λ&amp;gt;0 are the Hölder order and constant, respectively. More generally for an arbitrary continuous function f on [0,1]d with a modulus of continuity ωf(⋅) , the constructive approximation rate is 2ωf(2d)2−N+ωf(2d2−N) . Moreover, we extend such a result to general bounded continuous functions on a bounded set E⊆Rd . As a consequence, this new class of networks overcomes the curse of dimensionality in approximation power when the variation of ωf(r) as r→0 is moderate (e.g., ωf(r)≲rα for Hölder continuous functions), since the major term to be concerned in our approximation rate is essentially d times a function of N independent of d within the modulus of continuity. Finally, we extend our analysis to derive similar approximation results in the Lp -norm for p∈[1,∞) via replacing Floor-Exponential-Step activation functions by continuous activation functions.},
  archive      = {J_NN},
  author       = {Zuowei Shen and Haizhao Yang and Shijun Zhang},
  doi          = {10.1016/j.neunet.2021.04.011},
  journal      = {Neural Networks},
  pages        = {160-173},
  shortjournal = {Neural Netw.},
  title        = {Neural network approximation: Three hidden layers are enough},
  volume       = {141},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Unsupervised foveal vision neural architecture with top-down
attention. <em>NN</em>, <em>141</em>, 145–159. (<a
href="https://doi.org/10.1016/j.neunet.2021.03.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning architectures are an extremely powerful tool for recognizing and classifying images. However, they require supervised learning and normally work on vectors of the size of image pixels and produce the best results when trained on millions of object images. To help mitigate these issues, we propose an end-to-end architecture that fuses bottom-up saliency and top-down attention with an object recognition module to focus on relevant data and learn important features that can later be fine-tuned for a specific task, employing only unsupervised learning . In addition, by utilizing a virtual fovea that focuses on relevant portions of the data, the training speed can be greatly improved. We test the performance of the proposed Gamma saliency technique on the Toronto and CAT 2000 databases, and the foveated vision in the large Street View House Numbers (SVHN) database. The results with foveated vision show that Gamma saliency performs at the same level as the best alternative algorithms while being computationally faster. The results in SVHN show that our unsupervised cognitive architecture is comparable to fully supervised methods and that saliency also improves CNN performance if desired. Finally, we develop and test a top-down attention mechanism based on the Gamma saliency applied to the top layer of CNNs to facilitate scene understanding in multi-object cluttered images. We show that the extra information from top-down saliency is capable of speeding up the extraction of digits in the cluttered multidigit MNIST data set, corroborating the important role of top down attention .},
  archive      = {J_NN},
  author       = {Ryan Burt and Nina N. Thigpen and Andreas Keil and Jose C. Principe},
  doi          = {10.1016/j.neunet.2021.03.003},
  journal      = {Neural Networks},
  pages        = {145-159},
  shortjournal = {Neural Netw.},
  title        = {Unsupervised foveal vision neural architecture with top-down attention},
  volume       = {141},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Residual wide-kernel deep convolutional auto-encoder for
intelligent rotating machinery fault diagnosis with limited samples.
<em>NN</em>, <em>141</em>, 133–144. (<a
href="https://doi.org/10.1016/j.neunet.2021.04.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper deals with the development of a novel deep learning framework to achieve highly accurate rotating machinery fault diagnosis using residual wide-kernel deep convolutional auto-encoder. Unlike most existing methods, in which the input data is processed by fast Fourier transform (FFT) and wavelet transform , this paper aims to learn important features from limited raw vibration signals . Firstly, the wide-kernel convolutional layer is introduced in the convolutional auto-encoder that can ensure the model can learn effective features from the data without any signal processing. Secondly, the residual learning block is introduced in convolutional auto-encoder that can ensure the model with sufficient depth without gradient vanishing and overfitting problems. Thirdly, convolutional auto-encoder can learn constructive features without massive data. To evaluate the performance of the proposed model, Case Western Reserve University (CWRU) bearing dataset and Southeast University (SEU) gearbox dataset are used to test. The experiment results and comparisons verify the denoising and feature extraction ability of the proposed model in the case of very few training samples.},
  archive      = {J_NN},
  author       = {Daoguang Yang and Hamid Reza Karimi and Kangkang Sun},
  doi          = {10.1016/j.neunet.2021.04.003},
  journal      = {Neural Networks},
  pages        = {133-144},
  shortjournal = {Neural Netw.},
  title        = {Residual wide-kernel deep convolutional auto-encoder for intelligent rotating machinery fault diagnosis with limited samples},
  volume       = {141},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Implicit adversarial data augmentation and robustness with
noise-based learning. <em>NN</em>, <em>141</em>, 120–132. (<a
href="https://doi.org/10.1016/j.neunet.2021.04.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a Noise-based Learning (NoL) approach for training neural networks that are intrinsically robust to adversarial attacks . We find that the learning of random noise introduced with the input with the same loss function used during posterior maximization, improves a model’s adversarial resistance. We show that the learnt noise performs implicit adversarial data augmentation boosting a model’s adversary generalization capability. We evaluate our approach’s efficacy and provide a simplistic visualization tool for understanding adversarial data, using Principal Component Analysis . We conduct comprehensive experiments on prevailing benchmarks such as MNIST, CIFAR10, CIFAR100, Tiny ImageNet and show that our approach performs remarkably well against a wide range of attacks. Furthermore, combining NoL with state-of-the-art defense mechanisms, such as adversarial training, consistently outperforms prior techniques in both white-box and black-box attacks.},
  archive      = {J_NN},
  author       = {Priyadarshini Panda and Kaushik Roy},
  doi          = {10.1016/j.neunet.2021.04.008},
  journal      = {Neural Networks},
  pages        = {120-132},
  shortjournal = {Neural Netw.},
  title        = {Implicit adversarial data augmentation and robustness with noise-based learning},
  volume       = {141},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-periodicity of switched neural networks with time
delays and periodic external inputs under stochastic disturbances.
<em>NN</em>, <em>141</em>, 107–119. (<a
href="https://doi.org/10.1016/j.neunet.2021.03.039">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents new theoretical results on the multi-periodicity of recurrent neural networks with time delays evoked by periodic inputs under stochastic disturbances and state-dependent switching. Based on the geometric properties of activation function and switching threshold, the neuronal state space is partitioned into 5 n 5n regions in which 3 n 3n ones are shown to be positively invariant with probability one. Furthermore, by using Itô’s formula, Lyapunov functional method, and the contraction mapping theorem, two criteria are proposed to ascertain the existence and mean-square exponential stability of a periodic orbit in every positive invariant set . As a result, the number of mean-square exponentially stable periodic orbits increases to 3 n 3n from 2 n 2n in a neural network without switching. Two illustrative examples are elaborated to substantiate the efficacy and characteristics of the theoretical results.},
  archive      = {J_NN},
  author       = {Zhenyuan Guo and Jingxuan Ci and Jun Wang},
  doi          = {10.1016/j.neunet.2021.03.039},
  journal      = {Neural Networks},
  pages        = {107-119},
  shortjournal = {Neural Netw.},
  title        = {Multi-periodicity of switched neural networks with time delays and periodic external inputs under stochastic disturbances},
  volume       = {141},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Exponential quasi-synchronization of coupled delayed
memristive neural networks via intermittent event-triggered control.
<em>NN</em>, <em>141</em>, 98–106. (<a
href="https://doi.org/10.1016/j.neunet.2021.01.013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Firstly, an intermittent event-triggered control (IETC), as a combination of intermittent control and event-triggered control, is proposed. Then, the quasi-synchronization problem of coupled memristive neural networks with time-varying delays (CDMNN) is discussed under this IETC. To include more of the existing work, aperiodic intermittent control and event-triggered control with combined measurement errors are adopted in the IETC. Under the IETC, it is shown that Zeno behavior cannot be exhibited for CDMNN. At the same time, two new differential inequalities are established, and some simple and practical criteria for CDMNN quasi-synchronization and synchronization are obtained by using these inequalities. In the obtained results, synchronization is a spatial case of quasi-synchronization, and the activation functions of DMNN do not need to be bounded. Finally, a numerical example and some simulations are provided to test the results in theoretical analysis.},
  archive      = {J_NN},
  author       = {Jiejie Chen and Boshan Chen and Zhigang Zeng},
  doi          = {10.1016/j.neunet.2021.01.013},
  journal      = {Neural Networks},
  pages        = {98-106},
  shortjournal = {Neural Netw.},
  title        = {Exponential quasi-synchronization of coupled delayed memristive neural networks via intermittent event-triggered control},
  volume       = {141},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Real-time face &amp; eye tracking and blink detection using
event cameras. <em>NN</em>, <em>141</em>, 87–97. (<a
href="https://doi.org/10.1016/j.neunet.2021.03.019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Event cameras contain emerging, neuromorphic vision sensors that capture local-light​ intensity changes at each pixel, generating a stream of asynchronous events. This way of acquiring visual information constitutes a departure from traditional frame-based cameras and offers several significant advantages — low energy consumption , high temporal resolution, high dynamic range and low latency. Driver monitoring systems (DMS) are in-cabin safety systems designed to sense and understand a drivers physical and cognitive state. Event cameras are particularly suited to DMS due to their inherent advantages. This paper proposes a novel method to simultaneously detect and track faces and eyes for driver monitoring. A unique, fully convolutional recurrent neural network architecture is presented. To train this network, a synthetic event-based dataset is simulated with accurate bounding box annotations, called Neuromorphic-HELEN. Additionally, a method to detect and analyse drivers’ eye blinks is proposed, exploiting the high temporal resolution of event cameras. Behaviour of blinking provides greater insights into a driver level of fatigue or drowsiness. We show that blinks have a unique temporal signature that can be better captured by event cameras.},
  archive      = {J_NN},
  author       = {Cian Ryan and Brian O’Sullivan and Amr Elrasad and Aisling Cahill and Joe Lemley and Paul Kielty and Christoph Posch and Etienne Perot},
  doi          = {10.1016/j.neunet.2021.03.019},
  journal      = {Neural Networks},
  pages        = {87-97},
  shortjournal = {Neural Netw.},
  title        = {Real-time face &amp; eye tracking and blink detection using event cameras},
  volume       = {141},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Deep joint learning for language recognition. <em>NN</em>,
<em>141</em>, 72–86. (<a
href="https://doi.org/10.1016/j.neunet.2021.03.026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning methods for language recognition have achieved promising performance. However, most of the studies focus on frameworks for single types of acoustic features and single tasks. In this paper, we propose the deep joint learning strategies based on the Multi-Feature (MF) and Multi-Task (MT) models. First, we investigate the efficiency of integrating multiple acoustic features and explore two kinds of training constraints, one is introducing auxiliary classification constraints with adaptive weights for loss functions in feature encoder sub-networks, and the other option is introducing the Canonical Correlation Analysis (CCA) constraint to maximize the correlation of different feature representations. Correlated speech tasks, such as phoneme recognition, are applied as auxiliary tasks in order to learn related information to enhance the performance of language recognition. We analyze phoneme-aware information from different learning strategies, like joint learning on the frame-level, adversarial learning on the segment-level, and the combination mode. In addition, we present the Language-Phoneme embedding extraction structure to learn and extract language and phoneme embedding representations simultaneously. We demonstrate the effectiveness of the proposed approaches with experiments on the Oriental Language Recognition (OLR) data sets. Experimental results indicate that joint learning on the multi-feature and multi-task models extracts instinct feature representations for language identities and improves the performance, especially in complex challenges, such as cross-channel or open-set conditions.},
  archive      = {J_NN},
  author       = {Lin Li and Zheng Li and Yan Liu and Qingyang Hong},
  doi          = {10.1016/j.neunet.2021.03.026},
  journal      = {Neural Networks},
  pages        = {72-86},
  shortjournal = {Neural Netw.},
  title        = {Deep joint learning for language recognition},
  volume       = {141},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Reducing bias to source samples for unsupervised domain
adaptation. <em>NN</em>, <em>141</em>, 61–71. (<a
href="https://doi.org/10.1016/j.neunet.2021.03.021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised Domain Adaptation (UDA) makes predictions for the target domain data while labels are only available in the source domain. Lots of works in UDA focus on finding a common representation of the two domains via domain alignment, assuming that a classifier trained in the source domain can be generalized well to the target domain. Thus, most existing UDA methods only consider minimizing the domain discrepancy without enforcing any constraint on the classifier. However, due to the uniqueness of each domain, it is difficult to achieve a perfect common representation, especially when there is low similarity between the source domain and the target domain. As a consequence, the classifier is biased to the source domain features and makes incorrect predictions on the target domain. To address this issue, we propose a novel approach named reducing bias to source samples for unsupervised domain adaptation (RBDA) by jointly matching the distribution of the two domains and reducing the classifier’s bias to source samples. Specifically, RBDA first conditions the adversarial networks with the cross-covariance of learned features and classifier predictions to match the distribution of two domains. Then to reduce the classifier’s bias to source samples, RBDA is designed with three effective mechanisms: a mean teacher model to guide the training of the original model, a regularization term to regularize the model and an improved cross-entropy loss for better supervised information learning. Comprehensive experiments on several open benchmarks demonstrate that RBDA achieves state-of-the-art results, which show its effectiveness for unsupervised domain adaptation scenarios.},
  archive      = {J_NN},
  author       = {Yalan Ye and Ziwei Huang and Tongjie Pan and Jingjing Li and Heng Tao Shen},
  doi          = {10.1016/j.neunet.2021.03.021},
  journal      = {Neural Networks},
  pages        = {61-71},
  shortjournal = {Neural Netw.},
  title        = {Reducing bias to source samples for unsupervised domain adaptation},
  volume       = {141},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Combining a parallel 2D CNN with a self-attention dilated
residual network for CTC-based discrete speech emotion recognition.
<em>NN</em>, <em>141</em>, 52–60. (<a
href="https://doi.org/10.1016/j.neunet.2021.03.013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A challenging issue in the field of the automatic recognition of emotion from speech is the efficient modelling of long temporal contexts. Moreover, when incorporating long-term temporal dependencies between features, recurrent neural network (RNN) architectures are typically employed by default. In this work, we aim to present an efficient deep neural network architecture incorporating Connectionist Temporal Classification (CTC) loss for discrete speech emotion recognition (SER). Moreover, we also demonstrate the existence of further opportunities to improve SER performance by exploiting the properties of convolutional neural networks (CNNs) when modelling contextual information. Our proposed model uses parallel convolutional layers (PCN) integrated with Squeeze-and-Excitation Network (SEnet), a system herein denoted as PCNSE, to extract relationships from 3D spectrograms across timesteps and frequencies; here, we use the log-Mel spectrogram with deltas and delta–deltas as input. In addition, a self-attention Residual Dilated Network (SADRN) with CTC is employed as a classification block for SER. To the best of the authors’ knowledge, this is the first time that such a hybrid architecture has been employed for discrete SER. We further demonstrate the effectiveness of our proposed approach on the Interactive Emotional Dyadic Motion Capture (IEMOCAP) and FAU-Aibo Emotion corpus (FAU-AEC). Our experimental results reveal that the proposed method is well-suited to the task of discrete SER, achieving a weighted accuracy (WA) of 73.1\% and an unweighted accuracy (UA) of 66.3\% on IEMOCAP, as well as a UA of 41.1\% on the FAU-AEC dataset.},
  archive      = {J_NN},
  author       = {Ziping Zhao and Qifei Li and Zixing Zhang and Nicholas Cummins and Haishuai Wang and Jianhua Tao and Björn W. Schuller},
  doi          = {10.1016/j.neunet.2021.03.013},
  journal      = {Neural Networks},
  pages        = {52-60},
  shortjournal = {Neural Netw.},
  title        = {Combining a parallel 2D CNN with a self-attention dilated residual network for CTC-based discrete speech emotion recognition},
  volume       = {141},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Synchronization for stochastic coupled networks with lévy
noise via event-triggered control. <em>NN</em>, <em>141</em>, 40–51. (<a
href="https://doi.org/10.1016/j.neunet.2021.03.028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the realization of almost sure synchronization problem for a new array of stochastic networks associated with delay and Lévy noise via event-triggered control. The coupling structure of the network is governed by a continuous-time homogeneous Markov chain . The nodes in the networks communicate with each other and update their information only at discrete-time instants so that the network workload can be minimized. Under the framework of stochastic process including Markov chain and Lévy process , and the convergence theorem of non-negative semi-martingales, we show that the Markovian coupled networks can achieve the almost sure synchronization by event-triggered control methodology. The results are further extended to the directed topology, where the coupling structure can be asymmetric. Furthermore, we also proved that the Zeno behavior can be excluded under our proposed approach, indicating that our framework is practically feasible. Numerical simulations are provided to demonstrate the effectiveness of the obtained theoretical results.},
  archive      = {J_NN},
  author       = {Hailing Dong and Ming Luo and Mingqing Xiao},
  doi          = {10.1016/j.neunet.2021.03.028},
  journal      = {Neural Networks},
  pages        = {40-51},
  shortjournal = {Neural Netw.},
  title        = {Synchronization for stochastic coupled networks with lévy noise via event-triggered control},
  volume       = {141},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Noise effect on the temporal patterns of neural synchrony.
<em>NN</em>, <em>141</em>, 30–39. (<a
href="https://doi.org/10.1016/j.neunet.2021.03.032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural synchrony in the brain is often present in an intermittent fashion, i.e., there are intervals of synchronized activity interspersed with intervals of desynchronized activity. A series of experimental studies showed that this kind of temporal patterning of neural synchronization may be very specific and may be correlated with behaviour (even if the average synchrony strength is not changed). Prior studies showed that a network with many short desynchronized intervals may be functionally different from a network with few long desynchronized intervals as it may be more sensitive to synchronizing input signals. In this study, we investigated the effect of channel noise on the temporal patterns of neural synchronization. We employed a small network of conductance-based model neurons that were mutually connected via excitatory synapses. The resulting dynamics of the network was studied using the same time-series analysis methods as used in prior experimental and computational studies. While it is well known that synchrony strength generally degrades with noise, we found that noise also affects the temporal patterning of synchrony. Noise, at a sufficient intensity (yet too weak to substantially affect synchrony strength), promotes dynamics with predominantly short (although potentially very numerous) desynchronizations. Thus, channel noise may be one of the mechanisms contributing to the short desynchronization dynamics observed in multiple experimental studies.},
  archive      = {J_NN},
  author       = {Joel Zirkle and Leonid L. Rubchinsky},
  doi          = {10.1016/j.neunet.2021.03.032},
  journal      = {Neural Networks},
  pages        = {30-39},
  shortjournal = {Neural Netw.},
  title        = {Noise effect on the temporal patterns of neural synchrony},
  volume       = {141},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Automatic, dynamic, and nearly optimal learning rate
specification via local quadratic approximation. <em>NN</em>,
<em>141</em>, 11–29. (<a
href="https://doi.org/10.1016/j.neunet.2021.03.025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In deep learning tasks, the update step size determined by the learning rate at each iteration plays a critical role in gradient-based optimization. However, determining the appropriate learning rate in practice typically relies on subjective judgment. In this work, we propose a novel optimization method based on local quadratic approximation (LQA). In each update step, we locally approximate the loss function along the gradient direction by using a standard quadratic function of the learning rate. Subsequently, we propose an approximation step to obtain a nearly optimal learning rate in a computationally efficient manner. The proposed LQA method has three important features. First, the learning rate is automatically determined in each update step. Second, it is dynamically adjusted according to the current loss function value and parameter estimates. Third, with the gradient direction fixed, the proposed method attains a nearly maximum reduction in the loss function. Extensive experiments were conducted to prove the effectiveness of the proposed LQA method.},
  archive      = {J_NN},
  author       = {Yingqiu Zhu and Danyang Huang and Yuan Gao and Rui Wu and Yu Chen and Bo Zhang and Hansheng Wang},
  doi          = {10.1016/j.neunet.2021.03.025},
  journal      = {Neural Networks},
  pages        = {11-29},
  shortjournal = {Neural Netw.},
  title        = {Automatic, dynamic, and nearly optimal learning rate specification via local quadratic approximation},
  volume       = {141},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep ANC: A deep learning approach to active noise control.
<em>NN</em>, <em>141</em>, 1–10. (<a
href="https://doi.org/10.1016/j.neunet.2021.03.037">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional active noise control (ANC) methods are based on adaptive signal processing with the least mean square algorithm as the foundation. They are linear systems and do not perform satisfactorily in the presence of nonlinear distortions . In this paper, we formulate ANC as a supervised learning problem and propose a deep learning approach, called deep ANC, to address the nonlinear ANC problem. The main idea is to employ deep learning to encode the optimal control parameters corresponding to different noises and environments. A convolutional recurrent network (CRN) is trained to estimate the real and imaginary spectrograms of the canceling signal from the reference signal so that the corresponding anti-noise can eliminate or attenuate the primary noise in the ANC system. Large-scale multi-condition training is employed to achieve good generalization and robustness against a variety of noises. The deep ANC method can be trained to achieve active noise cancellation no matter whether the reference signal is noise or noisy speech. In addition, a delay-compensated strategy is introduced to solve the potential latency problem of ANC systems. Experimental results show that deep ANC is effective for wideband noise reduction and generalizes well to untrained noises. Moreover, the proposed method can achieve ANC within a quiet zone and is robust against variations in reference signals.},
  archive      = {J_NN},
  author       = {Hao Zhang and DeLiang Wang},
  doi          = {10.1016/j.neunet.2021.03.037},
  journal      = {Neural Networks},
  pages        = {1-10},
  shortjournal = {Neural Netw.},
  title        = {Deep ANC: A deep learning approach to active noise control},
  volume       = {141},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021e). INN/ENNS/JNNS - membership applic. form. <em>NN</em>,
<em>140</em>, II. (<a
href="https://doi.org/10.1016/S0893-6080(21)00187-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NN},
  doi          = {10.1016/S0893-6080(21)00187-8},
  journal      = {Neural Networks},
  pages        = {II},
  shortjournal = {Neural Netw.},
  title        = {INN/ENNS/JNNS - membership applic. form},
  volume       = {140},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021e). Current events. <em>NN</em>, <em>140</em>, I. (<a
href="https://doi.org/10.1016/S0893-6080(21)00186-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NN},
  doi          = {10.1016/S0893-6080(21)00186-6},
  journal      = {Neural Networks},
  pages        = {I},
  shortjournal = {Neural Netw.},
  title        = {Current events},
  volume       = {140},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning dual-margin model for visual tracking. <em>NN</em>,
<em>140</em>, 344–354. (<a
href="https://doi.org/10.1016/j.neunet.2021.04.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing trackers usually exploit robust features or online updating mechanisms to deal with target variations which is a key challenge in visual tracking. However, the features being robust to variations remain little spatial information, and existing online updating methods are prone to overfitting. In this paper, we propose a dual-margin model for robust and accurate visual tracking. The dual-margin model comprises an intra-object margin between different target appearances and an inter-object margin between the target and the background. The proposed method is able to not only distinguish the target from the background but also perceive the target changes, which tracks target appearance changing and facilitates accurate target state estimation. In addition, to exploit rich off-line video data and learn general rules of target appearance variations, we train the dual-margin model on a large off-line video dataset. We perform tracking under a Siamese framework using the constructed appearance set as templates. The proposed method achieves accurate and robust tracking performance on five public datasets while running in real-time. The favorable performance against the state-of-the-art methods demonstrates the effectiveness of the proposed algorithm.},
  archive      = {J_NN},
  author       = {Nana Fan and Xin Li and Zikun Zhou and Qiao Liu and Zhenyu He},
  doi          = {10.1016/j.neunet.2021.04.004},
  journal      = {Neural Networks},
  pages        = {344-354},
  shortjournal = {Neural Netw.},
  title        = {Learning dual-margin model for visual tracking},
  volume       = {140},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Basic theorem and global exponential stability of
differential–algebraic neural networks with delay. <em>NN</em>,
<em>140</em>, 336–343. (<a
href="https://doi.org/10.1016/j.neunet.2021.01.017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A differential–algebraic neural network (DANN) with delay (DDANN) is proposed. Firstly, the global existence and uniqueness theorems are established for a DDANN, respectively. Next, a new differential–algebraic inequality is established. Then, a theorem on global exponential stability of DDANN is shown by using this inequality. As an application of DDANN, a very concise criterion on global exponential stability for a neutral-type neural network is given by using DDANNs. Finally, two examples are given to illustrate the theoretical results.},
  archive      = {J_NN},
  author       = {Jiejie Chen and Boshan Chen and Zhigang Zeng},
  doi          = {10.1016/j.neunet.2021.01.017},
  journal      = {Neural Networks},
  pages        = {336-343},
  shortjournal = {Neural Netw.},
  title        = {Basic theorem and global exponential stability of differential–algebraic neural networks with delay},
  volume       = {140},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multistability of delayed fractional-order competitive
neural networks. <em>NN</em>, <em>140</em>, 325–335. (<a
href="https://doi.org/10.1016/j.neunet.2021.03.036">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper is concerned with the multistability of fractional-order competitive neural networks (FCNNs) with time-varying delays. Based on the division of state space, the equilibrium points (EPs) of FCNNs are given. Several sufficient conditions and criteria are proposed to ascertain the multiple O ( t − α ) O(t−α) -stability of delayed FCNNs. The O ( t − α ) O(t−α) -stability is the extension of Mittag-Leffler stability of fractional-order neural networks , which contains monostability and multistability . Moreover, the attraction basins of the stable EPs of FCNNs are estimated, which shows the attraction basins of the stable EPs can be larger than the divided subsets. These conditions and criteria supplement and improve the previous results. Finally, the results are illustrated by the simulation examples.},
  archive      = {J_NN},
  author       = {Fanghai Zhang and Tingwen Huang and Qiujie Wu and Zhigang Zeng},
  doi          = {10.1016/j.neunet.2021.03.036},
  journal      = {Neural Networks},
  pages        = {325-335},
  shortjournal = {Neural Netw.},
  title        = {Multistability of delayed fractional-order competitive neural networks},
  volume       = {140},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A statistical framework for non-negative matrix
factorization based on generalized dual divergence. <em>NN</em>,
<em>140</em>, 309–324. (<a
href="https://doi.org/10.1016/j.neunet.2021.03.020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A statistical framework for non-negative matrix factorization based on generalized dual Kullback–Leibler divergence, which includes members of the exponential family of models, is proposed. A family of algorithms is developed using this framework, including under sparsity constraints, and its convergence proven using the Expectation–Maximization algorithm. The framework generalizes some existing methods for different noise structures and contrasts with the recently developed quasi-likelihood approach, thus providing a useful alternative for non-negative matrix factorization . A measure to evaluate the goodness-of-fit of the resulting factorization is described. The performance of the proposed methods is evaluated extensively using real life and simulated data and their utility in unsupervised and semi-supervised learning is illustrated using an application in cancer genomics. This framework can be viewed from the perspective of reinforcement learning , and can be adapted to incorporate discriminant functions and multi-layered neural networks within a deep learning paradigm.},
  archive      = {J_NN},
  author       = {Karthik Devarajan},
  doi          = {10.1016/j.neunet.2021.03.020},
  journal      = {Neural Networks},
  pages        = {309-324},
  shortjournal = {Neural Netw.},
  title        = {A statistical framework for non-negative matrix factorization based on generalized dual divergence},
  volume       = {140},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Self-organized operational neural networks with generative
neurons. <em>NN</em>, <em>140</em>, 294–308. (<a
href="https://doi.org/10.1016/j.neunet.2021.02.028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Operational Neural Networks (ONNs) have recently been proposed to address the well-known limitations and drawbacks of conventional Convolutional Neural Networks (CNNs) such as network homogeneity with the sole linear neuron model. ONNs are heterogeneous networks with a generalized neuron model. However the operator search method in ONNs is not only computationally demanding, but the network heterogeneity is also limited since the same set of operators will then be used for all neurons in each layer. Moreover, the performance of ONNs directly depends on the operator set library used, which introduces a certain risk of performance degradation especially when the optimal operator set required for a particular task is missing from the library. In order to address these issues and achieve an ultimate heterogeneity level to boost the network diversity along with computational efficiency, in this study we propose Self-organized ONNs (Self-ONNs) with generative neurons that can adapt (optimize) the nodal operator of each connection during the training process. Moreover, this ability voids the need of having a fixed operator set library and the prior operator search within the library in order to find the best possible set of operators. We further formulate the training method to back-propagate the error through the operational layers of Self-ONNs. Experimental results over four challenging problems demonstrate the superior learning capability and computational efficiency of Self-ONNs over conventional ONNs and CNNs.},
  archive      = {J_NN},
  author       = {Serkan Kiranyaz and Junaid Malik and Habib Ben Abdallah and Turker Ince and Alexandros Iosifidis and Moncef Gabbouj},
  doi          = {10.1016/j.neunet.2021.02.028},
  journal      = {Neural Networks},
  pages        = {294-308},
  shortjournal = {Neural Netw.},
  title        = {Self-organized operational neural networks with generative neurons},
  volume       = {140},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Manifold adversarial training for supervised and
semi-supervised learning. <em>NN</em>, <em>140</em>, 282–293. (<a
href="https://doi.org/10.1016/j.neunet.2021.03.031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new regularization method for deep learning based on the manifold adversarial training (MAT). Unlike previous regularization and adversarial training methods, MAT further considers the local manifold of latent representations. Specifically, MAT manages to build an adversarial framework based on how the worst perturbation could affect the statistical manifold in the latent space rather than the output space. Particularly, a latent feature space with the Gaussian Mixture Model (GMM) is first derived in a deep neural network . We then define the smoothness by the largest variation of Gaussian mixtures when a local perturbation is given around the input data point. On one hand, the perturbations are added in the way that would rough the statistical manifold of the latent space the worst. On the other hand, the model is trained to promote the manifold smoothness the most in the latent space. Importantly, since the latent space is more informative than the output space, the proposed MAT can learn a more robust and compact data representation, leading to further performance improvement. The proposed MAT is important in that it can be considered as a superset of one recently-proposed discriminative feature learning approach called center loss. We conduct a series of experiments in both supervised and semi-supervised learning on four benchmark data sets, showing that the proposed MAT can achieve remarkable performance, much better than those of the state-of-the-art approaches. In addition, we present a series of visualization which could generate further understanding or explanation on adversarial examples .},
  archive      = {J_NN},
  author       = {Shufei Zhang and Kaizhu Huang and Jianke Zhu and Yang Liu},
  doi          = {10.1016/j.neunet.2021.03.031},
  journal      = {Neural Networks},
  pages        = {282-293},
  shortjournal = {Neural Netw.},
  title        = {Manifold adversarial training for supervised and semi-supervised learning},
  volume       = {140},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). PyDiNet: Pyramid dilated network for medical image
segmentation. <em>NN</em>, <em>140</em>, 274–281. (<a
href="https://doi.org/10.1016/j.neunet.2021.03.023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical image segmentation is an important step in many generic applications such as population analysis and, more accessible, can be made into a crucial tool in diagnosis and treatment planning. Previous approaches are based on two main architectures: fully convolutional networks and U-Net-based architecture. These methods rely on multiple pooling and striding layers leading to the loss of important spatial information and fail to capture details in medical images. In this paper, we propose a novel neural network called PyDiNet (Pyramid Dilated Network) to capture small and complex variations in medical images while preserving spatial information. To achieve this goal, PyDiNet uses a newly proposed pyramid dilated module (PDM), which consists of multiple dilated convolutions stacked in parallel. We combine several PDM modules to form the final PyDiNet architecture . We applied the proposed PyDiNet to different medical image segmentation tasks. Experimental results show that the proposed model achieves new state-of-the-art performance on three medical image segmentation benchmarks. Furthermore, PyDiNet was very competitive on the 2020 Endoscopic Artifact Detection challenge.},
  archive      = {J_NN},
  author       = {Mourad Gridach},
  doi          = {10.1016/j.neunet.2021.03.023},
  journal      = {Neural Networks},
  pages        = {274-281},
  shortjournal = {Neural Netw.},
  title        = {PyDiNet: Pyramid dilated network for medical image segmentation},
  volume       = {140},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-resolution modulation-filtered cochleagram feature for
LSTM-based dimensional emotion recognition from speech. <em>NN</em>,
<em>140</em>, 261–273. (<a
href="https://doi.org/10.1016/j.neunet.2021.03.027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Continuous dimensional emotion recognition from speech helps robots or virtual agents capture the temporal dynamics of a speaker’s emotional state in natural human–robot interactions. Temporal modulation cues obtained directly from the time-domain model of auditory perception can better reflect temporal dynamics than the acoustic features usually processed in the frequency domain. Feature extraction, which can reflect temporal dynamics of emotion from temporal modulation cues, is challenging because of the complexity and diversity of the auditory perception model. A recent neuroscientific study suggests that human brains derive multi-resolution representations through temporal modulation analysis. This study investigates multi-resolution representations of an auditory perception model and proposes a novel feature called multi-resolution modulation-filtered cochleagram (MMCG) for predicting valence and arousal values of emotional primitives. The MMCG is constructed by combining four modulation-filtered cochleagrams at different resolutions to capture various temporal and contextual modulation information. In addition, to model the multi-temporal dependencies of the MMCG, we designed a parallel long short-term memory (LSTM) architecture. The results of extensive experiments on the RECOLA and SEWA datasets demonstrate that MMCG provides the best recognition performance in both datasets among all evaluated features. The results also show that the parallel LSTM can build multi-temporal dependencies from the MMCG features, and the performance on valence and arousal prediction is better than that of a plain LSTM method.},
  archive      = {J_NN},
  author       = {Zhichao Peng and Jianwu Dang and Masashi Unoki and Masato Akagi},
  doi          = {10.1016/j.neunet.2021.03.027},
  journal      = {Neural Networks},
  pages        = {261-273},
  shortjournal = {Neural Netw.},
  title        = {Multi-resolution modulation-filtered cochleagram feature for LSTM-based dimensional emotion recognition from speech},
  volume       = {140},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A clustering-based adaptive neighborhood retrieval
visualizer. <em>NN</em>, <em>140</em>, 247–260. (<a
href="https://doi.org/10.1016/j.neunet.2021.03.018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a novel adaptive version of the Neighborhood Retrieval Visualizer (NeRV). We maintain the advantages of the conventional NeRV method, while proposing an improvement of the data samples’ neighborhood width calculation, in the input and output data space. In the standard NeRV, the data samples’ neighborhood widths are determined in an arbitrary manner, in this way, inhibiting the possible quality of the resulting data visualization. We propose to compute the widths adaptively, on the basis of the input data scattering. Therefore, we first perform the preliminary input data clustering , next, we calculate the values of the inner-cluster variances, which convey the information on the input data scattering, then, we assign them to each data sample, and finally, we use them as the basis for the data samples’ neighborhood widths determination. The results of the experiments conducted on the three different real datasets confirm the effectiveness and usefulness of the proposed approach.},
  archive      = {J_NN},
  author       = {Dominik Olszewski},
  doi          = {10.1016/j.neunet.2021.03.018},
  journal      = {Neural Networks},
  pages        = {247-260},
  shortjournal = {Neural Netw.},
  title        = {A clustering-based adaptive neighborhood retrieval visualizer},
  volume       = {140},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bidirectional stochastic configuration network for
regression problems. <em>NN</em>, <em>140</em>, 237–246. (<a
href="https://doi.org/10.1016/j.neunet.2021.03.016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To adapt to the reality of limited computing resources of various terminal devices in industrial applications, a randomized neural network called stochastic configuration network (SCN), which can conduct effective training without GPU , was proposed. SCN uses a supervisory random mechanism to assign its input weights and hidden biases, which makes it more stable than other randomized algorithms but also leads to time-consuming model training. To alleviate this problem, we propose a novel bidirectional SCN algorithm (BSCN) in this paper, which divides the way of adding hidden nodes into two modes: forward learning and backward learning. In the forward learning mode, BSCN still uses the supervisory mechanism to configure the parameters of the newly added nodes, which is the same as SCN. In the backward learning mode, BSCN calculates the parameters at one time based on the residual error feedback of the current model. The two learning modes are performed iteratively until the prediction error of the model reaches an acceptable level or the number of hidden nodes reaches its maximum value. This semi-random learning mechanism greatly speeds up the training efficiency of the BSCN model and significantly improves the quality of the hidden nodes. Extensive experiments on ten benchmark regression problems , two real-life air pollution prediction problems, and a classical image processing problem show that BSCN can achieve faster training speed, higher stability, and better generalization ability than SCN.},
  archive      = {J_NN},
  author       = {Weipeng Cao and Zhongwu Xie and Jianqiang Li and Zhiwu Xu and Zhong Ming and Xizhao Wang},
  doi          = {10.1016/j.neunet.2021.03.016},
  journal      = {Neural Networks},
  pages        = {237-246},
  shortjournal = {Neural Netw.},
  title        = {Bidirectional stochastic configuration network for regression problems},
  volume       = {140},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Cycle consistent network for end-to-end style transfer TTS
training. <em>NN</em>, <em>140</em>, 223–236. (<a
href="https://doi.org/10.1016/j.neunet.2021.03.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a cycle consistent network based end-to-end TTS for speaking style transfer, including intra-speaker, inter-speaker, and unseen speaker style transfer for both parallel and unparallel transfers. The proposed approach is built upon a multi-speaker Variational Autoencoder (VAE) TTS model. The model is usually trained in a paired manner, which means the reference speech is totally paired with the output including speaker identity, text, and style. To achieve a better quality for style transfer, which for most cases is in an unpaired manner, we augment the model with an unpaired path with a separated variational style encoder. The unpaired path takes as input an unpaired reference speech and yields an unpaired output. The unpaired output, which lacks direct ground-truth target, is then successfully constrained by a delicately designed cycle consistent network. Specifically, the unpaired output of the forward transfer is fed into the model again as an unpaired reference input, and after the backward transfer yields an output expected to be the same as the original unpaired reference speech. Ablation study shows the effectiveness of the unpaired path, separated style encoders and cycle consistent network in the proposed model. The final evaluation demonstrates the proposed approach significantly outperforms the Global Style Token (GST) and VAE based systems for all the six style transfer categories, in metrics of naturalness , speech quality, similarity of speaker identity, and similarity of speaking style.},
  archive      = {J_NN},
  author       = {Liumeng Xue and Shifeng Pan and Lei He and Lei Xie and Frank K. Soong},
  doi          = {10.1016/j.neunet.2021.03.005},
  journal      = {Neural Networks},
  pages        = {223-236},
  shortjournal = {Neural Netw.},
  title        = {Cycle consistent network for end-to-end style transfer TTS training},
  volume       = {140},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Tumor attention networks: Better feature selection, better
tumor segmentation. <em>NN</em>, <em>140</em>, 203–222. (<a
href="https://doi.org/10.1016/j.neunet.2021.03.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compared with the traditional analysis of computed tomography scans, automatic liver tumor segmentation can supply precise tumor volumes and reduce the inter-observer variability in estimating the tumor size and the tumor burden, which could further assist physicians to make better therapeutic choices for hepatic diseases and monitoring treatment. Among current mainstream segmentation approaches , multi-layer and multi-kernel convolutional neural networks (CNNs) have attracted much attention in diverse biomedical/medical image segmentation tasks with remarkable performance. However, an arbitrary stacking of feature maps makes CNNs quite inconsistent in imitating the cognition and the visual attention of human beings for a specific visual task. To mitigate the lack of a reasonable feature selection mechanism in CNNs, we exploit a novel and effective network architecture , called Tumor Attention Networks ( TA-Net ), for mining adaptive features by embedding Tumor Attention layers with multi-functional modules to assist the liver tumor segmentation task . In particular, each tumor attention layer can adaptively highlight valuable tumor features and suppress unrelated ones among feature maps from 3D and 2D perspectives. Moreover, an analysis of visualization results illustrates the effectiveness of our tumor attention modules and the interpretability of CNNs for liver tumor segmentation. Furthermore, we explore different arrangements of skip connections in information fusion. A deep ablation study is also conducted to illustrate the effects of different attention strategies for hepatic tumors. The results of extensive experiments demonstrate that the proposed TA-Net increases the liver tumor segmentation performance with a lower computation cost and a small parameter overhead over the state-of-the-art methods, under various evaluation metrics on clinical benchmark data. In addition, two additional medical image datasets are used to evaluate generalization capability of TA-Net, including the comparison with general semantic segmentation methods and a non-tumor segmentation task . All the program codes have been released at https://github.com/shuchao1212/TA-Net .},
  archive      = {J_NN},
  author       = {Shuchao Pang and Anan Du and Mehmet A. Orgun and Yunyun Wang and Zhenmei Yu},
  doi          = {10.1016/j.neunet.2021.03.006},
  journal      = {Neural Networks},
  pages        = {203-222},
  shortjournal = {Neural Netw.},
  title        = {Tumor attention networks: Better feature selection, better tumor segmentation},
  volume       = {140},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Diversity-driven knowledge distillation for financial
trading using deep reinforcement learning. <em>NN</em>, <em>140</em>,
193–202. (<a
href="https://doi.org/10.1016/j.neunet.2021.02.026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep Reinforcement Learning (RL) is increasingly used for developing financial trading agents for a wide range of tasks. However, optimizing deep RL agents is notoriously difficult and unstable, especially in noisy financial environments, significantly hindering the performance of trading agents. In this work, we present a novel method that improves the training reliability of DRL trading agents building upon the well-known approach of neural network distillation. In the proposed approach, teacher agents are trained in different subsets of RL environment, thus diversifying the policies they learn. Then student agents are trained using distillation from the trained teachers to guide the training process, allowing for better exploring the solution space, while “mimicking” an existing policy/trading strategy provided by the teacher model. The boost in effectiveness of the proposed method comes from the use of diversified ensembles of teachers trained to perform trading for different currencies. This enables us to transfer the common view regarding the most profitable policy to the student, further improving the training stability in noisy financial environments. In the conducted experiments we find that when applying distillation, constraining the teacher models to be diversified can significantly improve their performance of the final student agents. We demonstrate this by providing an extensive evaluation on various financial trading tasks. Furthermore, we also provide additional experiments in the separate domain of control in games using the Procgen environments in order to demonstrate the generality of the proposed method.},
  archive      = {J_NN},
  author       = {Avraam Tsantekidis and Nikolaos Passalis and Anastasios Tefas},
  doi          = {10.1016/j.neunet.2021.02.026},
  journal      = {Neural Networks},
  pages        = {193-202},
  shortjournal = {Neural Netw.},
  title        = {Diversity-driven knowledge distillation for financial trading using deep reinforcement learning},
  volume       = {140},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dual self-paced multi-view clustering. <em>NN</em>,
<em>140</em>, 184–192. (<a
href="https://doi.org/10.1016/j.neunet.2021.02.022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {By utilizing the complementary information from multiple views, multi-view clustering (MVC) algorithms typically achieve much better clustering performance than conventional single-view methods. Although in this field, great progresses have been made in past few years, most existing multi-view clustering methods still suffer the following shortcomings: (1) most MVC methods are non-convex and thus are easily stuck into suboptimal local minima; (2) the effectiveness of these methods is sensitive to the existence of noises or outliers; and (3) the qualities of different features and views are usually ignored, which can also influence the clustering result . To address these issues, we propose dual self-paced multi-view clustering (DSMVC) in this paper. Specifically, DSMVC takes advantage of self-paced learning to tackle the non-convex issue. By applying a soft-weighting scheme of self-paced learning for instances, the negative impact caused by noises and outliers can be significantly reduced. Moreover, to alleviate the feature and view quality issues, we develop a novel feature selection approach in a self-paced manner and a weighting term for views. Experimental results on real-world data sets demonstrate the effectiveness of the proposed method.},
  archive      = {J_NN},
  author       = {Zongmo Huang and Yazhou Ren and Xiaorong Pu and Lili Pan and Dezhong Yao and Guoxian Yu},
  doi          = {10.1016/j.neunet.2021.02.022},
  journal      = {Neural Networks},
  pages        = {184-192},
  shortjournal = {Neural Netw.},
  title        = {Dual self-paced multi-view clustering},
  volume       = {140},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Parallel orthogonal deep neural network. <em>NN</em>,
<em>140</em>, 167–183. (<a
href="https://doi.org/10.1016/j.neunet.2021.03.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ensemble learning methods combine multiple models to improve performance by exploiting their diversity. The success of these approaches relies heavily on the dissimilarity of the base models forming the ensemble. This diversity can be achieved in many ways, with well-known examples including bagging and boosting. It is the diversity of the models within an ensemble that allows the ensemble to correct the errors made by its members, and consequently leads to higher classification or regression performance. A mistake made by a base model can only be rectified if other members behave differently on that particular instance, and provide the aggregator with enough information to make an informed decision. On the contrary, lack of diversity not only lowers model performance, but also wastes computational resources. Nevertheless, in the current state of the art ensemble approaches, there is no guarantee on the level of diversity achieved, and no mechanism ensuring that each member will learn a different decision boundary from the others. In this paper, we propose a parallel orthogonal deep learning architecture in which diversity is enforced by design, through imposing an orthogonality constraint. Multiple deep neural networks are created, parallel to each other. At each parallel layer, the outputs of different base models are subject to Gram–Schmidt orthogonalization . We demonstrate that this approach leads to a high level of diversity from two perspectives. First, the models make different errors on different parts of feature space, and second, they exhibit different levels of uncertainty in their decisions. Experimental results confirm the benefits of the proposed method, compared to standard deep learning models and well-known ensemble methods , in terms of diversity and, as a result, classification performance.},
  archive      = {J_NN},
  author       = {Peyman Sheikholharam Mashhadi and Sławomir Nowaczyk and Sepideh Pashami},
  doi          = {10.1016/j.neunet.2021.03.002},
  journal      = {Neural Networks},
  pages        = {167-183},
  shortjournal = {Neural Netw.},
  title        = {Parallel orthogonal deep neural network},
  volume       = {140},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Improved deep CNNs based on nonlinear hybrid attention
module for image classification. <em>NN</em>, <em>140</em>, 158–166. (<a
href="https://doi.org/10.1016/j.neunet.2021.01.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years have witnessed numerous successful applications of incorporating attention module into feed-forward convolutional neural networks . Along this line of research, we design a novel lightweight general-purpose attention module by simultaneously taking channel attention and spatial attention into consideration. Specifically, inspired by the characteristics of channel attention and spatial attention , a nonlinear hybrid method is proposed to combine such two types of attention feature maps, which is highly beneficial to better network fine-tuning. Further, the parameters of each attention branch can be adjustable for the purpose of making the attention module more flexible and adaptable. From another point of view, we found that the currently popular SE, and CBAM modules are actually two particular cases of our proposed attention module. We also explore the latest attention module ADCM. To validate the module, we conduct experiments on CIFAR10, CIFAR100, Fashion MINIST datasets. Results show that, after integrating with our attention module, existing networks tend to be more efficient in training process and have better performance as compared with state-of-the-art competitors. Also, it is worthy to stress the following two points: (1) our attention module can be used in existing state-of-the-art deep architectures and get better performance at a small computational cost; (2) the module can be added to existing deep architectures in a simple way through stacking the integration of networks block and our module.},
  archive      = {J_NN},
  author       = {Nan Guo and Ke Gu and Junfei Qiao and Jing Bi},
  doi          = {10.1016/j.neunet.2021.01.005},
  journal      = {Neural Networks},
  pages        = {158-166},
  shortjournal = {Neural Netw.},
  title        = {Improved deep CNNs based on nonlinear hybrid attention module for image classification},
  volume       = {140},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Uncorrelated feature encoding for faster image style
transfer. <em>NN</em>, <em>140</em>, 148–157. (<a
href="https://doi.org/10.1016/j.neunet.2021.03.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent image style transfer methods use a pre-trained convolutional neural network as their feature encoder. However, the pre-trained network is not optimal for image style transfer but rather for image classification . Furthermore, they require time-consuming feature alignment to consider the existing correlation among channels of the encoded feature map. In this paper, we propose an end-to-end learning method that optimizes both encoder and decoder networks for style transfer task and relieves the computational complexity of the existing correlation-aware feature alignment. First, we performed end-to-end learning that updates not only decoder but also encoder parameters for the task of image style transfer in the network training phase. Second, in addition to the previous style and content losses, we use uncorrelation loss , i.e., the total correlation coefficient among responses of encoder channels. Our uncorrelation loss allows the encoder network to generate a feature map of channels without correlation. Subsequently, our method results in faster forward processing with only a light-weighted transformer of correlation-unaware feature alignment. Moreover, our method drastically reduced the channel redundancy of the encoded feature during the network training process. This provides us a possibility to perform channel elimination with negligible degradation in generated style quality. Our method is applicable to multiple scaled style transfer by using the cascade network scheme and allows a user to control style strength through the usage of a content-style trade-off parameter.},
  archive      = {J_NN},
  author       = {Minseong Kim and Hyun-Chul Choi},
  doi          = {10.1016/j.neunet.2021.03.007},
  journal      = {Neural Networks},
  pages        = {148-157},
  shortjournal = {Neural Netw.},
  title        = {Uncorrelated feature encoding for faster image style transfer},
  volume       = {140},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Speaker separation in realistic noise environments with
applications to a cognitively-controlled hearing aid. <em>NN</em>,
<em>140</em>, 136–147. (<a
href="https://doi.org/10.1016/j.neunet.2021.02.020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Future wearable technology may provide for enhanced communication in noisy environments and for the ability to pick out a single talker of interest in a crowded room simply by the listener shifting their attentional focus. Such a system relies on two components, speaker separation and decoding the listener’s attention to acoustic streams in the environment. To address the former, we present a system for joint speaker separation and noise suppression , referred to as the Binaural Enhancement via Attention Masking Network (BEAMNET). The BEAMNET system is an end-to-end neural network architecture based on self-attention. Binaural input waveforms are mapped to a joint embedding space via a learned encoder, and separate multiplicative masking mechanisms are included for noise suppression and speaker separation. Pairs of output binaural waveforms are then synthesized using learned decoders, each capturing a separated speaker while maintaining spatial cues. A key contribution of BEAMNET is that the architecture contains a separation path , an enhancement path, and an autoencoder path. This paper proposes a novel loss function which simultaneously trains these paths, so that disabling the masking mechanisms during inference causes BEAMNET to reconstruct the input speech signals. This allows dynamic control of the level of suppression applied by BEAMNET via a minimum gain level, which is not possible in other state-of-the-art approaches to end-to-end speaker separation. This paper also proposes a perceptually-motivated waveform distance measure. Using objective speech quality metrics, the proposed system is demonstrated to perform well at separating two equal-energy talkers, even in high levels of background noise. Subjective testing shows an improvement in speech intelligibility across a range of noise levels, for signals with artificially added head-related transfer functions and background noise. Finally, when used as part of an auditory attention decoder (AAD) system using existing electroencephalogram (EEG) data, BEAMNET is found to maintain the decoding accuracy achieved with ideal speaker separation, even in severe acoustic conditions. These results suggest that this enhancement system is highly effective at decoding auditory attention in realistic noise environments, and could possibly lead to improved speech perception in a cognitively controlled hearing aid.},
  archive      = {J_NN},
  author       = {Bengt J. Borgström and Michael S. Brandstein and Gregory A. Ciccarelli and Thomas F. Quatieri and Christopher J. Smalt},
  doi          = {10.1016/j.neunet.2021.02.020},
  journal      = {Neural Networks},
  pages        = {136-147},
  shortjournal = {Neural Netw.},
  title        = {Speaker separation in realistic noise environments with applications to a cognitively-controlled hearing aid},
  volume       = {140},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Understanding the message passing in graph neural networks
via power iteration clustering. <em>NN</em>, <em>140</em>, 130–135. (<a
href="https://doi.org/10.1016/j.neunet.2021.02.025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The mechanism of message passing in graph neural networks (GNNs) is still mysterious. Apart from convolutional neural networks , no theoretical origin for GNNs has been proposed. To our surprise, message passing can be best understood in terms of power iteration. By fully or partly removing activation functions and layer weights of GNNs, we propose subspace power iteration clustering (SPIC) models that iteratively learn with only one aggregator . Experiments show that our models extend GNNs and enhance their capability to process random featured networks. Moreover, we demonstrate the redundancy of some state-of-the-art GNNs in design and define a lower limit for model evaluation by a random aggregator of message passing. Our findings push the boundaries of the theoretical understanding of neural networks.},
  archive      = {J_NN},
  author       = {Xue Li and Yuanzhi Cheng},
  doi          = {10.1016/j.neunet.2021.02.025},
  journal      = {Neural Networks},
  pages        = {130-135},
  shortjournal = {Neural Netw.},
  title        = {Understanding the message passing in graph neural networks via power iteration clustering},
  volume       = {140},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On decision regions of narrow deep neural networks.
<em>NN</em>, <em>140</em>, 121–129. (<a
href="https://doi.org/10.1016/j.neunet.2021.02.024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We show that for neural network functions that have width less or equal to the input dimension all connected components of decision regions are unbounded. The result holds for continuous and strictly monotonic activation functions as well as for the ReLU activation function. This complements recent results on approximation capabilities by Hanin and Sellke (2017) and connectivity of decision regions by Nguyen et al. (2018) for such narrow neural networks. Our results are illustrated by means of numerical experiments.},
  archive      = {J_NN},
  author       = {Hans-Peter Beise and Steve Dias Da Cruz and Udo Schröder},
  doi          = {10.1016/j.neunet.2021.02.024},
  journal      = {Neural Networks},
  pages        = {121-129},
  shortjournal = {Neural Netw.},
  title        = {On decision regions of narrow deep neural networks},
  volume       = {140},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Empirical strategy for stretching probability distribution
in neural-network-based regression. <em>NN</em>, <em>140</em>, 113–120.
(<a href="https://doi.org/10.1016/j.neunet.2021.02.030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In regression analysis under artificial neural networks , the prediction performance depends on determining the appropriate weights between layers. As randomly initialized weights are updated during back-propagation using the gradient descent procedure under a given loss function, the loss function structure can affect the performance significantly. In this study, we considered the distribution error, i.e., the inconsistency of two distributions (those of the predicted values and label), as the prediction error, and proposed weighted empirical stretching (WES) as a novel loss function to increase the overlap area of the two distributions. The function depends on the distribution of a given label, thus, it is applicable to any distribution shape. Moreover, it contains a scaling hyperparameter ( β β ) such that the appropriate parameter value maximizes the common section of the two distributions. To test the function capability, we generated ideal distributed curves (unimodal, skewed unimodal, bimodal, and skewed bimodal) as the labels, and used the Fourier-extracted input data from the curves under a feedforward neural network. In general, WES outperformed loss functions in wide use, and the performance was robust to the various noise levels. The improved results in RMSE for the extreme domain (i.e., both tail regions of the distribution) are expected to be utilized for prediction of abnormal events in non-linear complex systems such as natural disaster and financial crisis.},
  archive      = {J_NN},
  author       = {Eunho Koo and Hyungjun Kim},
  doi          = {10.1016/j.neunet.2021.02.030},
  journal      = {Neural Networks},
  pages        = {113-120},
  shortjournal = {Neural Netw.},
  title        = {Empirical strategy for stretching probability distribution in neural-network-based regression},
  volume       = {140},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Smoothing inertial neurodynamic approach for sparse signal
reconstruction via lp-norm minimization. <em>NN</em>, <em>140</em>,
100–112. (<a
href="https://doi.org/10.1016/j.neunet.2021.02.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a smoothing inertial neurodynamic approach (SINA) which is used to deal with Lp -norm minimization problem to reconstruct sparse signals . Note that the considered optimization problem is nonsmooth, nonconvex and non-Lipschitz. First, the problem is transformed into a smooth optimization problem based on smoothing approximation method, and the Lipschitz property of gradient of the smooth objective function is discussed. Then, SINA based on Karush–Kuhn–Tucker (KKT) condition, smoothing approximation and inertial dynamical approach, is designed to handle smooth optimization problem. The existence, uniqueness, global convergence and optimality of the solution of the SINA are discussed by the Cauchy–Lipschitz–Picard theorem, energy function and KKT condition. In addition, for p=1 , the SINA has a mean sublinear convergence rate O1∕t under some mild conditions. Finally, some numerical examples on sparse signal reconstruction and image restoration are given to illustrate the theoretical results and the efficiency of SINA.},
  archive      = {J_NN},
  author       = {You Zhao and Xiaofeng Liao and Xing He and Rongqiang Tang and Weiwei Deng},
  doi          = {10.1016/j.neunet.2021.02.006},
  journal      = {Neural Networks},
  pages        = {100-112},
  shortjournal = {Neural Netw.},
  title        = {Smoothing inertial neurodynamic approach for sparse signal reconstruction via lp-norm minimization},
  volume       = {140},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Speaker recognition based on deep learning: An overview.
<em>NN</em>, <em>140</em>, 65–99. (<a
href="https://doi.org/10.1016/j.neunet.2021.03.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Speaker recognition is a task of identifying persons from their voices. Recently, deep learning has dramatically revolutionized speaker recognition. However, there is lack of comprehensive reviews on the exciting progress. In this paper, we review several major subtasks of speaker recognition, including speaker verification, identification, diarization, and robust speaker recognition, with a focus on deep-learning-based methods. Because the major advantage of deep learning over conventional methods is its representation ability, which is able to produce highly abstract embedding features from utterances, we first pay close attention to deep-learning-based speaker feature extraction, including the inputs, network structures, temporal pooling strategies, and objective functions respectively, which are the fundamental components of many speaker recognition subtasks. Then, we make an overview of speaker diarization , with an emphasis of recent supervised, end-to-end, and online diarization. Finally, we survey robust speaker recognition from the perspectives of domain adaptation and speech enhancement, which are two major approaches of dealing with domain mismatch and noise problems. Popular and recently released corpora are listed at the end of the paper.},
  archive      = {J_NN},
  author       = {Zhongxin Bai and Xiao-Lei Zhang},
  doi          = {10.1016/j.neunet.2021.03.004},
  journal      = {Neural Networks},
  pages        = {65-99},
  shortjournal = {Neural Netw.},
  title        = {Speaker recognition based on deep learning: An overview},
  volume       = {140},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). LSTM-based approach for predicting periodic motions of an
impacting system via transient dynamics. <em>NN</em>, <em>140</em>,
49–64. (<a href="https://doi.org/10.1016/j.neunet.2021.02.027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamically impacting systems are characterised with inherent instability and complex non-linear phenomena which makes it practically difficult to predict the steady state response of the system at transient periods. This study investigates the ability of a data driven machine learning method using Long Short-Term Memory networks to learn the complex nonlinearity associated with co-existing impact responses from limited transient data. A one-degree-of-freedom impact oscillator has been used to represent the bit–rock interaction for percussive drilling. Simulated data results show velocity measurements to contribute most to predicting steady state responses from transient dynamics with most of the network models reaching an accuracy of over 95\%. Limitations to practically measurable variables in dynamic systems warranted the development of a feature based network model for impact motion classification. Experimental data from a two-degrees-of-freedom impacting system representing percussive bit penetration has been used to demonstrate the effectiveness of this method. The study thus provides a precise and less computational means of detecting and avoiding underperforming impact modes in percussive drilling.},
  archive      = {J_NN},
  author       = {Kenneth Omokhagbo Afebu and Yang Liu and Evangelos Papatheou and Bingyong Guo},
  doi          = {10.1016/j.neunet.2021.02.027},
  journal      = {Neural Networks},
  pages        = {49-64},
  shortjournal = {Neural Netw.},
  title        = {LSTM-based approach for predicting periodic motions of an impacting system via transient dynamics},
  volume       = {140},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Long-term cognitive network-based architecture for
multi-label classification. <em>NN</em>, <em>140</em>, 39–48. (<a
href="https://doi.org/10.1016/j.neunet.2021.03.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a neural system to deal with multi-label classification problems that might involve sparse features. The architecture of this model involves three sequential blocks with well-defined functions. The first block consists of a multilayered feed-forward structure that extracts hidden features, thus reducing the problem dimensionality . This block is useful when dealing with sparse problems. The second block consists of a Long-term Cognitive Network-based model that operates on features extracted by the first block. The activation rule of this recurrent neural network is modified to prevent the vanishing of the input signal during the recurrent inference process. The modified activation rule combines the neurons’ state in the previous abstract layer (iteration) with the initial state. Moreover, we add a bias component to shift the transfer functions as needed to obtain good approximations. Finally, the third block consists of an output layer that adapts the second block’s outputs to the label space. We propose a backpropagation learning algorithm that uses a squared hinge loss function to maximize the margins between labels to train this network. The results show that our model outperforms the state-of-the-art algorithms in most datasets.},
  archive      = {J_NN},
  author       = {Gonzalo Nápoles and Marilyn Bello and Yamisleydi Salgueiro},
  doi          = {10.1016/j.neunet.2021.03.001},
  journal      = {Neural Networks},
  pages        = {39-48},
  shortjournal = {Neural Netw.},
  title        = {Long-term cognitive network-based architecture for multi-label classification},
  volume       = {140},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Exploring the spatial reasoning ability of neural models in
human IQ tests. <em>NN</em>, <em>140</em>, 27–38. (<a
href="https://doi.org/10.1016/j.neunet.2021.02.018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although neural models have performed impressively well on various tasks such as image recognition and question answering, their reasoning ability has been measured in only few studies. In this work, we focus on spatial reasoning and explore the spatial understanding of neural models . First, we describe the following two spatial reasoning IQ tests: rotation and shape composition. Using well-defined rules, we constructed datasets that consist of various complexity levels. We designed a variety of experiments in terms of generalization, and evaluated six different baseline models on the newly generated datasets. We provide an analysis of the results and factors that affect the generalization abilities of models. Also, we analyze how neural models solve spatial reasoning tests with visual aids. We hope that our work can encourage further research into human-level spatial reasoning and provide a new direction for future work.},
  archive      = {J_NN},
  author       = {Hyunjae Kim and Yookyung Koh and Jinheon Baek and Jaewoo Kang},
  doi          = {10.1016/j.neunet.2021.02.018},
  journal      = {Neural Networks},
  pages        = {27-38},
  shortjournal = {Neural Netw.},
  title        = {Exploring the spatial reasoning ability of neural models in human IQ tests},
  volume       = {140},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). TigeCMN: On exploration of temporal interaction graph
embedding via coupled memory neural networks. <em>NN</em>, <em>140</em>,
13–26. (<a href="https://doi.org/10.1016/j.neunet.2021.02.016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing demand of mining rich knowledge in graph structured data, graph embedding has become one of the most popular research topics in both academic and industrial communities due to its powerful capability in learning effective representations. The majority of existing work overwhelmingly learn node embeddings in the context of static, plain or attributed, homogeneous graphs. However, many real-world applications frequently involve bipartite graphs with temporal and attributed interaction edges, named temporal interaction graphs. The temporal interactions usually imply different facets of interest and might even evolve over the time, thus putting forward huge challenges in learning effective node representations. Furthermore, most existing graph embedding models try to embed all the information of each node into a single vector representation , which is insufficient to characterize the node’s multifaceted properties. In this paper, we propose a novel framework named TigeCMN to learn node representations from a sequence of temporal interactions. Specifically, we devise two coupled memory networks to store and update node embeddings in the external matrices explicitly and dynamically, which forms deep matrix representations and thus could enhance the expressiveness of the node embeddings. Then, we generate node embedding from two parts: a static embedding that encodes its stationary properties and a dynamic embedding induced from memory matrix that models its temporal interaction patterns. We conduct extensive experiments on various real-world datasets covering the tasks of node classification , recommendation and visualization. The experimental results empirically demonstrate that TigeCMN can achieve significant gains compared with recent state-of-the-art baselines.},
  archive      = {J_NN},
  author       = {Zhen Zhang and Jiajun Bu and Zhao Li and Chengwei Yao and Can Wang and Jia Wu},
  doi          = {10.1016/j.neunet.2021.02.016},
  journal      = {Neural Networks},
  pages        = {13-26},
  shortjournal = {Neural Netw.},
  title        = {TigeCMN: On exploration of temporal interaction graph embedding via coupled memory neural networks},
  volume       = {140},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SPLASH: Learnable activation functions for improving
accuracy and adversarial robustness. <em>NN</em>, <em>140</em>, 1–12.
(<a href="https://doi.org/10.1016/j.neunet.2021.02.023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce SPLASH units, a class of learnable activation functions shown to simultaneously improve the accuracy of deep neural networks while also improving their robustness to adversarial attacks . SPLASH units have both a simple parameterization and maintain the ability to approximate a wide range of non-linear functions. SPLASH units are: (1) continuous; (2) grounded ( f ( 0 ) = 0 f(0)=0 ); (3) use symmetric hinges; and (4) their hinges are placed at fixed locations which are derived from the data (i.e. no learning required). Compared to nine other learned and fixed activation functions , including ReLU and its variants, SPLASH units show superior performance across three datasets (MNIST, CIFAR-10, and CIFAR-100) and four architectures (LeNet5, All-CNN, ResNet-20, and Network-in-Network). Furthermore, we show that SPLASH units significantly increase the robustness of deep neural networks to adversarial attacks . Our experiments on both black-box and white-box adversarial attacks show that commonly-used architectures, namely LeNet5, All-CNN, Network-in-Network, and ResNet-20, can be up to 31\% more robust to adversarial attacks by simply using SPLASH units instead of ReLUs. Finally, we show the benefits of using SPLASH activation functions in bigger architectures designed for non-trivial datasets such as ImageNet.},
  archive      = {J_NN},
  author       = {Mohammadamin Tavakoli and Forest Agostinelli and Pierre Baldi},
  doi          = {10.1016/j.neunet.2021.02.023},
  journal      = {Neural Networks},
  pages        = {1-12},
  shortjournal = {Neural Netw.},
  title        = {SPLASH: Learnable activation functions for improving accuracy and adversarial robustness},
  volume       = {140},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021f). INN/ENNS/JNNS - membership applic. form. <em>NN</em>,
<em>139</em>, II. (<a
href="https://doi.org/10.1016/S0893-6080(21)00178-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NN},
  doi          = {10.1016/S0893-6080(21)00178-7},
  journal      = {Neural Networks},
  pages        = {II},
  shortjournal = {Neural Netw.},
  title        = {INN/ENNS/JNNS - membership applic. form},
  volume       = {139},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021f). Current events. <em>NN</em>, <em>139</em>, I. (<a
href="https://doi.org/10.1016/S0893-6080(21)00179-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NN},
  doi          = {10.1016/S0893-6080(21)00179-9},
  journal      = {Neural Networks},
  pages        = {I},
  shortjournal = {Neural Netw.},
  title        = {Current events},
  volume       = {139},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A noisy label and negative sample robust loss function for
DNN-based distant supervised relation extraction. <em>NN</em>,
<em>139</em>, 358–370. (<a
href="https://doi.org/10.1016/j.neunet.2021.03.030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a major method for relation extraction, distantly supervised relation extraction (DSRE) suffered from the noisy label problem and class imbalance problem (these two problems are also common for many other NLP tasks, e.g., text classification). However, there seems no existing research in DSRE or other NLP tasks that can simultaneously solve both problems, which is a significant insufficiency in related researches. In this paper, we propose a loss function which is robust to noisy label and efficient for the imbalanced class dataset. More specific, first we quantify the negative impacts of the noisy label and class imbalance problems. And then we construct a loss function that can minimize these negative impacts through a linear programming method. As far as we know, this seems to be the first attempt to address the noisy label problem and class imbalance problem simultaneously. We evaluated the constructed loss function on the distantly labeled dataset, our artificially noised dataset, human-annotated dataset of Docred, as well as the artificially noised dataset of CoNLL 2003. Experimental results indicate that a DNN model adopting the constructed loss function can outperform other models that adopt the state-of-the-art noisy label robust or negative sample robust loss functions.},
  archive      = {J_NN},
  author       = {Lihui Deng and Bo Yang and Zhongfeng Kang and Shantian Yang and Shihu Wu},
  doi          = {10.1016/j.neunet.2021.03.030},
  journal      = {Neural Networks},
  pages        = {358-370},
  shortjournal = {Neural Netw.},
  title        = {A noisy label and negative sample robust loss function for DNN-based distant supervised relation extraction},
  volume       = {139},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Block-cyclic stochastic coordinate descent for deep neural
networks. <em>NN</em>, <em>139</em>, 348–357. (<a
href="https://doi.org/10.1016/j.neunet.2021.04.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a stochastic first-order optimization algorithm , named block-cyclic stochastic coordinate descent (BCSC), that adds a cyclic constraint to stochastic block-coordinate descent in the selection of both data and parameters. It uses different subsets of the data to update different subsets of the parameters, thus limiting the detrimental effect of outliers in the training set. Empirical tests in image classification benchmark datasets show that BCSC outperforms state-of-the-art optimization methods in generalization leading to higher accuracy within the same number of update iterations. The improvements are consistent across different architectures and datasets, and can be combined with other training techniques and regularizations .},
  archive      = {J_NN},
  author       = {Kensuke Nakamura and Stefano Soatto and Byung-Woo Hong},
  doi          = {10.1016/j.neunet.2021.04.001},
  journal      = {Neural Networks},
  pages        = {348-357},
  shortjournal = {Neural Netw.},
  title        = {Block-cyclic stochastic coordinate descent for deep neural networks},
  volume       = {139},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Synchronization in finite time for variable-order
fractional complex dynamic networks with multi-weights and discontinuous
nodes based on sliding mode control strategy. <em>NN</em>, <em>139</em>,
335–347. (<a
href="https://doi.org/10.1016/j.neunet.2021.03.033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper is concerned with the global synchronization in finite time for variable-order fractional complex dynamic networks with multi-weights, where the dynamic nodes are modeled to be discontinuous, and subject to the local Hölder nonlinear growth in a neighborhood of continuous points. Firstly, an inequality with respect to variable-order fractional derivative for convex functions is proposed. On the basis of the proposed inequality, a global convergence principle in finite time for absolutely continuous functions is developed. Secondly, based on proposed convergence principle in finite time, a new sliding mode surface is presented, and an appropriate sliding mode control law is designed to drive the trajectory of the error system to the prescribed sliding mode surface in finite time and remain on it forever. In addition, on the basis of differential inclusions theory and Lur’e Postnikov-type convex Lyapunov function approach, the sufficient conditions with respect to the global stability in finite time are established in terms of linear matrix inequalities for the error system on designed sliding mode surface. Moreover, the upper bound of the settling time is explicitly evaluated. Finally, the effectiveness and correction of synchronization strategies are illustrated through two simulation experiments.},
  archive      = {J_NN},
  author       = {Xia Li and Huaiqin Wu and Jinde Cao},
  doi          = {10.1016/j.neunet.2021.03.033},
  journal      = {Neural Networks},
  pages        = {335-347},
  shortjournal = {Neural Netw.},
  title        = {Synchronization in finite time for variable-order fractional complex dynamic networks with multi-weights and discontinuous nodes based on sliding mode control strategy},
  volume       = {139},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). End-to-end keyword search system based on attention
mechanism and energy scorer for low resource languages. <em>NN</em>,
<em>139</em>, 326–334. (<a
href="https://doi.org/10.1016/j.neunet.2021.04.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Keyword search (KWS) means searching for keywords given by the user from continuous speech. Conventional KWS systems are based on Automatic Speech Recognition (ASR), where the input speech has to be first processed by the ASR system before keyword searching. In the recent decade, as deep learning and deep neural networks (DNN) become increasingly popular, KWS systems can also be trained in an end-to-end (E2E) manner. The main advantage of E2E KWS is that there is no need for speech recognition, which makes the training and searching procedure much more straightforward than the traditional ones. This article proposes an E2E KWS model, which consists of four parts: speech encoder–decoder, query encoder–decoder, attention mechanism , and energy scorer. Firstly, the proposed model outperforms the baseline model . Secondly, we find that under various supervision, character or phoneme sequences, speech or query encoders can extract the corresponding information, resulting in different performances. Moreover, we introduce an attention mechanism and invent a novel energy scorer, where the former can help locate keywords. The latter can make final decisions by considering speech embeddings, query embeddings, and attention weights in parallel. We evaluate our model on low resource conditions with about 10-hour training data for four different languages. The experiment results prove that the proposed model can work well on low resource conditions.},
  archive      = {J_NN},
  author       = {Zeyu Zhao and Wei-Qiang Zhang},
  doi          = {10.1016/j.neunet.2021.04.002},
  journal      = {Neural Networks},
  pages        = {326-334},
  shortjournal = {Neural Netw.},
  title        = {End-to-end keyword search system based on attention mechanism and energy scorer for low resource languages},
  volume       = {139},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). CiwGAN and fiwGAN: Encoding information in acoustic data to
model lexical learning with generative adversarial networks.
<em>NN</em>, <em>139</em>, 305–325. (<a
href="https://doi.org/10.1016/j.neunet.2021.03.017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How can deep neural networks encode information that corresponds to words in human speech into raw acoustic data? This paper proposes two neural network architectures for modeling unsupervised lexical learning from raw acoustic inputs: ciwGAN (Categorical InfoWaveGAN) and fiwGAN (Featural InfoWaveGAN). These combine Deep Convolutional GAN architecture for audio data (WaveGAN; Donahue et al., 2019) with the information theoretic extension of GAN – InfoGAN (Chen et al., 2016) – and propose a new latent space structure that can model featural learning simultaneously with a higher level classification and allows for a very low-dimension vector representation of lexical items . In addition to the Generator and Discriminator networks, the architectures introduce a network that learns to retrieve latent codes from generated audio outputs. Lexical learning is thus modeled as emergent from an architecture that forces a deep neural network to output data such that unique information is retrievable from its acoustic outputs. The networks trained on lexical items from the TIMIT corpus learn to encode unique information corresponding to lexical items in the form of categorical variables in their latent space. By manipulating these variables, the network outputs specific lexical items. The network occasionally outputs innovative lexical items that violate training data, but are linguistically interpretable and highly informative for cognitive modeling and neural network interpretability . Innovative outputs suggest that phonetic and phonological representations learned by the network can be productively recombined and directly paralleled to productivity in human speech: a fiwGAN network trained on suit and dark outputs innovative start , even though it never saw start or even a [st] sequence in the training data. We also argue that setting latent featural codes to values well beyond training range results in almost categorical generation of prototypical lexical items and reveals underlying values of each latent code. Probing deep neural networks trained on well understood dependencies in speech bears implications for latent space interpretability and understanding how deep neural networks learn meaningful representations, as well as potential for unsupervised text-to-speech generation in the GAN framework.},
  archive      = {J_NN},
  author       = {Gašper Beguš},
  doi          = {10.1016/j.neunet.2021.03.017},
  journal      = {Neural Networks},
  pages        = {305-325},
  shortjournal = {Neural Netw.},
  title        = {CiwGAN and fiwGAN: Encoding information in acoustic data to model lexical learning with generative adversarial networks},
  volume       = {139},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Design and independent training of composable and reusable
neural modules. <em>NN</em>, <em>139</em>, 294–304. (<a
href="https://doi.org/10.1016/j.neunet.2021.03.034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Monolithic neural networks and end-to-end training have become the dominating trend in the field of deep learning , but the steady increase in complexity and training costs has raised concerns about the effectiveness and efficiency of this approach. We propose modular training as an alternative strategy for building modular neural networks by composing neural modules that can be trained independently and then kept for future use. We analyse the requirements and challenges regarding modularity and compositionality and, with that information in hand, we provide a detailed design and implementation guideline. We show experimental results of applying this modular approach to a Visual Question Answering (VQA) task parting from a previously published modular network and we evaluate its impact on the final performance, with respect to a baseline trained end-to-end. We also perform compositionality tests on CLEVR.},
  archive      = {J_NN},
  author       = {David Castillo-Bolado and Cayetano Guerra-Artal and Mario Hernández-Tejera},
  doi          = {10.1016/j.neunet.2021.03.034},
  journal      = {Neural Networks},
  pages        = {294-304},
  shortjournal = {Neural Netw.},
  title        = {Design and independent training of composable and reusable neural modules},
  volume       = {139},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Quantifying the separability of data classes in neural
networks. <em>NN</em>, <em>139</em>, 278–293. (<a
href="https://doi.org/10.1016/j.neunet.2021.03.035">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce the Generalized Discrimination Value (GDV) that measures, in a non-invasive manner, how well different data classes separate in each given layer of an artificial neural network . It turns out that, at the end of the training period, the GDV in each given layer L attains a highly reproducible value, irrespective of the initialization of the network’s connection weights. In the case of multi-layer perceptrons trained with error backpropagation , we find that classification of highly complex data sets requires a temporal reduction of class separability , marked by a characteristic ‘energy barrier’ in the initial part of the GDV(L) curve. Even more surprisingly, for a given data set, the GDV(L) is running through a fixed ‘master curve’, independently from the total number of network layers. Finally, due to its invariance with respect to dimensionality, the GDV may serve as a useful tool to compare the internal representational dynamics of artificial neural networks with different architectures for neural architecture search or network compression; or even with brain activity in order to decide between different candidate models of brain function.},
  archive      = {J_NN},
  author       = {Achim Schilling and Andreas Maier and Richard Gerum and Claus Metzner and Patrick Krauss},
  doi          = {10.1016/j.neunet.2021.03.035},
  journal      = {Neural Networks},
  pages        = {278-293},
  shortjournal = {Neural Netw.},
  title        = {Quantifying the separability of data classes in neural networks},
  volume       = {139},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). IHG-MA: Inductive heterogeneous graph multi-agent
reinforcement learning for multi-intersection traffic signal control.
<em>NN</em>, <em>139</em>, 265–277. (<a
href="https://doi.org/10.1016/j.neunet.2021.03.015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-agent deep reinforcement learning (MDRL) has been widely applied in multi-intersection traffic signal control. The MDRL algorithms produce the decentralized cooperative traffic-signal policies via specialized multi-agent settings in certain traffic networks. However, the state-of-the-art MDRL algorithms seem to have some drawbacks. (1) It is desirable that the traffic-signal policies can be smoothly transferred to diverse traffic networks, however, the adopted specialized multi-agent settings hinder the traffic-signal policies to transfer and generalize to new traffic networks. (2) Existing MDRL algorithms which are based on deep neural networks cannot flexibly tackle a time-varying number of vehicles traversing the traffic networks. (3) Existing MDRL algorithms which are based on homogeneous graph neural networks fail to capture the heterogeneous features of objects in traffic networks. Motivated by the above observations, in this paper, we propose an algorithm, referred to as Inductive Heterogeneous Graph Multi-agent Actor–critic (IHG-MA) algorithm, for multi-intersection traffic signal control. The proposed IHG-MA algorithm has two features: (1) It conducts representation learning using a proposed inductive heterogeneous graph neural network (IHG), which is an inductive algorithm. The proposed IHG algorithm can generate embeddings for previously unseen nodes (e.g., new entry vehicles) and new graphs (e.g., new traffic networks). But unlike the algorithms based on the homogeneous graph neural network, IHG algorithm not only encodes heterogeneous features of each node, but also encodes heterogeneous structural (graph) information. (2) It also conducts policy learning using a proposed multi-agent actor–critic(MA), which is a decentralized cooperative framework. The proposed MA framework employs the final embeddings to compute the Q -value and policy, and then optimizes the whole algorithm via the Q -value and policy loss. Experimental results on different traffic datasets illustrate that IHG-MA algorithm outperforms the state-of-the-art algorithms in terms of multiple traffic metrics, which seems to be a new promising algorithm for multi-intersection traffic signal control.},
  archive      = {J_NN},
  author       = {Shantian Yang and Bo Yang and Zhongfeng Kang and Lihui Deng},
  doi          = {10.1016/j.neunet.2021.03.015},
  journal      = {Neural Networks},
  pages        = {265-277},
  shortjournal = {Neural Netw.},
  title        = {IHG-MA: Inductive heterogeneous graph multi-agent reinforcement learning for multi-intersection traffic signal control},
  volume       = {139},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Synchronization of memristive neural networks with unknown
parameters via event-triggered adaptive control. <em>NN</em>,
<em>139</em>, 255–264. (<a
href="https://doi.org/10.1016/j.neunet.2021.02.029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper considers the drive-response synchronization of memristive neural networks (MNNs) with unknown parameters, where the unbounded discrete and bounded distributed time-varying delays are involved. Aiming at the unknown parameters of MNNs, the updating law of weight in response system and the gain of adaptive controller are proposed to realize the synchronization of delayed MNNs. In view of the limited communication and bandwidth, the event-triggered mechanism is introduced to adaptive control, which not only decreases the times of controller update and the amount of data sending out but also enables synchronization when parameters of MNNs are unknown. In addition, a relative threshold strategy, which is relative to fixed threshold strategy, is proposed to increase the inter-execution intervals and to improve the control effect. When the parameters of MNNs are known, the algebraic criteria of synchronization are established via event-triggered state feedback control by exploiting inequality techniques and calculus theorems. Finally, one simulation is presented to validate the effectiveness of the proposed results.},
  archive      = {J_NN},
  author       = {Yufeng Zhou and Hao Zhang and Zhigang Zeng},
  doi          = {10.1016/j.neunet.2021.02.029},
  journal      = {Neural Networks},
  pages        = {255-264},
  shortjournal = {Neural Netw.},
  title        = {Synchronization of memristive neural networks with unknown parameters via event-triggered adaptive control},
  volume       = {139},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Detecting slender objects with uncertainty based on
keypoint-displacement representation. <em>NN</em>, <em>139</em>,
246–254. (<a
href="https://doi.org/10.1016/j.neunet.2021.03.024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Slender objects are long and thin objects. Existing object detection networks are not specially designed for detecting slender objects. We propose a method to detect slender objects. We represent slender objects with a keypoint-displacement pattern instead of using axis-aligned bounding boxes , avoiding problems like orientation confusion and wrong elimination. In our network, three parallel branches predict keypoint heatmaps, displacement vector field, and displacement uncertainty heatmap respectively. We add the uncertainty branch to enable our network to give uncertainty together with detection results. The predicted uncertainty provides a continuous criterion to evaluate whether detection results are reliable. In addition, the uncertainty branch can lower the weight of ambiguous training samples, leading to more accurate detection results. We employ our proposed method in two typical practical applications. Edges of electrode sheets and pins of electronic chips are correctly detected as slender objects. Manufacturing quality is evaluated through analyzing the detection results, including keypoint number, displacement property, and uncertainty value.},
  archive      = {J_NN},
  author       = {Zelong Kong and Nian Zhang and Xinping Guan and Xinyi Le},
  doi          = {10.1016/j.neunet.2021.03.024},
  journal      = {Neural Networks},
  pages        = {246-254},
  shortjournal = {Neural Netw.},
  title        = {Detecting slender objects with uncertainty based on keypoint-displacement representation},
  volume       = {139},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Keyword spotting techniques to improve the recognition
accuracy of user-defined keywords. <em>NN</em>, <em>139</em>, 237–245.
(<a href="https://doi.org/10.1016/j.neunet.2021.03.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The existing keyword spotting (KWS) techniques can recognize pre-defined keywords well but have a poor recognition accuracy for user-defined keywords. In real use cases, there is a high demand for users to define their keywords for various reasons. To address the problem, in this work, three techniques have been proposed, including incremental training with revised loss function, data augmentation , and fine-grained training, to improve the accuracy for the user-defined keywords while maintaining high accuracy for pre-defined keywords. The proposed techniques are applied to a classical KWS model ( cnn-trad-fpool3 ) and a state-of-the-art KWS model ( res15 ) respectively. The experimental results show that the proposed techniques have better recognition accuracy than several existing methods for the recognition of use-defined keywords. With the proposed techniques, the recognition accuracy of user-defined keywords on cnn-trad-fpool3 and res15 are significantly improved by 21.78\% and 24.42\%, respectively.},
  archive      = {J_NN},
  author       = {Li Liu and Mingxue Yang and Xinyi Gao and Qingsong Liu and Zhengxi Yuan and Jun Zhou},
  doi          = {10.1016/j.neunet.2021.03.012},
  journal      = {Neural Networks},
  pages        = {237-245},
  shortjournal = {Neural Netw.},
  title        = {Keyword spotting techniques to improve the recognition accuracy of user-defined keywords},
  volume       = {139},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Stochastic quasi-synchronization of heterogeneous delayed
impulsive dynamical networks via single impulsive control. <em>NN</em>,
<em>139</em>, 223–236. (<a
href="https://doi.org/10.1016/j.neunet.2021.03.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the quasi-synchronization problem of the stochastic heterogeneous complex dynamical networks with impulsive couplings and multiple time-varying delays. It is shown that this kind of dynamical networks can achieve exponential quasi-synchronization by exerting impulsive control added on only one chosen pinning node. By employing the Lyapunov stability theory , some sufficient criteria on quasi-synchronization for this dynamical network are established, revealing the relationship between the quasi-synchronization performance and the stochastic perturbations as well as the frequency and strength of impulsive coupling. Finally, some numerical examples are used to illustrate the effectiveness of the main results.},
  archive      = {J_NN},
  author       = {Guang Ling and Ming-Feng Ge and Xinghua Liu and Gaoxi Xiao and Qingju Fan},
  doi          = {10.1016/j.neunet.2021.03.011},
  journal      = {Neural Networks},
  pages        = {223-236},
  shortjournal = {Neural Netw.},
  title        = {Stochastic quasi-synchronization of heterogeneous delayed impulsive dynamical networks via single impulsive control},
  volume       = {139},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Semi-dilated convolutional neural networks for epileptic
seizure prediction. <em>NN</em>, <em>139</em>, 212–222. (<a
href="https://doi.org/10.1016/j.neunet.2021.03.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Epilepsy is a neurological brain disorder that affects ∼ ∼ 75 million people worldwide. Predicting epileptic seizures holds great potential for improving the quality of life of people with epilepsy, but seizure prediction solely from the Electroencephalogram (EEG) is challenging. Classical machine learning algorithms and a variety of feature engineering methods have become a mainstay in seizure prediction , yet performance has been variable. In this work, we first propose an efficient data pre-processing method that maps the time-series EEG signals into an image-like format (a “scalogram”) using continuous wavelet transform . We then develop a novel convolution module named “semi-dilated convolution” that better exploits the geometry of wavelet scalograms and nonsquare-shape images. Finally, we propose a neural network architecture named “semi-dilated convolutional network (SDCN)” that uses semi-dilated convolutions to solely expand the receptive field along the long dimension (image width) while maintaining high resolution along the short dimension (image height). Results demonstrate that the proposed SDCN architecture outperforms previous seizure prediction methods, achieving an average seizure prediction sensitivity of 98.90\% for scalp EEG and 88.45–89.52\% for invasive EEG.},
  archive      = {J_NN},
  author       = {Ramy Hussein and Soojin Lee and Rabab Ward and Martin J. McKeown},
  doi          = {10.1016/j.neunet.2021.03.008},
  journal      = {Neural Networks},
  pages        = {212-222},
  shortjournal = {Neural Netw.},
  title        = {Semi-dilated convolutional neural networks for epileptic seizure prediction},
  volume       = {139},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). D-MONA: A dilated mixed-order non-local attention network
for speaker and language recognition. <em>NN</em>, <em>139</em>,
201–211. (<a
href="https://doi.org/10.1016/j.neunet.2021.03.014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attention-based convolutional neural network (CNN) models are increasingly being adopted for speaker and language recognition (SR/LR) tasks. These include time, frequency, spatial and channel attention, which can focus on useful time frames, frequency bands, regions or channels while extracting features. However, these traditional attention methods lack the exploration of complex information and multi-scale long-range speech feature interactions, which can benefit SR/LR tasks. To address these issues, this paper firstly proposes mixed-order attention (MOA) for low frame-level speech features to gain the finest grain multi-order information at higher resolution. We then combine that with a non-local attention (NLA) mechanism and a dilated residual structure to balance fine grained local detail with convolution from multi-scale long-range time/frequency regions in feature space. The proposed dilated mixed-order non-local attention network (D-MONA) exploits the detail available from the first and the second-order feature attention analysis, but achieves this over a much wider context than purely local attention. Experiments are conducted on three datasets, including two SR tasks of Voxceleb and CN-celeb, and one LR task, NIST LRE 07. For SR, D-MONA improves on ResNet-34 results by at least 29\% and 15\% for Voxceleb1 and CN-celeb respectively. For the LR task, a large improvement is achieved over ResNet-34 of 21\% for the challenging 3s utterance condition, 59\% for the 10s condition and 67\% for the 30s condition. It also outperforms the state-of-the-art deep bottleneck feature-DNN (DBF-DNN) x-vector system at all scales.},
  archive      = {J_NN},
  author       = {Xiaoxiao Miao and Ian McLoughlin and Wenchao Wang and Pengyuan Zhang},
  doi          = {10.1016/j.neunet.2021.03.014},
  journal      = {Neural Networks},
  pages        = {201-211},
  shortjournal = {Neural Netw.},
  title        = {D-MONA: A dilated mixed-order non-local attention network for speaker and language recognition},
  volume       = {139},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep neural network representation and generative
adversarial learning. <em>NN</em>, <em>139</em>, 199–200. (<a
href="https://doi.org/10.1016/j.neunet.2021.03.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NN},
  author       = {Ariel Ruiz-Garcia ( Guest Editors ) and Jürgen Schmidhuber and Vasile Palade and Clive Cheong Took and Danilo Mandic},
  doi          = {10.1016/j.neunet.2021.03.009},
  journal      = {Neural Networks},
  pages        = {199-200},
  shortjournal = {Neural Netw.},
  title        = {Deep neural network representation and generative adversarial learning},
  volume       = {139},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Computational reproductions of external force field adaption
without assuming desired trajectories. <em>NN</em>, <em>139</em>,
179–198. (<a
href="https://doi.org/10.1016/j.neunet.2021.01.030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimal feedback control is an established framework that is used to characterize human movement. However, it is not fully understood how the brain computes optimal gains through interactions with the environment. In the past study, we proposed a model of motor learning that identifies a set of feedback and feedforward controllers and a state predictor of the arm musculoskeletal system to control free reaching movements. In this study, we applied the model to force field adaptation tasks where normal reaching movements are disturbed by an external force imposed on the hand. Without a priori knowledge about the arm and environment, the model was able to adapt to the force field by generating counteracting forces to overcome it in a manner similar to what is reported in the behavioral literature. The kinematics of the movements generated by our model share characteristic features of human movements observed before and after force field adaptation. In addition, we demonstrate that the structure and learning algorithm introduced in our model induced a shift in the end-point’s equilibrium position and a static force modulation, accompanied by a fast and a slow learning process. Importantly, our model does not require desired trajectories , yields movements without specifying movement duration, and predicts force generation patterns by exploring the environment. Our model demonstrates a possible mechanism through which the central nervous system may control and adapt a point-to-point reaching movement without specifying a desired trajectory by continuously updating the body’s musculoskeletal model.},
  archive      = {J_NN},
  author       = {Hiroyuki Kambara and Atsushi Takagi and Haruka Shimizu and Toshihiro Kawase and Natsue Yoshimura and Nicolas Schweighofer and Yasuharu Koike},
  doi          = {10.1016/j.neunet.2021.01.030},
  journal      = {Neural Networks},
  pages        = {179-198},
  shortjournal = {Neural Netw.},
  title        = {Computational reproductions of external force field adaption without assuming desired trajectories},
  volume       = {139},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Cross knowledge-based generative zero-shot learning approach
with taxonomy regularization. <em>NN</em>, <em>139</em>, 168–178. (<a
href="https://doi.org/10.1016/j.neunet.2021.02.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although zero-shot learning (ZSL) has an inferential capability of recognizing new classes that have never been seen before, it always faces two fundamental challenges of the cross modality and cross-domain challenges. In order to alleviate these problems, we develop a generative network-based ZSL approach equipped with the proposed Cross Knowledge Learning (CKL) scheme and Taxonomy Regularization (TR). In our approach, the semantic features are taken as inputs, and the output is the synthesized visual features generated from the corresponding semantic features . CKL enables more relevant semantic features to be trained for semantic-to-visual feature embedding in ZSL, while Taxonomy Regularization (TR) significantly improves the intersections with unseen images with more generalized visual features generated from generative network. Extensive experiments on several benchmark datasets (i.e., AwA1, AwA2, CUB, NAB and aPY) show that our approach is superior to these state-of-the-art methods in terms of ZSL image classification and retrieval.},
  archive      = {J_NN},
  author       = {Cheng Xie and Hongxin Xiang and Ting Zeng and Yun Yang and Beibei Yu and Qing Liu},
  doi          = {10.1016/j.neunet.2021.02.009},
  journal      = {Neural Networks},
  pages        = {168-178},
  shortjournal = {Neural Netw.},
  title        = {Cross knowledge-based generative zero-shot learning approach with taxonomy regularization},
  volume       = {139},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Visual question answering based on local-scene-aware
referring expression generation. <em>NN</em>, <em>139</em>, 158–167. (<a
href="https://doi.org/10.1016/j.neunet.2021.02.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual question answering requires a deep understanding of both images and natural language. However, most methods mainly focus on visual concept; such as the relationships between various objects. The limited use of object categories combined with their relationships or simple question embedding is insufficient for representing complex scenes and explaining decisions. To address this limitation, we propose the use of text expressions generated for images, because such expressions have few structural constraints and can provide richer descriptions of images. The generated expressions can be incorporated with visual features and question embedding to obtain the question-relevant answer. A joint-embedding multi-head attention network is also proposed to model three different information modalities with co-attention. We quantitatively and qualitatively evaluated the proposed method on the VQA v2 dataset and compared it with state-of-the-art methods in terms of answer prediction. The quality of the generated expressions was also evaluated on the RefCOCO, RefCOCO+, and RefCOCOg datasets. Experimental results demonstrate the effectiveness of the proposed method and reveal that it outperformed all of the competing methods in terms of both quantitative and qualitative results.},
  archive      = {J_NN},
  author       = {Jung-Jun Kim and Dong-Gyu Lee and Jialin Wu and Hong-Gyu Jung and Seong-Whan Lee},
  doi          = {10.1016/j.neunet.2021.02.001},
  journal      = {Neural Networks},
  pages        = {158-167},
  shortjournal = {Neural Netw.},
  title        = {Visual question answering based on local-scene-aware referring expression generation},
  volume       = {139},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An effective SteinGLM initialization scheme for training
multi-layer feedforward sigmoidal neural networks. <em>NN</em>,
<em>139</em>, 149–157. (<a
href="https://doi.org/10.1016/j.neunet.2021.02.014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network initialization is the first and critical step for training neural networks . In this paper, we propose a novel network initialization scheme based on the celebrated Stein’s identity. By viewing multi-layer feedforward sigmoidal neural networks as cascades of multi-index models, the projection weights to the first hidden layer are initialized using eigenvectors of the cross-moment matrix between the input’s second-order score function and the response. The input data is then forward propagated to the next layer and such a procedure can be repeated until all the hidden layers are initialized. Finally, the weights for the output layer are initialized by generalized linear modeling. Such a proposed SteinGLM method is shown through extensive numerical results to be much faster and more accurate than other popular methods commonly used for training neural networks.},
  archive      = {J_NN},
  author       = {Zebin Yang and Hengtao Zhang and Agus Sudjianto and Aijun Zhang},
  doi          = {10.1016/j.neunet.2021.02.014},
  journal      = {Neural Networks},
  pages        = {149-157},
  shortjournal = {Neural Netw.},
  title        = {An effective SteinGLM initialization scheme for training multi-layer feedforward sigmoidal neural networks},
  volume       = {139},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Knowledge graph embedding with shared latent semantic units.
<em>NN</em>, <em>139</em>, 140–148. (<a
href="https://doi.org/10.1016/j.neunet.2021.02.013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge graph embedding (KGE) aims to project both entities and relations into a continuous low-dimensional space. However, for a given knowledge graph (KG), only a small number of entities and relations occur many times, while the vast majority of entities and relations occur less frequently. This data sparsity problem has largely been ignored by most of the existing KGE models. To this end, in this paper, we propose a general technique to enable knowledge transfer among semantically similar entities or relations. Specifically, we define latent semantic units (LSUs), which are the sub-components of entity and relation embeddings. Semantically similar entities or relations are supposed to share the same LSUs, and thus knowledge can be transferred among entities or relations. Finally, extensive experiments show that the proposed technique is able to enhance existing KGE models and can provide better representations of KGs.},
  archive      = {J_NN},
  author       = {Zhao Zhang and Fuzhen Zhuang and Meng Qu and Zheng-Yu Niu and Hui Xiong and Qing He},
  doi          = {10.1016/j.neunet.2021.02.013},
  journal      = {Neural Networks},
  pages        = {140-148},
  shortjournal = {Neural Netw.},
  title        = {Knowledge graph embedding with shared latent semantic units},
  volume       = {139},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Unsupervised anomaly detection in stream data with online
evolving spiking neural networks. <em>NN</em>, <em>139</em>, 118–139.
(<a href="https://doi.org/10.1016/j.neunet.2021.02.017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised anomaly discovery in stream data is a research topic with many practical applications. However, in many cases, it is not easy to collect enough training data with labeled anomalies for supervised learning of an anomaly detector in order to deploy it later for identification of real anomalies in streaming data. It is thus important to design anomalies detectors that can correctly detect anomalies without access to labeled training data. Our idea is to adapt the Online evolving Spiking Neural Network (OeSNN) classifier to the anomaly detection task. As a result, we offer an Online evolving Spiking Neural Network for Unsupervised Anomaly Detection algorithm (OeSNN-UAD), which, unlike OeSNN, works in an unsupervised way and does not separate output neurons into disjoint decision classes. OeSNN-UAD uses our proposed new two-step anomaly detection method. Also, we derive new theoretical properties of neuronal model and input layer encoding of OeSNN, which enable more effective and efficient detection of anomalies in our OeSNN-UAD approach. The proposed OeSNN-UAD detector was experimentally compared with state-of-the-art unsupervised and semi-supervised detectors of anomalies in stream data from the Numenta Anomaly Benchmark and Yahoo Anomaly Datasets repositories. Our approach outperforms the other solutions provided in the literature in the case of data streams from the Numenta Anomaly Benchmark repository. Also, in the case of real data files of the Yahoo Anomaly Benchmark repository, OeSNN-UAD outperforms other selected algorithms, whereas in the case of Yahoo Anomaly Benchmark synthetic data files, it provides competitive results to the results recently reported in the literature.},
  archive      = {J_NN},
  author       = {Piotr S. Maciąg and Marzena Kryszkiewicz and Robert Bembenik and Jesus L. Lobo and Javier Del Ser},
  doi          = {10.1016/j.neunet.2021.02.017},
  journal      = {Neural Networks},
  pages        = {118-139},
  shortjournal = {Neural Netw.},
  title        = {Unsupervised anomaly detection in stream data with online evolving spiking neural networks},
  volume       = {139},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Residual neural network precisely quantifies dysarthria
severity-level based on short-duration speech segments. <em>NN</em>,
<em>139</em>, 105–117. (<a
href="https://doi.org/10.1016/j.neunet.2021.02.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, we have witnessed Deep Learning methodologies gaining significant attention for severity-based classification of dysarthric speech. Detecting dysarthria , quantifying its severity, are of paramount importance in various real-life applications, such as the assessment of patients’ progression in treatments, which includes an adequate planning of their therapy and the improvement of speech-based interactive systems in order to handle pathologically-affected voices automatically. Notably, current speech-powered tools often deal with short-duration speech segments and, consequently, are less efficient in dealing with impaired speech, even by using Convolutional Neural Networks (CNNs). Thus, detecting dysarthria severity-level based on short speech segments might help in improving the performance and applicability of those systems. To achieve this goal, we propose a novel Residual Network (ResNet)-based technique which receives short-duration speech segments as input. Statistically meaningful objective analysis of our experiments, reported over standard Universal Access corpus, exhibits average values of 21.35\% and 22.48\% improvement, compared to the baseline CNN, in terms of classification accuracy and F1-score, respectively. For additional comparisons, tests with Gaussian Mixture Models and Light CNNs were also performed. Overall, the values of 98.90\% and 98.00\% for classification accuracy and F1-score, respectively, were obtained with the proposed ResNet approach, confirming its efficacy and reassuring its practical applicability.},
  archive      = {J_NN},
  author       = {Siddhant Gupta and Ankur T. Patil and Mirali Purohit and Mihir Parmar and Maitreya Patel and Hemant A. Patil and Rodrigo Capobianco Guido},
  doi          = {10.1016/j.neunet.2021.02.008},
  journal      = {Neural Networks},
  pages        = {105-117},
  shortjournal = {Neural Netw.},
  title        = {Residual neural network precisely quantifies dysarthria severity-level based on short-duration speech segments},
  volume       = {139},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Comparative study using inverse ontology cogency and
alternatives for concept recognition in the annotated national library
of medicine database. <em>NN</em>, <em>139</em>, 86–104. (<a
href="https://doi.org/10.1016/j.neunet.2021.01.018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces inverse ontology cogency, a concept recognition process and distance function that is biologically-inspired and competitive with alternative methods. The paper introduces inverse ontology cogency as a new alternative method. It is a novel distance measure used in selecting the optimum mapping between ontology-specified concepts and phrases in free-form text. We also apply a multi-layer perceptron and text processing method for named entity recognition as an alternative to recurrent neural network methods. Automated named entity recognition , or concept recognition, is a common task in natural language processing . Similarities between confabulation theory and existing language models are discussed. This paper provides comparisons to MetaMap from the National Library of Medicine (NLM), a popular tool used in medicine to map free-form text to concepts in a medical ontology. The NLM provides a manually annotated database from the medical literature with concepts labeled, a unique, valuable source of ground truth, permitting comparison with MetaMap performance. Comparisons for different feature set combinations are made to demonstrate the effectiveness of inverse ontology cogency for entity recognition. Results indicate that using both inverse ontology cogency and corpora cogency improved concept recognition precision 20\% over the best published MetaMap results. This demonstrates a new, effective approach for identifying medical concepts in text. This is the first time cogency has been explicitly invoked for reasoning with ontologies, and the first time it has been used on medical literature where high-quality ground truth is available for quality assessment.},
  archive      = {J_NN},
  author       = {George J. Shannon and Naga Rayapati and Steven M. Corns and Donald C. Wunsch II},
  doi          = {10.1016/j.neunet.2021.01.018},
  journal      = {Neural Networks},
  pages        = {86-104},
  shortjournal = {Neural Netw.},
  title        = {Comparative study using inverse ontology cogency and alternatives for concept recognition in the annotated national library of medicine database},
  volume       = {139},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dense residual network: Enhancing global dense feature flow
for character recognition. <em>NN</em>, <em>139</em>, 77–85. (<a
href="https://doi.org/10.1016/j.neunet.2021.02.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep Convolutional Neural Networks (CNNs), such as Dense Convolutional Network (DenseNet), have achieved great success for image representation learning by capturing deep hierarchical features. However, most existing network architectures of simply stacking the convolutional layers fail to enable them to fully discover local and global feature information between layers. In this paper, we mainly investigate how to enhance the local and global feature learning abilities of DenseNet by fully exploiting the hierarchical features from all convolutional layers . Technically, we propose an effective convolutional deep model termed Dense Residual Network (DRN) for the task of optical character recognition . To define DRN, we propose a refined residual dense block (r-RDB) to retain the ability of local feature fusion and local residual learning of original RDB, which can reduce the computing efforts of inner layers at the same time. After fully capturing local residual dense features, we utilize the sum operation and several r-RDBs to construct a new block termed global dense block (GDB) by imitating the construction of dense blocks to adaptively learn global dense residual features in a holistic way. Finally, we use two convolutional layers to design a down-sampling block to reduce the global feature size and extract more informative deeper features. Extensive results show that our DRN can deliver enhanced results, compared with other related deep models.},
  archive      = {J_NN},
  author       = {Zhao Zhang and Zemin Tang and Yang Wang and Zheng Zhang and Choujun Zhan and Zhengjun Zha and Meng Wang},
  doi          = {10.1016/j.neunet.2021.02.005},
  journal      = {Neural Networks},
  pages        = {77-85},
  shortjournal = {Neural Netw.},
  title        = {Dense residual network: Enhancing global dense feature flow for character recognition},
  volume       = {139},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Synchronization criteria of delayed inertial neural
networks with generally markovian jumping. <em>NN</em>, <em>139</em>,
64–76. (<a href="https://doi.org/10.1016/j.neunet.2021.02.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, the synchronization problem of inertial neural networks with time-varying delays and generally Markovian jumping is investigated. The second order differential equations are transformed into the first-order differential equations by utilizing the variable transformation method. The Markovian process in the systems is uncertain or partially known due to the delay of data transmission channel or the loss of data information, which is more general and practicable to consider generally Markovian jumping inertial neural networks . The synchronization criteria can be obtained by using the delay-dependent Lyapunov–Krasovskii functionals and higher order polynomial based relaxed inequality (HOPRII). In addition, the desired controllers are obtained by solving a set of linear matrix inequalities . Finally, the numerical examples are provided to demonstrate the effectiveness of the theoretical results.},
  archive      = {J_NN},
  author       = {Junyi Wang and Zhanshan Wang and Xiangyong Chen and Jianlong Qiu},
  doi          = {10.1016/j.neunet.2021.02.004},
  journal      = {Neural Networks},
  pages        = {64-76},
  shortjournal = {Neural Netw.},
  title        = {Synchronization criteria of delayed inertial neural networks with generally markovian jumping},
  volume       = {139},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Biomimetic FPGA-based spatial navigation model with grid
cells and place cells. <em>NN</em>, <em>139</em>, 45–63. (<a
href="https://doi.org/10.1016/j.neunet.2021.01.028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The mammalian spatial navigation system is characterized by an initial divergence of internal representations, with disparate classes of neurons responding to distinct features including location, speed, borders and head direction; an ensuing convergence finally enables navigation and path integration. Here, we report the algorithmic and hardware implementation of biomimetic neural structures encompassing a feed-forward trimodular, multi-layer architecture representing grid-cell, place-cell and decoding modules for navigation. The grid-cell module comprised of neurons that fired in a grid-like pattern, and was built of distinct layers that constituted the dorsoventral span of the medial entorhinal cortex . Each layer was built as an independent continuous attractor network with distinct grid-field spatial scales . The place-cell module comprised of neurons that fired at one or few spatial locations , organized into different clusters based on convergent modular inputs from different grid-cell layers, replicating the gradient in place-field size along the hippocampal dorso-ventral axis. The decoding module, a two-layer neural network that constitutes the convergence of the divergent representations in preceding modules, received inputs from the place-cell module and provided specific coordinates of the navigating object. After vital design optimizations involving all modules, we implemented the tri-modular structure on Zynq Ultrascale+ field-programmable gate array silicon chip, and demonstrated its capacity in precisely estimating the navigational trajectory with minimal overall resource consumption involving a mere 2.92\% Look Up Table utilization. Our implementation of a biomimetic , digital spatial navigation system is stable, reliable, reconfigurable, real-time with execution time of about 32 s for 100k input samples (in contrast to 40 minutes on Intel Core i7-7700 CPU with 8 cores clocking at 3.60 GHz) and thus can be deployed for autonomous-robotic navigation without requiring additional sensors.},
  archive      = {J_NN},
  author       = {Adithya Krishna and Divyansh Mittal and Siri Garudanagiri Virupaksha and Abhishek Ramdas Nair and Rishikesh Narayanan and Chetan Singh Thakur},
  doi          = {10.1016/j.neunet.2021.01.028},
  journal      = {Neural Networks},
  pages        = {45-63},
  shortjournal = {Neural Netw.},
  title        = {Biomimetic FPGA-based spatial navigation model with grid cells and place cells},
  volume       = {139},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A fast saddle-point dynamical system approach to robust deep
learning. <em>NN</em>, <em>139</em>, 33–44. (<a
href="https://doi.org/10.1016/j.neunet.2021.02.021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent focus on robustness to adversarial attacks for deep neural networks produced a large variety of algorithms for training robust models. Most of the effective algorithms involve solving the min–max optimization problem for training robust models (min step) under worst-case attacks (max step). However, they often suffer from high computational cost from running several inner maximization iterations (to find an optimal attack) inside every outer minimization iteration. Therefore, it becomes difficult to readily apply such algorithms for moderate to large size real world data sets. To alleviate this, we explore the effectiveness of iterative descent–ascent algorithms where the maximization and minimization steps are executed in an alternate fashion to simultaneously obtain the worst-case attack and the corresponding robust model. Specifically, we propose a novel discrete-time dynamical system-based algorithm that aims to find the saddle point of a min–max optimization problem in the presence of uncertainties. Under the assumptions that the cost function is convex and uncertainties enter concavely in the robust learning problem, we analytically show that our algorithm converges asymptotically to the robust optimal solution under a general adversarial budget constraints as induced by ℓ p ℓp norm, for 1 ≤ p ≤ ∞ 1≤p≤∞ . Based on our proposed analysis, we devise a fast robust training algorithm for deep neural networks . Although such training involves highly non-convex robust optimization problems, empirical results show that the algorithm can achieve significant robustness compared to other state-of-the-art robust models on benchmark data sets.},
  archive      = {J_NN},
  author       = {Yasaman Esfandiari and Aditya Balu and Keivan Ebrahimi and Umesh Vaidya and Nicola Elia and Soumik Sarkar},
  doi          = {10.1016/j.neunet.2021.02.021},
  journal      = {Neural Networks},
  pages        = {33-44},
  shortjournal = {Neural Netw.},
  title        = {A fast saddle-point dynamical system approach to robust deep learning},
  volume       = {139},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). End-to-end novel visual categories learning via auxiliary
self-supervision. <em>NN</em>, <em>139</em>, 24–32. (<a
href="https://doi.org/10.1016/j.neunet.2021.02.015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semi-supervised learning has largely alleviated the strong demand for large amount of annotations in deep learning . However, most of the methods have adopted a common assumption that there is always labeled data from the same class of unlabeled data , which is impractical and restricted for real-world applications. In this research work, our focus is on semi-supervised learning when the categories of unlabeled data and labeled data are disjoint from each other. The main challenge is how to effectively leverage knowledge in labeled data to unlabeled data when they are independent from each other, and not belonging to the same categories. Previous state-of-the-art methods have proposed to construct pairwise similarity pseudo labels as supervising signals. However, two issues are commonly inherent in these methods: (1) All of previous methods are comprised of multiple training phases, which makes it difficult to train the model in an end-to-end fashion. (2) Strong dependence on the quality of pairwise similarity pseudo labels limits the performance as pseudo labels are vulnerable to noise and bias. Therefore, we propose to exploit the use of self-supervision as auxiliary task during model training such that labeled data and unlabeled data will share the same set of surrogate labels and overall supervising signals can have strong regularization . By doing so, all modules in the proposed algorithm can be trained simultaneously, which will boost the learning capability as end-to-end learning can be achieved. Moreover, we propose to utilize local structure information in feature space during pairwise pseudo label construction, as local properties are more robust to noise. Extensive experiments have been conducted on three frequently used visual datasets, i.e., CIFAR-10, CIFAR-100 and SVHN, in this paper. Experiment results have indicated the effectiveness of our proposed algorithm as we have achieved new state-of-the-art performance for novel visual categories learning for these three datasets.},
  archive      = {J_NN},
  author       = {Yuanyuan Qing and Yijie Zeng and Qi Cao and Guang-Bin Huang},
  doi          = {10.1016/j.neunet.2021.02.015},
  journal      = {Neural Networks},
  pages        = {24-32},
  shortjournal = {Neural Netw.},
  title        = {End-to-end novel visual categories learning via auxiliary self-supervision},
  volume       = {139},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Convergence of the RMSProp deep learning method with penalty
for nonconvex optimization. <em>NN</em>, <em>139</em>, 17–23. (<a
href="https://doi.org/10.1016/j.neunet.2021.02.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A norm version of the RMSProp algorithm with penalty (termed RMSPropW) is introduced into the deep learning framework and its convergence is addressed both analytically and numerically. For rigour, we consider the general nonconvex setting and prove the boundedness and convergence of the RMSPropW method in both deterministic and stochastic cases. This equips us with strict upper bounds on both the moving average squared norm of the gradient and the norm of weight parameters throughout the learning process, owing to the penalty term within the proposed cost function. In the deterministic (batch) case, the boundedness of the moving average squared norm of the gradient is employed to prove that the gradient sequence converges to zero when using a fixed step size, while with diminishing stepsizes, the minimum of the gradient sequence converges to zero. In the stochastic case, due to the boundedness of the weight evolution sequence, it is further shown that the weight sequence converges to a stationary point with probability 1. Finally, illustrative simulations are provided to support the theoretical analysis, including a comparison with the standard RMSProp on MNIST, CIFAR-10, and IMDB datasets.},
  archive      = {J_NN},
  author       = {Dongpo Xu and Shengdong Zhang and Huisheng Zhang and Danilo P. Mandic},
  doi          = {10.1016/j.neunet.2021.02.011},
  journal      = {Neural Networks},
  pages        = {17-23},
  shortjournal = {Neural Netw.},
  title        = {Convergence of the RMSProp deep learning method with penalty for nonconvex optimization},
  volume       = {139},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A generative adversarial network approach to (ensemble)
weather prediction. <em>NN</em>, <em>139</em>, 1–16. (<a
href="https://doi.org/10.1016/j.neunet.2021.02.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We use a conditional deep convolutional generative adversarial network to predict the geopotential height of the 500 hPa pressure level, the two-meter temperature and the total precipitation for the next 24 h over Europe. The proposed models are trained on 4 years of ERA5 reanalysis data from with the goal to predict the associated meteorological fields in 2019. The forecasts show a good qualitative and quantitative agreement with the true reanalysis data for the geopotential height and two-meter temperature, while failing for total precipitation, thus indicating that weather forecasts based on data alone may be possible for specific meteorological parameters. We further use Monte-Carlo dropout to develop an ensemble weather prediction system based purely on deep learning strategies, which is computationally cheap and further improves the skill of the forecasting model , by allowing to quantify the uncertainty in the current weather forecast as learned by the model.},
  archive      = {J_NN},
  author       = {Alex Bihlo},
  doi          = {10.1016/j.neunet.2021.02.003},
  journal      = {Neural Networks},
  pages        = {1-16},
  shortjournal = {Neural Netw.},
  title        = {A generative adversarial network approach to (ensemble) weather prediction},
  volume       = {139},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021g). INN/ENNS/JNNS - membership applic. form. <em>NN</em>,
<em>138</em>, II. (<a
href="https://doi.org/10.1016/S0893-6080(21)00137-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NN},
  doi          = {10.1016/S0893-6080(21)00137-4},
  journal      = {Neural Networks},
  pages        = {II},
  shortjournal = {Neural Netw.},
  title        = {INN/ENNS/JNNS - membership applic. form},
  volume       = {138},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021g). Current events. <em>NN</em>, <em>138</em>, I. (<a
href="https://doi.org/10.1016/S0893-6080(21)00136-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NN},
  doi          = {10.1016/S0893-6080(21)00136-2},
  journal      = {Neural Networks},
  pages        = {I},
  shortjournal = {Neural Netw.},
  title        = {Current events},
  volume       = {138},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Explanation of emotion regulation mechanism of mindfulness
using a brain function model. <em>NN</em>, <em>138</em>, 198–214. (<a
href="https://doi.org/10.1016/j.neunet.2021.01.029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The emotion regulation mechanism of mindfulness plays an important role in the stress reduction effect. Many researchers in the fields of cognitive psychology and cognitive neuroscience have attempted to elucidate this mechanism by documenting the cognitive processes that occur and the neural activities that characterize each process. However, previous findings have not revealed the mechanism of information propagation in the brain that achieves emotion regulation during mindfulness. In this study, we constructed a functional brain model based on its anatomical network structure and a computational model representing the propagation of information between brain regions. We then examined the effects of mindfulness meditation on information propagation in the brain using simulations of changes in the activity of each region. These simulations of changes represent the degree of processing resource allocation to the neural activity via changes in the weights of each region’s output. As a result of the simulations, we reveal how the neural activity characteristic of emotion regulation in mindfulness, which has been reported in previous studies, is realized in the brain. Mindfulness meditation increases the weight of the output from each region of the thalamus and sensory cortex , which processes sensory stimuli from the external world. This sensory information activates the insula and anterior cingulate cortex (ACC). The orbitofrontal cortex and dorsolateral prefrontal cortex inhibit amygdala activity (i.e., top-down emotion regulation). However, when mindfulness meditation dominates bottom-up processing via sensory stimuli from the external world, amygdala activity increases through the insula and ACC activation.},
  archive      = {J_NN},
  author       = {Haruka Nakamura and Yoshimasa Tawatsuji and Siyuan Fang and Tatsunori Matsui},
  doi          = {10.1016/j.neunet.2021.01.029},
  journal      = {Neural Networks},
  pages        = {198-214},
  shortjournal = {Neural Netw.},
  title        = {Explanation of emotion regulation mechanism of mindfulness using a brain function model},
  volume       = {138},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fast convergence rates of deep neural networks for
classification. <em>NN</em>, <em>138</em>, 179–197. (<a
href="https://doi.org/10.1016/j.neunet.2021.02.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We derive the fast convergence rates of a deep neural network (DNN) classifier with the rectified linear unit (ReLU) activation function learned using the hinge loss. We consider three cases for a true model: (1) a smooth decision boundary, (2) smooth conditional class probability , and (3) the margin condition (i.e., the probability of inputs near the decision boundary is small). We show that the DNN classifier learned using the hinge loss achieves fast rate convergences for all three cases provided that the architecture (i.e., the number of layers, number of nodes and sparsity) is carefully selected. An important implication is that DNN architectures are very flexible for use in various cases without much modification. In addition, we consider a DNN classifier learned by minimizing the cross-entropy, and show that the DNN classifier achieves a fast convergence rate under the conditions that the noise exponent and margin exponent are large. Even though they are strong, we explain that these two conditions are not too absurd for image classification problems. To confirm our theoretical explanation, we present the results of a small numerical study conducted to compare the hinge loss and cross-entropy.},
  archive      = {J_NN},
  author       = {Yongdai Kim and Ilsang Ohn and Dongha Kim},
  doi          = {10.1016/j.neunet.2021.02.012},
  journal      = {Neural Networks},
  pages        = {179-197},
  shortjournal = {Neural Netw.},
  title        = {Fast convergence rates of deep neural networks for classification},
  volume       = {138},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Recurrent neural network with noise rejection for cyclic
motion generation of robotic manipulators. <em>NN</em>, <em>138</em>,
164–178. (<a
href="https://doi.org/10.1016/j.neunet.2021.02.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recurrent neural network (RNN), as a kind of neural network with outstanding computing capability, improvability, and hardware realizability, has been widely used in various fields, especially in robotics. In this paper, an RNN with noise rejection is deliberately constructed to remedy the issue of joint-angle drift frequently occurring during the cyclic motion generation (CMG) of a manipulator in a noisy environment . Different from general RNNs, the proposed RNN possesses inherent noise immunity , especially for time-varying polynomial noises. Besides, proofs on the convergence of the proposed RNN in the absence and presence of noises are given. Furthermore, we carry out simulations on manipulators PUMA 560 and UR5 to demonstrate the reliability of the proposed RNN in remedying joint-angle drift, and comparison simulations under different noisy conditions further verify its superiority. In addition, experiments are conducted on manipulator FRANKA Panda to elucidate the realizability of the proposed RNN.},
  archive      = {J_NN},
  author       = {Mei Liu and Li He and Bin Hu and Shuai Li},
  doi          = {10.1016/j.neunet.2021.02.002},
  journal      = {Neural Networks},
  pages        = {164-178},
  shortjournal = {Neural Netw.},
  title        = {Recurrent neural network with noise rejection for cyclic motion generation of robotic manipulators},
  volume       = {138},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Paradoxical sensory reactivity induced by functional
disconnection in a robot model of neurodevelopmental disorder.
<em>NN</em>, <em>138</em>, 150–163. (<a
href="https://doi.org/10.1016/j.neunet.2021.01.033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neurodevelopmental disorders are characterized by heterogeneous and non-specific nature of their clinical symptoms. In particular, hyper- and hypo-reactivity to sensory stimuli are diagnostic features of autism spectrum disorder and are reported across many neurodevelopmental disorders. However, computational mechanisms underlying the unusual paradoxical behaviors remain unclear. In this study, using a robot controlled by a hierarchical recurrent neural network model with predictive processing and learning mechanism, we simulated how functional disconnection altered the learning process and subsequent behavioral reactivity to environmental change. The results show that, through the learning process, long-range functional disconnection between distinct network levels could simultaneously lower the precision of sensory information and higher-level prediction. The alteration caused a robot to exhibit sensory-dominated and sensory-ignoring behaviors ascribed to sensory hyper- and hypo-reactivity, respectively. As long-range functional disconnection became more severe, a frequency shift from hyporeactivity to hyperreactivity was observed, paralleling an early sign of autism spectrum disorder. Furthermore, local functional disconnection at the level of sensory processing similarly induced hyporeactivity due to low sensory precision. These findings suggest a computational explanation for paradoxical sensory behaviors in neurodevelopmental disorders, such as coexisting hyper- and hypo-reactivity to sensory stimulus. A neurorobotics approach may be useful for bridging various levels of understanding in neurodevelopmental disorders and providing insights into mechanisms underlying complex clinical symptoms.},
  archive      = {J_NN},
  author       = {Hayato Idei and Shingo Murata and Yuichi Yamashita and Tetsuya Ogata},
  doi          = {10.1016/j.neunet.2021.01.033},
  journal      = {Neural Networks},
  pages        = {150-163},
  shortjournal = {Neural Netw.},
  title        = {Paradoxical sensory reactivity induced by functional disconnection in a robot model of neurodevelopmental disorder},
  volume       = {138},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Self-augmentation: Generalizing deep networks to unseen
classes for few-shot learning. <em>NN</em>, <em>138</em>, 140–149. (<a
href="https://doi.org/10.1016/j.neunet.2021.02.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot learning aims to classify unseen classes with a few training examples. While recent works have shown that standard mini-batch training with carefully designed training strategies can improve generalization ability for unseen classes, well-known problems in deep networks such as memorizing training statistics have been less explored for few-shot learning. To tackle this issue, we propose self-augmentation that consolidates self-mix and self-distillation. Specifically, we propose a regional dropout technique called self-mix, in which a patch of an image is substituted into other values in the same image. With this dropout effect, we show that the generalization ability of deep networks can be improved as it prevents us from learning specific structures of a dataset. Then, we employ a backbone network that has auxiliary branches with its own classifier to enforce knowledge sharing . This sharing of knowledge forces each branch to learn diverse optimal points during training. Additionally, we present a local representation learner to further exploit a few training examples of unseen classes by generating fake queries and novel weights. Experimental results show that the proposed method outperforms the state-of-the-art methods for prevalent few-shot benchmarks and improves the generalization ability.},
  archive      = {J_NN},
  author       = {Jin-Woo Seo and Hong-Gyu Jung and Seong-Whan Lee},
  doi          = {10.1016/j.neunet.2021.02.007},
  journal      = {Neural Networks},
  pages        = {140-149},
  shortjournal = {Neural Netw.},
  title        = {Self-augmentation: Generalizing deep networks to unseen classes for few-shot learning},
  volume       = {138},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Small universal spiking neural p systems with
dendritic/axonal delays and dendritic trunk/feedback. <em>NN</em>,
<em>138</em>, 126–139. (<a
href="https://doi.org/10.1016/j.neunet.2021.02.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In spiking neural P (SN P) systems, neurons are interconnected by means of synapses, and they use spikes to communicate with each other. However, in biology, the complex structure of dendritic tree is also an important part in the communication scheme between neurons since these structures are linked to advanced neural process such as learning and memory formation. In this work, we present a new variant of the SN P systems inspired by diverse dendrite and axon phenomena such as dendritic feedback, dendritic trunk, dendritic delays and axonal delays, respectively. This new variant is referred to as a spiking neural P system with dendritic and axonal computation (DACSN P system). Specifically, we include experimentally proven biological features in the current SN P systems to reduce the computational complexity of the soma by providing it with stable firing patterns through dendritic delays, dendritic feedback and axonal delays. As a consequence, the proposed DACSN P systems use the minimum number of synapses and neurons with simple and homogeneous standard spiking rules. Here, we study the computational capabilities of a DACSN P system. In particular, we prove that DACSN P systems with dendritic and axonal behavior are universal as both number-accepting/generating devices. In addition, we constructed a small universal SN P system using 39 neurons with standard spiking rules to compute any Turing computable function.},
  archive      = {J_NN},
  author       = {Luis Garcia and Giovanny Sanchez and Eduardo Vazquez and Gerardo Avalos and Esteban Anides and Mariko Nakano and Gabriel Sanchez and Hector Perez},
  doi          = {10.1016/j.neunet.2021.02.010},
  journal      = {Neural Networks},
  pages        = {126-139},
  shortjournal = {Neural Netw.},
  title        = {Small universal spiking neural p systems with dendritic/axonal delays and dendritic trunk/feedback},
  volume       = {138},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A new recursive least squares-based learning algorithm for
spiking neurons. <em>NN</em>, <em>138</em>, 110–125. (<a
href="https://doi.org/10.1016/j.neunet.2021.01.016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spiking neural networks (SNNs) are regarded as effective models for processing spatio-temporal information. However, their inherent complexity of temporal coding makes it an arduous task to put forward an effective supervised learning algorithm, which still puzzles researchers in this area. In this paper, we propose a Recursive Least Squares-Based Learning Rule (RLSBLR) for SNN to generate the desired spatio-temporal spike train . During the learning process of our method, the weight update is driven by the cost function defined by the difference between the membrane potential and the firing threshold. The amount of weight modification depends not only on the impact of the current error function, but also on the previous error functions which are evaluated by current weights. In order to improve the learning performance, we integrate a modified synaptic delay learning to the proposed RLSBLR. We conduct experiments in different settings, such as spiking lengths, number of inputs, firing rates, noises and learning parameters, to thoroughly investigate the performance of this learning algorithm. The proposed RLSBLR is compared with competitive algorithms of Perceptron-Based Spiking Neuron Learning Rule (PBSNLR) and Remote Supervised Method (ReSuMe). Experimental results demonstrate that the proposed RLSBLR can achieve higher learning accuracy, higher efficiency and better robustness against different types of noise. In addition, we apply the proposed RLSBLR to open source database TIDIGITS, and the results show that our algorithm has a good practical application performance.},
  archive      = {J_NN},
  author       = {Yun Zhang and Hong Qu and Xiaoling Luo and Yi Chen and Yuchen Wang and Malu Zhang and Zefang Li},
  doi          = {10.1016/j.neunet.2021.01.016},
  journal      = {Neural Networks},
  pages        = {110-125},
  shortjournal = {Neural Netw.},
  title        = {A new recursive least squares-based learning algorithm for spiking neurons},
  volume       = {138},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Towards effective deep transfer via attentive feature
alignment. <em>NN</em>, <em>138</em>, 98–109. (<a
href="https://doi.org/10.1016/j.neunet.2021.01.022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Training a deep convolutional network from scratch requires a large amount of labeled data, which however may not be available for many practical tasks. To alleviate the data burden, a practical approach is to adapt a pre-trained model learned on the large source domain to the target domain, but the performance can be limited when the source and target domain data distributions have large differences. Some recent works attempt to alleviate this issue by imposing feature alignment over the intermediate feature maps between the source and target networks. However, for a source model, many of the channels/spatial-features for each layer can be irrelevant to the target task. Thus, directly applying feature alignment may not achieve promising performance. In this paper, we propose an Attentive Feature Alignment (AFA) method for effective domain knowledge transfer by identifying and attending on the relevant channels and spatial features between two domains. To this end, we devise two learnable attentive modules at both the channel and spatial levels. We then sequentially perform attentive spatial- and channel-level feature alignments between the source and target networks, in which the target model and attentive module are learned simultaneously. Moreover, we theoretically analyze the generalization performance of our method, which confirms its superiority to existing methods. Extensive experiments on both image classification and face recognition demonstrate the effectiveness of our method. The source code and the pre-trained models are available at https://github.com/xiezheng-cs/AFA https://github.com/xiezheng-cs/AFA.},
  archive      = {J_NN},
  author       = {Zheng Xie and Zhiquan Wen and Yaowei Wang and Qingyao Wu and Mingkui Tan},
  doi          = {10.1016/j.neunet.2021.01.022},
  journal      = {Neural Networks},
  pages        = {98-109},
  shortjournal = {Neural Netw.},
  title        = {Towards effective deep transfer via attentive feature alignment},
  volume       = {138},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Compositional memory in attractor neural networks with
one-step learning. <em>NN</em>, <em>138</em>, 78–97. (<a
href="https://doi.org/10.1016/j.neunet.2021.01.031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compositionality refers to the ability of an intelligent system to construct models out of reusable parts. This is critical for the productivity and generalization of human reasoning, and is considered a necessary ingredient for human-level artificial intelligence. While traditional symbolic methods have proven effective for modeling compositionality, artificial neural networks struggle to learn systematic rules for encoding generalizable structured models. We suggest that this is due in part to short-term memory that is based on persistent maintenance of activity patterns without fast weight changes. We present a recurrent neural network that encodes structured representations as systems of contextually-gated dynamical attractors called attractor graphs. This network implements a functionally compositional working memory that is manipulated using top-down gating and fast local learning. We evaluate this approach with empirical experiments on storage and retrieval of graph-based data structures , as well as an automated hierarchical planning task. Our results demonstrate that compositional structures can be stored in and retrieved from neural working memory without persistent maintenance of multiple activity patterns. Further, memory capacity is improved by the use of a fast store-erase learning rule that permits controlled erasure and mutation of previously learned associations. We conclude that the combination of top-down gating and fast associative learning provides recurrent neural networks with a robust functional mechanism for compositional working memory.},
  archive      = {J_NN},
  author       = {Gregory P. Davis and Garrett E. Katz and Rodolphe J. Gentili and James A. Reggia},
  doi          = {10.1016/j.neunet.2021.01.031},
  journal      = {Neural Networks},
  pages        = {78-97},
  shortjournal = {Neural Netw.},
  title        = {Compositional memory in attractor neural networks with one-step learning},
  volume       = {138},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Unsupervised cross-domain named entity recognition using
entity-aware adversarial training. <em>NN</em>, <em>138</em>, 68–77. (<a
href="https://doi.org/10.1016/j.neunet.2020.12.027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The success of neural network based methods in named entity recognition (NER) is heavily relied on abundant manual labeled data. However, these NER methods are unavailable when the data is fully-unlabeled in a new domain. To address the problem, we propose an unsupervised cross-domain model which leverages labeled data from source domain to predict entities in unlabeled target domain. To relieve the distribution divergence when transferring knowledge from source to target domain, we apply adversarial training . Furthermore, we design an entity-aware attention module to guide the adversarial training to reduce the discrepancy of entity features between different domains. Experimental results demonstrate that our model outperforms other methods and achieves state-of-the-art performance.},
  archive      = {J_NN},
  author       = {Qi Peng and Changmeng Zheng and Yi Cai and Tao Wang and Haoran Xie and Qing Li},
  doi          = {10.1016/j.neunet.2020.12.027},
  journal      = {Neural Networks},
  pages        = {68-77},
  shortjournal = {Neural Netw.},
  title        = {Unsupervised cross-domain named entity recognition using entity-aware adversarial training},
  volume       = {138},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SAM-GAN: Self-attention supporting multi-stage generative
adversarial networks for text-to-image synthesis. <em>NN</em>,
<em>138</em>, 57–67. (<a
href="https://doi.org/10.1016/j.neunet.2021.01.023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Synthesizing photo-realistic images based on text descriptions is a challenging task in the field of computer vision . Although generative adversarial networks have made significant breakthroughs in this task, they still face huge challenges in generating high-quality visually realistic images consistent with the semantics of text. Generally, existing text-to-image methods accomplish this task with two steps, that is, first generating an initial image with a rough outline and color, and then gradually yielding the image within high-resolution from the initial image. However, one drawback of these methods is that, if the quality of the initial image generation is not high, it is hard to generate a satisfactory high-resolution image. In this paper, we propose SAM-GAN, Self-Attention supporting Multi-stage Generative Adversarial Networks, for text-to-image synthesis. With the self-attention mechanism, the model can establish the multi-level dependence of the image and fuse the sentence- and word-level visual-semantic vectors, to improve the quality of the generated image. Furthermore, a multi-stage perceptual loss is introduced to enhance the semantic similarity between the synthesized image and the real image, thus enhancing the visual-semantic consistency between text and images. For the diversity of the generated images, a mode seeking regularization term is integrated into the model. The results of extensive experiments and ablation studies, which were conducted in the Caltech-UCSD Birds and Microsoft Common Objects in Context datasets, show that our model is superior to competitive models in text-to-image synthesis.},
  archive      = {J_NN},
  author       = {Dunlu Peng and Wuchen Yang and Cong Liu and Shuairui Lü},
  doi          = {10.1016/j.neunet.2021.01.023},
  journal      = {Neural Networks},
  pages        = {57-67},
  shortjournal = {Neural Netw.},
  title        = {SAM-GAN: Self-attention supporting multi-stage generative adversarial networks for text-to-image synthesis},
  volume       = {138},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). General stochastic separation theorems with optimal bounds.
<em>NN</em>, <em>138</em>, 33–56. (<a
href="https://doi.org/10.1016/j.neunet.2021.01.034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Phenomenon of stochastic separability was revealed and used in machine learning to correct errors of Artificial Intelligence (AI) systems and analyze AI instabilities. In high-dimensional datasets under broad assumptions each point can be separated from the rest of the set by simple and robust Fisher’s discriminant (is Fisher separable ). Errors or clusters of errors can be separated from the rest of the data. The ability to correct an AI system also opens up the possibility of an attack on it, and the high dimensionality induces vulnerabilities caused by the same stochastic separability that holds the keys to understanding the fundamentals of robustness and adaptivity in high-dimensional data-driven AI. To manage errors and analyze vulnerabilities, the stochastic separation theorems should evaluate the probability that the dataset will be Fisher separable in given dimensionality and for a given class of distributions. Explicit and optimal estimates of these separation probabilities are required, and this problem is solved in the present work. The general stochastic separation theorems with optimal probability estimates are obtained for important classes of distributions: log-concave distribution, their convex combinations and product distributions. The standard i.i.d. assumption was significantly relaxed. These theorems and estimates can be used both for correction of high-dimensional data driven AI systems and for analysis of their vulnerabilities. The third area of application is the emergence of memories in ensembles of neurons, the phenomena of grandmother’s cells and sparse coding in the brain, and explanation of unexpected effectiveness of small neural ensembles in high-dimensional brain.},
  archive      = {J_NN},
  author       = {Bogdan Grechuk and Alexander N. Gorban and Ivan Y. Tyukin},
  doi          = {10.1016/j.neunet.2021.01.034},
  journal      = {Neural Networks},
  pages        = {33-56},
  shortjournal = {Neural Netw.},
  title        = {General stochastic separation theorems with optimal bounds},
  volume       = {138},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A survey on modern trainable activation functions.
<em>NN</em>, <em>138</em>, 14–32. (<a
href="https://doi.org/10.1016/j.neunet.2021.01.026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In neural networks literature, there is a strong interest in identifying and defining activation functions which can improve neural network performance. In recent years there has been a renovated interest in the scientific community in investigating activation functions which can be trained during the learning process, usually referred to as trainable , learnable or adaptable activation functions. They appear to lead to better network performance. Diverse and heterogeneous models of trainable activation function have been proposed in the literature. In this paper, we present a survey of these models. Starting from a discussion on the use of the term “activation function” in literature, we propose a taxonomy of trainable activation functions, highlight common and distinctive proprieties of recent and past models, and discuss main advantages and limitations of this type of approach. We show that many of the proposed approaches are equivalent to adding neuron layers which use fixed ( non-trainable ) activation functions and some simple local rule that constrains the corresponding weight layers.},
  archive      = {J_NN},
  author       = {Andrea Apicella and Francesco Donnarumma and Francesco Isgrò and Roberto Prevete},
  doi          = {10.1016/j.neunet.2021.01.026},
  journal      = {Neural Networks},
  pages        = {14-32},
  shortjournal = {Neural Netw.},
  title        = {A survey on modern trainable activation functions},
  volume       = {138},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fading memory echo state networks are universal.
<em>NN</em>, <em>138</em>, 10–13. (<a
href="https://doi.org/10.1016/j.neunet.2021.01.025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Echo state networks (ESNs) have been recently proved to be universal approximants for input/output systems with respect to various L p Lp -type criteria. When 1 ≤ p 1≤p&amp;lt;∞ , only p p -integrability hypotheses need to be imposed, while in the case p = ∞ p=∞ a uniform boundedness hypotheses on the inputs is required. This note shows that, in the last case, a universal family of ESNs can be constructed that contains exclusively elements that have the echo state and the fading memory properties. This conclusion could not be drawn with the results and methods available so far in the literature.},
  archive      = {J_NN},
  author       = {Lukas Gonon and Juan-Pablo Ortega},
  doi          = {10.1016/j.neunet.2021.01.025},
  journal      = {Neural Networks},
  pages        = {10-13},
  shortjournal = {Neural Netw.},
  title        = {Fading memory echo state networks are universal},
  volume       = {138},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A proximal neurodynamic model for solving inverse mixed
variational inequalities. <em>NN</em>, <em>138</em>, 1–9. (<a
href="https://doi.org/10.1016/j.neunet.2021.01.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a proximal neurodynamic model (PNDM) for solving inverse mixed variational inequalities (IMVIs) based on the proximal operator. It is shown that the PNDM has a unique continuous solution under the condition of Lipschitz continuity (L-continuity). It is also shown that the equilibrium point of the proposed PNDM is asymptotically stable or exponentially stable under some mild conditions. Finally, three numerical examples are presented to illustrate effectiveness of the proposed PNDM.},
  archive      = {J_NN},
  author       = {Xingxing Ju and Chuandong Li and Xing He and Gang Feng},
  doi          = {10.1016/j.neunet.2021.01.012},
  journal      = {Neural Networks},
  pages        = {1-9},
  shortjournal = {Neural Netw.},
  title        = {A proximal neurodynamic model for solving inverse mixed variational inequalities},
  volume       = {138},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021h). INN/ENNS/JNNS - membership applic. form. <em>NN</em>,
<em>137</em>, II. (<a
href="https://doi.org/10.1016/S0893-6080(21)00098-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NN},
  doi          = {10.1016/S0893-6080(21)00098-8},
  journal      = {Neural Networks},
  pages        = {II},
  shortjournal = {Neural Netw.},
  title        = {INN/ENNS/JNNS - membership applic. form},
  volume       = {137},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021h). Current events. <em>NN</em>, <em>137</em>, I. (<a
href="https://doi.org/10.1016/S0893-6080(21)00097-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NN},
  doi          = {10.1016/S0893-6080(21)00097-6},
  journal      = {Neural Networks},
  pages        = {I},
  shortjournal = {Neural Netw.},
  title        = {Current events},
  volume       = {137},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bilateral attention decoder: A lightweight decoder for
real-time semantic segmentation. <em>NN</em>, <em>137</em>, 188–199. (<a
href="https://doi.org/10.1016/j.neunet.2021.01.021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The encoder–decoder structure has been introduced into semantic segmentation to improve the spatial accuracy of the network by fusing high- and low-level feature maps. However, recent state-of-the-art encoder–decoder-based methods can hardly attain the real-time requirement due to their complex and inefficient decoders. To address this issue, in this paper, we propose a lightweight bilateral attention decoder for real-time semantic segmentation . It consists of two blocks and can fuse different level feature maps via two steps, i.e. , information refinement and information fusion. In the first step, we propose a channel attention branch to refine the high-level feature maps and a spatial attention branch for the low-level ones. The refined high-level feature maps can capture more exact semantic information and the refined low-level ones can capture more accurate spatial information, which significantly improves the information capturing ability of these feature maps. In the second step, we develop a new fusion module named pooling fusing block to fuse the refined high- and low-level feature maps. This fusion block can take full advantages of the high- and low-level feature maps, leading to high-quality fusion results. To verify the efficiency of the proposed bilateral attention decoder, we adopt a lightweight network as the backbone and compare our proposed method with other state-of-the-art real-time semantic segmentation methods on the Cityscapes and Camvid datasets. Experimental results demonstrate that our proposed method can achieve better performance with a higher inference speed. Moreover, we compare our proposed network with several state-of-the-art non-real-time semantic segmentation methods and find that our proposed network can also attain better segmentation performance .},
  archive      = {J_NN},
  author       = {Chengli Peng and Tian Tian and Chen Chen and Xiaojie Guo and Jiayi Ma},
  doi          = {10.1016/j.neunet.2021.01.021},
  journal      = {Neural Networks},
  pages        = {188-199},
  shortjournal = {Neural Netw.},
  title        = {Bilateral attention decoder: A lightweight decoder for real-time semantic segmentation},
  volume       = {137},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Building an adaptive interface via unsupervised tracking of
latent manifolds. <em>NN</em>, <em>137</em>, 174–187. (<a
href="https://doi.org/10.1016/j.neunet.2021.01.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In human–machine interfaces, decoder calibration is critical to enable an effective and seamless interaction with the machine. However, recalibration is often necessary as the decoder off-line predictive power does not generally imply ease-of-use, due to closed loop dynamics and user adaptation that cannot be accounted for during the calibration procedure. Here, we propose an adaptive interface that makes use of a non-linear autoencoder trained iteratively to perform online manifold identification and tracking, with the dual goal of reducing the need for interface recalibration and enhancing human–machine joint performance. Importantly, the proposed approach avoids interrupting the operation of the device and it neither relies on information about the state of the task, nor on the existence of a stable neural or movement manifold, allowing it to be applied in the earliest stages of interface operation, when the formation of new neural strategies is still on-going. In order to more directly test the performance of our algorithm, we defined the autoencoder latent space as the control space of a body–machine interface. After an initial offline parameter tuning, we evaluated the performance of the adaptive interface versus that of a static decoder in approximating the evolving low-dimensional manifold of users simultaneously learning to perform reaching movements within the latent space. Results show that the adaptive approach increased the representational efficiency of the interface decoder. Concurrently, it significantly improved users’ task-related performance, indicating that the development of a more accurate internal model is encouraged by the online co-adaptation process.},
  archive      = {J_NN},
  author       = {Fabio Rizzoglio and Maura Casadio and Dalia De Santis and Ferdinando A. Mussa-Ivaldi},
  doi          = {10.1016/j.neunet.2021.01.009},
  journal      = {Neural Networks},
  pages        = {174-187},
  shortjournal = {Neural Netw.},
  title        = {Building an adaptive interface via unsupervised tracking of latent manifolds},
  volume       = {137},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Statistical foundation of variational bayes neural networks.
<em>NN</em>, <em>137</em>, 151–173. (<a
href="https://doi.org/10.1016/j.neunet.2021.01.027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the popularism of Bayesian neural networks (BNNs) in recent years, its use is somewhat limited in complex and big data situations due to the computational cost associated with full posterior evaluations. Variational Bayes (VB) provides a useful alternative to circumvent the computational cost and time complexity associated with the generation of samples from the true posterior using Markov Chain Monte Carlo (MCMC) techniques. The efficacy of the VB methods is well established in machine learning literature. However, its potential broader impact is hindered due to a lack of theoretical validity from a statistical perspective. In this paper, we establish the fundamental result of posterior consistency for the mean-field variational posterior (VP) for a feed-forward artificial neural network model . The paper underlines the conditions needed to guarantee that the VP concentrates around Hellinger neighborhoods of the true density function. Additionally, the role of the scale parameter and its influence on the convergence rates has also been discussed. The paper mainly relies on two results (1) the rate at which the true posterior grows (2) the rate at which the Kullback–Leibler (KL) distance between the posterior and variational posterior grows. The theory provides a guideline for building prior distributions for BNNs along with an assessment of accuracy of the corresponding VB implementation.},
  archive      = {J_NN},
  author       = {Shrijita Bhattacharya and Tapabrata Maiti},
  doi          = {10.1016/j.neunet.2021.01.027},
  journal      = {Neural Networks},
  pages        = {151-173},
  shortjournal = {Neural Netw.},
  title        = {Statistical foundation of variational bayes neural networks},
  volume       = {137},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Controllable stroke-based sketch synthesis from a
self-organized latent space. <em>NN</em>, <em>137</em>, 138–150. (<a
href="https://doi.org/10.1016/j.neunet.2021.01.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning to synthesize free-hand sketches controllably according to specified categories and sketching styles is a challenging task, due to the lack of training data with category labels and style labels. One choice to control the synthesis is by self-organizing a latent coding space to preserve the similarity of structural patterns of the observed data. A practical way is introducing a Gaussian mixture prior over the latent codes, where each Gaussian component represents a specific categorical or stylistic pattern. As a result, we can generate sketches by sampling the latent variables from the Gaussian components or continuously manipulating the latent representations by interpolation. To achieve robust controllable sketch synthesis, it is critical to determine an appropriate Gaussian number. An underestimated Gaussian number cannot fully represent all the sketch patterns, i.e., some clusters have to contain sketches with more than one pattern. An overestimated one introduces redundant components, usually representing a chaotic collection of sketches with diverse patterns featured by other components. Both cases disturb pattern clustering over the coding space and make the internal code generation difficult to control for specific patterns. However, the Gaussian number is unavailable in this unsupervised task. In this paper, we present Rival Penalized Competitive Learning pixel to sequence (RPCL-pix2seq) to automatically determine the Gaussian number. Both quantitative and qualitative experimental results show RPCL-pix2seq can partition the codes for the sketches into an approximate stable number of clusters. Hence, we are able to do synthesis reasoning over the latent space, generating novel but reasonable sketches which neither appear in the training dataset nor exist in real life.},
  archive      = {J_NN},
  author       = {Sicong Zang and Shikui Tu and Lei Xu},
  doi          = {10.1016/j.neunet.2021.01.006},
  journal      = {Neural Networks},
  pages        = {138-150},
  shortjournal = {Neural Netw.},
  title        = {Controllable stroke-based sketch synthesis from a self-organized latent space},
  volume       = {137},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The exact asymptotic form of bayesian generalization error
in latent dirichlet allocation. <em>NN</em>, <em>137</em>, 127–137. (<a
href="https://doi.org/10.1016/j.neunet.2021.01.024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Latent Dirichlet allocation (LDA) obtains essential information from data by using Bayesian inference . It is applied to knowledge discovery via dimension reducing and clustering in many fields. However, its generalization error had not been yet clarified since it is a singular statistical model where there is no one-to-one mapping from parameters to probability distributions. In this paper, we give the exact asymptotic form of its generalization error and marginal likelihood, by theoretical analysis of its learning coefficient using algebraic geometry . The theoretical result shows that the Bayesian generalization error in LDA is expressed in terms of that in matrix factorization and a penalty from the simplex restriction of LDA’s parameter region. A numerical experiment is consistent with the theoretical result.},
  archive      = {J_NN},
  author       = {Naoki Hayashi},
  doi          = {10.1016/j.neunet.2021.01.024},
  journal      = {Neural Networks},
  pages        = {127-137},
  shortjournal = {Neural Netw.},
  title        = {The exact asymptotic form of bayesian generalization error in latent dirichlet allocation},
  volume       = {137},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The kolmogorov–arnold representation theorem revisited.
<em>NN</em>, <em>137</em>, 119–126. (<a
href="https://doi.org/10.1016/j.neunet.2021.01.020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is a longstanding debate whether the Kolmogorov–Arnold representation theorem can explain the use of more than one hidden layer in neural networks . The Kolmogorov–Arnold representation decomposes a multivariate function into an interior and an outer function and therefore has indeed a similar structure as a neural network with two hidden layers. But there are distinctive differences. One of the main obstacles is that the outer function depends on the represented function and can be wildly varying even if the represented function is smooth. We derive modifications of the Kolmogorov–Arnold representation that transfer smoothness properties of the represented function to the outer function and can be well approximated by ReLU networks. It appears that instead of two hidden layers, a more natural interpretation of the Kolmogorov–Arnold representation is that of a deep neural network where most of the layers are required to approximate the interior function.},
  archive      = {J_NN},
  author       = {Johannes Schmidt-Hieber},
  doi          = {10.1016/j.neunet.2021.01.020},
  journal      = {Neural Networks},
  pages        = {119-126},
  shortjournal = {Neural Netw.},
  title        = {The Kolmogorov–Arnold representation theorem revisited},
  volume       = {137},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Stochastic configuration network ensembles with selective
base models. <em>NN</em>, <em>137</em>, 106–118. (<a
href="https://doi.org/10.1016/j.neunet.2021.01.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Studies have demonstrated that stochastic configuration networks (SCNs) have good potential for rapid data modeling because of their sufficient adequate learning power, which is theoretically guaranteed. Empirical studies have verified that the learner models produced by SCNs can usually achieve favorable test performance in practice but more in-depth theoretical analysis of their generalization power would be useful for constructing SCN-based ensemble models with enhanced generalization capacities. In particular, given a collection of independently developed SCN-based learner models, it is useful to select certain base learners that can potentially obtain preferable test results rather than considering all of the base models together, before simply taking their average in order to build an effective ensemble model. In this study, we propose a novel framework for building SCN ensembles by exploring key factors that might potentially affect the generalization performance of the base model. Under a mild assumption, we provide a comprehensive theoretical framework for examining a learner model’s generalization error , as well as formulating a novel indicator that contains measurement information for the training errors, output weights, and a hidden layer output matrix, which can be used by our proposed algorithm to find a subset of appropriate base models from a pool of randomized learner models. A toy example of one-dimensional function approximation, a case study for developing a predictive model for forecasting student learning performance, and two large-scale data sets were used in our experiments. The experimental results indicate that our proposed method has some remarkable advantages for building ensemble models.},
  archive      = {J_NN},
  author       = {Changqin Huang and Ming Li and Dianhui Wang},
  doi          = {10.1016/j.neunet.2021.01.011},
  journal      = {Neural Networks},
  pages        = {106-118},
  shortjournal = {Neural Netw.},
  title        = {Stochastic configuration network ensembles with selective base models},
  volume       = {137},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The role of individual neuron ion conductances in the
synchronization processes of neuron networks. <em>NN</em>, <em>137</em>,
97–105. (<a href="https://doi.org/10.1016/j.neunet.2021.01.019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The partial phase synchronization (sometimes called cooperation) of neurons is fundamental for the understanding of the complex behavior of the brain. The lack or the excess of synchronization can generate brain disorders like Parkinson’s disease and epilepsy. The phase synchronization phenomenon is strongly related to the regular or chaotic dynamics of individual neurons . The individual dynamics themselves are a function of the ion channel conductances , turning the conductances into important players in the process of neuron synchronized health depolarization/repolarization processes. It is well known that many diseases are related to alterations of the ion-channel conductance properties. To normalize their functioning, drugs are used to block or activate specific channels, changing their conductances. We investigate the synchronization process of a Hodgkin–Huxley-type neural network as a function of the values of the individual neuron conductances, showing the dynamics of the neurons must be taken into account in the synchronization process . Particular sets of conductances lead to non-chaotic individual neuron dynamics allowing synchronization states for very weak coupling and resulting in a non-monotonic transition to synchronized states, as the coupling strength among neurons is varied. On the other hand, a monotonic transition to synchronized states is observed for individual chaotic dynamics of the neurons. We conclude the analysis of the individual dynamics of isolated neurons allows the prediction of the synchronization process of the network. We provide alternative ways to achieve the desired network state (phase synchronized or desynchronized) without any changes in the synaptic current of neurons but making just small changes in the neuron ion-channel conductances. The mechanism behind the control is the close relation between ion-channel conductance and the regular or chaotic dynamics of neurons. Finally, we show that by changing at least two conductances simultaneously the control may be much more efficient since the second conductance makes the synchronization possible just by performing a small change in the first. The study presented here may have an impact on new drug development research.},
  archive      = {J_NN},
  author       = {B.R.R. Boaretto and C. Manchein and T.L. Prado and S.R. Lopes},
  doi          = {10.1016/j.neunet.2021.01.019},
  journal      = {Neural Networks},
  pages        = {97-105},
  shortjournal = {Neural Netw.},
  title        = {The role of individual neuron ion conductances in the synchronization processes of neuron networks},
  volume       = {137},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Nonclosedness of sets of neural networks in sobolev spaces.
<em>NN</em>, <em>137</em>, 85–96. (<a
href="https://doi.org/10.1016/j.neunet.2021.01.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We examine the closedness of sets of realized neural networks of a fixed architecture in Sobolev spaces . For an exactly m m -times differentiable activation function ρ ρ , we construct a sequence of neural networks ( Φ n ) n ∈ N (Φn)n∈N whose realizations converge in order- ( m − 1 ) (m−1) Sobolev norm to a function that cannot be realized exactly by a neural network. Thus, sets of realized neural networks are not closed in order- ( m − 1 ) (m−1) Sobolev spaces W m − 1 , p Wm−1,p for p ∈ [ 1 , ∞ ) p∈[1,∞) . We further show that these sets are not closed in W m , p Wm,p under slightly stronger conditions on the m m th derivative of ρ ρ . For a real analytic activation function , we show that sets of realized neural networks are not closed in W k , p Wk,p for any k ∈ N k∈N . The nonclosedness allows for approximation of non-network target functions with unbounded parameter growth. We partially characterize the rate of parameter growth for most activation functions by showing that a specific sequence of realized neural networks can approximate the activation function’s derivative with weights increasing inversely proportional to the L p Lp approximation error. Finally, we present experimental results showing that networks are capable of closely approximating non-network target functions with increasing parameters via training.},
  archive      = {J_NN},
  author       = {Scott Mahan and Emily J. King and Alex Cloninger},
  doi          = {10.1016/j.neunet.2021.01.007},
  journal      = {Neural Networks},
  pages        = {85-96},
  shortjournal = {Neural Netw.},
  title        = {Nonclosedness of sets of neural networks in sobolev spaces},
  volume       = {137},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A training algorithm with selectable search direction for
complex-valued feedforward neural networks. <em>NN</em>, <em>137</em>,
75–84. (<a href="https://doi.org/10.1016/j.neunet.2021.01.014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper focuses on presenting an efficient training algorithm for complex-valued feedforward neural networks by utilizing a tree structure . The basic idea of the proposed algorithm is that, by introducing a set of direction factors, distinctive search directions are available to be selected at each iteration such that the objective function is reduced as much as possible. Compared with some well-known training algorithms, one of the advantages of our algorithm is that the determination of search direction is of great flexibility and thus more accurate solution is obtained with faster convergence speed. Experimental simulations on pattern recognition, channel equalization and complex function approximation are provided to verify the effectiveness and applications of the proposed algorithm.},
  archive      = {J_NN},
  author       = {Zhongying Dong and He Huang},
  doi          = {10.1016/j.neunet.2021.01.014},
  journal      = {Neural Networks},
  pages        = {75-84},
  shortjournal = {Neural Netw.},
  title        = {A training algorithm with selectable search direction for complex-valued feedforward neural networks},
  volume       = {137},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Words as a window: Using word embeddings to explore the
learned representations of convolutional neural networks. <em>NN</em>,
<em>137</em>, 63–74. (<a
href="https://doi.org/10.1016/j.neunet.2020.12.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As deep neural net architectures minimize loss, they accumulate information in a hierarchy of learned representations that ultimately serve the network’s final goal. Different architectures tackle this problem in slightly different ways, but all create intermediate representational spaces built to inform their final prediction. Here we show that very different neural networks trained on two very different tasks build knowledge representations that display similar underlying patterns. Namely, we show that the representational spaces of several distributional semantic models bear a remarkable resemblance to several Convolutional Neural Network (CNN) architectures (trained for image classification). We use this information to explore the network behavior of CNNs (1) in pretrained models, (2) during training, and (3) during adversarial attacks . We use these findings to motivate several applications aimed at improving future research on CNNs. Our work illustrates the power of using one model to explore another, gives new insights into the function of CNN models, and provides a framework for others to perform similar analyses when developing new architectures. We show that one neural network model can provide a window into understanding another.},
  archive      = {J_NN},
  author       = {Dhanush Dharmaretnam and Chris Foster and Alona Fyshe},
  doi          = {10.1016/j.neunet.2020.12.009},
  journal      = {Neural Networks},
  pages        = {63-74},
  shortjournal = {Neural Netw.},
  title        = {Words as a window: Using word embeddings to explore the learned representations of convolutional neural networks},
  volume       = {137},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A conditional triplet loss for few-shot learning and its
application to image co-segmentation. <em>NN</em>, <em>137</em>, 54–62.
(<a href="https://doi.org/10.1016/j.neunet.2021.01.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot learning tries to solve the problems that suffer the limited number of samples. In this paper we present a novel conditional Triplet loss for solving few-shot problems using deep metric learning. While the conventional Triplet loss suffers the limitation of random sampling of triplets which leads to slow convergence in training process, our proposed network tries to distinguish between samples so that it improves the training speed. Our main contributions are two-fold. (i) We propose a conditional Triplet loss to train a deep Triplet network for deep metric embedding. The proposed Triplet loss employs a penalty–reward technique to enhance the convergence of standard Triplet loss. (ii) We improve the performance of the existing image co-segmentation model by replacing the conventional loss function by our proposed conditional Triplet loss. To demonstrate the performance of the proposed network, experiments carry out on MNIST and CIFAR. Simulation results are evaluated by AUC and Recall (sensitivity) and indicate that the proposed conditional Triplet network achieves higher accuracy in comparison to state-of-the-arts.},
  archive      = {J_NN},
  author       = {Daming Shi and Maysam Orouskhani and Yasin Orouskhani},
  doi          = {10.1016/j.neunet.2021.01.002},
  journal      = {Neural Networks},
  pages        = {54-62},
  shortjournal = {Neural Netw.},
  title        = {A conditional triplet loss for few-shot learning and its application to image co-segmentation},
  volume       = {137},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MaskLayer: Enabling scalable deep learning solutions by
training embedded feature sets. <em>NN</em>, <em>137</em>, 43–53. (<a
href="https://doi.org/10.1016/j.neunet.2021.01.015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning-based methods have shown to achieve excellent results in a variety of domains, however, some important assets are absent. Quality scalability is one of them. In this work, we introduce a novel and generic neural network layer, named MaskLayer. It can be integrated in any feedforward network, allowing quality scalability by design by creating embedded feature sets. These are obtained by imposing a specific structure of the feature vector during training. To further improve the performance, a masked optimizer and a balancing gradient rescaling approach are proposed. Our experiments show that the cost of introducing scalability using MaskLayer remains limited. In order to prove its generality and applicability, we integrated the proposed techniques in existing, non-scalable networks for point cloud compression and semantic hashing with excellent results. To the best of our knowledge, this is the first work presenting a generic solution able to achieve quality scalable results within the deep learning framework.},
  archive      = {J_NN},
  author       = {Remco Royen and Leon Denis and Quentin Bolsee and Pengpeng Hu and Adrian Munteanu},
  doi          = {10.1016/j.neunet.2021.01.015},
  journal      = {Neural Networks},
  pages        = {43-53},
  shortjournal = {Neural Netw.},
  title        = {MaskLayer: Enabling scalable deep learning solutions by training embedded feature sets},
  volume       = {137},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Extracting and inserting knowledge into stacked denoising
auto-encoders. <em>NN</em>, <em>137</em>, 31–42. (<a
href="https://doi.org/10.1016/j.neunet.2021.01.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks (DNNs) with a complex structure and multiple nonlinear processing units have achieved great successes for feature learning in image and visualization analysis. Due to interpretability of the “black box” problem in DNNs, however, there are still many obstacles to applications of DNNs in various real-world cases. This paper proposes a new DNN model, knowledge-based deep stacked denoising auto-encoders (KBSDAE), which inserts the knowledge (i.e., confidence and classification rules) into the deep network structure. This model not only can offer a good understanding of the representations learned by the deep network but also can produce an improvement in the learning performance of stacked denoising auto-encoder (SDAE). The knowledge discovery algorithm is proposed to extract confidence rules to interpret the layerwise network (i.e., denoising auto-encoder (DAE)). The symbolic language is developed to describe the deep network and shows that it is suitable for the representation of quantitative reasoning in a deep network. The confidence rule insertion to the deep network is able to produce an improvement in feature learning of DAEs. The classification rules extracted from the data offer a novel method for knowledge insertion to the classification layer of SDAE. The testing results of KBSDAE on various benchmark data indicate that the proposed method not only effectively extracts knowledge from the deep network, but also shows better feature learning performance than that of those typical DNNs (e.g., SDAE).},
  archive      = {J_NN},
  author       = {Jianbo Yu and Guoliang Liu},
  doi          = {10.1016/j.neunet.2021.01.010},
  journal      = {Neural Networks},
  pages        = {31-42},
  shortjournal = {Neural Netw.},
  title        = {Extracting and inserting knowledge into stacked denoising auto-encoders},
  volume       = {137},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Passive filter design for fractional-order quaternion-valued
neural networks with neutral delays and external disturbance.
<em>NN</em>, <em>137</em>, 18–30. (<a
href="https://doi.org/10.1016/j.neunet.2021.01.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem on passive filter design for fractional-order quaternion-valued neural networks (FOQVNNs) with neutral delays and external disturbance is considered in this paper. Without separating the FOQVNNs into two complex-valued neural networks (CVNNs) or the FOQVNNs into four real-valued neural networks (RVNNs), by constructing Lyapunov–Krasovskii functional and using inequality technique, the delay-independent and delay-dependent sufficient conditions presented as linear matrix inequality (LMI) to confirm the augmented filtering dynamic system to be stable and passive with an expected dissipation are derived. One numerical example with simulations is furnished to pledge the feasibility for the obtained theory results.},
  archive      = {J_NN},
  author       = {Qiankun Song and Sihan Chen and Zhenjiang Zhao and Yurong Liu and Fuad E. Alsaadi},
  doi          = {10.1016/j.neunet.2021.01.008},
  journal      = {Neural Networks},
  pages        = {18-30},
  shortjournal = {Neural Netw.},
  title        = {Passive filter design for fractional-order quaternion-valued neural networks with neutral delays and external disturbance},
  volume       = {137},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robustifying models against adversarial attacks by langevin
dynamics. <em>NN</em>, <em>137</em>, 1–17. (<a
href="https://doi.org/10.1016/j.neunet.2020.12.024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adversarial attacks on deep learning models have compromised their performance considerably. As remedies, a number of defense methods were proposed, which however, have been circumvented by newer and more sophisticated attacking strategies. In the midst of this ensuing arms race, the problem of robustness against adversarial attacks still remains a challenging task. This paper proposes a novel, simple yet effective defense strategy where off-manifold adversarial samples are driven towards high density regions of the data generating distribution of the (unknown) target class by the Metropolis-adjusted Langevin algorithm (MALA) with perceptual boundary taken into account . To achieve this task, we introduce a generative model of the conditional distribution of the inputs given labels that can be learned through a supervised Denoising Autoencoder (sDAE) in alignment with a discriminative classifier. Our algorithm, called MALA for DEfense (MALADE), is equipped with significant dispersion—projection is distributed broadly. This prevents white box attacks from accurately aligning the input to create an adversarial sample effectively. MALADE is applicable to any existing classifier, providing robust defense as well as off-manifold sample detection. In our experiments, MALADE exhibited state-of-the-art performance against various elaborate attacking strategies.},
  archive      = {J_NN},
  author       = {Vignesh Srinivasan and Csaba Rohrer and Arturo Marban and Klaus-Robert Müller and Wojciech Samek and Shinichi Nakajima},
  doi          = {10.1016/j.neunet.2020.12.024},
  journal      = {Neural Networks},
  pages        = {1-17},
  shortjournal = {Neural Netw.},
  title        = {Robustifying models against adversarial attacks by langevin dynamics},
  volume       = {137},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021i). INN/ENNS/JNNS - membership applic. form. <em>NN</em>,
<em>136</em>, II. (<a
href="https://doi.org/10.1016/S0893-6080(21)00061-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NN},
  doi          = {10.1016/S0893-6080(21)00061-7},
  journal      = {Neural Networks},
  pages        = {II},
  shortjournal = {Neural Netw.},
  title        = {INN/ENNS/JNNS - membership applic. form},
  volume       = {136},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021i). Current events. <em>NN</em>, <em>136</em>, I. (<a
href="https://doi.org/10.1016/S0893-6080(21)00060-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NN},
  doi          = {10.1016/S0893-6080(21)00060-5},
  journal      = {Neural Networks},
  pages        = {I},
  shortjournal = {Neural Netw.},
  title        = {Current events},
  volume       = {136},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust facial landmark detection by cross-order
cross-semantic deep network. <em>NN</em>, <em>136</em>, 233–243. (<a
href="https://doi.org/10.1016/j.neunet.2020.11.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, convolutional neural networks (CNNs)-based facial landmark detection methods have achieved great success. However, most of existing CNN-based facial landmark detection methods have not attempted to activate multiple correlated facial parts and learn different semantic features from them that they can not accurately model the relationships among the local details and can not fully explore more discriminative and fine semantic features , thus they suffer from partial occlusions and large pose variations. To address these problems, we propose a cross-order cross-semantic deep network (CCDN) to boost the semantic features learning for robust facial landmark detection. Specifically, a cross-order two-squeeze multi-excitation (CTM) module is proposed to introduce the cross-order channel correlations for more discriminative representations learning and multiple attention-specific part activation. Moreover, a novel cross-order cross-semantic (COCS) regularizer is designed to drive the network to learn cross-order cross-semantic features from different activation for facial landmark detection. It is interesting to show that by integrating the CTM module and COCS regularizer, the proposed CCDN can effectively activate and learn more fine and complementary cross-order cross-semantic features to improve the accuracy of facial landmark detection under extremely challenging scenarios. Experimental results on challenging benchmark datasets demonstrate the superiority of our CCDN over state-of-the-art facial landmark detection methods.},
  archive      = {J_NN},
  author       = {Jun Wan and Zhihui Lai and Linlin Shen and Jie Zhou and Can Gao and Gang Xiao and Xianxu Hou},
  doi          = {10.1016/j.neunet.2020.11.001},
  journal      = {Neural Networks},
  pages        = {233-243},
  shortjournal = {Neural Netw.},
  title        = {Robust facial landmark detection by cross-order cross-semantic deep network},
  volume       = {136},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Low rank regularization: A review. <em>NN</em>,
<em>136</em>, 218–232. (<a
href="https://doi.org/10.1016/j.neunet.2020.09.021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low Rank Regularization (LRR), in essence, involves introducing a low rank or approximately low rank assumption to target we aim to learn, which has achieved great success in many data analysis tasks. Over the last decade, much progress has been made in theories and applications. Nevertheless, the intersection between these two lines is rare. In order to construct a bridge between practical applications and theoretical studies, in this paper we provide a comprehensive survey for LRR. Specifically, we first review the recent advances in two issues that all LRR models are faced with: ( 1 ) (1) rank-norm relaxation, which seeks to find a relaxation to replace the rank minimization problem ; ( 2 ) (2) model optimization, which seeks to use an efficient optimization algorithm to solve the relaxed LRR models. For the first issue, we provide a detailed summarization for various relaxation functions and conclude that the non-convex relaxations can alleviate the punishment bias problem compared with the convex relaxations . For the second issue, we summarize the representative optimization algorithms used in previous studies, and analyze their advantages and disadvantages. As the main goal of this paper is to promote the application of non-convex relaxations, we conduct extensive experiments to compare different relaxation functions. The experimental results demonstrate that the non-convex relaxations generally provide a large advantage over the convex relaxations . Such a result is inspiring for further improving the performance of existing LRR models.},
  archive      = {J_NN},
  author       = {Zhanxuan Hu and Feiping Nie and Rong Wang and Xuelong Li},
  doi          = {10.1016/j.neunet.2020.09.021},
  journal      = {Neural Networks},
  pages        = {218-232},
  shortjournal = {Neural Netw.},
  title        = {Low rank regularization: A review},
  volume       = {136},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Image manipulation with natural language using two-sided
attentive conditional generative adversarial network. <em>NN</em>,
<em>136</em>, 207–217. (<a
href="https://doi.org/10.1016/j.neunet.2020.09.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Altering the content of an image with photo editing tools is a tedious task for an inexperienced user, especially, when modifying the visual attributes of a specific object in an image without affecting other constituents such as background etc. To simplify the process of image manipulation and to provide more control to users, it is better to utilize a simpler interface like natural language. It also enables to semantically modify parts of an image according to the given text. Therefore, in this paper, we address the challenge of manipulating images using natural language descriptions. We propose the Two-sidEd Attentive conditional Generative Adversarial Network (TEA-cGAN) to generate semantically manipulated images. TEA-cGAN’s contribution is seen as two-fold. The first contribution aims to attend locations that need to be modified during generation. It introduces two types of architectures that provide fine-grained attention both in the generator and discriminator of Generative Adversarial Network (GAN). To be specific, the first one i.e., the Single-scale architecture used in the generator focuses to modify only the text-relevant regions in an image and leaves other regions untouched. While the second one i.e., Multi-scale architecture further extended this idea by taking the different scales of image features into account. The second contribution purpose is to generate higher resolution images (e.g., 256 × 256) as they provide better quality and stability. Quantitative and qualitative experiments conducted on CUB and Oxford-102 datasets confirm that TEA-cGAN different scale architectures outperform existing methods while generating 128 × 128 resolution images including generating higher resolution image i.e., 256 × 256.},
  archive      = {J_NN},
  author       = {Dawei Zhu and Aditya Mogadala and Dietrich Klakow},
  doi          = {10.1016/j.neunet.2020.09.002},
  journal      = {Neural Networks},
  pages        = {207-217},
  shortjournal = {Neural Netw.},
  title        = {Image manipulation with natural language using two-sided attentive conditional generative adversarial network},
  volume       = {136},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A neurodynamic optimization approach to supervised feature
selection via fractional programming. <em>NN</em>, <em>136</em>,
194–206. (<a
href="https://doi.org/10.1016/j.neunet.2021.01.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection is an important issue in machine learning and data mining . Most existing feature selection methods are greedy in nature thus are prone to sub-optimality. Though some global feature selection methods based on unsupervised redundancy minimization can potentiate clustering performance improvements, their efficacy for classification may be limited. In this paper, a neurodynamics-based holistic feature selection approach is proposed via feature redundancy minimization and relevance maximization. An information-theoretic similarity coefficient matrix is defined based on multi-information and entropy to measure feature redundancy with respect to class labels. Supervised feature selection is formulated as a fractional programming problem based on the similarity coefficients. A neurodynamic approach based on two one-layer recurrent neural networks is developed for solving the formulated feature selection problem. Experimental results with eight benchmark datasets are discussed to demonstrate the global convergence of the neural networks and superiority of the proposed neurodynamic approach to several existing feature selection methods in terms of classification accuracy , precision, recall, and F-measure.},
  archive      = {J_NN},
  author       = {Yadi Wang and Xiaoping Li and Jun Wang},
  doi          = {10.1016/j.neunet.2021.01.004},
  journal      = {Neural Networks},
  pages        = {194-206},
  shortjournal = {Neural Netw.},
  title        = {A neurodynamic optimization approach to supervised feature selection via fractional programming},
  volume       = {136},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A bioinspired angular velocity decoding neural network model
for visually guided flights. <em>NN</em>, <em>136</em>, 180–193. (<a
href="https://doi.org/10.1016/j.neunet.2020.12.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efficient and robust motion perception systems are important pre-requisites for achieving visually guided flights in future micro air vehicles. As a source of inspiration, the visual neural networks of flying insects such as honeybee and Drosophila provide ideal examples on which to base artificial motion perception models. In this paper, we have used this approach to develop a novel method that solves the fundamental problem of estimating angular velocity for visually guided flights. Compared with previous models, our elementary motion detector (EMD) based model uses a separate texture estimation pathway to effectively decode angular velocity, and demonstrates considerable independence from the spatial frequency and contrast of the gratings. Using the Unity development platform the model is further tested for tunnel centering and terrain following paradigms in order to reproduce the visually guided flight behaviors of honeybees. In a series of controlled trials, the virtual bee utilizes the proposed angular velocity control schemes to accurately navigate through a patterned tunnel, maintaining a suitable distance from the undulating textured terrain. The results are consistent with both neuron spike recordings and behavioral path recordings of real honeybees, thereby demonstrating the model’s potential for implementation in micro air vehicles which have only visual sensors.},
  archive      = {J_NN},
  author       = {Huatian Wang and Qinbing Fu and Hongxin Wang and Paul Baxter and Jigen Peng and Shigang Yue},
  doi          = {10.1016/j.neunet.2020.12.008},
  journal      = {Neural Networks},
  pages        = {180-193},
  shortjournal = {Neural Netw.},
  title        = {A bioinspired angular velocity decoding neural network model for visually guided flights},
  volume       = {136},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). How to teach neural networks to mesh: Application on 2-d
simplicial contours. <em>NN</em>, <em>136</em>, 152–179. (<a
href="https://doi.org/10.1016/j.neunet.2020.12.019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A machine learning meshing scheme for the generation of 2-D simplicial meshes is proposed based on the predictions of neural networks . The data extracted from meshed contours are utilized to train neural networks which are used to approximate the number of vertices to be inserted inside the contour cavity, their location, and connectivity. The accuracy of the scheme is evaluated by comparing the quality of the mesh generated by the neural networks with that generated by a reference mesher. Based on an element quality metric, after conducting tests on contours for a various number of edges, the results show a maximum average deviation of 15.2\% on the mean quality and 27 . 3\% 27.3\% on the minimum quality between the elements of the meshes generated by the scheme and the ones generated from the reference mesher; the scheme is able to produce good quality meshes that are suitable for meshing purposes. The meshing scheme is also applied to generate larger scale meshes with a recursive implementation . The findings encourage the adaption of the scheme for 3-D mesh generation.},
  archive      = {J_NN},
  author       = {Alexis Papagiannopoulos and Pascal Clausen and François Avellan},
  doi          = {10.1016/j.neunet.2020.12.019},
  journal      = {Neural Networks},
  pages        = {152-179},
  shortjournal = {Neural Netw.},
  title        = {How to teach neural networks to mesh: Application on 2-D simplicial contours},
  volume       = {136},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Creating and concentrating quantum resource states in noisy
environments using a quantum neural network. <em>NN</em>, <em>136</em>,
141–151. (<a
href="https://doi.org/10.1016/j.neunet.2021.01.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantum information processing tasks require exotic quantum states as a prerequisite. They are usually prepared with many different methods tailored to the specific resource state. Here we provide a versatile unified state preparation scheme based on a driven quantum network composed of randomly-coupled fermionic nodes. The output of such a system is then superposed with the help of linear mixing where weights and phases are trained in order to obtain desired output quantum states . We explicitly show that our method is robust and can be utilized to create almost perfect maximally entangled, NOON, W, cluster, and discorded states. Furthermore, the treatment includes energy decay in the system as well as dephasing and depolarization. Under these noisy conditions we show that the target states are achieved with high fidelity by tuning controllable parameters and providing sufficient strength to the driving of the quantum network . Finally, in very noisy systems, where noise is comparable to the driving strength, we show how to concentrate entanglement by mixing more states in a larger network.},
  archive      = {J_NN},
  author       = {Tanjung Krisnanda and Sanjib Ghosh and Tomasz Paterek and Timothy C.H. Liew},
  doi          = {10.1016/j.neunet.2021.01.003},
  journal      = {Neural Networks},
  pages        = {141-151},
  shortjournal = {Neural Netw.},
  title        = {Creating and concentrating quantum resource states in noisy environments using a quantum neural network},
  volume       = {136},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-scale attention convolutional neural network for time
series classification. <em>NN</em>, <em>136</em>, 126–140. (<a
href="https://doi.org/10.1016/j.neunet.2021.01.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid increase of data availability, time series classification (TSC) has arisen in a wide range of fields and drawn great attention of researchers. Recently, hundreds of TSC approaches have been developed, which can be classified into two categories: traditional and deep learning based TSC methods. However, it remains challenging to improve accuracy and model generalization ability . Therefore, we investigate a novel end-to-end model based on deep learning named as Multi-scale Attention Convolutional Neural Network (MACNN) to solve the TSC problem. We first apply the multi-scale convolution to capture different scales of information along the time axis by generating different scales of feature maps. Then an attention mechanism is proposed to enhance useful feature maps and suppress less useful ones by learning the importance of each feature map automatically. MACNN addresses the limitation of single-scale convolution and equal weight feature maps. We conduct a comprehensive evaluation of 85 UCR standard datasets and the experimental results show that our proposed approach achieves the best performance and outperforms the other traditional and deep learning based methods by a large margin.},
  archive      = {J_NN},
  author       = {Wei Chen and Ke Shi},
  doi          = {10.1016/j.neunet.2021.01.001},
  journal      = {Neural Networks},
  pages        = {126-140},
  shortjournal = {Neural Netw.},
  title        = {Multi-scale attention convolutional neural network for time series classification},
  volume       = {136},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A dual-dimer method for training physics-constrained neural
networks with minimax architecture. <em>NN</em>, <em>136</em>, 112–125.
(<a href="https://doi.org/10.1016/j.neunet.2020.12.028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data sparsity is a common issue to train machine learning tools such as neural networks for engineering and scientific applications, where experiments and simulations are expensive. Recently physics-constrained neural networks (PCNNs) were developed to reduce the required amount of training data. However, the weights of different losses from data and physical constraints are adjusted empirically in PCNNs. In this paper, a new physics-constrained neural network with the minimax architecture (PCNN-MM) is proposed so that the weights of different losses can be adjusted systematically. The training of the PCNN-MM is searching the high-order saddle points of the objective function. A novel saddle point search algorithm called Dual-Dimer method is developed. It is demonstrated that the Dual-Dimer method is computationally more efficient than the gradient descent ascent method for nonconvex–nonconcave functions and provides additional eigenvalue information to verify search results. A heat transfer example also shows that the convergence of PCNN-MMs is faster than that of traditional PCNNs.},
  archive      = {J_NN},
  author       = {Dehao Liu and Yan Wang},
  doi          = {10.1016/j.neunet.2020.12.028},
  journal      = {Neural Networks},
  pages        = {112-125},
  shortjournal = {Neural Netw.},
  title        = {A dual-dimer method for training physics-constrained neural networks with minimax architecture},
  volume       = {136},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Steganographer detection via a similarity accumulation graph
convolutional network. <em>NN</em>, <em>136</em>, 97–111. (<a
href="https://doi.org/10.1016/j.neunet.2020.12.026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Steganographer detection aims to identify guilty users who conceal secret information in a number of images for the purpose of covert communication in social networks. Existing steganographer detection methods focus on designing discriminative features but do not explore relationship between image features or effectively represent users based on features. In these methods, each image is recognized as an equivalent, and each user is regarded as the distribution of all images shared by the corresponding user. However, the nuances of guilty users and innocent users are difficult to recognize with this flattened method. In this paper, the steganographer detection task is formulated as a multiple-instance learning problem in which each user is considered to be a bag, and the shared images are multiple instances in the bag. Specifically, we propose a similarity accumulation graph convolutional network to represent each user as a complete weighted graph , in which each node corresponds to features extracted from an image and the weight of an edge is the similarity between each pair of images. The constructed unit in the network can take advantage of the relationships between instances so that common patterns of positive instances can be enhanced via similarity accumulations. Instead of operating on a fixed original graph, we propose a novel strategy for reconstructing and pooling graphs based on node features to iteratively operate multiple convolutions . This strategy can effectively address oversmoothing problems that render nodes indistinguishable although they share different instance-level labels. Compared with the state-of-the-art method and other representative graph-based models, the proposed framework demonstrates its effectiveness and reliability ability across image domains, even in the context of large-scale social media scenarios. Moreover, the experimental results also indicate that the proposed network can be generalized to other multiple-instance learning problems.},
  archive      = {J_NN},
  author       = {Zhi Zhang and Mingjie Zheng and Sheng-hua Zhong and Yan Liu},
  doi          = {10.1016/j.neunet.2020.12.026},
  journal      = {Neural Networks},
  pages        = {97-111},
  shortjournal = {Neural Netw.},
  title        = {Steganographer detection via a similarity accumulation graph convolutional network},
  volume       = {136},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Stacked DeBERT: All attention in incomplete data for text
classification. <em>NN</em>, <em>136</em>, 87–96. (<a
href="https://doi.org/10.1016/j.neunet.2020.12.018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose Stacked DeBERT , short for Stacked De noising B idirectional E ncoder R epresentations from T ransformers. This novel model improves robustness in incomplete data, when compared to existing systems, by designing a novel encoding scheme in BERT, a powerful language representation model solely based on attention mechanisms . Incomplete data in natural language processing refer to text with missing or incorrect words, and its presence can hinder the performance of current models that were not implemented to withstand such noises, but must still perform well even under duress. This is due to the fact that current approaches are built for and trained with clean and complete data, and thus are not able to extract features that can adequately represent incomplete data. Our proposed approach consists of obtaining intermediate input representations by applying an embedding layer to the input tokens followed by vanilla transformers. These intermediate features are given as input to novel denoising transformers which are responsible for obtaining richer input representations. The proposed approach takes advantage of stacks of multilayer perceptrons for the reconstruction of missing words’ embeddings by extracting more abstract and meaningful hidden feature vectors, and bidirectional transformers for improved embedding representation. We consider two datasets for training and evaluation: the Chatbot Natural Language Understanding Evaluation Corpus and Kaggle’s Twitter Sentiment Corpus. Our model shows improved F1-scores and better robustness in informal/incorrect texts present in tweets and in texts with Speech-to-Text error in the sentiment and intent classification tasks . 1},
  archive      = {J_NN},
  author       = {Gwenaelle Cunha Sergio and Minho Lee},
  doi          = {10.1016/j.neunet.2020.12.018},
  journal      = {Neural Networks},
  pages        = {87-96},
  shortjournal = {Neural Netw.},
  title        = {Stacked DeBERT: All attention in incomplete data for text classification},
  volume       = {136},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Chaos may enhance expressivity in cerebellar granular layer.
<em>NN</em>, <em>136</em>, 72–86. (<a
href="https://doi.org/10.1016/j.neunet.2020.12.020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent evidence suggests that Golgi cells in the cerebellar granular layer are densely connected to each other with massive gap junctions . Here, we propose that the massive gap junctions between the Golgi cells contribute to the representational complexity of the granular layer of the cerebellum by inducing chaotic dynamics. We construct a model of cerebellar granular layer with diffusion coupling through gap junctions between the Golgi cells, and evaluate the representational capability of the network with the reservoir computing framework. First, we show that the chaotic dynamics induced by diffusion coupling results in complex output patterns containing a wide range of frequency components. Second, the long non-recursive time series of the reservoir represents the passage of time from an external input. These properties of the reservoir enable mapping different spatial inputs into different temporal patterns.},
  archive      = {J_NN},
  author       = {Keita Tokuda and Naoya Fujiwara and Akihito Sudo and Yuichi Katori},
  doi          = {10.1016/j.neunet.2020.12.020},
  journal      = {Neural Networks},
  pages        = {72-86},
  shortjournal = {Neural Netw.},
  title        = {Chaos may enhance expressivity in cerebellar granular layer},
  volume       = {136},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). T-soft update of target network for deep reinforcement
learning. <em>NN</em>, <em>136</em>, 63–71. (<a
href="https://doi.org/10.1016/j.neunet.2020.12.023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a new robust update rule of target network for deep reinforcement learning (DRL), to replace the conventional update rule, given as an exponential moving average. The target network is for smoothly generating the reference signals for a main network in DRL, thereby reducing learning variance. The problem with its conventional update rule is the fact that all the parameters are smoothly copied with the same speed from the main network, even when some of them are trying to update toward the wrong directions. This behavior increases the risk of generating the wrong reference signals. Although slowing down the overall update speed is a naive way to mitigate wrong updates, it would decrease learning speed. To robustly update the parameters while keeping learning speed, a t-soft update method, which is inspired by Student-t distribution, is derived with reference to the analogy between the exponential moving average and the normal distribution . Through the analysis of the derived t-soft update, we show that it takes over the properties of the Student-t distribution. Specifically, with a heavy-tailed property of the Student-t distribution, the t-soft update automatically excludes extreme updates that differ from past experiences. In addition, when the updates are similar to the past experiences, it can mitigate the learning delay by increasing the amount of updates. In PyBullet robotics simulations for DRL, an online actor–critic algorithm with the t-soft update outperformed the conventional methods in terms of the obtained return and/or its variance. From the training process by the t-soft update, we found that the t-soft update is globally consistent with the standard soft update, and the update rates are locally adjusted for acceleration or suppression.},
  archive      = {J_NN},
  author       = {Taisuke Kobayashi and Wendyam Eric Lionel Ilboudo},
  doi          = {10.1016/j.neunet.2020.12.023},
  journal      = {Neural Networks},
  pages        = {63-71},
  shortjournal = {Neural Netw.},
  title        = {T-soft update of target network for deep reinforcement learning},
  volume       = {136},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Supervised and semi-supervised probabilistic learning with
deep neural networks for concurrent process-quality monitoring.
<em>NN</em>, <em>136</em>, 54–62. (<a
href="https://doi.org/10.1016/j.neunet.2020.11.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Concurrent process-quality monitoring helps discover quality-relevant process anomalies and quality-irrelevant process anomalies. It especially works well in chemical plants with faults that cause quality problems. Traditional monitoring strategies are limitedly applied in chemical plants because quality targets in training data are insufficient. It is hard for inflexible models to fully capture the strongly nonlinear process-quality correlations. Also, deterministic models are mapped from process variables to qualities without any consideration of uncertainties. Simultaneously, a slow sampling rate for quality variables is ubiquitous in chemical plants since a product quality test is often time-consuming and expensive. Motivated by these limitations, this paper proposes a new concurrent process-quality monitoring scheme based on a probabilistic generative deep learning model developed from variational autoencoder . The supervised model is firstly developed and then the semi-supervised version is extended to solve the issue of missing targets. Especially, the semi-supervised learning algorithm is accomplished with an optimal parameter estimation in the light of maximum likelihood principle and no any hyperparameters are introduced. Two case studies validate that the proposed method effectively outperforms the other comparative methods in concurrent process-quality monitoring.},
  archive      = {J_NN},
  author       = {Kai Wang and Xiaofeng Yuan and Junghui Chen and Yalin Wang},
  doi          = {10.1016/j.neunet.2020.11.006},
  journal      = {Neural Networks},
  pages        = {54-62},
  shortjournal = {Neural Netw.},
  title        = {Supervised and semi-supervised probabilistic learning with deep neural networks for concurrent process-quality monitoring},
  volume       = {136},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Smooth dendrite morphological neurons. <em>NN</em>,
<em>136</em>, 40–53. (<a
href="https://doi.org/10.1016/j.neunet.2020.12.021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A typical feature of hyperbox-based dendrite morphological neurons (DMN) is the generation of sharp and rough decision boundaries that inaccurately track the distribution shape of classes of patterns. This feature is because the minimum and maximum activation functions force the decision boundaries to match the faces of the hyperboxes. To improve the DMN response, we introduce a dendritic model that uses smooth maximum and minimum functions to soften the decision boundaries. The classification performance assessment is conducted on nine synthetic and 28 real-world datasets. Based on the experimental results, we demonstrate that the smooth activation functions improve the generalization capacity of DMN. The proposed approach is competitive with four machine learning techniques , namely, Multilayer Perceptron , Radial Basis Function Network, Support Vector Machine , and Nearest Neighbor algorithm. Besides, the computational complexity of DMN training is lower than MLP and SVM classifiers.},
  archive      = {J_NN},
  author       = {Wilfrido Gómez-Flores and Humberto Sossa},
  doi          = {10.1016/j.neunet.2020.12.021},
  journal      = {Neural Networks},
  pages        = {40-53},
  shortjournal = {Neural Netw.},
  title        = {Smooth dendrite morphological neurons},
  volume       = {136},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Quantization friendly MobileNet (QF-MobileNet) architecture
for vision based applications on embedded platforms. <em>NN</em>,
<em>136</em>, 28–39. (<a
href="https://doi.org/10.1016/j.neunet.2020.12.022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep Neural Networks (DNNs) have become popular for various applications in the domain of image and computer vision due to their well-established performance attributes. DNN algorithms involve powerful multilevel feature extractions resulting in an extensive range of parameters and memory footprints . However, memory bandwidth requirements, memory footprint and the associated power consumption of models are issues to be addressed to deploy DNN models on embedded platforms for real time vision-based applications. We present an optimized DNN model for memory and accuracy for vision-based applications on embedded platforms. In this paper we propose Quantization Friendly MobileNet (QF-MobileNet) architecture. The architecture is optimized for inference accuracy and reduced resource utilization. The optimization is obtained by addressing the redundancy and quantization loss of the existing baseline MobileNet architectures. We verify and validate the performance of the QF-MobileNet architecture for image classification task on the ImageNet dataset . The proposed model is tested for inference accuracy and resource utilization and compared to the baseline MobileNet architecture. The inference accuracy of the proposed QF-MobileNetV2 float model attained 73.36\% and the quantized model has 69.51\%. The MobileNetV3 float model attained an inference accuracy of 68.75\% and the quantized model has 67.5\% respectively. The proposed model saves 33\% of time complexity for QF-MobileNetV2 and QF-MobileNetV3 models against the baseline models . The QF-MobileNet also showed optimized resource utilization with 32\% fewer tunable parameters, 30\% fewer MAC’s operations per image and reduced inference quantization loss by approximately 5\% compared to the baseline models . The model is ported onto the android application using TensorFlow API. The android application performs inference on the native devices viz. smartphones, tablets and handheld devices. Future work is focused on introducing channel-wise and layer-wise quantization schemes to the proposed model. We intend to explore quantization aware training of DNN algorithms to achieve optimized resource utilization and inference accuracy.},
  archive      = {J_NN},
  author       = {Uday Kulkarni and Meena S.M. and Sunil V. Gurlahosur and Gopal Bhogar},
  doi          = {10.1016/j.neunet.2020.12.022},
  journal      = {Neural Networks},
  pages        = {28-39},
  shortjournal = {Neural Netw.},
  title        = {Quantization friendly MobileNet (QF-MobileNet) architecture for vision based applications on embedded platforms},
  volume       = {136},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Μ-law SGAN for generating spectra with more details in
speech enhancement. <em>NN</em>, <em>136</em>, 17–27. (<a
href="https://doi.org/10.1016/j.neunet.2020.12.017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of monaural speech enhancement is to separate clean speech from noisy speech. Recently, many studies have employed generative adversarial networks (GAN) to deal with monaural speech enhancement tasks. When using generative adversarial networks for this task, the output of the generator is a speech waveform or a spectrum, such as a magnitude spectrum, a mel-spectrum or a complex-valued spectrum. The spectra generated by current speech enhancement methods in the time–frequency domain usually lack details, such as consonants and harmonics with low energy. In this paper, we propose a new type of adversarial training framework for spectrum generation, named μ μ -law spectrum generative adversarial networks ( μ μ -law SGAN). We introduce a trainable μ μ -law spectrum compression layer (USCL) into the proposed discriminator to compress the dynamic range of the spectrum. As a result, the compressed spectrum can display more detailed information. In addition, we use the spectrum transformed by USCL to regularize the generator’s training, so that the generator can pay more attention to the details of the spectrum. Experimental results on the open dataset Voice Bank + DEMAND show that μ μ -law SGAN is an effective generative adversarial architecture for speech enhancement. Moreover, visual spectrogram analysis suggests that μ μ -law SGAN pays more attention to the enhancement of low energy harmonics and consonants.},
  archive      = {J_NN},
  author       = {Hongfeng Li and Yanyan Xu and Dengfeng Ke and Kaile Su},
  doi          = {10.1016/j.neunet.2020.12.017},
  journal      = {Neural Networks},
  pages        = {17-27},
  shortjournal = {Neural Netw.},
  title        = {μ-law SGAN for generating spectra with more details in speech enhancement},
  volume       = {136},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An enhanced approach to the robust discriminant analysis and
class sparsity based embedding. <em>NN</em>, <em>136</em>, 11–16. (<a
href="https://doi.org/10.1016/j.neunet.2020.12.025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent times, feature extraction attracted much attention in machine learning and pattern recognition fields. This paper extends and improves a scheme for linear feature extraction that can be used in supervised multi-class classification problems. Inspired by recent frameworks for robust sparse LDA and Inter-class sparsity , we propose a unifying criterion able to retain the advantages of these two powerful linear discriminant methods. We introduce an iterative alternating minimization scheme in order to estimate the linear transformation and the orthogonal matrix . The linear transformation is efficiently updated via the steepest descent gradient technique. The proposed framework is generic in the sense that it allows the combination and tuning of other linear discriminant embedding methods. We used our proposed method to fine tune the linear solutions delivered by two recent linear methods: RSLDA and RDA_FSIS. Experiments have been conducted on public image datasets of different types including objects, faces, and digits. The proposed framework compared favorably with several competing methods.},
  archive      = {J_NN},
  author       = {A. Khoder and F. Dornaika},
  doi          = {10.1016/j.neunet.2020.12.025},
  journal      = {Neural Networks},
  pages        = {11-16},
  shortjournal = {Neural Netw.},
  title        = {An enhanced approach to the robust discriminant analysis and class sparsity based embedding},
  volume       = {136},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptive transfer learning for EEG motor imagery
classification with deep convolutional neural network. <em>NN</em>,
<em>136</em>, 1–10. (<a
href="https://doi.org/10.1016/j.neunet.2020.12.013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, deep learning has emerged as a powerful tool for developing Brain–Computer Interface (BCI) systems. However, for deep learning models trained entirely on the data from a specific individual, the performance increase has only been marginal owing to the limited availability of subject-specific data. To overcome this, many transfer-based approaches have been proposed, in which deep networks are trained using pre-existing data from other subjects and evaluated on new target subjects. This mode of transfer learning however faces the challenge of substantial inter-subject variability in brain data. Addressing this, in this paper, we propose 5 schemes for adaptation of a deep convolutional neural network (CNN) based electroencephalography (EEG)-BCI system for decoding hand motor imagery (MI). Each scheme fine-tunes an extensively trained, pre-trained model and adapt it to enhance the evaluation performance on a target subject. We report the highest subject-independent performance with an average ( N = 54 N=54 ) accuracy of 84.19\% ( ± 9 . 98\% ±9.98\% ) for two-class motor imagery, while the best accuracy on this dataset is 74.15\% ( ± 15 . 83\% ±15.83\% ) in the literature. Further, we obtain a statistically significant improvement ( p = 0 . 005 p=0.005 ) in classification using the proposed adaptation schemes compared to the baseline subject-independent model.},
  archive      = {J_NN},
  author       = {Kaishuo Zhang and Neethu Robinson and Seong-Whan Lee and Cuntai Guan},
  doi          = {10.1016/j.neunet.2020.12.013},
  journal      = {Neural Networks},
  pages        = {1-10},
  shortjournal = {Neural Netw.},
  title        = {Adaptive transfer learning for EEG motor imagery classification with deep convolutional neural network},
  volume       = {136},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021j). INN/ENNS/JNNS - membership applic. form. <em>NN</em>,
<em>135</em>, II. (<a
href="https://doi.org/10.1016/S0893-6080(21)00005-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NN},
  doi          = {10.1016/S0893-6080(21)00005-8},
  journal      = {Neural Networks},
  pages        = {II},
  shortjournal = {Neural Netw.},
  title        = {INN/ENNS/JNNS - membership applic. form},
  volume       = {135},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021j). Current events. <em>NN</em>, <em>135</em>, I. (<a
href="https://doi.org/10.1016/S0893-6080(21)00004-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NN},
  doi          = {10.1016/S0893-6080(21)00004-6},
  journal      = {Neural Networks},
  pages        = {I},
  shortjournal = {Neural Netw.},
  title        = {Current events},
  volume       = {135},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Finite-time cluster synchronization in complex-variable
networks with fractional-order and nonlinear coupling. <em>NN</em>,
<em>135</em>, 212–224. (<a
href="https://doi.org/10.1016/j.neunet.2020.12.015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper is primarily concentrated on finite-time cluster synchronization of fractional-order complex-variable networks with nonlinear coupling by utilizing the non-decomposition method. Firstly, two control strategies are designed which are relevant to complex-valued sign functions. Thereafter, by employing fractional-order stability theory and complex function theory, several criteria are deduced to ensure finite-time cluster synchronization under the framework within a new norm consisting of absolute values for real and imaginary components . Furthermore, the setting time is effectively estimated based on some significant properties of fractional-order Caputo derivation and Mittag-Leffler functions. Lastly, two numerical examples are given to verify the effectiveness of theoretical results.},
  archive      = {J_NN},
  author       = {Shuai Yang and Cheng Hu and Juan Yu and Haijun Jiang},
  doi          = {10.1016/j.neunet.2020.12.015},
  journal      = {Neural Networks},
  pages        = {212-224},
  shortjournal = {Neural Netw.},
  title        = {Finite-time cluster synchronization in complex-variable networks with fractional-order and nonlinear coupling},
  volume       = {135},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Self-organized operational neural networks for severe image
restoration problems. <em>NN</em>, <em>135</em>, 201–211. (<a
href="https://doi.org/10.1016/j.neunet.2020.12.014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Discriminative learning based on convolutional neural networks (CNNs) aims to perform image restoration by learning from training examples of noisy-clean image pairs. It has become the go-to methodology for tackling image restoration and has outperformed the traditional non-local class of methods. However, the top-performing networks are generally composed of many convolutional layers and hundreds of neurons, with trainable parameters in excess of several million. We claim that this is due to the inherently linear nature of convolution-based transformation, which is inadequate for handling severe restoration problems. Recently, a non-linear generalization of CNNs, called the operational neural networks (ONN), has been shown to outperform CNN on AWGN denoising . However, its formulation is burdened by a fixed collection of well-known non-linear operators and an exhaustive search to find the best possible configuration for a given architecture, whose efficacy is further limited by a fixed output layer operator assignment. In this study, we leverage the Taylor series-based function approximation to propose a self-organizing variant of ONNs, Self-ONNs, for image restoration, which synthesizes novel nodal transformations on-the-fly as part of the learning process, thus eliminating the need for redundant training runs for operator search. In addition, it enables a finer level of operator heterogeneity by diversifying individual connections of the receptive fields and weights. We perform a series of extensive ablation experiments across three severe image restoration tasks. Even when a strict equivalence of learnable parameters is imposed, Self-ONNs surpass CNNs by a considerable margin across all problems, improving the generalization performance by up to 3 dB in terms of PSNR .},
  archive      = {J_NN},
  author       = {Junaid Malik and Serkan Kiranyaz and Moncef Gabbouj},
  doi          = {10.1016/j.neunet.2020.12.014},
  journal      = {Neural Networks},
  pages        = {201-211},
  shortjournal = {Neural Netw.},
  title        = {Self-organized operational neural networks for severe image restoration problems},
  volume       = {135},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Constraints on hebbian and STDP learned weights of a spiking
neuron. <em>NN</em>, <em>135</em>, 192–200. (<a
href="https://doi.org/10.1016/j.neunet.2020.12.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We analyse mathematically the constraints on weights resulting from Hebbian and STDP learning rules applied to a spiking neuron with weight normalisation. In the case of pure Hebbian learning , we find that the normalised weights equal the promotion probabilities of weights up to correction terms that depend on the learning rate and are usually small. A similar relation can be derived for STDP algorithms, where the normalised weight values reflect a difference between the promotion and demotion probabilities of the weight. These relations are practically useful in that they allow checking for convergence of Hebbian and STDP algorithms. Another application is novelty detection. We demonstrate this using the MNIST dataset.},
  archive      = {J_NN},
  author       = {Dominique Chu and Huy Le Nguyen},
  doi          = {10.1016/j.neunet.2020.12.012},
  journal      = {Neural Networks},
  pages        = {192-200},
  shortjournal = {Neural Netw.},
  title        = {Constraints on hebbian and STDP learned weights of a spiking neuron},
  volume       = {135},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Generative restricted kernel machines: A framework for
multi-view generation and disentangled feature learning. <em>NN</em>,
<em>135</em>, 177–191. (<a
href="https://doi.org/10.1016/j.neunet.2020.12.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a novel framework for generative models based on Restricted Kernel Machines (RKMs) with joint multi-view generation and uncorrelated feature learning , called Gen-RKM. To enable joint multi-view generation, this mechanism uses a shared representation of data from various views. Furthermore, the model has a primal and dual formulation to incorporate both kernel-based and (deep convolutional) neural network based models within the same setting. When using neural networks as explicit feature-maps, a novel training procedure is proposed, which jointly learns the features and shared subspace representation. The latent variables are given by the eigen-decomposition of the kernel matrix, where the mutual orthogonality of eigenvectors represents the learned uncorrelated features. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of generated samples on various standard datasets.},
  archive      = {J_NN},
  author       = {Arun Pandey and Joachim Schreurs and Johan A.K. Suykens},
  doi          = {10.1016/j.neunet.2020.12.010},
  journal      = {Neural Networks},
  pages        = {177-191},
  shortjournal = {Neural Netw.},
  title        = {Generative restricted kernel machines: A framework for multi-view generation and disentangled feature learning},
  volume       = {135},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Exploitation of image statistics with sparse coding in the
case of stereo vision. <em>NN</em>, <em>135</em>, 158–176. (<a
href="https://doi.org/10.1016/j.neunet.2020.12.016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The sparse coding algorithm has served as a model for early processing in mammalian vision. It has been assumed that the brain uses sparse coding to exploit statistical properties of the sensory stream. We hypothesize that sparse coding discovers patterns from the data set, which can be used to estimate a set of stimulus parameters by simple readout. In this study, we chose a model of stereo vision to test our hypothesis. We used the Locally Competitive Algorithm (LCA), followed by a naïve Bayes classifier , to infer stereo disparity. From the results we report three observations. First, disparity inference was successful with this naturalistic processing pipeline. Second, an expanded, highly redundant representation is required to robustly identify the input patterns. Third, the inference error can be predicted from the number of active coefficients in the LCA representation. We conclude that sparse coding can generate a suitable general representation for subsequent inference tasks.},
  archive      = {J_NN},
  author       = {Gerrit A. Ecke and Harald M. Papp and Hanspeter A. Mallot},
  doi          = {10.1016/j.neunet.2020.12.016},
  journal      = {Neural Networks},
  pages        = {158-176},
  shortjournal = {Neural Netw.},
  title        = {Exploitation of image statistics with sparse coding in the case of stereo vision},
  volume       = {135},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep multi-kernel auto-encoder network for clustering brain
functional connectivity data. <em>NN</em>, <em>135</em>, 148–157. (<a
href="https://doi.org/10.1016/j.neunet.2020.12.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we propose a deep-learning network model called the deep multi-kernel auto-encoder clustering network (DMACN) for clustering functional connectivity data for brain diseases. This model is an end-to-end clustering algorithm that can learn potentially advanced features and cluster disease categories. Unlike other auto-encoders, DMACN has an added self-expression layer and standard back-propagation is used to learn the features that are beneficial for clustering brain functional connectivity data. In the self-expression layer, the kernel matrix is constructed to extract effective features and a new loss function is proposed to constrain the clustering portion, which enables the training of a deep neural learning network that tends to cluster. To test the performance of the proposed algorithm, we applied the end-to-end deep unsupervised clustering algorithm to brain connectivity data. We then conducted experiments based on four public brain functional connectivity data sets and our own functional connectivity data set. The DMACN algorithm yielded good results in various evaluations compared with the existing clustering algorithm for brain functional connectivity data, the deep auto-encoder clustering algorithm, and several other relevant clustering algorithms. The deep-learning-based clustering algorithm has great potential for use in the unsupervised recognition of brain diseases.},
  archive      = {J_NN},
  author       = {Hu Lu and Saixiong Liu and Hui Wei and Chao Chen and Xia Geng},
  doi          = {10.1016/j.neunet.2020.12.005},
  journal      = {Neural Networks},
  pages        = {148-157},
  shortjournal = {Neural Netw.},
  title        = {Deep multi-kernel auto-encoder network for clustering brain functional connectivity data},
  volume       = {135},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep-gKnock: Nonlinear group-feature selection with deep
neural networks. <em>NN</em>, <em>135</em>, 139–147. (<a
href="https://doi.org/10.1016/j.neunet.2020.12.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection is central to contemporary high-dimensional data analysis. Group structure among features arises naturally in various scientific problems. Many methods have been proposed to incorporate the group structure information into feature selection. However, these methods are normally restricted to a linear regression setting. To relax the linear constraint , we design a new Deep Neural Network (DNN) architecture and integrating it with the recently proposed knockoff technique to perform nonlinear group-feature selection with controlled group-wise False Discovery Rate (gFDR). Experimental results on high-dimensional synthetic data demonstrate that our method achieves the highest power and accurate gFDR control compared with state-of-the-art methods. The performance of Deep-gKnock is especially superior in the following five situations: (1) nonlinearity relationship; (2) dimension p p greater than sample size n n ; (3) high between-group correlation; (4) high within-group correlation; (5) large number of associated groups. And Deep-gKnock is also demonstrated to be robust to the misspecification of the feature distribution and the change of network architecture . Moreover, Deep-gKnock achieves scientifically meaningful group-feature selection results for cutting-edge real world datasets.},
  archive      = {J_NN},
  author       = {Guangyu Zhu and Tingting Zhao},
  doi          = {10.1016/j.neunet.2020.12.004},
  journal      = {Neural Networks},
  pages        = {139-147},
  shortjournal = {Neural Netw.},
  title        = {Deep-gKnock: Nonlinear group-feature selection with deep neural networks},
  volume       = {135},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Semi-supervised disentangled framework for transferable
named entity recognition. <em>NN</em>, <em>135</em>, 127–138. (<a
href="https://doi.org/10.1016/j.neunet.2020.11.017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Named entity recognition (NER) for identifying proper nouns in unstructured text is one of the most important and fundamental tasks in natural language processing . However, despite the widespread use of NER models, they still require a large-scale labeled data set , which incurs a heavy burden due to manual annotation. Domain adaptation is one of the most promising solutions to this problem, where rich labeled data from the relevant source domain are utilized to strengthen the generalizability of a model based on the target domain. However, the mainstream cross-domain NER models are still affected by the following two challenges (1) Extracting domain-invariant information such as syntactic information for cross-domain transfer. (2) Integrating domain-specific information such as semantic information into the model to improve the performance of NER. In this study, we present a semi-supervised framework for transferable NER, which disentangles the domain-invariant latent variables and domain-specific latent variables. In the proposed framework, the domain-specific information is integrated with the domain-specific latent variables by using a domain predictor. The domain-specific and domain-invariant latent variables are disentangled using three mutual information regularization terms, i.e., maximizing the mutual information between the domain-specific latent variables and the original embedding, maximizing the mutual information between the domain-invariant latent variables and the original embedding, and minimizing the mutual information between the domain-specific and domain-invariant latent variables. Extensive experiments demonstrated that our model can obtain state-of-the-art performance with cross-domain and cross-lingual NER benchmark data sets.},
  archive      = {J_NN},
  author       = {Zhifeng Hao and Di Lv and Zijian Li and Ruichu Cai and Wen Wen and Boyan Xu},
  doi          = {10.1016/j.neunet.2020.11.017},
  journal      = {Neural Networks},
  pages        = {127-138},
  shortjournal = {Neural Netw.},
  title        = {Semi-supervised disentangled framework for transferable named entity recognition},
  volume       = {135},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Modular deep reinforcement learning from reward and
punishment for robot navigation. <em>NN</em>, <em>135</em>, 115–126. (<a
href="https://doi.org/10.1016/j.neunet.2020.12.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modular Reinforcement Learning decomposes a monolithic task into several tasks with sub-goals and learns each one in parallel to solve the original problem. Such learning patterns can be traced in the brains of animals. Recent evidence in neuroscience shows that animals utilize separate systems for processing rewards and punishments, illuminating a different perspective for modularizing Reinforcement Learning tasks. MaxPain and its deep variant, Deep MaxPain, showed the advances of such dichotomy-based decomposing architecture over conventional Q-learning in terms of safety and learning efficiency. These two methods differ in policy derivation. MaxPain linearly unified the reward and punishment value functions and generated a joint policy based on unified values; Deep MaxPain tackled scaling problems in high-dimensional cases by linearly forming a joint policy from two sub-policies obtained from their value functions. However, the mixing weights in both methods were determined manually, causing inadequate use of the learned modules. In this work, we discuss the signal scaling of reward and punishment related to discounting factor γ γ , and propose a weak constraint for signaling design. To further exploit the learning models, we propose a state-value dependent weighting scheme that automatically tunes the mixing weights: hard-max and softmax based on a case analysis of Boltzmann distribution . We focus on maze-solving navigation tasks and investigate how two metrics (pain-avoiding and goal-reaching) influence each other’s behaviors during learning. We propose a sensor fusion network structure that utilizes lidar and images captured by a monocular camera instead of lidar-only and image-only sensing. Our results, both in the simulation of three types of mazes with different complexities and a real robot experiment of an L-maze on Turtlebot3 Waffle Pi, showed the improvements of our methods.},
  archive      = {J_NN},
  author       = {Jiexin Wang and Stefan Elfwing and Eiji Uchibe},
  doi          = {10.1016/j.neunet.2020.12.001},
  journal      = {Neural Networks},
  pages        = {115-126},
  shortjournal = {Neural Netw.},
  title        = {Modular deep reinforcement learning from reward and punishment for robot navigation},
  volume       = {135},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Information aware max-norm dirichlet networks for predictive
uncertainty estimation. <em>NN</em>, <em>135</em>, 105–114. (<a
href="https://doi.org/10.1016/j.neunet.2020.12.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Precise estimation of uncertainty in predictions for AI systems is a critical factor in ensuring trust and safety. Deep neural networks trained with a conventional method are prone to over-confident predictions. In contrast to Bayesian neural networks that learn approximate distributions on weights to infer prediction confidence, we propose a novel method, Information Aware Dirichlet networks, that learn an explicit Dirichlet prior distribution on predictive distributions by minimizing a bound on the expected max norm of the prediction error and penalizing information associated with incorrect outcomes. Properties of the new cost function are derived to indicate how improved uncertainty estimation is achieved. Experiments using real datasets show that our technique outperforms, by a large margin, state-of-the-art neural networks for estimating within-distribution and out-of-distribution uncertainty, and detecting adversarial examples .},
  archive      = {J_NN},
  author       = {Theodoros Tsiligkaridis},
  doi          = {10.1016/j.neunet.2020.12.011},
  journal      = {Neural Networks},
  pages        = {105-114},
  shortjournal = {Neural Netw.},
  title        = {Information aware max-norm dirichlet networks for predictive uncertainty estimation},
  volume       = {135},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sparse deep dictionary learning identifies differences of
time-varying functional connectivity in brain neuro-developmental study.
<em>NN</em>, <em>135</em>, 91–104. (<a
href="https://doi.org/10.1016/j.neunet.2020.12.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, the focus of functional connectivity analysis of human brain has shifted from merely revealing the inter-regional functional correlation over the entire scan duration to capturing the time-varying information of brain networks and characterizing time-resolved reoccurring patterns of connectivity. Much effort has been invested into developing approaches that can track changes in re-occurring patterns of functional connectivity over time. In this paper, we propose a sparse deep dictionary learning method to characterize the essential differences of reoccurring patterns of time-varying functional connectivity between different age groups. The proposed method combines both the interpretability of sparse dictionary learning and the capability of extracting sparse nonlinear higher-level features in the latent space of sparse deep autoencoder . In other words, it learns a sparse dictionary of the original data by considering the nonlinear representation of the data in the encoder layer based on a sparse deep autoencoder . In this way, the nonlinear structure and higher-level features of the data can be captured by deep dictionary learning. The proposed method is applied to the analysis of the Philadelphia Neurodevelopmental Cohort. It shows that there exist essential differences in the reoccurrence patterns of function connectivity between child and young adult groups. Specially, children have more diffusive functional connectivity patterns while young adults possess more focused functional connectivity patterns, and the brain function transits from undifferentiated systems to specialized neural networks with the growth.},
  archive      = {J_NN},
  author       = {Chen Qiao and Lan Yang and Vince D. Calhoun and Zong-Ben Xu and Yu-Ping Wang},
  doi          = {10.1016/j.neunet.2020.12.007},
  journal      = {Neural Networks},
  pages        = {91-104},
  shortjournal = {Neural Netw.},
  title        = {Sparse deep dictionary learning identifies differences of time-varying functional connectivity in brain neuro-developmental study},
  volume       = {135},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Insights on the role of external globus pallidus in
controlling absence seizures. <em>NN</em>, <em>135</em>, 78–90. (<a
href="https://doi.org/10.1016/j.neunet.2020.12.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Absence epilepsy, characterized by transient loss of awareness and bilaterally synchronous 2–4 Hz spike and wave discharges (SWDs) on electroencephalography (EEG) during absence seizures , is generally believed to arise from abnormal interactions between the cerebral cortex (Ctx) and thalamus . Recent animal electrophysiological studies suggested that changing the neural activation level of the external globus pallidus (GPe) neurons can remarkably modify firing rates of the thalamic reticular nucleus (TRN) neurons through the GABAergic GPe–TRN pathway. However, the existing experimental evidence does not provide a clear answer as to whether the GPe–TRN pathway contributes to regulating absence seizures . Here, using a biophysically based mean-field model of the GPe-corticothalamic (GCT) network, we found that both directly decreasing the strength of the GPe–TRN pathway and inactivating GPe neurons can effectively suppress absence seizures. Also, the pallido-cortical pathway and the recurrent connection of GPe neurons facilitate the regulation of absence seizures through the GPe–TRN pathway. Specifically, in the controllable situation, enhancing the coupling strength of either of the two pathways can successfully terminate absence seizures. Moreover, the competition between the GPe–TRN and pallido-cortical pathways may lead to the GPe bidirectionally controlling absence seizures, and this bidirectional control manner can be significantly modulated by the Ctx–TRN pathway. Importantly, when the strength of the Ctx–TRN pathway is relatively strong, the bidirectional control of absence seizures by changing GPe neural activities can be observed at both weak and strong strengths of the pallido-cortical pathway.These findings suggest that the GPe–TRN pathway may have crucial functional roles in regulating absence seizures, which may provide a testable hypothesis for further experimental studies and new perspectives on the treatment of absence epilepsy.},
  archive      = {J_NN},
  author       = {Mingming Chen and Yajie Zhu and Renping Yu and Yuxia Hu and Hong Wan and Rui Zhang and Dezhong Yao and Daqing Guo},
  doi          = {10.1016/j.neunet.2020.12.006},
  journal      = {Neural Networks},
  pages        = {78-90},
  shortjournal = {Neural Netw.},
  title        = {Insights on the role of external globus pallidus in controlling absence seizures},
  volume       = {135},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Greedy auto-augmentation for n-shot learning using deep
neural networks. <em>NN</em>, <em>135</em>, 68–77. (<a
href="https://doi.org/10.1016/j.neunet.2020.11.015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of n-shot learning is the classification of input data from small datasets. This type of learning is challenging in neural networks , which typically need a high number of data during the training process. Recent advancements in data augmentation allow us to produce an infinite number of target conditions from the primary condition. This process includes two main steps for finding the best augmentations and training the data with the new augmentation techniques. Optimizing these two steps for n-shot learning is still an open problem. In this paper, we propose a new method for auto-augmentation to address both of these problems. The proposed method can potentially extract many possible types of information from a small number of available data points in n-shot learning. The results of our experiments on five prominent n-shot learning datasets show the effectiveness of the proposed method.},
  archive      = {J_NN},
  author       = {Alireza Naghizadeh and Dimitris N. Metaxas and Dongfang Liu},
  doi          = {10.1016/j.neunet.2020.11.015},
  journal      = {Neural Networks},
  pages        = {68-77},
  shortjournal = {Neural Netw.},
  title        = {Greedy auto-augmentation for n-shot learning using deep neural networks},
  volume       = {135},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Quantum-inspired canonical correlation analysis for
exponentially large dimensional data. <em>NN</em>, <em>135</em>, 55–67.
(<a href="https://doi.org/10.1016/j.neunet.2020.11.019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Canonical correlation analysis (CCA) serves to identify statistical dependencies between pairs of multivariate data . However, its application to high-dimensional data is limited due to considerable computational complexity . As an alternative to the conventional CCA approach that requires polynomial computational time, we propose an algorithm that approximates CCA using quantum-inspired computations with computational time proportional to the logarithm of the input dimensionality. The computational efficiency and performance of the proposed quantum-inspired CCA (qiCCA) algorithm are experimentally evaluated on synthetic and real datasets. Furthermore, the fast computation provided by qiCCA allows directly applying CCA even after nonlinearly mapping raw input data into high-dimensional spaces. The conducted experiments demonstrate that, as a result of mapping raw input data into the high-dimensional spaces with the use of second-order monomials , qiCCA extracts more correlations compared with the linear CCA and achieves comparable performance with state-of-the-art nonlinear variants of CCA on several datasets. These results confirm the appropriateness of the proposed qiCCA and the high potential of quantum-inspired computations in analyzing high-dimensional data.},
  archive      = {J_NN},
  author       = {Naoko Koide-Majima and Kei Majima},
  doi          = {10.1016/j.neunet.2020.11.019},
  journal      = {Neural Networks},
  pages        = {55-67},
  shortjournal = {Neural Netw.},
  title        = {Quantum-inspired canonical correlation analysis for exponentially large dimensional data},
  volume       = {135},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A comprehensive study of class incremental learning
algorithms for visual tasks. <em>NN</em>, <em>135</em>, 38–54. (<a
href="https://doi.org/10.1016/j.neunet.2020.12.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ability of artificial agents to increment their capabilities when confronted with new data is an open challenge in artificial intelligence . The main challenge faced in such cases is catastrophic forgetting, i.e., the tendency of neural networks to underfit past data when new ones are ingested. A first group of approaches tackles forgetting by increasing deep model capacity to accommodate new knowledge. A second type of approaches fix the deep model size and introduce a mechanism whose objective is to ensure a good compromise between stability and plasticity of the model. While the first type of algorithms were compared thoroughly, this is not the case for methods which exploit a fixed size model. Here, we focus on the latter, place them in a common conceptual and experimental framework and propose the following contributions: (1) define six desirable properties of incremental learning algorithms and analyze them according to these properties, (2) introduce a unified formalization of the class-incremental learning problem, (3) propose a common evaluation framework which is more thorough than existing ones in terms of number of datasets, size of datasets, size of bounded memory and number of incremental states, (4) investigate the usefulness of herding for past exemplars selection, (5) provide experimental evidence that it is possible to obtain competitive performance without the use of knowledge distillation to tackle catastrophic forgetting and (6) facilitate reproducibility by integrating all tested methods in a common open-source repository. The main experimental finding is that none of the existing algorithms achieves the best results in all evaluated settings. Important differences arise notably if a bounded memory of past classes is allowed or not.},
  archive      = {J_NN},
  author       = {Eden Belouadah and Adrian Popescu and Ioannis Kanellos},
  doi          = {10.1016/j.neunet.2020.12.003},
  journal      = {Neural Networks},
  pages        = {38-54},
  shortjournal = {Neural Netw.},
  title        = {A comprehensive study of class incremental learning algorithms for visual tasks},
  volume       = {135},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Resilient asynchronous state estimation of markov switching
neural networks: A hierarchical structure approach. <em>NN</em>,
<em>135</em>, 29–37. (<a
href="https://doi.org/10.1016/j.neunet.2020.12.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper deals with the issue of resilient asynchronous state estimation of discrete-time Markov switching neural networks . Randomly occurring signal quantization and packet dropout are involved in the imperfect measured output. The asynchronous switching phenomena appear among Markov switching neural networks , quantizer modes and filter modes, which are modeled by a hierarchical structure approach. By resorting to the hierarchical structure approach and Lyapunov functional technique, sufficient conditions are achieved, and asynchronous resilient filters are derived such that filtering error dynamic is stochastically stable. Finally, two examples are included to verify the validity of the proposed method.},
  archive      = {J_NN},
  author       = {Jun Cheng and Yuyan Wu and Lianglin Xiong and Jinde Cao and Ju H. Park},
  doi          = {10.1016/j.neunet.2020.12.002},
  journal      = {Neural Networks},
  pages        = {29-37},
  shortjournal = {Neural Netw.},
  title        = {Resilient asynchronous state estimation of markov switching neural networks: A hierarchical structure approach},
  volume       = {135},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Artificial fly visual joint perception neural network
inspired by multiple-regional collision detection. <em>NN</em>,
<em>135</em>, 13–28. (<a
href="https://doi.org/10.1016/j.neunet.2020.11.018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The biological visual system includes multiple types of motion sensitive neurons which preferentially respond to specific perceptual regions. However, it still keeps open how to borrow such neurons to construct bio-inspired computational models for multiple-regional collision detection . To fill this gap, this work proposes a visual joint perception neural network with two subnetworks — presynaptic and postsynaptic neural networks, inspired by the preferential perception characteristics of three horizontal and vertical motion sensitive neurons. Related to the neural network and three hazard detection mechanisms, an artificial fly visual synthesized collision detection model for multiple-regional collision detection is originally developed to monitor possible danger occurrence in the case where one or more moving objects appear in the whole field of view. The experiments can clearly draw two conclusions: (i) the acquired neural network can effectively display the characteristics of visual movement, and (ii) the collision detection model, which outperforms the compared models, can effectively perform multiple-regional collision detection at a high success rate, and only takes about 0.24s to complete the process of collision detection for each virtual or actual image frame with resolution 110 × × 60.},
  archive      = {J_NN},
  author       = {Lun Li and Zhuhong Zhang and Jiaxuan Lu},
  doi          = {10.1016/j.neunet.2020.11.018},
  journal      = {Neural Networks},
  pages        = {13-28},
  shortjournal = {Neural Netw.},
  title        = {Artificial fly visual joint perception neural network inspired by multiple-regional collision detection},
  volume       = {135},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DAPath: Distance-aware knowledge graph reasoning based on
deep reinforcement learning. <em>NN</em>, <em>135</em>, 1–12. (<a
href="https://doi.org/10.1016/j.neunet.2020.11.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge graph reasoning aims to find reasoning paths for relations over incomplete knowledge graphs (KG). Prior works may not take into account that the rewards for each position (vertex in the graph) may be different. We propose the distance-aware reward in the reinforcement learning framework to assign different rewards for different positions. We observe that KG embeddings are learned from independent triples and therefore cannot fully cover the information described in the local neighborhood. To this effect, we integrate a graph self-attention (GSA) mechanism to capture more comprehensive entity information from the neighboring entities and relations. To let the model remember the path, we incorporate the GSA mechanism with GRU to consider the memory of relations in the path. Our approach can train the agent in one-pass, thus eliminating the pre-training or fine-tuning process, which significantly reduces the problem complexity. Experimental results demonstrate the effectiveness of our method. We found that our model can mine more balanced paths for each relation.},
  archive      = {J_NN},
  author       = {Prayag Tiwari and Hongyin Zhu and Hari Mohan Pandey},
  doi          = {10.1016/j.neunet.2020.11.012},
  journal      = {Neural Networks},
  pages        = {1-12},
  shortjournal = {Neural Netw.},
  title        = {DAPath: Distance-aware knowledge graph reasoning based on deep reinforcement learning},
  volume       = {135},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021k). Current events. <em>NN</em>, <em>134</em>, I. (<a
href="https://doi.org/10.1016/S0893-6080(20)30431-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NN},
  doi          = {10.1016/S0893-6080(20)30431-7},
  journal      = {Neural Networks},
  pages        = {I},
  shortjournal = {Neural Netw.},
  title        = {Current events},
  volume       = {134},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Effect of diverse recoding of granule cells on optokinetic
response in a cerebellar ring network with synaptic plasticity.
<em>NN</em>, <em>134</em>, 173–204. (<a
href="https://doi.org/10.1016/j.neunet.2020.11.014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a cerebellar ring network for the optokinetic response (OKR), and investigate the effect of diverse recoding of granule (GR) cells on OKR by varying the connection probability p c pc from Golgi to GR cells. For an optimal value of p c ∗ ( = 0 . 06 ) pc∗(=0.06) , individual GR cells exhibit diverse spiking patterns which are in-phase, anti-phase, or complex out-of-phase with respect to their population-averaged firing activity. Then, these diversely-recoded signals via parallel fibers (PFs) from GR cells are effectively depressed by the error-teaching signals via climbing fibers from the inferior olive which are also in-phase ones. Synaptic weights at in-phase PF-Purkinje cell (PC) synapses of active GR cells are strongly depressed via strong long-term depression (LTD), while those at anti-phase and complex out-of-phase PF-PC synapses are weakly depressed through weak LTD. This kind of “effective” depression (i.e., strong/weak LTD) at the PF-PC synapses causes a big modulation in firings of PCs, which then exert effective inhibitory coordination on the vestibular nucleus (VN) neuron (which evokes OKR). For the firing of the VN neuron, the learning gain degree L g Lg , corresponding to the modulation gain ratio, increases with increasing the learning cycle, and it saturates at about the 300th cycle. By varying p c pc from p c ∗ pc∗ , we find that a plot of saturated learning gain degree L g ∗ Lg∗ versus p c pc forms a bell-shaped curve with a peak at p c ∗ pc∗ (where the diversity degree in spiking patterns of GR cells is also maximum). Consequently, the more diverse in recoding of GR cells, the more effective in motor learning for the OKR adaptation.},
  archive      = {J_NN},
  author       = {Sang-Yoon Kim and Woochang Lim},
  doi          = {10.1016/j.neunet.2020.11.014},
  journal      = {Neural Networks},
  pages        = {173-204},
  shortjournal = {Neural Netw.},
  title        = {Effect of diverse recoding of granule cells on optokinetic response in a cerebellar ring network with synaptic plasticity},
  volume       = {134},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). State bounding for fuzzy memristive neural networks with
bounded input disturbances. <em>NN</em>, <em>134</em>, 163–172. (<a
href="https://doi.org/10.1016/j.neunet.2020.11.016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the state bounding problem of fuzzy memristive neural networks (FMNNs) with bounded input disturbances. By using the characters of Metzler, Hurwitz and nonnegative matrices, this paper obtains the exact delay-independent and delay-dependent boundary ranges of the solution, which have less conservatism than the results in existing literatures. The validity of the results is verified by two numerical examples.},
  archive      = {J_NN},
  author       = {Yu Gao and Song Zhu and Chunyu Yang and Shiping Wen},
  doi          = {10.1016/j.neunet.2020.11.016},
  journal      = {Neural Networks},
  pages        = {163-172},
  shortjournal = {Neural Netw.},
  title        = {State bounding for fuzzy memristive neural networks with bounded input disturbances},
  volume       = {134},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bridging multimedia heterogeneity gap via graph
representation learning for cross-modal retrieval. <em>NN</em>,
<em>134</em>, 143–162. (<a
href="https://doi.org/10.1016/j.neunet.2020.11.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Information retrieval among different modalities becomes a significant issue with many promising applications. However, inconsistent feature representation of various multimedia data causes the “heterogeneity gap” among various modalities, which is a challenge in cross-modal retrieval. For bridging the “heterogeneity gap,” the popular methods attempt to project the original data into a common representation space, which needs great fitting ability of the model. To address the above issue, we propose a novel Graph Representation Learning (GRL) method for bridging the heterogeneity gap, which does not project the original feature into an aligned representation space but adopts a cross-modal graph to link different modalities. The GRL approach consists of two subnetworks , Feature Transfer Learning Network (FTLN) and Graph Representation Learning Network (GRLN). Firstly, FTLN model finds a latent space for each modality, where the cosine similarity is suitable to describe their similarity. Then, we build a cross-modal graph to reconstruct the original data and their relationships. Finally, we abandon the features in the latent space and turn into embedding the graph vertexes into a common representation space directly. During the process, the proposed Graph Representation Learning method bypasses the most challenging issue by utilizing a cross-modal graph as a bridge to link the “heterogeneity gap” among different modalities. This attempt utilizes a cross-modal graph as an intermediary agent to bridge the “heterogeneity gap” in cross-modal retrieval, which is simple but effective. Extensive experiment results on six widely-used datasets indicate that the proposed GRL outperforms other state-of-the-art cross-modal retrieval methods .},
  archive      = {J_NN},
  author       = {Qingrong Cheng and Xiaodong Gu},
  doi          = {10.1016/j.neunet.2020.11.011},
  journal      = {Neural Networks},
  pages        = {143-162},
  shortjournal = {Neural Netw.},
  title        = {Bridging multimedia heterogeneity gap via graph representation learning for cross-modal retrieval},
  volume       = {134},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep-learned spike representations and sorting via an
ensemble of auto-encoders. <em>NN</em>, <em>134</em>, 131–142. (<a
href="https://doi.org/10.1016/j.neunet.2020.11.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spike sorting refers to the technique of detecting signals generated by single neurons from multi-neuron recordings and is a valuable tool for analyzing the relationships between individual neuronal activity patterns and specific behaviors . Since the precision of spike sorting affects all subsequent analyses, sorting accuracy is critical. Many semi-automatic to fully-automatic spike sorting algorithms have been developed. However, due to unsatisfactory classification accuracy , manual sorting is preferred by investigators despite the intensive time and labor costs. Thus, there still is a strong need for fully automatic spike sorting methods with high accuracy. Various machine learning algorithms have been developed for feature extraction but have yet to show sufficient accuracy for spike sorting. Here we describe a deep learning-based method for extracting features from spike signals using an ensemble of auto-encoders, each with a distinct architecture for distinguishing signals at different levels of resolution. By utilizing ensemble of auto-encoder ensemble, where shallow networks better represent overall signal structure and deep networks better represent signal details, extraction of high-dimensional representative features for improved spike sorting performance is achieved. The model was evaluated on publicly available simulated datasets and single-channel and 4-channel tetrode in vivo datasets. Our model not only classified single-channel spikes with varying degrees of feature similarities and signal to noise levels with higher accuracy, but also more precisely determined the number of source neurons compared to other machine learning methods. The model also demonstrated greater overall accuracy for spike sorting 4-channel tetrode recordings compared to single-channel recordings.},
  archive      = {J_NN},
  author       = {Junsik Eom and In Yong Park and Sewon Kim and Hanbyol Jang and Sanggeon Park and Yeowool Huh and Dosik Hwang},
  doi          = {10.1016/j.neunet.2020.11.009},
  journal      = {Neural Networks},
  pages        = {131-142},
  shortjournal = {Neural Netw.},
  title        = {Deep-learned spike representations and sorting via an ensemble of auto-encoders},
  volume       = {134},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Approximation rates for neural networks with encodable
weights in smoothness spaces. <em>NN</em>, <em>134</em>, 107–130. (<a
href="https://doi.org/10.1016/j.neunet.2020.11.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We examine the necessary and sufficient complexity of neural networks to approximate functions from different smoothness spaces under the restriction of encodable network weights. Based on an entropy argument, we start by proving lower bounds for the number of nonzero encodable weights for neural network approximation in Besov spaces , Sobolev spaces and more. These results are valid for all sufficiently smooth activation functions . Afterwards, we provide a unifying framework for the construction of approximate partitions of unity by neural networks with fairly general activation functions . This allows us to approximate localized Taylor polynomials by neural networks and make use of the Bramble–Hilbert Lemma. Based on our framework, we derive almost optimal upper bounds in higher-order Sobolev norms. This work advances the theory of approximating solutions of partial differential equations by neural networks.},
  archive      = {J_NN},
  author       = {Ingo Gühring and Mones Raslan},
  doi          = {10.1016/j.neunet.2020.11.010},
  journal      = {Neural Networks},
  pages        = {107-130},
  shortjournal = {Neural Netw.},
  title        = {Approximation rates for neural networks with encodable weights in smoothness spaces},
  volume       = {134},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Efficient architecture for deep neural networks with
heterogeneous sensitivity. <em>NN</em>, <em>134</em>, 95–106. (<a
href="https://doi.org/10.1016/j.neunet.2020.10.017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we present a neural network that consists of nodes with heterogeneous sensitivity. Each node in a network is assigned a variable that determines the sensitivity with which it learns to perform a given task. The network is trained via a constrained optimization that maximizes the sparsity of the sensitivity variables while ensuring optimal network performance. As a result, the network learns to perform a given task using only a few sensitive nodes. Insensitive nodes, which are nodes with zero sensitivity, can be removed from a trained network to obtain a computationally efficient network. Removing zero-sensitivity nodes has no effect on the performance of the network because the network has already been trained to perform the task without them. The regularization parameter used to solve the optimization problem was simultaneously found during the training of the networks. To validate our approach, we designed networks with computationally efficient architectures for various tasks such as autoregression , object recognition, facial expression recognition, and object detection using various datasets. In our experiments, the networks designed by our proposed method provided the same or higher performances but with far less computational complexity .},
  archive      = {J_NN},
  author       = {Hyunjoong Cho and Jinhyeok Jang and Chanhyeok Lee and Seungjoon Yang},
  doi          = {10.1016/j.neunet.2020.10.017},
  journal      = {Neural Networks},
  pages        = {95-106},
  shortjournal = {Neural Netw.},
  title        = {Efficient architecture for deep neural networks with heterogeneous sensitivity},
  volume       = {134},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Generating photo-realistic training data to improve face
recognition accuracy. <em>NN</em>, <em>134</em>, 86–94. (<a
href="https://doi.org/10.1016/j.neunet.2020.11.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face recognition has become a widely adopted biometric in forensics, security and law enforcement thanks to the high accuracy achieved by systems based on convolutional neural networks (CNNs). However, to achieve good performance, CNNs need to be trained with very large datasets which are not always available. In this paper we investigate the feasibility of using synthetic data to augment face datasets. In particular, we propose a novel generative adversarial network (GAN) that can disentangle identity-related attributes from non-identity-related attributes. This is done by training an embedding network that maps discrete identity labels to an identity latent space that follows a simple prior distribution, and training a GAN conditioned on samples from that distribution. A main novelty of our approach is the ability to generate both synthetic images of subjects in the training set and synthetic images of new subjects not in the training set, both of which we use to augment face datasets. By using recent advances in GAN training, we show that the synthetic images generated by our model are photo-realistic, and that training with datasets augmented with those images can lead to increased recognition accuracy. Experimental results show that our method is more effective when augmenting small datasets. In particular, an absolute accuracy improvement of 8.42\% was achieved when augmenting a dataset of less than 60k facial images .},
  archive      = {J_NN},
  author       = {Daniel Sáez Trigueros and Li Meng and Margaret Hartnett},
  doi          = {10.1016/j.neunet.2020.11.008},
  journal      = {Neural Networks},
  pages        = {86-94},
  shortjournal = {Neural Netw.},
  title        = {Generating photo-realistic training data to improve face recognition accuracy},
  volume       = {134},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A brain-inspired network architecture for cost-efficient
object recognition in shallow hierarchical neural networks. <em>NN</em>,
<em>134</em>, 76–85. (<a
href="https://doi.org/10.1016/j.neunet.2020.11.013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The brain successfully performs visual object recognition with a limited number of hierarchical networks that are much shallower than artificial deep neural networks (DNNs) that perform similar tasks. Here, we show that long-range horizontal connections (LRCs), often observed in the visual cortex of mammalian species , enable such a cost-efficient visual object recognition in shallow neural networks. Using simulations of a model hierarchical network with convergent feedforward connections and LRCs, we found that the addition of LRCs to the shallow feedforward network significantly enhances the performance of networks for image classification , to a degree that is comparable to much deeper networks. We found that a combination of sparse LRCs and dense local connections dramatically increases performance per wiring cost. From network pruning with gradient-based optimization, we also confirmed that LRCs could emerge spontaneously by minimizing the total connection length while maintaining performance. Ablation of emerged LRCs led to a significant reduction of classification performance, which implies these LRCs are crucial for performing image classification. Taken together, our findings suggest a brain-inspired strategy for constructing a cost-efficient network architecture to implement parsimonious object recognition under physical constraints such as shallow hierarchical depth.},
  archive      = {J_NN},
  author       = {Youngjin Park and Seungdae Baek and Se-Bum Paik},
  doi          = {10.1016/j.neunet.2020.11.013},
  journal      = {Neural Networks},
  pages        = {76-85},
  shortjournal = {Neural Netw.},
  title        = {A brain-inspired network architecture for cost-efficient object recognition in shallow hierarchical neural networks},
  volume       = {134},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Necessary conditions for STDP-based pattern recognition
learning in a memristive spiking neural network. <em>NN</em>,
<em>134</em>, 64–75. (<a
href="https://doi.org/10.1016/j.neunet.2020.11.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work is aimed to study experimental and theoretical approaches for searching effective local training rules for unsupervised pattern recognition by high-performance memristor-based Spiking Neural Networks (SNNs). First, the possibility of weight change using Spike-Timing-Dependent Plasticity (STDP) is demonstrated with a pair of hardware analog neurons connected through a (CoFeB) x x (LiNbO 3 ) 1 − x 1−x nanocomposite memristor . Next, the learning convergence to a solution of binary clusterization task is analyzed in a wide range of memristive STDP parameters for a single-layer fully connected feedforward SNN. The memristive STDP behavior supplying convergence in this simple task is shown also to provide it in the handwritten digit recognition domain by the more complex SNN architecture with a Winner-Take-All competition between neurons. To investigate basic conditions necessary for training convergence, an original probabilistic generative model of a rate-based single-layer network with independent or competing neurons is built and thoroughly analyzed. The main result is a statement of “correlation growth-anticorrelation decay” principle which prompts near-optimal policy to configure model parameters. This principle is in line with requiring the binary clusterization convergence which can be defined as the necessary condition for optimal learning and used as the simple benchmark for tuning parameters of various neural network realizations with population-rate information coding. At last, a heuristic algorithm is described to experimentally find out the convergence conditions in a memristive SNN, including robustness to a device variability. Due to the generality of the proposed approach, it can be applied to a wide range of memristors and neurons of software- or hardware-based rate-coding single-layer SNNs when searching for local rules that ensure their unsupervised learning convergence in a pattern recognition task domain.},
  archive      = {J_NN},
  author       = {V.A. Demin and D.V. Nekhaev and I.A. Surazhevsky and K.E. Nikiruy and A.V. Emelyanov and S.N. Nikolaev and V.V. Rylkov and M.V. Kovalchuk},
  doi          = {10.1016/j.neunet.2020.11.005},
  journal      = {Neural Networks},
  pages        = {64-75},
  shortjournal = {Neural Netw.},
  title        = {Necessary conditions for STDP-based pattern recognition learning in a memristive spiking neural network},
  volume       = {134},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Particle swarm optimized neural networks based local
tracking control scheme of unknown nonlinear interconnected systems.
<em>NN</em>, <em>134</em>, 54–63. (<a
href="https://doi.org/10.1016/j.neunet.2020.09.020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a local tracking control (LTC) scheme is developed via particle swarm optimized neural networks (PSONN) for unknown nonlinear interconnected systems . With the local input–output data, a local neural network identifier is constructed to approximate the local input gain matrix and the mismatched interconnection, which are utilized to derive the LTC. To solve the local Hamilton–Jacobi–Bellman equation, a local critic NN is established to estimate the proper local value function, which reflects the mismatched interconnection. The weight vector of the local critic NN is trained online by particle swarm optimization , thus the success rate of system execution is increased. The stability of the closed-loop unknown nonlinear interconnected system is guaranteed to be uniformly ultimately bounded through Lyapunov’s direct method. Simulation results of two examples demonstrate the effectiveness of the developed PSONN-based LTC scheme.},
  archive      = {J_NN},
  author       = {Bo Zhao and Fangchao Luo and Haowei Lin and Derong Liu},
  doi          = {10.1016/j.neunet.2020.09.020},
  journal      = {Neural Networks},
  pages        = {54-63},
  shortjournal = {Neural Netw.},
  title        = {Particle swarm optimized neural networks based local tracking control scheme of unknown nonlinear interconnected systems},
  volume       = {134},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Distant supervision relation extraction via adaptive
dependency-path and additional knowledge graph supervision. <em>NN</em>,
<em>134</em>, 42–53. (<a
href="https://doi.org/10.1016/j.neunet.2020.10.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Relation Extraction systems train an extractor by aligning relation instances in Knowledge Base with a large amount of labeled corpora. Since the labeled datasets are very expensive, Distant Supervision Relation Extraction (DSRE) utilizes rough corpus annotated with Knowledge Graph to reduce the cost of acquisition. Nevertheless, the data noise problem limits the performance of the DSRE. Dependency trees can be used to filter the wrong-labeled instances in the distant supervision bag. However, existing dependency tree relation extraction strategies are all based on manually-set paths between the subject and object entities, and suffer from the problem of pruning the trees too aggressively or too insufficiently. To circumvent the shortcomings, in this paper, we propose a novel DSRE framework A 2 A2 DSRE, based on the A daptive dependency-path and A dditional KG supervision. To obtain the dependency paths related to entity relations adaptively, we introduce an advanced graph neural network—GeniePath into DSRE, which assigns higher weights to those direct neighbor words that contribute more to relation prediction through breadth exploration, and conducts depth exploration to determine the correlation between relations and high-order neighbors. In this way, the irrelevant nodes are pruned while the relevant nodes are kept, our method can obtain more appropriate paths associated with relations. At the same time, to further reduce the noises in the data, we incorporate additional supervision information from the knowledge graph by retracting the margin between the representation of the bag and the pre-training knowledge graph embedding . Extensive numerical experiments validate the effectiveness of our new method.},
  archive      = {J_NN},
  author       = {Yong Shi and Yang Xiao and Pei Quan and MingLong Lei and Lingfeng Niu},
  doi          = {10.1016/j.neunet.2020.10.012},
  journal      = {Neural Networks},
  pages        = {42-53},
  shortjournal = {Neural Netw.},
  title        = {Distant supervision relation extraction via adaptive dependency-path and additional knowledge graph supervision},
  volume       = {134},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning sparse and meaningful representations through
embodiment. <em>NN</em>, <em>134</em>, 23–41. (<a
href="https://doi.org/10.1016/j.neunet.2020.11.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How do humans acquire a meaningful understanding of the world with little to no supervision or semantic labels provided by the environment? Here we investigate embodiment with a closed loop between action and perception as one key component in this process. We take a close look at the representations learned by a deep reinforcement learning agent that is trained with high-dimensional visual observations collected in a 3D environment with very sparse rewards. We show that this agent learns stable representations of meaningful concepts such as doors without receiving any semantic labels. Our results show that the agent learns to represent the action relevant information, extracted from a simulated camera stream, in a wide variety of sparse activation patterns . The quality of the representations learned shows the strength of embodied learning and its advantages over fully supervised approaches.},
  archive      = {J_NN},
  author       = {Viviane Clay and Peter König and Kai-Uwe Kühnberger and Gordon Pipa},
  doi          = {10.1016/j.neunet.2020.11.004},
  journal      = {Neural Networks},
  pages        = {23-41},
  shortjournal = {Neural Netw.},
  title        = {Learning sparse and meaningful representations through embodiment},
  volume       = {134},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Modality independent adversarial network for generalized
zero shot image classification. <em>NN</em>, <em>134</em>, 11–22. (<a
href="https://doi.org/10.1016/j.neunet.2020.11.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zero Shot Learning (ZSL) aims to classify images of unseen target classes by transferring knowledge from source classes through semantic embeddings . The core of ZSL research is to embed both visual representation of object instance and semantic description of object class into a joint latent space and learn cross-modal (visual and semantic) latent representations. However, the learned representations by existing efforts often fail to fully capture the underlying cross-modal semantic consistency , and some of the representations are very similar and less discriminative. To circumvent these issues, in this paper, we propose a novel deep framework, called Modality Independent Adversarial Network (MIANet) for Generalized Zero Shot Learning (GZSL), which is an end-to-end deep architecture with three submodules . First, both visual feature and semantic description are embedded into a latent hyper-spherical space, where two orthogonal constraints are employed to ensure the learned latent representations discriminative. Second, a modality adversarial submodule is employed to make the latent representations independent of modalities to make the shared representations grab more cross-modal high-level semantic information during training. Third, a cross reconstruction submodule is proposed to reconstruct latent representations into the counterparts instead of themselves to make them capture more modality irrelevant information. Comprehensive experiments on five widely used benchmark datasets are conducted on both GZSL and standard ZSL settings, and the results show the effectiveness of our proposed method.},
  archive      = {J_NN},
  author       = {Haofeng Zhang and Yinduo Wang and Yang Long and Longzhi Yang and Ling Shao},
  doi          = {10.1016/j.neunet.2020.11.007},
  journal      = {Neural Networks},
  pages        = {11-22},
  shortjournal = {Neural Netw.},
  title        = {Modality independent adversarial network for generalized zero shot image classification},
  volume       = {134},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Episodic memory governs choices: An RNN-based reinforcement
learning model for decision-making task. <em>NN</em>, <em>134</em>,
1–10. (<a href="https://doi.org/10.1016/j.neunet.2020.11.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Typical methods to study cognitive function are to record the electrical activities of animal neurons during the training of animals performing behavioral tasks. A key problem is that they fail to record all the relevant neurons in the animal brain. To alleviate this problem, we develop an RNN-based Actor–Critic framework, which is trained through reinforcement learning (RL) to solve two tasks analogous to the monkeys’ decision-making tasks. The trained model is capable of reproducing some features of neural activities recorded from animal brain, or some behavior properties exhibited in animal experiments, suggesting that it can serve as a computational platform to explore other cognitive functions. Furthermore, we conduct behavioral experiments on our framework, trying to explore an open question in neuroscience : which episodic memory in the hippocampus should be selected to ultimately govern future decisions. We find that the retrieval of salient events sampled from episodic memories can effectively shorten deliberation time than common events in the decision-making process. The results indicate that salient events stored in the hippocampus could be prioritized to propagate reward information, and thus allow decision-makers to learn a strategy faster.},
  archive      = {J_NN},
  author       = {Xiaohan Zhang and Lu Liu and Guodong Long and Jing Jiang and Shenquan Liu},
  doi          = {10.1016/j.neunet.2020.11.003},
  journal      = {Neural Networks},
  pages        = {1-10},
  shortjournal = {Neural Netw.},
  title        = {Episodic memory governs choices: An RNN-based reinforcement learning model for decision-making task},
  volume       = {134},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Maintaining the publication infrastructure in a worldwide
pandemic. <em>NN</em>, <em>133</em>, xvi–xvii. (<a
href="https://doi.org/10.1016/S0893-6080(20)30405-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NN},
  author       = {Kenji Doya ( Co-Editors-in-Chief ) and DeLiang Wang},
  doi          = {10.1016/S0893-6080(20)30405-6},
  journal      = {Neural Networks},
  pages        = {xvi-xvii},
  shortjournal = {Neural Netw.},
  title        = {Maintaining the publication infrastructure in a worldwide pandemic},
  volume       = {133},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Neural networks referees in 2020. <em>NN</em>, <em>133</em>,
x–xv. (<a href="https://doi.org/10.1016/S0893-6080(20)30404-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NN},
  doi          = {10.1016/S0893-6080(20)30404-4},
  journal      = {Neural Networks},
  pages        = {x-xv},
  shortjournal = {Neural Netw.},
  title        = {Neural networks referees in 2020},
  volume       = {133},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021k). INN/ENNS/JNNS - membership applic. form. <em>NN</em>,
<em>133</em>, II. (<a
href="https://doi.org/10.1016/S0893-6080(20)30408-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NN},
  doi          = {10.1016/S0893-6080(20)30408-1},
  journal      = {Neural Networks},
  pages        = {II},
  shortjournal = {Neural Netw.},
  title        = {INN/ENNS/JNNS - membership applic. form},
  volume       = {133},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021l). Current events. <em>NN</em>, <em>133</em>, I. (<a
href="https://doi.org/10.1016/S0893-6080(20)30406-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NN},
  doi          = {10.1016/S0893-6080(20)30406-8},
  journal      = {Neural Networks},
  pages        = {I},
  shortjournal = {Neural Netw.},
  title        = {Current events},
  volume       = {133},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DMMAN: A two-stage audio–visual fusion framework for sound
separation and event localization. <em>NN</em>, <em>133</em>, 229–239.
(<a href="https://doi.org/10.1016/j.neunet.2020.10.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Videos are used widely as the media platforms for human beings to touch the physical change of the world. However, we always receive the mixed sound from the multiple sound objects, and cannot distinguish and localize the sounds as the separate entities in videos. In order to solve this problem, a model named the Deep Multi-Modal Attention Network (DMMAN), is established to model the unconstrained video datasets for further finishing the sound source separation and event localization tasks in this paper. Based on the multi-modal separator and multi-modal matching classifier module, our model focuses on the sound separation and modal synchronization problems using two stage fusion of the sound and visual features. To link the multi-modal separator and multi-modal matching classifier modules, the regression and classification losses are employed to build the loss function of the DMMAN. The estimated spectrum masks and attention synchronization scores calculated by the DMMAN can be easily generalized to the sound source and event localization tasks. The quantitative experimental results show the DMMAN not only separates the high quality of the sound sources evaluated by Signal-to-Distortion Ratio and Signal-to-Interference Ratio metrics, but also is suitable for the mixed sound scenes that are never heard jointly. Meanwhile, DMMAN achieves better classification accuracy than other contrast baselines for the event localization tasks.},
  archive      = {J_NN},
  author       = {Ruihan Hu and Songbing Zhou and Zhi Ri Tang and Sheng Chang and Qijun Huang and Yisen Liu and Wei Han and Edmond Q. Wu},
  doi          = {10.1016/j.neunet.2020.10.003},
  journal      = {Neural Networks},
  pages        = {229-239},
  shortjournal = {Neural Netw.},
  title        = {DMMAN: A two-stage audio–visual fusion framework for sound separation and event localization},
  volume       = {133},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). ClsGAN: Selective attribute editing model based on
classification adversarial network. <em>NN</em>, <em>133</em>, 220–228.
(<a href="https://doi.org/10.1016/j.neunet.2020.10.019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attribution editing has achieved remarkable progress in recent years owing to the encoder–decoder structure and generative adversarial network (GAN). However, it remains challenging to generate high-quality images with accurate attribute transformation. Attacking these problems, the work proposes a novel selective attribute editing model based on classification adversarial network (referred to as ClsGAN) that shows good balance between attribute transfer accuracy and photo-realistic images. Considering that the editing images are prone to be affected by original attribute due to skip-connection in encoder–decoder structure, an upper convolution residual network (referred to as Tr-resnet) is presented to selectively extract information from the source image and target label. In addition, to further improve the transfer accuracy of generated images, an attribute adversarial classifier (referred to as Atta-cls) is introduced to guide the generator from the perspective of attribute through learning the defects of attribute transfer images. Experimental results on CelebA demonstrate that our ClsGAN performs favorably against state-of-the-art approaches in image quality and transfer accuracy. Moreover, ablation studies are also designed to verify the great performance of Tr-resnet and Atta-cls.},
  archive      = {J_NN},
  author       = {Ying Liu and Heng Fan and Fuchuan Ni and Jinhai Xiang},
  doi          = {10.1016/j.neunet.2020.10.019},
  journal      = {Neural Networks},
  pages        = {220-228},
  shortjournal = {Neural Netw.},
  title        = {ClsGAN: Selective attribute editing model based on classification adversarial network},
  volume       = {133},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Consensus guided incomplete multi-view spectral clustering.
<em>NN</em>, <em>133</em>, 207–219. (<a
href="https://doi.org/10.1016/j.neunet.2020.10.014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Incomplete multi-view clustering which aims to solve the difficult clustering challenge on incomplete multi-view data collected from diverse domains with missing views has drawn considerable attention in recent years. In this paper, we propose a novel method, called consensus guided incomplete multi-view spectral clustering (CGIMVSC), to address the incomplete clustering problem . Specifically, CGIMVSC seeks to explore the local information within every single-view and the semantic consistent information shared by all views in a unified framework simultaneously, where the local structure is adaptively obtained from the incomplete data rather than pre-constructed via a k-nearest neighbor approach in the existing methods. Considering the semantic consistency of multiple views, CGIMVSC introduces a co-regularization constraint to minimize the disagreement between the common representation and the individual representations with respect to different views, such that all views will obtain a consensus clustering result . Experimental comparisons with some state-of-the-art methods on seven datasets validate the effectiveness of the proposed method on incomplete multi-view clustering.},
  archive      = {J_NN},
  author       = {Jie Wen and Huijie Sun and Lunke Fei and Jinxing Li and Zheng Zhang and Bob Zhang},
  doi          = {10.1016/j.neunet.2020.10.014},
  journal      = {Neural Networks},
  pages        = {207-219},
  shortjournal = {Neural Netw.},
  title        = {Consensus guided incomplete multi-view spectral clustering},
  volume       = {133},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An EEG channel selection method for motor imagery based
brain–computer interface and neurofeedback using granger causality.
<em>NN</em>, <em>133</em>, 193–206. (<a
href="https://doi.org/10.1016/j.neunet.2020.11.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motor imagery (MI) brain–computer interface (BCI) and neurofeedback (NF) with electroencephalogram (EEG) signals are commonly used for motor function improvement in healthy subjects and to restore neurological functions in stroke patients. Generally, in order to decrease noisy and redundant information in unrelated EEG channels, channel selection methods are used which provide feasible BCI and NF implementations with better performances. Our assumption is that there are causal interactions between the channels of EEG signal in MI tasks that are repeated in different trials of a BCI and NF experiment. Therefore, a novel method for EEG channel selection is proposed which is based on Granger causality (GC) analysis. Additionally, the machine-learning approach is used to cluster independent component analysis (ICA) components of the EEG signal into artifact and normal EEG clusters. After channel selection, using the common spatial pattern (CSP) and regularized CSP (RCSP), features are extracted and with the k-nearest neighbor (k-NN), support vector machine (SVM) and linear discriminant analysis (LDA) classifiers, MI tasks are classified into left and right hand MI. The goal of this study is to achieve a method resulting in lower EEG channels with higher classification performance in MI-based BCI and NF by causal constraint. The proposed method based on GC, with only eight selected channels, results in 93.03\% accuracy, 92.93\% sensitivity, and 93.12\% specificity, with RCSP feature extractor and best classifier for each subject, after being applied on Physionet MI dataset, which is increased by 3.95\%, 3.73\%, and 4.13\%, in comparison with correlation-based channel selection method.},
  archive      = {J_NN},
  author       = {Hesam Varsehi and S. Mohammad P. Firoozabadi},
  doi          = {10.1016/j.neunet.2020.11.002},
  journal      = {Neural Networks},
  pages        = {193-206},
  shortjournal = {Neural Netw.},
  title        = {An EEG channel selection method for motor imagery based brain–computer interface and neurofeedback using granger causality},
  volume       = {133},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Echo memory-augmented network for time series
classification. <em>NN</em>, <em>133</em>, 177–192. (<a
href="https://doi.org/10.1016/j.neunet.2020.10.015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Echo State Networks (ESNs) are efficient recurrent neural networks (RNNs) which have been successfully applied to time series modeling tasks. However, ESNs are unable to capture the history information far from the current time step, since the echo state at the present step of ESNs mostly impacted by the previous one. Thus, ESN may have difficulty in capturing the long-term dependencies of temporal data. In this paper, we propose an end-to-end model named Echo Memory-Augmented Network (EMAN) for time series classification. An EMAN consists of an echo memory-augmented encoder and a multi-scale convolutional learner. First, the time series is fed into the reservoir of an ESN to produce the echo states, which are all collected into an echo memory matrix along with the time steps. After that, we design an echo memory-augmented mechanism employing the sparse learnable attention to the echo memory matrix to obtain the Echo Memory-Augmented Representations (EMARs). In this way, the input time series is encoded into the EMARs with enhancing the temporal memory of the ESN. We then use multi-scale convolutions with the max-over-time pooling to extract the most discriminative features from the EMARs. Finally, a fully-connected layer and a softmax layer calculate the probability distribution on categories. Experiments conducted on extensive time series datasets show that EMAN is state-of-the-art compared to existing time series classification methods. The visualization analysis also demonstrates the effectiveness of enhancing the temporal memory of the ESN.},
  archive      = {J_NN},
  author       = {Qianli Ma and Zhenjing Zheng and Wanqing Zhuang and Enhuan Chen and Jia Wei and Jiabing Wang},
  doi          = {10.1016/j.neunet.2020.10.015},
  journal      = {Neural Networks},
  pages        = {177-192},
  shortjournal = {Neural Netw.},
  title        = {Echo memory-augmented network for time series classification},
  volume       = {133},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). FMixCutMatch for semi-supervised deep learning. <em>NN</em>,
<em>133</em>, 166–176. (<a
href="https://doi.org/10.1016/j.neunet.2020.10.018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mixed sample augmentation (MSA) has witnessed great success in the research area of semi-supervised learning (SSL) and is performed by mixing two training samples as an augmentation strategy to effectively smooth the training space. Following the insights on the efficacy of cut-mix in particular, we propose FMixCut, an MSA that combines Fourier space-based data mixing (FMix) and the proposed Fourier space-based data cutting (FCut) for labeled and unlabeled data augmentation. Specifically, for the SSL task, our approach first generates soft pseudo-labels using the model’s previous predictions. The model is then trained to penalize the outputs of the FMix-generated samples so that they are consistent with their mixed soft pseudo-labels. In addition, we propose to use FCut, a new Cutout-based data augmentation strategy that adopts the two masked sample pairs from FMix for weighted cross-entropy minimization. Furthermore, by implementing two regularization techniques, namely, batch label distribution entropy maximization and sample confidence entropy minimization, we further boost the training efficiency. Finally, we introduce a dynamic labeled–unlabeled data mixing (DDM) strategy to further accelerate the convergence of the model. Combining the above process, we finally call our SSL approach as ”FMixCutMatch”, in short FMCmatch. As a result, the proposed FMCmatch achieves state-of-the-art performance on CIFAR-10/100, SVHN and Mini-Imagenet across a variety of SSL conditions with the CNN-13, WRN-28-2 and ResNet-18 networks. In particular, our method achieves a 4.54\% test error on CIFAR-10 with 4K labels under the CNN-13 and a 41.25\% Top-1 test error on Mini-Imagenet with 10K labels under the ResNet-18. Our codes for reproducing these results are publicly available at https://github.com/biuyq/FMixCutMatch.},
  archive      = {J_NN},
  author       = {Xiang Wei and Xiaotao Wei and Xiangyuan Kong and Siyang Lu and Weiwei Xing and Wei Lu},
  doi          = {10.1016/j.neunet.2020.10.018},
  journal      = {Neural Networks},
  pages        = {166-176},
  shortjournal = {Neural Netw.},
  title        = {FMixCutMatch for semi-supervised deep learning},
  volume       = {133},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). PM2.5 concentration modeling and prediction by using
temperature-based deep belief network. <em>NN</em>, <em>133</em>,
157–165. (<a
href="https://doi.org/10.1016/j.neunet.2020.10.013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Air quality prediction is a global hot issue, and PM 2.5 2.5 is an important factor affecting air quality. Due to complicated causes of formation, PM 2.5 2.5 prediction is a thorny and challenging task. In this paper, a novel deep learning model named temperature-based deep belief networks (TDBN) is proposed to predict the daily concentrations of PM 2.5 2.5 for the next day. Firstly, the location of PM 2.5 2.5 concentration prediction is Chaoyang Park in Beijing of China from January 1, 2018 to October 27, 2018. The auxiliary variables are selected as input variables of TDBN by Partial Least Square (PLS), and the corresponding data is divided into three independent sections: training samples, validating samples and testing samples. Secondly, the TDBN is composed of temperature-based restricted Boltzmann machine (RBM), where temperature is considered as an effective physical parameter in energy balance of training RBM. The structural parameters of TDBN are determined by minimizing the error in the training process, including hidden layers number, hidden neurons and value of temperature. Finally, the testing samples are used to test the performance of the proposed TDBN on PM 2.5 2.5 prediction, and the other similar models are tested by the same testing samples for convenience of comparison with TDBN. The experimental results demonstrate that TDBN performs better than its peers in root mean square error (RMSE), mean absolute error (MAE) and coefficient of determination (R 2 ).},
  archive      = {J_NN},
  author       = {Haixia Xing and Gongming Wang and Caixia Liu and Minghe Suo},
  doi          = {10.1016/j.neunet.2020.10.013},
  journal      = {Neural Networks},
  pages        = {157-165},
  shortjournal = {Neural Netw.},
  title        = {PM2.5 concentration modeling and prediction by using temperature-based deep belief network},
  volume       = {133},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adversarial symmetric GANs: Bridging adversarial samples and
adversarial networks. <em>NN</em>, <em>133</em>, 148–156. (<a
href="https://doi.org/10.1016/j.neunet.2020.10.016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative adversarial networks have achieved remarkable performance on various tasks but suffer from training instability. Despite many training strategies proposed to improve training stability, this issue remains as a challenge. In this paper, we investigate the training instability from the perspective of adversarial samples and reveal that adversarial training on fake samples is implemented in vanilla GANs, but adversarial training on real samples has long been overlooked. Consequently, the discriminator is extremely vulnerable to adversarial perturbation and the gradient given by the discriminator contains non-informative adversarial noises, which hinders the generator from catching the pattern of real samples. Here, we develop adversarial symmetric GANs (AS-GANs) that incorporate adversarial training of the discriminator on real samples into vanilla GANs, making adversarial training symmetrical. The discriminator is therefore more robust and provides more informative gradient with less adversarial noise, thereby stabilizing training and accelerating convergence. The effectiveness of the AS-GANs is verified on image generation on CIFAR-10, CIFAR-100, CelebA, and LSUN with varied network architectures . Not only the training is more stabilized, but the FID scores of generated samples are consistently improved by a large margin compared to the baseline. Theoretical analysis is also conducted to explain why AS-GAN can improve training. The bridging of adversarial samples and adversarial networks provides a new approach to further develop adversarial networks.},
  archive      = {J_NN},
  author       = {Faqiang Liu and Mingkun Xu and Guoqi Li and Jing Pei and Luping Shi and Rong Zhao},
  doi          = {10.1016/j.neunet.2020.10.016},
  journal      = {Neural Networks},
  pages        = {148-156},
  shortjournal = {Neural Netw.},
  title        = {Adversarial symmetric GANs: Bridging adversarial samples and adversarial networks},
  volume       = {133},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). FPGAN: Face de-identification method with generative
adversarial networks for social robots. <em>NN</em>, <em>133</em>,
132–147. (<a
href="https://doi.org/10.1016/j.neunet.2020.09.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a new face de-identification method based on generative adversarial network (GAN) to protect visual facial privacy, which is an end-to-end method (herein, FPGAN). First, we propose FPGAN and mathematically prove its convergence. Then, a generator with an improved U-Net is used to enhance the quality of the generated image, and two discriminators with a seven-layer network architecture are designed to strengthen the feature extraction ability of FPGAN. Subsequently, we propose the pixel loss, content loss, adversarial loss functions and optimization strategy to guarantee the performance of FPGAN. In our experiments, we applied FPGAN to face de-identification in social robots and analyzed the related conditions that could affect the model. Moreover, we proposed a new face de-identification evaluation protocol to check the performance of the model. This protocol can be used for the evaluation of face de-identification and privacy protection. Finally, we tested our model and four other methods on the CelebA, MORPH, RaFD, and FBDe datasets. The results of the experiments show that FPGAN outperforms the baseline methods .},
  archive      = {J_NN},
  author       = {Jiacheng Lin and Yang Li and Guanci Yang},
  doi          = {10.1016/j.neunet.2020.09.001},
  journal      = {Neural Networks},
  pages        = {132-147},
  shortjournal = {Neural Netw.},
  title        = {FPGAN: Face de-identification method with generative adversarial networks for social robots},
  volume       = {133},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Gradient-based training and pruning of radial basis function
networks with an application in materials physics. <em>NN</em>,
<em>133</em>, 123–131. (<a
href="https://doi.org/10.1016/j.neunet.2020.10.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many applications, especially in physics and other sciences, call for easily interpretable and robust machine learning techniques . We propose a fully gradient-based technique for training radial basis function networks with an efficient and scalable open-source implementation. We derive novel closed-form optimization criteria for pruning the models for continuous as well as binary data which arise in a challenging real-world material physics problem. The pruned models are optimized to provide compact and interpretable versions of larger models based on informed assumptions about the data distribution. Visualizations of the pruned models provide insight into the atomic configurations that determine atom-level migration processes in solid matter; these results may inform future research on designing more suitable descriptors for use with machine learning algorithms .},
  archive      = {J_NN},
  author       = {Jussi Määttä and Viacheslav Bazaliy and Jyri Kimari and Flyura Djurabekova and Kai Nordlund and Teemu Roos},
  doi          = {10.1016/j.neunet.2020.10.002},
  journal      = {Neural Networks},
  pages        = {123-131},
  shortjournal = {Neural Netw.},
  title        = {Gradient-based training and pruning of radial basis function networks with an application in materials physics},
  volume       = {133},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). AutoTune: Automatically tuning convolutional neural networks
for improved transfer learning. <em>NN</em>, <em>133</em>, 112–122. (<a
href="https://doi.org/10.1016/j.neunet.2020.10.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transfer learning enables solving a specific task having limited data by using the pre-trained deep networks trained on large-scale datasets. Typically, while transferring the learned knowledge from source task to the target task, the last few layers are fine-tuned (re-trained) over the target dataset. However, these layers are originally designed for the source task that might not be suitable for the target task. In this paper, we introduce a mechanism for automatically tuning the Convolutional Neural Networks (CNN) for improved transfer learning. The pre-trained CNN layers are tuned with the knowledge from target data using Bayesian Optimization . First, we train the final layer of the base CNN model by replacing the number of neurons in the softmax layer with the number of classes involved in the target task. Next, the CNN is tuned automatically by observing the classification performance on the validation data (greedy criteria). To evaluate the performance of the proposed method, experiments are conducted on three benchmark datasets, e.g., CalTech-101, CalTech-256, and Stanford Dogs. The classification results obtained through the proposed AutoTune method outperforms the standard baseline transfer learning methods over the three datasets by achieving 95.92\%, 86.54\%, and 84.67\% accuracy over CalTech-101, CalTech-256, and Stanford Dogs, respectively. The experimental results obtained in this study depict that tuning of the pre-trained CNN layers with the knowledge from the target dataset confesses better transfer learning ability. The source codes are available at https://github.com/JekyllAndHyde8999/AutoTune_CNN_TransferLearning .},
  archive      = {J_NN},
  author       = {S.H. Shabbeer Basha and Sravan Kumar Vinakota and Viswanath Pulabaigari and Snehasis Mukherjee and Shiv Ram Dubey},
  doi          = {10.1016/j.neunet.2020.10.009},
  journal      = {Neural Networks},
  pages        = {112-122},
  shortjournal = {Neural Netw.},
  title        = {AutoTune: Automatically tuning convolutional neural networks for improved transfer learning},
  volume       = {133},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Unsupervised feature learning for self-tuning neural
networks. <em>NN</em>, <em>133</em>, 103–111. (<a
href="https://doi.org/10.1016/j.neunet.2020.10.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years transfer learning has attracted much attention due to its ability to adapt a well-trained model from one domain to another. Fine-tuning is one of the most widely-used methods which exploit a small set of labeled data in the target domain for adapting the network. Including a few methods using the labeled data in the source domain, most transfer learning methods require labeled datasets, and it restricts the use of transfer learning to new domains. In this paper, we propose a fully unsupervised self-tuning algorithm for learning visual features in different domains. The proposed method updates a pre-trained model by minimizing the triplet loss function using only unlabeled data in the target domain. First, we propose the relevance measure for unlabeled data by the bagged clustering method . Then triplets of the anchor, positive, and negative data points are sampled based on the ranking violations of the relevance scores and the Euclidean distances in the embedded feature space. This fully unsupervised self-tuning algorithm improves the performance of the network significantly. We extensively evaluate the proposed algorithm using various metrics, including classification accuracy , feature analysis, and clustering quality , on five benchmark datasets in different domains. Besides, we demonstrate that applying the self-tuning method on the fine-tuned network help achieve better results.},
  archive      = {J_NN},
  author       = {Jongbin Ryu and Ming-Hsuan Yang and Jongwoo Lim},
  doi          = {10.1016/j.neunet.2020.10.011},
  journal      = {Neural Networks},
  pages        = {103-111},
  shortjournal = {Neural Netw.},
  title        = {Unsupervised feature learning for self-tuning neural networks},
  volume       = {133},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Advanced deep learning methods for biomedical information
analysis: An editorial. <em>NN</em>, <em>133</em>, 101–102. (<a
href="https://doi.org/10.1016/j.neunet.2020.10.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NN},
  author       = {Yu-Dong Zhang ( Guest Editors ) and Francesco Carlo Morabito and Dinggang Shen and Khan Muhammad},
  doi          = {10.1016/j.neunet.2020.10.006},
  journal      = {Neural Networks},
  pages        = {101-102},
  shortjournal = {Neural Netw.},
  title        = {Advanced deep learning methods for biomedical information analysis: An editorial},
  volume       = {133},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Improved approach to the problem of the global
mittag-leffler synchronization for fractional-order
multidimension-valued BAM neural networks based on new inequalities.
<em>NN</em>, <em>133</em>, 87–100. (<a
href="https://doi.org/10.1016/j.neunet.2020.10.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies the problem of the global Mittag-Leffler synchronization for fractional-order multidimension-valued BAM neural networks (FOMVBAMNNs) with general activation functions (AFs). First, the unified model is established for the researched systems of FOMVBAMNNs which can be turned into the corresponding multidimension-valued systems as long as the state variables, the connection weights and the AFs of the neural networks are valued to be real, complex, or quaternion. Then, without any decomposition, the criteria in unified form are derived by constructing the new Lyapunov–Krasovskii functionals (LKFs) in vector form, combining two new inequalities and considering the easy controllers. It is worth mentioning that the obtained criteria have many advantages in higher flexibility, more diversity, smaller computation, and lower conservatism. Finally, a simulation example is provided to illustrate the availability and improvements of the acquired results.},
  archive      = {J_NN},
  author       = {Jianying Xiao and Shouming Zhong and Shiping Wen},
  doi          = {10.1016/j.neunet.2020.10.008},
  journal      = {Neural Networks},
  pages        = {87-100},
  shortjournal = {Neural Netw.},
  title        = {Improved approach to the problem of the global mittag-leffler synchronization for fractional-order multidimension-valued BAM neural networks based on new inequalities},
  volume       = {133},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). CEGAN: Classification enhancement generative adversarial
networks for unraveling data imbalance problems. <em>NN</em>,
<em>133</em>, 69–86. (<a
href="https://doi.org/10.1016/j.neunet.2020.10.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The data imbalance problem in classification is a frequent but challenging task. In real-world datasets, numerous class distributions are imbalanced and the classification result under such condition reveals extreme bias in the majority data class. Recently, the potential of GAN as a data augmentation method on minority data has been studied. In this paper, we propose a classification enhancement generative adversarial networks (CEGAN) to enhance the quality of generated synthetic minority data and more importantly, to improve the prediction accuracy in data imbalanced condition. In addition, we propose an ambiguity reduction method using the generated synthetic minority data for the case of multiple similar classes that are degenerating the classification accuracy . The proposed method is demonstrated with five benchmark datasets. The results indicate that approximating the real data distribution using CEGAN improves the classification performance significantly in data imbalanced conditions compared with various standard data augmentation methods.},
  archive      = {J_NN},
  author       = {Sungho Suh and Haebom Lee and Paul Lukowicz and Yong Oh Lee},
  doi          = {10.1016/j.neunet.2020.10.004},
  journal      = {Neural Networks},
  pages        = {69-86},
  shortjournal = {Neural Netw.},
  title        = {CEGAN: Classification enhancement generative adversarial networks for unraveling data imbalance problems},
  volume       = {133},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multiple graphs learning with a new weighted tensor nuclear
norm. <em>NN</em>, <em>133</em>, 57–68. (<a
href="https://doi.org/10.1016/j.neunet.2020.10.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an effective convex relaxation of the rank minimization model, the tensor nuclear norm minimization based multi-view clustering methods have been attracting more and more interest in recent years. However, most existing clustering methods regularize each singular value equally, restricting their capability and flexibility in tackling many practical problems, where the singular values should be treated differently. To address this problem, we propose a novel weighted tensor nuclear norm minimization (WTNNM) based method for multi-view spectral clustering . Specifically, we firstly calculate a set of transition probability matrices from different views, and construct a 3-order tensor whose lateral slices are composed of probability matrices. Secondly, we learn a latent high-order transition probability matrix by using our proposed weighted tensor nuclear norm, which directly considers the prior knowledge of singular values. Finally, clustering is performed on the learned transition probability matrix, which well characterizes both the complementary information and high-order information embedded in multi-view data. An efficient optimization algorithm is designed to solve the optimal solution. Extensive experiments on five benchmarks demonstrate that our method outperforms the state-of-the-art methods.},
  archive      = {J_NN},
  author       = {Deyan Xie and Quanxue Gao and Siyang Deng and Xiaojun Yang and Xinbo Gao},
  doi          = {10.1016/j.neunet.2020.10.010},
  journal      = {Neural Networks},
  pages        = {57-68},
  shortjournal = {Neural Netw.},
  title        = {Multiple graphs learning with a new weighted tensor nuclear norm},
  volume       = {133},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning interaction dynamics with an interactive LSTM for
conversational sentiment analysis. <em>NN</em>, <em>133</em>, 40–56. (<a
href="https://doi.org/10.1016/j.neunet.2020.10.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conversational sentiment analysis is an emerging, yet challenging subtask of the sentiment analysis problem. It aims to discover the affective state and sentimental change in each person in a conversation based on their opinions. There exists a wealth of interaction information that affects speaker sentiment in conversations. However, existing sentiment analysis approaches are insufficient in dealing with this subtask due to two primary reasons: the lack of benchmark conversational sentiment datasets and the inability to model interactions between individuals. To address these issues, in this paper, we first present a new conversational dataset that we created and made publicly available, named ScenarioSA, to support the development of conversational sentiment analysis models. Then, we investigate how interaction dynamics are associated with conversations and study the multidimensional nature of interactions, which is understandability , credibility and influence. Finally, we propose an interactive long short-term memory (LSTM) network for conversational sentiment analysis to model interactions between speakers in a conversation by (1) adding a confidence gate before each LSTM hidden unit to estimate the credibility of the previous speakers and (2) combining the output gate with the learned influence scores to incorporate the influences of the previous speakers. Extensive experiments are conducted on ScenarioSA and IEMOCAP, and the results show that our model outperforms a wide range of strong baselines and achieves competitive results with the state-of-art approaches.},
  archive      = {J_NN},
  author       = {Yazhou Zhang and Prayag Tiwari and Dawei Song and Xiaoliu Mao and Panpan Wang and Xiang Li and Hari Mohan Pandey},
  doi          = {10.1016/j.neunet.2020.10.001},
  journal      = {Neural Networks},
  pages        = {40-56},
  shortjournal = {Neural Netw.},
  title        = {Learning interaction dynamics with an interactive LSTM for conversational sentiment analysis},
  volume       = {133},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Prespecified-time synchronization of switched coupled neural
networks via smooth controllers. <em>NN</em>, <em>133</em>, 32–39. (<a
href="https://doi.org/10.1016/j.neunet.2020.10.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper considers the prespecified-time synchronization issue of switched coupled neural networks (SCNNs) under some smooth controllers. Different from the traditional finite-time synchronization (FTS), the synchronization time obtained in this paper is independent of control gains, initial values or network topology , which can be pre-set as to the task requirements. Moreover, unlike the existing nonsmooth or even discontinuous FTS control strategies, the new proposed control protocols are fully smooth, which abandon the common fractional power feedbacks or signum functions . Finally, two illustrative examples are provided to illustrate the effectiveness of the theoretical results.},
  archive      = {J_NN},
  author       = {Shao Shao and Xiaoyang Liu and Jinde Cao},
  doi          = {10.1016/j.neunet.2020.10.007},
  journal      = {Neural Networks},
  pages        = {32-39},
  shortjournal = {Neural Netw.},
  title        = {Prespecified-time synchronization of switched coupled neural networks via smooth controllers},
  volume       = {133},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Model-free motion control of continuum robots based on a
zeroing neurodynamic approach. <em>NN</em>, <em>133</em>, 21–31. (<a
href="https://doi.org/10.1016/j.neunet.2020.10.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a result of inherent flexibility and structural compliance, continuum robots have great potential in practical applications and are attracting more and more attentions. However, these characteristics make it difficult to acquire the accurate kinematics of continuum robots due to uncertainties, deformation and external loads. This paper introduces a method based on a zeroing neurodynamic approach to solve the trajectory tracking problem of continuum robots. The proposed method can achieve the control of a bellows-driven continuum robot just relying on the actuator input and sensory output information, without knowing any information of the kinematic model . This approach reduces the computational load and can guarantee the real time control. The convergence, stability, and robustness of the proposed approach are proved by theoretical analyses. The effectiveness of the proposed method is verified by simulation studies including tracking performance, comparisons with other three methods, and robustness tests .},
  archive      = {J_NN},
  author       = {Ning Tan and Peng Yu and Xinyu Zhang and Tao Wang},
  doi          = {10.1016/j.neunet.2020.10.005},
  journal      = {Neural Networks},
  pages        = {21-31},
  shortjournal = {Neural Netw.},
  title        = {Model-free motion control of continuum robots based on a zeroing neurodynamic approach},
  volume       = {133},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Structural plasticity on an accelerated analog neuromorphic
hardware system. <em>NN</em>, <em>133</em>, 11–20. (<a
href="https://doi.org/10.1016/j.neunet.2020.09.024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In computational neuroscience , as well as in machine learning , neuromorphic devices promise an accelerated and scalable alternative to neural network simulations. Their neural connectivity and synaptic capacity depend on their specific design choices, but is always intrinsically limited. Here, we present a strategy to achieve structural plasticity that optimizes resource allocation under these constraints by constantly rewiring the pre- and postsynaptic partners while keeping the neuronal fan-in constant and the connectome sparse. In particular, we implemented this algorithm on the analog neuromorphic system BrainScaleS-2. It was executed on a custom embedded digital processor located on chip, accompanying the mixed-signal substrate of spiking neurons and synapse circuits. We evaluated our implementation in a simple supervised learning scenario, showing its ability to optimize the network topology with respect to the nature of its training data, as well as its overall computational efficiency.},
  archive      = {J_NN},
  author       = {Sebastian Billaudelle and Benjamin Cramer and Mihai A. Petrovici and Korbinian Schreiber and David Kappel and Johannes Schemmel and Karlheinz Meier},
  doi          = {10.1016/j.neunet.2020.09.024},
  journal      = {Neural Networks},
  pages        = {11-20},
  shortjournal = {Neural Netw.},
  title        = {Structural plasticity on an accelerated analog neuromorphic hardware system},
  volume       = {133},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Reverse graph self-attention for target-directed atomic
importance estimation. <em>NN</em>, <em>133</em>, 1–10. (<a
href="https://doi.org/10.1016/j.neunet.2020.09.022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimating the importance of each atom in a molecule is one of the most appealing and challenging problems in chemistry, physics, and materials science. The most common way to estimate the atomic importance is to compute the electronic structure using density functional theory (DFT), and then to interpret it using domain knowledge of human experts. However, this conventional approach is impractical to the large molecular database because DFT calculation requires large computation, specifically, O ( n 4 ) O(n4) time complexity w.r.t. the number of electronic basis functions. Furthermore, the calculation results should be manually interpreted by human experts to estimate the atomic importance in terms of the target molecular property. To tackle this problem, we first exploit the machine learning-based approach for the atomic importance estimation based on the reverse self-attention on graph neural networks and integrating it with graph-based molecular description. Our method provides an efficiently-automated and target-directed way to estimate the atomic importance without any domain knowledge of chemistry and physics.},
  archive      = {J_NN},
  author       = {Gyoung S. Na and Hyun Woo Kim},
  doi          = {10.1016/j.neunet.2020.09.022},
  journal      = {Neural Networks},
  pages        = {1-10},
  shortjournal = {Neural Netw.},
  title        = {Reverse graph self-attention for target-directed atomic importance estimation},
  volume       = {133},
  year         = {2021},
}
</textarea>
</details></li>
</ul>

</body>
</html>
